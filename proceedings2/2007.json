[
    {
        "title": "Influence of Tempo and Subjective Rating of Music in Step Frequency of Running.",
        "author": [
            "Teemu Tuomas Ahmaniemi"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418107",
        "url": "https://doi.org/10.5281/zenodo.1418107",
        "ee": "https://zenodo.org/records/1418107/files/Ahmaniemi07.pdf",
        "abstract": "The objective of this work was to study how the tempo and subjective motivational rating of personal music influence in step frequency during an exercise session. The participants (n=8) were requested to bring their own music to the test and rate it according to the motivational effect of each song. The test was conducted on a sports field where the participants were asked to perform a 30 minute exercise without paying attention to the test setup. Significant correlation was found between the subjective motivational rating of music and step frequency, while tempo did not have any influence.",
        "zenodo_id": 1418107,
        "dblp_key": "conf/ismir/Ahmaniemi07",
        "keywords": [
            "tempo",
            "subjective motivational rating",
            "personal music",
            "step frequency",
            "exercise session",
            "sports field",
            "significant correlation",
            "motivational effect",
            "music",
            "step frequency"
        ],
        "content": "\u0001 \u0001\n\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0002\u0007\u0006\b\t\u0003\b\n\u0006\u000b\f\t\b\r\u0002\u000e\b\u000f\u0005\u0010\u0011\u0006\u0007\n\u0001\u0012\u0006\b\u0013\r\n\u0001\u0002\u0014\b\t\u0003\b \n\u000b\u0005\u000f\u0001\u0007\b\u0001\u0002\b\u000f\n\u0006\f\b\u0003\u0013\u0006\u0015\u0005\u0006\u0002\u0007\u0016\b\t\u0003\b\u0013\u0005\u0002\u0002\u0001\u0002\u0014 \n\b \n\u0017\u0017\u0018\u0019\b\r\u001a\u0018\u001b\u001c\u001d\u0017\u0018\u001d\b \b\n \u0002\u0003\u0004\u0005\u0006\u0001\u0007\b\t\b\u0006\n\u000b\f\u0001\r\b\u000e\u000f\b\n\u0001\n\u0010\u000f\u0011\u0012\b\n\b\u000e\u0004\u0006\u000f\u0013\u0001\u0014\u0014\u0015\u0014\u0016\u0001\n\u0017\u0010\u0002\u0015\u0018\u0018\u0014\u0019\u0018\u0001\u001a\b\u001b\t\u0005\u000e\u0004\u0005\u0001\n \u0001\n\r\u0010\u000f\n\u0013\r\u0007\n\b\n\u001c\f\b\u0001\u0003\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001\u0003 \u0001\u000f\f\u0005\t\u0001!\u0003\n\u0004\u0001!\u0006\t\u0001\u000f\u0003\u0001\t\u000f\u0013\"#\u0001\f\u0003!\u0001\u000f\f\b\u0001\u000f\b\u0012 $\u0003\u0001 \n\u0006\u000e\"\u0001 \t\u0013\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \n\u0006\u000f\u0005\u000e%\u0001 \u0003 \u0001 $\b\n\t\u0003\u000e\u0006\u001b\u0001 \u0012\u0013\t\u0005 \u000b\u0001 \n\u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001 \u0005\u000e\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \"\u0013\n\u0005\u000e%\u0001 \u0006\u000e\u0001 \b'\b\n\u000b\u0005\t\b\u0001 \t\b\t\t \u0005\u0003\u000e(\u0001\n\u001c\f\b\u0001$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001)\u000e*\u0019+\u0001!\b\n\b\u0001\n\b&\u0013\b\t\u000f\b\"\u0001\u000f\u0003\u0001\u001d\n\u0005\u000e%\u0001\u000f\f\b\u0005 \n\u0001\u0003!\u000e\u0001 \n\u0012\u0013\t\u0005\u000b\u0001\u000f\u0003\u0001\u000f\f\b\u0001\u000f\b\t\u000f\u0001\u0006\u000e\"\u0001\n\u0006\u000f\b\u0001\u0005\u000f\u0001\u0006\u000b\u000b\u0003\n\"\u0005\u000e%\u0001\u000f\u0003\u0001\u000f\f\b\u0001\u0012\u0003\u000f\u0005 \u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \n\b  \b\u000b\u000f\u0001\u0003 \u0001\b\u0006\u000b\f\u0001\t\u0003\u000e%(\u0001\u001c\f\b\u0001\u000f\b\t\u000f\u0001!\u0006\t\u0001\u000b\u0003\u000e\"\u0013\u000b\u000f\b\"\u0001\u0003\u000e\u0001\u0006\u0001\t$ \u0003\n\u000f\t\u0001 \n \u0005\b\u001b\"\u0001!\f\b\n\b\u0001\u000f\f\b\u0001$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001!\b\n\b\u0001\u0006\t\u0004\b\"\u0001\u000f\u0003\u0001$\b\n \u0003\n\u0012\u0001 \u0006\u0001\u0016\u0018\u0001 \n\u0012\u0005\u000e\u0013\u000f\b\u0001\b'\b\n\u000b\u0005\t\b\u0001!\u0005\u000f\f\u0003\u0013\u000f\u0001$\u0006#\u0005\u000e%\u0001\u0006\u000f\u000f\b\u000e\u000f\u0005\u0003\u000e\u0001\u000f\u0003\u0001\u000f\f\b\u0001\u000f\b\t \u000f\u0001\t\b\u000f\u0013$(\u0001\n,\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u0001\u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001!\u0006\t\u0001 \u0003\u0013\u000e\"\u0001\u001d\b\u000f!\b\b\u000e\u0001\u000f\f\b\u0001\t\u0013\u001d\u001e\b \u000b\u000f\u0005\u001f\b\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \n\u0006\u000f\u0005\u000e%\u0001 \u0003 \u0001 \u0012\u0013\t\u0005\u000b\u0001 \u0006\u000e\"\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#-\u0001 !\f \u0005\u001b\b\u0001\n\u000f\b\u0012$\u0003\u0001\"\u0005\"\u0001\u000e\u0003\u000f\u0001\f\u0006\u001f\b\u0001\u0006\u000e#\u0001\u0005\u000e \u001b\u0013\b\u000e\u000b\b(\u0001\u0001\n\u001e\u001f \u0001\u0001\u0002\n\u0013\t\u000e\u0005\u0007\n\u0001\t\u0002\b\n.\u001b\u0003\u000e%\u0001!\u0005\u000f\f\u0001\u000f\f\b\u0001\"\b\u001f\b\u001b\u0003$\u0012\b\u000e\u000f\u0001\u0003 \u0001\u0012\u0003\u001d\u0005\u001b\b\u0001\u0012\u0013\t\u0005\u000b\u0001$\u001b\u0006#\b\n\t\u0001 \u0006\u000e\"\u0001 \n\u0005\u000e\u000b\n\b\u0006\t\b\"\u0001\t\u000f\u0003\n\u0006%\b\u0001\u000b\u0006$\u0006\u000b\u0005\u000f#-\u0001\u0012\u0003\n\b\u0001\u0006\u000f\u000f\b\u000e\u000f\u0005\u0003\u000e\u0001\u0012\u0013\t\u000f\u0001\u001d\b\u0001 $\u0006\u0005\"\u0001 \n\u000f\u0003\u0001 \b\u0006\t#\u0001 \u0012\u0006\u000e\u0006%\b\u0012\b\u000e\u000f\u0001 \u0003 \u0001 \u000f\f\b\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000b\u0003\u000e\u000f\b\u000e\u000f(\u0001 \u0010\u000e\u0001 \u0006\u0001 \n\u000b\f\u0006\u001b\u001b\b\u000e%\u0005\u000e%\u0001 \u000b\u0003\u000e\u000f\b'\u000f\u0001 \u0003 \u0001 \u0013\t\b\u0001 \t\u0013\u000b\f\u0001 \u0006\t\u0001 \t$\u0003\n\u000f-\u0001 \u000b\u0003\n\n\b\u000b\u000f\u001b# \u0001 \n\t\b\u001b\b\u000b\u000f\b\"\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u0012\u0006#\u0001 \u0006\u001b\t\u0003\u0001 \u0012\u0006\u0005\u000e\u000f\u0006\u0005\u000e\u0001 \u000f\f\b\u0001 \b'\b\n\u000b\u0005\t\u0005\u000e%\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e(\u0001/\u0013\t\u0005\u000b\u0001\t\u000f#\u001b\b\u0001\u0006\u000e\"\u0001\u000f\b\u0012$\u0003\u0001\f\u0006\u001f\b\u0001\t\f\u0003!\u000e\u0001\u000f\u0003\u0001\f\u0006\u001f \b\u0001 \n\u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001\u0005\u000e\u0001\u000f\f\b\u0001\b'\b\n\u000b\u0005\t\b(\u0001\u001c\b\u000e\b\u000e\u001d\u0006\u0013\u0012\u0001\b\u000f\u0001\u0006\u001b(\u0001012\u0001$\n\b \t\b\u000e\u000f\b\"\u0001 \n\n\b\t\u0013\u001b\u000f\t\u0001 !\f\b\n\b\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000f#$\b\u0001 \t\b\b\u0012\b\"\u0001 \u000f\u0003\u0001 \f\u0006\u001f\b\u0001 \u0005\u0012$\u0006\u000b\u000f\u0001 \u0003\u000e\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0001 \u0006\u000e\"\u0001 $\b\n \u0003\n\u0012\u0006\u000e\u000b\b\u0001 \u0003 \u0001 \u0006\u0012\u0006\u000f\b\u0013\n\u0001 \n\u0013\u000e\u000e\b\n\t(\u0001 \n\u0010\u000e\t$\u0005\n\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \u0012\u0013\t\u0005\u000b\u0001 !\u0006\t\u0001  \u0003\u0013\u000e\"\u0001 \u000f\u0003\u0001 \u001d\b\u0001 \u0012\u0003\n\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u000e% \u0001 \n\u000f\f\u0006\u000e\u0001\n\u0003\u000b\u0004-\u0001\"\u0006\u000e\u000b\b\u0001\u0003\n\u0001\t\u0005\u001b\b\u000e\u000b\b(\u0001/\b\u0005\t\u0001032\u0001\t\u000f\u0013\"\u0005\b\"\u0001\u000f\f\b\u0001\b   \b\u000b\u000f\u0001\u0003 \u0001 \n$\n\b \b\n\n\b\"-\u0001\u000e\u0003\u000e\u0015$\n\b \b\n\n\b\"\u0001\u0006\u000e\"\u0001\u000e\b\u0013\u000f\n\u0006\u001b\u0001\u0012\u0013\t\u0005\u000b\u0001\u0003\u000e\u0001\u0006\u0001\u000b#\u000b \u001b\u0005\u000e%\u0001 \n\u000f\u0006\t\u0004(\u0001 \u001c\f\b\u0001 \u0012\u0003\t\u000f\u0001 \t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u0001 \"\u0005  \b\n\b\u000e\u000b\b\t\u0001 !\b\n\b\u0001  \u0003\u0013\u000e\"\u0001 \u0005 \u000e\u0001 \n\t\u0013\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001\u0007\u0006\u000f\u0005\u000e%\u0001\u0003 \u00014\b\n\u000b\b\u0005\u001f\b\"\u00015'\b\n\u000f\u0005\u0003\u000e\u0001)\u000745+-\u0001 \u0006\u000f\u0005 %\u0013\b-\u0001 \n\u000f\b\u000e\t\u0005\u0003\u000e\u0001 \u0006\u000e\"\u0001 \u001f\u0005%\u0003\n(\u0001 5  \b\u000b\u000f\u0001 \u0003 \u0001 \u000b\f\u0006\u000e%\u0005\u000e%\u0001 \u000f\f\b\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000f#$ \b\u0001 \n\"\u0013\n\u0005\u000e%\u0001\u000f\f\b\u0001\b'\b\n\u000b\u0005\t\b\u0001\t\b\t\t\u0005\u0003\u000e\u0001!\u0006\t\u0001\t\u000f\u0013\"\u0005\b\"\u0001\u001d#\u0001,6\u0006\u001d\u0003-\u0001, \u0012\u0006\u001b\u001b\u0001 \n\u0006\u000e\"\u0001 7\b\u0005%\f\u0001 082(\u0001 \u0010\u000f\u0001 !\u0006\t\u0001 \n\b$\u0003\n\u000f\b\"\u0001 \u000f\f\u0006\u000f\u0001 \t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u001b#\u0001\f \u0005%\f\b\n\u0001 \n!\u0003\n\u0004\u001b\u0003\u0006\"\u0001!\u0006\t\u0001\u0006\u000b\u000b\u0003\u0012$\u001b\u0005\t\f\b\"\u0001!\f\b\u000e\u0001\u0012\u0013\t\u0005\u000b\u0001\u0005\t\u0001\u0006\n\n\u0006\u000e%\b\"\u0001\t\u0003 \u0001 \n\u000f\f\u0006\u000f\u0001\u0005\u000f\u0001\u001d\b\u000b\u0003\u0012\b\t\u0001 \u0006\t\u000f\b\n\u0001\u000f\u0003!\u0006\n\"\t\u0001\u000f\f\b\u0001\b\u000e\"\u0001\u0003 \u0001\u000f\f\b\u0001\t\b\t\t\u0005 \u0003\u000e(\u0001\u0001\n9\u0006\n\u0006%\b\u0003\n%\f\u0005\t-\u0001 \u001c\b\n\n#\u0001 \u0006\u000e\"\u0001 7\u0006\u000e\b\u0001 0:2\u0001 \u0005\u000e\u000f\n\u0003\"\u0013\u000b\b\"\u0001 \u0006\u0001 \n\u001f\u0006\u001b\u0005\"\u0006\u000f\u0005\u0003\u000e\u0001 \u0005\u000e\t\u000f\n\u0013\u0012\b\u000e\u000f\u0001  \u0003\n\u0001 \b\u001f\u0006\u001b\u0013\u0006\u000f\u0005\u000e%\u0001 \u000f\f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e \u0006\u001b\u0001 \n&\u0013\u0006\u001b\u0005\u000f\u0005\b\t\u0001 \u0003 \u0001 \u0012\u0013\t\u0005\u000b-\u0001 ;\n\u0013\u000e\b\u001b\u0001 /\u0013\t\u0005\u000b\u0001 \u0007\u0006\u000f\u0005\u000e%\u0001 \u0010\u000e\u001f\b\u000e\u000f\u0003\n#\u0001 \n);/\u0007\u0010+(\u0001 \u001c\f\b\u0001 \u0005\u000e\t\u000f\n\u0013\u0012\b\u000e\u000f\u0001 \"\u0005\u001f\u0005\"\b\t\u0001 \u000f\f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \n&\u0013\u0006\u001b\u0005\u000f\u0005\b\t\u0001\u0003 \u0001\u0012\u0013\t\u0005\u000b\u0001\u0005\u000e\u000f\u0003\u0001:\u0001\t\u0013\u001d%\n\u0003\u0013$\t<\u0001\u0006\t\t\u0003\u000b\u0005\u0006\u000f\u0005\u0003\u000e-\u0001\u000b \u0013\u001b\u000f\u0013\n\u0006\u001b\u0001 \n\u0005\u0012$\u0006\u000b\u000f-\u0001 \u0012\u0013\t\u0005\u000b\u0006\u001b\u0005\u000f#\u0001 \u0006\u000e\"\u0001 \n\f#\u000f\f\u0012\u0001 \n\b\t$\u0003\u000e\t\b(\u0001 .\u0001 %\b\u000e\b\n\u0006\u001b\u0001 \n$\n\u0003\u001d\u001b\b\u0012\u0001\u0003 \u0001\u000f\f\b\u0001\u0005\u000e\t\u000f\n\u0013\u0012\b\u000e\u000f\u0001\u0006$$\b\u0006\n\b\"\u0001\u000f\u0003\u0001\u001d\b\u0001\u000f\f\b\u0001\u000b\u0003\u0012$\u001b\b '\u0005\u000f#\u0001\n\u0003 \u0001\u000f\f\b\u0001\u0012\u0013\t\u0005\u000b\u0006\u001b\u0001&\u0013\u0006\u001b\u0005\u000f\u0005\b\t\u0001\u000f\u0003\u0001\u001d\b\u0001\b\u001f\u0006\u001b\u0013\u0006\u000f\b\"-\u0001!\f\u0005\u000b\f\u0001!\u0006\t \u0001\u000f\f\b\u0001 \n\n\b\u0006\t\u0003\u000e\u0001  \u0003\n\u0001 \u000e\u0003\u000f\u0001 \u0013\t\u0005\u000e%\u0001 \u000f\f\b\u0001 \u0005\u000e\t\u000f\n\u0013\u0012\b\u000e\u000f\u0001 \u0005\u000e\u0001 \u000f\f\u0005\t\u0001 \u000f\b\t\u000f(\u0001 \u001c \f\b\u0001 \n\u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001\u0003 \u0001\u000f\f\b\u0001\u0012\u0013\t\u0005\u000b\u0001\n\u0006\u000f\u0005\u000e%\u0001\u0005\u000e\u0001$\b\n \u0003\n\u0012\u0006\u000e\u000b\b\u0001!\u0006\t\u0001\t\u000f \u0013\"\u0005\b\"\u0001 \n\u001d#\u00015\u001b\u001b\u0005\u0003\u000f\u000f-\u0001\r\u0006\n\n\u0001\u0006\u000e\"\u0001=\n\u0012\b\u00010\u00162(\u00017\u0005\t\u000f\b\u000e\u0005\u000e%\u0001\u000f\u0003\u0001\u0012\u0013\t\u0005\u000b\u0001\" \u0013\n\u0005\u000e%\u0001 \n\u000f\f\b\u0001\b'\b\n\u000b\u0005\t\b\u0001\u0005\u000e\u000b\n\b\u0006\t\b\"\u0001\u000f\f\b\u0001\"\u0005\t\u000f\u0006\u000e\u000b\b\u0001\u000f\n\u0006\u001f\b\u001b\b\"\u0001\u0013\t\u0005\u000e%\u0001 \u0006\u0001\u000b#\u000b\u001b\b\u0001 \n\b\n%\u0003\u0001 \u0012\b\u000f\b\n-\u0001 \u001d\u0013\u000f\u0001 \u000f\f\b\n\b\u0001 !\b\n\b\u0001 \u000e\u0003\u0001 \u0003\u001d\t\b\n\u001f\b\"\u0001 \"\u0005  \b\n\b\u000e\u000b\b\t\u0001\n\u001d\b\u000f!\b\b\u000e\u0001 \u000f\f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \u0006\u000e\"\u0001 \u000e\u0003\u000e\u0015\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \u0012\u0013\t\u0005\u000b \u0001 \n\u000b\u0003\u000e\"\u0005\u000f\u0005\u0003\u000e\t(\u0001\u0001\n\u001c\u0003\u0001 \u0005\u000e\u000b\n\b\u0006\t\b\u0001 \n\u0013\u000e\u000e\u0005\u000e%\u0001 \t$\b\b\"-\u0001 \u0012\u0003\n\b\u0001 \b  \u0003\n\u000f\u0001 \f\u0006\t\u0001 \u000f\u0003\u0001 \u001d\b\u0001 \n\u0012\u0006\"\b(\u0001 \u001c\f\b\u0001 \u000b\u0006\t\b\u0001 \u0005\t\u0001 \u0012\u0003\n\b\u0001 \u000b\u0003\u0012$\u001b\b'\u0001 !\u0005\u000f\f\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#( \u0001 \n>\f\b\u000e\u0001\n\u0013\u000e\u000e\u0005\u000e%-\u0001\u000f\f\b\u0001\t$\b\b\"\u0001\u0005\t\u0001\u0005\u000e\u000b\n\b\u0006\t\b\"\u0001\u0012\u0003\t\u000f\u001b#\u0001\u001d#\u0001\u000f\u0006\u0004\u0005 \u000e%\u0001 \n\u001b\u0003\u000e%\b\n\u0001 \t\u000f\b$\t-\u0001 \u000e\u0003\u000f\u0001 \t\u0003\u0001 \u0012\u0013\u000b\f\u0001 \u001d#\u0001 \u0005\u000e\u000b\n\b\u0006\t\u0005\u000e%\u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001 \n \n\b&\u0013\b\u000e\u000b#\u0001 0?2(\u0001 =\u000e\u001b#\u0001 \u0005\u000e\u0001 \u000f\f\b\u0001 \f\u0005%\f\b\n\u0001 \t$\b\b\"\t-\u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001\n \n\b&\u0013\b\u000e\u000b#\u0001 \t\u000f\u0006\n\u000f\t\u0001 \u000f\u0003\u0001 \u0005\u000e\u000b\n\b\u0006\t\b\u0001 \u0012\u0003\n\b\u0001 \t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u001b#(\u0001 >\f \b\u000e\u0001 \n!\u0006\u001b\u0004\u0005\u000e%-\u0001 \u0003\u000e\u0001 \u000f\f\b\u0001 \u0003\u000f\f\b\n\u0001 \f\u0006\u000e\"-\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \u0005\t\u0001 \u0012\u0003\n\b\u0001\n\u001b\u0005\u000e\b\u0006\n\u001b#\u0001$\n\u0003$\u0003\n\u000f\u0005\u0003\u000e\u0006\u001b\u0001\u000f\u0003\u0001\u000f\f\b\u0001\t$\b\b\"\u0001!\u0005\u000f\f\u0005\u000e\u0001\u000f\f\b\u0001!\f\u0003\u001b\b \u0001\n\u0006\u000e%\b\u0001 \n0\u00142(\u0001\u001a\u0003!\b\u001f\b\n-\u0001\u000f\f\b\u0001\u0004\b#\u0001&\u0013\b\t\u000f\u0005\u0003\u000e\u0001\u0006\u000b\u000b\u0003\n\"\u0005\u000e%\u0001\u000f\u0003\u0001\u000f\f\u0005\t\u0001\t\u000f \u0013\"#\u0001\u0005\t<\u0001 \n\u0005 \u0001 \u000f\f\b\n\b\u0001 \u0006\n\b\u0001 \u001f\u0006\n\u0005\u0006\u000f\u0005\u0003\u000e\t\u0001 \u0005\u000e\u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \"\u0013\n\u0005\u000e %\u0001 \n\n\u0013\u000e\u000e\u0005\u000e%-\u0001 \"\u0003\u0001 \u000f\f\b#\u0001 \u000b\u0003\n\n\b\u001b\u0006\u000f\b\u0001 !\u0005\u000f\f\u0001 \u000f\f\b\u0001 \u001f\u0006\n\u0005\u0006\u000f\u0005\u0003\u000e\t\u0001 \u0005\u000e\u0001 \u0012 \u0013\t\u0005\u000b\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001\n\u0006\u000f\u0005\u000e%\u0001\u0003\n\u0001\u000f\b\u0012$\u0003@\u0001\n\u0010\u000e\u0001 \u000f\f\u0005\t\u0001 \u000f\b\t\u000f-\u0001 \b  \b\u000b\u000f\u0001 \u0003 \u0001 \u0012\u0013\t\u0005\u000b\u0001 \t\u0013\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e \u0006\u001b\u0001 \n\n\u0006\u000f\u0005\u000e%\u0001\u0006\u000e\"\u0001\u000f\b\u0012$\u0003\u0001\u0005\u000e\u0001\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b#\u0001!\u0006\t\u0001\u0012\b\u0006\t\u0013\n\b\"(\u0001\u001c\f \b\u0001 \n\t\u000f\u0013\"#\u0001\b\u0012$\f\u0006\t\u0005\t\b\"\u0001\u0003\u000e\u0001$\b\n\t\u0003\u000e\u0006\u001b\u0001\u0012\u0013\t\u0005\u000b\u0001\t\b\u001b\b\u000b\u000f\u0005\u0003\u000e\u0001\u0006\u000e\"\u0001\u000e\u0003 \u000e\u0015\n\u001b\u0006\u001d\u0003\n\u0006\u000f\u0003\n#\u0015\u001b\u0005\u0004\b\u0001 \b\u000e\u001f\u0005\n\u0003\u000e\u0012\b\u000e\u000f(\u0001 \u001c\f\b\u0001 \u0006\u0005\u0012\u0001 !\u0006\t\u0001 \u000f\u0003\u0001  \u0005\u000e\"\u0001 \u0003\u0013 \u000f\u0001\n\f\u0003!\u0001\u0012\u0013\u000b\f\u0001\u0012\u0013\t\u0005\u000b\u0001\u000b\u0006\u000e\u0001\u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001\u0005\u000e\u0001\u000f\f\b\u0001\b'\b\n\u000b\u0005\t\b\u0001\u0005\u000e\u000f\b\u000e\t \u0005\u000f#\u0001 \n!\u0005\u000f\f\u0005\u000e\u0001 \u000f\f\b\u0001 \t\b\t\t\u0005\u0003\u000e(\u0001 \u001c\f\u0005\t\u0001 \u000b\u0003\u0013\u001b\"\u0001 $\n\u0003\u001f\u0005\"\b\u0001 \u0012\u0003\n\b\u0001\u0005\u000e\t\u0005%\f\u000f \u0001\u0003\u000e\u0001\n!\f\b\u000f\f\b\n\u0001\u000f\f\b\u0001\u0013\t\b\n\u0001\u000b\u0006\u000e\u0001\u001d\b\u0001\u000b\u0003\u0006\u000b\f\b\"\u0001\u0005\u000e\u000f\u0003\u0001\u0006\u0001\"\b\t\u0005\n\b\"\u0001\u0005\u000e\u000f\b \u000e\t\u0005\u000f#\u0001 \n$\n\u0003 \u0005\u001b\b\u0001\u0003\u000e\u001b#\u0001!\u0005\u000f\f\u0001\u0012\u0013\t\u0005\u000b(\u0001\n \u001f \u0001\n\u0006\u000f\n\b\r\u0013\u0013\r\u0002\u0014\u0006\u000b\u0006\u0002\n\u000f\b\n\u001c\f\b\u0001\u0012\u0006\u0005\u000e\u0001$\n\u0005\u000e\u000b\u0005$\u001b\b\t\u0001 \u0003\n\u0001\u000f\f\b\u0001 \u0005\b\u001b\"\u0001\u000f\b\t\u000f\t\u0001!\b\n\b<\u0001\u0006+\u0001\u001c\f \b\u0001\u000f\b\t\u000f\u0001 \n\t\f\u0003\u0013\u001b\"\u0001\u001d\b\u0001\u0006\n\n\u0006\u000e%\b\"\u0001\u0005\u000e\u0001\u0006\t\u0001\u000e\u0006\u000f\u0013\n\u0006\u001b\u0001\b\u000e\u001f\u0005\n\u0003\u000e\u0012\b\u000e\u000f\u0001 \u0003\n\u0001\b' \b\n\u000b\u0005\t\b\u0001 \n\u0006\t\u0001 $\u0003\t\t\u0005\u001d\u001b\b\u0001 \u001d+\u0001 \u000f\f\b\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000f\u0003\u0001 \u001d\b\u0001 \u0013\t\b\"\u0001 \t\f\u0003\u0013\u001b\"\u0001 \u001d\b\u0001 \u0003 \u0001 \n$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000fA\t\u0001\u0003!\u000e\u0001$\b\n\t\u0003\u000e\u0006\u001b\u0001\u000b\f\u0003\u0005\u000b\b(\u0001\u0001\n\u001c\f\b\u0001\u0012\u0013\t\u0005\u000b\u0001$\u001b\u0006#\u001d\u0006\u000b\u0004\u0001\"\b\u001f\u0005\u000b\b\u0001!\u0006\t\u0001\u0006\u0001\u0012\u0003\u001d\u0005\u001b\b\u0001$\f\u0003\u000e\b-\u0001!\u0005\u000f\f\u0001\n\f\b\u0006\"$\f\u0003\u000e\b\t(\u0001,\b\u000e\t\u0003\n\u0001\f\u0006\n\"!\u0006\n\b\u0001\"\b\u001f\b\u001b\u0003$\b\"\u0001 \u0003\n\u0001\u000f\f\b\u0001$\u0013\n$\u0003 \t\b\u0001 \n$\n\u0003\u001f\u0005\"\b\"\u0001 \u0006\u0001 \u0016\u0015\"\u0005\u0012\b\u000e\t\u0005\u0003\u000e\u0006\u001b\u0001 \u0006\u000b\u000b\b\u001b\b\n\u0006\u000f\u0005\u0003\u000e\u0001 \t\u0005%\u000e\u0006\u001b\u0001 \u0006\u000e\"\u0001 \n!\u0005\n\b\u001b\b\t\t\u0001 \u000b\u0003\u000e\u000e\b\u000b\u000f\u0005\u0003\u000e\u0001 \u000f\u0003\u0001 \u000f\f\b\u0001 \u0012\u0003\u001d\u0005\u001b\b\u0001 $\f\u0003\u000e\b(\u0001 \u001c\f\b\u0001 \u0012\u0013\t\u0005\u000b\u0001\n$\u001b\u0006#\b\n\u0001 \u0006$$\u001b\u0005\u000b\u0006\u000f\u0005\u0003\u000e\u0001 \u0005\u000e\u0001 \u000f\f\b\u0001 \u0012\u0003\u001d\u0005\u001b\b\u0001 $\f\u0003\u000e\b\u0001 \u0006\u000e\u0006\u001b#\t\b\"\u0001 \u000f\f\b \u0001 \n\t\b\u000e\t\u0003\n\u0001 \t\u0005%\u000e\u0006\u001b\u0001 \u0006\u000e\"\u0001 \u000b\u0006\u001b\u000b\u0013\u001b\u0006\u000f\b\"\u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \u0005\u000e\u0001 3\u0001 \n\t\b\u000b\u0003\u000e\"\u0001\u0005\u000e\u000f\b\n\u001f\u0006\u001b\t\u0001!\u0005\u000f\f\u0001\u0006\u000e\u0001\u0006\u0013\u000f\u0003\u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001\u0006\u001b%\u0003\n\u0005\u000f\f\u0012( \u0001.\u001b\t\u0003-\u0001 \n\u000f\f\b\u0001\u0006$$\u001b\u0005\u000b\u0006\u000f\u0005\u0003\u000e\u0001\t\b\u001b\b\u000b\u000f\b\"\u0001\u000f\f\b\u0001\u000f\n\u0006\u000b\u0004\t\u0001\u000f\u0003\u0001\u001d\b\u0001$\u001b\u0006#\b\"\u0001\"\u0013 \n\u0005\u000e%\u0001\u000f\f\b\u0001 \n\t\b\t\t\u0005\u0003\u000e\u0001\u0006\u000e\"\u0001\t\u0006\u001f\b\"\u0001\u000f\f\b\u0001\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b#\u0001\u0006\u000e\"\u0001\u000b\u0013\n\n\b\u000e\u000f\u0001\u0012\u0013 \t\u0005\u000b\u0001 \n\u000f\n\u0006\u000b\u0004\u0001$\u0006\n\u0006\u0012\b\u000f\b\n\t\u0001\u0005\u000e\u000f\u0003\u0001\u0006\u0001\u001b\u0003%\u0001 \u0005\u001b\b\u0001 \u0003\n\u0001\u001b\u0006\u000f\b\n\u0001\u0006\u000e\u0006\u001b#\t\u0005\t (\u0001\u0001\n\u001c\f\b\u0001$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001)\u000e*\u0019+\u0001 \u0003\n\u0001\u000f\f\b\u0001\u000f\b\t\u000f\u0001!\b\n\b\u0001\t\b\u001b\b\u000b\u000f\b\"\u0001 \n\u0003\u0012\u0001 \n\u0006\u0001\u0012\u0006\n\u0006\u000f\f\u0003\u000e\u0001\u000b\u001b\u0013\u001d\u0001\u0003 \u0001\u0002\u0003\u0004\u0005\u0006\u0001\r\u0003\n$\u0003\n\u0006\u000f\u0005\u0003\u000e\u0001\u0005\u000e\u0001\u001a\b\u001b\t\u0005\u000e\u0004\u0005\u0001\n\b %\u0005\u0003\u000e(\u0001 \n;\b \u0003\n\b\u0001 \u000f\f\b\u0001 \u0006\u000b\u000f\u0013\u0006\u001b\u0001 \u000f\b\t\u000f\u0001 \u000f\f\b\u0001 $\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001 !\b\n\b\u0001 \u0006\t\u0004\b\"\u0001 \u000f\u0003\u0001 \n\t\b\u001b\b\u000b\u000f\u00013\u0018\u0001\u0003\n\u0001\u0012\u0003\n\b\u0001\u000f\n\u0006\u000b\u0004\t\u0001\u0003 \u0001\u0012\u0013\t\u0005\u000b\u0001\u000f\f\u0006\u000f\u0001\u000f\f\b#\u0001\u0013\t\u0013\u0006\u001b\u001b# \u0001\u001b\u0005\t\u000f\b\u000e\u0001 \n\u000f\u0003\u0001 !\f\u0005\u001b\b\u0001 \n\u0013\u000e\u000e\u0005\u000e%(\u0001 \u001c\f\b#\u0001 \n\u0006\u000f\b\"\u0001 \b\u0006\u000b\f\u0001 \t\u0003\u000e%\t\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006 \u001b\u0001 \n\b  \b\u000b\u000f\u0001\u001d#\u0001\u0006\u000e\t!\b\n\u0005\u000e%\u0001\u000f\f\b\u0001&\u0013\b\t\u000f\u0005\u0003\u000e<\u0001B\u001a\u0003!\u0001\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u000e%\u0001\u0005 \t\u0001\u000f\f\u0005\t\u0001 \n\u000f\n\u0006\u000b\u0004\u0001 \u0003\n\u0001#\u0003\u0013\n\u0001\b'\b\n\u000b\u0005\t\b@C\u0001\u001c\f\b\u0001%\u0005\u001f\b\u000e\u0001\t\u000b\u0006\u001b\b\u0001 \u0003\n\u0001\u000f\f\b\u0001\n \u0006\u000f\u0005\u000e%\u0001 \n!\u0006\t\u0001 \n\u0003\u0012\u0001\u0014\u0001\u000f\u0003\u0001\u0014\u0018\u0018\u0001\u0006\u000e\"\u0001$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001!\b\n\b\u0001\u0006\t\u0004\b\"\u0001\u000f\u0003\u0001\u0013\t \b\u0001\u000f\f\b\u0001 \n\n\u0006\u000e%\b\u0001 \u0006\t\u0001 !\u0005\"\b\u001b#\u0001 \u0006\t\u0001 $\u0003\t\t\u0005\u001d\u001b\b(\u0001 \u001c\f\b\u0001 \u000f\b\t\u000f\u0001 !\u0006\t\u0001 \u0006\n\n\u0006\u000e%\b\"\u0001\n\u0003\u0013\u000f\"\u0003\u0003\n\t\u0001\u0005\u000e\u0001\u0006\u0001\t$\u0003\n\u000f\t\u0001 \u0005\b\u001b\"\u0001!\u0005\u000f\f\u0001\u00163\u0018\u0012\u0001\n\u0013\u000e\u000e\u0005\u000e%\u0001\u000f\n\u0006\u000b\u0004( \u0001\u001c\f\b\u0001 \n\u000f\b\t\u000f\u0001\"\u0013\n\u0006\u000f\u0005\u0003\u000e\u0001!\u0006\t\u0001\u0016\u0018\u0001\u0012\u0005\u000e\u0013\u000f\b\t\u0001!\f\u0005\u000b\f\u0001\b\u000e\u0006\u001d\u001b\b\"\u0001$\u001b\u0006#\u001d\u0006\u000b\u0004 \u0001\u0003 \u0001 \n8\u0015\u0014\u0018\u0001\t\u0003\u000e%\t\u0001$\b\n\u0001\t\b\t\t\u0005\u0003\u000e(\u0001\u0001\nD\u0001?\u0018\u00181\u0001.\u0013\t\u000f\n\u0005\u0006\u000e\u0001\r\u0003\u0012$\u0013\u000f\b\n\u0001,\u0003\u000b\u0005\b\u000f#\u0001)=\rE+(\u0001\n\u0001\u0001 \u0001\n\u0001\n!\u001f \u0001\u0013\u0006\u000f\u0005\u0004\n\u000f\b\n\u001c\f\b\u0001 \u000e\u0013\u0012\b\n\u0005\u000b\u0006\u001b\u0001 \u0003\u0013\u000f$\u0013\u000f\u0001 \u0003 \u0001 \u000f\f\b\u0001  \u0005\b\u001b\"\u0001 \u000f\b\t\u000f\u0001 !\u0006\t\u0001 \u000f\f\b\u0001 \u001b\u0003%\u0001  \u0005\u001b\b\u0001 \n$\n\u0003\"\u0013\u000b\b\"\u0001\u001d#\u0001\u000f\f\b\u0001\u0012\u0013\t\u0005\u000b\u0001$\u001b\u0006#\b\n\u0001\u0006$$\u001b\u0005\u000b\u0006\u000f\u0005\u0003\u000e\u0001\u0005\u000e\u0001\u000f\f\b\u0001\u0012\u0003\u001d \u0005\u001b\b\u0001 \n$\f\u0003\u000e\b-\u0001\u0005\u001b\u001b\u0013\t\u000f\n\u0006\u000f\b\"\u0001\u0005\u000e\u0001\u0017\u0005%(\u0001\u0014(\u0001\u0001\n\u0001\n\u0003\u001d\"\u0019#\u0017\b\u001e (\u00017\u0003%\u0001 \u0005\u001b\b\u0001\u0005\u001b\u001b\u0013\t\u000f\n\u0006\u000f\u0005\u0003\u000e \u0001\n\u001c\f\b\u0001 \u0005\u001b\b\u0001\u000b\u0003\u000e\u000f\u0006\u0005\u000e\b\"\u0001\"\b\u000f\u0006\u0005\u001b\t\u0001\u0003 \u0001\b\u0006\u000b\f\u0001\t\b\t\t\u0005\u0003\u000e\u0001\t\u0006\u0012$\u001b\b\"\u0001 \u0005\u000e\u00013\u0001 \n\t\b\u000b\u0003\u000e\"\u0001$\b\n\u0005\u0003\"\t\u0001\u0005\u000e\u000b\u001b\u0013\"\u0005\u000e%\u0001\u000f\f\b\u0001\u0006\u001f\b\n\u0006%\b\u0001\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b# \u0001\u0003 \u0001 \n\u000f\f\b\u0001\u001b\u0006\t\u000f\u00013\u0001\t\b\u000b\u0003\u000e\"\t\u0001)\t\u0003\u001b\u0005\"\u0001\u001b\u0005\u000e\b+-\u0001\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001\n\u0006\u000f\u0005\u000e %\u0001)\"\u0003\u000f\u000f\b\"\u0001 \n\u001b\u0005\u000e\b+\u0001 \u0006\u000e\"\u0001 \u000f\b\u0012$\u0003\u0001 )\"\u0006\t\f\b\"\u0001 \u001b\u0005\u000e\b+(\u0001 \u0017\u0005\n\t\u000f-\u0001 \u000f\f\b\u0001 \u0006\u001f\b\n\u0006%\b\u0001 \t \u000f\b$\u0001 \n \n\b&\u0013\b\u000e\u000b#\u0001 \"\u0013\n\u0005\u000e%\u0001 \b\u0006\u000b\f\u0001 \t\u0003\u000e%\u0001 !\u0006\t\u0001 \u000b\u0006\u001b\u000b\u0013\u001b\u0006\u000f\b\"(\u0001 \u0002\u0003!-\u0001 \u000f\f\b \u0001 \n\u0013\u000e\u0005\u000f\u0001 \u0003 \u0001 \u0006\u000e\u0006\u001b#\t\u0005\t\u0001 !\u0006\t\u0001 \u000b\f\u0006\u000e%\b\"\u0001 \u0005\u000e\u000f\u0003\u0001 \u0006\u0001 \t\u0003\u000e%\u0001 \u0005\u000e\t\u000f\b\u0006\"\u0001 \u0003 \u0001 3\u0001 \n\t\b\u000b\u0003\u000e\"\u0001\u0005\u000e\u000f\b\n\u001f\u0006\u001b(\u0001\u0010\u000e\u0001\u0003\n\"\b\n\u0001\u000f\u0003\u0001\b&\u0013\u0006\u001b\u00056\b\u0001\u000f\f\b\u0001\u001f\u0006\n\u0005\u0003\u0013\t\u0001\n \u0006\u000f\u0005\u000e%\t-\u0001\n\u000f\b\u0012$\u0003\t\u0001 \u0006\u000e\"\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b\u0005\b\t\u0001 \u000f\f\b\u0001 \"\u0006\u000f\u0006\u0001 !\u0006\t\u0001 \t\u000b\u0006\u001b\b\"\u0001 \u0005\u000e\u000f \u0003\u0001 \n\n\u0006\u000e%\b\u0001\u001d\b\u000f!\b\b\u000e\u0001\u0015\u0014\u0001\u0006\u000e\"\u0001\u0014\u0001\t\u0003\u0001\u000f\f\u0006\u000f\u0001\u000f\f\b\u0001\u0012\u0005\u000e\u0005\u0012\u0013\u0012\u0001\u001f\u0006\u001b\u0013\b\u0001\u0003 \u0001\u000f\f\b\u0001 \n\t\b\t\t\u0005\u0003\u000e\u0001 !\u0006\t\u0001 \t\b\u000f\u0001 \u000f\u0003\u0001 \u0015\u0014\u0001 \u0006\u000e\"\u0001 \u0012\u0006'\u0005\u0012\u0013\u0012\u0001 \u000f\u0003\u0001 \u0014\u0001 \n\b\t$\b\u000b\u000f\u0005\u001f\b\u001b# (\u0001\n\u0017\u0005\u000e\u0006\u001b\u001b#-\u0001\u0006\u001b\u001b\u0001\u000f\f\b\u0001\t\b\t\t\u0005\u0003\u000e\t\u0001!\b\n\b\u0001\u000b\u0003\u0012\u001d\u0005\u000e\b\"\u0001\u0005\u000e\u000f\u0003\u0001\u0003\u000e\b\u0001\"\u0006 \u000f\u0006\u0001\t\b\u000f\u0001 \n\u0005\u000e\u000b\u001b\u0013\"\u0005\u000e%\u0001 \u0006\u001b\u001b\u0001 \u000f\f\b\u0001 \t\u0003\u000e%\t\u0001 \u0003 \u0001 \u0006\u001b\u001b\u0001 \u0019\u0001 $\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t(\u0001 \u001c\f\b\u0001  \u0005\u000e\u0006\u001b\u0001 \n\"\u0006\u000f\u0006\u0001 \u000b\u0003\u000e\t\u0005\t\u000f\b\"\u0001 \u0003 \u0001 3\u0019\u0001 \u0005\u000f\b\u0012\t\u0001 )\t\u0003\u000e%\t+\u0001 !\u0005\u000f\f\u0001 \u000b\u0003\n\n\b\t$\u0003\u000e\"\u0005 \u000e%\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \n\u0006\u000f\u0005\u000e%-\u0001 \u000f\b\u0012$\u0003-\u0001 \u0006\u000e\"\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#(\u0001 \u0010\u000e\u0001 \u000f\f\b\u0001 \n\u0003\u001d\u000f\u0006\u0005\u000e\b\"\u0001\"\u0006\u000f\u0006-\u0001\u000f\f\b\n\b\u0001!\u0006\t\u0001\u000e\u0003\u0001\t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u0001\u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e \u0001 \u0003\u0013\u000e\"\u0001 \n\u001d\b\u000f!\b\b\u000e\u0001 \u000f\f\b\u0001 \u000f\b\u0012$\u0003\u0001 \u0006\u000e\"\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#(\u0001 \r\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001 \n\u000b\u0003\b  \u0005\u000b\u0005\b\u000e\u000f\u0001  \u0003\n\u0001 \u000f\f\b\u0001 \"\u0006\u000f\u0006\u0001 !\u0006\t\u0001 \u0018(\u0014F\u0001 !\f\u0005\u001b\b\u0001 \u000f\f\b\u0001 \u000f\u0006\u001d\u0013\u001b\u0006\u000f \b\"\u0001 \n\u000f\f\n\b\t\f\u0003\u001b\"\u0001  \u0003\n\u0001 \t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000b\b\u0001 \u0005\t\u0001 \u0018(?1\u0016\u0001 )$*\u0018(\u00183-\u0001 \u000e*3\u0019+( \u0001\n\u0010\u000e\t\u000f\b\u0006\"-\u0001\u000f\f\b\u0001\t\u0013\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001\n\u0006\u000f\u0005\u000e%\u0001$\n\u0003\u001f\b\"\u0001 \u000f\u0003\u0001\f\u0006\u001f\b\u0001 \n\u0006\u0001\u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001!\u0005\u000f\f\u0001\u000f\f\b\u0001\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b#\u0001)\u0017\u0005%(\u0001?+(\u0001\u0001\n\u0001\n\u0003\u001d\"\u0019#\u0017\b (\u0001 ,\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \u0006\t\u0001 \u0006\u0001  \u0013\u000e\u000b\u000f\u0005\u0003\u000e\u0001 \u0003 \u0001 \t\u0013\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001\n\u0006\u000f\u0005\u000e%( \u0001\n\r\u0006\u001b\u000b\u0013\u001b\u0006\u000f\b\"\u0001 \u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001 \u000b\u0003\b  \u0005\u000b\u0005\b\u000e\u000f\u0001 \u001d\b\u000f!\b\b\u000e\u0001 \u000f\f\b\t\b\u0001 \n$\u0006\n\u0006\u0012\b\u000f\b\n\t\u0001 !\u0006\t\u0001 \u0018(8?\u0001 !\f\u0005\u000b\f\u0001 \u0005\t\u0001 \u000b\u001b\b\u0006\n\u001b#\u0001 \u0006\u001d\u0003\u001f\b\u0001 \u000f\f\b\u0001 \u001b\u0005\u0012\u0005 \u000f\u0001 \n\u0018(?1\u0016\u0001)$*\u0018(\u00183-\u0001\u000e*3\u0019+(\u0001\u0001$\u001f \u0001\u000f\u0005\u000b\u000b\r\u0013\u0016\b\r\u0002\u000e\b\u0007\t\u0002\u0007\u0004\u0005\u000f\u0001\t\u0002\u000f\b\n\u0010\u000e\u0001\u000f\f\b\u0001\u000f\b\t\u000f-\u0001\u0006\u0001\t\u0005%\u000e\u0005 \u0005\u000b\u0006\u000e\u000f\u0001\u000b\u0003\n\n\b\u001b\u0006\u000f\u0005\u0003\u000e\u0001!\u0006\t\u0001 \u0003\u0013\u000e\"\u0001\u001d\b \u000f!\b\b\u000e\u0001 \n\u000f\f\b\u0001\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001\n\u0006\u000f\u0005\u000e%\u0001\u0003 \u0001\t\u0003\u000e%\t\u0001\u0006\u000e\"\u0001\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b# (\u00015\u001f\b\u000e\u0001 \n\u000f\f\u0003\u0013%\f\u0001 \u000f\f\b\u0001  \u0005\u000e\"\u0005\u000e%\u0001 \t\b\b\u0012\t\u0001 \u000b\u001b\b\u0006\n-\u0001 \u000f\f\b\n\b\u0001 \u0006\n\b\u0001 \t\u0003\u0012\b\u0001\n\u000b\u0003\u000e\t\u0005\"\b\n\u0006\u000f\u0005\u0003\u000e\t\u0001\u000f\f\u0006\u000f\u0001\t\f\u0003\u0013\u001b\"\u0001\u001d\b\u0001\u000f\u0006\u0004\b\u000e\u0001\u0005\u000e\u000f\u0003\u0001\u0006\u000b\u000b\u0003\u0013\u000e\u000f\u0001\u001d\b  \u0003\n\b\u0001 \n\u0012\u0006\u0004\u0005\u000e%\u0001  \u0013\n\u000f\f\b\n\u0001 \u000b\u0003\u000e\u000b\u001b\u0013\t\u0005\u0003\u000e\t(\u0001 \u0017\u0005\n\t\u000f\u001b#-\u0001 \u000f\f\b\u0001 \t\b\u001b\b\u000b\u000f\b\"\u0001 \n$\u0006\n\u000f\u0005\u000b\u0005$\u0006\u000e\u000f\t\u0001 !\b\n\b\u0001 \u0006\u000b\u000f\u0005\u001f\b\u0001 \u0012\u0006\n\u0006\u000f\f\u0003\u000e\u0001 \n\u0013\u000e\u000e\b\n\t-\u0001 !\f\u0005\u000b\f\u0001 \u0006\n \b\u0001 \n\u000f\n\u0006\u0005\u000e\b\"\u0001 \u0003\n\u0001\b'\b\n\u000b\u0005\t\u0005\u000e%\u0001\u000b\b\n\u000f\u0006\u0005\u000e\u0001!\u0006#(\u0001\u0007\u0003\u0013\u000f\u0005\u000e\b\u0001\u0012\u0006#\u0001\u000b\u0006\u0013 \t\b\u0001\u0006\u0001 \n\n\b\"\u0013\u000b\b\"\u0001 \u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001 \u0003 \u0001 \b'\u000f\b\n\u000e\u0006\u001b\u0001 \t\u000f\u0005\u0012\u0013\u001b\u0005\u0001 \u0005\u000e\u000f\u0003\u0001 \u000f\f\b\u0001 \n$\b\n \u0003\n\u0012\u0006\u000e\u000b\b-\u0001 \b(%(\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000f#$\b\u0001 \u0005\u000e\u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#( \u0001 \n,\b\u000b\u0003\u000e\"\u001b#-\u0001\t\u0005\u000e\u000b\b\u0001\u000f\f\b\u0001\u0012\u0013\t\u0005\u000b\u0001!\u0006\t\u0001\n\u0006\u000f\b\"\u0001\u001d#\u0001\u000f\f\b\u0001\t\u0006\u0012\b\u0001$\b\n \t\u0003\u000e\u0001 \n!\f\u0003\u0001\u0006\u000f\u000f\b\u000e\"\b\"\u0001\u000f\f\b\u0001\n\u0013\u000e\u000e\u0005\u000e%\u0001\u000f\b\t\u000f-\u0001\u000f\f\b\u0001\u001f\u0006\n\u0005\u0006\u000f\u0005\u0003\u000e\t\u0001\u0005\u000e\u0001\u000f\f \b\u0001\t\u000f\b$\u0001 \n \n\b&\u0013\b\u000e\u000b#\u0001\u0012\u0006#\u0001\u001d\b\u0001\u0005\u000e \u001b\u0013\b\u000e\u000b\b\"\u0001\u001d#\u0001\u000f\f\b\u0001\u000b\u0003\u0012\u0012\u0005\u000f\u0012\b\u000e\u000f\u0001\u0003 \u0001\u000f\f \b\u0001 \n$\b\n\t\u0003\u000e\u0001 \u000f\u0003\u0001 \t\f\u0003!\u0001 \u000f\f\u0006\u000f\u0001 \u000f\f\b\u0001 \n\u0006\u000f\u0005\u000e%\u0001 \u0005\t\u0001 \u001f\u0006\u001b\u0005\"(\u0001 \u001a\u0003!\b\u001f\b\n-\u0001 \u000f \f\b\u0001 \n\n\b\t\u0013\u001b\u000f\t\u0001 \u0003 \u0001 \u000f\f\b\u0001 \t\u000f\u0013\"#\u0001 \u0006\n\b\u0001 \u000b\u001b\b\u0006\n\u001b#\u0001 \u0005\u000e\"\u0005\u000b\u0006\u000f\u0005\u000e%\u0001 \u000f\f\u0006\u000f\u0001 \u0012\u0013 \t\u0005\u000b\u0001 \n\n\u0006\u000f\u0005\u000e%\u0001 \f\u0006\t\u0001 \u0006\u0001 \t\u000f\n\u0003\u000e%\u0001 \u0005\u000e \u001b\u0013\b\u000e\u000b\b\u0001 \u0005\u000e\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#(\u0001 /\u0003 \n\b\u0001 \n\n\b\t\b\u0006\n\u000b\f\u0001!\u0005\u000f\f\u0001\u0003\u000f\f\b\n\u0001\u0003\u001d\u001e\b\u000b\u000f\u0005\u001f\b\u0001\u0012\b\u0006\t\u0013\n\b\t\u0001\t\u0013\u000b\f\u0001\u0006\t\u0001\t$\b\b \"\u0001\u0006\u000e\"\u0001 \n\f\b\u0006\n\u000f\u0001\n\u0006\u000f\b\u0001\t\f\u0003\u0013\u001b\"\u0001\u001d\b\u0001\u000b\u0003\u000e\"\u0013\u000b\u000f\b\"(\u0001\n%\u001f \u0001\u0013\u0006\u0003\u0006\u0013\u0006\u0002\u0007\u0006\u000f\b\n0\u00142 \u0001\r\u0006\u001f\u0006%\u000e\u0006-\u0001E(\u0001G\u0001\u0017\n\u0006\u000e6\b\u000f\u000f\u0005-\u00014(\u0001B\u001c\f\b\u0001\"\b\u000f\b\n\u0012\u0005\u000e\u0006\u000e\u000f\t\u0001\u0003 \u0001 \n\t\u000f\b$\u0001 \n\b&\u0013\b\u000e\u000b#\u0001\u0005\u000e\u0001!\u0006\u001b\u0004\u0005\u000e%\u0001\u0005\u000e\u0001\f\u0013\u0012\u0006\u000e\tC(\u0001 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\t\b\n\n\u000b\f\r\u000e\u0002\u0007\u0002\u000f\f -\u0001\u00161\u0016<\u0001?\u00163\u0015?:?-\u0001\u0014F\u00198(\u0001\n0?2 \u0001\r\u0006\u001f\u0006%\u000e\u0006-\u0001E(-\u0001\u0017\n\u0006\u000e6\b\u000f\u000f\u0005-\u00014(-\u0001\u001a\b%\u001b\u0013\u000e\"-\u0001\u0002(-\u0001>\u0005\u001b\u001b\b\u0012\t-\u0001 \n4(\u0001 B\u001c\f\b\u0001 \"\b\u000f\b\n\u0012\u0005\u000e\u0006\u000e\u000f\t\u0001 \u0003 \u0001 \u000f\f\b\u0001 \t\u000f\b$\u0001  \n\b&\u0013\b\u000e\u000b#\u0001 \u0005\u000e\u0001 \n\n\u0013\u000e\u000e\u0005\u000e%-\u0001 \u000f\n\u0003\u000f\u000f\u0005\u000e%\u0001 \u0006\u000e\"\u0001 \f\u0003$$\u0005\u000e%\u0001 \u0005\u000e\u0001 \u0012\u0006\u000e\u0001 \u0006\u000e\"\u0001 \u0003\u000f\f\b\n\u0001 \n\u001f\b\n\u000f\b\u001d\n\u0006\u000f\b\tC(\u0001 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\t\b\n\u000b\f\r\u000e\u0002\u0007\u0002\u000f\f -\u0001 \u0016FF<\u0001 \u0019\u0014\u0015F?-\u0001 \n\u0014F\u0019\u0019(\u0001\n0\u00162 \u00015\u001b\u001b\u0005\u0003\u000f\u000f-\u0001 H(-\u0001 \r\u0006\n\n-\u0001 ,(-\u0001 =\n\u0012\b-\u0001 H(\u0001 B\u001c\f\b\u0001 \b  \b\u000b\u000f\u0001 \u0003 \u0001 \n\u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 \u0012\u0013\t\u0005\u000b\u0001 \u0003\u000e\u0001 \t\u0013\u001d\u0015\u0012\u0006'\u0005\u0012\u0006\u001b\u0001 \b'\b\n\u000b\u0005\t\bC(\u0001 \n\u0010\u0003\u0004\u0002\u0011\u0012\u0006\u0005\b\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\t\b\u0013\u0011\u0002\u0004\u0014\b\u0013\u0015\u000e\u0012\u0005\u0015\u0012 (\u00013)?+<\u0001F1\u0015\u0014\u00188-\u0001 \n?\u0018\u00183(\u0001\n0:2 \u00019\u0006\n\u0006%\b\u0003\n%\f\u0005\t-\u0001 \r(-\u0001 \u001c\b\n\n#-\u0001 4(-\u0001 7\u0006\u000e\b-\u0001 .(\u0001 \nBH\b\u001f\b\u001b\u0003$\u0012\b\u000e\u000f\u0001 \u0006\u000e\"\u0001 \u0005\u000e\u0005\u000f\u0005\u0006\u001b\u0001 \u001f\u0006\u001b\u0005\"\u0006\u000f\u0005\u0003\u000e\u0001 \u0003 \u0001 \u0006\u000e\u0001 \n\u0005\u000e\t\u000f\n\u0013\u0012\b\u000e\u000f\u0001 \u000f\u0003\u0001 \u0006\t\t\b\t\t\u0001 \u000f\f\b\u0001 \u0012\u0003\u000f\u0005\u001f\u0006\u000f\u0005\u0003\u000e\u0006\u001b\u0001 &\u0013\u0006\u001b\u0005\u000f\u0005\b\t\u0001 \u0003 \u0001\n\u0012\u0013\t\u0005\u000b\u0001 \u0005\u000e\u0001 \b'\b\n\u000b\u0005\t\b\u0001 \u0006\u000e\"\u0001 \t$\u0003\n\u000f<\u0001 \u001c\f\b\u0001 ;\n\u0013\u000e\b\u001b\u0001 /\u0013\t\u0005\u000b\u0001 \n\u0007\u0006\u000f\u0005\u000e%\u0001\u0010\u000e\u001f\b\u000e\u000f\u0003\n#C(\u0001 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\t\b\u0013\u0011\u0002\u0004\u0014\b\u0013\u0015\u000e\u0012\u0005\u0015\u0012\r -\u0001\u00141<\u0001\n1\u0014\u0016\u00151?:-\u0001\u0014FFF(\u0001\n032 \u0001/\b\u0005\t-\u0001 I(\u0001 B/\u0003\"\u0005 \u0005\u000b\u0006\u000f\u0005\u0003\u000e\u0001 \u0003 \u0001 $\b\n\u000b\b\u0005\u001f\b\"\u0001 \b\u000e\u001e\u0003#\u0012\b\u000e\u000f-\u0001 \n\b'\b\n\u000f\u0005\u0003\u000e\u0001 \u0006\u000e\"\u0001 $\b\n \u0003\n\u0012\u0006\u000e\u000b\b\u0001 \u0006\u0012\u0003\u000e%\u0001 \u000e\u0003\u001f\u0005\u000b\b\u0001 \u0006\u000e\"\u0001 \n\b'$\b\n\u0005\b\u000e\u000b\b\"\u0001 \b'\b\n\u000b\u0005\t\b\n\t<\u0001 \u0006\u0001 \u000b\u0003%\u000e\u0005\u000f\u0005\u001f\b\u0015\u001d\b\f\u0006\u001f\u0005\u0003\n\u0006\u001b\u0001 \n\u0006$$\n\u0003\u0006\u000b\f\u0001 \u000f\u0003\u0001 $\b\n\u000b\b$\u000f\u0013\u0006\u001b\u0001 \u000b\f\u0006\u000e%\bC(\u0001 H\u0003\u000b\u000f\u0003\n\u0001 \u0003 \u0001 \n4\f\u0005\u001b\u0003\t\u0003$\f#\u0001 \u000f\f\b\t\u0005\t(\u0001 \u001c\f\b\u0001 \u0017\u001b\u0003\n\u0005\"\u0006\u0001 \t\u000f\u0006\u000f\b\u0001 \u0013\u000e\u0005\u001f\b\n\t\u0005\u000f#-\u0001 \n?\u0018\u0018\u0016(\u0001\n082 \u0001,6\u0006\u001d\u0003-\u0001 .(-\u0001 ,\u0012\u0006\u001b\u001b-\u0001 .(-\u0001 7\b\u0005%\f-\u0001 /(\u0001 B\u001c\f\b\u0001 \b  \b\u000b\u000f\t\u0001 \u0003 \u0001 \n\t\u001b\u0003!\u0015\u0001\u0006\u000e\"\u0001 \u0006\t\u000f\u0015$\u0006\u000b\b\"\u0001\u000b\u001b\u0006\t\t\u0005\u000b\u0006\u001b\u0001\u0012\u0013\t\u0005\u000b\u0001\u0003\u000e\u0001$\n\u0003%\n\b\t\t\u0005\u001f\b \u0001 \n\u000b#\u000b\u001b\u0005\u000e%\u0001\u000f\u0003\u0001\u001f\u0003\u001b\u0013\u000e\u000f\u0006\n#\u0001$\f#\t\u0005\u000b\u0006\u001b\u0001\b'\f\u0006\u0013\t\u000f\u0005\u0003\u000eC(\u0001 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0002\t\b\u0013\u0011\u0002\u0004\u0014\r\b\u0016\u0012\u0017\u000e\u0015\u000e\u0005\u0012\b\u0006\u0005\u0017\b\n\u000b\f\r\u000e\u0015\u0006\u0007\b\u0018\u000e\u0014\u0005\u0012\r\r (\u0001\u0016F<\u0001??\u0018\u0015\n??3-\u0001\u0014FFF(\u0001\n012 \u0001\u001c\b\u000e\b\u000e\u001d\u0006\u0013\u0012-\u0001 E(-\u0001 7\u0005\"\u0003\n-\u0001 \u0007(-\u0001 7\u0006\u001f#\u0006\u000e-\u0001 \u0002(-\u0001 /\u0003\n\n\u0003!-\u0001 \n9(-\u0001\u001c\u0003\u000e\u000e\b\u001b-\u0001,(-\u0001E\b\n\t\f%\u0003\n\b\u000e-\u0001.(-\u0001/\b\u0005\t-\u0001I(-\u0001I\u0003\f\u000e\t\u0003\u000e-\u0001\n/(\u0001 B\u001c\f\b\u0001 \b  \b\u000b\u000f\u0001 \u0003 \u0001 \u0012\u0013\t\u0005\u000b\u0001 \u000f#$\b\u0001 \u0003\u000e\u0001 \n\u0013\u000e\u000e\u0005\u000e%\u0001 \n$\b\n\t\b\u001f\b\n\u0006\u000e\u000b\b\u0001 \u0006\u000e\"\u0001 \u000b\u0003$\u0005\u000e%\u0001 !\u0005\u000f\f\u0001 \b  \u0003\n\u000f\u0001 \t\b\u000e\t\u0006\u000f\u0005\u0003\u000e\tC(\u0001 \n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\t\b\u0013\u0011\u0002\u0004\u0014\b\u0006\u0005\u0017\b\u0010\u0019\u0012\u0004\u0015\u000e\r\u0012 -\u00013<\u0001\u0019F\u0015\u0014\u0018F-\u0001?\u0018\u0018:(\u0001\n\u0001\u0002\u0003\u0004\u0005\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0003\u0004\n\u000b \u0007\f\r\u000e\u000f\u0002 \u0007\u0010\u0003\r\u000f\u0007\u0011\b\r\u0012\u0006 \u0007\u0001\u0002\u0003\u0004\u0005 \u0006\n\u0007\u0002\b\t \u0006"
    },
    {
        "title": "Methodological Considerations in Studies of Musical Similarity.",
        "author": [
            "Hamish Allan",
            "Daniel Müllensiefen",
            "Geraint A. Wiggins"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416956",
        "url": "https://doi.org/10.5281/zenodo.1416956",
        "ee": "https://zenodo.org/records/1416956/files/AllanMW07.pdf",
        "abstract": "There are many different aspects of musical similarity [7]. Some relate to acoustic properties, such as melodic [5], rhythmic [10], harmonic [9] and timbral [2]. Others are bound up in cultural aspects: artists involved in creation, year of first release, subject matter of lyrics, demographics of listeners, etc. In judgments about musical similarity, the relative importance of each of these aspects will change, not only for different listeners, but also for the same listener in different contexts [11]. Extra care must therefore be taken when designing studies in musical similarity to ensure that the context is an explicit variable. This paper describes the methodology behind our work in context-based musical similarity; introduces a novel system through which users can specify by example the context and focus of their retrieval needs; and details the design of a study to find parameters for our system which can also be adapted to test the system as a whole. 1 INTRODUCTION Though musical similarity is multi-faceted, it is sometimes considered useful to distill it into a single measure; for example, to present a simple ordered list of results from a search-engine-style query. There are various measures for different aspects of similarity in the literature (for a full exploration of these, see [7]): melodic and timbral measures have generally received the most attention, but rhythmic and harmonic ones have also been considered, and metadata such as artist, lyrics, year of release, sales figures, chart position and label classification may also be examined. A single measure might combine several of these, but the relative weighting of the components of such a combination has a great impact of the utility of the measure. For a start, the perceptual prominence of aspects may vary across listeners; in addition, the context of the query exercise has an impact on which aspects are perceived as most salient [11]. We present a novel technique for allowing users to create example sets of pieces of music which are used to determine a weighting for various existing similarity measures. This allows for users lacking a musical vocabulary c⃝2007 Austrian Computer Society (OCG). (or unwilling to constrict their query into one) to make queries like, “I think all these pieces of music are similar in a way that appeals to me. Find me music that is similar to them in the same way.” The technique also incorporates a negative example set and a feedback loop through which the user can train the aspect more accurately (see Figure 1). We believe this dynamic approach addresses the subjectivity and context issues whilst retaining a good balance between expressive power and ease of use. It differs fundamentally from almost all existing approaches in the genre classification and similarity retrieval literature, in which parameters are fixed after an initial training phase rather than being iteratively tailored to the needs of individual users. The technique only provides a relative weighting for the individual measures. Therefore, we also propose an experimental study to determine one or more initial weightings according to their perceptual salience for an average user (or several classes of user according to a clustering exercise). This is closely related to the task of weighting features to create a perceptually valid similarity measure [6]. However, when the separation of different aspects is considered, certain problematic assumptions are revealed which must be carefully examined. These assumptions, and a rigorous method which overcomes them, are the primary topic of this paper. 2 CONCEPTUAL OUTLINE OF ASPECT WEIGHTING To demonstrate the concept of aspect weighting, we present an example using four distance measures and a corpus of 20 pieces of music. (Our full study will actually use a database of over 14,000 MIDI-encoded western pop songs and over 40 distance measures.) The selection and normalisation of these measures will be covered in detail in the next section, once we have explained our motivation. Figure 2 shows two-dimensional projections of the example measures. For instance, these data might correspond to melodic (D1), rhythmic (D2), harmonic (D3) and timbral (D4) similarities. The projection itself is merely for demonstration: we assume only the pairwise distances between songs, rather than any of the underlying features. The shorter the distance, the greater the similarFigure 1. Choosing positive (left) and negative (right) examples by adding tracks to a playlist. ity, with zero distance between identical pieces. Most of the measures will be metric spaces, though this does not necessarily have to be the case. In our example, the user decides that tracks 2, 4 and 6 are all similar in her chosen aspect. A straightforward way for her to indicate this might be to drag them from a library of titles into a playlist marked “positive” (see Figure 1). Intuitively, this means that D1 and D3, in which those tracks are closer together than in D2 and D4 (see the solid thick lines in Figure 2) should be weighted more heavily. One way of achieving this is to divide the mean distance between all the tracks in Dk by the mean distance between only the tracks in Dk in the positive example set (i.e., dividing the mean of all the line lengths by the mean of only the three solid thick line lengths for each Dk in Figure 2). To avoid division by zero, a small constant should be added to both means. In this example, this gives an overall weighting for D(+{2, 4, 6}, −φ) of",
        "zenodo_id": 1416956,
        "dblp_key": "conf/ismir/AllanMW07",
        "keywords": [
            "musical similarity",
            "acoustic properties",
            "cultural aspects",
            "listeners",
            "context",
            "user input",
            "example sets",
            "negative examples",
            "feedback loop",
            "aspect weighting"
        ],
        "content": "METHODOLOGICAL CONSIDERATIONS IN\nSTUDIES OF MUSICAL SIMILARITY\nHamish Allan, Daniel M ¨ullensiefen, Geraint Wiggins\nGoldsmiths, University of London\nNew Cross, London SE14 6NW\n{hamish.allan,d.mullensiefen,g.wiggins }@gold.ac.uk\nABSTRACT\nThere are many different aspects of musical similarity [7].\nSome relate to acoustic properties, such as melodic [5],\nrhythmic [10], harmonic [9] and timbral [2]. Others are\nbound up in cultural aspects: artists involved in creation,\nyear of ﬁrst release, subject matter of lyrics, demograph-\nics of listeners, etc. In judgments about musical similar-\nity, the relative importance of each of these aspects will\nchange, not only for different listeners, but also for the\nsame listener in different contexts [11]. Extra care must\ntherefore be taken when designing studies in musical sim-\nilarity to ensure that the context is an explicit variable.\nThis paper describes the methodology behind our work in\ncontext-based musical similarity; introduces a novel sys-\ntem through which users can specify by example the con-\ntext and focus of their retrieval needs; and details the de-\nsign of a study to ﬁnd parameters for our system which\ncan also be adapted to test the system as a whole.\n1 INTRODUCTION\nThough musical similarity is multi-faceted, it is some-\ntimes considered useful to distill it into a single measure;\nfor example, to present a simple ordered list of results\nfrom a search-engine-style query. There are various mea-\nsures for different aspects of similarity in the literature (for\na full exploration of these, see [7]): melodic and timbral\nmeasures have generally received the most attention, but\nrhythmic and harmonic ones have also been considered,\nand metadata such as artist, lyrics, year of release, sales\nﬁgures, chart position and label classiﬁcation may also\nbe examined. A single measure might combine several\nof these, but the relative weighting of the components of\nsuch a combination has a great impact of the utility of the\nmeasure. For a start, the perceptual prominence of aspects\nmay vary across listeners; in addition, the context of the\nquery exercise has an impact on which aspects are per-\nceived as most salient [11].\nWe present a novel technique for allowing users to cre-\nate example sets of pieces of music which are used to de-\ntermine a weighting for various existing similarity mea-\nsures. This allows for users lacking a musical vocabulary\nc/circlecopyrt2007 Austrian Computer Society (OCG).(or unwilling to constrict their query into one) to make\nqueries like, “I think all these pieces of music are similar\nin a way that appeals to me. Find me music that is similar\nto them in the same way.” The technique also incorpo-\nrates a negative example set and a feedback loop through\nwhich the user can train the aspect more accurately (see\nFigure 1). We believe this dynamic approach addresses\nthe subjectivity and context issues whilst retaining a good\nbalance between expressive power and ease of use. It dif-\nfers fundamentally from almost all existing approaches in\nthe genre classiﬁcation and similarity retrieval literature,\nin which parameters are ﬁxed after an initial training phase\nrather than being iteratively tailored to the needs of indi-\nvidual users.\nThe technique only provides a relative weighting for\nthe individual measures. Therefore, we also propose\nan experimental study to determine one or more initial\nweightings according to their perceptual salience for an\naverage user (or several classes of user according to a\nclustering exercise). This is closely related to the task of\nweighting features to create a perceptually valid similar-\nity measure [6]. However, when the separation of differ-\nent aspects is considered, certain problematic assumptions\nare revealed which must be carefully examined. These as-\nsumptions, and a rigorous method which overcomes them,\nare the primary topic of this paper.\n2 CONCEPTUAL OUTLINE OF ASPECT\nWEIGHTING\nTo demonstrate the concept of aspect weighting, we\npresent an example using four distance measures and a\ncorpus of 20 pieces of music. (Our full study will actually\nuse a database of over 14,000 MIDI-encoded western pop\nsongs and over 40 distance measures.) The selection and\nnormalisation of these measures will be covered in detail\nin the next section, once we have explained our motiva-\ntion.\nFigure 2 shows two-dimensional projections of the ex-\nample measures. For instance, these data might corre-\nspond to melodic ( D1), rhythmic ( D2), harmonic ( D3)\nand timbral ( D4) similarities. The projection itself is\nmerely for demonstration: we assume only the pairwise\ndistances between songs, rather than any of the underlying\nfeatures. The shorter the distance, the greater the similar-Figure 1 . Choosing positive (left) and negative (right) ex-\namples by adding tracks to a playlist.\nity, with zero distance between identical pieces. Most of\nthe measures will be metric spaces, though this does not\nnecessarily have to be the case.\nIn our example, the user decides that tracks 2, 4 and\n6 are all similar in her chosen aspect. A straightforward\nway for her to indicate this might be to drag them from a\nlibrary of titles into a playlist marked “positive” (see Fig-\nure 1). Intuitively, this means that D1andD3, in which\nthose tracks are closer together than in D2andD4(see\nthe solid thick lines in Figure 2) should be weighted more\nheavily. One way of achieving this is to divide the mean\ndistance between all the tracks in Dkby the mean dis-\ntance between only the tracks in Dkin the positive ex-\nample set (i.e., dividing the mean of all the line lengths\nby the mean of only the three solid thick line lengths for\neach Dkin Figure 2). To avoid division by zero, a small\nconstant should be added to both means. In this example,\nthis gives an overall weighting for D(+{2,4,6},−φ)of\n3.000·D1+ 1.536·D2+ 3.312·D3+ 1.347·D4. Ac-\ncording to this weighting, the nearest track on average to\ntracks 2, 4 and 6 is track 9. However, let us imagine that\nwhen we report this to the user, she puts track 9 into the\nnegative example set. This means that our weighting does\nnot reﬂect the aspect she had in mind. We need to incorpo-\nrate the negative example set, for example by multiplying\nour existing weighting for Dkby the mean distance be-\ntween all [positive, negative] pairs in Dk(i.e., the mean\nlength of the three dashed lines for each Dkin Figure 2).\nContrary to initial appearances, D4is now a much better\nmeasure for the aspect the user has in mind. In this ex-\nample, the overall weighting D(+{2,4,6},−{9})is now\n0.462·D1+ 0.270·D2+ 0.492·D3+ 1.030·D4. Ac-\ncording to this weighting, the track nearest to tracks 2, 4\nand 6 and furthest from 9 on average is now track 17 (the\nsystem can of course report more than one closest track).\nThe user can continue to add positive and negative tracks\nto tune her description of the aspect she is interested in.\nWe believe that this feedback cycle is a good way to de-\ntermine the subjective and context-based similarity needs\nof an average user lacking a comprehensive agreed musi-\ncal vocabulary. The authors are aware of one other piece\nof work [4] using relevance feedback in a similar way, but\nwhich requires both positive and negative initial examples\nand has a more complicated feedback cycle. There are,\nhowever, commercial recommendation systems based on\ncollaborative ﬁltering (e.g. Last.fm, Pandora) which use a\nthumbs-up / thumbs-down style of interface.\nD1\n1\n2\n345\n678\n9\n1011121314\n1516\n17\n1819\n20\nD2\n12\n34\n56\n78\n910\n111213\n1415\n161718\n1920\nD3\n1\n234\n567\n89\n1011\n12\n1314151617\n1819\n20\nD4\n12\n34\n56\n7 8\n91011\n1213\n14\n15161718\n1920Figure 2 . Two-dimensional projections of example dis-\ntance measures.3 DESIGN OF USER STUDY TO DETERMINE\nMUSICAL SIMILARITY\nThe example in Section 2 gives the distance measures\nequal weighting before the example sets are taken into ac-\ncount. However, there is no reason to assume that they all\nrelate to aspects with equal perceptual salience. We have\ntherefore designed an experimental study to determine an\ninitial weighting for an average user. Although the bal-\nance of salient aspects may in fact be unique to each lis-\ntener, an initial weighting derived from a study is likely to\nbe better than a ﬂat equal weighting. The results from the\nstudy may form clusters, in which case modelling more\nthan one type of average user may be beneﬁcial.\nThere is also the question of which distance measures\nto select from the literature. Measures for which the task\nis more closely speciﬁed (e.g., “similarity in melodic con-\ntour” or “similarity in rhythmic complexity”) are likely\nto have more perceptually reliable experimental valida-\ntion, so we select those in preference to measures claim-\ning ‘generic’ similarity. We select as many measures as\nwe can, with the proviso that we must be able to ﬁnd a\nsampling of the corpus such that every measure has a sim-\nilar distribution of pairwise distances in the sample to that\nit has in the whole corpus. Because the sampling of a sub-\nset of the corpus is not independent from the whole set,\nwe compare the distributions by taking the difference be-\ntween the cumulative distribution functions for the whole\nset and the subset (see Figure 3; for more about using\nCDF to compare distribution, see [1]). The size of the\nsample is limited by the pragmatics of the user study as\ndescribed below. We must also be able to normalise each\nmeasure so that the linear combination described above\nmakes sense; as the pairwise distances are never negative,\nthis can be achieved by dividing each measure’s distances\nby their mean to give a range of values from 0 upwards\nwith a mean of 1, giving each measure a similar scale\nwhilst also allowing outliers to be incorporated in the ex-\nample sets. Linear combinations of normalised measures\nmay still be problematic if the percept to which the mea-\nsure corresponds is non-linear or if the measures are not\nfully independent; further research needs to be done to de-\ntermine the optimal combination. Also, we may have to\nrestrict the number of measures selected, to avoid overﬁt-\nting.\nOne way in which pieces of music can be similar is\nif they have the same large-scale structure; for example,\nquiet verse followed by loud chorus. However, we will be\npresenting short excerpts rather than whole pieces to sub-\njects, and for the sake of simplicity the excerpts we choose\nwill be largely self-similar (self-similarity being assessed\nusing the same distance measures as those selected for the\nstudy).\nWe need from the user study an empirical distance\nmeasure to estimate each wk:\nDstudy =/epsilon1+n/summationdisplay\nk=1(wk·Dk) (1)\nFigure 3 . Comparing distributions using the cumulative\ndensity function. The solid line is the CDF of the parent\ndistribution; the dashed line is the CDF of a much more\nrepresentative sub-sample than that of the dotted line. The\narea between each curve can be taken to be the distance\nbetween each distribution.\nfor which the error term /epsilon1can be minimised. There are\nvarious empirical approaches that can be taken to obtain\na complete pairwise distance matrix for Dstudy , but we\nchoose the method of triadic comparisons [12] because it\ninvolves discrete decisions from subjects rather than use\nof a continuous scale, which means that subjects are less\nlikely to have changed their calibrations as the experiment\nproceeds.\nIn this triadic comparison, excerpts from three pieces\nof music are played to the subject who is subsequently\nasked to judge which two are most similar. We have\nimplemented a browser-based interface for this, with a\ngraphical control designed to minimise visual biasing (see\nFigure 4). The control changes to reﬂect which piece the\nsubject is currently listening to and also to indicate how to\ngo about indicating their judgment. Subjects must listen to\nall three pieces in order; if they wish to listen again, they\nmust listen to all three again, in the same order. The in-\nterface is connected to a back-end which records the sub-\nject’s choices in a database; it also ensures that the triads\nare apportioned to subjects according to the methodologi-\ncal design principles described below.\nFor a complete block design (CBD) with ntracks, the\nnumber of different permutations is n(n−1)(n−2). Let\nus suppose that a subject can listen to an excerpt from each\nof three tracks and make a judgement about their relative\nsimilarities within 30 seconds. This equates to 120 triads\nin an hour (with no time for breaks), which is the full per-\nmutations for n= 6 tracks. For n= 8, we have 336\ntriads or 5.6 hours. Clearly as the number of tracks rises,\nthe number of triads quickly exceeds what a subject might\nreasonably be expected to tackle.\nIn order to overcome this combinatorial explosion,\nNovello et al [6] suggest the use of a balanced incomplete\nblock design (BIBD). Firstly, no combination of three\npieces is reordered and presented in more than one per-(a) (b) (c)\nFigure 4 . Main control from web interface to empirical\nstudy. (a) Before and after playback the control shows\nthree numbers with equal emphasis. (b) During playback\nthe control shows which piece is playing, e.g., piece 2\nin this example. (c) After playback the user is asked to\nchoose which two are most similar: here the mouse is hov-\nering between tracks 1 and 3 and the control has changed\nto indicate that clicking here will select 1 and 3 as being\nmost similar.\nmutation. Secondly, only a subset of such combinations\nare presented: instead of presenting each pair of tracks\n6(n−2)times (i.e., each pair alongside each other track),\neach pair is only presented λtimes. Novello et al suggest\nλ= 2 which gives 102 triads for n= 18 , as opposed to\n4896 triads for the CBD. The tracks presented alongside\neach pair are balanced as far as possible, and the presen-\ntation order is varied to compensate for presenting only a\nsingle permutation of each triad: for a full description, see\n[12].\nHowever, we believe that these steps are insufﬁcient\nfor the task at hand. Tversky [11] argues that presenta-\ntion order alters the context in which similarity is judged.\nWhen a subject is asked to judge the similarity of tracks\nA, B and C, in that order, after hearing tracks A and B he\nhas their common features in mind, and is likely to judge\nC along those same lines; whereas if he had heard B, C\nand A he would be judging A against the common fea-\ntures of B and C. Therefore it is not inconsistent for tri-\nads ABC and BCA to be given different judgments by the\nsame subject. Although Novello et al try to balance the\npresentation order as far as possible, we believe that a full\nbalancing for the incomplete design can only be achieved\nthrough knowledge of the relative perceptual salience of\nthe aspects shared by the ﬁrst two tracks in each triad,\nwhich is not known before the study (a circular depen-\ndency: Catch-22). Tversky also details other types of\nasymmetry, e.g., one piece may be considered a referent\n(such as an older or more well-known piece). Further-\nmore there is an inherent asymmetry in presenting a triad\nbecause the ﬁrst and the third tracks are necessarily sep-\narated in time by the second, whereas the other two pairs\nrun onto each other (even if the subject repeats listening\nto the triad immediately thereby running the third and ﬁrst\npieces together, there will still be a bias towards the other\npairs). Therefore we propose that balancing can only re-\nally be achieved by presenting all the permutations of each\ncombination of triad.\nSimilarly, the incomplete block design for λ < n −2\nalso leaves gaps which cast doubt on the validity of com-\nplete pairwise distances being calculated from the studydata. For example with λ= 2, if each of the two tracks\npresented alongside a given pair are similar in any aspect\nto at least one of the tracks in that pair, the pair will be\njudged completely dissimilar regardless of its actual sim-\nilarity. That is, if the pair AB is presented in the contexts\nof C and D, if AC is judged the most similar of ABC and\nBD the most similar of BCD, A and B are taken to be have\nzero similarity, whereas if they had been presented in fur-\nther contexts (E, F, etc.) they might have proved similar to\nsome extent. Conversely, if AB and CD are very dissim-\nilar, BC will receive the maximal similarity rating even\nif B and C are not particularly similar. Only a complete\nblock design provides for enough granularity to construct\na complete pairwise distance matrix.\nAs previously mentioned, for any number of pieces n\nmore than just a handful, it is unreasonable to expect a\nsingle subject to tackle the complete block design. We\ntherefore propose partitioning the block over multiple sub-\njects. This is not ideal on account of the possibility of dif-\nferent listeners favouring different aspects, but as we are\nattempting to capture average listener characteristics, the\ncompromise is justiﬁed. We should certainly be able to do\nbetter than a ﬂat equal initial weighting.\nThe partitioning must also be balanced to maximise the\nreturn of information gathering (although redundancy for\nsubject consistency testing must be re-incorporated later).\nThe most obvious partitioning involves six subjects, each\nwith a different permutation of each combination (see Fig-\nure 5). We can also partition each row further with the\nconstraints that each piece is presented the same number\nof times to each user (see Figure 6). However, we can\nimprove the balance by ensuring that there is no correla-\ntion between pieces and their positions in the triads (see\nFigures 7 and 8). We refer to this design as the Balanced\nComplete Block Partitioning (BCBP).\nOur implementation of BCBP uses best-ﬁrst search [8]\nto allocate a full combination of triads into groups as if for\na single combination, according to the second balancing\nconstraint described above (see Figure 9), then allocates\npermutations according to the third constraint.\nSplitting a complete block design for n= 18 , giving\neach subject 102 triads (51 minutes) each requires 48 sub-\njects. Note that a fully balanced BCBP partitioned over 48\nsubjects is equivalent to a BIBD with n= 18 andλ= 2\ngiven to 48 subjects if the BIBD is balanced between sub-\njects as well as within them. (Novello et al use 36 subjects\nwhich gives a maximum coverage of 0.75)\nOnly some group sizes are suitable for balancing. The\nconstraints are as follows: the combinations must be di-\nvisible into groups of equal size (Equation 2), and each\ngroup must contain an equal number of occurrences of\neach piece (Equation 3).\n/parenleftbiggn\n3/parenrightbigg\nmod g= 0 (2)\n/parenleftbign\n3/parenrightbig\ngmod n= 0 (3)ABC ABD ABE ACD ACE ADE BCD BCE BDE CDE\nACB ADB AEB ADC AEC AED BDC BEC BED CED\nBAC BAD BAE CAD CAE DAE CBD CBE DBE DCE\nBCA BDA BEA CDA CEA DEA CDB CEB DEB DEC\nCAB DAB EAB DAC EAC EAD DBC EBC EBD ECD\nCBA DBA EBA DCA ECA EDA DCB ECB EDB EDC\nFigure 5 . Full permutations for ﬁve pieces. Partitioning\nfor six subjects is straightforward: one row for each sub-\nject, ten pieces each.\nACD ABE ACE BCD BDE ABD ABC ADE BCE CDE\nADC AEB AEC BDC BED ADB ACB AED BEC CED\nCAD BAE CAE CBD DBE BAD BAC DAE CBE DCE\nCDA BEA CEA CDB DEB BDA BCA DEA CEB DEC\nDAC EAB EAC DBC EBD DAB CAB EAD EBC ECD\nDCA EBA ECA DCB EDB DBA CBA EDA ECB EDC\nFigure 6 . Full permutations for ﬁve pieces, partitioned for\ntwelve subjects, balanced so that each piece appears three\ntimes for each subject.\nABC DAE CEA BCD EDB ABD CDA EAB BCE DEC\nACB DEA CAE BDC EBD ADB CAD EBA BEC DCE\nBAC ADE ECA CBD DEB BAD DCA AEB CBE EDC\nBCA AED EAC CDB DBE BDA DAC ABE CEB ECD\nCAB EDA ACE DBC BED DAB ACD BEA EBC CDE\nCBA EAD AEC DCB BDE DBA ADC BAE ECB CED\nFigure 7 . Balanced Complete Block Partitioning (BCBP).\nFull permutations for ﬁve pieces, partitioned for twelve\nsubjects, balanced so that each piece not only appears\nthree times for each subject, but also appears once in each\nposition.\nADF EAC DCB BFE ABD FAC CEB DFE AEB DAE CBF FCD\nAFD ECA DBC BEF ADB FCA CBE DEF ABE DEA CFB FDC\nDAF AEC CDB FBE BAD AFC ECB FDE EAB ADE BCF CFD\nDFA ACE CBD FEB BDA ACF EBC FED EBA AED BFC CDF\nFAD CEA BDC EBF DAB CFA BCE EDF BAE EDA FCB DFC\nFDA CAE BCD EFB DBA CAF BEC EFD BEA EAD FBC DCF\nACD BAF DEB EFC ABC EAF BFD CDE\nADC BFA DBE ECF ACB EFA BDF CED\nCAD ABF EDB FEC BAC AEF FBD DCE\nCDA AFB EBD FCE BCA AFE FDB DEC\nDAC FBA BDE CEF CAB FEA DBF ECD\nDCA FAB BED CFE CBA FAE DFB EDC\nFigure 8 . Balanced Complete Block Partitioning (BCBP).\nFull permutations for six pieces, partitioned into thirty\ngroups, balanced so that each piece appears twice in\neach group. Note that although each piece cannot appear\nonce in each position as per Figure 7, the positioning is\nnonetheless balanced; e.g., in the bottom right hand group,\npieces D and F both appear in positions 1 and 2, pieces A\nand B in positions 2 and 3, and pieces C and E in positions\n1 and 3.1. Distribute all combinations of npieces over ggroups by adding one to\neach group in turn from an alphabetically ordered list of all combinations\nuntil the list is exhausted. This produces a partitioning closer to being\nbalanced than if the ﬁrst npieces had gone to the ﬁrst group, the second n\nto the second group, and so on. Create an initial node for a priority queue,\nwith this grouping as the node’s state and with the average variance of the\nnumber of times each piece appears in each group as its score.\n2. Remove a node from the head of the queue and perform the following on\nit:\nFor each group Gi(i= 1. . . g ):\nCount the number of each piece in the group\nFor each triad Tacontaining each piece appearing more\nthan the average number of times:\nFor each other group Gj(j= 1. . . g, i /negationslash=j):\nCount the number of each piece in the group\nFor each triad Tbcontaining each piece appearing\nfewer than the average number of times:\nCreate a copy of the node with TaandTbex-\nchanged, and add it to the priority queue with\nits score as described in the step above.\nRepeat this step until the score of the node at the head of the queue is zero\n(i.e., its grouping is fully balanced).\nFigure 9 . Algorithm for balancing a complete block par-\ntitioning, based on a priority queue [3]. This algorithm\nbalances the partitioning of the unordered combination of\nnpieces over ggroups (i.e., a single row in Figure 6). A\nsimilar algorithm (not detailed here) is employed to bal-\nance the ordering of pieces within each triad (as per Fig-\nures 7 and 8).\nA third constraint may be added to ensure perfect balanc-\ning: that for each group, each piece appears exactly the\nsame number of times in each position (Equation 4).\n3/parenleftbign\n3/parenrightbig\nn·gmod 3 = 0 (4)\nHowever, a partitioning can still be considered balanced if\neach position is occupied an equal number of times by an\nequal number of pieces (see Figure 8).\nTo test for consistency, each subject should also be\ngiven a number of triads (e.g. 5) more than once (within-\nsubject consistency) and a number of common triads (e.g.\n5) should be given to every subject (between-subjects con-\nsistency). These courses of action will violate the balanc-\ning constraints, but are important for determining the ad-\nmissibility of results. The extra triads will be injected into\neach subject’s partitions roughly equally spaced through-\nout.\nThe BCBP method is by no means limited to the study\nfor our initial weighting; it is suitable for any task to which\na BIBD might be applied, such as attempting to determine\na ground truth. We are also intending to use a triadic de-\nsign based on the same web interface for the evaluation of\nthe aspect weighting system described in Section 2.\n4 CONCLUSION\nWe have presented an examination of the methodology\nof studies in musical similarity, and outlined why incom-\nplete block designs for user studies, even if balanced, maynot capture enough information for determining a com-\nplete pairwise distance measure. We have described the\nBalanced Complete Block Partitioning which is designed\nto address these issues and described how a study based\non this design might be implemented. We have also pre-\nsented a novel system for context-based speciﬁcation of\nmusical similarity and a user-centric interface for such a\nsystem. We are currently undergoing a study to parame-\nterise this system using pieces from our database of more\nthan 14,000 MIDI-encoded pieces of western popular mu-\nsic, the results of which will be presented at ISMIR 2007\nand published in a later paper.\nACKNOWLEDGEMENTS\nThe ﬁrst author is supported by a Doctoral Bursary from\nthe Department of Computing at Goldsmiths, University\nof London. The second author is supported by EPSRC\ngrant EP/D038855/1.\n5 REFERENCES\n[1] Hamish Allan and Geraint Wiggins. Further aspects\nof similarity. In Proceedings of the 2nd Digital Mu-\nsic Research Network Summer Conference , 2006.\n[2] Jean-Julien Aucouturier. Ten Experiments on the\nModelling of Polyphonic Timbre . PhD thesis, Uni-\nversity of Paris 6, Paris, France, May 2006.\n[3] Thomas H. Cormen, Charles E. Leiserson, Ronald L.\nRivest, and Clifford Stein. Introduction to Algo-\nrithms , chapter 6.5, pages 138–142. MIT Press and\nMcGraw-Hill, 2nd edition, 2001.\n[4] Michael I. Mandel, Graham E. Poliner, and\nDaniel P.W. Ellis. Support vector machine active\nlearning for music retrieval, 2006.\n[5] Daniel M ¨ullensiefen and Klaus Frieler. Optimizing\nmeasures of melodic similarity for the exploration\nof a large folk song database. In Proceedings of the\nFifth Annual International Symposium on Music In-\nformation Retrieval: ISMIR 2004 . Universitat Pom-\npeu Fabra, 2004.\n[6] Alberto Novello, Martin F. McKinney, and Armin\nKohlrausch. Perceptual evaluation of music sim-\nilarity. In Proceedings of the Seventh Annual In-\nternational Symposium on Music Information Re-\ntrieval: ISMIR 2006 . University of Victoria, British\nColumbia Canada, 2006.\n[7] Elias Pampalk. Computational Models of Music\nSimilarity and their Application in Music Informa-\ntion Retrieval . PhD thesis, Vienna University of\nTechnology, Vienna, Austria, March 2006.\n[8] J Pearl. Heuristics: Intelligent Search Strategies for\nComputer Problem Solving . Addison-Wesley, 1984.[9] Jeremy Pickens. Harmonic Modeling for Polyphonic\nMusic Retrieval . PhD thesis, University of Mas-\nsachusetts Amherst, MA, USA, May 2004.\n[10] Godfried Toussaint. A comparison of rhythmic sim-\nilarity measures. In Proceedings of the Fifth An-\nnual International Symposium on Music Information\nRetrieval: ISMIR 2004 . Universitat Pompeu Fabra,\n2004.\n[11] Amos Tversky. Features of similarity. Psychological\nReview , 84(4):327–352, 1977.\n[12] Susan C. Weller and A. Kimball Romney. Qualita-\ntive Research Methods: Systematic Data Collection .\n10. Sage Publications, California, 1988."
    },
    {
        "title": "Virtual Communities for Creating Shared Music Channels.",
        "author": [
            "Amelie Anglade",
            "Marco Tiemann",
            "Fabio Vignoli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414710",
        "url": "https://doi.org/10.5281/zenodo.1414710",
        "ee": "https://zenodo.org/records/1414710/files/AngladeTV07.pdf",
        "abstract": "We present an approach to automatically create virtual communities of users with similar music tastes. Our goal is to create personalized music channels for these communities in a distributed way, so that they can for example be used in peer-to-peer networks. To find suitable techniques for creating these communities we analyze graphs created from real-world recommender datasets and identify specific properties of these datasets. Based on these properties we select and evaluate different graph-based community-extraction techniques. We select a technique that exploits identified properties to create clusters of music listeners. We validate the suitability of this technique using a music dataset and a large movie dataset. On a graph of 6,040 peers, the selected technique assigns at least 85% of the peers to optimal communities, and obtains a mean classification error of less than 0.05 over the remaining peers that are not assigned to the best community. 1 INTRODUCTION Music recommender systems and algorithms continue to attract scientific and commercial interest. Generally, music recommender research focuses on either service-based recommender systems, that can be used over the Internet, or on autonomous recommender systems for non networkenabled devices such as portable music players. In this paper we are concerned with providing music recommendations and personalized music radio channels directly between connected systems and devices using peer-to-peer networks. The concept of peer-to-peer radio emerged recently. Such systems are already publicly available and prove its technical feasibility (e.g. [9]). Regarding personalized recommendation over peer-to-peer networks relatively little research has been conducted to date. One such research project on distributed recommendation is 1 Now with Queen Mary University of London, Centre for Digital Music, Mile End Road, London E1 4NS, UK; M.Sc. student at Chalmers University of Technology, Department of Applied Physics, G¨oteborg, Sweden during the study. c⃝2007 Austrian Computer Society (OCG). the TRIBLER project (cf. Wang et al. [12]). TRIBLER provides users with personalized recommendations for items shared in peer-to-peer networks using a distributed version of the widely-used collaborative filtering approach (Resnick et al. provide an introduction to collaborative filtering in [11]). We describe in this paper our approach to cluster peers into groups that share similar music preferences and potentially other criteria, and to provide music recommendations and shared music radio channels to these groups. Music shared by peers in such channels can then be transmitted directly between peers within the groups or broadcasted to the groups of peers depending on the underlying infrastructure. The paper is organized as follows: In Section 2, we discuss the specific problems that need to be addressed to realize distributed clustering of peers by music preferences. In Section 3, we perform an analysis of typical datasets for recommender systems, we identify properties of those datasets which can be exploited by using graphbased clustering techniques. In Section 4, we comparatively evaluate several clustering techniques and identify the most suitable according to its performance on optimal assignment of peers to communities. We close with final remarks and pointers to future work in Section 5. 2 PROBLEM DEFINITION AND SOLUTION APPROACH The main challenge when automatically creating groups or communities of users in the desired setting is to identify clusters of similar peers in a peer-to-peer network so that every peer in the system can enjoy a selection of music that suits her music preferences. Creating communities of peers can be expressed as an optimization problem which is composed of several potentially complementary, but also potentially competitive objectives described in the following Section.",
        "zenodo_id": 1414710,
        "dblp_key": "conf/ismir/AngladeTV07",
        "keywords": [
            "music preferences",
            "peer-to-peer networks",
            "distributed recommendation",
            "collaborative filtering",
            "graph-based clustering",
            "optimization problem",
            "complementary objectives",
            "competitive objectives",
            "music recommendations",
            "shared music radio channels"
        ],
        "content": "VIRTUAL COMMUNITIES\nFOR CREATING SHARED MUSIC CHANNELS\nAm´elie Anglade1, Marco Tiemann, Fabio Vignoli\nPhilips Research Europe\nHigh Tech Campus 34\n5656 AE Eindhoven, The Netherlands\namelie.anglade@elec.qmul.ac.uk, fmarco.tiemann, fabio.vignoli g@philips.com\nABSTRACT\nWe present an approach to automatically create virtual\ncommunities of users with similar music tastes. Our goal\nis to create personalized music channels for these com-\nmunities in a distributed way, so that they can for example\nbe used in peer-to-peer networks. To ﬁnd suitable tech-\nniques for creating these communities we analyze graphs\ncreated from real-world recommender datasets and iden-\ntify speciﬁc properties of these datasets. Based on these\nproperties we select and evaluate different graph-based\ncommunity-extraction techniques. We select a technique\nthat exploits identiﬁed properties to create clusters of mu-\nsic listeners. We validate the suitability of this technique\nusing a music dataset and a large movie dataset. On a\ngraph of 6,040 peers, the selected technique assigns at\nleast 85% of the peers to optimal communities, and ob-\ntains a mean classiﬁcation error of less than 0.05 over the\nremaining peers that are not assigned to the best commu-\nnity.\n1 INTRODUCTION\nMusic recommender systems and algorithms continue to\nattract scientiﬁc and commercial interest. Generally, mu-\nsic recommender research focuses on either service-based\nrecommender systems, that can be used over the Internet,\nor on autonomous recommender systems for non network-\nenabled devices such as portable music players. In this pa-\nper we are concerned with providing music recommenda-\ntions and personalized music radio channels directly be-\ntween connected systems and devices using peer-to-peer\nnetworks. The concept of peer-to-peer radio emerged re-\ncently. Such systems are already publicly available and\nprove its technical feasibility (e.g. [9]). Regarding person-\nalized recommendation over peer-to-peer networks rela-\ntively little research has been conducted to date. One\nsuch research project on distributed recommendation is\n1Now with Queen Mary University of London, Centre for Digital\nMusic, Mile End Road, London E1 4NS, UK; M.Sc. student at Chalmers\nUniversity of Technology, Department of Applied Physics, G ¨oteborg,\nSweden during the study.\nc\r2007 Austrian Computer Society (OCG).the TRIBLER project (cf. Wang et al. [12]). TRI-\nBLER provides users with personalized recommendations\nfor items shared in peer-to-peer networks using a dis-\ntributed version of the widely-used collaborative ﬁltering\napproach (Resnick et al. provide an introduction to col-\nlaborative ﬁltering in [11]). We describe in this paper our\napproach to cluster peers into groups that share similar\nmusic preferences and potentially other criteria, and to\nprovide music recommendations and shared music radio\nchannels to these groups. Music shared by peers in such\nchannels can then be transmitted directly between peers\nwithin the groups or broadcasted to the groups of peers\ndepending on the underlying infrastructure.\nThe paper is organized as follows: In Section 2, we dis-\ncuss the speciﬁc problems that need to be addressed to\nrealize distributed clustering of peers by music prefer-\nences. In Section 3, we perform an analysis of typical\ndatasets for recommender systems, we identify properties\nof those datasets which can be exploited by using graph-\nbased clustering techniques. In Section 4, we compara-\ntively evaluate several clustering techniques and identify\nthe most suitable according to its performance on optimal\nassignment of peers to communities. We close with ﬁnal\nremarks and pointers to future work in Section 5.\n2 PROBLEM DEFINITION AND SOLUTION\nAPPROACH\nThe main challenge when automatically creating groups\nor communities of users in the desired setting is to iden-\ntify clusters of similar peers in a peer-to-peer network so\nthat every peer in the system can enjoy a selection of mu-\nsic that suits her music preferences. Creating communi-\nties of peers can be expressed as an optimization problem\nwhich is composed of several potentially complementary,\nbut also potentially competitive objectives described in the\nfollowing Section.\n2.1 A dynamical, distributed optimization problem\n\u000fEach peer must be satisﬁed with the music selection\nproposed to her . Recommended or played music\nmust match the peer’s personal taste.\n\u000fWithin each community a good average satisfaction\nmust be obtained . Optimally, each peer should bemore satisﬁed with the recommendations provided\nby her community than she would be in any other\ncommunity.\n\u000fAdding a new peer to a community must not deteri-\norate the average satisfaction . When adding a new\npeer to a community, the previously stated objective\nmust still be fulﬁlled both for her and for all mem-\nbers of the community.\n\u000fExtreme community sizes must be avoided . Both\ncreating very large communities and very small\ncommunities must be avoided. Assigning most or\nall peers to a single large community can result\nin an unsatisfactory listening experience; fragment-\ning peers into many very small communities limits\nthe exposure to additional music not included in a\npeer’s music collection.\nMoreover, various dynamical aspects have to be taken into\naccount for resolving the optimization problem:\n\u000fPeer-to-peer systems are intrinsically dynamic .\nPeers may enter or leave the system at any given\ntime. This may require the system to dynamically\nadapt the community structure.\n\u000fMusic preferences evolve over time . A peer may\ntemporarily or permanently change her music pref-\nerences. Reﬂecting such changes in a timely man-\nner is important in order to satisfy listeners.\nFinally, given the distributed nature of peer-to-peer sys-\ntems, all computations for creating and maintaining com-\nmunities must be performed by the peers themselves. We\nprovide an in-depth discussion of solutions for this aspect\nelsewhere [2].\n2.2 Solution approach\nWe adopt a graph-theoretic approach to solve the given\nproblems. We start by constructing a graph of all peers\nand then apply a clustering technique on that graph in or-\nder to form communities of peers. For constructing this\ngraph, we link each peer to her Nmost similar peers in\nterms of music preferences. Each peer is characterized\nby a vector of preference-song pairs, where the preference\nvalues can be numerical (e.g. on a scale from 1 to 5) or\nbinary values (like or dislike). In our case, we assume that\nthese preferences reﬂect explicit ratings provided by each\npeer.\nWe use the Pearson correlation coefﬁcient to deﬁne the\nsimilarity measure sbetween two peers aandb. This is a\nsimilarity measure that is frequently used in collaborative\nﬁltering recommender systems (e.g. Resnick et al. [11]):\ns(a;b) =P\nj(va;j\u0000\u0016va) (vb;j\u0000\u0016vb)qP\nj(va;j\u0000\u0016va)2P\nj(vb;j\u0000\u0016vb)2(1)\nwherejis the index of the song for songs present in both\npeer’s preference vectors, va;iis the preference value bypeeraon songi,vacorresponds to the mean preference\nvalue for peer aover her complete song preference vector\nPa:\n\u0016va=1\njPajX\ni2Pava;i (2)\nUsing this similarity measure we use a three step approach\nto ﬁnd a suitable graph-based solution to automatically\ncreate communities of peers:\n\u000fWe create and analyze graphs of peers using real-\nworld reference datasets and identify relevant com-\nmon properties of such datasets.\n\u000fWe comparatively evaluate several candidate clus-\ntering techniques. We then select the most appropri-\nate technique for extracting communities of peers,\nbased on our objectives formulated in Section 2.\n\u000fWe validate the performance of the selected tech-\nnique experimentally.\nWe present selected results of our work in the following\nsections. A detailed presentation of the work carried out\ncan be found in [2].\n3 GRAPH OF PEERS\n3.1 Datasets\nTwo datasets from the music and movie domain collected\nunder real-world conditions have been analyzed. The\ndatasets vary in the number of peers and in the amount\nof preference data available for peers:\n1. The EasyAccess (EA) dataset gathered within\nPhilips Research contains 74,631 ratings provided\nby 462 users over 5,234 songs from 234 artists. It\ndoes not contain any content metadata.\n2. The MovieLens (ML) dataset [8] contains 1,000,029\nratings over 3,593 movies provided by 6,040 users.\nThe MovieLens dataset is widely used as a ref-\nerence dataset for evaluating recommender algo-\nrithms.\nAdditional evaluation results for a medium-sized dataset\nare available in [2].\nBoth datasets contain regular ratings on a ﬁve point nu-\nmerical scale from one to ﬁve. We report evaluation re-\nsults both for these regular ratings and for binary ratings.\nBinary ratings are constructed as follows: values strictly\nlarger than 3 are considered as positive ratings, others are\nconsidered as negative ratings. This results in an approxi-\nmately equal amount of positive and negative ratings with-\nout having to discard rating data.\nThe parameter Nfor the number of most similar peers\nused is a variable in the analysis in order to study its inﬂu-\nence on the topology of the resulting graphs. In all evalua-\ntions we report results for values of N= 5, 10, 20, 30, and\n46. Larger values of Nare impractical for distributed so-\nlutions: it would be prohibitively time-consuming to com-\npute the exact Nmost similar peers for each user for largevalues ofNover a peer-to-peer system (especially for\nlarge datasets as the time needed to compute these sim-\nilar peers increases with the size of the dataset and with\nN).\n3.2 Graph properties\nPeer-to-peer systems can easily contain thousands or mil-\nlions of peers. The recently established scientiﬁc ﬁeld of\ncomplex network theory is concerned with studying and\nsimulating complex relations and dynamics in large real-\nworld networks [14]. Complex network theory is based on\ngraph-theoretic foundations and is speciﬁcally concerned\nwith real-world phenomena that exhibit network proper-\nties [1]. Thus insights and solutions from this ﬁeld may\nprove helpful for addressing the problems we are inter-\nested in.\nA graph can be constructed as a directed graph , in\nwhich all edges between nodes are directed, or as an undi-\nrected graph , in which edges are not directed. A compo-\nnent is a maximal sub-graph of nodes for which a path\nexists between every pair of nodes. A connected graph is\na graph that has one unique component: a path exists for\nevery pair of nodes of this graph. A disconnected graph\nconsists of several separated components. The distance\ndijbetween two nodes iandjis the length (i.e. the num-\nber of edges) of the shortest path between the two nodes.\nThe distance between nodes in two components is inﬁnite.\nTheaverage distance lin a graph is the mean distance be-\ntween node pairs in the graph. The neighborhood of a\nnodeiare the nodes immediately connected to it and its\ndegreekiis the number of edges connected to it. In a di-\nrected graph the indegree andoutdegree are the number of\nincoming and outgoing edges of a node.\n3.3 Graph components and average distance\nThe “most similar peers”-relation used to construct graphs\nof peers is not a symmetric relation. It is most suitably\nrepresented as a directed graph of peers. However, di-\nrected graphs created from the evaluation datasets are dis-\nconnected graphs for all experimental conditions. The\nresulting graphs consist of one giant component and nu-\nmerous small components of size at most of the order of\nln(n), wherenis the number of users (cf. Table 1). This\nbehavior is commonly found in real-world networks and\nhas been studied in [5].\nGiven the objectives deﬁned in Section 2, a large fraction\nof the small components found must be considered as too\nsmall to constitute communities by themselves. Therefore\nwe use undirected graphs by discarding all information\nabout directedness of edges. This transformation is com-\nmonly used in complex networks analysis. It can be ap-\nplied when it is reasonable to assume that connections ex-\npressed by edges can be considered to be symmetric, as it\nis the case for user similarities using item ratings. The re-\nsulting undirected graphs are fully connected for all exper-\nimental conditions. The average distance between nodes\nin the resulting undirected graph is no longer inﬁnite but\nin the range between 2 and 3 (cf. Table 1).schkilrl CrC\nEAN=5 4 9.5 2.73 2.64 0.02 0.10\nEAN=10 2 17.8 2.13 2.36 0.04 0.10\nEAN=20 1 31.9 1.77 2.10 0.07 0.10\nEAN=46 1 62.7 1.48 1.87 0.14 0.17\nMLN=5 3 10.0 3.79 0.0017 0.12\nMLN=10 3 19.9 2.91 0.0033 0.10\nMLN=20 5 39.4 2.37 0.0065 0.10\nMLN=46 3 89.1 1.94 0.015 0.10\nTable 1 . Size of the second biggest component scin the\ndirected graph . Average degreehki, average distance l\nand clustering coefﬁcient Cof the undirected graphs of\npeers for binary ratings and several values of number of\npeersN. The average distance is missing for the ML data\nset due to computing complexity. lrandCrare the av-\nerage distance and the clustering coefﬁcient of a random\ngraph of the same size and same average degree.\n3.4 Clustering coefﬁcient\nTheclustering coefﬁcient Ciof a nodeimeasures the ten-\ndency of the neighbors of a node ito be also neighbors of\neach other. Watts and Strogatz [13] deﬁne it as the num-\nber of linksEibetween the neighbors of a node divided by\nthe total number of links that can possibly exist between\nthose neighborski(ki\u00001)\n2. The clustering coefﬁcient Cof\na graph is the average of the clustering coefﬁcients of all\nits nodes. A clique is a set of nodes in a graph in which\nevery node is the neighbor of every other node; as a con-\nsequence, the clustering coefﬁcient of a clique is 1.\nThe clustering coefﬁcient measures whether groups (or\ncliques) of nodes tend to form in a graph. This is a po-\ntentially interesting property given the objectives in Sec-\ntion 2. Frequent occurrences of cliques may be exploited\nfor creating communities of peers. For the evaluation\ndatasets, clustering coefﬁcients are indeed consistently\nlarger, and often much larger, than clustering coefﬁcients\nof a random graph with the same number of nodes and\naverage degree (cf. Table 1). This observation is inde-\npendent of the size of the dataset analyzed. Peers are\nhighly clustered irrespectively of the size of the network.\nHowever, undirected graphs of most similar peers also ex-\nhibit small average distances. This combination of prop-\nerties indicates that the analyzed graphs have properties\nof small-world networks. Small-world networks are fre-\nquently encountered when analyzing real-world phenom-\nena [13]. They tend to form a single very compact cluster\nwith very small average distances. Here this cluster tends\nto be very compact since the connection density (linked\ntoN) is high. This makes it difﬁcult to apply traditional\nclustering techniques successfully.\n3.5 Degree distribution\nSince the small average distance within the graphs of\nusers makes it difﬁcult to apply clustering techniques ef-\nfectively, we study the degree distribution of the created\ngraphs to ﬁnd other potentially useful properties. The10110210310410−410−310−210−1100\nkP(k)\n  Figure 1 . Degree distribution of the MovieLens graphs\nforregular ratings andN= 10 . On a log-log scale it\ncan be approximated by a straight line (power law distri-\nbution) or a broken line of negative slope.\nundirected graphs consistently display the same charac-\nteristics across all simulation conditions, as indicated in\nFigure 1. While the degree distribution of the graphs is\nnot a power-law distribution, it is similar to one (as shown\nin Figure 1), and indeed the graphs exhibit properties sim-\nilar to those of scale-free networks.\nA scale-free network is a graph following a power-law\ndistribution deﬁned by P(k)\u0018k\u0000\r[3]. One property of\nsuch networks that can be useful for us is that connections\nbetween nodes in scale-free networks are not as evenly\ndistributed as they would be in random graphs. Instead,\nscale-free networks contain some nodes that are highly\nconnected to many other nodes, termed “hubs”, and many\nnodes with very small degrees. The graphs obtained from\nthe EA and ML datasets exhibit exactly such properties.\nThis presence of hubs can be exploited for cluster analy-\nsis using speciﬁc techniques as discussed in Section 4.\n3.6 Robustness to node removal\nAnother property of scale-free networks is their resilience :\nScale-free networks are robust to random node removal,\nas long as the removed random node is not a hub node.\nHub node removal however can have a large impact on\na scale-free network, potentially splitting the network into\nseveral components. As already mentioned in Section 3.3,\nit would be difﬁcult to create and maintain communities in\ngraphs with several components.\nTo analyze the robustness to node removal, nodes are\nrandomly removed from the graphs for different values of\nN(see Figure 2). For neighborhood sizes of 10 or larger,\nthe network proves to be highly robust to random node re-\nmoval: even when 50% of the nodes are removed from the\nnetwork simultaneously, graphs still consist of one unique\ncomponent. Smaller neighborhood sizes lead to a higher\nsensitivity to node removal. Interestingly, the graphs also\nprove to be relatively robust to hub removal. This shows\nthat while the graphs do contain hub nodes, they only par-\ntially exhibit properties of scale-free networks.\n204060801000102030\npercent. of removed nodesavg. number of comp.\n204060801000102030405060708090\npercentage of removed nodessize of the largest comp.N=5\nN=10\nN=20\nN=30\nN=46Figure 2 . Robustness of the EasyAccess graph to node\nremoval. Average number of components and percentage\nof nodes of the initial graph in the largest component (i.e.\npercentage of nodes of the initial graph in the largest com-\nponent) over 50 trials as a function of the percentage of\nrandomly selected removed nodes.\n4 A CLUSTERING TECHNIQUE FOR\nEXTRACTING COMMUNITIES\nGiven the highly compact nature of the analyzed graphs,\nselecting a suitable clustering technique to extract com-\nmunities of peers is not a straightforward task. Well-\nstudied hierarchical clustering techniques, for example,\ncan not be readily applied given this property, because the\ngraphs are too compact to be divided.\n4.1 Considered clustering techniques\nTwo well-known graph-partitioning clustering algorithms\nwere evaluated for the task of clustering peers: the Max-\nFlow technique introduced by Flake et al. [6], and\nthe Edge-Betweenness clustering algorithm introduced by\nGirvan and Newman [7].\nThe Max-Flow algorithm is based on a source-sink pro-\ncedure that starts from a representative set of seed nodes\n(source), and extracts a cluster by cutting the minimum\nnumber of edges linking it to the rest of its graph (sink).\nThe Edge-Betweenness technique is a hierarchical di-\nvisive clustering technique. Edges are progressively re-\nmoved from a graph to reveal clusters. Assuming that the\ndensity of edges is higher within a cluster than between\nclusters, Girvan and Newman show that all shortest paths\nbetween communities go along inter-cluster edges. Based\nhereon they deﬁne edge-betweenness as “the number of\nshortest paths between a pair of vertices that run along it”\n[7], and we progressively remove the edges with the high-\nest edge-betweenness to reveal clusters.\nBesides these graph-divisive algorithms we can try to\nexploit the presence of hubs in the evaluated graphs to\ncluster them. They exhibit several interesting properties.\nSince the outdegree of a node is ﬁxed to N, a hub is a node\nwith a high indegree, so a user similar to a high number of\nother ones. Moreover, the average number of song prefer-\nences per hub is always considerably smaller than the av-\nerage number of songs per user over the whole network,10010210410−210−1100cumulative probability\n10010210410−210−1100\nsize of the communityFigure 3 . Cumulative distribution of the sizes of the com-\nmunities for binary ratings, N= 20 , 150 starting hubs\nML data-set (left) and for regular ratings, N= 46 , 46\nstarting hubs EA data-set (right) using Hub-Based clus-\ntering.\nwith hub users having around a third of this value. Hubs\ncan be seen as users with songs commonly loved by their\nneighbors, and so can federate around them communities\nof users with common tastes. Thus a clustering technique\nthat exploits this idea is also evaluated: the Hub-Based\nclustering technique introduced by Da Fontoura Costa [4].\nIn Hub-Based clustering each hub in a large network\nis considered as the centroid of a cluster. Starting from\nthese centroids, clusters are obtained by propagating la-\nbels from the hubs to other nodes through shortest paths.\nThis procedure is performed as simultaneous wave fronts\nand repeated until every node is labeled. This technique\ncan easily be applied in distributed environments, since\nthe labels are propagated through purely local interactions\nbetween users and their direct neighbors. Experiments of\nthis last technique with edges that were assigned with cor-\nrelation values as weights were also performed, but had\nundesirable effects on the sizes of the created communi-\nties.\n4.2 Evaluation of clustering techniques\nThe three clustering techniques are applied on the de-\nscribed datasets, again evaluating all experimental condi-\ntions described in Section 3.1.\nEvaluation results of both the Max-Flow and the Edge-\nBetweenness technique are similar across all experimen-\ntal conditions: some individual nodes and few very small\nclusters (of 2-3 nodes) are isolated from the rest of the\ngraph, which remains as a single giant component. This is\nnot sufﬁcient given the objectives deﬁned in Section 2.\nThe evaluation of Hub-Based clustering shows that this\ntechnique does not generally lead to such extreme cluster\nsizes. Figure 3 shows cluster sizes created using Hub-\nBased clustering for the EasyAccess and MovieLens data-\nsets. Hub-Based clustering results in a larger range of\ndifferent cluster sizes for all experimental conditions. It\nsatisﬁes the requirement to avoid only creating extreme\ncluster sizes (see Section 2). Moreover, the cumulative\ndistributions of the cluster sizes are very similar to the de-gree distributions of the networks of most similar peers.\nOn a log-log scale they can be approximated by a straight\nline (power law distribution) or a broken line of negative\nslope. Here the created cluster structures from the Hub-\nBased clustering technique seem to closely reﬂect the in-\ntrinsic topology of the network of peers.\nIn conclusion, both the Max-Flow and the Edge-\nBetweenness technique only create clusters of extreme\nsizes. This is likely caused by the speciﬁc properties\nfound in the evaluation datasets. The compactness of\nthe graphs can not be divided into clusters sufﬁciently\nby these two techniques. Hub-Based clustering creates a\nrange of different cluster sizes, which makes it a suitable\ncandidate for further validation.\n4.3 Validation measure\nValidating the suitability of the Hub-Based clustering\ntechnique to create clusters or communities of peers ac-\ncording to our objectives requires us to assess the quality\nof the clustering achieved.\nA frequently used quality measure for clustering tech-\nniques is the modularityQintroduced by Girvan and\nNewman [10]. The modularity determines whether a clus-\nter structure tends to have dense connections within clus-\nters and sparser connection between them. For the highly\ncompact graphs of the evaluation datasets (see Section\n3.4) usingQto evaluate community formation is not feasi-\nble. The graphs of the evaluation datasets are too compact\nfor any clustering technique to achieve expected values for\ngood clustering results using Q. The modularityQis not\nwell-suited for this validation, at least when considering\nstandard interpretations of modularity values.\nInstead, we evaluate the quality of the communities of\npeers we obtain by measuring the peers’ satisfaction with\ntheir assignment to a cluster. To evaluate the satisfaction\nusing the evaluation datasets, we proceed as follows.\nFor a community we deﬁne the preference list of songs\nto be a set containing all songs of all members of the clus-\nter. The average rating for a song is used as the community\nscore of a song. For each user ua truncated preference list\nof her community Cuis computed. This truncated pref-\nerence list is composed of the ratings of all users of the\ncommunity except for those of user u. To measure how\nsatisﬁed a user is by her community, we evaluate whether\nthe song selection of the community she was assigned to\nwithout her being part of it suits her better than the music\nof other communities. A user uis satisﬁed by her com-\nmunity if:\ns(u;Cu)\u00151\njCj\u00001X\nc2C\u0000fCugs(u;c) (3)\nwhereCis the ensemble of all communities and sis the\nPearson correlation coefﬁcient computed on the prefer-\nence lists. The classiﬁcation error e, made when a user\nis not satisﬁed by her community (misclassiﬁed), is:\ne=\u0016\u0000s(u;Cu\u0000fug)\n2(4)where\n\u0016=1\njCj\u00001X\nc2C\u0000fCugs(u;c) (5)\n4.4 Validation results\nOur validation of the clusters found using the Hub-Based\nclustering technique results in several noteworthy ﬁnd-\nings. Firstly, the percentage of well-classiﬁed users varies\nbetween 48% and92%. It increases with the size of the\nnetwork. Using regular ratings leads to better classiﬁca-\ntion results than using binary ratings. The neighborhood\nsize chosen also inﬂuences the classiﬁcation accuracy: the\npercentage of satisﬁed users reaches its maximum values\nfor numbers of most similar peers N= 20 , andN= 30 .\nIt is consistently above 75% for all datasets used for vali-\ndation for those values of Nwhen they are combined with\nregular ratings and an optimal number of starting hubs\nchosen so that the average number of users per community\nis between 20 and 30 (cf. [2]). For the largest dataset used\nthis percentage is consistently above 85%. This indicates\nthat the percentage of correctly assigned peers increases\nwith the overall community size.\nThe classiﬁcation error for the remaining users varies\nbetween 0.03 and 0.16. Again, this error decreases as the\nsize of the dataset increases and is consistently smaller\nthan 0.05 for the best parameters described above. This\nmeans that the music of their community is almost as good\nfor them as the music from the other communities.\nSummarizing our evaluation, we ﬁnd that the evalu-\nated Hub-Based clustering technique assigns a large ma-\njority of users to the clusters optimal to them and assigns\nthe non-optimally assigned users to clusters that are very\nclose to optimal clusters for neighborhood sizes of 20 and\n30 most similar peers. The results of the validation indi-\ncate that the technique performs with increasing quality\nthe larger the dataset it is applied on is.\n5 CONCLUSION\nIn this paper we presented a technique to cluster users ac-\ncording to their explicitly or implicitly stated music pref-\nerences. In addition to dealing with the highly compact\nnature of the graphs of similar peers it also respects and\nuses the intrinsic topology of those graphs to build com-\nmunities. It did so with convincing results, consistently\nassigning at least 85% of all peers to optimal communi-\nties, and assigning remaining users to communities that\nare almost as suitable as their optimal community. The\ngraph of peers created using the Pearson correlation mea-\nsure showed to be highly robust to random node removal.\nGraphs with such properties are well suited to be used in\npeer-to-peer networks.\nFuture work we are planning includes investigating\nhow the proposed solution performs in dynamic envi-\nronments with many rapidly joining and parting network\nmembers, and in which user preferences change over time.\nWe also intend to examine the inﬂuence of different simi-\nlarity measures on the graph structure, and to evaluate theperformance and applicability of different similarity mea-\nsures in peer-to-peer networks.\n6 ACKNOWLEDGMENTS\nThe authors acknowledge the support of the UK Engineer-\ning and Physical Research Council (EPSRC) for support\nof the OMRAS2 project (EP/E017614/1) in which Am ´elie\nAnglade is involved. The authors would like to thank the\nGroupLens Reseach Group for providing the MovieLens\nratings dataset.\n7 REFERENCES\n[1] R. Albert and A.-L. Barab ´asi. Statistical mechanics\nof complex networks. Reviews of Modern Physics ,\n74(1):47–97, 2002.\n[2] A. Anglade. Virtual communities for creating shared\nmusic channels. Master’s thesis, Chalmers University\nof Technology, G ¨oteborg, Sweden, 2007.\n[3] A.-L. Barab ´asi and R. Albert. Emergence of scaling in\nrandom networks. Science , 286:509–512, 1999.\n[4] L. da F. Costa. Hub-based community ﬁnding.\narXiv:cond-mat/0405022 , 2004.\n[5] P. Erd ˝os and A. R ´enyi. On the evolution of random\ngraphs. Publ. Math. Inst. Hung. Acad. Sci. , 5:17–61,\n1960.\n[6] G. W. Flake, S. Lawrence, C. L. Giles, and F. M. Coet-\nzee. Self-organization and identiﬁcation of web com-\nmunities. IEEE Computer , 35(3):66–71, 2002.\n[7] M. Girvan and M. E. J. Newman. Community struc-\nture in social and biological networks. In Proc. Natl.\nAcad. Sci. USA , volume 99, pages 7821–7826, 2002.\n[8] http://www.grouplens.org/.\n[9] http://www.mercora.com/.\n[10] M. E. J. Newman and M. Girvan. Finding and evaluat-\ning community structure in networks. Physical Review\nE, 69, 2004.\n[11] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and\nJ. Riedl. Grouplens: An open architecture for col-\nlaborative ﬁltering of netnews. In Proc. of the Con-\nference on Computer Supported Cooperative Work ,\npages 175–186, New York, 1994. ACM.\n[12] J. Wang, J. A. Pouwelse, J. Fokker, and M.J.T. Rein-\nders. Personalization of a peer-to-peer television sys-\ntem. In Proc. of EuroITV 2006 , Athens, 2006.\n[13] D. J. Watts and S. H. Strogatz. Collective dynamics of\n‘small-world’ networks. Nature , 393:440–442, 1998.\n[14] D.J. Watts. Six Degrees: The Science of a Connected\nAge. Norton, 2003."
    },
    {
        "title": "Music Retrieval by Rhythmic Similarity Applied on Greek and African Traditional Music.",
        "author": [
            "Iasonas Antonopoulos",
            "Aggelos Pikrakis",
            "Sergios Theodoridis",
            "Olmo Cornelis",
            "Dirk Moelants",
            "Marc Leman"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417503",
        "url": "https://doi.org/10.5281/zenodo.1417503",
        "ee": "https://zenodo.org/records/1417503/files/AntonopoulosPTCML07.pdf",
        "abstract": "This paper presents a method for retrieving music recordings by means of rhythmic similarity in the context of traditional Greek and African music. To this end, Self Similarity Analysis is applied either on the whole recording or on instances of a music thumbnail that can be extracted from the recording with an optional thumbnailing scheme. This type of analysis permits the extraction of a rhythmic signature per music recording. Similarity between signatures is measured with a standard Dynamic Time Warping technique. The proposed method was evaluated on corpora of Greek and African traditional music where human improvisation plays a key role and music recordings exhibit a variety of music meters, tempi and instrumentation. 1 INTRODUCTION In the context of Music Information Retrieval (MIR), finding music recordings with similar rhythmic characteristics is a highly desired task both for the untrained listener and the musicologist. Over the years, several methods have been proposed in the context of Western music for retrieving music with similar rhythmic characteristics, e.g. tempo, meter and rhythmic patterns. Here a short overview of relevant papers is given: The work in [1] measures the similarity between rhythmic patterns extracted from music recordings and artificially generated percussive sounds. The approach in [2] extracts temporal patterns from the energy envelop of the signal in an attempt to classify music recordings to predefined classes. In [3] a set of classification schemes are proposed that are based on extracting rhythmic patterns from the signal’s spectrum. The method proposed in [4] focuses on ballroom dances and is based on features stemming from the histogram of Inter-Onset Intervals. Finally, the work in [5] evolves around self similarity analysis of the music recording. In some of the above methods, the term “rhythmic signature” is used to as a means to encode fundamental rhythmic characteristics of the music recordings. This paper focuses on rhythmic similarity in non Western music, i.e., Traditional Greek and African music, whic⃝2007 Austrian Computer Society (OCG). ch have so far received little attention in the field of MIR. Such traditions impose a number of research challenges, mainly due to the complexity of the music meters, the system of music intervals and the highly improvisational attitude of the music performers. The latter gives an additional research challenge as serves the preservation of cultural heritage and also highlights the importance of MIR systems to apply to corpora that fall outside the traditional Western schemes. In an attempt to measure rhythmic similarity in such music corpora, this paper exploits the repetitive nature of the music recordings by means of Self Similarity Analysis. This type of analysis reveals periodicities that are inherent in the music signal. Such periodicities are expressed as a sequence of values to which we also refer by the term rhythmic signature. To this end, we investigate the possibility of applying an optional thumbnailing scheme as a preprocessing step to extracting rhythmic signatures. Similarity measurement between signatures is performed by means of a standard Dynamic Time Warping technique. Section 2 presents the proposed audio thumbnailing scheme and Section 3 describes how rhythmic signatures are extracted from the music signal. The proposed similarity measure is presented in Section 4. Results and implementation details are given in Section 5 and conclusions are drawn in Section 6. 2 THUMBNAILING SCHEME The proposed audio thumbnailing scheme is optional and is considered to be a variation of the method proposed in [6], in the sense that a different feature extraction scheme is used in this paper.",
        "zenodo_id": 1417503,
        "dblp_key": "conf/ismir/AntonopoulosPTCML07",
        "keywords": [
            "Self Similarity Analysis",
            "Dynamic Time Warping",
            "thumbnailing scheme",
            "rhythmic similarity",
            "Traditional Greek and African music",
            "music meters",
            "tempi",
            "instrumentation",
            "human improvisation",
            "corpora of Greek and African traditional music"
        ],
        "content": "MUSIC RETRIEVALBY RHYTHMICSIMILARITY APPLIED ON\nGREEK ANDAFRICANTRADITIONAL MUSIC\nIasonas Antonopoulos,Aggelos Pikrakis\nandSergios Theodoridis\nDept. of Informatics &Telecommunications\nUniversity OfAthens, GreeceOlmo Cornelis,DirkMoelants\nand Marc Leman\nIPEM- Dept. ofMusicology\nGhentUniversity, Belgium\nABSTRACT\nThis paper presents a method for retrieving music record-\ningsbymeansofrhythmicsimilarityinthecontextoftra-\nditional Greek and African music. To this end, Self Sim-\nilarity Analysis is applied either on the whole recording\noroninstancesofamusicthumbnailthatcanbeextracted\nfromtherecordingwithanoptionalthumbnailingscheme.\nThis type of analysis permits the extraction of a rhythmic\nsignature per music recording. Similarity between signa-\nturesismeasuredwithastandardDynamicTimeWarping\ntechnique. The proposed method was evaluated on cor-\nporaofGreekandAfricantraditionalmusicwherehuman\nimprovisation plays a key role and music recordings ex-\nhibitavarietyofmusicmeters,tempiandinstrumentation.\n1 INTRODUCTION\nInthecontextofMusicInformationRetrieval( MIR),ﬁnd-\ningmusicrecordingswithsimilarrhythmiccharacteristic s\nis a highly desired task both for the untrained listener and\nthe musicologist.\nOver the years, several methods have been proposed\nin the context of Western music for retrieving music with\nsimilar rhythmic characteristics, e.g. tempo, meter and\nrhythmic patterns. Here a short overview of relevant pa-\npers is given: The work in [1] measures the similarity\nbetween rhythmic patterns extracted from music record-\nings and artiﬁcially generated percussive sounds. The ap-\nproach in [2] extracts temporal patterns from the energy\nenvelop of the signal in an attempt to classify music re-\ncordings to predeﬁned classes. In [3] a set of classiﬁca-\ntion schemes are proposed that are based on extracting\nrhythmicpatternsfromthesignal’sspectrum. Themethod\nproposed in [4] focuses on ballroom dances and is based\non features stemming from the histogram of Inter-Onset\nIntervals. Finally,theworkin[5]evolvesaroundselfsim-\nilarity analysis of the music recording. In some of the\nabove methods, the term “rhythmic signature” is used to\nas a means to encode fundamental rhythmic characteris-\nticsof the musicrecordings.\nThispaperfocusesonrhythmicsimilarityinnonWest-\nernmusic,i.e.,TraditionalGreekandAfricanmusic,whi-\nc/circlecopyrt2007 AustrianComputer Society (OCG).ch have so far received little attention in the ﬁeld of MIR.\nSuch traditions impose a number of research challenges,\nmainlyduetothecomplexityofthemusicmeters,thesys-\ntem of music intervals and the highly improvisational at-\ntitude of the music performers. The latter gives an addi-\ntionalresearchchallengeasservesthepreservationofcul -\ntural heritage and also highlights the importance of MIR\nsystemstoapplytocorporathatfalloutsidethetraditiona l\nWesternschemes. Inanattempttomeasurerhythmicsim-\nilarityinsuchmusiccorpora,thispaperexploitstherepet -\nitivenatureofthemusicrecordingsbymeansofSelfSim-\nilarityAnalysis. Thistypeofanalysisrevealsperiodicit ies\nthatareinherentinthemusicsignal. Suchperiodicitiesar e\nexpressed as a sequence of values to which we also refer\nby the term rhythmic signature . To this end, we inves-\ntigate the possibility of applying an optional thumbnail-\ningschemeasapreprocessingsteptoextractingrhythmic\nsignatures. Similarity measurement between signatures is\nperformedbymeansofastandard DynamicTimeWarping\ntechnique.\nSection 2 presents the proposed audio thumbnailing\nscheme and Section 3 describes how rhythmic signatures\nareextractedfromthemusicsignal. Theproposedsimilar-\nity measure is presented in Section 4. Results and imple-\nmentation details are given in Section 5 and conclusions\naredrawn inSection 6.\n2 THUMBNAILING SCHEME\nThe proposed audio thumbnailing scheme is optional and\nis considered to be a variation of the method proposed in\n[6], in the sense that a different feature extraction scheme\nisused inthis paper.\n2.1 Feature extraction\nAtaﬁrststep,themusicrecordingisshort-termprocessed\nby means of a moving window technique. The short-\nterm frames are chosen to be ≃186msecslong, non-\noverlapping and are multiplied by a Hamming window.\nEach frame is given as input to a mel-scale ﬁlter bank [8]\nthat consists of overlapping triangular ﬁlters. The center\nfrequencies of the ﬁlters coincide with the frequencies of\nwholetonesonachromaticscale,startingfrom F0= 110\nHz and moving up to ≃6.3KHz, resulting into 36ﬁlterswhich cover approximately six octaves. In the sequel we\nwillrefertothistypeofMFCCsas chroma-basedMFCCs\ndue to the similarities it bears with the “chroma vector”\n[6]. Further details are given in[7].\nTo proceed, let c(n)be the 36×1vector of chroma-\nbasedMFCCsfromthe n-thframe. Thesequence ofvec-\ntorscan bewritteninmatrixnotation as\nC36×N= [c(1)c(2)... c(N)],\nwhere Nisthe number of short-termframes.\nAt a next step, Singular Value Decomposition (SVD)\nis applied on the transpose, CT, ofC, i.e.,CT=UΣV,\nwhereUN×36andV36×36aretheprojectionmatricesand\nΣ36×36isthematrixofsingularvalues. Theﬁrstsixrows\nofthetranspose, UT,ofU,areﬁnallyselectedasthefea-\nturesequence.\n2.2 Thumbnail selection\nTheSSMisgenerated fromtheﬁrstsixrows of UTusing\ntheEuclideanDistancefunctionasmetric[5]. Byitsdeﬁ-\nnitionthe SSMissymmetricaroundthemaindiagonaland\nitthereforesufﬁcestofocusonitslowertriangle. Ataﬁrst\nstep, theSSMis correlated with a rectangular window, w\n(sizeD×D). The window has 1’s on the main diagonal\nand zeros elsewhere. If ( i,j) are the position indices of\nan element of SSM, the upper left corner of wis chosen\nto coincide with ( i,j). The correlation result, S(i,j), for\nSSM(i,j)istherefore computed asfollows:\nS(i,j) =D−1/summationdisplay\nd1=0D−1/summationdisplay\nd2=0SSM(i+d1,j+d2)w(d1,d2)\n=D−1/summationdisplay\nd=0SSM(i+d,j+d) (1)\nAt a second step, let S(k,m)be the lowest value of S.\nS(k,m)residesonthediagonalwithindex k−mandel-\nements {S(k,m),S(k+1,m+1),... ,S (k+D−1,m+\nD−1)}form a segment on the diagonal that deﬁnes the\ndesired thumbnail. The two corresponding feature subse-\nquences, i.e.,twoinstances of thethumbnail, are\n{UT\nk,UT\nk+1,... UT\nk+D−1}\nand\n{UT\nm,UT\nm+1,... ,UT\nm+D−1},\nrespectively. Parameter Dcontrols the size of the thumb-\nnail and is user deﬁned, depending on the corpus under\nstudy (seeSection 5).\nIthastobenotedthattheproposedthumbnailingsche-\nme is optional and depends on the dataset under study. If\nit is skipped, the rhythmic signatures (see next section)\nwill be extracted by taking into account the whole audio\nrecording. This is desirable if one is unsure whether the\ntwoinstancesoftheextractedthumbnailareindeedrepre-\nsentative of the complete music recording. In the sequel,\nthetermmusicsignalwillrefertoeitherthetwoinstances\nof the selected thumbnail or the complete music record-\ning.3 EXTRACTING RHYTHMIC SIGNATURES\n3.1 Feature extraction\nAtaﬁrststep,themusicsignal(i.e.,thetwothumbnailin-\nstancesorthecompleterecording)isshort-termprocessed\ntoextractasequenceof chroma-basedMFCCs,asinSec-\ntion 2.1. However, this time, shorter, overlapping win-\ndowsareused(windowlengthis ≃93msecsandwindow\nstepis 11.6msecs). Followingthenotationthatwasintro-\nduced in Section 2.1, let C= [c(1)c(2)... c(N)], be\nthenew sequence of MFCCs.\nAt a ﬁrst step, Cis long-term segmented with a mov-\ning long-term window (window length is 4secsand step\nis1sec). To simplify notation, let Ct= [ct(1)ct(2)...\nct(M)], be the subsequence that corresponds to the t-th\nlong-term window, where Mis the window length mea-\nsured in number of frames. The SSMis then calculated\nfor each long-term window, using the Euclidean Distance\nmetric. For the t-th long-term window, the mean value,\nRt(k),ofeachdiagonalinthelower SSMtriangleiscom-\nputed,i.e., Rt(k) =1\nM−k/summationtextM\nl=k/vextenddouble/vextenddoublect(l),ct(l−k)/vextenddouble/vextenddouble,where\nkis the diagonal index and /bardbl./bardblis the Euclidean distance\nfunction. Each Rtistreatedasasignal. Atanextstep,the\nmean signal, Rµ, ofall Rt’sis computed, i.e.,\nRµ(k) =1\nTT/summationdisplay\nt=1Rt(k),\nwhere Tis the number of long-term windows. Rµis then\nnormalized tounity, i.e., Rµ(k) =Rµ(k)\nmax(Rµ).\nAs can be seen in Figure 1, Rµexhibits a number of\nvalleys (local minima). Each valley corresponds to a pe-\nriodicity that is inherent in the music signal. Such peri-\nodicities are related to the rhythmic characteristics of th e\nrecording, e.g., music meter and tempo [7]. In what fol-\nlows, we will refer to Rµas therhythmic signature of the\nmusic recording. The main idea behind this approach, is\nthat, recordings with similar rhythmic characteristics ar e\nexpected to yield “similar” signatures (as can be seen in\nthe upper part of Figure 1). On the contrary, different\nrhythmic characteristics will result into “dissimilar” si g-\nnatures (bottom part of Figure 1). Therefore, the next\nchallengeistodeviseasimilaritymeasureforsignatures.\n4 SIMILARITY MEASURE FOR SIGNATURES\nIfLis the number of music recordings in a corpus, L\nrhythmicsignaturesareﬁrstextractedandstoredasmeta-\ndata. In order to measure similarity between signatures, a\nstandardDynamicTimeWarping costhasbeenemployed.\nAs is the case with DTWtechniques [8], a set of local\npath constraints needs to be ﬁrst deﬁned. In our study we\nexperimented with two types of constraints, i.e., Sakoe-\nChibaandItakuraand adopted theformer.\nIf a rhythmic signature is drawn from the corpus, its\nmatchingcostagainsttheremaining L-1signaturesiscal-\nculated using the adopted DTW technique. This proce-\ndureyields L−1costvalueswhicharesortedinascending0 0.5 1 1.5 2 2.5 3 3.5 400.20.40.60.81\nTime (sec)Normalized Mean Values Of SSM Diagonals For Chroma Based MFCCs\n0 0.5 1 1.5 2 2.5 3 3.5 400.20.40.60.81Normalized Mean Values Of SSM Diagonals For Chroma Based MFCCs\nTime (sec)song1: 7/8 meter, 268 bpm\n \n \nsong3: 2/4 meter, 91 bpmsong1 :7/8 meter, 268 bpm\n \n \nsong1 :7/8 meter, 260 bpm\nFigure 1.Top:Signatures from two recordings of music\nmeter7\n8.Bottom:Signatures from a recording of meter7\n8\nand a recording of meter2\n4.\norder, with the lowest values indicating highest similar-\nity. The next section focuses on evaluating this matching\nscheme on two corpora of traditional Greek and African\nmusic.\n5 EXPERIMENTS AND RESULTS\n5.1 Corpus of Greek Traditional Dance music\nThe ﬁrst corpus of our study consists of 220tracks of\nGreek Traditional Dance Music, which are drawn from\nvarious Greek regions. The tracks were manually cate-\ngorised into four genres as shown in Table 1. These gen-\nres exhibit certain variety in terms of instrumentation and\nrhythm. Fromthecorpusdescriptionitcanbenoticedthat\nthe longest music meter duration (approximately 2secs)\nappears in class 2, for tempo ∼=90bpmand music meter\n3\n4. Byusingathumbnailwhichis 10secslong,thelongest\nmusic meter is repeated up to 5times in each long-term\nsegment. Our study has revealed that, this expected num-\nberofrepetitionsissufﬁcientfortheextractionofreliab le\nrhythmic signatures and justiﬁes our choice for the length\nof thumbnails. Similarly, the longest meter duration af-\nfects the length of the long-term window in Section 3.1.\nBy the deﬁnition of self similarity analysis, a periodicity\nofklagswillmanifestitselfasavalleyof Rµ,ifthelong-\nterm window is at least 2klong. Therefore the length of\nthe long-term window was chosen equal to 4secsto cap-\nture the periodicity of the longest music meter. Finally,\ntherangeoflagsof Rµonwhich DTWisemployedstarts\nwith the lag corresponding to the fastest tempo value and\nreachesuptothelagthatcorrespondstothelongestmeter-\ntempo pair.\nTable 2 presents the confusion matrix for the Greek\ncorpus, where the leave-one-out method was applied on\nthe complete corpus. Table 2 reveals that, when only thebest result (lowest matching cost) was examined, limited\nconfusionoccuredbetweentheclasses 3and4andclasses\n1and2. Further experimentation revealed that, when the\ntwo lowest matching costs were taken into account, the\nconfusionmatrixremainedthesamewithinstatisticalcon-\nﬁdence.\nclass id #of songs metertempo range (bpm)\n1 53 2/4 91-95\n2 63 3/4 93-105\n3 62 7/8 250-280\n4 42 2/4 150-180\nTable 1. Descriptionof Greek Traditional Dance corpus.\nPrecision % Class 1 Class2 Class 3 Class 4\nClass1 94.3 3.2 1.7 0\nClass2 3.8 96.8 0 0\nClass3 1.9 096.6 10.9\nClass4 0 01.7 89.1\nRecall % Class 1 Class2 Class 3 Class 4\nClass1 94.3 3.8 1.9 0\nClass2 3.2 96.8 0 0\nClass3 1.6 090.3 8.1\nClass4 0 02.4 97.6\nTable2. PrecisionandrecallforGreekTraditionalcorpus.\n5.2 Corpus of African music\nA collection of 103 pieces was selected from the music\narchives of the Royal Museum of Central-Africa in Ter-\nvuren (Brussels). This institute has one of the most im-\nportant collections of African music in the world.1The\ncurrent selection contains ﬁeld recordings of Congo and\nRwanda recorded during the second half of the 20thcen-\ntury [9]. Similarly to the Greek music corpus, the rhyth-\nmicstructureishighlyrepetitiveandcontainsawiderange\nof rhythmic structures, including irregular meters that ar e\nseldom found inWesternmusic[10].\nclass id #of songs meter\n1 27 3/4\n2 26 4/4\n3 24 5/4\n4 26 6/4\nTable 3. Description of the 1stsetof the Africancorpus.\nThe corpus has been manually annotated with a (percei-\nved) ground truth which has been used to evaluate the\ncomputeranalysis. Twotypesofclassiﬁcationweremade:\none according to the meter, the other focusing on a selec-\ntionofcharacteristicrepetitiverhythmicpatterns. Theﬁ rst\nclassiﬁcation,Table3,usesfourmetricclasses3\n4,4\n4,5\n4and\n1http://music.africamuseum.bePrecision%\nClassid 1234\n168.84.3200\n212.582.6123.7\n315.613643.7\n43.10492.6\nRecall %\nClassid 1234\n178.63.617.90\n214.870.411.13.7\n32012644\n43.703.792.6\nTable 4. Precision and recall ( 1stsetof Africancorpus).\n6\n4. Table 5, is restricted to 44 pieces which can be classi-\nﬁedasvariantsof5prototypicalpatterns: short-long-lon g\n(quintuple); short-long-long-short-long-long-long (se xtu-\nple); long-short-short-long (triple1); short-long (trip le2);\nand short-short-long(duple).\nclassid # of songs pattern\nquintuple 10\nsextuple 14\ntriple1 8\ntriple2 5\nduple 7\nTable 5. Description of the 2ndset ofAfrican corpus.\nThe thumbnail that was selected for each piece by the\nproposedmethod,oftentendedtocontainpartsofthesong\nwhere the most percussive events occurred. Since this\ncould lead to a dense regular structure that can easily be\nconfused with patterns in which each beat is articulated\ntherhythmic signatures ,Rµ’s were extracted from whole\naudio recordings and the thumbnail scheme was skipped.\nTables 4, 6 reveal that the results of both sets of African\nmusic are promising. When manually checking the prob-\nlematiccases,mistakescanmostlyberelatedtotheoccur-\nrenceofvariantsofthemainpatternwithinonepiece. The\npossiblevariationsaremostlytheadditionofapercussive\neventwherearestusedtobe,ortheopposite: omissionof\na percussive event. The meter and beat stay however the\nsame.\n6 CONCLUSIONS\nThis paper presented a music retrieval method based on\nrhythmic similarity measurement. The method yielded\nsatisfactory results on coprora of traditional Greek and\nAfrican music. In future work, more sophisticated DTW\ntechniques will be used on larger corpora and the possi-\nbility to extract multiple rhythmic signatures per music\nrecording willbe investigated.Precision %\nClassid 12345\n170.803.101014.3\n24.284.4000\n38.33.186.700\n48.3013.3600\n58.39.403085.7\nRecall %\nClassid 12345\n1 855055\n23.696.4000\n312.56.381.300\n4 20020600\n514.321.4021.442.9\nTable 6. Precisionand recall ( 2ndset ofAfrican corpus).\n7 REFERENCES\n[1] J. Paulus and A. Klapuri, “Measuring the similarity\nof rhythmic patterns.”, Proceedings of ISMIR , Paris,\nFrance, September 2002.\n[2] S. Dixon, F. Gouyon and G. Widmer, “Towards char-\nacterisationofmusicviarhythmicpatterns.”, Proceed-\nings of ISMIR , Barcelona, Spain, 2004.\n[3] G. Peeters, “Rhythm Classiﬁcation Using Spectral\nRhythm Patterns”, Proceedings of ISMIR 2005 , Lon-\ndon, September, 2005.\n[4] F. Gouyon and S. Dixon, “Dance music classiﬁca-\ntion: a tempo-based approach.”, Proceedings of IS-\nMIR, Barcelona, Spain, 2004.\n[5] J.Foote,M.CooperandU.Nam,“AudioRetrievalby\nRhythmic Similarity”, Proceedings of ISMIR , Paris,\nFrance, September 2002.\n[6] M. Bartsch and G.H. Wakeﬁeld, “Audio thumbnail-\ning of popular music using chroma-based representa-\ntions”,IEEE Transactions on Multimedia , 7(1), 96-\n104, 2005.\n[7] A. Pikrakis, I. Antonopoulos and S. Theodoridis,\n“MusicMeterandTempoTrackingfromrawpolypho-\nnic audio”, Proceedings of ISMIR , Barcelona, Spain,\n2004.\n[8] S. Theodoridis and K. Koutroumbas, Pattern recogni-\ntion, Academic Press, 3dEdition, 2006.\n[9] O. Cornelis et al., “Digitisation of the ethnomusico-\nlogical Sound Archive of the Royal Museum for Cen-\ntralAfrica.”, IASAJournal , pp 35-43, 2005.\n[10] O. Cornelis et al., “Problems and Opportunities of\nApplying Data- & Audio-Mining Techniques to Eth-\nnicMusic”, JournalofIntangibleHeritage ,(inpress),\n2007."
    },
    {
        "title": "Signal + Context = Better Classification.",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet",
            "Pierre Roy",
            "Anthony Beurivé"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416852",
        "url": "https://doi.org/10.5281/zenodo.1416852",
        "ee": "https://zenodo.org/records/1416852/files/AucouturierPRB07.pdf",
        "abstract": "Typical signal-based approaches to extract musical descriptions from audio only have limited precision. A possible explanation is that they do not exploit context, which provides important cues in human cognitive processing of music: e.g. electric guitar is unlikely in 1930s music, children choirs rarely perform heavy metal, etc. We propose an architecture to train a large set of binary classifiers simultaneously, for many different musical metadata (genre, instrument, mood, etc.), in such a way that correlation between metadata is used to reinforce each individual classifier. The system is iterative: it uses classification decisions it made on some classification problems as new features for new, harder problems; and hybrid: it uses a signal classifier based on timbre similarity to bootstrap symbolic inference with decision trees. While further work is needed, the approach seems to outperform signal-only algorithms by 5% precision on average, and sometimes up to 15% for traditionally difficult problems such as cultural and subjective categories. 1 INTRODUCTION: BOOTSTRAPPING SYMBOLIC REASONING WITH ACOUSTIC ANALYSIS People routinely use many varied high-level descriptions to talk and think about music. Songs are commonly said to be “energetic”, to make us “sad” or ”nostalgic”, to sound “like film music” and to be perfect to “drive a car on the highway” among a possible infinity of similar metaphors. The Electronic Music Distribution industry is in demand of robust computational techniques to extract such descriptions from musical audio signals. The majority of existing systems to this aim rely on a common model of the signal as the long-term accumulative distribution of frame-based spectral features. Musical audio signals are typically cut into short overlapping frames (e.g. 50ms with a 50% overlap), and for each frame, a feature vector is computed. Features usually consists of generic, all-purpose spectral representations such as melfrequency cepstrum coefficients (MFCCs), but can also be e.g. rhythmic features [1]. The features are then fed to a statistical model, such as a Gaussian mixture model (GMM), which estimates their global distribution over the c⃝2007 Austrian Computer Society (OCG). total length of the extract. Global distributions can then be used to compute decision boundaries between classes (to build e.g. a genre classification system such as [2]) or directly compared to one another to yield a measure of acoustic similarity [3]. While such signal-based approaches are by far the most dominant paradigm currently, recent research increasingly suggests they are plagued with important intrinsic limitations [3, 5]. One possible explanation is that they take an auditory-only approach to music classification. However, many of our musical judgements are not low-level immediate perceptions, but rather high-level cognitive reasoning which accounts for the evidence found in the signal, but also depends on cultural expectations, a priori knowledge, interestingness and “remarkability” of an event, etc. Typical musical descriptions only have a weak and ambiguous mapping to intrinsic acoustic properties of the signal. In [6], subjects were asked to rate the similarity between pairs of 60 sounds and 60 words. The study concludes that there is no immediately obvious correspondence between single acoustic attributes and single semantic dimensions, and go as far as suggesting that the sound/word similarity judgment is a forced comparison (“to what extent would a sound spontaneously evoke the concepts that it is judged to be similar to?”). Similarly, we studied in [4] the performance of a typical classifier on a heterogeneous set of more than 800 high-level musical symbols, manually annotated for more than 4,000 songs. We observed that surprisingly few of such descriptions can be mapped with reasonable precision to acoustic properties of the corresponding signals. Only 6% of the attributes in the database are estimated with more than 80% precision, and more than a half of the database’s attributes are estimated with less that 65% precision (which hardly better than a binary random choice, i.e. 50%). The technique provides very precise estimates for attributes such as homogeneous genre categories or extreme moods like “aggressive” or “warm”, but typically fails on more cultural or subjective attributes which bear little correlation with the actual sound of the music being described, such as “Lyric Content”, or complex moods or genres (such as “Mysterious” or “Electronica”). This does not mean human musical judgements are beyond computational approximation, naturally. The study in [4] shows that there are large amounts of correlation between musical descriptions at the symbolic level. Table 1 shows a selection of pairs of musical metadata items (from a large manually-annotated set), which were found Table 1. Selected pairs of musical metadata with their Φ score (χ2 normalized to the size of the population), between 0 (corresponding to statistical independence between the variables) and 1 (complete deteministic association). Data analysed on a a set of 800 metadata values manually annotated for more than 4,000 songs, used in previous study [4] Attribute1 Attribute2 Φ Music-independant Textcategory Christmas Genre Special Occasions",
        "zenodo_id": 1416852,
        "dblp_key": "conf/ismir/AucouturierPRB07",
        "keywords": [
            "cognitive processing",
            "binary classifiers",
            "musical metadata",
            "correlation between metadata",
            "iteration",
            "symbolic inference",
            "signal classifier",
            "timbre similarity",
            "decision trees",
            "bootstrap"
        ],
        "content": "SIGNAL +CONTEXT =BETTER CLASSIFICATION\nJean-Julien Aucouturier\nGrad. SchoolofArts andSciences\nThe University ofTokyo, JapanFranc ¸ois Pachet,PierreRoy, AnthonyBeuriv ´e\nSONYCSLParis\n6rue Amyot, 75005 Paris,France\nABSTRACT\nTypical signal-based approaches to extract musical de-\nscriptions from audio only have limited precision. A pos-\nsibleexplanationisthattheydonotexploitcontext,which\nprovidesimportantcuesinhumancognitiveprocessingof\nmusic: e.g. electricguitarisunlikelyin1930smusic,chil -\ndren choirs rarely perform heavy metal, etc. We propose\nan architecture to train a large set of binary classiﬁers si-\nmultaneously,formanydifferentmusicalmetadata(genre,\ninstrument,mood,etc.),insuchawaythatcorrelationbe-\ntweenmetadataisusedtoreinforceeachindividualclassi-\nﬁer. Thesystemisiterative: itusesclassiﬁcationdecisio ns\nit made on some classiﬁcation problems as new features\nfornew,harderproblems;andhybrid: itusesasignalclas-\nsiﬁer based on timbre similarity to bootstrap symbolic in-\nferencewithdecisiontrees. Whilefurtherworkisneeded,\nthe approach seems to outperform signal-only algorithms\nby5%precisiononaverage,andsometimesupto15%for\ntraditionally difﬁcult problems such as cultural and sub-\njective categories.\n1 INTRODUCTION: BOOTSTRAPPING\nSYMBOLIC REASONING WITHACOUSTIC\nANALYSIS\nPeople routinely use many varied high-level descriptions\nto talk and think about music. Songs are commonly said\nto be “energetic”, to make us “sad” or ”nostalgic”, to\nsound “like ﬁlm music” and to be perfect to “drive a\ncar on the highway” among a possible inﬁnity of similar\nmetaphors. The Electronic Music Distribution industry\nis in demand of robust computational techniques to ex-\ntract such descriptions from musical audio signals. The\nmajority of existing systems to this aim rely on a com-\nmon model of the signal as the long-term accumulative\ndistribution of frame-based spectral features. Musical au -\ndio signals are typically cut into short overlapping frames\n(e.g. 50ms with a 50% overlap), and for each frame, a\nfeature vector is computed. Features usually consists of\ngeneric, all-purpose spectral representations such as mel -\nfrequency cepstrum coefﬁcients (MFCCs), but can also\nbe e.g. rhythmic features [1]. The features are then fed\nto a statistical model, such as a Gaussian mixture model\n(GMM),whichestimatestheirglobaldistributionoverthe\nc/circlecopyrt2007 AustrianComputer Society (OCG).total length of the extract. Global distributions can then\nbe used to compute decision boundaries between classes\n(to build e.g. a genre classiﬁcation system such as [2])\nor directly compared to one another to yield a measure of\nacoustic similarity[3].\nWhilesuchsignal-basedapproachesarebyfarthemost\ndominantparadigmcurrently,recentresearchincreasingl y\nsuggests they are plagued with important intrinsic limita-\ntions [3, 5]. One possible explanation is that they take an\nauditory-only approach to music classiﬁcation. However,\nmany of our musical judgements are not low-level imme-\ndiate perceptions, but rather high-level cognitive reason -\ning which accounts for the evidence found in the signal,\nbut also depends on cultural expectations, a priori knowl-\nedge,interestingnessand“remarkability”ofanevent,etc .\nTypical musical descriptions only have a weak and am-\nbiguous mapping to intrinsic acoustic properties of the\nsignal. In [6], subjects were asked to rate the similar-\nity between pairs of 60 sounds and 60 words. The study\nconcludesthatthereisnoimmediatelyobviouscorrespon-\ndence between single acoustic attributes and single se-\nmantic dimensions, and go as far as suggesting that the\nsound/word similarity judgment is a forced comparison\n(“to what extent would a sound spontaneously evoke the\nconcepts that it is judged to be similar to?”). Similarly,\nwestudiedin[4]theperformanceofatypicalclassiﬁeron\na heterogeneous set of more than 800 high-level musical\nsymbols, manually annotated for more than 4,000 songs.\nWe observed that surprisingly few of such descriptions\ncanbemappedwithreasonableprecisiontoacousticprop-\nerties of the corresponding signals. Only 6% of the at-\ntributesinthedatabaseareestimatedwithmorethan80%\nprecision,andmorethanahalfofthedatabase’sattributes\nare estimated with less that 65% precision (which hardly\nbetter than a binary random choice, i.e. 50%). The tech-\nnique provides very precise estimates for attributes such\nas homogeneous genre categories or extreme moods like\n“aggressive” or “warm”, but typically fails on more cul-\ntural or subjective attributes which bear little correlati on\nwith the actual sound of the music being described, such\nas “Lyric Content”, or complex moods or genres (such as\n“Mysterious” or“Electronica”).\nThisdoesnotmeanhumanmusicaljudgementsarebe-\nyond computational approximation, naturally. The study\nin [4] shows that there are large amounts of correlation\nbetween musical descriptions at the symbolic level. Ta-\nble1showsaselectionofpairsofmusicalmetadataitems\n(from a large manually-annotated set), which were foundTable 1. Selected pairs of musical metadata with their Φscore ( χ2normalized to the size of the population), between\n0 (corresponding to statistical independence between the v ariables) and 1 (complete deteministic association). Data\nanalysed on aaset of 800 metadata values manually annotated for morethan 4,000 songs, usedinprevious study[4]\nAttribute1 Attribute2 Φ\nMusic-independant\nTextcategory Christmas Genre Special Occasions 0.89\nMood strong Character powerful 0.68\nMood harmonious Character well-balanced 0.60\nCharacter robotic Mood technical 0.55\nMood negative Character mean 0.51\nMusic-dependant\nMain Instruments Spoken Vocals Style Rap 0.75\nStyleReggae Country Jamaica 0.62\nMusical Setup Rock Band MainInstruments Guitar (distortion) 0.54\nCharacter Mean StyleMetal 0.53\nMusical Setup BigBand Aera/Epoch 1940-1950 0.52\nMain Instruments transverseﬂute Character Warm 0.51\nto particularly fail a Pearson’s χ2-test ([7]) of statistical\nindependence. χ2teststhehypothesisthattherelativefre-\nquencies of occurrence of observed events follow a ﬂat\nrandom distribution (e.g. that hard rock songs are not\nsigniﬁcantly more likely to talk about violence than non\nhard-rock songs). On the one hand, we observe consid-\nerablecorrelativerelationsbetweenmetadata,whichhave\nlittle to do with the actual musical usage of the words.\nForinstance,theanalysisrevealscommon-senserelations\nsuch as “Christmas” and “Special occasions” or “Well-\nknown” and “Popular”. This illustrates that the process\nof categorizing music is consistent with psycholinguistic s\nevidences of semantic associations, and that the speciﬁc\nusage of words that describe music is largely consistent\nwith their generic usage: it is difﬁcult to think of music\nthat is e.g. both strong and not powerful. On the other\nhand, we also ﬁnd important correlations which are not\nintrinsic properties of the words used to describe music,\nbut rather extrinsic properties of the music domain be-\ning described. Some of these relations capture historical\n(“ragtime is music from the 1930’s”) or cultural knowl-\nedge (“rock uses guitars”), but also more subjective as-\npectslinkedtoperceptionoftimbre(“ﬂutesoundswarm”,\n“heavy metal sounds mean”).\nHence, we arefacing asituationwhere:\n1. Traditional signal-based approaches (e.g. nearest-\nneighborclassiﬁcationwithtimbresimilarity)work\nforonlyafewwell-deﬁnedcategories,whichhavea\nclearandunambiguoussoundsignature(e.g. Heavy\nmetal).\n2. Correlations at the symbolic level are potentially\nuseful for many categories, and can be easily ex-\nploitedbymachinelearningtechniquessuchasDe-\ncision Trees [8]. However, these require the avail-\nability of values for non-categorical attributes, to\nbe used as features for prediction: we have to ﬁrst\nknow that “this has distorted guitar”, to infer that“it’sprobably rock”.\nThis paper quite logically proposes to use the former\nto bootstrap the latter. First, we use a timbre-based clas-\nsiﬁer to estimate the values of a few timbre-correlated\nattributes. Then we use decision trees to make further\npredictions of cultural attributes on the basis of the pool\nof timbre-correlated attributes. This results in an itera-\ntive system which tries to solve simultaneously a setof\nclassiﬁcation problems, by using classiﬁcation decisions\nitmadeonsomeproblemsasnewfeaturesfornew,harder\nproblems.\n2 ALGORITHM\nThis section describes the hybrid classiﬁcation algorithm ,\nstarting with its 2 sub-components: an acoustic classiﬁer\nbasedontimbresimilarity,andadecision-treeclassiﬁert o\nexploit symbolic-level correlations between metadata. In\nthefollowing,metadataitemsarenotatedas attributes Ai,\nwhich take boolean values Ai(S)for a given song S(e.g.\nhasguitar( S)∈ {true,false }).\n2.1 Sub-component1: Signal-based classiﬁer\nThe acoustic component of the system is a nearest neigh-\nbor classiﬁer based on timbre similarity. We use the tim-\nbre similarity algorithm described in [3]: 20-coefﬁcient\nMFCCs, modelled with 50-state GMMs, compared with\nMonte-Carlo approximation of the Kullback-Leibler dis-\ntance. Theclassiﬁerinfersthevalueofagivenattribute A\nforagivensong Sbylookingatthevaluesof Aforsongs\nthat are timbrally similar to S. For instance, if 9 out of\nthe10nearestneighbors ofagivensongare“HardRock”\nsongs, then it is very likely that the seed song be a “Hard\nRock” songitself.\nMore precisely, we deﬁne as our observation OA(S)\nthe number of songs among the set NSof the 10 nearestneighbors of Sforwhich Aistrue, i.e.\nOA(S) =card{Si\\ Si∈ N S∧ A(Si)}(1)\nWemakeamaximum-likelihooddecision(withﬂatprior)\non thevalue of theattribute Abased on OA(S):\nA(S) =p(OA(S)/A(S))> p(OA(S)/A(S))(2)\nwhere p(OA(S)/A(S))is the probability to observe a\nnumber OA(S)of true values in the set of nearest neigh-\nbors of S, given that Ais true, and p(OA(S)/A(S))is\nthe probability to make the same observation given that\nAis false. The likelihood distribution p(OA(S)/A(S))\nis estimated on a training database by the histogram of\ntheempiricalfrequenciesofthenumberofpositiveneigh-\nbors for all songs having A(S) =true(similarly for\nP(OA(S)/A(S))).\n2.2 Sub-component2: Decision-tree classiﬁer\nThe symbolic component in our system is a decision-tree\nclassiﬁer [8]. It predicts the value of a given attribute (th e\ncategoryattribute)onthebasisofthevaluesofsome non-\ncategoryattributes,inahierarchicalmanner. Forinstance,\natypical treecould classifyasong as “natural/acoustic”\n•ifitisnot aggressive\n•else,ifitisfromthe50’s(wherelittleampliﬁcation\nwas used)\n•else,ifit’safolkor ajazz band that performs it,\n•else,ifit doesn’t use guitar withdistortion,etc.\nDecision rules are learned on a training database with the\nimplementationofC4.5providedbytheWekalibrary[8].\nAsmentionnedabove, adecisiontreeforagivenattribute\nisonlyabletopredictitsvalueforagivensongifwehave\naccess to all the values of the other non-categorical at-\ntributes for that same song. Therefore, it is of little use\nas such. The algorithm described in the next section uses\ntimbresimilarityinferencetobootstraptheautomaticcat -\negorization with estimates of a few timbre-grounded at-\ntributes, and then use these estimates in decision trees to\npredict non-timbre correlated attributes.\n2.3 Training procedure\nThe algorithm is a training procedure to generate a set of\nclassiﬁers for Nattributes {Ak;k∈[1,N]}. Training is\niterative, and requires a database of musical signals with\nannotated values for all Ak. At each iteration i, we pro-\nduce a set of classiﬁers {/tildewidestAki;k∈[1,N]}, which each\nestimates the attribute Akat iteration i. Each classiﬁer is\nassociatedwithaprecision p(/tildewidestAki). Ateachiteration i,we\ndeﬁne as best(/tildewidestAki)the best classiﬁer of Aksofar,i.e.\nbest(/tildewidestAki) =/tildewidestAkm,m= arg max\nj≤ip(/tildewidestAkj)(3)BEST > θ BEST > θ\nBEST > θ BEST > θ\nBEST > θ BEST > θΑ1i\nΑki\nΑniΑ1i+1 Α1i+2\nΑki+1 Αki+2\nΑni+2 Αni+1\nFigure 1. The algorithm constructs successive classiﬁers\nfor the attributes Ak. At each iteration, the features of the\nnew classiﬁers is the set of the previous classiﬁers (in the\nset of {Al}l/negationslash=k) with precision greater than threshold θ.\nAteachiteration,foreachattribute,onlythebestestimat e\nso far is stored for future use. The Ai\nkare estimated by\ntimbreinferencefor i= 1,andbydecisiontreesfor i≥2\nMore precisely, at each iteration i, each of the newly-\nbuilt classiﬁers takes as input (aka features) the output of\nclassiﬁers from the previous generations, based on their\nprecision. Hence, each iteration in this overall training\nprocedure requires some training (to build the classiﬁers)\nandsometesting(toselecttheinputofthenextiteration’s\nclassiﬁers).\n•i= 1: Bootstrapwithtimbreinference\nClassiﬁers: Theclassiﬁers/tildewidestAk1arebasedontimbre\ninference, as described in Section 2.1. Each of\nthese classiﬁers estimates the value of an attribute\nAkforasong Sbasedontheaudiosignalonly,and\ndoesn’t requireany attributevalue ininput.\nTraining: Timbre inference requires train-\ning to evaluate the likelihood distributions\np(OAk(S)/Ak(S)), and hence a training set\nL1(Ak)for each attribute Ak\nTesting: Each/tildewidestAk1is tested on a testing set\nT1(Ak). For each song S∈ T1(Ak), estimates\n/tildewidestAk1(S)are compared to groundtruth Ak(S), to\nyield aprecision value p(/tildewidestAk1).\n• ∀i≥2: Iterative improvement by decision trees\nClassiﬁers: The classiﬁers/tildewidestAkiare decision trees,\nasdescribedinSection2.2. Eachofthemestimates\nthe value of an attribute Akbased on the output of\nthebestclassiﬁersfrompreviousgenerations. More\nprecisely, the non-category attributes (aka features)\nusedinthe/tildewidestAkiaretheattributeestimatesgenerated\nby a subset Fkiof all previous classiﬁers {/tildewidestAlj;l∈\n[1,N],j < i}, deﬁned as:\nFki={best(/tildewidestAli−1);l/\\e}atio\\slash=k,p(best(/tildewidestAli−1))≥θ}\n(4)\nwhere 0≤θ≤1is a precision threshold. Fkicon-\ntainstheestimategeneratedbythebestclassiﬁerso\nfar (up to iteration i−1) for every attribute other\nthanAk, provided that its precision be greater than\nθ. This isillustratedinFigure 1.\nTraining: Decision trees require training to buildand trim decision rules, and hence a training set\nLi(Ak)for each category attribute Akand each it-\neration i≥2: new trees have to be trained for ev-\nery new set of features attributes Fki, which are\nselected based on their precision at previous it-\nerations. Trees are trained using the truevalues\n(groundtruth) of the non-categorical attributes (but\nthey will be tested using estimated values for these\nsameattributes,see below).\nTesting: Each/tildewidestAkiis tested on a testing set\nTi(Ak). For each song S∈ Ti(Ak), estimates\n/tildewidestAki(S)are computed using the estimated values\nbest(/tildewidestAli−1)(S)of the non-categorical attributes\nAl, i.e. values computed by the corresponding best\nclassiﬁer,andcomparedtothe truevalueAk(S),to\nyieldaprecision value p(/tildewidestAki).\n•Stop condition: The training procedure terminates\nwhen there is no more improvement of precision\nbetween successive classiﬁers for any attribute, i.e.\ntheset ofall best(/tildewidestAki)reaches aﬁxed point.\n2.4 Output\nThe output of the above training procedure is a ﬁnal set\nof classiﬁers, containing the best classiﬁers for each Ak,\ni.e.{best(/tildewidestAkif),k∈[1,N]}, where ifis the iteration\nwhere stop condition is reached. For a given attribute Ak,\nthe ﬁnal classiﬁer is a set of 1≤n≤N.ifcomponent\nclassiﬁers, arranged in a tree where parent classiﬁers use\nresults of their children. The top-level node is a decision\ntree1forAk,theintermediatenodesaredecisiontreesfor\nAkbut also part of the other attributes Al,l∈[1,N], and\ntheleavesaretimbreclassiﬁersalsoforpartofthe Al,l∈\n[1,N]. Each component classiﬁer has ﬁxed parameters\n(likelihooddistributionsfortimbreclassiﬁersandrules for\ndecisiontrees)andﬁxedfeatures(the Fki),asdetermined\nby the above training process. Therefore, they are stand-\nalone algorithms which take as input an audio signal S,\nand outputs an estimate /tildewiderAk(S).\nFigure 2 illustrates a possible outcome scenario of the\nabove process, using a set of attributes including “Style\nMetal”, “Character Warm”, “Style Rap” (which are at-\ntributes well correlated with timbre) and “TextCategory\nLove” and “Setup Female Singer”, which are poor tim-\nbre estimates (the former being arguably “too cultural”,\nand the latter apparently too complex to be precisely de-\nscribed by timbre). The ﬁrst set of classiﬁers/tildewidestSA0is built\nusing timbre inference, and logically performs well for\nthe timbre-correlated attributes, and poorly for the other s.\nClassiﬁersatiteration2estimateeachoftheattributesus -\ning a decision tree on the output of the timbre classiﬁers\n(onlykeepingclassiﬁersabove θ= 0.75, whichappearin\ngray). For instance, the classiﬁer for “Style Metal” uses a\ndecision tree on the output of the classiﬁers for “Charac-\n1ora timbreclassiﬁer for Akifif= 1\nFigure 2. An example scenario of iterative attribute esti-\nmation\nter Warm” and “Style Rap”, and achieves poorer classiﬁ-\ncation precision that the original timbre classiﬁer. Simi-\nlarly,theclassiﬁerfor“SetupFemaleSinger”usesadeci-\nsion tree on “Style Metal”, “Character Warm” and “Style\nRap”, which results on better precision than the original\ntimbre classiﬁer. At the next iteration, the just-produced\nclassiﬁerfor“SetupFemaleSinger”(whichhappenstobe\nabovethreshold θ)isusedinadecisiontreetogiveagood\nestimate of “TextCategory Love” (as e.g. the knowledge\nof whether the singer is a female may give some infor-\nmation about the lyric contents of the song). At the next\niteration, all best classiﬁers so far may be used in a de-\ncision tree to yield a classiﬁer of “Style Metal” which is\neven better than the original timbre classiﬁer (as it uses\nsomeadditional cultural information).\n3 RESULTS\n3.1 Database\nWe report here on preliminary results of the above al-\ngorithm, using a database of human-made judgments of\nhigh-levelmusicaldescriptions,collectedforalargequa n-\ntity of commercial music pieces. The data is proprietary,\nand made available to the authors by research partner-\nships. The database contains 4936 songs, each described\nby a set of 801 boolean attributes (e.g. “Mood happy”=\ntrue). These attributes are grouped in 18 categories, some\nof which being correlated with some acoustic aspect of\nthe sound (“Main Instrument”,“Dynamics”), while others\nseem to result from a more cultural take on the music ob-\nject (“Genre”, “Mood”, “Situation2”). Attribute values\nwere ﬁlled in manually by human listeners, under a pro-\ncessrelatedtoCollaborative Tagging, inabusinessinitia -\ntive comparable tothe Pandora project3.\n2i.e. in which everyday situation would the user like to liste n to a\ngivensong, e.g. “this is music fora birthdayparty”\n3http://www.pandora.com/3.2 About theisolationbetween training and testing\nAs seen above, there are several distinct training and test-\ning stages in the training procedure described here. For\na joint optimisation of Nattributes over ifiterations,\nas many as N.iftraining sets Li(Ak)and testing sets\nTi(Ak)havetobeconstructeddynamically. Additionally,\nforthepurposeofevaluationwhenthistrainingprocedure\nisﬁnished,ﬁnalalgorithmsforall Akhavetobetestedon\nseparate testingsets W(Ak).\nThe construction of these various datasets has to re-\nspectseveralconstraintstoensureisolationbetweentrai n-\ning and testingdata.\n•Nooverlap between Ti(Ak)andLi(Ak)\n•No overlap between Ti(Ak)and∪j<i,∀lLj(Al)\n(since the classiﬁer/tildewidestAkiuses component classiﬁers\nfromprevious iterations,for possiblyall attributes)\n•No overlap between W(Ak)and all other training\nand testing sets, i.e. {Li(Al);1≤i≤if,1≤l≤\nN} ∪ {Ti(Al);1≤i≤if,1≤l≤N}.\nIn practice, these set constraints are very difﬁcult to\nenforce when one requires balanced datasets (roughly as\nmany positive and negative examples for all attributes in\nall training and testing sets): it is a complex combinato-\nrial problem, all the more so as the number of attributes\nNincreases (which is a desirable feature as seen in Sec-\ntion 3.3). Unbalanced datasets create additional learning\nproblems which we found were also difﬁcult to handle\nin the current iterative framework, notably because cross-\nvalidation cannot be conducted at every iteration[9].\nTherefore, we opted for an approximation strategy\nwhere datasets weretaken as:\n•AllTi(Ak)equal∀i; allLi(Ak)equal∀i\n• Ti(Ak)andLi(Ak)contain as many positive and\nnegative examples for Ak\n•No overlap between W(Ak)and{Li(Ak)} ∪\n{Ti(Ak)}.\nSuch datasets cannot guarantee complete isolation be-\ntween training ( L) and testing ( T) dataduring the train-\ning procedure . This doesn’t affect the reliability of the\nﬁnaltestingstage,asthe Wsetsareproperlyindependent\nfrom the LandTdata used during training. However,\ninteractions between the sets used for training probably\nleads to over-estimations of the performance on training\ndata(LandTsets),aswellasthehighvarianceobserved\non test performance ( Wsets) (see below). On the whole,\nthis is a consequence of the rather unconventional learn-\ningarchitectureinvestigatedhere,andisclearlysubject ed\ntofurther workand clariﬁcation.\n3.3 Evaluation\nTable2showsthetestperformanceoftheabovealgorithm\non a set of 45 randomly chosen attributes, using θ= 0.7.30 out of the 45 attributes see their classiﬁcation preci-\nsion improved by the iterative process (the remaining 15\ndo not appear in the table). We observe that, for 10 clas-\nsiﬁers, the precision improves by more than 10% (abso-\nlute), and that 15 classiﬁers have a ﬁnal precision greater\nthan 70%. Cultural attributes such as “Situation Sailing”\nor“SituationLove”canbeestimatedwithreasonablepre-\ncision, whereas their initial timbre estimate was poor. It\nalsoappearsthattwo“MainInstrument”attributes(guitar\nand choir), that were surprinsingly bad timbre correlates,\nhave been reﬁned using correlations between cultural at-\ntributes. This is consistent with the example scenario in\nFigure 2.\nTable 2. Set Optimization of 45 attributeestimates\nAttribute p(/tildewidestAk0)p(/tildewidestAkif)if∆(p)\nSituation Sailing 0.48 0.71100.23\nSituation Flying 0.49 0.64 30.15\nSituation Rain 0.50 0.64 90.14\nInstrumentGuitar 0.60 0.69 40.09\nSituation Sex 0.59 0.68110.09\nSituation Love 0.63 0.70 30.07\nLyrics Love 0.61 0.67110.06\nSituation Party 0.60 0.66 60.06\nTempomedium 0.59 0.64 40.05\nCharacterslick 0.65 0.69110.04\nAera/Epoch 90s 0.71 0.75130.04\nCharacterharmony 0.62 0.66 60.04\nRhythmicsrhythmic 0.64 0.68 40.04\nGenre Dancemusic 0.65 0.68120.03\nMooddreamy 0.64 0.67 20.03\nStyle Pop 0.71 0.74 60.03\nMoodpositive 0.58 0.61 60.03\nMoodharmonious 0.62 0.65 40.03\nInstrumentChoir 0.60 0.63130.03\nDynamics up+down 0.61 0.63 50.02\nLyrics Associations 0.57 0.59100.02\nVariant expressive 0.62 0.64 20.02\nSetupPop Band 0.72 0.74 70.02\nLyrics Poetry 0.57 0.59100.02\nCharacterfriendly 0.65 0.67 60.02\nCharacterrepeating 0.63 0.64 90.01\nRhythmicsgroovy 0.63 0.64 40.01\nMoodromantic 0.69 0.70 90.01\nLyrics Wisdom 0.58 0.59 40.01\nLyrics Romantics 0.65 0.66140.01\nFigure3showstheinﬂuenceofthenumberofattributes\nconsideredforjointclassiﬁcationontheaverageimprove-\nment of precision. It appears that using more attributes\nleads to larger improvements of (testing) precision over\npurely signal-based approaches: this allows the decision\ntrees to exploit stronger correlations than in smaller sets .\nLarger sets also improve the stability of the results: the\nperformance on small sets depends critically on the qual-\nity of the original timbre-correlated estimates, which are5  10 15 20 40 60 80 100 1502002503004000.0050.010.0150.020.0250.030.0350.040.0450.050.055Influence of the number of attributes in the optimized set on the final precision improvement, \naveraged over all attributes in the set (averaged over 50 trials, using keep ratio = 0.7)\nnumber of attributes (log scale) final mean precision improvement (linear scale)\nFigure 3. Inﬂuence of the number of attributes consid-\neredforjointoptimizationontheaverageimprovementof\nprecision\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.0050.010.0150.020.0250.030.035\nKeep ratioFinal mean precision improvementInfluence of the value of the keep ratio on the final precision improvement (averaged over all attributes)\n(over 20 tests with sets of 50 random attributes)\nFigure 4. Inﬂuence of precision threshold θon the aver-\nage improvement of precision\nused forbootstrap.\nOnthewhole,itappearsthattheapproachcanimprove\nprecisionoversimplesignal-basedapproachesbyasmuch\nas 5% on average, when considering sets of several hun-\ndreds ofattributes.\nFigure 4 shows the inﬂuence of the precision thresh-\nold parameter, used at each iteration to select classiﬁers\nfrom previous iterations to be used as features. The pa-\nrameter is a tradeoff between quantity and quality of the\ncorrelations to be exploited in decision trees. The curve\nhas an intuitive inverted-U shape: small θvalues lead to\nselecting too many bad classiﬁers, whereas large θval-\nuesconstrainthesystemtouseonlyhigh-qualityfeatures,\nwhich are ultimately too few to boostrap correlation anal-\nysis. The optimal value is found around 70% precision,\nwhichisconsistentwiththeempiricalupper-boundfound\nwithsignal-onlyapproaches(so-called“glassceiling”)[ 3]\n4 CONCLUSION\nWehavedescribedaniterativeprocedure totrainsimulta-\nneously a set of classiﬁers for high-level music metadata.\nThesystemexploitscorrelationsbetweenmetadata,using\ndecision trees, to reinforce each individual classiﬁer. Th e\napproach outperforms signal-only algorithms by 5% pre-\ncision on average when a sufﬁcient number of metadata\nare considered jointly. It provides reasonable solutions t o\ntraditionally difﬁcult problems, such as complex genres\nor “situations in which one would like to play the song”.However,theconcurrenttrainingandtestingofverymany\nclassiﬁcation algorithms makes the task of constructing\nwell-behavedtrainingandtestingdatasetsunusuallydifﬁ -\ncult. Some solutions remain to be found, either in the di-\nrection of algorithms to resample balanced datasets (e.g.\ncombinatorialoptimisation)oralternativeformulations of\nthelearning architecture (e.g. Bayesian belief networks) .\n5 REFERENCES\n[1] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer,\n“Probabilisticcombinationoffeaturesformusicclas-\nsiﬁcation,” in Proceedings of the 5th International\nConferenceonMusicInformationRetrieval ,Victoria,\nBC(Canada), 2006.\n[2] G. Tzanetakis, G. Essl, and P. Cook, “Automatic mu-\nsicalgenreclassiﬁcationofaudiosignals,”in proceed-\nings ISMIR , 2001.\n[3] J.-J. Aucouturier and F. Pachet, “Improving timbre\nsimilarity: Howhigh’sthesky?” JournalofNegative\nResults in Speech and Audio Sciences , vol. 1, no. 1,\n2004.\n[4] ——,“Howmuchauditioninvolvedineverydaycate-\ngorization of music?” Cognitive Science (submitted) ,\n2007.\n[5] C. McKay and I. Fujinaga, “Musical genre classiﬁ-\ncation: Is it worth pursuing and how can it be im-\nproved?” in Proceedings International Conference\non Music Information Retrieval (ISMIR) , Vancouver\n(BC),Canada, 2006.\n[6] P. Janata, “Timbre and semantics,” keynote\nPresentation, Journ ´ees fondatrices Perception\nSonore, Lyon (France), January 2007. Available:\nhttp://www.sfa.asso.fr/fr/gps.\n[7] D. Freedman, R. Pisani, and R. Purves, Statistics, 3rd\nedition. W.W.Norton and Co., New York, 1997.\n[8] J. Quinlan, C4.5: Programs for Machine Learning .\nMorgan Kauffman, 1993.\n[9] J. Zhang and I. Mani, “k-nn approach to unbalanced\ndata distributions,” in Proceedings of the Interna-\ntional Conference on Machine Learning (Workshop\nonLearningfromUnbalancedDatasets) ,Washington\nDC, USA, 2003."
    },
    {
        "title": "Variable-Size Gaussian Mixture Models for Music Similarity Measures.",
        "author": [
            "Wietse Balkema"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415506",
        "url": "https://doi.org/10.5281/zenodo.1415506",
        "ee": "https://zenodo.org/records/1415506/files/Balkema07.pdf",
        "abstract": "An algorithm to efficiently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efficient and prevents the model from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classification and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classification precision and is approximately 40% less computationally expensive. 1 INTRODUCTION In the current boom of web 2.0, the market for music recommender systems seems to have taken off. Last.fm, iLike, myStrands and others analyze a user’s listening behaviour and compare it with other user’s profiles. Music can be tagged with personal tags which allows new ways to explore music collections. One of the major problems of these community based recommender systems is their robustness in new databases and dealing with underrepresented data. Once a song is chosen as favourite, a loop mechanism can keep this song as favourite for a long time. Community based recommenders can be sensitive to attacks that try to influence a specific song’s rating. Two kinds of attack strategies are shilling (promoting an item) and nuking (demoting an item) [8]. Another approach for recommender systems that do not suffer from loops, shilling or nuking is content-based recommendation. The acoustical content of a song is analyzed, and songs found to be ‘similar’ to songs a user likes are recommended. Audio similarity is multifaceted, so a common approach to evaluate audio similarity measures is to perform a genre classification task. Pampalk et al. [11] c⃝2007 Austrian Computer Society (OCG). found that a combination of 70% timbral and 30% temporal features provide a good audio similarity measure. Hubs, songs that are found to be very similar to a very large number of other songs, are a major problem for audio-based music recommender systems. Aucouturier and Pachet [3] showed that in a purely timbre-based nearest neighbor retrieval system, the number of hubs significantly increases when discarding the 5% least significant clusters from a Gaussian mixture model. The computational complexity for calculating the distance between Gaussian mixture models scales linearly with the number of clusters in a mixture model for most distance measures. Reducing the number of clusters in a model thus has great impact in computational complexity, but influences performance. We present a method to reduce the number of mixture components without sacrificing retrieval performance. The required number of Gaussians in a Gaussian mixture model is estimated for each song individually. The number of clusters is then used by the Estimation Maximization algorithm to model the song data. Using individual song complexity estimation prevents overfitted models on ‘simple’ songs with too complex models, while still offering ‘complex’ songs an adequate model complexity. The remainder of this paper is organized as follows: In section 2 we present a short overview of related work. Section 3 describes our feature modelling approach in detail. This section is followed by a performance analysis with respect to the effect of our algorithm on hubs and on a genre classification task. In our last section we summarize our results and give some recommendations for further research. 2 RELATED WORK Berenzweig et al. [4] compare anchor space based and GMM based similarity measures with a similarity matrix retrieved from a user survey. The anchor space method performs very similarly to the GMM method. Aucouturier and Pachet [2] systematically explore feature parameter space for timbre similarity experiments and evaluate performance with a nearest neighbour retrieval task. Optimal R-precision was found with 20 dimensional MFCCs and a Gaussian mixture model with 50 components. The number of model components however is of less influence than the number of feature dimensions. Their conclusion is that ‘Everything performs the same’ and that there seems to be a glass ceiling in R-precision. Flexer et al. [7] compare Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMM) describing spectral similarity of songs. It is shown that HMMs are capable of representing the underlying data better than GMMs, even if the GMM has more degrees of freedom. In a genre classification task, both methods show very similar results. 3 FEATURE MODELLING We calculate song similarity on 15 dimensional MFCC vectors (without the 0th coefficient), modelled with a Gaussian Mixture Model: p (x, Θ) = k X i=1 αiG(x, µ, ΣX) (1) with x a single feature vector and Θ the model parameters: cluster mean µ and cluster covariance Σ. The mixture weights αi are nonnegative and add up to one.",
        "zenodo_id": 1415506,
        "dblp_key": "conf/ismir/Balkema07",
        "keywords": [
            "Gaussian mixture model",
            "hub occurrences",
            "genre classification",
            "computational complexity",
            "variable size mixture models",
            "MDL-based method",
            "strong correlation",
            "Estimation Maximization algorithm",
            "audio similarity",
            "timbral and temporal features"
        ],
        "content": "V ARIABLE-SIZE GAUSSIAN MIXTURE MODELS FOR MUSIC\nSIMILARITY MEASURES\nWietse Balkema\nRobert Bosch GmbH - Corporate Research\nPo. Box 77 77 77\n31137 Hildesheim, Germany\nABSTRACT\nAn algorithm to efﬁciently determine an appropriate num-\nber of components for a Gaussian mixture model is pre-\nsented. For determining the optimal model complexity\nwe do not use a classical iterative procedure, but use the\nstrong correlation between a simple clustering method\n(BSAS [13]) and an MDL-based method [6]. This ap-\nproach is computationally efﬁcient and prevents the model\nfrom representing statistically irrelevant data.\nThe performance of these variable size mixture mod-\nels is evaluated with respect to hub occurrences, genre\nclassiﬁcation and computational complexity. Our variable\nsize modelling approach marginally reduces the number\nof hubs, yields 3-4% better genre classiﬁcation precision\nand is approximately 40% less computationally expen-\nsive.\n1 INTRODUCTION\nIn the current boom of web 2.0, the market for music\nrecommender systems seems to have taken off. Last.fm,\niLike, myStrands and others analyze a user’s listening be-\nhaviour and compare it with other user’s proﬁles. Music\ncan be tagged with personal tags which allows new ways\nto explore music collections.\nOne of the major problems of these community based\nrecommender systems is their robustness in new databases\nand dealing with underrepresented data. Once a song is\nchosen as favourite, a loop mechanism can keep this song\nas favourite for a long time. Community based recom-\nmenders can be sensitive to attacks that try to inﬂuence\na speciﬁc song’s rating. Two kinds of attack strategies\nareshilling (promoting an item) and nuking (demoting an\nitem) [8].\nAnother approach for recommender systems that do\nnot suffer from loops, shilling or nuking is content-based\nrecommendation. The acoustical content of a song is ana-\nlyzed, and songs found to be ‘similar’ to songs a user likes\nare recommended. Audio similarity is multifaceted, so a\ncommon approach to evaluate audio similarity measures is\nto perform a genre classiﬁcation task. Pampalk et al. [11]\nc/circlecopyrt2007 Austrian Computer Society (OCG).found that a combination of 70% timbral and 30% tempo-\nral features provide a good audio similarity measure.\nHubs, songs that are found to be very similar to a very\nlarge number of other songs, are a major problem for\naudio-based music recommender systems. Aucouturier\nand Pachet [3] showed that in a purely timbre-based near-\nest neighbor retrieval system, the number of hubs signiﬁ-\ncantly increases when discarding the 5% least signiﬁcant\nclusters from a Gaussian mixture model.\nThe computational complexity for calculating the dis-\ntance between Gaussian mixture models scales linearly\nwith the number of clusters in a mixture model for most\ndistance measures. Reducing the number of clusters in a\nmodel thus has great impact in computational complexity,\nbut inﬂuences performance.\nWe present a method to reduce the number of mixture\ncomponents without sacriﬁcing retrieval performance.\nThe required number of Gaussians in a Gaussian mixture\nmodel is estimated for each song individually. The num-\nber of clusters is then used by the Estimation Maximiza-\ntion algorithm to model the song data. Using individual\nsong complexity estimation prevents overﬁtted models on\n‘simple’ songs with too complex models, while still offer-\ning ‘complex’ songs an adequate model complexity.\nThe remainder of this paper is organized as follows:\nIn section 2 we present a short overview of related work.\nSection 3 describes our feature modelling approach in de-\ntail. This section is followed by a performance analysis\nwith respect to the effect of our algorithm on hubs and on\na genre classiﬁcation task. In our last section we summa-\nrize our results and give some recommendations for fur-\nther research.\n2 RELATED WORK\nBerenzweig et al. [4] compare anchor space based and\nGMM based similarity measures with a similarity matrix\nretrieved from a user survey. The anchor space method\nperforms very similarly to the GMM method.\nAucouturier and Pachet [2] systematically explore fea-\nture parameter space for timbre similarity experiments\nand evaluate performance with a nearest neighbour re-\ntrieval task. Optimal R-precision was found with 20 di-\nmensional MFCCs and a Gaussian mixture model with 50\ncomponents. The number of model components howeveris of less inﬂuence than the number of feature dimensions.\nTheir conclusion is that ‘Everything performs the same’\nand that there seems to be a glass ceiling in R-precision.\nFlexer et al. [7] compare Hidden Markov Models\n(HMMs) with Gaussian Mixture Models (GMM) describ-\ning spectral similarity of songs. It is shown that HMMs\nare capable of representing the underlying data better than\nGMMs, even if the GMM has more degrees of freedom. In\na genre classiﬁcation task, both methods show very simi-\nlar results.\n3 FEATURE MODELLING\nWe calculate song similarity on 15 dimensional MFCC\nvectors (without the 0thcoefﬁcient), modelled with a\nGaussian Mixture Model:\np(x,Θ) =k/summationdisplay\ni=1αiG(x,µ,ΣX) (1)\nwithxa single feature vector and Θthe model parame-\nters: cluster mean µand cluster covariance Σ. The mix-\nture weights αiare nonnegative and add up to one.\n3.1 Parameter Estimation\nWhen the number of components in a mixture is known in\nadvance, the Expectation Maximization (EM) algorithm\n[5] provides an efﬁcient method to estimate the parameters\nof the distribution of ndata samples X={x1, . . .,xn}.\nThe EM algorithm is an iterative procedure and is guar-\nanteed to converge to a local maximum of the maximum\n(log-)likelihood estimate of the mixture parameters:\nˆΘML= argmax\nΘ(logp(X|Θ)) (2)\nEach iteration consists of two steps:\n•E-step: Assign each sample to the mixture compo-\nnent that is most likely to have generated the sam-\nple, based on the current estimate of the model pa-\nrameters.\n•M-step: Recompute the model parameters based on\nthe current sample membership estimation.\nThese steps are repeated until the likelihood estimate con-\nverges.\n3.2 Complexity Estimation\nDuring the training phase of the EM algorithm, the num-\nber of mixture components remains constant, even if the\nmodel over- or underﬁts the data. When listening to var-\nious kinds of music, it is clear that there are broad varia-\ntions in musical structure and sound. M¨ orchen et al. [9]\nrecognized this issue on a genre level and used different\nfeature sets for each genre for determining genre likeli-\nhood.Pampalk [10] allows variable-size models for each in-\ndividual song. A k-means model is ﬁtted to the song fea-\ntures and a minimal distance between clusters is deﬁned.\nWhen two cluster centers are within this minimal distance,\nthey are merged.\nWe introduce a similar approach to Pampalk, and use it\nto generate gaussian mixture models.\n3.2.1 Optimal Models\nModel selection algorithms try to ﬁnd the number of com-\nponents k, that minimize the cost function C(ˆΘ(k), k):\nˆk= arg min\nk{C(ˆΘ(k), k)}, k=kmin, . . ., k max (3)\nThe cost function C(ˆΘ(k), k)consists of two parts:\n•A part expressing the goodness of ﬁt of a model\nwithkcomponents. This function is a monotoni-\ncally increasing function of k.\n•A part penalizing models with higher k.\nFigueiredo and Jain [6] presented an algorithm that op-\ntimizes a cost function based on the Minimum Description\nLength (MDL) criterion. This criterion is based on the as-\nsumption that if one can describe some observed data with\na short code, one has a good model of the source generat-\ning the data. Other algorithms optimizing a cost function\nlike in Equation 3 exist (eg. [12], based on the Bayesian\nInformation Criterion), but have not been investigated.\nFigueiredo uses a modiﬁed version of the EM algo-\nrithm for ﬁtting a GMM in the dataset. The algorithm\nstarts with a high number of components and eliminates\ncomponents of the mixture in the M-step.\nfori= 1, . . ., k :\nˆαi(t+ 1) =max/braceleftBig\n0,/parenleftBig/summationtextn\nj=1w(j)\ni/parenrightBig\n−N\n2/bracerightBig\n/summationtextk\ni=1max/braceleftBig\n0,/parenleftBig/summationtextn\nj=1w(j)\ni/parenrightBig\n−N\n2/bracerightBig(4)\nwhere w(j)\niis the conditional expectation that sample j\nbelongs to mixture component i. When the EM algo-\nrithm is converged and the number of components is still\nlarger than kmin, the component with the smallest sup-\nport is forced to zero. This procedure is repeated until\nk=kmin.\n3.2.2 Optimal Model Approximation\nAs a consequence of using EM to iterate through the vari-\nous model sizes, Figueiredo’s algorithm is very slow. We\nfound that the number of clusters found by the much sim-\npler ‘Basic Sequential Algorithmic Scheme’ (BSAS, [13])\nshows high correlation with the number of clusters as\nfound by Figueiredo. This algorithm only takes two pa-\nrameters: the threshold θfor determining whether a new\ncluster has to be formed, and NmaxClust , the maximum\nnumber of clusters to be formed.\nThe basic algorithm in pseudocode consists of the fol-\nlowing steps:1:Nclust= 1\n2:C1={x1}\n3:forj= 2tondo\n4: ﬁndCi:d(xj, Ci) = min ∀kd(xj, Ck)\n5: if(d(xj, Ck)> θ) and ( Nclust< NmaxClust )then\n6: Nclust=Nclust+ 1\n7: CNclust={xj}\n8: else\n9: Ck=Ck∪ {xj}\n10: end if\n11:end for\nThis algorithm was modiﬁed to accept new clusters even\nif the maximum number of clusters has already been\nreached, but only if there is a cluster that was assigned\nless than 1% of the data. This smallest cluster is then dis-\ncarded and replaced by the new cluster. After the algo-\nrithm has ﬁnished, all clusters containing less than 1% of\nthe data are discarded. The cluster centers found by BSAS\nare used as input for an EM algorithm to ﬁt a GMM in the\ndata. The EM algorithm uses all samples, including those\nin the clusters that were discarded in the BSAS algorithm.\nInitializing the EM process with the clustering result of\nBSAS signiﬁcantly decreases the number of iterations the\nEM algorithm needs to converge when we compare it with\nmethods that discard insigniﬁcant clusters in the training\nphase of the algorithm.\nWe compared the number of components found by\nFigueiredo with that of BSAS, on a subset of 234 songs\nfrom the Magnatune dataset. This subset covers all mu-\nsic genres available in the Magnatune data. The Pear-\nson’s correlation coefﬁcient between the number of clus-\nters found by both algorithms is 0.78.\n4 EV ALUATION\nWe selected a subset of 331 songs from the Magnatune\ndataset, covering six genres. This dataset is modelled both\nwith ﬁxed size GMMs with 20 clusters and with variable\nsize GMMs with a maximum 30 clusters. The number\nof clusters in the variable size model case is determined\nby the BSAS algorithm as presented in section 3.2.2. The\nmean number of clusters over our dataset was 15. The EM\nmodelling complexity scales approximately linearly with\nthe number of clusters, we therefore obtained a 25% com-\nputing time gain for the variable size models. We use the\nEarth Mover’s Distance to determine the distance between\nthe GMMs [4]. Computation of the full song similarity\nmatrix was approximately 40% faster for the variable size\nmodels.\n4.1 Hub-analysis\nRobustness of music similarity measures can be evalu-\nated by means of a hub-analysis. Aucouturier [3] uses\ntwo methods to assess the hubness of various algorithms:\nN-occurrences andNeighbour Angle . In this subsection\nwe evaluate the hubness of our dataset, modelled withthe variable and the ﬁxed size models, using the N-\noccurrence measure.\n4.1.1 N-occurrences\nTheN-occurrence measure counts the number of times a\nsong occurs within the Nnearest neighbours of all songs\nin a data set. The measure is a constant-sum measure:\nthe mean N-occurrence is equal to N. When studying\nhubs, we are interested in the amount of songs that occur\nmuch more frequently in the Nnearest neighbours than\nthe expected average value. In Figure 1 we show the N-\noccurrence histograms for N= 50 . The number of songs\nthat occur more than 150 times differs only marginally be-\ntween the two model types: 11 for the variable size models\nand 12 for the ﬁxed size models. The use of variable size\nmodels thus seems to have small positive impact on hub\noccurrences.\nAucouturier [3] showed that discarding statistically ir-\nrelevant clusters ( homogenization ) caused a dramatic in-\ncrease in the number of hubs. With this experiment we\nshowed that reducing the number of mixture components\ncan be done without having negative inﬂuence on the\nnumber of hubs. Apparently, not all songs require the\nsame number of mixture components.\n4.2 Genre classiﬁcation\nThe most common evaluation procedure for music simi-\nlarity measures is genre classiﬁcation. Since we are only\ninterested in the comparison between ﬁxed- and variable\nsize models, we do not apply an artist-ﬁlter as has been\nsuggested by Pampalk et al. in [11].\nAucouturier and Pachet [1] dispute the use of genre\nclassiﬁcation for evaluating timbre similarity. Differen t\nartists within one single genre may have a very broad ‘tim-\nbral’ spectrum. Our data set only contains very few artists\nper genre. As a consequence of this, and under the as-\nsumption that each single artist only uses a narrow timbral\nspectrum, we can generalize genre distance to timbral dis-\ntance.\n4.2.1 Classiﬁcation results\nWe use a simple k-nearest neighbour classiﬁer and clas-\nsify with a leave one out cross validation procedure. In\nFigure 2(a) we depict the classiﬁcation accuracy for a\nrange of k, for variable-size models with 15 Gaussians on\naverage and for ﬁxed-size models with 20 Gaussians. We\nsee that the variable-size models consequently outperform\n0 100 200 300050\nN# occurrences\n(a) Fixed size models, 20 Gaus-\nsians0 100 200 300050\nN# occurrences\n(b) Variable size models\nFigure 1 .N-occurrence analysisthe ﬁxed-size models, even with an average lower model\ncomplexity.\n0 5 10 150.760.780.80.820.840.860.88\n  \nvarG\n20G\nkClassiﬁcation accuracy\n(a)kvs accuracy\nelectronic\nclassical\njazz/blues\nfolk\nhardrock\npop/rock\n(b) Variable size  \nelectronic\nclassical\njazz/blues\nfolk\nhardrock\npop/rock\n(c) 20 Gaussians\nFigure 2 . Genre classiﬁcation performance\n4.2.2 Inter- and intra genre distance\nAucouturier and Pachet [1] use the mean distance between\nsongs within a single genre and between different genres\nto express the limited use of genre classiﬁcation for tim-\nbral similarity evaluations. Artists within a certain genr e\nwithout a ‘coherent’ sound make it difﬁcult to ﬁnd a direct\nrelationship between timbre similarity and genre similar-\nity.\nOur database consists of few artists per genre and all\nhave a ‘coherent’ sound. We can thus use the measure to\ncompare timbre discrimination performance of the ﬁxed\nsize Gaussian models with the variable size models to\neach other. Both for the 20 Gaussians and the variable size\nmodels we ﬁnd a ratio of 1 : 1.32. Although the variable\nsize models have a lower mean number of components,\nthe timbre information seems to be captured just as well\nas by the more complex ﬁxed size models.\n5 CONCLUSIONS\nIn this paper we presented an algorithm to estimate an op-\ntimal number of cluster components for each individual\nsong. We compared the number of hub occurrences be-\ntween a 20-Gaussian model and our variable size mod-\nelling approach with 15 clusters on average. Our variable\nsize modelling approach marginally reduces the number\nof hubs.\nWe analyzed the timbral discrimination performance of\nour measure with a genre classiﬁcation task on a small\ndatabase with homogenous genres. Variable size models\noutperformed ﬁxed size models with respect to genre clas-\nsiﬁcation by 3-4% and shows the same mean inter- to intra\ngenre distance ratio at average lower model complexity.Computation of a full song distance matrix using the\nEarth Mover’s Distance is approximately 40% faster for\nthe variable size models.\nReferences\n[1] J. J. Aucouturier and F. Pachet. Music similarity\nmeasures: What ’s the use? In Proc. ISMIR 02 ,\n2002.\n[2] J. J. Aucouturier and F. Pachet. Improving timbre\nsimilarity: How high’s the sky? Journal of Negative\nResults in Speech and Audio Sciences , 2004.\n[3] J. J. Aucouturier and F. Pachet. A scale-free distri-\nbution of false positives for a large class of audio\nsimilarity measures. Pattern Recognition , 2007.\n[4] A. Berenzweig, B. T. Logan, D. P. W. Ellis, and\nB. Whitman. A large-scale evaluation of acoustic\nand subjective music similarity measures. In Proc.\nISMIR 03 , 2003.\n[5] A. P. Dempster, D. B. Rubin, and N. Laird. Maxi-\nmum likelihood from incomplete data via the em al-\ngorithm. J.Royal Statistical Soc., Series B (Method-\nological) , 1977.\n[6] M. A. T. Figueiredo and A. K. Jain. Unsupervised\nlearning of ﬁnite mixture models. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n2002.\n[7] A. Flexer, E. Pampalk, and G. Widmer. Hidden\nmarkov models for spectral similarity of songs. In\nProc. DAFx 05 , 2005.\n[8] B. Mobasher, R. Burke, C. Williams, and R. Bhau-\nmik. Analysis and detection of segment-focused at-\ntacks against collaborative recommendation. In WE-\nBKDD , volume 4198 of Lecture Notes in Computer\nScience , 2005.\n[9] F. M¨ orchen, I. Mierswa, and A. Ultsch. Understand-\nable models of music collections based on exhaus-\ntive feature generation with temporal statistics. In\nProc. 12th ACM SIGKDD Int. Conf. on Knowledge\nDiscovery and Data Mining , 2006.\n[10] E. Pampalk. Speeding up music similarity. Technical\nreport, 2005.\n[11] E. Pampalk, A. Flexer, and G. Widmer. Improve-\nments of audio-based music similarity and genre\nclassiﬁcation. In Proc. ISMIR 05 , 2005.\n[12] D. Pelleg and A. W. Moore. X-means: Extending\nk-means with efﬁcient estimation of the number of\nclusters. In Proc. 7th Int. Conf. on Machine Learn-\ning, 2000.\n[13] S. Theodoridis and K. Koutroumbas. Pattern Recog-\nnition . Elsevier Academic Press, second edition,\n2003."
    },
    {
        "title": "High Time-Resolution Estimation of Multiple Fundamental Frequencies.",
        "author": [
            "Jayme Garcia Arnal Barbedo",
            "Amauri Lopes",
            "Patrick J. Wolfe"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415972",
        "url": "https://doi.org/10.5281/zenodo.1415972",
        "ee": "https://zenodo.org/records/1415972/files/BarbedoLW07.pdf",
        "abstract": "This paper presents a high time-resolution strategy to estimate multiple fundamental frequencies in musical signals. The signal is first divided into overlapping blocks, and a high-resolution estimate made of the short-term spectrum. The resulting spectrum is modified such that only the most relevant spectral components are considered, and an iterative algorithm based on earlier work by Klapuri is used to identify candidate fundamental frequencies. Finally, a context-based rule is used to improve the accuracy of fundamental frequency estimates. The performance of this technique is investigated under both noiseless and noisy conditions, and its accuracy is examined in cases where the polyphony is known and unknown a priori. 1 INTRODUCTION The estimation of fundamental frequencies (F0) of mixtures of several sound sources is a problem whose solution can benefit several areas of digital audio processing, including automatic music transcription, music information retrieval, and sound source separation, among others. Early work relating to this problem aimed to solve the problem of transcribing polyphonic music [1, 2]; however, such methods worked well only under very restrictive constraints. A new phase in multiple-F0 estimation started with the work of Meddis and Hewitt [3], which has provided the foundation for most approaches used in more recent methods. Cheveign´e explored the model proposed in [3] to develop an iterative procedure in which the sound component corresponding to a particular estimated F0 is removed, and a new round of F0 estimation then proceeds using the residual [4]. Tolonen and Karjalainen simplified the approach of [3] to create a strategy reported to be both accurate and computationally efficient [5], and statistical inference was used by Davy and Godsill to estimate c⃝2007 Austrian Computer Society (OCG). multiple fundamental frequencies [6]. Klapuri proposed a method based in the harmonicity and spectral smoothness of the signals [7], as well as a more recent perceptually motivated strategy that uses an iterative estimationcancellation approach [8]. Like other approaches, the method proposed here begins with the division of the musical signal into overlapping short-time frames. The high-resolution spectral estimation proposed in [9] is then applied to each frame in turn, in order to allow a finer analysis of the spectral structure of the signal. The resulting spectrum is modified in such a way only the most relevant frequency components are considered. Additionally, the remaining components are quantized into only two levels, making the data more homogeneous. The modified spectrum is then analyzed using an iterative algorithm based on the procedure presented in [8], but which also introduces some further processing intended to refine the selection of the correct F0. If the polyphony is known a priori, the iterations stop as soon as the number of sound sources has been reached. If the polyphony is unknown, the iterations are interrupted if one out a set of rules is fulfilled. After the fundamental frequencies have been determined for all frames, a further procedure is applied to improve the estimates. All frames contained in a segment between two events 1 are analyzed, and the estimates are all made the same according to majority rules. In the cases where the polyphony is known a priori, this last procedure only changes the values of F0, but in cases where the polyphony is unknown at the outset, there can be changes in the estimated number of sound sources. Those context-corrected F0 estimates comprise the output of the algorithm. The remainder of this paper describes this algorithm and its application in more detail, and is organized as follows. Section 2 presents a description of the algorithm. The analysis of the results is presented in Section 3, and Section 4 presents the conclusions and final remarks. 1 An event, in the context of this work, is any change in the number of sound sources and/or fundamental frequencies present in the signal. Division into 46.4 ms Frames Input Signal High-Resolution Modified Spectrum Iterative Determination of Multiple F0 Context-Based Correction Final F0 Figure 1. Block diagram of the estimation strategy 2 ALGORITHM DESCRIPTION Figure 1 shows the basic structure of the method. As can be seen, there are four basic procedures, which will be described in the following.",
        "zenodo_id": 1415972,
        "dblp_key": "conf/ismir/BarbedoLW07",
        "keywords": [
            "fundamental frequencies",
            "multiple fundamental frequencies",
            "musical signals",
            "high time-resolution strategy",
            "polyphony",
            "overlap",
            "short-time frames",
            "high-resolution spectral estimation",
            "context-based rule",
            "context-corrected F0"
        ],
        "content": "HIGH TIME-RESOLUTION ESTIMATION OF MULTIPLE\nFUNDAMENTAL FREQUENCIES\nJayme Garcia Arnal Barbedo1,2, Amauri Lopes2, and Patrick J. Wolfe1\n1School of Engineering and Applied Sciences, Harvard University\n33 Oxford Street, Cambridge, MA 02138-2901, USA\n{jbarbedo, patrick }@seas.harvard.edu\n2School of Electrical and Computer Engineering, State University of Campinas\nCidade Universit ´aria Zeferino Vaz, C.P. 6101, CEP: 13083-970, Campinas, SP, Brazil\n{jgab, amauri }@decom.fee.unicamp.br\nABSTRACT\nThis paper presents a high time-resolution strategy to esti-\nmate multiple fundamental frequencies in musical signals.\nThe signal is ﬁrst divided into overlapping blocks, and a\nhigh-resolution estimate made of the short-term spectrum.\nThe resulting spectrum is modiﬁed such that only the most\nrelevant spectral components are considered, and an iter-ative algorithm based on earlier work by Klapuri is used\nto identify candidate fundamental frequencies. Finally, a\ncontext-based rule is used to improve the accuracy of fun-\ndamental frequency estimates. The performance of this\ntechnique is investigated under both noiseless and noisy\nconditions, and its accuracy is examined in cases wherethe polyphony is known and unknown a priori.\n1 INTRODUCTION\nThe estimation of fundamental frequencies (F0) of mix-\ntures of several sound sources is a problem whose solu-\ntion can beneﬁt several areas of digital audio processing,\nincluding automatic music transcription, music informa-tion retrieval, and sound source separation, among others.\nEarly work relating to this problem aimed to solve the\nproblem of transcribing polyphonic music [1, 2]; however,such methods worked well only under very restrictive con-\nstraints. A new phase in multiple-F0 estimation started\nwith the work of Meddis and Hewitt [3], which has pro-vided the foundation for most approaches used in more\nrecent methods. Cheveign ´e explored the model proposed\nin [3] to develop an iterative procedure in which the soundcomponent corresponding to a particular estimated F0 is\nremoved, and a new round of F0 estimation then proceeds\nusing the residual [4]. Tolonen and Karjalainen simpli-ﬁed the approach of [3] to create a strategy reported to be\nboth accurate and computationally efﬁcient [5], and statis-\ntical inference was used by Davy and Godsill to estimate\nc/circlecopyrt2007 Austrian Computer Society (OCG).multiple fundamental frequencies [6]. Klapuri proposed\na method based in the harmonicity and spectral smooth-\nness of the signals [7], as well as a more recent percep-tually motivated strategy that uses an iterative estimation-\ncancellation approach [8].\nLike other approaches, the method proposed here be-\ngins with the division of the musical signal into overlap-\nping short-time frames. The high-resolution spectral es-\ntimation proposed in [9] is then applied to each frame inturn, in order to allow a ﬁner analysis of the spectral struc-\nture of the signal. The resulting spectrum is modiﬁed in\nsuch a way only the most relevant frequency componentsare considered. Additionally, the remaining components\nare quantized into only two levels, making the data more\nhomogeneous. The modiﬁed spectrum is then analyzedusing an iterative algorithm based on the procedure pre-\nsented in [8], but which also introduces some further pro-\ncessing intended to reﬁne the selection of the correct F0.If the polyphony is known a priori, the iterations stop as\nsoon as the number of sound sources has been reached. If\nthe polyphony is unknown, the iterations are interruptedif one out a set of rules is fulﬁlled. After the fundamental\nfrequencies have been determined for all frames, a fur-\nther procedure is applied to improve the estimates. All\nframes contained in a segment between two events\n1are\nanalyzed, and the estimates are all made the same accord-ing to majority rules. In the cases where the polyphony is\nknown a priori, this last procedure only changes the val-\nues of F0, but in cases where the polyphony is unknown atthe outset, there can be changes in the estimated number\nof sound sources. Those context-corrected F0 estimates\ncomprise the output of the algorithm.\nThe remainder of this paper describes this algorithm\nand its application in more detail, and is organized as fol-\nlows. Section 2 presents a description of the algorithm.\nThe analysis of the results is presented in Section 3, and\nSection 4 presents the conclusions and ﬁnal remarks.\n1An event, in the context of this work, is any change in the number\nof sound sources and/or fundamental frequencies present in the signal.Division into 46.4 \nms FramesInput\nSignalHigh-Resolution\nModified \nSpectrumIterative \nDetermination of \nMultiple F0Context-Based\nCorrectionFinal F0\nFigure 1 . Block diagram of the estimation strategy\n2 ALGORITHM DESCRIPTION\nFigure 1 shows the basic structure of the method. As can\nbe seen, there are four basic procedures, which will bedescribed in the following.\n2.1 Division into Frames\nThe ﬁrst step of the algorithm is the division of the signal\ninto frames. Assuming a sample rate f\ns= 44100 kHz,\nthe length of the frames is taken to be 2048 samples, cor-\nresponding to 46.4ms. This value was chosen in order\nto provide a good temporal resolution to the analysis—thereby minimizing, as much as possible, the estimation\nproblems that can arise when a new event occurs in the\nmiddle of a frame. (This also provides better estimateswhen new events occur in short time intervals.) In spec-\ntral analysis, better time resolution comes at the expense\nof worse frequency resolution. To minimize this prob-lem, a high-resolution estimate of the spectrum was em-\nployed [9], as described in Section 2.2 below.\n2.2 Computing High-Resolution Modiﬁed Spectrum\nThe tight compromise between time and frequency reso-\nlutions in spectral analysis has motivated the use of an al-\ngorithm that makes a high-resolution estimate of the spec-\ntrum [9]. The Matlab code and more details can be foundin [10]. More speciﬁcally, a spectrum with 44100 points\n(i.e.,1Hz resolution) is estimated based on a frame length\nof2048 samples. The resultant estimates are reasonably\naccurate, and were observed to provide much better results\nthan using the regular Discrete Fourier Transform (DFT).\nBesides providing a high-resolution spectral estimate,\nthe method of [9] also reduces greatly the amplitude of\nsidelobes that appear in the computation of regular DFT.This is achieved by extrapolating the data, and properly\nexploring the autocorrelation function of the extended data.\n(The mathematical details of the procedure can be foundin [9].) The only major shortcoming of this strategy is\nthe computational burden, which is much higher than that\nrequired by the Fast Fourier Transform (FFT) algorithm.\nThe high-resolution spectrum is then modiﬁed to con-\nsider only relevant components. First, all spectral compo-\nnents that do not represent a local magnitude peak are setto zero. The component with greatest magnitude is identi-\nﬁed and taken as a reference value X\nm. Then, the follow-\ning assignment is applied for k∈{1,2,...,⌊fs/2⌋}:\n⎧\n⎪⎨\n⎪⎩Sm[k]=0 X[k]<0.01Xm\nSm[k] = 100 0 .01Xm≤X[k]<0.1Xm\nSm[k] = 1000 X[k]≥0.1Xm,where Xrepresents the magnitude spectrum, kis the fre-\nquency index, and Xmis the value of the component of\nlargest magnitude.\nThe resulting modiﬁed spectrum Smis then used to\ndetermine the fundamental frequencies present, accordingto the procedures described in Section 2.3 below.\n2.3 Iterative Determination of Multiple F0\nPart of the iterative procedure to determine the fundamen-\ntal frequencies was based on part of the strategy presentedin [8]. This procedure is performed according to the fol-\nlowing steps:\n1. A residual spectrum S\nris initialized and set equal\nto the modiﬁed spectrum Sm.\n2. A simpliﬁed version of the fundamental period esti-\nmation presented in [8] is applied, given by\nλ(τ)=/parenleftbiggfs√τ/parenrightbiggτ/2/summationdisplay\nj=1max\nk∈kj,τ(Sr(k)), (1)\nwhere fsis the sampling frequency, τis the candidate fun-\ndamental period, and kj,τis a set that deﬁnes a range of\nfrequency bins in the neighborhood of the harmonic par-\ntials of the k/primeth F0 candidate, given by\nkj,τ=/bracketleftbigkmin\nj,τ···k/prime···kmax\nj,τ/bracketrightbig\n,\nwhere\nkmin\nj,τ=⌊jK/(τ+1 )⌋+1\nkmax\nj,τ=m a x/parenleftbig\n⌊jK/(τ−1)⌋+1,kmin\nj,τ/parenrightbig\nandK=fs= 44100 is the total number of spectral\nbins of the modiﬁed spectrum. As can be seen, there aresome differences between the procedure adopted here and\nthat described in [8]. Particularly, the weighting factor\nthat simulates the bandwidth of the auditory ﬁlters wasnot used here, nor the balancing operation over λ. This\nis because such operations were not seen to improve the\nresults of the method proposed here. Additionally,√\nτis\nused in (1), instead of τitself; this decision was taken as\na result of optimization tests performed with the methodover large databases, as described in Section 3 below.\n3. The candidate fundamental frequency is given by\nf\nc=fs/max (λ).\n4. In the next step, the partials corresponding to fcare\nremoved from the residual spectrum Sraccording to\nSr(Pm)=0 ,where for m∈{1,2,...,⌊fs\n2fc⌋},Pmis deﬁned as\nPm=a r g m a x\nn∈N∩[α1,α2]Sr(n);\nα1=⌊(0.975−a)fc⌋,α 2=⌊(0.975 + a)fc⌋,\na=m a x/parenleftBig\n(˜fc−1),0/parenrightBig\n·10(˜fc−25)/5,\nwith˜fcthe candidate F0 in kHz. As can be seen, the in-\nterval around each partial grows for large frequencies in\norder to account for deviations caused by inharmonicity.\n5. If the polyphony is known a priori, Steps 1–4 are\nrepeated until the number of estimated fundamentals co-\nincides with the number of concurrent sounds. Otherwise,some stopping criteria must be applied. In particular, if at\niteration ithe stopping criteria\nmax/parenleftbig\nS\ni\nr/parenrightbig\n≤100 and/summationdisplay\nkSi\nr(k)<200\nare met, then the candidate fundamental fc(i)will be ac-\ncepted and the algorithm will proceed to Step 6 below.\nAlternatively, deﬁne at iteration ithe criteria\nfc(i)≥500and/summationdisplay\nkSi−1\nr(k)−/summationdisplay\nkSi\nr(k)<200\nfc(i)<500and/summationdisplay\nkSi−1\nr(k)−/summationdisplay\nkSi\nr(k)<300,\nwhere Si−1\nrandSi−1\nrare, respectively, the residual spec-\ntra before and after removing the partials of the current F0\nestimate, and S0\nr=Sm. If either of these criteria are met,\nthen the candidate fc(i)will be rejected and the algorithm\nwill proceed to Step 6 below. Such rules interrupt the iter-\nations if there are too few signiﬁcant spectral components\nremaining, or if the current fundamental frequency was\nestimated using too few spectral components. The rules\nare tighter for low F0, because more signiﬁcant spectralcomponents are expected to be present in such situations.\n6. Finally, the ﬁrst estimated F0 is checked to verify\nthat it is the lowest of all estimated F0. If so, the estimatesresulting from Steps 1–5 are the ﬁnal output of this stage\nof the algorithm. If not, these procedures are repeated,\nthis time forcing the lowest detected frequency to be theﬁrst to be considered in the iterative process. This method\nis employed as in many cases an overtone partial of the\nactual F0 is taken as the estimated F0, leading to an esti-\nmation error. In that case, the correct F0 is normally also\ndetected as a potential F0 in a subsequent iteration. More-\nover, in such cases it was often observed that the correct\nF0 would be the lowest among all estimated F0. Forcing\nthe lowest partial to be considered ﬁrst greatly reduces thefrequency of this problem.\n2.4 Context-Based Correction\nThe last stage of the algorithm is a context-based cor-\nrection of the estimated F0. The homogeneous segmentsbetween two events normally will contain more than onePoly- Context Correction Klapuri Method [8]\nphony Without With 46ms 92ms\n1 1.5 1.1 7.2 2.1\n2 5.2 3.9 12.0 7.0\n3 7.5 6.0 21.3 10.2\n4 13.0 9.9 26.9 12.8\n5 19.8 15.3 35.5 17.1\n6 29.0 24.2 42.4 21.3\nTable 1 . Estimation error (%) with known polyphony\nframe and, since it is expected that all frames of the seg-\nment present the same results, a procedure to homogenize\nthe estimates is applied. If the polyphony Nis known a\npriori, the set of estimated F0 for all frames in the segment\nwill comprise the Nfundamental frequencies that appear\nmost frequently among these frames. If the polyphony is\nunknown, then a given F0 is included in the set of esti-\nmates of all frames in the segment only if it appears inat least 50% of the frames; otherwise it is removed. This\nsimple procedure improves the overall algorithm perfor-\nmance by about 25%, as will be seen in the next section.\n3 RESULTS\nThe database used for testing is composed of 1200 mix-\ntures of one to six concurrent sounds, taken from both the\nRWC Music Instrument Sound database [11] and from the\nUniversity of Iowa Musical Instrument Samples database(http://theremin.music.uiowa.edu/MIS.html). These mix-\ntures have lengths between 0.05and1second, with an\naverage of about 250ms. Sounds from 30instruments\nwere used, and each mixture was the result of a random se-\nlection among all instruments and respective note ranges.\nCalibration was performed using 200mixtures, and the\nremaining 1000 signals were used in the tests.\nTable 1 shows the percentage of mis-estimates obtained\nfor the method in the case where the polyphony is known,\nwith and without the context correction. The results arecompared with the 46-ms and 92-ms frame versions of\nthe method proposed by Klapuri, implemented according\nto the guidelines presented in [8]. As can be seen in Ta-ble 1, the strategy performs very well for few concurrent\nsounds, and the performance begins to degrade rapidly\nwhen more sounds are present. This is due, ﬁrstly, tothe tendency of any method to lose reliability when too\nmany spectral components are present. Additionally, the\nelimination of spectral components considered irrelevant(see Section 2.2) sometimes removes partials of actual F0,\nleading to an error. However, the method performs better\nwith this “clean” spectrum than without any component\nselection. Table 1 also reveals that the context-based cor-\nrection reduces errors by approximately 25%. It is impor-tant to note that the effects of correction are more effective\nwhen the segment between events is longer; as described\nabove, the results of Table 1 were obtained for segmentswith lengths between 50ms and 1s(250ms on average).Poly- Without Correction With Correction\nphony Corr. Miss False Corr. Miss False\n1 98.5 1.5 3.4 98.9 1.1 1.2\n2 94.1 5.9 6.6 95.6 4.4 4.2\n3 91.0 9.0 8.8 92.5 7.5 6.6\n4 84.7 15.3 12.3 87.1 12.9 10.3\n5 77.9 22.1 18.0 80.0 20.0 15.9\n6 68.1 31.9 23.4 71.8 28.2 23.0\nTable 2 . Estimation error (%) with unknown polyphony\nTable 2 shows the performance of the technique for the\ncase in which polyphony is unknown. Its columns show\nthe percentage, with respect to the number of actual F0, of\nthe correct estimates, missed F0, and incorrectly detectedF0. As can be seen, the results exhibit similar behavior to\nthat observed in Table 1, but since the polyphony is un-\nknown, the results are (as expected) slightly worse. It also\ncan be seen that there is a balance between the number of\nmissed F0, and false F0. The method was calibrated inthis way, but simple changes in the algorithm can change\nthe compromise between missed and false detections.\nThe results shown in Tables 1 and 2 were obtained from\nmixtures where the levels of the sounds were the same.\nIf the relative levels between the sounds change, the ac-curacy of the technique tends to be reduced. To test the\ninﬂuence of level in algorithm performance, all mixtures\nof three sounds were taken and each estimation procedure\nwas repeated, with the level of one of the sounds being\nreduced at a time. Figure 2 shows the percentage of F0\nmis-identiﬁcations in the level-reduced sound, as a func-tion of the reduction factor. As can be seen, the technique\nis quite robust to mild variations in the level of the sounds,\nbut it quickly begins to lose reliability if the target soundis more than 5 dB below the levels of the others.\n4 CONCLUSIONS\nThis paper presented a new method to estimate multiple\nfundamental frequencies of concurrent sounds. It uses a\nmodiﬁed spectrum as input to an iterative algorithm that\nestimates a candidate set of F0. A set of rules is applied to\nimprove these estimates, and an additional context-basedcorrection procedure provides the ﬁnal F0 estimates. The\nmethod performs well in cases where the polyphony is\nknown or unknown a priori, and is robust to mild differ-ences in the levels of the sounds. The main shortcoming of\nthe technique is its high computational complexity; future\nwork will search for solutions to this problem. New proce-\ndures and statistical models to replace current rule-based\nheuristics will be investigated, in an attempt to improveestimation performance and quantify F0 uncertainty.\nAcknowledgments: Special thanks are extended to FAPESP\nand Capes for supporting this work under Grants 04/08281-\n0 and 03/09858-6 (FAPESP), and Grant 2234/06-8 (Capes).0 −1 −5 −10 −15 −205101520253035404550\nRelative Level (dB)Error (%)Error Versus Relative Level\nFigure 2 . Estimation error (%) for varying relative level\n5 REFERENCES\n[1] J. A. Moorer, “On the transcription of musical sound by\ncomputer,” Comput. Mus. J. , vol. 1, no. 4, pp. 32–38, 1977.\n[2] C. Chafe and D. Jaffe, “Source separation and note iden-\ntiﬁcation in polyphonic music,” in Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process. , 1986, pp. 1289–1292.\n[3] R. Meddis and M. J. Hewitt, “Virtual pitch and phase sen-\nsitivity of a computer model of the auditory periphery I:\nPitch identiﬁcation,” J. Acoust. Soc. Am. , vol. 89, no. 6,\npp. 2866–2882, 1991.\n[4] A. de Cheveign ´e, “Separation of concurrent harmonic\nsounds: Fundamental frequency estimation and a time-\ndomain cancellation model of auditory processing,” J.\nAcoust. Soc. Am. , vol. 93, no. 6, pp. 3271–3290, 1993.\n[5] T. Tolonen and M. Karjalainen, “A computationally ef-\nﬁcient multipitch analysis model,” IEEE Trans. Speech\nAudio Process. , vol. 8, no. 11, pp. 708–716, 2000.\n[6] M. Davy, S. J. Godsill, and J. Idier, “Bayesian analysis of\nWestern tonal music,” J. Acoust. Soc. Am. , vol. 119, no. 4,\npp. 2498–2517, 2006.\n[7] A. P. Klapuri, “Multiple fundamental frequency estima-\ntion based on harmonicity and spectral smoothness,” IEEE\nTrans. Speech Audio Process. , vol. 11, no. 6, pp. 804–815,\n2003.\n[8] A. P. Klapuri, “A perceptually motivated multiple-F0 es-\ntimation method,” in Proc. IEEE Workshop Appl. Signal\nProcess. Audio Acoust. , 2005.\n[9] V. Y. Liepin’sh, “An algorithm for evaluating a discrete\nFourier transform for incomplete data,” Autom. Control\nComput. Sci. , vol. 30, no. 3, pp. 27–40, 1996.\n[10] V. Y. Liepin’sh, “Extended DFT,” Available:\nhttp://www.mathworks.com/matlabcentral/ﬁleexchange/\nloadFile.do?objectId=11020objectType=File\n[11] M. Goto, “Development of the RWC music database,” in\nProc. 18th Int. Congr. Acoust. , 2004, pp. 553–556."
    },
    {
        "title": "Audio-Based Cover Song Retrieval Using Approximate Chord Sequences: Testing Shifts, Gaps, Swaps and Beats.",
        "author": [
            "Juan Pablo Bello"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417911",
        "url": "https://doi.org/10.5281/zenodo.1417911",
        "ee": "https://zenodo.org/records/1417911/files/Bello07.pdf",
        "abstract": "This paper presents a variation on the theme of using string alignment for MIR in the context of cover song identification in audio collections. Here, the strings are derived from audio by means of HMM-based chord estimation. The characteristics of the cover-song ID problem and the nature of common chord estimation errors are carefully considered. As a result strategies are proposed and systematically evaluated for key shifting, the cost of gap insertions and character swaps in string alignment, and the use of a beat-synchronous feature set. Results support the view that string alignment, as a mechanism for audiobased retrieval, cannot be oblivious to the problems of robustly estimating musically-meaningful data from audio. 1 INTRODUCTION The term musical similarity can be used to imply a relationship between songs that goes beyond texture, genre or artist, and that is more akin to purely musicological comparisons between songs, e.g. in terms of their melody, harmony and/or rhythm. In this context, cover song identification in popular music can be seen as a good, albeit limited, test of the ability to model musical similarity. The task of identifying cover songs poses many difficulties for audio-based music retrieval since renditions are often quite different from the original in one or many attributes including instrumentation, key or genre to name a few. In this paper we propose an approach to cover song identification based on the use of string alignment for the scoring of approximate chord sequences. These sequences are extracted from audio using chroma features and hidden Markov Models [2]. They are approximate because chord estimation from audio is never 100% accurate. Song sequences in a collection are ranked according to the score of their alignment with a query sequence. The use of approximate string matching is favored as the sequential ordering of events in the signal is taken into account. We argue that in order to maximize retrieval results, one has to consider not only the key or tempo differences between cover song sequences, but also the ways in which these sequences approximate (or not) the songs they represent. c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417911,
        "dblp_key": "conf/ismir/Bello07",
        "keywords": [
            "cover song identification",
            "audio collections",
            "HMM-based chord estimation",
            "cover-song ID problem",
            "common chord estimation errors",
            "string alignment",
            "musical similarity",
            "cover song sequences",
            "chroma features",
            "hidden Markov Models"
        ],
        "content": "AUDIO-BASED COVER SONG RETRIEVAL USING APPROXIMATE\nCHORD SEQUENCES: TESTING SHIFTS, GAPS, SWAPS AND BEATS\nJuan Pablo Bello\nMusic Technology, New York University\njpbello@nyu.edu\nABSTRACT\nThis paper presents a variation on the theme of using string\nalignment for MIR in the context of cover song identiﬁ-\ncation in audio collections. Here, the strings are derived\nfrom audio by means of HMM-based chord estimation.\nThe characteristics of the cover-song ID problem and the\nnature of common chord estimation errors are carefully\nconsidered. As a result strategies are proposed and sys-\ntematically evaluated for key shifting, the cost of gap in-\nsertions and character swaps in string alignment, and the\nuse of a beat-synchronous feature set. Results support\nthe view that string alignment, as a mechanism for audio-\nbased retrieval, cannot be oblivious to the problems of ro-\nbustly estimating musically-meaningful data from audio.\n1 INTRODUCTION\nThe term musical similarity can be used to imply a rela-\ntionship between songs that goes beyond texture, genre or\nartist, and that is more akin to purely musicological com-\nparisons between songs, e.g. in terms of their melody,\nharmony and/or rhythm. In this context, cover song iden-\ntiﬁcation in popular music can be seen as a good, albeit\nlimited, test of the ability to model musical similarity.\nThe task of identifying cover songs poses many difﬁcul-\nties for audio-based music retrieval since renditions are\noften quite different from the original in one or many at-\ntributes including instrumentation, key or genre to name a\nfew.\nIn this paper we propose an approach to cover song iden-\ntiﬁcation based on the use of string alignment for the scor-\ning of approximate chord sequences. These sequences are\nextracted from audio using chroma features and hidden\nMarkov Models [2]. They are approximate because chord\nestimation from audio is never 100% accurate. Song se-\nquences in a collection are ranked according to the score\nof their alignment with a query sequence. The use of ap-\nproximate string matching is favored as the sequential or-\ndering of events in the signal is taken into account. We\nargue that in order to maximize retrieval results, one has\nto consider not only the key or tempo differences between\ncover song sequences, but also the ways in which these\nsequences approximate (or not) the songs they represent.\nc\r2007 Austrian Computer Society (OCG).1.1 Previous Work and Motivation\nThere is a long history of using approximate string match-\ning in Music Information Retrieval. A notable example in\nthe symbolic domain is the use of string alignment for the\ncharacterization of melodic similarity in both monophonic\nand polyphonic databases [9, 13]. This is unsurprising if\nwe consider that melodies are well posed to be character-\nized as sequences of symbols representing, for example,\npitches or intervals.\nThis reasoning is also behind early attempts to incorpo-\nrate audio into MIR systems in the context of Query by\nHumming (QBH). This problem is largely deﬁned as one\nof matching between a monophonic audio query and a\nsymbolic and often polyphonic database. The transfor-\nmation of signals into strings can be achieved using well-\nknown signal processing algorithms, thus sequence align-\nment is featured prominently in QBH research ([1] is a\nrecent example). However, audio-based analysis, even\nin the monophonic case, adds an extra layer of complex-\nity that is bound to negatively impact the performance of\nthese systems [12]. This is all the more acute for the case\nwhen polyphonic audio signals, the format in which most\nmusic is available, are used both as queries and as docu-\nments in the database. In [10] the cosine distance is used\nbetween the most repeated melodic fragments of songs,\nrepresented as key-invariant and beat-synchronous spec-\ntral lines, to measure pairwise similarity. This approach\nuses cover-song identiﬁcation as a test of melodic simi-\nlarity in audio collections. While showing great promise,\nit suffers from the great difﬁculties of robustly estimating\nmelody from complex signals.\nAlternatively, music similarity can be characterized by har-\nmonic, rather than melodic, content using so-called chroma\nfeatures, or pitch class proﬁles. In [11], a successful sys-\ntem is presented for the identiﬁcation of excerpts (10-30s)\nin orchestral music. The method relies on short-time statis-\ntics, quantization and resampling of chroma features in or-\nder to ﬁnd similar excerpts despite tempo variations. In [6]\na cover song ID system is proposed that cross-correlates\nbeat-synchronous chroma features to characterize pairwise\nsimilarity. Key invariance is achieved by performing all\n12 shifted versions of the cross-correlation. This approach\nperformed best on the 2006 MIREX cover song identiﬁ-\ncation task.\nAn interesting variation on the theme of using chroma fea-tures for characterizing music similarity is proposed in [3].\nIn this work, chroma features are collapsed into string se-\nquences using vector quantization (VQ) and best retrieval\nis achieved by calculating the string-edit distance between\nthese sequences. Success is demonstrated for ﬁnding re-\npeated patterns within a song. This paper also provides a\nstrong argumentation in favor of using string-based meth-\nods that take into account the ordering of events in the\nsignal, an issue which is consistently ignored in models\nfor texture-based similarity. However, the lack of inter-\npretability of the VQ-produced strings encourages the use\nof metrics that consider all character swaps to be the same,\na strategy that we purposefully avoid in this paper. Al-\nternatively, [8] uses strings representing chord sequences.\nThis approach to cover song ID, on which our work is\nbased, relies on the calculation of chord sequences by means\nof HMM-based analysis (supervised in their case, unsu-\npervised in ours) and the computation of pairwise simi-\nlarity on key-transposed sequences using DTW. While re-\nsults are promising, this work fails to justify why the cho-\nsen scoring methodology is a good ﬁt to the nature of the\nproblem and to the data it uses. This, in turn, branches\nout into other fundamental questions: How does this ap-\nproach cope with the inexactitude of the sequence estima-\ntion? What is the impact of attempting to introduce key\ninvariance? Is there a purpose for introducing beat-based\nrhythmic invariance (as proposed by [6] and [10]) into the\nprocess? These questions motivate our work, and our at-\ntempts to answer them constitute its main contributions.\n1.2 Organization of this paper\nSection 2 brieﬂy explains how chord sequences are esti-\nmated and analyzes their likely pattern of confusion. Sec-\ntion 3 discusses the very basics of sequence alignment,\nintroduces our strategy for scoring character substitutions\nand explains our approach to key-invariant alignment. Sec-\ntion 4 presents the results and discussion of four exper-\niments on cover song identiﬁcation aimed at measuring\nthe impact that certain parameter conﬁgurations have on\nretrieval. Section 5 presents conclusions and future work.\n2 ESTIMATING CHORD SEQUENCES\nIn [2] a methodology was introduced for robustly gen-\nerating sequences of major and minor triads from audio\nsignals. The approach, to be brieﬂy summarized in the\nfollowing, is used as the front end to our cover song iden-\ntiﬁcation system. First, 36-dimensional chroma vectors,\nor pitch-class proﬁles, are calculated from the audio sig-\nnal by collapsing constant-Q spectral data into one oc-\ntave. These vectors are tuned and, optionally, averaged\nwithin beats, before being quantized into 12-bin vectors\nrepresenting the spectral energy distribution across notes\nof the chromatic scale. These features are used as observa-\ntions on a 24-state hidden Markov model, where each state\ncorresponds to one of the major and minor triads. The\nparameters of the model, initialized using simple musi-cal knowledge, are trained in an unsupervised fashion us-\ning the Expectation-Maximization (EM) algorithm. Dur-\ning training, state-to-observation parameters are clamped,\nthus resulting on a ‘semi-blind’ optimization. The ﬁnal se-\nquence of triads is obtained by decoding the model using\nthe Viterbi algorithm.\nTP: 69.64 PAR: 3.44 REL: 5.81 V: 4.00\nIV: 2.21 III: 2.42 OTH: 7.69 NR: 4.79\nTable 1 . Chord estimation results using frame-based chro-\nmas. Values are in percentage of total detections.\nWhile chord estimation results are available on the origi-\nnal paper, it is more relevant to this work to discuss a more\nrecent evaluation of the system. Table 1 depicts results and\nconfusion on a chord recognition test performed against\n110 manually-annotated chord sequences of recordings by\nthe Beatles (see [7] for more details about this dataset).\nFor the test we assume enharmonic equivalence and map\ncomplex chords, e.g. 6ths,7ths, to their base triad (e.g.\nEm7 = Em). Numbers in the table indicate the percentage\nof total detections for the following categories: true pos-\nitives (TP), parallel major/minor confusions (PAR), rela-\ntive major/minor confusions (REL), dominant confusions\n(V), sub-dominant confusions (IV), third or sixth confu-\nsions (III), confusions not in the above categories (OTH)\nand chords which are not recognized by the system and\ncounted as errors (NR, e.g. diminished, augmented, si-\nlences). The results are revealing in that they show that\nnearly half the errors that the system makes (REL + V +\nIV + III) are in the immediate vicinity of the true posi-\ntive in the doubly-nested circle of 5thsof major and mi-\nnor triads [2]. Assuming that these results can be gen-\neralized to the larger set we use for retrieval, then true\npositives and these closely-related errors account for 85%\nof total sequence content. Beyond the obvious relation\nbetween these results and our choice of initialization for\nthe HMM’s state-transition probability matrix, lies the fact\nthat the ordering of chords in the circle provides a good\nmodel for the scoring of character substitutions, an issue\nat the heart of sequence alignment methodologies.\n3 SEQUENCE ALIGNMENT\nFinding the globally-optimal alignment between strings is\nan extensively researched topic, notably in bioinformatics\n[5]. The idea is to ﬁnd the best possible path between the\nstrings by allowing inexact character matches (i.e. substi-\ntutions or swaps) and the introduction of gaps in either of\nthe sequences. In this context, the best path is the one that\nmaximizes a score function, usually the sum of individual\nscores for aligned pairs of characters, under the consid-\neration that both gap insertions and substitutions imply a\npenalty, to be respectively known as \rand^S. Because\nthe number of substitutions and gaps is expected to be\nlow between similar sequences, the resulting score is a\ngood measure of similarity. In our application, measur-\ning similarity using string matching provides the addedFigure 1 . MatrixSbased on unitary distances on the\ndoubly-nested circle of 5ths\nbeneﬁt of taking the sequence ordering into consideration,\nand thus the temporal structure of the musical piece. This\nstands in contraposition to the common “bag-of-features”\napproach where feature ordering is mostly, or totally, ig-\nnored. In the present system we use a standard solution to\nglobally-optimal string alignment, based in Dynamic Pro-\ngramming, and known as the Needleman-Wunsch-Sellers\n(NWS) algorithm (see [5] for a detailed explanation). We\nuse the implementation in NeoBio , and open-source li-\nbrary of bio-informatics algorithms in Java [4].\n3.1 Substitution Matrix\nSome string alignment implementations use a uniform pe-\nnalty for all substitutions (e.g. string-edit distance). How-\never, our chord sequences are inaccurate and, more im-\nportantly, they follow an non-uniform error pattern that\ncan be predicted from the data on Table 1. Hence, it is\nbest to use a score function, i.e. a substitution matrix, that\nis able to favor certain chord swaps above others. The\nmatrix is deﬁned such that a positive/negative value on\nthe matrix results on an increase/decrease of the global\nscore. For our experiments we use the substitution matrix\n^S= (S\u0000\u000b)\u0002\f, whereS, in Figure 1, is derived from\nthe ordering of chords in the doubly-nested circle of 5ths;\n\u000bis an offset that changes the distribution of positive and\nnegative values in the matrix; and \fis a scaling factor (=\n10 in the rest of this paper). Values in the main diago-\nnal ofS(characterizing perfect matches) are equal to 12.\nIn any given column, going a step up or down from the\nmain diagonal results on a unitary decrease on the substi-\ntution value. This pattern is repeated until we reach zero\nat the opposite end of the circle (e.g. for a C/F# substi-\ntution). From that point on, values start to increase again\nuntil we reach full circle. As can be seen in the ﬁgure, the\nmatrix favors substitutions between harmonically-related\ntriads (e.g. between C and e/a or F/G), i.e. between those\ntriads that, according to results in Table 1, are more likely\nto be confused.3.2 Key-Invariant Alignment\nThe characterization of similarity using chord sequence\nalignment is key dependent. Except perhaps for a few\ncases, e.g. when the key shift is a relative minor or a\ndominant, the scoring of the alignment will be badly af-\nfected by variations on the key context. Even in those\ncases, key dependency increases the probability that non-\nrelevant songs that happen to be on the same key as the\nquery will be scored higher. As we cannot assume that\ndifferent versions of a song will all be in the same key, we\npropose a simple mechanism for key matching between\nsequences before alignment. Let us deﬁne xandyas two\ninteger sequences (of any length) such that their elements\nxi;yi20::23. This integer range corresponds to the 24\nmajor and minor triads organized from C to A minor, fol-\nlowing the ordering in the axes of Figure 1. Let us also de-\nﬁneXandYas the normalized histograms of sequences\nxandyrespectively. We propose that the score is maxi-\nmized for the alignment between xand^y\u001e, a key-shifted\nversion ofydeﬁned as ^y\u001e= (y+\u001e)mod 24, where\n\u001e= argmaxm(X\u0001^Ym),^Ym=Y[(n\u0000m)mod 24],\n8n20::23andm20 : 2 : 22 is deﬁned such that\nonly major/major or minor/minor shifts are allowed. This\nvery simple approach is only bound to be effective when\nhistogram shapes are similar, as we hope to be the case\nbetween cover songs. The latter assumption is not neces-\nsarily true when the structure of the songs being compared\nis signiﬁcantly different.\n4 EXPERIMENTS\nA collection of 3208 mp3 ﬁles of commercially-available\nmusic is used for testing. It contains songs on a wide\nvariety of genres with an emphasis on Anglo-American\nPop and Rock. Within that collection there is a cover\nsong sub-set of 157 songs representing 36 different pieces\nof music. This averages to 4.36 versions per piece, al-\nthough actual numbers oscillate between 2 and 16 ver-\nsions per piece. This sub-set is quite heterogeneous, rang-\ning from 22 studio-live pairs by the same band (out of 391\ncover-song pairs) such as Nirvana’s “Come as you Are” in\nNevermind and in MTV Unplugged in New York , to radi-\ncal interpretations such as Rancid’s remake of Bob Mar-\nley’s “No Woman No Cry” or REM’s remake of Gloria\nGaynor’s “I Will Survive”. Most versions are by different\nartists and usually involve changes on instrumentation.\nPerformance is evaluated by using all 157 cover songs as\nqueries and measuring precision and recall based on the\nranking of the other versions of each query. The queries,\nwhich are always retrieved at rank 1, are removed be-\nfore evaluation. Since this is a standard IR evaluation,\nwhere the number of relevant documents is known, we\nuse common performance measures such as the average\nR-Precision (Precision at rank R, where R is the total num-\nber of relevant items), the average Mean Reciprocal Rank\n(MRR - 1/rank of the ﬁrst relevant item) and average 11-\npoint Precision/Recall graphs for visualization.Figure 2 . 11-point P/R graphs for retrieval with and with-\nout key shifting.\nParameters Results (0-1)\nKey-shift \r\u000b Scope\u000e R-P MRR\non -10 10 frame 12 0.221 0.395\noff -10 10 frame 12 0.089 0.223\non 0 10 frame 12 0.116 0.220\non -20 10 frame 12 0.182 0.337\non -10 2 frame 12 0.122 0.265\non -10 6 frame 12 0.149 0.317\non -10 10 beat 1 0.168 0.320\non -10 10 beat 2 0.170 0.323\non -10 10 beat 4 0.145 0.285\non -10 10 frame 6 0.245 0.401\non -10 10 frame 20 0.208 0.377\nTable 2 . Results for various model parameters (Best set\nin bold).\nIn our experiments we aim at measuring the impact of the\nfollowing actions: (i) using key-shifting - in Section 4.1;\n(ii) varying the gap penalty \rused by the scoring algo-\nrithm - see Section 4.2; (iii) changing the distribution of\npositive and negative values on the substitution matrix ^S\nby varying the offset value \u000b- in Section 4.3; and (iv) us-\ning beat synchronous instead of frame-based chroma fea-\ntures for the sequence estimation - see Section 4.4. Since\nour sequences are highly redundant and the NWS algo-\nrithm is computationally very expensive ( O(n2)for se-\nquence length n), sequences are downsampled by a factor\n\u000e. The impact of this resampling in the performance of\nthe system is also measured on the last experiment. For\nthe same reason, we avoid the testing of all combinations\nin the parameter space by assuming that parameters are\nindependent from each other. This is an arguable assump-\ntion but a necessary one. Table 2 shows averages of R-\nPrecision (R-P) and Mean Reciprocal Rank (MRR) for allthe combinations of parameters tested in our experiments.\nValues range from 0 to 1, with 1 being the best possible\nvalue. The low values in the table hint at the difﬁculties\nof the task of cover song identiﬁcation. Since an open test\ncollection is not available, comparisons cannot be made\nwith existing approaches. However, by the time of publi-\ncation, results of this method in the context of the MIREX\n2007 Cover-song ID task will be available for comparison.\nTo get an idea of what the numbers in this paper mean in\npractice, the reader is encouraged to look at the full list of\nmusic and test results on the author’s website1.\n4.1 Testing Shifts\nIn the ﬁrst experiment, we test the impact of key shifting\non the system’s performance. Figure 2 shows the aver-\nage 11-point P/R graph with and without the key-shifting\nalgorithm described in Section 3.2. For this experiment:\n\r=\u000010,\u000b= 10 ,\u000e= 12 , and feature scope = frames.\nResults on Table 2 and Figure 2 show how key-shifting\nbrings about signiﬁcant improvement on retrieval results.\nPrecision increases for all recall rates showing that rele-\nvant items are consistently ranked higher using this ap-\nproach. This increase is particularly acute ( >15%) for\nrecall rates between 0.3 and 0.6. These results are no sur-\nprise as they corroborate the intuition that key-shifting is\na solution to the known key independence of cover songs.\nHowever they cannot be taken for granted, since key shift-\ning also increases the risk that non-relevant songs with\nsimilar chord progressions, a common occurrence in pop\nmusic, will be ranked higher than relevant songs. It is\npossible that our simple key-shifting approach might help\ndecrease this risk by favoring alignment between songs\nwith very similar chord distributions. On the other hand,\nthis approach might be precluding covers which are sig-\nniﬁcantly different in structure, and thus bound to have\ndissimilar chord distributions, to be ranked higher.\n4.2 Testing Gaps\nExperiment 2 is aimed at testing the sensitivity of the sys-\ntem to changes on the gap penalty \r. For this experiment\nwe use\r= 0, -10 and -20, while \u000b= 10 ,\u000e= 12 , key-\nshifting is on and feature scope = frames. Results in Fig-\nure 3 show how worst performance is achieved for the case\nwhen no penalty is used, i.e. \r= 0 . This indicates that,\nif allowed to time-scale at no cost, many a non-relevant\nchord sequence can be matched to a query. Again, this is\nrelated to the constant use of similar chord progressions\nin popular music, where harmonic palettes are often less\nvaried that in orchestral music, for instance. However, the\nfact that results are consistently better for \r=\u000010than\nfor\r=\u000020indicates that over-penalizing for gap inser-\ntions also has a negative effect on performance. This is\nintuitive since large gap penalties do not allow the ﬂexi-\nbility needed to match similar songs with different tempi\nor with slight changes of form.\n1http://homepages.nyu.edu/ \u0018jb2843/Publications/ismir07.htmlFigure 3 . 11-point P/R graphs for variations of the gap\npenalty\r\nFigure 4 . 11-point P/R graphs for variations of \u000b\n4.3 Testing Swaps\nAlthough the order of preference of chord swaps is pre-\ndeﬁned by the values in matrix S, changes in the offset\nvalue\u000b, used to deﬁne the substitution matrix ^S, signify\nwhich swaps have a positive or negative impact on the\nscore function. Experiment 3 is aimed at testing how per-\nformance is affected by these changes. For this test we use\n\u000b= 2, 6 and 10, with \r=\u000010,\u000e= 12 , key-shifting on\nand feature scope = frames. The range of \u000bwas selected\nto be symmetrical with respect to the center of the circle of\n5ths(corresponding to \u000b= 6), while avoiding values that\nwill render ^Scompletely positive or negative, i.e. \u000b\u00140\nand\u000b\u001512. Figure 4 shows that results are worse whenscoring for swaps is too permissive, e.g. for \u000b= 2 when\nmost values in ^Sare positive. Results are slightly better\nfor\u000b= 6 and much better for \u000b= 10:This is an impor-\ntant observation as the increase of \u000bis the same between 2\nand 6 as it is between 6 and 10, while the rate of improve-\nment is notably different. This difference highlights the\nsuitability of using positive scoring only for those swaps\nwhich are close in the circle of 5ths, as suggested by the\ninformation on Table 1. These results strongly support the\nview that an adequate choice of substitution matrix can\nhelp offset the negative impact that chord estimation er-\nrors can have on the retrieval of similar songs.\n4.4 Testing Beats\nThe ﬁnal experiment tests: (a) the impact of using beat-\nsynchronous instead of frame-based chroma features, and\n(b) the effect of downsampling sequences by a factor \u000e\nbefore alignment. These tests are grouped together be-\ncause both these parameters affect the length of the se-\nquences to be aligned, and thus the computational expense\nof querying the system. In fact, beat-synchronous estima-\ntion reduces the average sequence length to one-sixth of\nthe frame-based length. As a result we test frame-based\nfeatures with \u000e= 6, 12 and 20, against beat-based features\nwith\u000e= 1, 2 and 4. The other parameters are set to: \r=\n-10,\u000b= 10 and key-shifting on. Figure 5 shows results\nfor this experiment. Because of the density of this graph,\nthe ﬁgure only depicts a detail of the 11-point P/R curves\nfor Precision2[0:05;0:7]and Recall2[0:1;1].\nAgainst our expectations, frame-based analysis consistent-\nly outperforms beat-synchronous analysis. The difference\nis further emphasized when comparing parameter combi-\nnations with similar computational complexity (e.g. [beat,\n\u000e= 1] with [frame, \u000e= 6]). Perhaps this is an indication\nof the difﬁculties in performing robust onset detection and\nbeat-tracking on a large collection of music with many\ndifferent styles and instrumentations. If beat-tracking is\nnoisy, e.g. if beat segments include chord transitions, then\nour chord labels will be prone to errors. Furthermore, it is\nvery unlikely that the error distribution will correspond to\nvalues in Table 1, thus rendering our swap scoring strat-\negy useless. This is by no means a reﬂection on all beat-\ntracking strategies. These results could very well be due\nto the shortcomings of our beat-based analysis (see a de-\nscription in [2]). However they do highlight the risks taken\nwhen segmenting prior to sequence estimation.\nThe results for the various values of \u000eare more predictable.\nAs expected, an increase of \u000e, implying a lossy compres-\nsion of the sequence, entails a decrease in performance.\nThis can be seen for both frame and beat-based analysis.\nAs a result, best performance overall is for frame-based\nanalysis with the smallest downsampling factor ( \u000e= 6).\nIt is also logical to expect that frame-based analysis with-\nout downsampling ( \u000e= 1) would perform even better, but\nthis experiment takes too long to run under our current\nconﬁguration.Figure 5 . Detail of 11-point P/R graphs for variations of\nfeature scope and downsampling factor \u000e\n5 CONCLUSIONS\nWe present a solution to cover-song identiﬁcation using\napproximate chord sequences and string alignment. More\nso than the approach itself, the emphasis is on the choice\nof a parameter set that: (i) helps us characterize the essence\nof cover songs independently of key, tempo or instrumen-\ntation; while (ii) taking into account the error-prone na-\nture of chord sequences estimated from audio. Speciﬁ-\ncally, the paper contributes a systematic evaluation of key\nshifting, the cost of gap insertions and character swaps in\nstring alignment, and the use of a beat-synchronous fea-\nture set. Results show that frame-based analysis consis-\ntently outperforms beat-synchronous segmentation, con-\ntradicting our intuition that such pre-processing could help\novercome tempo differences between covers. We specu-\nlate, in the absence of a full evaluation, that this is due\nto the inability of our beat-based analysis to generalize to\nmusic of different styles and instrumentation. This neg-\native result could be reversed in future implementations\nby the use of a more sophisticated beat-tracking system,\nsuch as the one used in [6]. Results also show that con-\nsiderable improvement is brought about by pairwise key\nmatching, moderately penalizing gaps and positively em-\nphasizing swaps that are related to common confusions\nof our chord estimation algorithm. These results support\nthe view that string alignment, as a mechanism for audio-\nbased retrieval, cannot be oblivious to the problems of ro-\nbustly estimating musically-meaningful information from\naudio. Future research will concentrate on overcoming the\nlimitations imposed by the high computational cost of the\nimplemented approach (in excess of 100ms per pairwise\ncomparison, resulting in 5+ minutes of computation per\nquery). Possible solutions to this problem could include\nthe use of efﬁcient search methodologies such as iterativedeepening [1], or the use of representative parts of a song\n(e.g. chorus) for comparison.\n6 ACKNOWLEDGMENTS\nThe author would like to thank Tim Crawford, Jeremy\nPickens, Matija Marolt, Agnieszka Rogniska and Ernest\nLi for their ideas and support during the development of\nthis work.\n7 REFERENCES\n[1] N. Adams, D. Marquez, and G. Wakeﬁeld. Iterative\ndeepening for melody alignment and retrieval. In Pro-\nceedings of ISMIR-05, London, UK , 2005.\n[2] J.P. Bello and J. Pickens. A robust mid-level represen-\ntation for harmonic content in music signals. In Pro-\nceedings of ISMIR-05, London, UK. , 2005.\n[3] M. Casey and M. Slaney. The importance of sequences\nin music similarity. In Proceedings of ICASSP-06,\nToulouse, France , 2006.\n[4] S.A. de Carvalho. NeoBio: Bio-informatics algo-\nrithms in Java. http://neobio.sourceforge.net.\n[5] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Bio-\nlogical Sequence Analysis . Cambridge UP, 1998.\n[6] D. Ellis and G. Poliner. Identifying cover songs\nwith chroma features and dynamic programming beat\ntracking. In Proceedings of ICASSP-07, Hawai’i,\nUSA, 2007.\n[7] C. Harte, M.B. Sandler, S.A. Abdallah, and E. G ´omez.\nSymbolic representation of musical chords: A pro-\nposed syntax for text annotations. In Proceedings of\nISMIR-05, London, UK , 2005.\n[8] K. Lee. Identifying cover songs from audio using har-\nmonic representation. In MIREX task on Audio Cover\nSong ID , 2006.\n[9] K. Lemstr ¨om.String Matching Techniques for Music\nRetrieval . PhD thesis, University of Helsinki, Depart-\nment of Computer Science, 2000.\n[10] M. Marolt. A mid-level melody-based representation\nfor calculating audio similarity. In Proceedings of\nISMIR-06, Victoria, Canada , 2006.\n[11] M. M uller, F. Kurth, and M. Clausen. Audio matching\nvia chroma-based statistical features. In Proceedings\nof ISMIR-05, London, UK , 2005.\n[12] C. Meek and W.P. Birmingham. A comprehensive\ntrainable error model for sung music queries. J. Artif.\nIntell. Res. (JAIR) , 22:57–91, 2004.\n[13] R. Typke. Music Retrieval based on Melodic Similar-\nity. PhD thesis, Utrecht University, Netherlands, 2007."
    },
    {
        "title": "Audio Identification Using Sinusoidal Modeling and Application to Jingle Detection.",
        "author": [
            "Michaël Betser",
            "Patrice Collen",
            "Jean-Bernard Rault"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416890",
        "url": "https://doi.org/10.5281/zenodo.1416890",
        "ee": "https://zenodo.org/records/1416890/files/BetserCR07.pdf",
        "abstract": "This article presents a new descriptor dedicated to Audio Identification (audioID), based on sinusoidal modeling. The core idea is an appropriate selection of the sinusoidal components of the signal to be detected. This new descriptor is robust against usual distortions found in audioID tasks. It has several advantages compared to classical subband-based descriptors including an increased robustness to additive noise, especially non-random noise such as additional speech, and a robust detection of short audio events.  This descriptor is compared to a classical subband-based feature for a jingle detection task on broadcast radio. It is shown that the new introduced descriptor greatly improves the performance in terms of recall/precision.",
        "zenodo_id": 1416890,
        "dblp_key": "conf/ismir/BetserCR07",
        "keywords": [
            "Audio Identification",
            "Sinusoidal modeling",
            "Robustness",
            "Signal detection",
            "Additive noise",
            "Non-random noise",
            "Broadcast radio",
            "Jingle detection",
            "Performance improvement",
            "Recall/precision"
        ],
        "content": "AUDIO IDENTIFICATION USING SINUSOIDAL \nMODELING AND APPLICATION TO JINGLE DETECTION \nMichaël Betser Patrice Collen Jean-Bernard Rault \nFrance Télécom R&D  \n4 rue du clos courtel, 35510 Cesson-Sévigné, France  \ne-mail: firstname.lastname@orange-ftgroup.com \nABSTRACT \nThis article presents a new descriptor dedicated to  \nAudio Identification (audioID), based on sinusoidal  \nmodeling. The core idea is an appropriate selection  of \nthe sinusoidal components of the signal to be detec ted. \nThis new descriptor is robust against usual distort ions \nfound in audioID tasks. It has several advantages \ncompared to classical subband-based descriptors \nincluding an increased robustness to additive noise , \nespecially non-random noise such as additional spee ch, \nand a robust detection of short audio events.  This  \ndescriptor is compared to a classical subband-based  \nfeature for a jingle detection task on broadcast ra dio. It \nis shown that the new introduced descriptor greatly  \nimproves the performance in terms of recall/precisi on. \n1.  INTRODUCTION \nThis last decade has seen a rapid increase in avail able \nmultimedia content. The question of rapid and easy \naccess to these data has become a strategic researc h \nsubject, especially in the audio domain. Audio \nidentification concerns numerous applications which  can \nbe gathered in three categories: research of inform ation \nabout a document (identification via cell-phone, CD  \ntracks identification etc.), document detection for  \nstructuring purposes (jingle detection...) and docu ment \ndetection for broadcast control and copyright purpo ses.  \nAll media involving audio contents are concerned. \nAlthough two families of audio identification syste m \nexist, namely audio fingerprinting and audio \nwatermarking, the former is more popular, being mor e \nrobust and non-intrusive. Fingerprinting systems in volve \nthe construction of a fingerprint for each document  \nwhich should uniquely characterize the document. Th e \nfingerprint should also be robust to any alteration  the \ndocument might suffer. Extensive lists of possible \nalterations can be found in many studies, and actua lly \ndepend on the application [1] [2].  \nNon-random additive noises such as speech are a \nvery frequent problem in the case of broadcast radi o \nanalysis. This problem is not usually addressed by \nresearchers because most audio identification syste ms \nare aimed at musical pieces identification and musi cal database management. This type of modification can \nseriously alter most of the classically used finger prints. \nThis article will present a new type of fingerprint , \nbased on a sinusoidal parameter extraction, which c an \nhandle this kind of noise as well as the other usua l \ndistortions. \nOne other major issue, concerning audio \nidentification constraints is computational efficie ncy.  \nMost systems use hashing procedures, and precompute d \nlook-up tables to speed up the process [2]. The pur pose \nof this article is not computational performance \ncomparison, but rather to demonstrate the robustnes s of \nthis sinusoidal fingerprint, bearing in mind that h ashing \nprocedures can also be adapted to this parameter. \nIn section 2, the paper begins with a brief \npresentation of the fingerprinting systems, and a r eview \nof the different types of fingerprints used for aud io \nidentification.  The proposed sinusoidal fingerprin t \nextraction and comparison scheme will be presented in \nsection 3 and 4.  Application to jingle detection a nd \nexperimental comparisons end the main part of the \ndocument in section 5 before a conclusion note. \n2.  FINGERPRINT SYSTEM \nAll fingerprint-based methods present the same clas sical \nanalysis scheme [1], which is presented in Figure 1. \nAudio Stream Meta data Meta data Reference \nFingerprint \nDB Collection of \naudio objects \nStream \nFingerprint Match  \n \nFigure 1:  Fingerprint-based audio identification \nA fingerprint-based identification system is compos ed \nof three distinctive parts: a fingerprint extractio n \nmodule, a storage module ('DB' for database), a \nfingerprint comparison module ('match'). \nWe have made a distinction between the reference \nfingerprint and the stream fingerprint computation \nmodules. We will see in section 3 that computing a \ndifferent fingerprint for the reference audio objec ts and © 2007 Austrian Computer Society (OCG). \n   \n \nthe audio streams makes sense and is useful for \nsinusoidal fingerprinting in order to take superimp osed \nsounds into account. \nThis article focuses on the fingerprint extraction \nmodule, which is described in detail in the next se ction, \nbut also on the comparison module which directly \ndepends on the nature of the fingerprint. Usually t he \ncomparison procedure is kept as simple as possible for \ncomputational purposes. \n2.1.  Classical feature extraction \nThe generation of the fingerprint is usually based on a \nshort-time frequency analysis, using a windowed Fas t \nFourier transform (FFT). The fingerprint of an audi o \ndocument is therefore composed of a collection of s ub-\nfingerprints regularly spaced in time. In most syst ems, \nthe FFT spectrum is divided into sub-bands, and a \nspectral characteristic is extracted from each sub- bands \nto form the sub-fingerprint. The spectral character istic \nextracted can be the spectral magnitude [3] , the e nergy \ndifference along the time and frequency axes [2], t he \nspectral flatness measure [4], the modulation spect rum \nof the energy flux [5], the binary state of activat ion [6], \nthe Mel cepstrum coefficients [7]. \nThe method retained as reference for comparison is \nthe one proposed by Haitsma et al. [2]. This method  is \none of the most utilized for comparison purposes si nce it \nexhibits very good performances on musical \nidentification tasks. \n2.2.  Limitations of classical fingerprints \nPerceptually, recognizing a sound object essentiall y \nrelies upon the information carried by the object's  \npredominant sinusoidal components. Even if there is  no \ncomplete psycho-acoustic study on the subject, \nexperiments seem to confirm this fact [6]. The main  flaw \nof the descriptors described in the previous sectio n is \nthat they only partially take into account this fac t.  \nAnother problem concerns the low energy portions of  \nthe audio signal which are often kept to compute th e \nfingerprints and which make them more fragile. \nThe proposed solution is to analyze the reference \nsignal, in order to extract the strongest sinusoida l \ncomponents, which will be less prone to noise \nperturbation. Actually severe distortions on these \ncomponents will result into such important changes in \nthe perception of the sound object that the object can \nhardly be considered identical anymore. \n3.  SINUSOIDAL FINGERPRINT \nThe general procedure of the sinusoidal fingerprint  \ncreation is presented on Figure 2. It has four steps, the \npre-selection and the compression steps being optio nal. \n3.1.  Preselection \nThe pre-selection step corresponds to any pre-proce ssing \ndone before the FFT, like band filtering. Here it i s a low-pass filtering of the signal with a frequency c ut at 4 \nkHz. It will render the signature less prone to ban d pass \nlimitation, and concentrate the processing on the m ost \ninformative part of the signal.  \n \nFingerprint  Sinusoidal \npeak extraction  Audio data  Pre-selection \nSinusoidal \nselection Compression \n \nFigure 2:  Fingerprint-based audio identification \n3.2.  Sinusoidal peak extraction \nSinusoidal modeling is based on the decomposition o f \naudio signals into a sum of sinusoidal components p lus a \nnoise residual part. The sinusoidal components are \nmodeled by a sinusoid with a set of parameters incl uding \namplitude, phase and frequency. Sinusoidal paramete rs \nextraction consists in estimating these parameters for \neach sinusoid present in the signal. For audio \nidentification, the phase is not a discriminant par ameter \nand therefore will not be retained.  \nNumerous approaches have been proposed, many of \nwhich being based on Fourier analysis. The Fourier-\nbased estimation procedure has proven almost optima l, \ngiven that the sinusoids are well resolved by the F ourier \ntransform and respect the underlying sinusoidal mod el \n[9]. These methods are also computationally effecti ve, \nmany of them being only slightly more complex \ncompared to a FFT. The method retained for frequenc y \nestimation is one of the so-called Discrete Fourier  \nInterpolator using phase, described in [10]. \n3.3.  Sinusoidal peak selection \nAll the extracted sinusoidal peaks are not of equal  \ninterests: many of them have low amplitudes, which \nmake them more vulnerable to noise perturbation, wh ilst \nothers do not verify the sinusoidal model, which ca uses \nimprecision in the method of estimation. In fact, o nly the \nmost predominant and stable peaks, relatively to th e \nestimation method, are required to identify an audi o \ndocument. Consequently, a selection procedure is \nneeded both for computational purposes, as fewer pe aks \nwill require less processing, and robustness purpos es. \nThe selection in itself can be different for the \nreference fingerprint and stream fingerprint. This has \ntwo advantages. First, the number of sinusoidal \ncomponent in the reference may vary from frame to \nframe, and if a noise with strong sinusoidal peaks is \nadded, the peaks to be detected might not be the \nstrongest ones anymore. It is therefore interesting  to \nkeep more peaks in the stream fingerprint than in t he \nreference.  Secondly, contrary to the reference \nfingerprint, the stream fingerprint is usually comp uted \non-the-fly. A less complex signature creation will \ntherefore save precious time. A concrete example of  \npeak selection will now be detailed.   \n \nThe reference peak selection consists of three step s. \nFirst, all of the low energy peaks are removed usin g an \nadaptive threshold. Only the sinusoids whose amplit ude \nis superior to a fraction of the power of the signa l are \nkept. Secondly, to avoid the time-shifting problems , and \nto suppress the unstable peaks, the frequency of pe aks \nhas to lie within a tolerance of Tf Hz when the fra me is \nshifted by H/2 and –H/2 samples, where H is the ste p \nsize between two frames. Finally, the M most energe tic \npeaks are kept. In order to have a reliable fingerp rint, M \nshould be superior to a hundred. \nThe stream peak selection consists in keeping the Q \nmost energetic peaks per frame. Q should be greater \nthan the maximum number of peaks kept within a fram e, \nduring the reference signature extraction step. \n3.4.  Compression \nThe compacting module consists in keeping only the \nfrequency of each peak, coded on 16 bits. The \ncorresponding precision, for a frequency interval o f \n[0,4000], is 0.1 Hz. The amplitude is not kept, bei ng \nmore prone to noise perturbation. For a given frame , the \nfinal signature is a vector containing the frequenc ies of \nall the selected peaks. \n4.  FINGERPRINT COMPARISON AND \nDECISION \nFigure 3  represents the general scheme of sinusoidal \nfingerprint comparison. The stream fingerprint is a  block \nof T frames, referred to as Bs. This block is compa red to \nall the possible blocks Bj,t of length T in all the  \nreference object, where j is the index of the refer ence \nobject and t the frame index in the reference objec t j. \n \nDetection Stream \nfingerprint Sinusoidal \npairing Similarity \nmeasure \nDecision Reference \nfingerprint \n \nFigure 3:  Fingerprint-based audio identification \nThe comparison of Bs and Bj,t  is done frame by frame. \nA frequency of one frame in Bj,t  is considered as paired \n(detected) with a frequency of the corresponding fr ame \nin Bs if they are equal, within a tolerance Tf. This \ntolerance is the same as the one used in the refere nce \nfingerprint creation, in section 3.  \nThe chosen similarity measure is the number of \nreference frequencies correctly detected in the aud io \nstream normalized by Mj, the number of peaks per \nsecond in the reference j. Dividing by Mj favors the parts \nof the reference which have a number of frequencies  per \nframe superior to the mean, and are therefore more \nreliable. If all the frequencies are detected, and if the \nlength of the reference audio object is equal to T, then the similarity is equal to 1. In the general case, when the \nsimilarity is close to one or above, the detection is \nconsidered as very reliable. The robustness on addi tive \nnon-random noise is ensured by the fact that only \ncorrectly detected sinusoidal peaks will increase t he \nsimilarity measure. Additional sinusoidal peaks in the \nstream fingerprint will have no impact. \nThe similarity measure can be considered as a \nfunction of the time t for each reference j. If a m aximum \nis detected in this function of time, a decision is  taken \nusing two thresholds as in [3]. \n5.  APPLICATION TO JINGLE DETECTION \n5.1.  Corpus \nThe system is applied to a jingle detection task fo r \nbroadcast radio. The task consists in detecting 30 \nextracts of jingles from the French news radio Fran ce \nInfo. The jingles to be detected have a length vary ing \nfrom 3 seconds to 10 seconds. Their length is not \ndetermining for the performance evaluation because the \ndetection is realized for a fixed block size of 1 s econd. \nThe train corpus used for reference fingerprint \ncreation is composed of one example of each jingle \nrecorded in FM. The development corpus for paramete r \ntuning is composed of 15 extracts of one minute of \nFrance Info recorded separately, each containing on e \njingle. \nThe test corpus is composed of 18 hours of France \nInfo radio program recorded in AM. A total of 243 \njingle occurrences are present in the corpus. Among  \nthem 33 corresponds to shorter versions of the jing les. In \nradio programs, these short jingles are used to ann ounce \nnew topics for example. They are usually fragments of \ntheir longer counterparts. In order to test noise \nrobustness, this corpus has been altered using two other \nkinds of distortions: mp3 compression at 16 kb/s \n(AM+MP3) and speech addition with a 0 dB SNR \n(AM+SP). We have also added 48 hours from two other  \nmusical French radios, RFM and Skyrock for false al arm \nverification. \n5.2.  Parameters \nIn Table 1, the parameters used for both algorithms are \nsummarized. `Sinusoidal' refers to the algorithm \ndescribed in this article and `HKO' to the classica l \nalgorithm described in [2].  \n \n Sinusoidal HKO \nSampling frequency 8000 8000 \nFrame size 512 4096 \nStep size 128 96 \nFrame per block 62 84 \nTable 1: Parameter comparison \nThe two approaches have very different FFT \nparameters: the former approach uses long Fourier \ntransforms, with an important overlap, whereas the   \n \nsinusoidal method uses short Fourier transform with  a \nsmall overlap. For both methods the parameters have  \nbeen set up to respect an entrance block size of \napproximately 1 second.  \nThe other parameters have been optimized on the \ndevelopment corpus. The tolerance Tf is connected t o \nthe precision of the frequency estimator used. If t he \nsinusoidal model is respected, i.e. the amplitude a nd \nfrequency is locally constant, then the maximum err or \non the frequency estimation will stay much lower th an \nthe Fourier precision, even for strong white noise \nperturbations [9].  Tf should be slightly higher th an this \nmaximum. For the parameters used in our experiments , \nthe maximum error for a -10 dB white noise perturba tion \nis approximately 2Hz [10], and Tf has been set to 3 Hz. \nThe HKO algorithm uses 33 frequency bands with an \nexponential repartition, and the maximum bit error rate \nhas been set to 0.25, which is the same value as in  [2]. \n5.3.  Results \nThe algorithms are compared in terms of \nrecall/precision. Nevertheless, as both algorithms \npresent no false alarm, the precision has been omit ted, \nbeing always equal to 100 %. Two different measures  of \nthe recall are used. The recall in terms of occurre nces is \nthe number of jingle detected divided by the number  of \njingle present in the corpus. The recall in terms o f \nduration is the total block length correctly detect ed \ndivided by the total length of the jingles in the c orpus. \n \n AM AM+MP3 AM+SP \nSinusoidal 97 95 83 \nHKO 89 85 67 \nTable 2: Occurrence recall comparison in percent \n AM AM+MP3 AM+SP \nSinusoidal 79 68 53 \nHKO 60 57 34 \nTable 3: Duration recall comparison in percent \nA significant difference between the occurrence rec all \nand the duration recall appears. As every jingle is  at \nleast several seconds long, there is still a high \nprobability to find at least one of the blocks \ncorresponding to an occurrence of the jingle. To a lesser \nextent, another fact explaining the differences bet ween \nthe two measures comes from the block-based \ncomparison scheme.  The blocks on edges of the jing les \nmight not be detected if the blocks do not contain \nenough jingle frames. \nFor an AM only perturbation, both algorithms \nperform well, with an advantage to the sinusoidal \nalgorithm. The remaining errors, in the case of the  \noccurrence measure, come from the short jingles. So me \nof them are too short to offer a reliable detection . Both \nalgorithms are fairly robust to a strong mp3 \ncompression, in terms of occurrence. As expected th e \nsinusoidal algorithm performs particularly well on a speech additive perturbation compared to the HKO \nalgorithm. The duration recall decreases significan tly, \nespecially in the case of the HKO algorithm. \n6.  CONCLUSION \nIn this article, a new type of fingerprint dedicate d to \naudio identification and based on sinusoidal modeli ng \nhas been presented. The advantages of this fingerpr int \ncompared to classical subband-based fingerprint are  \ntwofold. First a better modeling of the signal to b e \nrecognized, focused on the most informative part of  the \nsignal, allows to reliably recognizing segments of sounds \nas short as 1 second. Secondly, as the comparison i tself \nis based only on frequency, this fingerprint presen ts an \nincreased robustness to compression, to noise addit ion, \neven for strong non-random signals such as speech, and \nto subband filtering modifications (equalization). \nIn future work, a fast version of the comparison \nalgorithm, based on the principle of hashing tables , will \nbe investigated. Adaptations of the algorithm for t ime \nstretching deformations using dynamic programming \nwill also be explored. \n7.  REFERENCES \n[1]  P. Cano et al., “Audio fingerprinting: concepts and  \napplications,” Studies in computational \nintelligence,  vol. 2, pp. 233–245, 2005. \n[2]  J. Haitsma, et al., “Robust audio hashing for \ncontent identification,” International Workshop on \nContent-Based Multimedia Indexing,  Sep 2001. \n[3]  J. Pinquier and R. André-Obrecht, “Jingle \ndetection and identification in audio document,” \nProc. of ICASSP,  2004. \n[4]  O. Hellmuth, et al., “Advanced audio identification  \nusing mpeg-7 content description,” AES 111th \nconvention,  Sep 2001. \n[5]  J. Laroche, “Patent : Process for identifying audio  \ncontent,” Number : WO88900, Nov 2001. \n[6]  D. Fragoulis, et al., “On the automated recognition  \nof seriously distorted musical recordings,” trans. \non Sig. Proc., vol. 49, no. 9, pp. 898–908, 2001. \n[7]  L. Gomes, et al., “Audio watermarking and \nfingerprinting: for which applications?,” J. of New \nMusic Research,  vol. 32, no. 1, pp. 65–81, 2003. \n[8]  C. Burges, et al., “Distortion discriminant analysi s \nfor audio fingerprinting,” IEEE trans. on Sp. and \nAudio Proc.,  vol. 11, no. 3, pp. 165–174, 2003. \n[9]  M. Betser, et al., “Review and discussion on \nSTFT-based frequency estimation methods,” AES \n120 th  Convention,  2006. \n[10]  M. Betser, et al., “Frequency estimation based on \nadjacent DFT bins,” Proc. of EUSIPCO,  2006."
    },
    {
        "title": "A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources.",
        "author": [
            "John Ashley Burgoyne",
            "Laurent Pugin",
            "Greg Eustace",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418203",
        "url": "https://doi.org/10.5281/zenodo.1418203",
        "ee": "https://zenodo.org/records/1418203/files/BurgoynePEF07.pdf",
        "abstract": "Binarisation of greyscale images is a critical step in optical music recognition (OMR) preprocessing. Binarising music documents is particularly challenging because of the nature of music notation, even more so when the sources are degraded, e.g., with ink bleed-through from the other side of the page. This paper presents a comparative evaluation of 25 binarisation algorithms tested on a set of 100 music pages. A real-world OMR infrastructure for early music (Aruspix) was used to perform an objective, goaldirected evaluation of the algorithms’ performance. Our results differ significantly from the ones obtained in studies on non-music documents, which highlights the importance of developing tools specific to our community. 1 INTRODUCTION Binarising music documents, that is separating the foreground from the background in order to prepare for other tasks such as optical music recognition (OMR), is much more challenging than binarising text documents. In text documents, the letters are all of approximately the same size and are regularly and uniformly distributed throughout the page. Music symbols, on the other hand, exhibit a wide range of sizes and markedly uneven distribution: they are clustered around musical staves. Large black areas, such as note heads, are conducive to ink accumulation during printing, which often results in strong bleedthrough (elements from the verso visible through the paper), especially for early sources. Large blank areas without foreground elements can disturb some binarisation techniques because bleed-through is often considered to be foreground. We will show in this paper that because of these conditions, the most widely used binarisation methods fail to produce suitable results for OMR. Evaluating the performance of binarisation algorithms is a difficult task. Very often, due to a lack of any evaluation infrastructure, researchers use subjective approaches, e.g., marking output as “better”, “same” or “worst” [3, 4]. When the binarisation is performed for the purpose of further image processing tasks, such as optical character c⃝2007 Austrian Computer Society (OCG). recognition (OCR) or OMR, it makes more sense to use an objective evaluation. Evaluating the algorithms within the context of a real-world application enables goal-directed evaluation, which rates a binarisation algorithm on its ability to improve the post-binarisation task [10]. Furthermore, it has been shown that when document images have graphical particularities like music documents do, the use of goal-directed evaluation can lead to significant performance improvements [7]. 2 METHODS For our experiments, we used Aruspix, a software application for OMR on early music prints [7]. We selected five 16th-century music books (RISM 1520-2, 1532-10, 15385, M-0579 and M-0582 [8]) that suffer from severe degradation and transcribed 20 pages from each (100 total) to obtain ground-truth data for the evaluation. We tested 25 different binarisation algorithms over a range of parameters, which resulted in a set of 8,000 images. The images were deskewed and normalised to a consistent staff height (100 pixels) by Aruspix before applying the binarisation algorithm, and after binarisation, Aruspix was used again for the OMR evaluation. Binarisation methods can be categorised according to differences in the criteria used for thresholding. Sezgin and Sankur have proposed a taxonomy of thresholding techniques, including those based on the shape of the greyvalue histogram, measurement-space clustering, image entropy, connected-component attributes, spatial correlation, and the properties of a small, local windows around each pixel [9]. We chose a range of top-performing algorithms for both document images and what Sezgin and Sankur call non-destructive testing (NDT) images, which have more photo-like qualities; for the reasons mentioned above, music documents would be expected to fall somewhere between these two extremes. Methods based on histogram shape include those proposed by Sezan (1985) and Ramesh et al. (1995). Popular measurement-space clustering methods include those proposed by Ridler and Calvard (1978), Otsu (1979), Lloyd (1985), Kittler and Illingworth (1986), Yanni and Horne (1994), and Jawahar et al. (1997). Some entropy-based methods, also popular, are those of Dunn et al. (1984), Kapur et al. (1985), Li and Lee (1993), Shanbhag (a) RISM 1520-2 (b) RISM 1538-5 (c) RISM M-0582 Figure 1. Three sample images from the test set. (a) Brink and Pendock 1996 (b) Gatos et al. 2004 (c) Otsu 1979 Figure 2. Binarisation output from three algorithms on RISM 1538-5. Algorithm Win. Recall Precision Rate Odds multiplier Rate Odds multiplier Brink and Pendock 1996 79.2",
        "zenodo_id": 1418203,
        "dblp_key": "conf/ismir/BurgoynePEF07",
        "keywords": [
            "Binarisation",
            "greyscale images",
            "optical music recognition",
            "preprocessing",
            "music documents",
            "degraded sources",
            "ink bleed-through",
            "early music",
            "Aruspix",
            "objective evaluation"
        ],
        "content": "A COMPARATIVE SURVEY OF IMAGE BINARISATION ALGORITHMS\nFOR OPTICAL RECOGNITION ON DEGRADED MUSICAL SOURCES\nJohn Ashley Burgoyne Laurent Pugin Greg Eustace Ichiro Fujinag a\nCentre for Interdisciplinary Research in Music and Media Tech nology\nSchulich School of Music of McGill University\nMontr ´eal, Qu ´ebec, Canada H3A 1E3\n{ashley,laurent,greg,ich }@music.mcgill.ca\nABSTRACT\nBinarisation of greyscale images is a critical step in optic al\nmusic recognition (OMR) preprocessing. Binarising mu-\nsic documents is particularly challenging because of the\nnature of music notation, even more so when the sources\nare degraded, e.g., with ink bleed-through from the other\nside of the page. This paper presents a comparative eval-\nuation of 25 binarisation algorithms tested on a set of 100\nmusic pages. A real-world OMR infrastructure for early\nmusic (Aruspix) was used to perform an objective, goal-\ndirected evaluation of the algorithms’ performance. Our\nresults differ signiﬁcantly from the ones obtained in stud-\nies on non-music documents, which highlights the impor-\ntance of developing tools speciﬁc to our community.\n1 INTRODUCTION\nBinarising music documents, that is separating the fore-\nground from the background in order to prepare for other\ntasks such as optical music recognition (OMR), is much\nmore challenging than binarising text documents. In text\ndocuments, the letters are all of approximately the same\nsize and are regularly and uniformly distributed through-\nout the page. Music symbols, on the other hand, exhibit\na wide range of sizes and markedly uneven distribution:\nthey are clustered around musical staves. Large black ar-\neas, such as note heads, are conducive to ink accumula-\ntion during printing, which often results in strong bleed-\nthrough (elements from the verso visible through the pa-\nper), especially for early sources. Large blank areas with-\nout foreground elements can disturb some binarisation tech -\nniques because bleed-through is often considered to be\nforeground. We will show in this paper that because of\nthese conditions, the most widely used binarisation meth-\nods fail to produce suitable results for OMR.\nEvaluating the performance of binarisation algorithms\nis a difﬁcult task. Very often, due to a lack of any evalua-\ntion infrastructure, researchers use subjective approach es,\ne.g., marking output as “better”, “same” or “worst” [3, 4].\nWhen the binarisation is performed for the purpose of\nfurther image processing tasks, such as optical character\nc/circlecopyrt2007 Austrian Computer Society (OCG).recognition (OCR) or OMR, it makes more sense to use an\nobjective evaluation. Evaluating the algorithms within th e\ncontext of a real-world application enables goal-directed\nevaluation, which rates a binarisation algorithm on its abi l-\nity to improve the post-binarisation task [10]. Further-\nmore, it has been shown that when document images have\ngraphical particularities like music documents do, the use\nof goal-directed evaluation can lead to signiﬁcant perfor-\nmance improvements [7].\n2 METHODS\nFor our experiments, we used Aruspix, a software applica-\ntion for OMR on early music prints [7]. We selected ﬁve\n16th-century music books (RISM 1520-2, 1532-10, 1538-\n5, M-0579 and M-0582 [8]) that suffer from severe degra-\ndation and transcribed 20 pages from each (100 total) to\nobtain ground-truth data for the evaluation. We tested 25\ndifferent binarisation algorithms over a range of parame-\nters, which resulted in a set of 8,000 images. The images\nwere deskewed and normalised to a consistent staff height\n(100 pixels) by Aruspix before applying the binarisation\nalgorithm, and after binarisation, Aruspix was used again\nfor the OMR evaluation.\nBinarisation methods can be categorised according to\ndifferences in the criteria used for thresholding. Sezgin\nand Sankur have proposed a taxonomy of thresholding\ntechniques, including those based on the shape of the grey-\nvalue histogram, measurement-space clustering, image en-\ntropy, connected-component attributes, spatial correlat ion,\nand the properties of a small, local windows around each\npixel [9]. We chose a range of top-performing algorithms\nfor both document images and what Sezgin and Sankur\ncall non-destructive testing (NDT) images, which have\nmore photo-like qualities; for the reasons mentioned above ,\nmusic documents would be expected to fall somewhere\nbetween these two extremes. Methods based on histogram\nshape include those proposed by Sezan (1985) and Ramesh\net al. (1995). Popular measurement-space clustering meth-\nods include those proposed by Ridler and Calvard (1978),\nOtsu (1979), Lloyd (1985), Kittler and Illingworth (1986),\nYanni and Horne (1994), and Jawahar et al. (1997). Some\nentropy-based methods, also popular, are those of Dunn et\nal. (1984), Kapur et al. (1985), Li and Lee (1993), Shanbhag(a) RISM 1520-2 (b) RISM 1538-5 (c) RISM M-0582\nFigure 1 . Three sample images from the test set.\n(a) Brink and Pendock 1996 (b) Gatos et al. 2004 (c) Otsu 1979\nFigure 2 . Binarisation output from three algorithms on RISM 1538-5.\nAlgorithm Win.Recall Precision\nRate Odds multiplier Rate Odds multiplier\nBrink and Pendock 1996 79.2 1.00 – 80.6 1.00 –\nPugin 2007 78.2 0.94 (0.89, 0.98) 79.5 0.92 (0.88, 0.97)\nGatos et al. 2004 21 76.4 0.84 (0.80, 0.88) 76.8 0.77 (0.74, 0.81)\nSezan 1985 76.4 0.84 (0.80, 0.88) 77.5 0.82 (0.78, 0.86)\nLi and Lee 1993 75.9 0.82 (0.78, 0.86) 77.0 0.79 (0.76, 0.83)\nPikaz and Averbuch 1996 72.9 0.68 (0.65, 0.71) 74.1 0.67 (0.64, 0.70)\nMardia and Hainsworth 1988 72.1 0.66 (0.63, 0.69) 74.1 0.67 (0.64, 0.70)\nYanowitz and Bruckstein 1989 68.9 0.56 (0.53, 0.58) 68.9 0.51 (0.48, 0.53)\nBernsen 1986 17 67.0 0.51 (0.49, 0.53) 67.6 0.47 (0.45, 0.50)\nNiblack 1986 31 66.7 0.50 (0.48, 0.52) 66.8 0.45 (0.43, 0.47)\nYanni and Horne 1994 63.3 0.42 (0.41, 0.44) 65.3 0.42 (0.41, 0.44)\nOtsu 1979 62.7 0.41 (0.40, 0.43) 64.6 0.41 (0.39, 0.43)\nRidler and Calvard 1978 62.3 0.40 (0.39, 0.42) 64.2 0.40 (0.39, 0.42)\nLloyd 1985 60.2 0.37 (0.35, 0.38) 62.7 0.37 (0.36, 0.39)\nKittler and Illingworth 1986 58.2 0.34 (0.32, 0.35) 60.4 0.34 (0.32, 0.35)\nJawahar et al. 1997 57.3 0.32 (0.31, 0.34) 60.5 0.34 (0.33, 0.36)\nRamesh et al. 1995 56.0 0.31 (0.29, 0.32) 59.4 0.32 (0.31, 0.34)\nSauvola and Pietaksinen 2000 31 55.5 0.30 (0.29, 0.31) 58.4 0.30 (0.29, 0.32)\nShanbhag 1994 53.2 0.28 (0.27, 0.29) 60.9 0.36 (0.34, 0.37)\nKapur et al. 1985 43.4 0.18 (0.17, 0.19) 51.4 0.23 (0.22, 0.24)\nAbutaleb 1989 39.9 0.15 (0.15, 0.16) 45.1 0.17 (0.17, 0.18)\nLeung and Lam 1998 37.6 0.14 (0.13, 0.14) 53.0 0.24 (0.23, 0.25)\nWhite and Rohrer 1983 15 36.5 0.13 (0.12, 0.14) 42.0 0.15 (0.14, 0.16)\nYen et al. 1995 23.7 0.07 (0.06, 0.07) 41.5 0.15 (0.14, 0.16)\nDunn et al. 1984 2.7 0.01 (0.01, 0.01) 13.2 0.03 (0.03, 0.03)\nTable 1 . Overall results. All window-based algorithms are evaluat ed at their best window size. In addition to general\nrecall and precision rates, more precise estimates of the od ds multipliers and their 95% conﬁdence intervals are given.Figure 3 . Recognition performance (recall) of Gatos et\nal. 2004, the top-performing window-based local binari-\nsation algorithm, on each of the books in the test set. Note\nthat there is no one optimal threshold.\n(1994), Yen et al. (1995), and Brink and Pendock (1996).\nWe chose two object attribute methods, Pikaz and Aver-\nbuch (1996) and Leung and Lam (1998), and one spatial\ninformation method, Abutaleb (1989). Local threshold-\ning techniques, which rely on the properties of a small\nwindow surrounding each pixel are commonly used de-\nspite their computational expense; we chose the meth-\nods of White and Rohrer (1983), Bernsen (1986), Niblack\n(1986), Mardia and Hainsworth (1988), Yanowitz and Bruck-\nstein (1989), Sauvola and Pietaksinen (2000), and Gatos\net al. (2004). Full references for these algorithms may be\nfound in [1, 2, 9]. Finally, we developed a new algorithm\nthat considers binarisation to be not a 2-class but a 3-class ,\nforeground–bleed-through–background problem [6].\n3 RESULTS\nAlthough there is no standard metric for performance in\nOMR, Aruspix provides symbol-level recall and precision\nrates, as is standard in speech recognition and many other\ntasks in information retrieval. The appropriate statisti-\ncal tool for analysing precision and recall across a data\nset is binomial regression, also known as logistic regres-\nsion. The regression parameters are best interpreted as\nodds multipliers relative to some baseline such as a well-\nknown or high-performing algorithm [5]. In this paper, we\nhave taken the baseline to be the performance of our best\nalgorithm, which means that the odds multipliers range\nconveniently from 0 to 1 and represent the fraction of best\npossible performance one can expect from an algorithm.\nBefore assessing performance across the data set, we\nneeded to ﬁnd the best window sizes for the locally adap-\ntive algorithms. Previous work had suggested that the\nideal region size was between 8 and 32 pixels for nor-Figure 4 . Recognition performance (recall) of all algo-\nrithms across the entire test set. One can see that the best\nalgorithms perform fairly consistently while the poor al-\ngorithms lack consistency.\nmalised staves, or between 1/2and 1 1/2staff spaces [7].\nSomewhat surprisingly, we found that there is no optimal\nregion size. Figure 3, for example, shows the recogni-\ntion performance of Gatos et al. 2004 for each book in\nthe test set. For two books, the performance increases\nwith window size for a time and then plateaus. For two\nothers, the performance peaks at a window size of about\n18 pixels and begins to decrease signiﬁcantly as window\nsize increases further. If binarisation had to be performed\nbefore staff recognition, tuning this parameter would be\neven more difﬁcult if not impossible. Despite their advan-\ntages on manuscripts with markedly uneven (or stained)\nbackground, there is good reason to seek non-parametric\nalternatives to these locally adaptive algorithms.\nFortunately, there are a number of non-parametric al-\ngorithms that perform much better than the window-based\nset. Table 1 presents a complete list of the performance of\nall algorithms. It is divided into the two traditional met-\nrics of recall , or the percentage of symbols on the page\nthat were found during the OMR process, and precision ,\nor the percentage of symbols in the OMR output that were\nin fact on the page. In addition to overall recall and preci-\nsion rates, we present the odds multiplier scores, which\nare a much more accurate means of comparing the al-\ngorithms. The odds multipliers are presented along withtheir 95-percent conﬁdence intervals to enable the reader\nto identify where the rankings are statistically signiﬁcan t\nand where they represent clusters. Wherever the conﬁ-\ndence intervals are separated, e.g., between Niblack 1986,\nwith a low point of 0.48, and Yanni and Horne, with a high\npoint of 0.44, the difference is statistically signiﬁcant.\nTable 1 is ordered by recall performance, which is the\nmost important measure to optimise when trying to reduce\nhuman editing costs after the OMR process. The clear\nwinner is Brink and Pendock 1996, which performs sig-\nniﬁcantly better than all others in both performance and\nrecall; a sample of its output appears in ﬁgure 2a. The new\nPugin 2007 algorithm is a close second. The only locally\nadaptive algorithm to perform well was Gatos et al. 2004\n(see ﬁgure 2b), a variant of Niblack 1986 that has been de-\nsigned especially to treat document degradation. There is\na large and signiﬁcant performance gap in both recall and\nprecision after Li and Lee 1993, which suggests that OMR\nresearchers should concentrate their efforts on the top ﬁve\nalgorithms in the table. None of them has received much\nattention to date, and indeed, the most commonly used\nbinarisation algorithm is the notably mediocre Otsu 1979\n(see ﬁgure 2c).\nA more visual representation of some of the data in\nTable 1 appears in ﬁgure 4. This ﬁgure is a box-and-\nwhisker plot on recall performance for every image in the\ntest set. The whiskers extend to the maximum and min-\nimum performance for each algorithm, excepting cases\nwhere the performance is so high or low that it should\nbe considered an unrepresentative outlier. These outliers\nare marked with small crossed boxes and tend to occur for\nthe most difﬁcult images in the set. The open white rect-\nangles, which range from the ﬁrst to third quartiles, give\na visual cue to the amount of variance in each algorithm,\nand the line in the centre of each box denotes median per-\nformance. The most interesting aspect of this diagram is\nthat the difference among these algorithms is not their best\nperformance, which is acceptable for almost all of them,\nbut rather their consistency of performance. The algo-\nrithms that rank poorly overall do so because their per-\nformance is widely variable, which puts undue pressure\non the OMR process and makes quality control difﬁcult.\nThe best algorithms, in contrast, perform well not just on\naverage but consistently almost all the time.\nAs mentioned earlier, we selected these 24 algorithms\nin particular because they have historically performed wel l\non either or both of document and NDT images. Our re-\nsults, however, differ considerably from Sezgin and San-\nkur’s evaluations on these two classes of algorithms. Brink\nand Pendock 1996, for example, is only an average per-\nformer on Sezgin and Sankur’s document images and the\nworst performer on NDT images. Sauvola and Pietksi-\nnen 2000 and Kittler and Illingworth 1986, in contrast, are\nSezgin and Sankur’s top performers for document images\nwhereas they only obtain recall scores of 0.34 and 0.30\nhere. These differences conﬁrm the special nature of mu-\nsical documents and the necessity of developing a distinct\nset of image processing techniques for them.4 SUMMARY AND FUTURE WORK\nUsing a quantitative, goal-directed evaluation technique ,\nwe have performed an analysis of unprecedented scope for\nOMR. The project synthesises the most important surveys\nof image binarisation techniques and supplements them\nwith the most recent work in the ﬁeld. The results demon-\nstrate the value of music-speciﬁc methods for image pro-\ncessing and provide a music-speciﬁc performance refer-\nence for the most successful binarisation algorithms in the\nﬁeld. The particular success of the three-class model in\nPugin 2007 suggests that the other best-performing algo-\nrithms could be adapted fruitfully to improve their perfor-\nmance still further on documents suffering from difﬁcult\nbleed-through.\n5 ACKNOWLEDGEMENTS\nWe would like to thank the Canada Foundation for Inno-\nvation and the Social Sciences and Humanities Research\nCouncil of Canada for their ﬁnancial support.\n6 REFERENCES\n[1] S. Bøe, “XITE – X-based image processing tools and envi-\nronment – Reference manual for version 3.45,” Image Pro-\ncessing Laboratory, Dept. of Informatics, Univ. of Oslo,\nTech. Rep., September 2004.\n[2] B. Gatos, I. Pratikakis, and S. J. Perantonis, “An adaptive\nbinarisation technique for low quality historical documents,”\ninProc. 6th Int. Work. Doc. Anal. Sys. , pp. 102–13.\n[3] E. Kavallieratou and E. Stamatatos, “Adaptive bina-\nrization of historical document images,” in Proc. 18th\nInt. Conf. Pat. Rec. , pp. 742–45.\n[4] G. Leedham, S. Varma, A. Patankar, and V . Govindarayu,\n“Separating text and background in degraded document im-\nages – a comparison of global threshholding techniques for\nmulti-stage threshholding,” in Proceedings of the Eighth In-\nternational Workshop on Frontiers in Handwriting Recogni-\ntion (IWFHR’02) , 2002.\n[5] P. McCullagh and J. A. Nelder, Generalized Linear Models ,\n2nd ed. London: Chapman and Hall, 1989.\n[6] L. Pugin, “A new binarisation algorithm for documents with\nbleed-through,” McGill University, Montreal, Tech. Rep.\nMUMT-DDMAL-07-01, March 2007.\n[7] L. Pugin, J. A. Burgoyne, and I. Fujinaga, “Goal-directed\nevaluation for the improvement of optical music recognition\non early music prints,” in Proc. ACM/IEEE Joint Conf. Dig-\nital Libraries , Vancouver, Canada, 2007, pp. 303–04.\n[8] R ´epertoire international des sources musicales (RISM), Sin-\ngle Prints Before 1800 , ser. Series A/I. Kassel: B ¨arenreiter,\n1971–81.\n[9] M. Sezgin and B. Sankur, “Survey over image thresholding\ntechniques and quantitative performance evaluation,” Jour-\nnal of Electronic Imaging , vol. 13, no. 1, pp. 146–65, 2004.\n[10] Ø. D. Trier and A. K. Jain, “Goal-directed evaluation of bi-\nnarization methods,” IEEE Transactions on PAMI , vol. 17,\nno. 12, pp. 1191–1201, 1995."
    },
    {
        "title": "A Cross-Validated Study of Modelling Strategies for Automatic Chord Recognition in Audio.",
        "author": [
            "John Ashley Burgoyne",
            "Laurent Pugin",
            "Corey Kereliuk",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416816",
        "url": "https://doi.org/10.5281/zenodo.1416816",
        "ee": "https://zenodo.org/records/1416816/files/BurgoynePKF07.pdf",
        "abstract": "Although automatic chord recognition has generated a number of recent papers in MIR, nobody to date has done a proper cross validation of their recognition results. Cross validation is the most common way to establish baseline standards and make comparisons, e.g., for MIREX competitions, but a lack of labelled aligned training data has rendered it impractical. In this paper, we present a comparison of several modelling strategies for chord recognition, hidden Markov models (HMMs) and conditional random fields (CRFs), on a new set of aligned ground truth for the Beatles data set of Sheh and Ellis (2003). Consistent with previous work, our models use pitch class profile (PCP) vectors for audio modelling. Our results show improvement over previous literature, provide precise estimates of the performance of both old and new approaches to the problem, and suggest several avenues for future work. 1 INTRODUCTION The task of automatic, continuous chord recognition is an area of active study in the MIR community. When working with audio, chord recognition is an especially difficult task because a chord represents such a wide range of possible musical events. Recent studies have shown the benefit of applying stochastic modelling to this task [1, 7, 8, 11, 13]. The most commonly used model is the hidden Markov model (HMM) [10], but more recent work has also explored discriminative models [9] such as the conditional random field (CRF) [14]. In this paper, we use the work of Sheh and Ellis as our departure point [13]. These authors used HMMs to perform chord recognition on a set of 20 Beatles songs. Although their recognition rates were poor, they laid a foundation for future study. We use the same data set, but in addition to their tests, we perform a 10-fold cross validation to verify the validity of our results, training on 18 songs for each run and testing on the remaining 2. Cross validation is essential for obtaining unbiased estimates of model performance when data is limited [5], but because c⃝2007 Austrian Computer Society (OCG). it is so time-consuming to label audio files, no previous studies of audio chord recognition have tried it. Crossvalidated recognition rates will be lower than the best possible test rate on a single song, e.g., the metric used in [7], but they give a more realistic depiction of the state of the art and are the only fair way to compare different models. Another problem that is often encountered when building HMMs is a lack of aligned, labelled training data. When the training data is not aligned, the model must be initialised using a so-called flat start. In a flat start, training audio is uniformly segmented based on an unaligned transcription. One hopes that enough of the uniformly segmented labels in the flat start will match the correct alignment so that the model parameters will improve during successive training iterations, but this is unlikely in musical applications because chord lengths vary so widely. Using a flat-start with this training data has indeed been shown to result in poor recognition performance [13], and so for our training, we avoided it. Our results demonstrate the usefulness of stochastic modelling and highlight the benefits of CRFs, which until now have received very little attention in the MIR community. 2 PITCH CLASS PROFILE VECTORS Pitch class profile (PCP) vectors, which were introduced by Fujishima in 1999 [3], have been widely used in chord recognition systems [1, 4, 6, 13]. In essence, PCP vectors represent a logarithmically warped and wrapped version of the short time frequency spectrum. The following equations show the computation of a PCP vector: PCP[i] ≜ X {k : p[k]=i} ∥X[k]∥2 (1) p[k] ≜ \u001a round \u0014 D log2 \u0012 k N · fs fref \u0013\u0015\u001b mod D (2) In equation 2, k is an FFT bin, fref is the reference pitch, N is the FFT window size, fs is the sampling rate, and D is the dimensionality of the PCP vector. The log2 operation warps the frequency spectrum to a logarithmic scale, while the modulo operation wraps the frequency spectrum at integer multiples (octaves) of the reference frequency. The frequency components in each of the warped and wrapped frequency bands are then summed (equation 1). In this study, we used D = 12 and fref = 261.6 Hz (C4). Thus, each PCP vector represents 12 semi-tones of a chromatic scale under the same modulus as most Western music theorists as well as the MIDI note number specification.",
        "zenodo_id": 1416816,
        "dblp_key": "conf/ismir/BurgoynePKF07",
        "keywords": [
            "automatic chord recognition",
            "MIR community",
            "cross validation",
            "hidden Markov models (HMMs)",
            "conditional random fields (CRFs)",
            "Beatles data set",
            "pitch class profile (PCP) vectors",
            "stochastic modelling",
            "CRFs",
            "audio chord recognition"
        ],
        "content": "A CROSS-V ALIDATED STUDY OF MODELLING STRATEGIES\nFOR AUTOMATIC CHORD RECOGNITION IN AUDIO\nJohn Ashley Burgoyne Laurent Pugin Corey Kereliuk Ichiro Fuji naga\nCentre for Interdisciplinary Research in Music and Media Tech nology\nSchulich School of Music of McGill University\nMontr ´eal, Qu ´ebec, Canada H3A 1E3\n{ashley,laurent,corey,ich }@music.mcgill.ca\nABSTRACT\nAlthough automatic chord recognition has generated a num-\nber of recent papers in MIR, nobody to date has done a\nproper cross validation of their recognition results. Cros s\nvalidation is the most common way to establish baseline\nstandards and make comparisons, e.g., for MIREX com-\npetitions, but a lack of labelled aligned training data has\nrendered it impractical. In this paper, we present a com-\nparison of several modelling strategies for chord recogni-\ntion, hidden Markov models (HMMs) and conditional ran-\ndom ﬁelds (CRFs), on a new set of aligned ground truth\nfor the Beatles data set of Sheh and Ellis (2003). Con-\nsistent with previous work, our models use pitch class\nproﬁle (PCP) vectors for audio modelling. Our results\nshow improvement over previous literature, provide pre-\ncise estimates of the performance of both old and new ap-\nproaches to the problem, and suggest several avenues for\nfuture work.\n1 INTRODUCTION\nThe task of automatic, continuous chord recognition is an\narea of active study in the MIR community. When work-\ning with audio, chord recognition is an especially difﬁ-\ncult task because a chord represents such a wide range\nof possible musical events. Recent studies have shown\nthe beneﬁt of applying stochastic modelling to this task\n[1, 7, 8, 11, 13]. The most commonly used model is the\nhidden Markov model (HMM) [10], but more recent work\nhas also explored discriminative models [9] such as the\nconditional random ﬁeld (CRF) [14].\nIn this paper, we use the work of Sheh and Ellis as our\ndeparture point [13]. These authors used HMMs to per-\nform chord recognition on a set of 20 Beatles songs. Al-\nthough their recognition rates were poor, they laid a foun-\ndation for future study. We use the same data set, but in\naddition to their tests, we perform a 10-fold cross vali-\ndation to verify the validity of our results, training on 18\nsongs for each run and testing on the remaining 2. Cross\nvalidation is essential for obtaining unbiased estimates o f\nmodel performance when data is limited [5], but because\nc/circlecopyrt2007 Austrian Computer Society (OCG).it is so time-consuming to label audio ﬁles, no previous\nstudies of audio chord recognition have tried it. Cross-\nvalidated recognition rates will be lower than the best pos-\nsible test rate on a single song, e.g., the metric used in [7],\nbut they give a more realistic depiction of the state of the\nart and are the only fair way to compare different models.\nAnother problem that is often encountered when build-\ning HMMs is a lack of aligned, labelled training data.\nWhen the training data is not aligned, the model must\nbe initialised using a so-called ﬂat start . In a ﬂat start,\ntraining audio is uniformly segmented based on an un-\naligned transcription. One hopes that enough of the uni-\nformly segmented labels in the ﬂat start will match the\ncorrect alignment so that the model parameters will im-\nprove during successive training iterations, but this is un -\nlikely in musical applications because chord lengths vary\nso widely. Using a ﬂat-start with this training data has\nindeed been shown to result in poor recognition perfor-\nmance [13], and so for our training, we avoided it.\nOur results demonstrate the usefulness of stochastic\nmodelling and highlight the beneﬁts of CRFs, which until\nnow have received very little attention in the MIR com-\nmunity.\n2 PITCH CLASS PROFILE VECTORS\nPitch class proﬁle (PCP) vectors, which were introduced\nby Fujishima in 1999 [3], have been widely used in chord\nrecognition systems [1, 4, 6, 13]. In essence, PCP vec-\ntors represent a logarithmically warped and wrapped ver-\nsion of the short time frequency spectrum. The following\nequations show the computation of a PCP vector:\nPCP[i]/defines/summationdisplay\n{k:p[k]=i}/bardblX[k]/bardbl2(1)\np[k]/defines/braceleftbigg\nround/bracketleftbigg\nDlog2/parenleftbiggk\nN·fs\nfref/parenrightbigg/bracketrightbigg/bracerightbigg\nmodD(2)\nIn equation 2, kis an FFT bin, frefis the reference pitch,\nNis the FFT window size, fsis the sampling rate, and D\nis the dimensionality of the PCP vector. The log2opera-\ntionwarps the frequency spectrum to a logarithmic scale,\nwhile the modulo operation wraps the frequency spec-\ntrum at integer multiples (octaves) of the reference fre-\nquency. The frequency components in each of the warpedand wrapped frequency bands are then summed (equation\n1). In this study, we used D=12 and fref=261.6 Hz\n(C4). Thus, each PCP vector represents 12 semi-tones of a\nchromatic scale under the same modulus as most Western\nmusic theorists as well as the MIDI note number speciﬁ-\ncation.\n2.1 Gaussian Distributions\nThe choice of parameters used to model a PCP vector is\nnot trivial. Each dimension in the normalised PCP vec-\ntor has a continuous output and thus can be modelled as a\nprobability distribution. In [13], Sheh and Ellis modelled\nPCP vectors using single Gaussians. Using our labelled\ntraining data, it is possible to show that single Gaussians\ndo not provide an adequate model of the PCP vectors.\nFigure 1 shows 3 examples of a single Gaussian superim-\nposed on 1 dimension of an A-minor PCP vector. Clearly\na mixture of Gaussians would be more suitable to accu-\nrately model the shape of these distributions. In this paper ,\nwe used a mixture of Gaussians to model the probability\ndistributions of the PCP vectors, and as can be seen from\nthe results, the models trained using a mixture of Gaus-\nsians outperform those using single Gaussians.\n2.2 Dirichlet Distributions\nBecause all values in a normalised PCP vector must be\nnon-negative and sum to one, one may think of them as\nthe parameters to a hypothetical multinomial distribution .\nAlthough mixtures of Gaussians can theoretically be used\nto model any probability distribution, for multinomials,\nthere is another common model known as the Dirichlet\ndistribution. This distribution is the so-called conjugat e\nprior of the multinomial distribution, i.e., given a set of p a-\nrameters that represent an archetypal multinomial distri-\nbution, it represents the probability that any other multin o-\nmial distribution might arise instead. Unlike mixtures of\nGaussians, Dirichlets enforce the constraint that the outp ut\ndistributions be valid multinomial distributions, which i s\nequivalent to the constraints on valid normalised PCP vec-\ntors. Moreover, they require fewer parameters to train:\nDir(p,u)/defines1\nZ(u)D/productdisplay\ni=1pui−1\ni (3)\nThepicorrespond to the bins of the multinomial distri-\nbution, which are the 12 values of the PCP vector in our\ncase. The parameter vector u={u1,u2,... ,u D}, where\nDis the number of bins in the multinomial, determines\nboth the mean and the variance of the distribution. The\nnormalisation term Z(u)is beyond the scope of this pa-\nper, but more information can be found in [2], an earlier\napplication of Dirichlet distributions to chord recogniti on.3 CHORD SEQUENCE MODELS\n3.1 Hidden Markov Models\nHMMs are generative stochastic models that attempt to\nmodel a hidden ﬁrst-order Markov process based on a set\nof observable outputs. Nonetheless, it should be noted\nthat in reality chord progressions are high-order Markov\nprocesses, and so the ﬁrst-order Markov assumption may\nresult in model deﬁciencies.\nHMMs for chord recognition have been used with two\ndifferent approaches. Sheh and Ellis [13] have experi-\nmented with a model-discriminant (MD) approach, which\nmeans that every potential chord is modelled by its own\nleft-right, single-state HMM. Each model is trained ei-\nther individually or by using an embedded version of the\nBaum-Welch algorithm that concatenates the HMMs. In\norder to perform this step, one needs labelled—but not\nnecessarily aligned—training data. Chord recognition is\nperformed using the Viterbi algorithm on the network of\ncomponent HMMs to yield the most likely sequence of\ncomponent models . Bello and Pickens [1] and Lee and\nSlaney [6], on the other hand, have experimented with a\npath-discriminant (PD) approach, in which every chord is\nmodelled by one state in a larger, fully connected HMM,\nwhich can be trained with the expectation-maximisation\n(EM) algorithm and does not require labelled data. Chord\nrecognition with the PD approach uses the standard Viterbi\nalgorithm to obtain the most likely sequence of states .\n3.2 Conditional Random Fields\nGiven a sequence of observations X, HMMs seek to max-\nimise the joint probability P(X,Y)for a hidden state se-\nquence Y. This method works well in practise, but from\na theoretical perspective, it is not quite the question that\none ought to be asking at recognition time. During recog-\nnition, the observation sequence is always ﬁxed, and so it\nmay make more sense to model only the conditional prob-\nability distribution P(Y|X). This frees the recogniser\nfrom needing to enforce any particular model P(X)of the\ndata, and thus it is no longer necessary for the components\nof the observation vectors to be conditionally independent ,\nas they must be for HMMs. Such a model may include\nthousands or even millions of observation features.\nThe closest analogue to the HMM that uses this mod-\nelling technique is known as the linear-chain CRF, one\nof the most commonly used member of the larger CRF\nfamily [14]. Besides the probabilistic characteristics me n-\ntioned above, CRFs differ from the HMMs in that each\nhidden state depends not just on the current observation\nbut on the complete observation sequence. At decoding\ntime, linear-chain CRFs are quite similar to HMMs, us-\ning a variant of the Viterbi algorithm. Unlike an HMM,\nhowever, in order to train a linear-chain CRF, one must\nhave access to fully labelled and aligned training data.\nTraining is considerably slower for CRFs than it is for an\nHMMs regardless of whether one uses a path-discriminant\nor model-discriminant approach, but fortunately, there ar eFigure 1 . Single Gaussian (smooth curve) plotted over three differe nt dimensions of a PCP vector (histogram).\nRoots: A, A ♭, B, B♭, C, D, D ♭, E, E♭, F, F♯, G\nFamilies: maj, min, aug, dim\nExamples: A, Bm, E+, Fdim\nTable 1 . List of chords used in recognition experiments.\noptimisation techniques to improve the training speed. We\nchose the limited-memory Broyden–Fletcher–Goldfarb–\nShanno method, a variant of Newton’s method [12].\n4 EXPERIMENTS AND RESULTS\nThe Beatles recordings in our data set were ﬁltered and re-\nsampled at 11 025 Hz to remove the high-frequency con-\ntent. An STFT was calculated on each song using an\nFFT size of 2048 samples and a hop size of 1024 samples\n(92 ms). From the STFT, 12-dimensional PCP vectors\nwere calculated (for simplicity, we did not attempt the tun-\ning adjustments used in [1] or [4]), and given knowledge\nabout the original key of each song, a second set of PCP\nvectors was was generated by transposing (rotating) the\nﬁrst set to C major so as to reduce the chances of learning\nkey-dependent harmonic relationships. (In a large-scale\napplication, an automatic key-ﬁnding algorithm could be\nused for this purpose.) Each song was hand-transcribed\nwith chord labels, and these labels were then simpliﬁed to\ntriads only: major, minor, augmented, and diminished (see\ntable 1). These labels are a departure from Sheh and Ellis,\nwho attempted to recognise 7th chords as well, but we felt\nthat the data set was insufﬁcient to estimate so many mod-\nels properly. When used with the rotated PCP vectors,\nthe labels were also transposed to C major. After sim-\npliﬁcation, both the rotated and unrotated sets of chord\nlabels contained 24 distinct chord symbols out of the 48\nthat would have been theoretically possible.\nThe HMM-MD model was implemented using the Hid-\nden Markov Model Toolkit (HTK)1with single-state com-\nponent models and 1, 6, or 12 Gaussians to model each\nPCP bin. For the HMM-PD, an ergodic (fully-connected)\nHMM with one state for each chord of the list was trained\n1http://htk.eng.cam.ac.uk/Model Gau.Recognition rate (%)\nRotated Unrotated\nHMM-PD 1 24.2 28.8\nHMM-PD 6 31.9 34.1\nHMM-PD 12 37.9 36.1\nHMM-PD 20 45.1 40.5\nHMM-MD 1 37.1 39.7\nHMM-MD 6 48.8 45.5\nHMM-MD 12 48.8 47.1\nHMM-PD 24 48.7 31.6\nCRF-D – 39.5 45.3\nCRF-G 1 34.7 29.9\nCRF-DG 1 39.9 42.4\nTable 2 . Frame-by-frame recognition results for all mod-\nels with varying numbers of Gaussians.\nusing the Torch machine learning library.2During ini-\ntialisation, every portion of the labelled data was assigne d\nto its corresponding state. We experimented using 1, 6,\n12 and 20 Gaussians per state. Training took a couple of\nseconds on a 2.7 GHz PowerPC G5 processor.\nThe linear-chain CRFs3were trained with transition\nfeatures between all chords in the training sets and special\nfeatures denoting starting and ending symbols. We tried\nthree versions of the emission features, the ﬁrst equivalen t\nto a single Gaussian (CRF-G), the second equivalent to a\nDirichlet distribution (CRF-D), and the third a combina-\ntion of both sets of emission features (CRF-GD), taking\nadvantage of the fact that the features in CRFs need not be\nindependent for the model to run properly. The L-BFGS\noptimisation routine was allowed to run for 250 iterations\nwith a constraint on the parameters that their standard de-\nviation be no more than 10. The purpose of these limits\nwas to avoid over-training, which is a particular risk with\nCRFs. It took four to six hours to train each run of each\nmodel on a 2.7 GHz PowerPC G5 processor.\nOur results are summarised in table 2. Our evaluation\nwas done by carrying out a frame-by-frame comparison\n2http://www.torch.ch/\n3http://crf.sourceforge.net/of the recognised labels with the hand marked labels. The\nnumber of correct frames overall was divided by the total\nnumber of frames overall in order to give a percentage\nscore to the recognition. Unlike some other papers, e.g.,\n[7], we did not allow for any fuzziness in recognition at\nthe boundaries, and so the ﬁgures represented here will be\nlower but more precise. Results are presented for several\nmixture sizes of Gaussians.\nThe simplest model here, the path-discriminant HMM\n(HMM-PD), also performs the worst. When PCP bins are\nmodelled as single Gaussians, its best performance is 28.8\npercent. The model-discriminant HMM (HMM-MD), in\ncontrast, performs much better, at 39.7 percent even with a\nsingle Gaussian and reaching 48.8 percent with 12 Gaus-\nsians. At 24 Gaussians, over-training starts to reduce per-\nformance, especially for the unrotated vectors. This model\nis the same as in [13], but our best recognition rates are\na more than twofold improvement over theirs using the\nsame training set and evaluation script. We speculate that\nthe large difference is due to the inclusion of a mixture\nof Gaussians, the exclusion of a ﬂat-start during model\ntraining, and a reduction of the classiﬁcation set to triads .\nFor both HMM-PD and -MD, the unrotated PCP vectors\nperform better with smaller numbers of Gaussians and the\nrotated PCP vectors become slightly better as the the num-\nber of Gaussians increases. Although the differences be-\ntween performance on the two PCP sets is never more than\nﬁve percentage points for the HMMs, this pattern warrants\nfurther investigation.\nAt the single-Gaussian level (CRF-G), CRFs perform\nbetter than the PD HMMs but do not quite match the per-\nformance of the MD HMMs, perhaps on account of over-\ntraining; unlike the HMMs, the single-Gaussian CRFs per-\nform much better on rotated PCPs than unrotated. The\nmost interesting feature of the CRF results, however, is\nthe large improvement in performance when using Dirich-\nlet distributions (CRF-D and CRF-DG). Although CRF-D\nis not quite able to match HMM-MD performance at its\nmaximal number of Gaussians, it comes very close to it\nwhile using a factor of 40 fewer model parameters. There\nis no question that these distributions warrant further stu dy\nfor chord recognition.\n5 SUMMARY AND FUTURE WORK\nWe presented a comparison of both traditional and new\napproaches, HMMs and CRFs, to audio chord recognition\nusing PCP vectors. Overall, our results compare favourably\nwith previous research for this task, but they also suggest\nthat on their own, PCP vectors may not be sufﬁcient for\nreliable discrimination. Moreover, we were able to per-\nform our experiments with a fully annotated training set\nof live recordings, which is rare for the ﬁeld. These anno-\ntations allowed us to cross-validate our results for a more\naccurate representation of the state of the art.\nOur results suggest that further research is needed in\nmodelling audio features for chord recognition. One ap-\nproach would be to incorporate Dirichlet distributions mor ewidely, e.g., in the HMM-based models we used. An-\nother would be to investigate alternatives or supplements\nto PCP vectors, e.g., [6]. Both should be explored as au-\ndio chord recognition enters the MIREX competitions in\ncoming years.\n6 ACKNOWLEDGEMENTS\nWe would like to thank the Canada Foundation for Inno-\nvation and the Social Sciences and Humanities Research\nCouncil of Canada for ﬁnancial support, Tristan Matthews\nfor preparing the ground truth, and Aaron Courville for his\nadvice about stochastic models.\n7 REFERENCES\n[1] J. P. Bello and J. Pickens, “A robust mid-level representa-\ntion for harmonic content in music signals,” in Proc. 6th\nInt. Conf. Mus. Inf. Ret. , 2005, pp. 304–11.\n[2] J. A. Burgoyne and L. K. Saul, “Learning harmonic relation-\nships in digital audio with Dirichlet-based hidden Markov\nmodels,” in Proc. 6th Int. Conf. Mus. Inf. Ret. , 2005, pp.\n438–43.\n[3] T. Fujishima, “Realtime chord recognition of musical\nsound: A system using Common Lisp Music,” in\nProc. Int. Comp. Mus. Conf. , 1999.\n[4] C. A. Harte and M. B. Sandler, “Automatic chord iden-\ntiﬁcation using a quantised chromagram,” in Proc. 118th\nConv. Aud. Eng. Soc. , 2005.\n[5] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of\nStatistical Learning . Berlin: Springer, 2001.\n[6] K. Lee, “Automatic chord recognition from audio using en-\nhanced pitch class proﬁle,” in Proc. Int. Comp. Mus. Conf. ,\n2006.\n[7] K. Lee and M. Slaney, “Automatic chord recognition from\naudio using an HMM with supervised learning,” in Proc. 7th\nInt. Conf. Mus. Inf. Ret. , 2006, pp. 133–7.\n[8] J.-F. Paiement, D. Eck, and S. Bengio, “A proba-\nbilistic model for chord progressions,” in Proc. 6th\nInt. Conf. Mus. Inf. Ret. , 2005, pp. 312–9.\n[9] G. E. Poliner and D. P. W. Ellis, “A discriminative model\nfor polyphonic piano transcription,” EURASIP Journal on\nAdvances in Signal Processing , forthcoming.\n[10] L. R. Rabiner, “A tutorial on hidden Markov models and\nselected applications in speech recognition,” Proc. IEEE ,\nvol. 77, pp. 257–87, 1989.\n[11] C. Sailer and K. Rosenbauer, “A bottom-up approach to\nchord detection,” in Proc. Int. Comp. Mus. Conf. , 2006, pp.\n612–15.\n[12] F. Sha and F. Pereira, “Shallow parsing with conditional ran-\ndom ﬁelds,” in Proc. Human Lang. Tech. Conf. , 2003, pp.\n213–20.\n[13] A. Sheh and D. P. W. Ellis, “Chord segmentation and\nrecognition using EM-trained hidden Markov models,” in\nProc. 4th Int. Conf. Mus. Inf. Ret. , 2003, pp. 185–91.\n[14] C. Sutton and A. McCallum, “An introduction to conditional\nrandom ﬁelds for relational learning,” in Introduction to Sta-\ntistical Relational Learning , L. Getoor and B. Taskar, Eds.\nMIT Press, 2006."
    },
    {
        "title": "Monaural Source Separation from Musical Mixtures Based on Time-Frequency Timbre Models.",
        "author": [
            "Juan José Burred",
            "Thomas Sikora"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416396",
        "url": "https://doi.org/10.5281/zenodo.1416396",
        "ee": "https://zenodo.org/records/1416396/files/BurredS07.pdf",
        "abstract": "We present a system for source separation from monaural musical mixtures based on sinusoidal modeling and on a library of timbre models trained a priori. The models, which rely on Principal Component Analysis, serve as time-frequency probabilistic templates of the spectral envelope. They are used to match groups of sinusoidal tracks and assign them to a source, as well as to reconstruct overlapping partials. The proposed method does not make any assumptions on the harmonicity of the sources, and does not require a previous multipitch estimation stage. Since the timbre matching stage detects the instruments present on the mixture, the system can also be used for classification and segmentation. 1 INTRODUCTION Separation of a musical mixture into its sources can greatly facilitate content analysis for Music Information Retrieval purposes, and allows other applications like remixing or upmixing to a larger number of channels than the original if multitrack recordings are not available. We address separation from a single channel, which is a highly underdetermined problem that requires either strong assumptions about the nature of the sources, a fair amount of a priori information, or a combination of both. The main assumption taken in underdetermined separation is the sparsity of the sources, which leads to the use of elaborate signal models. An example thereof is the use of Nonnegative Sparse Coding [1]. Other approaches are based on sinusoidal modeling, which allows a detailed handling of overlapping partials and is also a highly sparse model. They are based on grouping the extracted partials according to Auditory Scene Analysis cues. In [2], amplitude smoothness is modeled by performing basis decomposition on the harmonic structures and their evolution in time. In [3], spectral filtering techniques are used to resolve overlapping sinusoids. Most approaches based on sinusoidal modeling rely either on a previous multipitch estimation stage or on the knowledge of the pitch score of the mixture [2, 3]. c⃝2007 Austrian Computer Society (OCG). The above methods are unsupervised (i.e., there is no training phase) and are based on generic source models. To further improve separation, statistical models of the sources can be trained beforehand on a database of isolated source samples. Examples of this supervised approach include the use of learnt spectral priors with bayesian harmonic models [4] and the derivation of templates for timbral features [5]. In the present contribution, we propose a system for the separation of sources from single-channel mixtures of musical instruments based on sinusoidal modeling and on a library of pre-trained timbre models. Since it also outputs onset/offset information and the instrument each note belongs to, it can also be used for segmentation or polyphonic instrument recognition. The timbre models are time-frequency templates that describe in detail spectral shapes and their evolution in time. In contrast to most previously existing approaches, no assumptions on harmonicity are made, which allows to separate highly inharmonic sounds or to separate chords played by a single instrument. Furthermore, no previous multipitch estimation or any kind of a priori pitch-related score is needed. Instead, separation is solely based on common onset properties of the partials, and on the analysis of the evolution in time of the spectral envelope they define. The knowledge of the number and names of the instruments is not mandatory, but will obviously increase the performance. Figure 1 shows an overview of the proposed separation system. First, the mixture signal is subjected to sinusoidal modeling, obtaining a set of sinusoidal tracks. A simple onset detector based on identifying synchronously starting tracks then allows to select the partial tracks that are going to be matched with the timbre models in the next stage. After each common-onset group of partial tracks has been assigned to an instrument, the overlapping part of the tracks is retrieved from the models. Finally, the separated tracks are synthesized using additive synthesis. 2 TRAINING OF THE TIMBRE MODELS The used timbre models are based on the spectral envelope and its evolution in time. Their design and training process has been described in detail in [6]. It is based on performing Principal Component Analysis (PCA) on the set of training spectral envelopes extracted from a database Sinusoidal Modeling Onset detection Track grouping Timbre matching Timbre model library Track extension Resynthesis MIXTURE ... Segmentation results ... ... SOURCES Figure 1. System overview. of isolated notes. The final result is a set of prototype curves in a reduced-dimensional timbre space. When projected back to the t-f domain, each prototype trajectory corresponds to a prototype envelope consisting of a mean surface and a variance surface, which we will denote by Mi(k, r) and Σi(k, r), respectively, where i = 1, . . . , I is the instrument index, k = 1, . . . , K is the frequency bin index, and r = 1, . . . , R is the frame index (we will consider the same number of frames R for all models). Analogously, this can be interpreted as a Gaussian Process with parameters varying in the time-frequency plane. 3 SEGMENTATION AND SEPARATION",
        "zenodo_id": 1416396,
        "dblp_key": "conf/ismir/BurredS07",
        "keywords": [
            "source separation",
            "musical mixtures",
            "sinusoidal modeling",
            "timbre models",
            "Principal Component Analysis",
            "onset detection",
            "track grouping",
            "timbre matching",
            "instrument recognition",
            "additive synthesis"
        ],
        "content": "MONAURAL SOURCE SEPARATION FROM MUSICAL MIXTURES\nBASED ON TIME-FREQUENCY TIMBRE MODELS\nJuan Jos ´e Burred and Thomas Sikora\nCommunication Systems Group\nTechnical University of Berlin, Germany\n{burred,sikora }@nue.tu-berlin.de\nABSTRACT\nWe present a system for source separation from monau-\nral musical mixtures based on sinusoidal modeling and\non a library of timbre models trained a priori. The mod-\nels, which rely on Principal Component Analysis, serve as\ntime-frequency probabilistic templates of the spectral en-\nvelope. They are used to match groups of sinusoidal tracks\nand assign them to a source, as well as to reconstruct over-\nlapping partials. The proposed method does not make any\nassumptions on the harmonicity of the sources, and does\nnot require a previous multipitch estimation stage. Since\nthe timbre matching stage detects the instruments present\non the mixture, the system can also be used for classiﬁca-\ntion and segmentation.\n1 INTRODUCTION\nSeparation of a musical mixture into its sources can greatly\nfacilitate content analysis for Music Information Retrieval\npurposes, and allows other applications like remixing or\nupmixing to a larger number of channels than the original\nif multitrack recordings are not available. We address sep-\naration from a single channel, which is a highly underde-\ntermined problem that requires either strong assumptions\nabout the nature of the sources, a fair amount of a priori\ninformation, or a combination of both.\nThe main assumption taken in underdetermined sepa-\nration is the sparsity of the sources, which leads to the\nuse of elaborate signal models. An example thereof is the\nuse of Nonnegative Sparse Coding [1]. Other approaches\nare based on sinusoidal modeling, which allows a detailed\nhandling of overlapping partials and is also a highly sparse\nmodel. They are based on grouping the extracted partials\naccording to Auditory Scene Analysis cues. In [2], ampli-\ntude smoothness is modeled by performing basis decom-\nposition on the harmonic structures and their evolution in\ntime. In [3], spectral ﬁltering techniques are used to re-\nsolve overlapping sinusoids. Most approaches based on\nsinusoidal modeling rely either on a previous multipitch\nestimation stage or on the knowledge of the pitch score of\nthe mixture [2, 3].\nc/circlecopyrt2007 Austrian Computer Society (OCG).The above methods are unsupervised (i.e., there is no\ntraining phase) and are based on generic source mod-\nels. To further improve separation, statistical models of\nthe sources can be trained beforehand on a database of\nisolated source samples. Examples of this supervised\napproach include the use of learnt spectral priors with\nbayesian harmonic models [4] and the derivation of tem-\nplates for timbral features [5].\nIn the present contribution, we propose a system for\nthe separation of sources from single-channel mixtures of\nmusical instruments based on sinusoidal modeling and on\na library of pre-trained timbre models. Since it also out-\nputs onset/offset information and the instrument each note\nbelongs to, it can also be used for segmentation or poly-\nphonic instrument recognition. The timbre models are\ntime-frequency templates that describe in detail spectral\nshapes and their evolution in time. In contrast to most\npreviously existing approaches, no assumptions on har-\nmonicity are made, which allows to separate highly in-\nharmonic sounds or to separate chords played by a single\ninstrument. Furthermore, no previous multipitch estima-\ntion or any kind of a priori pitch-related score is needed.\nInstead, separation is solely based on common onset prop-\nerties of the partials, and on the analysis of the evolution\nin time of the spectral envelope they deﬁne. The knowl-\nedge of the number and names of the instruments is not\nmandatory, but will obviously increase the performance.\nFigure 1 shows an overview of the proposed separation\nsystem. First, the mixture signal is subjected to sinusoidal\nmodeling, obtaining a set of sinusoidal tracks. A simple\nonset detector based on identifying synchronously start-\ning tracks then allows to select the partial tracks that are\ngoing to be matched with the timbre models in the next\nstage. After each common-onset group of partial tracks\nhas been assigned to an instrument, the overlapping part\nof the tracks is retrieved from the models. Finally, the\nseparated tracks are synthesized using additive synthesis.\n2 TRAINING OF THE TIMBRE MODELS\nThe used timbre models are based on the spectral envelope\nand its evolution in time. Their design and training pro-\ncess has been described in detail in [6]. It is based on per-\nforming Principal Component Analysis (PCA) on the set\nof training spectral envelopes extracted from a databaseSinusoidal \nModeling\nOnset detection\nTrack grouping\nTimbre matching\nTimbre \nmodel\nlibrary\nTrack extension\nResynthesisMIXTURE\n...Segmentation \nresults\n...\n...\nSOURCESFigure 1 . System overview.\nof isolated notes. The ﬁnal result is a set of prototype\ncurves in a reduced-dimensional timbre space. When pro-\njected back to the t-f domain, each prototype trajectory\ncorresponds to a prototype envelope consisting of a mean\nsurface and a variance surface, which we will denote by\nMi(k, r)andΣi(k, r), respectively, where i= 1, . . . , I\nis the instrument index, k= 1, . . . , K is the frequency\nbin index, and r= 1, . . . , R is the frame index (we will\nconsider the same number of frames Rfor all models).\nAnalogously, this can be interpreted as a Gaussian Pro-\ncess with parameters varying in the time-frequency plane.\n3 SEGMENTATION AND SEPARATION\n3.1 Sinusoidal modeling\nThe sinusoidal model approximates a signal as a sum of\nsinusoids with time-varying amplitudes, frequencies and\nphases. The successive stages of spectral peak picking and\npartial tracking are performed to obtain a frame-wise ap-\nproximation to that model, yielding a triplet of amplitude,\nfrequency and phase information xpr= (Apr, fpr, θpr),\nfor each partial pand each time frame r. We use a stan-\ndard procedure, as described in [8].\n3.2 Onset detection\nSinusoidal extraction is followed by a basic onset detec-\ntion stage consisting of counting the number of new tracks\nwithin a certain frame range. If b(r)is a function giving\nthe number of tracks born at frame r, we deﬁne an onset\ndetection function o(r)as a moving average of order C:\no(r) = 1 /C/summationtextC−1\nc=0b(r−c). Its highest peaks are declared\nas the onset positions Lon\noforo= 1, . . . , O .\n3.3 Track grouping and labeling\nAll tracks tthaving its ﬁrst frame within the interval\n[Lon\no−C, Lon\no+C]for a given onset location Lon\noare\ngrouped into the set To. A track belonging to this set can\nbe of one of the following types:1.Nonoverlapping: if it corresponds to a new partial\nnot present in the previous note or group of notes.\n2.Overlapping with previous track: if its initial fre-\nquency is close, within a narrow margin, to the ﬁ-\nnal frequency of a partial from the previous note or\ngroup of notes.\n3.Overlapping with synchronous track: if it coincides\nin frequency, within a narrow margin, with a track\nbelonging to the same track group To.\nTracks of type 2 are easily detected, and correspondingly\nlabeled, by searching the set To−1for a track fulﬁlling the\nnarrow frequency margin condition. Whether the rest of\ntracks are of type 1 or type 3 cannot be detected at this\npoint of the system. Tracks of type 2 and 3 can be further-\nmore classiﬁed as resulting from overlaps between partials\nbelonging to the same or different instruments. Whether\na track of type 2 corresponds to the same or to different\ninstruments is irrelevant for our purposes since the corre-\nsponding notes will be segmented and separated anyway.\nOn the contrary, tracks of type 3 belonging to the same\ninstrument will be left intact without separation in order\nto detect same-instrument chords as belonging to a single\nsource (note that our goal is different than that of tran-\nscription, which would require to detect the constituent\nnotes of the chord). Currently, type 3 tracks from differ-\nent instruments cannot be currently reliably separated, and\nthus the system will not support separation of notes from\ndifferent instruments and exactly the same onsets. Thus,\nall tracks of types 1 and 3 are considered as belonging to\nthe same source. Finally, the offset Loff\nocorresponding\nto a given onset Lon\nois declared as the last frame of the\nlongest partial of group To.\n3.4 Timbre detection\nThe timbre detection stage matches each one of the onset-\nrelated track groups Towith each one of the prototype\nenvelopes derived from the timbre models, and selects the\ninstrument corresponding to the highest match. As mea-\nsure of timbre similarity between the track group Toand\nthe model formed by parameters θi= (Mi,Σi), we use\nthe following likelihood function:\nL(To|θi) =/productdisplay\nt,rp(Atr|Mi(ftr),Σi(ftr)) (1)\nwhere Atrandftrare the amplitude and frequency, re-\nspectively, on the r-th frame of the t-th track tt∈To,\np(x)denotes a unidimensional Gaussian distribution, and\nMi(ftr)andΣi(ftr)denote the evaluation of each pa-\nrameter matrix at the frequency point ftr. In order to\nobtain the quantities Mi(ftr)andΣi(ftr)for each data\npoint, the model frames closest in time to the input frames\nare chosen, and the corresponding values for the mean\nand the variance are linearly interpolated from neighbor-\ning data.\nIn order to deal with amplitude and temporal scaling\nuncertainty, a 2-D parameter search must be performed to0 5 10 15 20 25 30 35 400200400600800100012001400160018002000\nTime framesFrequency (Hz)L1onL2onL1offL3on\n1-NOV-Long1-NOV-Short1-NOV-Short 2-OV-Long\n2-NOV-Longextensionextension\nsubstitutionsubstitutionFigure 2 . Track extension and substitution.\nﬁnd the best matches. Amplitude scaling is denoted by\nterm αand time scaling is performed by stretching the\npartial tracks towards the offset. Then, the ﬁnally used\nlikelihood function becomes:\nL(To|θi) = max\nα,N/productdisplay\nt,rp/parenleftbig\nAN\ntr+α|Mi(fN\ntr),Σi(fN\ntr)/parenrightbig\n(2)\nwhere AN\ntrandfN\ntrdenote the amplitude and frequency\nvalues for a track belonging to a group that has been\nstretched so that its last frame is N.\n3.5 Track extension and substitution\nOnce a track group Tohas been declared as produced by\ninstrument i, the corresponding prototype envelope means\nMiare used for the two following purposes:\n1. Tracks of type 1 or 3 that are shorter than the cur-\nrent note (which can either result from a partial am-\nplitude approximating the noise threshold and thus\nremaining undetected or by the imminent appear-\nance of a partial from the next onset group overlap-\nping with it) are extended towards the offset by se-\nlecting the appropriate frames from Miand linearly\ninterpolating the amplitudes at the mean frequency\nof the remainder of the track. The amplitudes re-\ntrieved from the model are scaled so that the ampli-\ntude transition between original and extended sec-\ntions of the partial is smooth.\n2. Overlapping tracks of type 2 are retrieved from the\nmodel in their entirety by interpolating the model at\nthe frequency support of the track. If the track is\nshorter than the note, it is again extended using the\nsame procedure as above.\nFigure 2 shows an example of the results of the track\nextension block.The OV labels indicate overlapping tracks\nof type 2 and NOV means nonoverlapping tracks (type 1).Polyphony 2 3\n1. Intervals / arpeggios 8.95 5.38\n2. Sequences 3.17 2.26\nTable 1 . Results from experiments 1 and 2: Spectral\nSource-to-Residual Ratios (SSRR) in dB.\nShort, nonoverlapping partials are extended to the offset\n(marked by extension ) and overlapping tracks of the sec-\nond offset are marked by substitution . Note that any re-\ngion marked as substitution additionally implies an exten-\nsion of the nonverlapping tracks from the previous onset.\n4 EXPERIMENTS AND RESULTS\nSince the phases of the sinusoids are not preserved by the\nseparation algorithm, the evaluation is done in the t-f do-\nmain. We use a spectral signal-to-residual ratio (SSRR):\nSSRR = 10 log/summationtext\nk,r|S(k, r)|2\n/summationtext\nk,r(|S(k, r)| − | ˆS(k, r)|)2(3)\nwhere S(k, r)and ˆS(k, r)are respectively the spectro-\ngrams of the original and separated sources. Although\nthe SSRR is a separation quality measure, it should be\nnoted that it will also reﬂect errors of any other part of the\nsystem, like timbre detection or onset/offset deﬁnition. A\nnote being classiﬁed with the wrong instrument will have\nits overlapping partials extracted from the wrong model,\nand thus will decrease separation quality. All samples\nused for the experiments were extracted from the RWC\nMusical Instrument sound database [7]. A selection of\nseparation audio examples is available on line1.\n4.1 Experiment 1: Intervals and Arpeggios\nFor the ﬁrst, simplest evaluation test, we consider mix-\ntures of single notes from different instruments playing\n2-note intervals or 3-note arpeggios. For each case, 10\ndifferent mixtures were generated. Here, the instruments\ncontained in the mixture were considered unknown and\na library of 5 instrument models (piano, oboe, clarinet,\ntrumpet and violin), trained over the 4th octave, was used.\nTable 1 shows the results of averaging all SSRR values for\neach detected source and for all mixtures.\n4.2 Experiment 2: Note sequences\nFor the second test we used sequences of up to 4 con-\nsecutive notes played by each instrument. This situation\nis more demanding, since all notes from each instrument\nneed to be correctly classiﬁed in order to be grouped into\nthe same track. 5 mixtures with different melodies and\nwith up to 3 simultaneous instruments were generated.\nFor this test, the instruments were known a priori.\n1www.nue.tu-berlin.de/research/projects/\nsourcesep/sepmodels/\t\n\t\n\t\u0017\u0016\n\u0017\u0016\n\u0017\u0016Oboe\nClarinet\nPiano  ¡D\n Ì\n\u0011Ì¡¡¡\n\u0011Ì\n¡ÌD\n  ¡\n\u0011\u0011\u0011ÌÌÌD\n¡Ì\n\u0011\u0011\u0011ÌÌÌ(a) Musical score.\n0 1 2 3 4 5 6 7 8 101\nTime (s)Mixture\n(b) Mixture.\n0 20 40 60 80 100 120 140 160 180\nTime (frames)Oboe\nClarinet\nPiano\n(c) Segmentation chart.\n 0.500.5\nOboe\n 0.500.5\nClarinet\n0 1 2 3 4 5 6 7 8 1 0.500.5\nTime (s)Piano\n(d) Separated sources.\nFigure 3 . Example of separation including chords.\n4.3 Experiment 3: Sequences including chords\nOur ﬁnal test involves more complex sequences that can\ncontain common-onset chords played by a single instru-\nment. Several test sequences with up to 3 simultaneous in-\nstruments were generated. Figure 3 shows an example of\nsegmentation and separation of a sequence by three instru-\nments (piano, clarinet and oboe), in which piano chords\nappear. In this case, no systematic evaluation has been\nperformed to the current time. This and other audio ex-\namples corresponding to this experiment can be found on\nthe web page mentioned above.\n5 CONCLUSIONS\nA method has been proposed that allows separation of\nmusical instrument sounds from a single-channel mixture\nwithout making any assumptions on harmonicity, and with-\nout a previous multipitch estimation stage. This makes the\nextraction of the notes and chords played by a single in-\nstrument possible without pitch-related a priori informa-\ntion. The system is based on a stored library of proba-\nbilistic timbre models describing the characteristic behav-\nior of each instrument’s spectral envelope in time and in\nfrequency. Experiments using mixtures of up to 3 notes\nfrom up to 5 instruments, including mixtures with single-\ninstrument chords, have been shown to demonstrate theviability of the method.\nThe main limitation of the system is that notes with\ncommon onsets played by different instruments cannot be\ncurrently separated. A ﬁrst direction towards solving this\nwas to match the tracks to the timbre models individu-\nally, rather than in common-onset groups, and declaring\nan onset group as a mixture of two instruments if the in-\ndividual track classiﬁcation result was spread across the\ncorresponding classes. Although some success has been\nobtained using this approach, it still lacks robustness. In\nour future research, the present system will be extended\nto stereo mixtures, so that the additionally available spa-\ntial information will allow to detect common onset notes\nfrom different instruments.\n6 ACKNOWLEDGEMENTS\nPart of this research was performed at the Analy-\nsis/Synthesis team, IRCAM, Paris. The research work\nleading to this paper has been supported by the European\nCommission under the IST research network of excellence\nK-SPACE of the 6th Framework Programme.\n7 REFERENCES\n[1] Virtanen, T. ”Separation of Sound Sources by Convo-\nlutive Sparse Coding”, Proc. ISCA SAPA Workshop ,\nJeju, Korea, 2004.\n[2] Virtanen, T. ”Algorithm for the Separation of Har-\nmonic Sounds with Time-Frequency Smoothness\nConstraint”, Proc. DAFX , London, UK, 2003.\n[3] Every, M. R. and Szymanski, J. E. ”Separation of Syn-\nchronous Pitched Notes by Spectral Filtering of Har-\nmonics”, IEEE Trans. on Audio, Speech, and Lan-\nguage Processing , 2006.\n[4] Vincent, E. and Plumbley, M. D. ”Single-Channel\nMixture Decomposition Using Bayesian Harmonic\nModels”, Proc. Int. Conference on Independent\nComponent Analysis and Blind Source Separation ,\nCharleston, USA, 2006.\n[5] Kinoshita, T., Sakai, S. and Tanaka, H. ”Musi-\ncal Sound Source Identiﬁcation Based on Frequency\nComponent Adaptation”, Proc. IJCAI CASA Work-\nshop, Stockholm, Sweden, 1999.\n[6] Burred, J. J., R ¨obel, A. and Rodet, X. ”An Accurate\nTimbre Model for Musical Instruments and its Appli-\ncation to Classiﬁcation”, Proc. Workshop on Learning\nthe Semantics of Audio Signals , Athens, Greece, 2006.\n[7] Goto, M., Hashiguchi, H., Nishimura, T. and Oka, R.\n”RWC Music Database: Music Genre Database and\nMusical Instrument Sound Database”, Proc. ISMIR ,\nBaltimore, USA, 2003.\n[8] Serra, X. ”Musical Sound Modeling with Sinusoids\nplus Noise”, in Roads, C., Pope, S., Picialli, A. and\nDe Poli, G. (Eds.), Musical Signal Processing , Swets\n& Zeitlinger, 1997."
    },
    {
        "title": "Singing Melody Extraction in Polyphonic Music by Harmonic Tracking.",
        "author": [
            "Chuan Cao",
            "Ming Li 0026",
            "Jian Liu",
            "Yonghong Yan 0002"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414708",
        "url": "https://doi.org/10.5281/zenodo.1414708",
        "ee": "https://zenodo.org/records/1414708/files/CaoLLY07.pdf",
        "abstract": "This paper proposes an effective method for automatic melody extraction in polyphonic music, especially vocal melody songs. The method is based on subharmonic summation spectrum and harmonic structure tracking strategy. Performance of the method is evaluated using the LabROSA database 1 . The pitch extraction accuracy of our method is 82.2% on the whole database, while 79.4% on the vocal part. 1 INTRODUCTION Melody is widely considered as a concise and representative description of polyphonic music and it can be used in numerous applications such as “Query-by-humming” system, music structure analysis and music classification. However, the automatic melody extraction is recognized to be very tough and remains unsolved up to now. Yet, amount of remarkable work has been done recently. In 1999, Goto for the first time used a monophonic pitch sequence to represent music melody and achieved transcription from real world music with his famous PreFEst algorithm [5]. Klapuri [1] then proposed a perceptual motivated algorithm in 2005. Poliner and Ellis introduced a novel classification approach using SVM theory for the transcription task [2]. Also, Paiva et al. [6] and Dressler [4] proposed methods generally based on spectral peaks picking and post-tracking. In most methods above, pitch information (pitch candidates, instantaneous frequency (IF) estimations or others) is analyzed frame by frame, and then integrated with temporal/spectral restrictions. However in polyphonic music, especially vocal melody songs, some local frames are inevitably dominated by non-melody intrusions and thus local pitch information is polluted somewhat, or even destroyed sometimes. So integration process based on the polluted information could hardly find the true melody at those local frames. In this paper, we propose a harmonic tracking method attempting to solve this problem. Briefly speaking, we 1 The database can be downloaded from the web site of: http://labrosa.ee.columbia.edu/projects/melody/ c⃝2007 Austrian Computer Society (OCG). trace a sound with its harmonic structure in frequency domain. Here harmonic structure mainly refers to the harmonic partials’ frequencies and their relative amplitudes. If given target harmonic structure for a specific local frame, we could find the partials from the same sound in adjacent frames, by a tracking strategy. In real applications, target harmonic structure is not known priorly and thus has to be estimated from the mixed signal. We analyze the predominant pitch of the mixture to find stable harmonic structure seeds and then use them to track forward and backward. Also, rather than tracing all the harmonic partials, we use subharmonic-summation (SHS) spectrum as the tracking feature for simplicity, which can be considered as an integrative representation of the whole harmonic family. And a verification procedure is needed to make up the gap between full partial tracking and integrative feature tracking. 2 METHOD DESCRIPTION",
        "zenodo_id": 1414708,
        "dblp_key": "conf/ismir/CaoLLY07",
        "keywords": [
            "polyphonic music",
            "vocal melody songs",
            "subharmonic summation spectrum",
            "harmonic structure tracking",
            "pitch extraction accuracy",
            "LabROSA database",
            "monophonic pitch sequence",
            "PreFEst algorithm",
            "perceptual motivated algorithm",
            "SVM theory"
        ],
        "content": "SINGING MELODY EXTRACTION IN POLYPHONIC MUSIC BY\nHARMONIC TRACKING\nChuan Cao, Ming Li, Jian Liu and Yonghong Yan\nThinkit Speech Lab., Institute of Acoustics,\nChinese Academy of Sciences,\nfccao,mli,jliu,yyan g@hccl.ioa.ac.cn\nABSTRACT\nThis paper proposes an effective method for automatic\nmelody extraction in polyphonic music, especially vocal\nmelody songs. The method is based on subharmonic sum-\nmation spectrum and harmonic structure tracking strat-\negy. Performance of the method is evaluated using the\nLabROSA database1. The pitch extraction accuracy of\nour method is 82.2% on the whole database, while 79.4%\non the vocal part.\n1 INTRODUCTION\nMelody is widely considered as a concise and represen-\ntative description of polyphonic music and it can be used\nin numerous applications such as “Query-by-humming”\nsystem, music structure analysis and music classiﬁcation.\nHowever, the automatic melody extraction is recognized\nto be very tough and remains unsolved up to now.\nYet, amount of remarkable work has been done recently.\nIn 1999, Goto for the ﬁrst time used a monophonic pitch\nsequence to represent music melody and achieved tran-\nscription from real world music with his famous PreFEst\nalgorithm [5]. Klapuri [1] then proposed a perceptual mo-\ntivated algorithm in 2005. Poliner and Ellis introduced\na novel classiﬁcation approach using SVM theory for the\ntranscription task [2]. Also, Paiva et al. [6] and Dressler\n[4] proposed methods generally based on spectral peaks\npicking and post-tracking.\nIn most methods above, pitch information (pitch candi-\ndates, instantaneous frequency (IF) estimations or others)\nis analyzed frame by frame, and then integrated with tem-\nporal/spectral restrictions. However in polyphonic music,\nespecially vocal melody songs, some local frames are in-\nevitably dominated by non-melody intrusions and thus lo-\ncal pitch information is polluted somewhat, or even de-\nstroyed sometimes. So integration process based on the\npolluted information could hardly ﬁnd the true melody at\nthose local frames.\nIn this paper, we propose a harmonic tracking method\nattempting to solve this problem. Brieﬂy speaking, we\n1The database can be downloaded from the web site of:\nhttp://labrosa.ee.columbia.edu/projects/melody/\nc°2007 Austrian Computer Society (OCG).trace a sound with its harmonic structure in frequency do-\nmain. Here harmonic structure mainly refers to the har-\nmonic partials’ frequencies and their relative amplitudes.\nIf given target harmonic structure for a speciﬁc local frame,\nwe could ﬁnd the partials from the same sound in adjacent\nframes, by a tracking strategy. In real applications, target\nharmonic structure is not known priorly and thus has to be\nestimated from the mixed signal. We analyze the predom-\ninant pitch of the mixture to ﬁnd stable harmonic structure\nseeds and then use them to track forward and backward.\nAlso, rather than tracing all the harmonic partials, we use\nsubharmonic-summation (SHS) spectrum as the tracking\nfeature for simplicity, which can be considered as an inte-\ngrative representation of the whole harmonic family. And\na veriﬁcation procedure is needed to make up the gap be-\ntween full partial tracking and integrative feature tracking.\n2 METHOD DESCRIPTION\n2.1 Subharmonic Summation Spectrum\nSubharmonic-summation algorithm used here is based on\nHermes’ pitch-determination algorithm [3], concluded as:\nH(f) =NX\nn=1hnP(nf) (1)\nwhere, H(f)is the subharmonic-summation value of the\nhypothetic pitch value f,P(¤)is the STFT power spec-\ntrum and hnthe compression factor (usually hn=hn¡1).\n2.2 Predominant F0Estimation\nWe estimate the predominant pitches (not necessarily mel-\nody) frame by frame with the f0s that maximize the frame-\nwise SHS spectrum H(f0), noted as Fp. With the assump-\ntion that singing voice dominates in most frames, which\naccords well with the reality, we can declare that most of\nFpbelong to the singing melody. Further processing is\ngenerally based on this Fp.\n2.3 Stable Harmonic Structure Detection\nHere, stable harmonic structure refers to the harmonic stru-\ncture that dominates the mixture for some time no shorter\nthanµs, the stable length threshold. Since pitches from thesame sound have good temporal continuity, we can easily\nrecognize stable harmonic structures by analyzing Fp. A\nsequence of continuous pitches from Fplonger than µs\nindicates a stable harmonic structure deﬁned above. No-\ntably, we store their time axis start positions in Pe.\n2.4 Harmonic Tracking and Identity Veriﬁcation\nAs referred above, we use the SHS spectrum to track har-\nmonic structure instead of partials’ IF for feasibility and\nsimplicity concerns. For a speciﬁc frame, pitch candi-\ndates Fcand are selected only if they are close enough to\nthe last conﬁrmed pitch and also they should indicate lo-\ncal maxima in the SHS spectrum. Since the locality, these\nF0hypotheses may be false and indicate an invalid pitch\nvalue, so a veriﬁcation procedure follows. We try to use\ntimbre information and calculate the correlation of rela-\ntive amplitudes between the hypothetic harmonic family\nand the conﬁrmed harmonic family. If the correlation is\nlarger than the identity threshold, the hypothetic pitch sur-\nvives in the Fcand pool. Then the F0hypothesis with the\nbiggest saliency is selected to be conﬁrmed and the track-\ning process goes on.\nPredominant pitches at every Peare utilized to initial-\nize the process and it goes forward and backward until\nnoF0hypothesis survives. All the pitches from every\ntrack are considered as a whole and represent the har-\nmonic structure they belong to. Because of the backward\nand forward mechanism, we do not guarantee that there is\nonly one harmonic structure valid at a speciﬁc frame. So\na following mapping algorithm is needed.\n2.5 Final Pitch Streaming\nFor every two competing harmonic structures (which have\ntemporal overlapping part), pitches of the overlapping part\nare decided as follows: 1. Saliency of the two overlapping\nparts are calculated respectively by summing saliency of\nall the pitches in that part. 2. The part with higher saliency\nis reserved and the other is removed.\nAfter all competing pairs have been processed, the ﬁnal\npitch stream is formed.\n3 EXPERIMENT RESULT\n3.1 Experiment Description\nFor evaluation, we chose the database released by LabRO-\nSA of Columbia University, which was originally made\nas part of MIREX 2005 Audio Melody Extraction test set.\nThe database is composed of 9 vocal songs and 4 midi\nmusic. Extraction results were simply compared to the\nground-truth pitch sequences for melody frames, with the\ntolerance of 1/4 tone. And accuracy of the predominant\npitch sequences was also calculated for comparison. As\nwe focus on the singing melody extraction, we also tested\nour system on a database that contains vocal only songs, 9\nvocal songs from the LabROSA database and 4 pop songs\nfrom ISMIR2004 Audio Melody Extraction test set.3.2 Results\nAs seen in table 1, raw pitch accuracy of the proposed\nmethod is 82.23% on the whole LabROSA database, com-\npared to that of the predominant pitches 78.30%. And\ntests on vocal only songs showed accuracy of 79.39% for\nthe ﬁnal pitches, while 74.12% for the predominant pitches.\nFile Af(%) Ap(%) File Af(%) Ap(%)\ntrack01 84.75 83.43 track11 95.68 91.39\ntrack02 59.47 61.52 track12 99.26 95.98\ntrack03 77.00 68.82 track13 83.19 83.59\ntrack04 73.31 70.29 pop1 78.62 75.44\ntrack05 87.29 83.46 pop2 83.32 79.34\ntrack06 66.84 55.36 pop3 82.65 70.50\ntrack07 80.14 77.05 pop4 88.34 75.51\ntrack08 82.24 80.12 Overall\ntrack09 86.09 76.43 LabROSA 82.23 78.30\ntrack10 91.35 84.27 V ocal 79.39 74.12\nTable 1 . Results on the LabROSA database, Afrepre-\nsents the raw pitch accuracy of the ﬁnal pitch, while Ap\nthe accuracy of the predominant pitch.\n4 CONCLUSION AND FUTURE WORK\nThe improvement upon predominant pitch is 3.87% on the\nwhole database and 5.27% on the vocal only set. Actually,\nthe improvement could be considered much more signiﬁ-\ncant than the ﬁgures shown above since the non-organized\npredominant pitches are grouped and organized in har-\nmonic structure units, which can be taken as a whole for\nfurther considerations. Since the tracking and veriﬁcation\nrules used are quite primary, the method can be improved\nfurther.\n5 REFERENCES\n[1]A.Klapuri. “A perceptually motivated multiple-f0 estimation\nmethod,” In Proc. IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics , pp291-294, 2005.\n[2]G.E.Poliner and D.P.W.Ellis. “A classiﬁcation approach to\nmelody transcription,” In Proc.6th International Conference\non Music Information Retrieval , pp161-166, 2005.\n[3]Dik Hermes. “Measurement of pitch by subharmonic sum-\nmation,” Journal of Acoustic of Society of America , vol.83,\npp.257-264,1988.\n[4]K.Dressler. “Extraction of the melody pitch contour from\npolyphonic audio,” In Proc.6th International Conference on\nMusic Information Retrieval , 2005.\n[5]M.Goto. “A real-time music scene description system:\nPredominant-f0 estimation for detecting melody and bass\nlines in real-world audio signals,” In Speech Communica-\ntion, vol.43, no.4, pp.311-329,2004.\n[6]R.P.Paiva, T.Mendes, and A.Cardoso. “On the detection of\nmelody notes in polyphonic audio,” In Proc.6th Interna-\ntional Conference on Music Information Retrieval , pp.175-\n182, 2005."
    },
    {
        "title": "Raag Recognition Using Pitch-Class and Pitch-Class Dyad Distributions.",
        "author": [
            "Parag Chordia",
            "Alex Rae"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416888",
        "url": "https://doi.org/10.5281/zenodo.1416888",
        "ee": "https://zenodo.org/records/1416888/files/ChordiaR07.pdf",
        "abstract": "We describe the results of the first large-scale raag recognition experiment. Raags are the central structure of Indian classical music, each consisting of a unique set of complex melodic gestures. We construct a system to recognize raags based on pitch-class distributions (PCDs) and pitch-class dyad distributions (PCDDs) calculated directly from the audio signal. A large, diverse database consisting of 20 hours of recorded performances in 31 different raags by 19 different performers was assembled to train and test the system. Classification was performed using support vector machines, maximum a posteriori (MAP) rule using a multivariate likelihood model (MVN), and Random Forests. When classification was done on 60s segments, a maximum classification accuracy of 99.0% was attained in a cross-validation experiment. In a more difficult unseen generalization experiment, accuracy was 75%. The current work clearly demonstrates the effectiveness of PCDs and PCDDs in discriminating raags, even when musical differences are subtle. 1 BACKGROUND",
        "zenodo_id": 1416888,
        "dblp_key": "conf/ismir/ChordiaR07",
        "keywords": [
            "Indian classical music",
            "raag recognition",
            "pitch-class distributions",
            "pitch-class dyad distributions",
            "audio signal",
            "support vector machines",
            "maximum a posteriori",
            "multivariate likelihood model",
            "Random Forests",
            "classification accuracy"
        ],
        "content": "RAAG RECOGNITION USING PITCH-CLASS AND PITCH-CLASS DYAD\nDISTRIBUTIONS\nParag Chordia and Alex Rae\nGeorgia Institute of Technology, Department of Music\n840 Mc Millan St., Atlanta GA 30332\n{ppc,arae3 }@gatech.edu\nABSTRACT\nWe describe the results of the ﬁrst large-scale raag recog-\nnition experiment. Raags are the central structure of In-\ndian classical music, each consisting of a unique set of\ncomplex melodic gestures. We construct a system to rec-\nognize raags based on pitch-class distributions (PCDs)\nand pitch-class dyad distributions (PCDDs) calculated di-\nrectly from the audio signal. A large, diverse database\nconsisting of 20 hours of recorded performances in 31 dif-\nferent raags by 19 different performers was assembled to\ntrain and test the system. Classiﬁcation was performed us-\ning support vector machines, maximum a posteriori (MAP)\nrule using a multivariate likelihood model (MVN), and\nRandom Forests. When classiﬁcation was done on 60s\nsegments, a maximum classiﬁcation accuracy of 99.0%\nwas attained in a cross-validation experiment. In a more\ndifﬁcult unseen generalization experiment, accuracy was\n75%. The current work clearly demonstrates the effective-\nness of PCDs and PCDDs in discriminating raags , even\nwhen musical differences are subtle.\n1 BACKGROUND\n1.1Raag in Indian Classical Music\nRaag is a melodic abstraction around which almost all In-\ndian classical music is organized. A raag is most easily\nexplained as a collection of melodic gestures and a tech-\nnique for developing them. The gestures are sequences of\nnotes that are often inﬂected with various micro-pitch al-\nterations and articulated with an expressive sense of tim-\ning. Longer phrases are built by joining these melodic\natoms together.\nBy building phrases in this way, a tonal hierarchy is\ncreated. Some tones appear more often in the basic phrases,\nor are sustained longer. Indian music theory has a rich vo-\ncabulary for describing the function of notes in this frame-\nwork. The most stressed note is called the vadi and the\nsecond most stressed, traditionally a ﬁfth or fourth away,\nis called the samvadi . There are also less commonly used\nterms for tones on which phrases begin and end. A typical\nsummary of a raag includes its scale type ( that),vadi and\nc/circlecopyrt2007 Austrian Computer Society (OCG).samvadi . A pitch-class distribution (PCD), which gives\nthe relative frequency of each scale degree, neatly sum-\nmarizes this information.\nIndian classical music (ICM) uses approximately one\nhundred raags , of which ﬁfty are common. Despite micro-\ntonal variation, the notes in any given raag conform to one\nof the twelve chromatic pitches of a standard just-intoned\nscale. There are theoretically thousands of scale types; in\npractice, however, raags conform to a much smaller set\nof scales, and many of the most common raags share the\nsame set of notes.\nThe performance context of raag music is essentially\nmonophonic, although vocalists will usually be shadowed\nby an accompanying melody instrument. The rhythmic\naccompaniment of the tabla is also present in metered sec-\ntions. There is usually an accompanying drone that sounds\nthe tonic and ﬁfth using a harmonically rich timbre.\n2 RELATED WORK\n2.1 Western Tonality\nKrumhansl and Shephard [11] as well as Castellano et al.\n[3] have shown that stable pitch distributions give rise to\nmental schemas that structure expectations and facilitate\nthe processing of musical information. Using the now\nfamous probe-tone method, Krumhansl [12] showed that\nlisteners’ ratings of the appropriateness of a test tone in\nrelation to a tonal context is directly related to the rela-\ntive prevalence of that pitch-class in a given key. Huron\n[10] has shown that emotional adjectives used to describe\na tone are highly correlated with that tone’s frequency in a\nrelevant corpus of music. Further, certain qualities seemed\nto be due to higher-order statistics, such as note-to-note\ntransition probabilities. These experiments show that lis-\nteners are sensitive to PCDs and internalize them in ways\nthat affect their experience of music.\nThe demonstration that PCDs are relatively stable in\nlarge corpora of tonal Western music led to the develop-\nment of key- and mode-ﬁnding algorithms based on cor-\nrelating PCDs of a given excerpt, with empirical PCDs\ncalculated on a large sample of related music [6, 14, 9].2.2Raag Classiﬁcation\nRaag classiﬁcation has been a central topic in Indian mu-\nsic theory for centuries, inspiring rich debate on the essen-\ntial characteristics of raags and the features that make two\nraags similar or dissimilar [1].\nPandey [13] developed a system to automatically rec-\nognize raags Yaman andBhupali using a Markov model.\nA success rate of 77% was reported on thirty-one samples\nin a two-target test, although the methodology was not\nwell documented. An additional stage that searched for\nspeciﬁc pitch sequences improved performance to 87%.\nIn an exploratory step, Chordia [4] classiﬁed one hun-\ndred thirty segments of sixty seconds each, from thirteen\nraags . The feature vector was the Harmonic pitch class\nproﬁle (HPCP) for each segment. Perfect results were\nobtained using a K-NN classiﬁer with 60/40% train/test\nsplit. This was further developed in [5] where PCDs\nand PCDDs were used as features with more sophisticated\nlearning algorithms. In a 17 target experiment with 142\nsegments, classiﬁcation accuracy of 94% was attained us-\ning 10-fold cross-validation. However, the signiﬁcance of\nthe results in both cases was limited by the size of the\ndatabase.\n3 MOTIVATION\nRaag is the most important concept in Indian music, mak-\ning accurate recognition a prerequisite to almost all mu-\nsical analysis. Further, because a raag deﬁnes the under-\nlying emotional character of the music, correct classiﬁ-\ncation captures qualities essential to the subjective expe-\nrience. Practically, automatic raag recognition thus has\ntremendous potential use in music discovery and auto-\nmatic playlist generation for ICM. It is also useful for in-\nteractive work featuring ICM.\nAn additional motivation is to examine whether con-\nceptions of tonality appropriate to Western tonal music\nare applicable cross-culturally. If PCDs could be used to\nidentify raags , this would mean that they are important\nto establishing a fundamental tonal context in very differ-\nent musical traditions; showing this common underlying\nmechanism would be an important discovery.\n4RAAG DATABASE\nFor this study, a substantial database was assembled from\na variety of sources. The samples were chosen to in-\nclude considerable diversity across several dimensions: in\nraag, musician, instrument, playing style, presence or ab-\nsence of accompaniment, and recording quality. Commer-\ncial recordings were included along with close to twenty\nhours of unaccompanied raags recorded speciﬁcally for\nthis study. The performances can be heard at http://\nparagchordia.com/research/ .\nA total of thirty one raags were represented in the data-\nbase, comprising a signiﬁcant fraction of the commonly\nplayed corpus of ICM. Table 1 summarizes the databaseC D /flatD E /flatE F F /sharpG A /flatA B /flatB\nYamanK. • • • • • • • •\nYaman • • • • • • •\nMaruBihag • • • • • • •\nGaudSarang • • • • • • •\nHameer • • • • • • •\nDesh • • • • • • •\nTilakKamod • • • • • • •\nGaudMalhar • • • • • • • •\nJaijaiwante • • • • • • • •\nKhamaj • • • • • • • •\nBihag • • • • • • • •\nKedar • • • • • • • • •\nRageshri • • • • • •\nBageshri • • • • • • •\nBhimpalasi • • • • • • •\nA.Bhairav • • • • • • • •\nDarbari • • • • • • •\nJaunpuri • • • • • • •\nK.Kanhra • • • • • • •\nMalkauns • • • • •\nMultani • • • • • • •\nShree • • • • • • •\nP.Dhanashri • • • • • • •\nK.R.Asaveri • • • • • • •\nTodi • • • • • • •\nBhairavi • • • • • • • •\nKa.Bhairavi • • • • • • • • •\nB.Todi • • • • • • •\nBhairav • • • • • • •\nMarwa • • • • • •\nBhatiyar • • • • • • • •\nTable 1 . Summary of scale degrees used by raags in\ndatabase. Notes are listed with C as the tonic.\nbyraag and pitch content. Most performances used had\nboth a very slow, unmetered section ( alap), and a faster,\nrhythmic section as well ( bandish orgat). Nineteen mu-\nsicians’ playing was included; six were instrumentalists\nplaying either the plucked string instruments sarod orsitar,\nor the blown instruments shenai or ﬂute, and the remain-\ning thirteen were vocalists, both male and female. Record-\nings made expressly for this project were unaccompanied\nby either drone or tabla , providing a clean and isolated\nsignal, while the commercial recordings contained a full\nrange of accompaniment, and sometimes were of a signif-\nicantly degraded sound quality.\nIndividual recordings were between three and sixty min-\nutes in length (with no more than seven minutes taken\nfrom any single one), and were segmented into 30s and\n60s chunks. There were in total 20 hours of segmented\nmaterial, forming the largest raag classiﬁcation database\nto date.\n5 FEATURE EXTRACTION\n5.1 Annotation\nIn order to facilitate the creation of pitch proﬁles relevant\nto the particular tuning of the performances, each raag\nsample was labeled with the frequency value for the tonicof that recording. This was done manually, by tuning an\noscillator and noting the value in Hz.\n5.2 Pitch Detection\nPitch detection was done using a version of the Harmonic\nProduct Spectrum (HPS) algorithm [7, 15]. Each segment\nwas divided into 40ms frames, using a Gaussian window.\nThe frames were overlapped by 75%, so that the pitch\nwas estimated every 10 ms. Visualization of the result-\ning pitch-tracks showed the estimates to be quite robust,\ndespite occasional octave errors and confusion with ac-\ncompaniment.\n5.3 Onset Detection\nNote onsets in each segment were found by thresholding\na complex detection function (DF) [8]. The segment was\ndivided into 128 sample regions, overlapped 50% using\na rectangular window, and the DFT of each region was\ncomputed and used to construct the DF. Adaptive thresh-\nolding, proportional to the median over a sliding window,\nwas used to choose the peaks to be labeled as onsets.\n5.4 Pitch-Class Distributions\nThe PCDs were calculated without reference to the de-\ntected onsets, by simply taking histograms of the pitch-\ntracks. The bins corresponded to each note of ﬁve octaves\nof a chromatic scale centered about the tonic for that seg-\nment. Speciﬁcally, the ratios of the just-intoned scale and\nthe tonic frequency were used to calculate the center of\neach bin, and the edges were determined as the log mean.\nThe ﬁve octaves were then folded into one and the values\nnormalized to create a pitch-class distribution. This nulled\nany signiﬁcance of octave errors in the HPS algorithm.\nBeing frame-based, the PCDs were not vulnerable to\nerrors in onset detection, and were able as well to capture\ninformation from lengthy held notes, a feature especially\nimportant in the slow alap, where a full minute might only\ninclude eight to ten discrete notes. On the other hand, they\nwere also guaranteed to include noise, as they added every\nerroneous pitch estimate to the histogram.\nFigures 1 and 2 show an illustrative subset of this data;\nthey demonstrate the discriminative potential of PCDs.\nFigure 1 shows the distributions for the ﬂat second scale\ndegree (D /flat) for each raag. The tonic (C) is omitted from\nFigure 2 due to its ubiquity in all raags .\nAdditionally, harmonic pitch class proﬁles (HPCP) were\ncalculated from the spectra using the same frequency bin-\nning and octave-folding method [6], in order to make a\ncomparison with the PCDs.\n5.5 Pitch-class Dyad Distributions\nTo determine the PCDDs, the detected onsets were used\nto segment the pitch-tracks into notes. Each note was\nthen assigned a pitch-class label: ﬁrst the raw pitch es-\ntimates were discretized by assigning to each the center\nFigure 2 . Pitch-class distribution for raag Darbari and\nJaunpuri\nvalue of the bins deﬁned for the pitch histogram, and then\nthe mode was calculated for each note. The label of the\ncorresponding chromatic pitch was assigned to that note.\nThis process dealt quite effectively with variations due to\nmicro-pitch structure, attacks, and errors by the detection\nalgorithm. The octaves were folded into one as with the\nPCDs.\nThe pitch-classes were then arranged in groups of two\n(bi-grams), or in musical terms, dyads. Tables 4 and 5\nshow two examples, taken from raags that share the same\nset of notes.\nA signiﬁcant complication of calculating PCDDs in this\nmusical context is the occurrence of notes that are played\nby sliding up or down to that pitch from a previous note,\nwithout any clear onset. When pitches are histogrammed\nfor each time-frame, as in PCDs, this poses no problem.\nHowever, in PCDDs this characteristic poses difﬁculties,\nand ideally the algorithm would not rely on explicit on-\nsets. In the current work, this problem was not solved,\nand so the PCDDs entail a certain level of abstraction, as\nsome of the values recorded in them are in actuality the\nbi-grams for the closest pairs of clearly articulated notes,\nrather than simply bi-grams of adjacent notes. This also\nmakes PCCDs substantially more vulnerable than PCDs\nto variations in recording quality, accompaniment, and in-\nstrumentation.\n6 CLASSIFICATION\nSoundﬁles were segmented using a rectangular window.\nOne set used segments of 30s and the other of 60s, each\noverlapped by 50%, leading to a total of 4676 thirty sec-\nond segments and 2248 sixty second segments. Success\nrates were calculated using 10-fold cross-validation (CV).\nTo further test generalization, classiﬁcation was attempted\nusing ”unseen” cases, in which raag excerpts in the train-\ning set came from different performances than those in the\ntraining set. In addition to those listed below, classiﬁca-\ntion was also attempted with a number of other methods,\nspeciﬁcally feed-forward neural networks, K-star, and aFigure 1 . Boxplot comparison of use of ‘D /flat’ in target raags\ntree-based classiﬁer (CART). The results, however, were\nsigniﬁcantly worse than the three most effective (typically\naround 70%), so we exclude them from our discussion.\nThe feature vector was modeled using an MVN distri-\nbution. The parameters were estimated from the training\ndata using a common covariance matrix for each class.\nThe priors were calculated empirically from the training\ndata. The label was selected using a maximum a posteri-\nori (MAP) rule.\nClassiﬁcation was attempted using the Random Forests\nmethod [2]. This somewhat newer algorithm is essentially\nan aggregate of decision trees, where each is grown by\ntaking a bootstrap sampling of the training set, and each\nnode of a given tree is constructed by randomly choosing\nsome small subset of features and choosing the best split;\nthe trees are not pruned. The resulting set of tree classi-\nﬁers (forest) outputs a decision by taking a vote over all\nthe individual trees.\nSupport vector machines were learned using the se-\nquential minimal optimization algorithm as implemented\nin WEKA [16]. A series of binary classiﬁers, one for each\npair of raags , were trained and used to make the multi-\ncategory decision.\n7 RESULTS AND DISCUSSION\nTable 2 shows the primary results. In the CV experiment\nnearly perfect results (99.0%) were attained using PCD\nand PCDD features with a SVM classiﬁer. In the more\ndifﬁcult unseen case, accuracy was 75%. To determine\nthe contribution of PCD and PCDD separately, we ran\nthe SVM algorithm using each feature set alone, yield-\ning success rates of 78.0% and 97.1% respectively for the\nCV case, and 75.3% and 57.1% for the unseen case. TheExperiment Type\nFeature Used CV Unseen\nPCD 78.0 75.2\nPCDD 97.1 57.1\nBoth 99.0 73.7\nTable 2 . Summary of primary classiﬁcation results using\nfor SVM classiﬁer using 60s segments.\nSVM classiﬁer easily outperformed MVN (96.1%) and\nRF (96.1%) in the CV experiment. A 1-3% reduction in\nperformance was observed using 30s segments. This was\nsomewhat surprising considering the paucity of notes in\ncertain slow sections. We were interested to see if simi-\nlar results to PCD alone could be obtained without explicit\npitch tracking, using HPCP features. The success rate was\n64.4%, demonstrating the utility of pitch tracking. PCA\nwas attempted but did not improve results in any of the\nexperiments.\nIn the unseen case, while the PCD results are on par\nwith those from the CV case, PCDDs seem to degrade\nperformance, despite their success in the the CV case. We\nhypothesize that errors in onset detection lead to distinc-\ntive dyads that are dependent on both performance and\nraag. This effect is discussed in Section 5.5. More work\nneeds to be done, however, to show that PCDDs are in\ngeneral not performance-speciﬁc.\nIt is important to note that informal listening shows\nthat melodic variation from segment to segment within a\nperformance is as great as the variation between perfor-\nmances. Since purely melodic information is being used,\nwithout any timbral information, the CV classiﬁcation re-\nsults are impressive and indicate that PCDD is capturing\nsomething important. Reﬁnement of the PCDD calcula-As Ba Bh Bi Da Ga Kh Ma MwBp De Ja To Ya Yk Mu Pu Ah Bt Gd Sh Br Bl Ha Ja Ks Kk Ke Mb Ra Tk\nAs80 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nBa. 88 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nBh. . 180 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nBi. . . 137 . . . . . . . . . . . . . . . . . . . . . . . . 1 . .\nDa. . . . 264 . . . . . . . . . . . . . . . . . . . . . . . . . .\nGa. . . . . 72 2 . . . 4 . . . . . . . . 1 . . . . . . . . . . .\nKh. . . 1 . 1 148 . . . 2 . . . . . . . . . . . . . . . . . . . .\nMa. . . . . . . 39 . . . . . . . . . . . . . . . . . . . . . . .\nMw. . . . . . . . 68 . . . . . . . . . . . . . . . . . . . . . .\nBp. . . . 1 . . . . 176 . . . . . . . . . . . . . . . . . . . . .\nDe. . . . 1 4 . . . . 176 . . . . . . . . 1 . . . . . . . . . . .\nJa. . . . . . . . . . . 27 . . . . . . . . . . . . . . . . . . .\nTo. . . . . . . . . . . . 41 . . . . . . . . . . . . . . . . . .\nYa. . . . . . . . . . . . . 19 . . . . . . . . . . . . . . . . .\nYk. . . . . . . . . . . . . . 126 . . . . . . . . . . . . . . . .\nMu. . . . . . . . . . . . . . . 20 . . . . . . . . . . . . . . .\nPu. . . . . . . . . . . . . . . . 22 . . . . . . . . . . . . . .\nAh. . . . . . . . . . . . . . . . . 56 . . . . . . . . . . . . .\nBt. . . . . . . . . . . . . . . . . . 31 . . . . . . . . . . . .\nGd. . . . . . . . . . . . . . . . . . . 33 . . . . . . . . . . .\nSh. . . . . . . . . . . . . . . . . . . . 67 . . . . . . . . . .\nBr. . . . . . . . . . . . . . . . . . . . . 33 . . . . . . . . .\nBl. . . . . . . . . . . . . . . . . . . . . . 9 . . . . . . . .\nHa. . . . . . . . . . . . . . . . . . . . . . . 55 1 . . . . . .\nJa. . . . . . . . . . 2 . . . . . . . . . . . . . 38 . . . . . .\nKs. . . . . . . . . . . . . . . . . . . . . . . . . 6 . . . . .\nKk. . . . . . . . . . . . . . . . . . . . . . . . . . 10 . . . .\nKe. . . . . . . . . . . . . . . . . . . . . . . . . . . 20 . . .\nMb. . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 . .\nRa. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 .\nTk. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nTable 3 . Confusion matrix using 60s segments with SVM classiﬁer.\nC D E /flat F G A /flat B/flat\nC 19.97 1.1 0.58 0.97 4.23 1.2 1.36\nD 1.22 2.5 0.62 0.05 0.29 0.1 0.22\nE/flat0.93 0.37 1.58 0.93 0.39 0.22 0.16\nF 0.75 0.21 0.2 1.14 1.06 0.14 0.53\nG 4.23 0.32 0.61 0.8 9 1.11 0.64\nA/flat 0.94 0.06 0.15 0.26 12.68 0.83\nB/flat 1.26 0.5 0.2 0.3 0.71 0.53 2.31\nTable 4 . Pitch-class dyad distribution for raag Darbari .\nTransitions are from row to column. Numbers given are\npercentages and sum to 100% for the matrix.\ntion, by improving onset detection, will be a major focus\nof future work and will likely lead to much better general-\nization.\nTable 3 shows the confusion matrix for the SVM clas-\nsiﬁer on 60s segments. Of a total of twenty one errors,\neighteen were made amongst similar raags that shared the\nsame scale tones and had phrases in common. The other\nclassiﬁers made similar types of errors.\nFigure 2 shows the average PCD of raags Darbari and\nJaunpuri , two raags with the same scale. In Darbari , F\nis often used in passing, and is rarely held. On the other\nhand, many phrases in Darbari linger on E /flat, a note used\nmore in passing in Jaunpuri . Both these characteristics\nare clearly visible in the PCDs.\nTables 4 and 5 show the dyad probabilities for Darbari\nandJaunpuri . Signiﬁcant differences are bolded. Here,\nmore subtle distinctions become apparent. For example,\nthe tendency of Jaunpuri to skip E /flatin ascending phrases,\na rare event in Darbari , can be seen from comparing the\nprobabilities of the dyad ‘D F’ (1.25% vs .05%). Like-C D E /flat F G A /flat B/flat\nC 17.96 1.59 0.13 0.64 1.13 1.32 1.72\nD 1.23 3.67 0.23 1.25 0.49 0.4 0.31\nE/flat0.19 0.89 1.46 0.95 0.06 0.08 0\nF 0.74 0.35 0.34 1.77 2.66 0.42 0.1\nG 1.75 0.82 0.36 1.31 7.38 1.12 0.56\nA/flat 1.05 0.22 0 0.43 2.56 3.77 0.34\nB/flat1.41 0.09 0 0.02 0.13 0.55 0.43\nTable 5 . Pitch-class dyad distribution for raag Jaunpuri .\nTransitions are from row to column. Numbers given are\npercentages and sum to 100% for the matrix.\nwise, many essential phrases in Darbari center around the\ndescending transition from B /flatto G (skipping A /flat), whereas\ninJaunpuri the descent is usually taken sequentially through\nall three notes. The B /flatto G dyad probabilities reveal this\n(.71% vs. .13%).\n8 CONCLUSIONS AND FUTURE WORK\nThe 99% (CV) and 75% (unseen) success rates clearly\ndemonstrate the efﬁcacy of PCDs and PCDDs for raag\nclassiﬁcation, even when many raags used an identical set\nof notes (Table 1). This suggests that essential melodic\ncharacteristics beyond simple scale type are captured, al-\nlowing the system to recognize stylistic distinctions which\nfor a human require extensive immersion in the genre to\nlearn.\nFuture work will focus on more difﬁcult cases, such as\nrelatively loud and complex accompaniment and low SNR\nconditions. As even larger databases are assembled, it will\nbe possible to make comparisons between instrument typeand performance style. It will also be possible to begin to\nmodel sequential structure beyond dyads. As discussed,\nmore robust methods for analyzing melodies with gliding\ntones will need to be developed.\n9 REFERENCES\n[1] V.N. Bhatkande. Hindusthani Sangeet Paddhati .\nSangeet Karyalaya, 1934.\n[2] Leo Breiman. Random forests. Machine Learning ,\n45(1), 2001.\n[3] MA Castellano, JJ Bharucha, and CL Krumhansl.\nTonal hierarchies in the music of north india. Journal\nof Experimental Psychology , 1984.\n[4] Parag Chordia. Automatic rag classiﬁcation using\nspectrally derived tone proﬁles. In Proceedings of the\nInternational Computer Music Conference , 2004.\n[5] Parag Chordia. Automatic raag classiﬁcation of pitch-\ntracked performances using pitch-class and pitch-class\ndyad distributions. In Proceedings of International\nComputer Music Conference , 2006.\n[6] Ching-Hua Chuan and Elaine Chew. Audio key-\nﬁnding using the spiral array ceg algorithm. In Pro-\nceedings of International Conference on Multimedia\nand Expo , 2005.\n[7] Patricio de la Cuadra, Aaron Master, and Craig Sapp.\nEfﬁcient pitch detection techniques for interactive mu-\nsic. In Proceedings of the International Computer Mu-\nsic Conference , pages 403–406, 2001.\n[8] C. Duxbury, J. P. Bello, M. Davies, and M. Sandler.\nA combined phase and amplitude based approach to\nonset detection for audio segmentation. In Proc. of the\n4th European Workshop on Image Analysis for Multi-\nmedia Interactive Services (WIAMIS–03) , pages 275–\n280, London, 2003.\n[9] E. Gomez and P. Herrera. Estimating the tonality\nof polyphonic audio ﬁles: Cognitive versus machine\nlearning modelling strategies. In Proceedings of Inter-\nnational Conference on Music Information Retrieval ,\n2004.\n[10] David Huron. Sweet Anticipation: Music and the Psy-\nchology of Expectation . MIT Press, 2006.\n[11] C. Krumhansl and R. Shepard. Quantiﬁcation of the\nhierarchy of tonal functions within a diatonic context.\nJournal of Experimental Psychology: Human Percep-\ntion and Performance , 5(4):579–594, 1979.\n[12] Carol Krumhansl. Cognitive Foundations of Musical\nPitch . Oxford University Press, 1990.[13] Gaurav Pandey, Chaitanya Mishra, and Paul Ipe.\nTansen : A system for automatic raga identiﬁcation.\nInProceedings of the 1st Indian International Con-\nference on Artiﬁcial Intelligence , pages 1350–1363,\n2003.\n[14] Craig Sapp. Visual hierarchical key analysis. Comput-\ners in Entertainment , 3(4), October 2005.\n[15] Xuejing Sun. A pitch determination algorithm based\non subharmonic-to-harmonic ratio. In In Proc. of In-\nternational Conference of Speech and Language Pro-\ncessing , 2000.\n[16] Ian H. Witten and Eibe Frank. Data Mining: Practical\nmachine learning tools and techniques . Morgan Kauf-\nmann, 2005."
    },
    {
        "title": "A Dynamic Programming Approach to the Extraction of Phrase Boundaries from Tempo Variations in Expressive Performances.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416584",
        "url": "https://doi.org/10.5281/zenodo.1416584",
        "ee": "https://zenodo.org/records/1416584/files/ChuanC07.pdf",
        "abstract": "We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses for modeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin’s Preludes Nos. 1 and 7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein’s and Evgeny Kissin’s performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies should be considered for detailed interpretation analyses. 1 INTRODUCTION Musical phrasing in expressive performance groups the notes in a piece so as to present a coherent interpretation of its ideas. A performer’s tasks include the determining of some viable groupings of the piece that make musical sense, and the communication of this grouping in performance through the manipulation of expressive parameters such as tempo and loudness. The problem we are concerned with is the automatic extraction of phrases −the groupings of notes in a piece. Phrase structure analysis can begin with the score, or from a performance of a piece. The first focuses on features such as motives, melodies and chord progressions. The latter begins with an expressive performanceof a piece. The interpretation is manifested as a set of grouping strategies inherent in the performance. A goal of this paper is to propose a computational approach to automatically extract phrase boundaries from expressive performances based on tempo variations. Our approach finds the best fit sequence of phrase tempo curves using dynamic programming (DP). This paper also explores the relation between c⃝2007 Austrian Computer Society (OCG). tempo variation and phrase structure. The phrase extraction steps consist of: tempo extraction from audio recordings, tempo smoothing to obtain trajectories, and determination of phrase boundaries from tempo information. We present a Java program for extracting tempo from manually tapping to beat-level onsets. We introduce a DP algorithm for determining the phrase boundaries by curve fitting. We test two quadratic curve types for modeling phrase-level tempo variations: asymmetric concave curves, and splines. 2 RELATED WORK Most researchers focus on three dimensions of expressive musical performance: tempo, dynamics, and articulation. Gabrielsson [2], Kendall & Carterette [3], Todd [7, 8], and Palmer [5] have found that performers tend to indicate phrase boundaries by lengthening note values at these boundaries, and by increasing the time between successive tones. Similarly, Palmer & Hutchins [6] noted that the phrase is a musical unit that is often demarcated by prosodic cues. Large & Palmer [4] used oscillator models and the product of the probabilities that an onset deviates from expected and that it is late as a measure of phrase boundary likelihood. The oscillator models require initial phase and period information. Cheng & Chew [1] used local maxima in the loudness time series to detect phrases. Our present approach uses DP to fit tempo trajectories using a sequence of quadratic curves so as to determine phrase boundaries. No prior information is required in the latter two techniques. 3 SYSTEM DESCRIPTION The system consists of two parts: tempo extraction and phrase boundary determination. Figure 1 shows the system diagram. We describe each part in this section.",
        "zenodo_id": 1416584,
        "dblp_key": "conf/ismir/ChuanC07",
        "keywords": [
            "tempo extraction",
            "phrase boundary determination",
            "tempo variation",
            "phrase structure analysis",
            "tempo smoothing",
            "tempo trajectories",
            "asymmetric concave curves",
            "splines",
            "tempo extraction from audio recordings",
            "tempo smoothing to obtain trajectories"
        ],
        "content": "A DYNAMICPROGRAMMING APPROACH TOTHEEXTRACTION OF\nPHRASE BOUNDARIES FROM TEMPO VARIATIONSINEXPRESSIVE\nPERFORMANCES\nChing-Hua Chuan∗andElaineChew†\nUniversityofSouthernCaliforniaViterbiSchool ofEngine ering\n∗DepartmentofComputerScienceand†EpsteinDepartment ofIndustrialand SystemsEngineering\nIntegrated MediaSystemsCenter, LosAngeles,CA\n{chinghuc,echew }@usc.edu\nABSTRACT\nWepresentanapproachtophrasesegmentationthatstarts\nwith an expressivemusic performance. Previousresearch\nhas shown that phrasesare delineatedby tempo speedups\nand slowdowns. We proposea dynamic programmingal-\ngorithm for extracting phrases from tempo information.\nWetesttwohypothesesformodelingphrasetemposhapes:\na quadratic model, and a spline curve. We test the two\nmodels on phrase extraction from performances of en-\ntire classical romantic pieces namely, Chopin’s Preludes\nNos. 1and7. The algorithms determined 21 of the 26\nphraseboundariescorrectlyfromArthurRubinstein’sand\nEvgeny Kissin’s performances. We observe that not all\ntempo slowdowns signify a boundary (some are agogic\naccents),andmultiplelevelsofphrasingstrategiesshoul d\nbeconsideredfordetailedinterpretationanalyses.\n1 INTRODUCTION\nMusical phrasing in expressive performance groups the\nnotes in a piece so as to present a coherent interpretation\nof its ideas. A performer’s tasks include the determining\nof some viable groupings of the piece that make musical\nsense, and the communicationof this grouping in perfor-\nmancethroughthemanipulationofexpressiveparameters\nsuch as tempo and loudness. The problem we are con-\ncerned with is the automatic extraction of phrases −the\ngroupingsofnotesina piece.\nPhrase structure analysis can begin with the score, or\nfrom a performance of a piece. The ﬁrst focuses on fea-\ntures such as motives, melodies and chord progressions.\nThelatterbeginswithanexpressiveperformanceofapiece.\nTheinterpretationismanifestedasasetofgroupingstrate -\ngies inherent in the performance. A goal of this paper\nis to propose a computational approach to automatically\nextract phrase boundaries from expressive performances\nbasedontempovariations. Ourapproachﬁndsthebestﬁt\nsequenceofphrasetempocurvesusingdynamicprogram-\nming (DP). This paper also exploresthe relation between\nc/circlecopyrt2007AustrianComputerSociety(OCG).tempovariationandphrasestructure.\nThe phrase extraction steps consist of: tempo extrac-\ntion from audio recordings, tempo smoothing to obtain\ntrajectories, and determinationof phraseboundariesfrom\ntempo information. We present a Java program for ex-\ntractingtempofrommanuallytappingtobeat-levelonsets.\nWe introduce a DP algorithm for determining the phrase\nboundaries by curve ﬁtting. We test two quadratic curve\ntypes for modeling phrase-level tempo variations: asym-\nmetricconcavecurves,andsplines.\n2 RELATED WORK\nMost researchersfocusonthreedimensionsofexpressive\nmusical performance: tempo, dynamics, and articulation.\nGabrielsson [2], Kendall & Carterette [3], Todd [7, 8],\nand Palmer [5] have found that performers tend to indi-\ncatephraseboundariesbylengtheningnotevaluesatthese\nboundaries, and by increasing the time between succes-\nsive tones. Similarly, Palmer & Hutchins [6] noted that\nthe phrase is a musical unit that is often demarcated by\nprosodiccues. Large&Palmer[4]usedoscillatormodels\nand the productof the probabilitiesthat an onset deviates\nfrom expected and that it is late as a measure of phrase\nboundarylikelihood. The oscillator modelsrequireinitia l\nphaseandperiodinformation. Cheng&Chew[1]usedlo-\ncal maxima in the loudness time series to detect phrases.\nOur present approach uses DP to ﬁt tempo trajectories\nusing a sequence of quadratic curves so as to determine\nphraseboundaries. Nopriorinformationisrequiredinthe\nlatter twotechniques.\n3 SYSTEM DESCRIPTION\nThe system consists of two parts: tempo extraction and\nphrase boundary determination. Figure 1 shows the sys-\ntem diagram. We describeeachpartinthissection.\n3.1 Tempo Extraction\nWe develop a Java program (the tapper in Figure 1) to\nrecord the time of a user’s tapping for generating the Tapper Data \nSmoothin g Curve \nFitting  DP  \nInput\nAudioOutput\nPhrase\nSegment\nTempo Variations\n Phrase Segments\nPhrase Boundary Determination\nBeat Extraction\nFigure1. Phraseextractionsystemdiagram\ntempo time series from an expressive performance. Fig-\nure 2 shows a screenshot of the program. The interface\ncontains three panels, showing a representation of the\nscorewith pitchheightandonsets(fromMIDIinput),the\namplitude (from the audio signal), and the tempo (calcu-\nlatedfromtheuser’stapping)respectively.\nFigure2. Screenshotofthe tapperprogram\nTo extract tempo information, one of the authors\n(Chuan)tapsthebeatalongwiththeaudiorecordingwhile\nreading the score; she taps each performance ﬁve times,\nand we use the average of the ﬁve as the beat onset time\nseries. Note that tapping at the beat level performsa ﬁrst\nlevelsmoothingofthedata. Thetempoateachbeatiscal-\nculated as the inverse of the inter-onset-interval between\nbetween the current and the previous beat. The program\nchecksfortapseverymillisecond.\n3.2 Phrase BoundaryDetermination\nThis section describes three of our considerations in the\nphraseboundarydeterminationstage.\n3.2.1 DataSmoothing\nA consideration in the phrase boundary determination\nstageisdatasmoothing. Therawtempodatageneratedby\nthe processdescribedin Section 3.1canbe noisy. We use\nanon-causalmovingaveragetosmooththedata. Figure3\nshows the tempo data from EvgenyKissin’s performance\nof Chopin’s Prelude No. 1 , before and after smoothing.\nWe useda windowsize of2bars,i.e.,4beatsin 2/8time.0 5 10 15 20 25 3030405060708090100\nBarTempo (bpm)before smoothing110\nafter smoothing\nFigure3. Tempotimeseries: beforeandaftersmoothing\n3.2.2 Curve Fitting\nWeconsidertwotypesofcurvesformodelingtheshapeof\nthe tempo variation in a phrase. Based on previous ﬁnd-\nings of tempo slowdownsat phrase boundaries,the curve\nshould consist of two lower ends at the boundaries, with\nhighervaluesinbetween. Wetestedtheuseofasymmetric\nconcavequadraticcurves,andquadraticsplines, tomodel\nthephrasetempovariations.\nThe ﬁrst model, the quadratic curve, possesses four\ncharacteristics: (1) it is deﬁned by a degree two polyno-\nmial function;(2)the curveis concave,with valuesin the\nmiddle higher than the two ends; (3) the two sides can be\nasymmetric; and,(4) the peakdoesnot neednot to be ex-\nactlyinthemiddle. Thebest-ﬁtcurveisdeterminedbythe\nleast mean square error (LMSE), a commonly employed\nmeasure, between the ﬁtted curve and the original data\npoints. We use quadratic programming to solve the con-\nstrainedleast-squaresproblemtoﬁndthebest-ﬁt curve.\nSupposewe have the followingonset times and tempo\nvalues for a phrase, given by X={x1, x2, . . ., x n}and\nY={y1, y2, . . . , y n}respectively. We use two quadratic\nfunctionstomodelthetwoasymmetricsidesofthecurve:\nˆy=a1x2+b1x+c1; ˆy=a2x2+b2x+c2,(1)\nwhere a1, a2≤0(concavity constraint). The two curves\npeakandmeetataspeciﬁc xvalue, xp. Weiteratethrough\nall candidate values, xp∈X, to ﬁnd the best ﬁt curve.\nWe prunethe searchspace by restricting eachequationto\ndescendononlyoneside ofthecurve:\nb2\n2a2≤xp≤b1\n2a1. (2)\nThe second model uses the quadratic spline, a piece-\nwise polynomialfunctionwith onecontinuousderivative.\nA local minimum search determines the boundaries after\ncurveﬁtting.\n3.2.3 DeterminingBoundaries\nWe use DP to determine the phrase boundaries from the\nexpressive performance tempo graph. The objective of\nthe DP algorithm is to minimize the sum of LMSE when\napproximating the tempo time series by a sequence of\nquadratic curves (or spline curves); in the process, it seg-\nmentsthe entiredatastream intoa sequenceofphrases.\nThe DP algorithm is based on the observation that\nthe optimal objective value (minimum error) for beats 1through iis the sum of the error of the optimal solutions\nfor beats 1 through jand the curve ﬁt error for beats j\nthrough i,for1≤j < i, asshowninEquation3:\nOpt(1, i) = min\nj[Opt(1, j) +Err(j, i)],(3)\nwhere j= 1, . . ., i −1. In implementation, the initial\noptimalcostsforbeats1and2aresettozero, Opt(1,1) =\nOpt(1,2) = 0,becauseatleastthreepointsareneededfor\ndeﬁninga quadraticcurve. The smallest interval between\ntwo boundaries is set at two bars. The DP algorithm is\nshownin Table1.\nTable1. DPalgorithmforphraseboundarydetermination\nn=length ofpiece;% inbeats\np=twobars;% minimumphrasesize\nOpt(1, a) = 0∀a∈[1,2];% initialization\nPre(b) = 1 ∀b∈[p, n];% initialization\nfori=p+ 1 :n\nforj= 1 :i−p\nOptj(1, i) =Opt(1, j) +Err(j, i);\nend\nOpt(1, i) = min j∈[1,i−p]Optj(1, i);\nPre(i) = argmin j∈[1,i−p]Optj(1, i);\nend\nreturn Pre(n), Pre(Pre(n)), . . . ,1;\nForthesplinemodel,someofthebest-ﬁtcurvesgener-\nated by the DP algorithm may be convexdue to a lack of\nshape constraints. In such cases, we search for the local\nminimato ﬁndthephraseboundaries.\n4 EMPIRICAL RESULTS AND DISCUSSIONS\nWetestthealgorithmsusingtheperformancesofChopin’s\nPreludes Nos. 1 and 7 by Evgeny Kissin and Arthur Ru-\nbinstein −RCA CD recordings −ASIN: B00002DE5F\n(Kissin)andASIN:B000031WBN(Rubinstein,1946).\nFigures4 through 7 showthe extractedphrasebound-\naries for Kissin’s and Rubinstein’s performances of Pre-\nludes Nos. 1 and7, modeled by quadratic curves and\nsplines respectively. We refer to the two settings of the\nDP algorithmsasDP(quadratic)andDP(spline). The ver-\nticaldashedlinesemanatingdownfromthesmoothcurves\nrepresent the boundaries determined by the system. The\nvertical dashed lines cutting across the entire plane are\nthe highest level (several layers of phrasings were la-\nbeled) phrase boundaries annotated according to the per-\nformancesbyoneoftheauthors,anexpertpianist(Chew).\nWe observe that the algorithms, both DP(quadratic)\nandDP(spline),retrievemostofthephraseboundariesin-\ndicatedbythe expert. We discoverdifferenttempostrate-\ngies employed by the two performers on the same piece,\nforexample,slowdownsattheendsofphrasesversusslowstarts at the beginnings. Some other challenges of deter-\nmining phrase boundaries from only tempo information\narediscussedin thefollowingsections.\n4.1 Kissin'sChopin PreludeNo. 1\nFigure 4 shows the results for Kissin’s performance of\nChopin’s Prelude No. 1 . The DP(quadratic) algorithm\nsuccessfully ﬁnds two, and DP(splines) three, of the four\nannotatedboundaries(bars8,16, 24,and28). Inthe ﬁnal\nphrase, beginning in bar 29, Kissin employs a slow-start\nstrategy in this performance: the start of this ﬁnal phrase\nis as slow as the end of the previousphrase. We consider\ndetectedboundariesabarofffromthegroundtruthdueto\nsuchslow startstrategiesascorrect.\n123456789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34405060708090100\nBarTempo (bpm)\nDP(quadratic)\n123456789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34405060708090100\nBarTempo (bpm )\nDP (spline) tempo\n/f_it curve\nFigure4. Kissin’sChopin Prelude1 boundaries\n123456789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34020406080100\nBarTempo (bpm)\nDP (quadratic)\n123456789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34020406080100\nBarTempo (bpm)\nDP (spline)tempo\n/f_it curve \nFigure5. Rubinstein’sChopin Prelude1 boundaries\n4.2 Rubinstein'sChopin PreludeNo. 1\nRubinstein’s performance of Chopin’s Prelude No. 1 ,\nshown in Figure 5, exhibits two distinct characteristics in\ncontrasttoKissin’sperformance.Theslowdownatbar22,\nbeforethebar24phraseboundary,functionsasanagogic\naccent,anemphasis. Thisperformancealso usesmultiplelevelsofgroupings,forexample,fourtwo-barsub-phrases\nwithinaneight-barphraseinbars1 through8.\n4.3 Kissin's ChopinPreludeNo. 7\nFigure 6 shows the results for Kissin’s performance of\nChopin’s Prelude No. 7 . The results, both quadratic and\nspline,matchtwo ofthethreeannotatedboundaries.\n1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 62025303540\nBarTempo (bpm)\nDP (quadratic)\n1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 62025303540\nBarTempo (bpm)\nDP (spline) tempo\n/f_it curve\nFigure6. Kissin’sChopin Prelude7 boundaries\n1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 61015202530\nBarTempo (bpm)\nDP (quadratic)\n1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 61015202530\nBarTempo (bpm)\nDP (spline) tempo\n/f_it curve\nFigure7. Rubinstein’sChopin Prelude7 boundaries\n4.4 Rubinstein's ChopinPreludeNo. 7\nFigure7showstheresultsofRubinstein’sperformanceof\nChopin’s Prelude No. 7 . Comparing Kissin’s and Rubin-\nstein’s performances, we observe two radically different\ntempo strategies. In Kissin’s performance, two sweeping\nconcave curvesare observedin bars (1, 8) and (8, 12) re-\nspectively, followed by a relatively level slowdown. In\nRubinstein’s performance, the tempo shows a predomi-\nnantlydecreasingslopefromstart toend.\n5 CONCLUSIONS AND FUTUREWORK\nWe have proposed a DP approachfor determining phrase\nboundaries from tempo graphs extracted from expressiveperformances. The algorithmaccuratelydeterminedmost\nof the boundaries annotated by an expert in the test data.\nWe discover widely differing tempo strategies. We also\nuncover some challenges in the determining of phrase\nboundaries based only on tempo variations: performers\nsometimes employ multiple levels of grouping strategies,\nincreasing the complexity of phrase boundary analysis;\ntempo variation alone is sometimes inadequate for deter-\nminingphraseboundaries(forexample,aslow-startstrat-\negy can obfuscate the true boundary); and, tempo slow-\ndowns are not always used for segmenting phrases, it is\nsometimesusedforemphasis.\nFuture work will explore methods for extracting mul-\ntiple levels of phrase structure and disambiguating slow-\ndown functions in expressive performances. It will also\nincorporateotherfeaturessuch as dynamics(loudness)in\nphrase analysis. More manually annotated performances\nwill betested.\n6 ACKNOWLEDGEMENTS\nThis material is based upon work supported by the\nNational Science Foundation (NSF) under grants No.\n0347988and EEC-9529152. Any opinions, ﬁndings, and\nconclusions or recommendations expressed in this mate-\nrialarethoseoftheauthors,anddonotnecessarilyreﬂect\ntheviewsofNSF.\n7 REFERENCES\n[1] Cheng, E. and Chew, E. “A Local Maximum Phrase\nDetectionMethodandtheAnalysisofPhrasingStrate-\ngies in Expressive Performances”, Proc. of Math and\nComputationin Music ,Berlin,Germany,2007.\n[2] Gabrielsson, A. ”Music Performance”, Psychology of\nMusic,New York: AcademicPress, 1999.\n[3] Kendall,R.A.andCarterette,E.C.”Thecommunica-\ntionofmusicalexpression”, Music Perception ,Vol.8,\nNo.2,pp.129-164,1990.\n[4] Large, E. W., and Palmer, C. “Perceiving temporal\nregularity in music”, Cognitive Science , 26, pp. 1-37,\n2002.\n[5] Palmer, C. ”Music Performance”, Ann. Rev. of Psy-\nchology,Vol.48,pp.115-138,1997.\n[6] Palmer, C. and Hutchins, S. ”What is musical\nprosody?”, Psycology of Learning and Motivation ,\nVol.46,ElsevierPress,2005.\n[7] Todd,N. P. M. ”The dynamicsof dynamics: A model\nof musical expression”, J. of the Acoustical Soc. of\nAmerica,Vol.91,Issue 6,pp.3540-3550,June1992.\n[8] Todd, N. P. M. ”The kinematics of musical expres-\nsion”,J. of the Acoustical Soc. of America , Vol.97,\nIssue3,pp.1940-1949,March1995."
    },
    {
        "title": "Evaluation of Real-Time Audio-to-Score Alignment.",
        "author": [
            "Arshia Cont",
            "Diemo Schwarz",
            "Norbert Schnell",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416304",
        "url": "https://doi.org/10.5281/zenodo.1416304",
        "ee": "https://zenodo.org/records/1416304/files/ContSSR07.pdf",
        "abstract": "This article explains evaluation methods for real-time audio to score alignment, or score following, that allow for the quantitative assessment of the robustness and preciseness of an algorithm. The published ground truth data base and the evaluation framework, including file formats for the score and the reference alignments, are presented. The work, started for MIREX 2006, is meant as a first step towards a standardized evaluation process contributing to the exchange and progress in this field. 1 INTRODUCTION Score following is the real-time alignment of a known musical score to the audio signal produced by a musician playing this score, usually in order to synchronise an electronic accompaniment of the music to the performer, leaving him with all possibilities of expressive performance. Despite its history of more than 20 years [1], only very few attempts of a systematic evaluation of the result of score following have been made, and therefore, representation of research work is usually limited to demonstration of short examples or subjective ways to assess the quality of such systems. Evaluation gives an indication of the quality of an alignment algorithm and allows the comparison of different",
        "zenodo_id": 1416304,
        "dblp_key": "conf/ismir/ContSSR07",
        "keywords": [
            "real-time audio",
            "alignment",
            "score following",
            "quantitative assessment",
            "algorithm robustness",
            "preciseness",
            "ground truth data",
            "evaluation framework",
            "file formats",
            "MIREX 2006"
        ],
        "content": "EV ALUATION OF REAL-TIME AUDIO-TO-SCORE ALIGNMENT\nArshia Cont\nIrcam UMR CNRS 9912\nCRCA, UCSD\ncont@ircam.frDiemo Schwarz, Norbert Schnell\nIrcam–Centre Pompidou, Paris\nUMR CNRS 9912\n{schwarz, schnell }@ircam.frChristopher Raphael\nIndiana University\nBloomington, IN\ncraphael@indiana.edu\nABSTRACT\nThis article explains evaluation methods for real-time au-\ndio to score alignment, or score following , that allow for\nthe quantitative assessment of the robustness and precise-\nness of an algorithm. The published ground truth data\nbase and the evaluation framework, including ﬁle formats\nfor the score and the reference alignments, are presented.\nThe work, started for MIREX 2006, is meant as a ﬁrst step\ntowards a standardized evaluation process contributing to\nthe exchange and progress in this ﬁeld.\n1 INTRODUCTION\nScore following is the real-time alignment of a known mu-\nsical score to the audio signal produced by a musician\nplaying this score, usually in order to synchronise an elec-\ntronic accompaniment of the music to the performer, leav-\ning him with all possibilities of expressive performance.\nDespite its history of more than 20 years [1], only very\nfew attempts of a systematic evaluation of the result of\nscore following have been made, and therefore, represen-\ntation of research work is usually limited to demonstration\nof short examples or subjective ways to assess the quality\nof such systems.\nEvaluation gives an indication of the quality of an align-\nment algorithm and allows the comparison of different\nmethods, implementations, parameters, etc., and the quan-\ntiﬁcation of improvements gained by training. The lack of\na uniﬁed common evaluation framework has led to differ-\nent and personal approaches to the problem of score align-\nment. This clearly makes further progress of the problem\nmore difﬁcult and assessment of the results uncertain.\n2 EV ALUATION METHOD\nWe start by deﬁning ﬁve basic event measures from which\ntheassessment metrics are calculated:\nTheerror ei=te\ni−tr\niis deﬁned as the time lapse be-\ntween the alignment positions of corresponding events in\nthe reference tr\niand the estimated alignment time te\nifor\nscore events i. A real-time system cannot correctly detect\na note until sometime after it has occurred. We deﬁne the\nlatency ℓi=td\ni−te\ni>0of a detection to be the difference\nbetween the time a dectection is made td\niand the estimated\nnote onset time. The offset oi=td\ni−tr\niis the lag between\nthe time the event occurred tr\niand the reporting of the de-\ntection td\ni, which is important for purely reactive systems.\nc/circlecopyrt2007 Austrian Computer Society (OCG).Missed notes are events that are not recognized, i.e. that\nexist in the reference but are not reported. Misaligned\nnotes are events in the score that are recognized but are\ntoo far (regarding a given threshold θe, e.g. 300 ms) from\nthe reference alignment to be considered correct .\nIn fact, due to score–performance mismatch and misses\nor false matches, the events recognised by the alignment\nalgorithm do not necessarily correspond one-to-one to the\nreference events. For instance, if a recognizer is uncertai n\nabout the onset location of a note, it may be better not to\nreport that onset rather than report it incorrectly. Thus,\nwhen evaluating a score follower, it makes sense to distin-\nguish between missed notes and misaligned notes.\nGiven these event measures, the assessment metrics\ncharacterizing the quality of an alignment are then: The\nmiss rate pmis the percentage of missed score events.\nThe misalign rate peis the percentage of misaligned\nevents with their absolute error |ei|greater than θe. They\nconstitute cases in which the recognizer is not merely in-\naccurate, but simply mistaken. Piece completion pcis\nthe percentage of the events that was followed until the\nfollower hangs, i.e. from where on there are only mis-\naligned events. The average latency µℓfor non-misalign-\ned events, is an overally measure of the latency of the sys-\ntem. The average absolute offset µofor non-misaligned\nevents gives an indication of the reactivity of the follower .\nThevariance of error σeis the standard deviation of ei\nfor non-misaligned events and shows the imprecision or\nspread of the alignment error. The average imprecision\nµeis the average absolute error of non-misaligned events\nand shows the global imprecision.\nSystems are evaluated by two measures: Piecewise pre-\ncision rate as the average of the percentage of correctly\ndetected notes for each piece group in Table 1, and over-\nall precision rate on the whole database.\n3 EV ALUATION FRAMEWORK\nThe evaluation procedure is as follows: Each score fol-\nlowing system to be evaluated has to implement the eval-\nuation interface that allows for the insertion of the system\ninto the evaluation procedure. It deﬁnes the way the sys-\ntem is invoked, the order of the parameters passed to the\nsystem, and the way the results are returned. It is a sim-\nple commandline call with as parameters the names of the\nscore and audio ﬁles to be read, and the path to the output\nﬁle to be written. In general some glue code must be writ-\nten to interface the system with the evaluation framework.\nThe batch processor invokes and controls the score fol-\nlowing system using the interface with each pair of scoreand performance from the evaluation database. The eval-\nuator then reads all alignment results, and calculates and\noutputs statistics of the evaluation measures described in\nsection 2.\nThe ﬁle format deﬁnitions pertain to the performance\naudio ﬁles, the score ﬁles, the reference alignment ﬁles,\nand the alignment result ﬁles: For the audio ﬁles , the\nAIFF and RIFF-wave standards are sufﬁcient. The score\nﬁles pose the well known problem of a missing standard\nfor a score representation format that fulﬁlls all our re-\nquirements. In 2006, we used MIDI ﬁles, but already\nsuch a simple musical event like a trill could not be ade-\nquately represented in MIDI. Therefore, we deﬁned a text\nformat1with one line per event and a ﬁxed number of\ncolumns. One column carries a unique ID for each event,\nvalid across all ﬁle types, others the score position in met-\nric and time position ts\ni. The reference alignment ﬁles\nconstitute a ground truth alignment between a score and\nits performance. They reuse the score ﬁle format, replac-\ning the score time with the reference alignment time tr\ni.\nThe two types of result ﬁles represent the alignment found\nby a score following system between a score and a record-\ning of a performance of it. It is again based on the score\nﬁle format, with ﬁrst the estimated event onset time in te\ni\nand second the reporting time relative to the performance\naudio ﬁle td\niat the place of ts\ni.\n4 GROUND TRUTH DATABASE\nThe database of ground truth alignments is the most valu-\nable asset of the evaluation framework, and the most te-\ndious to assemble. Since the performance of a particular\nscore following system usually depends on the instrument\nand the repertoire (i.e. style) of the chosen pieces it is\ndesirable that the reference database covers a wide range\nof instruments and styles. As the usual pragmatic com-\npromise, the database is constituted by contributions from\nthe participants in the evaluation exchange.\nThe current database, detailed in Table 1, has been con-\nstituted by the authors and used for the ﬁrst evaluation\nexchange in 2006. There are various solo instruments\nin the database and we have classical music as well as\ncontemporary music performances. An example from the\ndatabase is available on the web.2\nPiece name Composer Instrument Files Duration Events\nExplosante-Fixe Boulez Flute 47 17:10 2022\nViolin Sonatas Bach Violin 3 13:50 3996\nK. 370 Mozart Clarinet 4 14:44 2710\nDorabella Mozart V oice 4 01:44 229\nTotal 58 47:38 8957\nTable 1 . MIREX06 Score Following Reference Database\nIn order to achieve high-resolution alignment, we ﬁrst\ngathered a database of monophonic (or slightly poly-\nphonic) audio with their score, and reference alignments\nthat give the time-position of each note in the recordings.\n1http://www.music-ir.org/mirex2007/index.php/Score FileFormat\n2http://crca.ucsd.edu/arshia/mirex06-scofo/The references can come from two sources: For sim-\nple pieces to align, the output of a score following sys-\ntem itself, is precise enough to be used as reference, after\nformat conversion, veriﬁcation, and manual correction if\nnecessary (see below). Although this invalidates the data\nto be used for comparison of different systems because it\nwould confer an unfair advantage to the system generating\nthe data, it is useful to do regression testing of a follower,\nand to measure improvements by training or changes in\nthe code. For more complex pieces, an existing off-line\naudio-to-score alignment system based on dynamic time-\nwarping (DTW) [2] is used. However, even after off-line\nDTW alignment, the preciseness of the segmentation is\nsometimes not sufﬁcient. For this reason, methodologies\nfor hand correction of the alignment results were devel-\noped, that combine the off-line alignment with transient\nmarks found by an onset detection algorithim, overlaid\nover the spectrogram.\n5 CONCLUSION AND FUTURE WORK\nOur evaluation methodology and framework allow for the\nﬁrst time to assess the robustness and preciseness of a\nscore following system in a way that gives objective and\nrepeatable results. This enables us to track improvements\nin the code, parameters, or training data sets of a score\nfollowing system, and to compare different systems.\nThis methodology has also been applied to a ﬁrst\nMIREX evaluation exchange amongst research groups\nwith related interests, and it will be improved and its ap-\nplicability enlarged to allow more systems to be evalu-\nated and compared this year. Results of the evaluation\nprocess deﬁned in this paper are available through the\nMIREX 2006 result page.3The presented evaluation\nmethodology could possibly be extended to other variants\nof alignment such as MIDI-to-score alignment, audio-to-\naudio alignment, and off-line score-to-audio alignment.\n6 ACKNOWLEDGEMENTS\nWe would like to thank Miller Puckette, Roger Dannen-\nberg for sharing their experience and contributing record-\nings and scores to the evaluation database, and all partic-\nipants in the MIREX score following evaluation endeav-\nour. This work is partially conducted in the framework of\nthe i-Maestro project, partially supported by the European\nCommunity under the IST Information Society Technolo-\ngies priority of the 6thFramework Programme for R&D.4\n7 REFERENCES\n[1] Nicola Orio, Serge Lemouton, Diemo Schwarz, and\nNorbert Schnell. Score Following: State of the Art\nand New Developments. In New Interfaces for Musi-\ncal Expression (NIME) , Montreal, 2003.\n[2] Ferrol Soulez, Xavier Rodet, and Diemo Schwarz. Im-\nproving Polyphonic and Poly-Instrumental Music to\nScore Alignment. In ISMIR , Baltimore, 2003.\n3http://www.music-ir.org/mirex2006/index.php/Score Following Results\n4IST-026883 http://www.i-maestro.org"
    },
    {
        "title": "How Many Beans Make Five? The Consensus Problem in Music-Genre Classification and a New Evaluation Method for Single-Genre Categorisation Systems.",
        "author": [
            "Alastair J. D. Craft",
            "Geraint A. Wiggins",
            "Tim Crawford"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414982",
        "url": "https://doi.org/10.5281/zenodo.1414982",
        "ee": "https://zenodo.org/records/1414982/files/CraftWC07.pdf",
        "abstract": "Genre definition and attribution is generally considered to be subjective. This makes evaluation of any genrelabelling system intrinsically difficult, as the ground-truth against which it is compared is based upon subjective responses, with little inter-participant consensus. This paper presents a novel method of analysing the results of a genre-labelling task, and demonstrates that there are groups of genre-labelling behaviour which are selfconsistent. It is proposed that the evaluation of any genre classification system uses this modified analysis method. 1 INTRODUCTION Several genre-classification systems have been proposed in the literature (surveyed in [1, §4.2]). There has not been a corresponding interest in the evaluation of such systems, or in the actual genre-labelling behaviour of people. Whilst it is generally accepted that genre labels are subjective, with little industry [6], or inter-participant [3] consensus, this is rarely, if ever, included in the evaluation of any genre-categorisation system: most evaluations assume some ground truth, be it industry defined or just the categorisation of the experimenter. Despite the acceptance of the ground-truth problems there has been relatively little work on how people categorise genre (see §3). Whilst this partially excuses experimenters’ reliance on some form of absolute ground truth in system evaluation, it does mean that the results of any studies are questionable, as there is little understanding of what such systems are trying to model. In addition the most commonly referenced ground-truth study, [7], is as yet unpublished, and was not designed as a study of inter-participant consensus, but rather of how much audio a participant needed to establish a consistent genre label [2]. The approach we propose towards ground truth in genre classification is as follows: ground truth is an artefact of an individual’s response to music, not an artefact of the audio itself. Therefore the establishment of any ground truth will be the study of responses to music, and is therec⃝2007 Austrian Computer Society (OCG). fore predominately a cultural study. Any unity of response is because of the widespread agreed nature of the musical cues to genre in particular pieces, but the expected response from a group of individuals will be a diversity. If there is a diversity of responses in terms of genre labels to any particular piece, or set of pieces, the standard evaluation methodology that uses single genres as ground truth will, necessarily, not describe all the dataset adequately. 2 WHAT TO EXPECT OF GENRE LABELS? We propose there are two main factors at play in how a listener assigns a single genre label to a piece of music: the number of musical cues associated with different genres in the piece; and the participant’s knowledge and experience of the genres involved. The first of these factors is a feature of what the composer intended of the piece in question: as an intentional act a composer can draw upon stylistic elements from one, or more, genres. In cases of this type the assignment of multiple or compound genre labels is justified. The second factor is a feature of the social and cultural",
        "zenodo_id": 1414982,
        "dblp_key": "conf/ismir/CraftWC07",
        "keywords": [
            "genre",
            "labelling",
            "system",
            "ground-truth",
            "consensus",
            "evaluation",
            "subjective",
            "inter-participant",
            "cultural",
            "study"
        ],
        "content": "HOWMANY BEANSMAKE FIVE?THE CONSENSUS PROBLEM IN\nMUSIC-GENRE CLASSIFICATIONAND ANEW EVALUATIONMETHOD\nFORSINGLE-GENRE CATEGORISATIONSYSTEMS\nAlastairJ. D. Craft, GeraintA.Wiggins,Tim Crawford\nIntelligentSound and MusicSystems,CentreforCognition, Computationand Culture\nGoldsmiths,UniversityofLondon\n{a.craft,g.wiggins,t.crawford}@gold.ac.uk\nABSTRACT\nGenredeﬁnitionandattributionisgenerallyconsidered\nto besubjective . This makes evaluation of any genre-\nlabellingsystemintrinsicallydifﬁcult,astheground-tr uth\nagainst which it is compared is based upon subjective\nresponses, with little inter-participant consensus. This\npaper presents a novel method of analysing the results\nof a genre-labelling task, and demonstrates that there\naregroups of genre-labelling behaviour which are self-\nconsistent. It is proposedthat the evaluationof any genre\nclassiﬁcationsystem usesthismodiﬁedanalysismethod.\n1 INTRODUCTION\nSeveral genre-classiﬁcation systems have been proposed\nintheliterature(surveyedin[1, §4.2]). Therehasnotbeen\na corresponding interest in the evaluation of such sys-\ntems,orintheactualgenre-labellingbehaviourofpeople.\nWhilst it is generally accepted that genre labels are sub-\njective,withlittleindustry[6],orinter-participant[3 ]con-\nsensus, this is rarely, if ever, included in the evaluation\nof any genre-categorisation system: most evaluations as-\nsume some groundtruth, be it industrydeﬁnedor just the\ncategorisationoftheexperimenter.\nDespite the acceptance of the ground-truth problems\nthere has been relatively little work on how people cat-\negorise genre (see §3). Whilst this partially excuses ex-\nperimenters’ reliance on some form of absolute ground\ntruthinsystem evaluation,it doesmeanthattheresultsof\nany studies are questionable, as there is little understand -\ning of what such systems are trying to model. In addition\nthe most commonlyreferencedground-truthstudy, [7], is\nas yet unpublished, and was not designed as a study of\ninter-participantconsensus,butratherofhowmuchaudio\na participant needed to establish a consistent genre label\n[2].\nTheapproachweproposetowardsgroundtruthingenre\nclassiﬁcation is as follows: ground truth is an artefact of\nan individual’s response to music, not an artefact of the\naudio itself. Therefore the establishment of any ground\ntruthwillbethestudyof responses tomusic,andisthere-\nc/circlecopyrt2007AustrianComputerSociety(OCG).forepredominatelya culturalstudy. Any unityofresponse\nis because of the widespread agreed nature of the mu-\nsical cues to genre in particular pieces, but the expected\nresponsefromagroupofindividualswill bea diversity.\nIfthereisa diversityofresponsesin termsofgenrela-\nbels to any particular piece, or set of pieces, the standard\nevaluationmethodologythat usessingle genresasground\ntruth will, necessarily, not describe all the dataset ad-\nequately.\n2 WHATTOEXPECTOFGENRELABELS?\nWe propose there are two main factors at play in how\na listener assigns a single genre label to a piece of mu-\nsic: the number of musical cues associated with differ-\nent genres in the piece; and the participant’s knowledge\nand experience of the genres involved. The ﬁrst of these\nfactors is a feature of what the composer intended of the\npiece in question: as an intentional act a composer can\ndraw upon stylistic elements from one, or more, genres.\nIn cases of this type the assignment of multipleor com-\npoundgenrelabelsisjustiﬁed.\nThesecondfactorisafeatureofthesocialandcultural\nbackground of any particular participant: in order for a\npersonreliablytolabelapiecewithagenretheyhavetobe\ncognisantofthestylistic featuresofthatgenre. Iftheyar e\nnot then the assigned label can not be seen as reliable: it\ncouldberandomlyassigned,orelse anumberofdifferent\ngenrescouldbeconﬂatedunderonegenrelabel.\nThe second factor can also inﬂuence the outcome of a\ngenre-labelling task where the pieces involved are inﬂu-\nenced by the ﬁrst: if a participant has no knowledge of\nthe stylistic featuresof contributorygenresthen they wil l\nlabel inaccurately, as above; if a participant has know-\nledge of the stylistic features of only one of the inﬂuen-\ncing genres then they are most likely to assign that genre\nlabel to it; if they are knowledgeable in all the contribut-\nory stylistic features then they will have to choose which\nis most determinant. In this case a participant’s relative\nknowledge of each genre style will give rise to different\nanswers.\nTheevaluationofgenrelabellingsystemsishighlyde-\npendent upon the play of these two factors. For the eval-\nuation of genre labelling systems that only use individualgenresthese culturalfactorsmustbeeliminated.\nAttemptstoaddresstheseculturalissueshavebeenpro-\nposed, and are surveyed in [1, §4.5]. However, these do\nnotaddresstheissuethattheculturalmetriccouldproduce\nwidelydifferingresultsfordifferentgroupsofpeople.\n3 HUMAN GROUND-TRUTHSTUDIES\nThere have been relatively few studies of human genre-\nlabellingbehaviour. We areonlyawareoffour.\n3.1 Studies withincompletecoverageofthedataset\nTherearetwostudies([5]and[4])whichgiveincomplete\ncoverage of the dataset, and a full analysis is given in [1,\n§4.6.1]. The reliability of [5] is questionable, as insufﬁ-\ncientdetailsofthe experimentareprovided,andthedata-\nsetisnotadequatelycoveredbyallparticipants. Similarl y\n[4]hasincompletedatasetcoverageinoneofthetwocon-\nditions, and therefore its use in any understanding of the\nground-truth is precluded. The complete coverage under\nthe other condition allows an analysis like that described\nin§4to beapplied.\n3.2 Studies WithCompleteCoverageofthe Dataset\n3.2.1 Perrot andGjerdingen(1999)[7]\nIn this much cited but still-unpublished study, 52 college\nstudents were presented with short excerpts of music of\nvarious lengths, and asked to categorise the excerpts into\none of ten genres. In one of the conditions the parti-\ncipants’agreementwiththeclassiﬁcationprovidedbyweb-\nbased CD-vendors was around 70%. The exact details of\ntheexperimentaregivenin[1, §4.6.2].\nThe ‘results’ of this study are those most frequently\ncited in the literature, and the 70% accuracyis oftenused\nas some sort of benchmark. Whilst the research and res-\nultsmaywellbesoundtheexperimentcanonlybeviewed\nas anecdotal,since it is unpublishedand the experimental\ndataisunavailable[2].\n3.2.2 Lippens et al.(2004)[3]\nLippenset al.[3] conducted a human labelling of the\nMAMI dataset, a collection of 160 full tracks of music.\nThe tracks came annotated with 11 musical genres, but\nsomeof thegenreswereverypoorlyrepresented( i.e.had\nvery few tracks) or were heterogeneous. As a result the\nauthorsconducteda user studywith 6 genres(Pop,Rock,\nClassical, Dance, Rap and Other) to conﬁrm that their\ndeﬁnitions were consistent among different subjects [3,\n§3.1].\n27 participants listened to the central 30 seconds of\neach track mand independentlychose one genre sout of\nthe 6 possibilities. In evaluating the genre-classiﬁcatio n\nsystemtheauthorsonlyusedtracksthatsatisﬁedtwocon-\nditions: the elected genre could be any but ‘other’; the\nnumber of votes for the elected genre had to be greater\nthan 18. The trackswhich satisﬁed these conditionswere\nusedtoconstructa seconddataset: MAMI2.4 ANALYSIS OFLIPPENS ET AL.\nTheoriginalanalysisoftheseresultsisgivenin [3]. They\nrequired, for the purposes of their evaluation, for each\ntracktohaveauniquegenre. Thishastwoadverseeffects:\nan artiﬁcial worsening of the performance of human par-\nticipantsintheMAMIdataset;andamisunderstandingof\ntheactualstructureofthedata.\nWe have re-analysed the data from this study. With\nreference to this new analysis we illustrate and address\nthese twoissues.1\n4.1 TracksWithMultiple ElectedGenres\nThere were 4 tracks which had multiple elected genres.\nFor these four tracks only one category was used in the\noriginal analysis as the categoryfor the track. If either of\nthe genres that satisfy G(m)\nmax2is allowed as the genre for\na track then the percentage corresponding classiﬁcation\nacrossthewholedatasetis 77.3%,withstandarddeviation\nof 6.1% and minimum and maximum values of 59.38%\nand 86.88% respectively. This is differentto the statistic s\npresentedintheoriginalpaperfortheMAMIdataset. The\nambiguity over these four tracks was not highlighted in\n[3], althoughtheywereremovedin theMAMI2dataset.\n4.2 The `Other'Problem\nInconstructingtheMAMI2dataset[3]discardedanydata\nwhere Q(m)\nmax<18,3as these are “mainly tracks with\nmanyvotesfor‘other’,andlittleconsensusamongthehu-\nman listeners” [3, §3.1]. The handling of these tracks to\nformtheMAMI2datasetwasnotstatisticallywell-founded.\n4.2.1 WhatDoes‘Other’Mean?\nThe main problem with the ‘other’ category is that its\nmeaning is undeﬁned to the participants. It can be inter-\npretedinseveralways: thecategorytoassigntoanytrack\nthat does not fall into any of the other categories; the cat-\negory to assign to any track that a particular participant\ndoes not know the genre of; or the category to assign to\nanytrackthat usesfeaturesoftwo,ormore,genres.\nThe ﬁrst and third of these served the purposes of the\nexperimenters well, as they eliminates tracks that do not\nfall into any genres, and those that fall into hybridgenres\nrespectively. However, the second causes problems as it\nintroducesnoiseintothedatabydemandinganambiguous\nanswerfromtheparticipants.\nIf,forinstance,thereisatwo-waysplitbetween‘other’\nand another genre the possible interpretation of this data\nis dependant upon which of the ﬁrst two interpretations\nare in play above. Those who voted for ‘other’ may not\nknow the genre of the piece, or else realise that it is in a\nsub-genre of the alternate genre. It is therefore possible\nthat tracks with many votes for ‘other’ couldbe strongly\ncategorised if a different question had been asked of the\nparticipants.\n1Forthebeneﬁtofthereader wehaveused thesamenotation asu sed\nin theoriginal study where appropriate.\n2Thegenre with the maximum number of votes for aparticular tr ack\n3Q(m)\nmaxis the number ofvotes for the elected genre for track m.4.2.2 VotesFor‘Other’inTracks ThatAren’tinMAMI2\nFigure1showsthedistributionofvotesfor‘other’intrack s\nnot in MAMI2. Although many of the tracks have many\nvotesfor‘other’,manyhavefew,ornovotesfor‘other’—\nthe modal value is 0, and 36% of these tracks have less\nthan 6 votes for ‘other’. This is contrary to the statement\nthat “these are mainly tracks with many votes for ‘other’\n”[3,§3.1]\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17510\nnumberofvotesfor‘other’numberoftracks\nFigure1. Histogramofthenumberofvotesfor‘other’in\ntracksnotinMAMI2dataset\nThe votesgivenfor tracks that did not make it into the\nMAMI2 dataset, excluding those where G(m) =‘other’\nareshownin Figure2.\nAs can be seen from the data there is little subjectiv-\nity in the votes for some of these tracks: votes are typ-\nically distributed between two genres, rather than more\nrandomly assigned across several genres. In many of the\ncaseswherethereisa3-genrespliteitheroneofthegenres\nis ‘other’, which illustrates the possibility that ‘other’ is\nbeing used as a wild-card, or else one of the genres has\noneortwovotes,andthereforemaybeaparticipanterror.\nThemajorityofthetracksarevotedforinwayssimilarto\nother tracks: whilst an individualvote may be subjective,\nthese results collectively are only subjective if a unique\ngenreisrequiredtorepresenteachtrack.\n4.3 Howmany genresareidentiﬁed?\nThe authors of [3] required a dataset which was unam-\nbiguous in order to evaluate their system. However, the\nmethod they used to create this dataset was not sufﬁcient\ntoguaranteeunambiguousplacementoftracksintounique\ngenres.\nIn order for the desired criteria to be achieved we pro-\nposetwonecessaryconditions:\n1. Inorderfortrack mtoformpartofthesetoftracks\nrepresentative of a particular genre Git has to be\nvotedforinawaythatis dissimilar tothewaytracks\nin adifferentgenrearevotedfor.\n2. In order for a track mto form part of the set of\ntracks representative of a particular genre Git has\nto be voted for in a way that is similarto the way\ntracksin thesamegenrearevotedfor.\nTheauthorsof[3]informallysatisﬁedtheﬁrstofthese\nconditions,buttacitlyassumedthatsatisfyingtheﬁrstco n-\ndition would, necessarily, satisfy the second. We have\nusedtheKolmogorov-Smirnovtestforthesepurposes,which5 10 15 20 25\nOther Rap Dance Classical Rock Pop144158122354542471495114514798954969515399104100101108113829785889086604825802211061554972641597975\ntracknumbernumberofvotes\nFigure2. Distributionofvotesfortrackswith Q(m)\nmax<18\nandG(m)/negationslash=‘other’\ndetermines whether two underlying probability distribu-\ntionsdiffer,baseduponﬁnitesamples. Itprovidesameas-\nureofsimilaritybetweenthetwoprobabilitydistribution s.\nFigure 3 shows the similarity between all tracks in the\nMAMIdatasetasa similaritymatrix,sortedsothattracks\nthathaveasimilar distributionofgenrevotesaregrouped\ntogether, within coarse boundaries that reﬂect G(m)for\neach track. White areas show groups of tracks that are\nsimilar, black areas show tracks that are dissimilar. As\ncan be seen in this graph there is strong structure in the\nwayspeoplevotedfordifferenttracks.\nGroupsoftracksthat areall similar to eachotherform\nsquares that run along the main diagonal. Every actual\ngenre in the study has such a group of tracks. However,\ntherearegroupsoftrackswhicharesimilarbutwhichspan\nmore than one genre and some global genre boundaries\nincludemorethanonegroupofsimilartracks.\nFromFigure3weinferthattheparticipantsidentiﬁed8\ngenres,basedupontheassumptionthattracksinthesame\ngenre will be voted for in a similar way, irrespective ofOther Classical Pop RockDanceRap\nFigure 3. Similarity matrix for data collected by Lippens\net al.. Heavy black boxes indicate genre majority votes,\ndottedblackboxesindicateself-similargroupingsnotam-\nbiguouswithothergenres.\nhowthosevotesarespreadacrossthedifferentchoices.\n4.4 Inclusion/exclusion criteria\nIf we wish to have an human-annotated test set with un-\nambiguousgenre-labelsthentrackswhicharefoundtobe\nequivalenttoothersinadifferentgenremustbediscarded.\nSimilarly, any minor sub-genres within a genre that are\nidentiﬁed should be discarded, as they betray the inﬂu-\nence of other genres: there are 13 tracks in MAMI2 that\nshouldhavebeendiscarded.\n5 TOWARDSAN IMPROVEDTEST SET AND\nEVALUATION METHODOLOGY\nWhilstitmaybeusefultohaveagenrelabellingofadata-\nset that is entirely unambiguous, it is also unrealistic: a\npractialcategorisationsystemneedstobeabletodealwith\nalltracks,notjust unambiguousdata.\nWe propose that the results of a genre classiﬁcation\nsystem shouldbe weightedto reﬂect theamountof ambi-\nguitygiventothegenrelabellinginahumantask. Higher\npenaltiesshouldbe incurredformisclassiﬁcation ofthose\ntrackswhichthe humanparticipantsunambiguouslyclas-\nsify. If the system were to misclassify a track which is\nambiguous in terms of a single genre the system should\nnotbepenalisedforcategorisingintoanyofthosegenres.\nHowever,it shouldbe penalisedforcategorisingthetrack\nintoanygenrethatisnotoneofthosegenres. Thismethod\nwould adequately cover around 85% of the MAMI data-\nset.\n6 CONCLUSIONS\nThis paper has reviewed some important issues inherent\nintheevaluationofthegenreclassiﬁcationtask. Weargue\nthat researchers have not paid sufﬁcient attention to the\nevaluationofthetaskastotheclassiﬁcationmethodsused,\nrelyingfrequentlyon adhocmethodsofevaluation,typic-\nally againstan anecdotalresult [7]. However,at the sametime authors have frequently stated that musical genre is\ninherentlysubjective,whichcallssuchamethodofevalu-\nationtotask.\nWe have proposed that there are cultures of genre-\nlabelling behaviour, and that the structure of these cul-\ntures of behaviour needs to be better understood in order\nto evaluateproperlythe results ofany system that models\ngenre-labelling. We are currently undertaking a number\nofexperimentsto analysetheseculturesofpractice.\n7 ACKNOWLEDGMENTS\nWe would like to thank all those with whom we have in-\nformallydiscussedtheseissues,inparticularDavidLewis\nandDanielM¨ ullensiefen,whoalsoprovidedhelpfulfeed-\nback on this paper. We would also like to thank Matthias\nVarewyck for providingus with the data originallyrepor-\ntedin[3]andBobGjerdingenforhisclariﬁcationofvari-\nousissuesarisingfrom[7].\n8 REFERENCES\n[1] Alastair J. D. Craft. The role of culture in the mu-\nsic genre classiﬁcation task: human behaviourand its\neffect of methodology and evaluation. PhD Transfer\nReport, 2007. http://www.doc.gold.ac.uk/\n˜map01ac/transfer.pdf .\n[2] Robert O. Gjerdingen. Personal communication to\nAlastairCraft, June2006.\n[3] Stefaan Lippens, Jean-Pierre Martens, Tom\nDe Mulder, and George Tzanetakis. A comparison of\nhuman and automatic musical genre classiﬁcation. In\nIEEE International Conference on Acoustics Speech\nand Signal Processing (ICAASSP) , volume 4, pages\n223–236,2004.\n[4] Anders Meng, Peter Ahrendt, and Jan Larsen. Im-\nproving music genre classiﬁcation by short-time\nfeature integration. In IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing—\nProceedings ,volumeV,pages497–500,March2005.\n[5] Anders Meng and John Shawe-Taylor. An investiga-\ntion of feature models for music genre classiﬁcation\nusing the support vector classiﬁer. In Joshua D. Reiss\nand Geraint A. Wiggins, editors, Proceedings of the\nSixth International Conference on Music Information\nRetrieval, pages 604–609, London, September 2005.\nQueenMary,UniversityofLondon.\n[6] Franc ¸ois Pachet and Daniel Cazaly. A taxonomy of\nmusical genres. In Proceedings of the Content-Based\nMultimedia Information Access Conference (RIAO) ,\nApril2000.\n[7] D.PerrotandR.O.Gjerdingen.Scanningthedial: An\nexploration of factors in the identiﬁcation of musical\nstyle. Research notes, Department of Music, North-\nwesternUniversity,1999."
    },
    {
        "title": "Finding New Music: A Diary Study of Everyday Encounters with Novel Songs.",
        "author": [
            "Sally Jo Cunningham",
            "David Bainbridge 0001",
            "Dana McKay"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417083",
        "url": "https://doi.org/10.5281/zenodo.1417083",
        "ee": "https://zenodo.org/records/1417083/files/CunninghamBM07.pdf",
        "abstract": "This paper explores how we, as individuals, purposefully or serendipitously encounter “new music” (that is, music that we haven’t heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of “laid back” searching.",
        "zenodo_id": 1417083,
        "dblp_key": "conf/ismir/CunninghamBM07",
        "keywords": [
            "new music",
            "music information retrieval",
            "diary study",
            "Grounded Theory",
            "location",
            "time",
            "music searching",
            "music discovery",
            "recommender systems",
            "design implications"
        ],
        "content": "FINDING NEW MUSIC: A DIARY STUDY OF EVERYDAY ENCOUNTERS WITH NOVEL SONGSSally Jo Cunningham David Bainbridge Dana McKay sallyjo@cs.waikato.ac.nz Department of Computer Science University of Waikato Hamilton New Zealand  davidb@ cs.waikato.ac.nz Department of Computer Science University of Waikato Hamilton New Zealand dmckay@swin.edu.au Information Resources Swinburne University of Technology Melbourne Australia ABSTRACT This paper explores how we, as individuals, purposefully or serendipitously encounter “new music” (that is, music that we haven’t heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of “laid back” searching. 1. INTRODUCTION A variety of information sources and systems are available to support people in locating music: artist and music review websites, music recommender systems (for example, Pandora, and Last.FM), music databases, even general purpose search tools such as Google. However, there has been relatively little research to date on how people encounter new music ‘in the wild’—that is, the formal and informal ways that they search, browse, borrow, overhear, discuss, recommend, and otherwise run across music that they are unfamiliar with. We argue that a rich understanding of behavior related to finding new music can inform the refinement of existing MIR systems. In this paper, we describe a diary study conducted to begin building that rich understanding. As noted above, relatively little related work has been reported in this area. Two earlier studies also use a diary study methodology to examine the impressions people have of everyday music encounters: one formal [2] and the other informal (an online survey conducted by the BBC11). Both focused solely on ‘intrusive music’ that participants heard but had no control over playing—for example, music played in retail environments. This paper extends that work by broadening the scope to all encounters with new music, both music that is purposefully sought by and that which is thrust upon the participants. We relate the findings of this prior work to our analysis in Section 3, Results and Discussion. 2. METHODOLOGY A diary study methodology was chosen to gather real-life experiences of encounters with unfamiliar music. Diary studies are particularly useful in capturing the “little experiences of everyday life” [7]; participants literally fill in a diary detailing their activities, thoughts, and feelings for the topic or task that is the focus of the study. The diary provides a record of events as they actually occur, rather than the retrospective (and sometimes faulty) recollections provided by, for example, surveys and interviews. A diary study can also allow the researchers to examine a relatively longitudinal record of behavior (over the course of time that the diary is maintained) [1]. Participants for this study were drawn from a third year university course on Human-Computer Interaction. The diary study was an initial step in a semester-long project to design and prototype a system that would support the user in locating interesting new music. A total of 41 students were charged with the task of maintaining a diary for three days, recording each incident in which they encountered music that was unfamiliar to them.  \nTable 1. Demographic details of participants To preserve anonymity, each participant is referred to with one or two randomly assigned letters of the alphabet (e.g., [G] or [BB]). Table 1 summarizes the gender, age, and national origin of the participants. The participants are representative of tertiary IT students: the group is dominated by male participants, the majority are school leavers who went straight to university or at most took a gap year, and the high Chinese demographic is characteristic of IT courses in New Zealand. We are aware that this study documents the behavior of a narrow demographic (one which is more likely than Gender Count (%) National Origin Count (%) Male 33 (80.5%) NZ 17 (41.5%) Female 8 (19.5%) China 17 (41.5%)   Other Asian 4 (9.6%)   Other 3 (7.3%) Age range: 20 – 37 years Average age: 22.8 years \n© 2007 Austrian Computer Society (OCG).     \n the general population to use electronic media for music discovery), however, at the time of the study it was not feasible to recruit a sizable sample of other participants, and, for reasons outlined below, other participants may have produced lower-quality results The diary consisted of a paper template. For each incident in which novel music was encountered, the participants were requested to record: the time and date; their physical location; a description of the activity or circumstance that led to their hearing the music; and any comments that they had about the incident (for example, whether or not they liked the music). A total of 409 data entries were recorded. Diary entries in such studies tend to be terse—the ones in our study averaged 28.7 words in length—consequently participants are typically interviewed after the diaries are completed, to flesh out the event descriptions. For ethical reasons we waited until after the end of the semester before using the diaries as research data, however this meant it was not possible to schedule de-briefing interviews. Instead, the students were asked to write a 1–2 page report summarizing their diary entries. These reports were used to guide the research analysis in the interpretation of the diary data. Diary studies are known to have limitations beyond those of other qualitative data gathering techniques such as interviews and surveys [1]. Most significant is the relatively high level of effort required of participants, to create detailed and comprehensive records of their activities leading to an attrition rate in participants over time. Because the students involved knew that they would be using the data collected in their own diary as part of the requirements gathering for a subsequent prototype, we posit that they had a stronger sense of commitment to the diary process and deliberated more over writing entries than typical diary study participants. For the same reason (that the diary was part of a graded project), attrition was not a problem for this research. Compliance can also be improved by reducing the burden of data entry for the participant: in this case by limiting the diary period to three days, and by requesting as few details about each incident as is practical. The diary design was also intended to minimize under-reporting: a template prompts the participant to describe each of the aspects of interest.  Reactance—a change in participants’ behavior as a consequence of participation in the study—is of concern in diary studies [1]. The students noted that during the study they were more consciously aware that they were hearing new music, although it is not clear that this actually affected their activities leading to actual exposure to new music:  The serendipitous examples in the diary, I would argue, only came into my conscious mind because I knew I was going to have to report on them. On an average day I would not register what was playing on the radio ... [it] is on as background noise. [L] However, other portions of the requirements gathering for their projects almost certainly did alter their behavior. Specifically, it is probable that some reported uses of online music recommendation systems (for example, Pandora and Last.FM) occurred as a result of a requirement, for another part of the course, to critique an existing recommender system. A Grounded Theory approach was used to analyze the diaries and the participants’ summaries of their behavior [3]. Grounded theory is particularly well suited to analyzing this type of data, because it allows patterns to emerge from data where no hypotheses exist. A grounded theory analysis involves identifying possible areas of similarity, and ‘coding’ them. For this data, codings were identified to summarize the location category in the diary template, and themes were identified in the free text diary entries (description of the event, comments).   3. RESULTS AND DISCUSSION We now present and discuss the results of our analysis. We start by considering the time distribution and frequency of incidents, and their physical location. Then we study whether the participant liked in general what they heard and whether the incident lead to any follow-up action. Visual considerations and how participants hear (and hear about) music form the last two categories of analysis. Terms in the narrative text which correspond to classification labels are typeset in bold.  \nFigure 1. Time distribution of encounters with new music 3.1. Time distribution and frequency of incidents A striking aspect of the time distribution of encounters with new music is that they occur throughout the day (see Figure 1). Many participants live their lives immersed in music, and in opportunities to find new music: \n   \n The range of times are from 8:15AM (just after I got up) through to 11:40PM. This tells me that I am liable to encounter new music at any time through out the day, except quite obviously when I am sleeping. [K] For some participants, opportunities for exposure to new music are linked to the availability of leisure time: I seem to find new music later in the day. This is probably because I typically only have time to listen to music for extended lengths of time after midday or so. [O]  …lack of new music encounters…there is very little time in my life that is available for listening to music. [L] For others, music is used frequently as a background when studying and during paid employment—extending the opportunity to encounter new music throughout most of their waking day. The degree of exposure to music, and the variety of activities that lead to encounters with new music, came as a surprise to some participants (“…I never thought I was doing all these things and exposing myself to new music as often as I am” [I]).  3.2. Physical Location Just as the participants could encounter music at most times of day (Figure 1), they also reported discovering new music in a variety of locations (Table 2). The vast majority of new music encounters occurred in private residences: in the participants’ own home, or while visiting the homes of friends and relatives.   The next most significant exposure occurred while participants were en route from one location to another: driving, taking a bus, or walking. When making short trips by car or bus, the radio is frequently used as a background, and may not be consciously attended to (“On an average day I would not register what was playing on the radio in the car. ... the radio is on as background noise.”; [L]). For longer trips the choice of music is more crucial, and generally one of the passengers will provide music for everyone in the car to listen to; it can be awkward if the passengers do not all share the same tastes. When walking, participants sometimes overhear new music (from passing cars, or other chance sources), and sometimes are surprised to encounter new music on their own mobile devices (“skipping randomly through tracks on my iPod, found something new”; [O]).  Clubs and retail environments bombard us with music as we drink, shop and dine, and the participants’ workplaces are primarily these types of commercial organizations as well. While at the University, students overhear music in the hallways, trade music on USB drives and CDs, listen in on each other’s mp3 player headphones, and play music in the labs late at night. Music complements gym activities, although those songs may not be appreciated in other contexts (“before didn't really like the music … maybe listening to them while doing exercise is a better combination” [N]). Table 2. Physical location when encountering new music A total of 409 incidents were recorded, giving an average of 3.33 encounters per day per participant. This incident count is a lower bound on the number of new songs encountered—the duration of the incidents ranged from a few seconds (hearing a mobile phone ring) to six hours (listening to a classmate’s music during a late-night computer lab session). The range for incidents per participant ranged from three to 17  encounters in three days. 3.3. Was the music any good? Each incident was coded as Positive (the participant liked at least one of the songs encountered), Negative (the participant disliked all the songs encountered), Indifferent (the participant neither liked nor disliked the songs; they were “so-so” [R] or “not too bad” [J]); or Unknown (the participant didn’t record any reaction to the song). The majority of incidents—nearly two-thirds—resulted in the participants being introduced to one or more songs that they liked (and sometimes that they liked very much indeed; for example, “Classic, a masterpiece” [D] or “Love it! Want to hear more” [S]).Table 3 summarises the overall reactions to new music This correlates with the findings of the informal BBC diary study of reactions to ‘non-chosen music’ (that is, music that the participants had not personally selected to play and correspond to our ‘passive’ labelled Location Description Count  (% of Total) Liked it? (% of Location) Residence Residence of participants, friends, relatives 241 (58.9%) 167 (69.3%) En Route In car, bus, or while walking 58 (14.2%) 30 (51.7%) Retail  Stores, restaurants 39 (9.5%) 25 (64.1%) University Lecture halls, labs, hallways 30 (7.3%) 18 (60%) Workplace Employment outside university 17 (4.2%) 8 (47.1%) Club Nightclubs, bars, pubs 8 (2%) 4 (50%) Gym Exercise music at gym 4 (1%) 3 (75%) Unknown Location not identified  4 (1%) 3 (75%) Other Hospital, band practice, etc. 7 (1.7%) 5 (7.1%) Total  409 incidents (100%) 255 Positive incidents    \n incidents analyzed in Section 3.4). In the BBC study, 62% of diary entries were either positive (28%) or neutral (38%); most of the non-chosen music was not viewed negatively. Table 3. Reactions to new music 3.4. Active and Passive Diary Incidents To gain further insight in to what our participants were doing when they encountered new music, we classified entries as either passive or active. The active category captures incidents when participants were specifically looking for new music: Web activity being the most common (this perhaps reflects the nature of the participants, who are IT students and universally web-savvy), but also including purposefully going into a music store to purchase a CD, for instance. The passive category correlated strongly (but not exclusively) with exposure to a broadcast medium such as radio or TV. Music encounters on broadcast media were not always passive, however; for example if a participant recorded that they specifically went looking for music to listen to on a music channel during an advert break, then this was tagged as active, as opposed to some flatmates having a music channel on in the background (passive). Sometimes an incident started out passive, but then transitioned to active: Participant [D] was watching a music channel and “The music video looked great, so I tried a few other songs, but they were just too noisy for me.” We recorded such events as active. 260 (63.6%) of incidents of new music encountered were passive, 146 (35.7%) active, and 3 could not be determined from the information recorded. We studied the data further to see if there was any correlation between active or passive incidents and any follow up action taken. While there was no significant difference in the desire for follow up action between the two categories—24.7% and 22.7% for active and passive cases respectively—a strong recurrent theme in the comments column for passive entries was that the participants felt that there was little they could do immediately to act on this desire. The participants’ location (in a car, at a friend’s house, in a shop) or the activity at the moment (talking to a friend, watching a movie) made it difficult to record details of the music for later searching, much less actually attempt to locate the music. This is a point we return to in Section 4, when we discuss implications for music information retrieval based software. Several participants recognized that their overall strategy for new music acquisition was either passive (“…I found I do not actively search for new music. It more finds me” [J]) or active (“I heard and try to find new music when I was waiting and feel boring. So the result of feeling is to try to find something around to interest myself.” [AA]; “…encountering new music I like is clearly not a day-to-day activity that just happens. I usually have to initialise the search for new music.” [FF]). There is a clear need to support both styles of interaction; this will be discussed further in Section 4. 3.5. Media: how people hear (about) music As we go about our everyday lives we hear, and hear about, new music in an astonishing variety of ways (Table 4). Students seem to practically live on the Internet (again, the participants' academic background in computing probably contributes to this), and the participants came across new music in practically every Web resource imaginable (in addition to music-related websites, MSN chat, MySpace, email, news feeds, etc). Other computer-related sources of diary entries include media players, music, video, and games.  \nTable 4. Media with which new music is encountered Radio, TV, and Movie/DVD encounters with music were, to a great extent, also under the control of the participants: they could, and often did, channel surf until they found music that was at least tolerable (“just flipping through the channels” [P]). In retail environments music is broadcast to the public—a prime example of the ‘intrusive music’ described in [2]—the hearer may find the music pleasant or neutral. A surprising number of diary entries (17) referred to live performances; these included professional musicians as well as amateurs (“Builders next door woke me up with their singing and butchering of an AC/DC song” [CC]). Participants shared music by exchanging or listening to music with CDs and their mp3 players. Ringtones frequently include snatches of songs that are overheard when a nearby cellphone goes off, or sometimes listened to for fun (“laying out with mates and looking through ringtones. yea! Got them to 'bluetooth' the tones to me.” [J]). Conversations about music often led to recommendations of music or artists to try.  3.6. Visual Aspects of Music The visual aspects of music can strongly influence the amount of attention paid to a new song, and the final like/dislike decision about the music. Sometimes the first encounter with the music is largely or even solely visual, rather than aural: first time saw hayley westenra new album…and got notice to that; like the cover when first saw it. What a beautiful girl [E] Positive Negative Indifferent Unknown 225 (62.3%) 100 (11.1%) 21 (5.1%) 33 (8.1%) \nMedia Count Media Count Internet 81 (21.8%) Performance 17 (4.2%) Radio 77 (18.8%) MP3 Player 16 (3.9%) TV 54 (13.2%) Ringtone 6 (1.5%) CD 46 (11.2%) Conversation 5 (1.2%) Public broadcast  27 (6.6%) Other 12 (2.9%) Computer 24 (5.9%) Unknown 26 (6.4%) Movie/DVD 18 (4.4%) Total 409 (100%)    \n An attractive music video catches the eye and causes us to focus more on the music than we otherwise might: I turned on the tv and C4 was playing a show called 'blender' with a section called \"Try it…you might like it\" Death Cab for Cutie were playing, who I have never heard before. I really liked it and I stared at the screen to make sure I didn't miss the name of the band... I will pursue them. [V] The best music videos complement the song (“Quite like it. The song and the video matched perfectly.” [HH]). The worst draw our attention to the aspects of the music, artist, and genre that we dislike (“Awful stuff. Beonce—no substance just boobs” [L]). Sometimes, though, the music is a disappointing accompaniment to interesting visuals (“the video was ok, but not really like the music at all.” [M]). Table 5 summarizes references to the visual aspects of music recorded in the diaries. \nTable 5. Visual aspects of new music Soundtracks to movies, TV shows, and TV commercials may be interesting, but it can be difficult to find out enough about the songs to be able to locate them later (TV: “I am out of the room when they show the name of the group so am unable to look them up.” [S]; Movie: “Good songs, but didn't know who they were by so will have to review credits.” [CC]). The difficulty of identifying, and recording, music metadata for later use is a recurring theme in the diaries. 4. IMPLICATIONS FOR MIR SYSTEM DESIGN Given these insights into how we encounter new music, we now turn our attention to how—in the context of user-centered design [5]—music information retrieval software can better support such patterns of behavior. Some comments reaffirm the rationale for existing software functionality and features; some suggest innovative ways that software (and related peripheral devices) can be shaped in the future. From the diary entries we can see that active approaches to music encountering are fairly well supported by existing software and information systems, particularly internet-based approaches.  Not so well-supported, however, are passive encounters with new music, particularly those that involve non-internet media, such as radio or television.  The suggestions we make here deal primarily with more passive music encounters, and those that occur in situations where users can do nothing to follow up on an encounter immediately. The participants encountered new music at a wide variety of locations, at all hours of the day and night. In the majority of diary incidents, the students were not actively seeking new music; rather, new songs were encountered while engaged in other activities. New music might be encountered completely serendipitously (for example, another person’s mobile phone ringtone) or might be a windfall from another pursuit (for example, the soundtrack of a movie, or background music at a restaurant). This anytime/anywhere occurrence strongly suggests the use of a mobile platform to help capture ‘discovered’ music (or at the very least, to help capture the necessary metadata to follow up one’s music discoveries). Nearly a quarter (23.4%) of the diary entries sparked a follow-on action of some sort. Media players and rich media sites (based around technologies such as Flash) that expose users to new music cater well for this pattern of use, for example, with “now playing” information and built in web browsing capabilities and links to related information.  Traditional radio and TV, however, fare less well with entries such as: “I like the R&B songs. Trying to memorise.” [Z], when listening to the radio and “Good songs, but didn't know who they were by so will have to review credits.” [CC], while watching a movie. These comments reflect frustration of potential music discoveries; sparks of interest have to be expressed as intentions rather than actions because the environments participants are in do not support doing anything else. The two quotations above are representative of several such remarks relating to radio and TV. The data points to a significant gap in new music encounters between these media and on-line based experiences. Even shifting radio and TV to digital broadcast does not seem to improve the experience much, as—despite the much touted benefits of the integrated media experience—broadcasters to date do not seem to either have the inclination (or resources) to exploit the capabilities. Here we propose a way of bridging this divide between experience of new music and follow-up based on the technique of “laid back” searching [4]. Our suggestion has the additional benefit of unifying other forms of discovery, such as capturing social interactions when friends recommend music. “Laid back” searching is a web searching method that caters for when users are not on-line, allowing them to record web queries at the moment they think of them on their (off-line) mobile devices. When a user’s mobile device re-establishes a network connection, the recorded queries are run and a bespoke interface allows users to browse the results. Recast to support users as they discover music, a “laid back” search approach is well suited to being out and about: in the car, retail outlet, or friend’s house—popular places in our study.  We propose an extension of this technique on a device with a microphone. The device would normally be on “standby”, and when a user hears new music (or any music of interest) that they want to follow up, a simple button press on their mobile device (phone, iPod, PDA) would record a snippet of the music. The device would also record the time of day and other metadata Visual media  Diary entries Liked it? Music Video 33 17 Movie (in theatre, on TV, on DVD) 12 10 Other TV (commercials, show soundtracks, unspecified content) 27 15 Other DVD (content not specified) 6 6    \n (such as GPS co-ordinates, if available) to help present the recorded snippets when the device is docked. Once the device is docked, the software would use audio fingerprinting [6] in connection with a server to assign metadata (song title, artist, etc.) to each sample. Additional audio-based music information retrieval techniques could also be used to determine (with manual override) whether the audio being sampled is music, or speech; this information could then be used to provide richer user interaction when the device is docked. In a mode tailored for radio support, the device would record preemptively—say with a 10-minute buffer. This opens up the possibility of going back to the start of the song if the user signals interest in it. When the device is docked, MIR techniques would be used to find the beginning and end of the song and capture any speech at either end of the song (which may be the DJ naming the song or the artist); this information would in turn be presented to the user in a timeline. The software could also exploit information from stations broadcasting textual metadata using the RDS (Radio Data System). Using RDS metadata is notably simple for digital radio.  Personal recommendations made by friends would be captured with the device in ‘dictaphone’ mode, unless the friend could also play or sing the song, in which case radio mode-like interaction would be used. Some instant messaging (IM) services allow users to publicize the music they are currently listening to; 1.5% of the diary entries in this study reflect new music encounters as a result of IM (“Internet surfing, saw friend's MSN list showing the music he listening. Ask him sent it to me.” [R]). This type of music discovery could be promoted with a new play mode for an existing music player; in the new mode the player would monitor what a user’s ‘buddies’ are listening to and allow the user to hear excerpts of that music. Rather than using a peer-to-peer service (which raises copyright issues), we suggest excerpts be found heuristically using text based metadata and the user’s existing music sources. Heuristic matching does present the risk of the ‘wrong’ music being played, but this may also be a fun way for users to discover new music. In light of this study, online radio-style recommender systems such as Pandora are well aligned with passive activities. Beyond searching for a seed song interaction is passive: the recommender system plays recommended songs and the user responds to them (yes/no/indifferent). More active behaviors (searching, browsing, exploring) are not well supported in this case—yet frequently the participants transition from passive to active behavior. This suggests a recommender system that also offered the search features offered by many media players would better meet user’s needs. 5. CONCLUSION There is a tension in software design between the functionality provided and how it is, at times, put to use by a user-community. It is a symbiotic relationship: novel software applications, for instance, can guide users to new possibilities and ways of interacting. Conversely, users can use (subvert!) software in unexpected ways—to achieve goals not envisaged by the original designers—which can ultimately feed back to further software innovation. In this paper we have considered one such relationship within the context of music information retrieval: how people find and encounter new music. Using a diary study, we intentionally set the scope of the study to look beyond just computer-based encounters to gain a richer picture of how people find ‘new’ music. In relating our findings to music information retrieval software, we have reaffirmed the usefulness of existing software functionality, such as the low-effort aspect of the interface to recommender systems such as Pandora, and provided some insights for future design, such as the extension of ‘laid back’ searching to music retrieval, including a discussion of how music discovery might be supported by mobile devices. 6. REFERENCES [1] Bolger, N., Davis, A., and Rafaeli, E. “Diary Methods: Capturing Life as it is Lived”. Annual Review of Psychology 54, 2003, pp. 579-616. [2] Gavin, H. “Intrusive Music: the perception of everyday music explored by diaries”. The Qualitative Report 11/5, 2006, pp 550 – 565. [3] Glaser, B., Strauss, A. The Discovery of Grounded Theory. Aldine, Chicago IL, 1967. [4] Jones, M., Buchanan, G., Cheng, T-C, and Jain, P. “Changing the pace of search: Supporting “Background” information seeking”. JASIST 57(6), 2006, pp. 838-842. [5] Norman, D., and Draper, S. User Centered System Design: New Perspectives on Human-Computer Interaction. Lawrence Erlbaum Associates, Mahwah (NJ), 1986. [6] Wang, A. “An industrial-strength audio search algorithm”. Proceedings of the 4th International Conference on Music Information Retrieval, Baltimore, 2003. [7] Wheeler, L., and Reis, H.T. “Self-recording of everyday life events: origins, types, and uses”. Journal of Personality 59, 1991, pp. 339-354.                                                              1 , http://www.bbc.co.uk/radio4/arts/frontrow/reith_diary.shtml"
    },
    {
        "title": "Finding Music in Scholarly Sets and Series: The Index to Printed Music (IPM).",
        "author": [
            "Elizabeth Davis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417747",
        "url": "https://doi.org/10.5281/zenodo.1417747",
        "ee": "https://zenodo.org/records/1417747/files/Davis07.pdf",
        "abstract": "The Index to Printed Music (IPM) provides access to sets and series of  music published beginning  in the 19th century.  Prepared by scholars and researchers, these titles  vary  considerably  in length  (single  to multiple volumes), types (topical, pedagogical, historical, etc.), format   (treatises,  dissertations,  editions,  etc.),  and geographic origin (chiefly Europe and North America). Bibliographical access to their contents is not readily available through library cataloging or reference works. IPM provides title, format, genre, instrumentation, and other  metadata  access  to  these  publications  in  three inter-connected  databases:  Bibliography,  Index,  and Names,  available  by  subscription  through  NISC International, Inc.  The Index Database now contains over 255,000 entries, the Bibliography Database over 10,000 entries, and the Names Database over 15,000 authority records.  Having built this groundwork, future steps involve linking to full-text score images where available  through  non-commercial  projects,  and partnerships with publishers and commercial vendors.",
        "zenodo_id": 1417747,
        "dblp_key": "conf/ismir/Davis07",
        "keywords": [
            "Index to Printed Music (IPM)",
            "access to sets and series of music",
            "19th century",
            "scholars and researchers",
            "varying in length",
            "types",
            "format",
            "geographic origin",
            "Bibliography Database",
            "Names Database"
        ],
        "content": "FINDING MUS IC IN S CHOLA RLY SETS AND SE RIES: \nTHE INDEX TO PRINTED MUSIC (IPM)\nElizabeth Davis\nHead of Library\nColumbia  University\nWiener Music & Arts Library\nABS TRAC T\nThe Index to Printed  Music  (IPM) provides access to sets \nand series of  music publi shed beginning in the 19th \ncentury.  Prepared by scholars and researchers, these \ntitles vary considerably in length (single to multiple \nvolumes), types (topical, pedagogical, historical, etc.), \nformat  (treatises, dissertations, editions, etc.), and \ngeographic origin (chiefly Europe and North America). \nBibliographical access to their contents is not readily \navailable through library cataloging or reference works.\nIPM provides title, format, genre, instrumentation, and \nother metadata access to these publi cations in three \ninter-connected databases: Bibliography, Index, and \nNames, available by subscription through NISC \nInternational, Inc.  The Index Database now contains \nover 255,000  entries, the Bibliography Database over \n10,000  entries, and the Names Database over 15,000 \nauthority records.  Having built this groundwork, futur e \nsteps involve linking to full-text score images where \navailable through non-commercial projects, and \npartnerships with publi shers and commercial vendors. \n1.INTR ODUCTI ON\nThe typical format of musicological score publi cations \nbeginning in the 19th  century is in sets or series of \ncollected works.[1]  These editions provide the main \nsource material of the Western classical repertory for \nscholars, researchers, performers, educators, and music \nlovers.  In many cases, they contain the only available \nprinted version of a musi cal piece.  The Index to Print ed \nMusic  (IPM) project was established to index these \ncollected works.\n2.CATEGORIES OF PUBLICA TIONS\nCategories of these collected works include: a) \ncomposer-based titles , generally multi-volume \npublications; b) works of national or geographic origins \nin single or multi -volumes; c) anthologies of music on \ntopical subjects; d) pedagogical anthologies; and e) \nanthologies with a chronological organization.  The IPM \nproject is not limit ed to publications devoted exclusively \nto scores, but also indexes musi cal works found in \ndissertations and treatises on individual manuscripts. \nFacsimile publications, while  not editions but \nreproductions, are also indexed.  3.BIBLIOGRAPHICAL ACCES S\nBibliographical access to individual scores within series \nis severely limited.  Library cataloging generally \ndescribes such works at the series-title level,  or at best, \nat the individual title level. In card catalogs, individual \npieces within each volume were sometimes found in \ncontents notes cataloged under collective headings \n(Lieder, Werke, Works, etc.).  \nWith the advent of online catalogs, if the contents notes \nfrom catalog cards were converted, and if keyw ord \nsearching were enabled for contents fields,  a user might \nbe able to retrieve individual pieces through keyword \nsearches. Whether through the card or online catalog, \nlibrary users of music cataloging require specialized \ninstruction on finding individual piec es.\n4.IPM PROJECT \nIn 1982, a group of scholars led by Dr. George R. Hill \ndesig ned the IPM project to index individual pieces of \nmusic within these scholarly sets and series.  Funded by \ngrants from the National Endowment for the \nHum anities, the bibliographic titles to be indexed were \nidentified, and indexing was completed for half of these \ntitles .  The bibliography of titles  to be indexed was \npubli shed in 1997.[2]  In 2004 , the project received a \ngrant from the Mell on Foundation to finish the indexing \nof the original titles and to index subsequently-\npubli shed titles . Currently, the project databases number \n257,945 index records, 10,709 bibli ography records, and \n15,976 name authority records. In addition, the project \nis available on the web by subscription through NISC \nInternational,  Inc.1\nIPM combines three separate databases: Index, \nBibliography, and Names.  The Bibliography Database \nconsists of those sets and series completely indexed or \nin progress.  The Index Database contains the entries \nwith information about individual pieces contained in \nthe titles  in the Bibli ography Database.  The Names \nDatabase contains the entries for every personal name—\ncomposer, text author, editor--used in the project. All \nIPM databases are lower-ASCII flat files.\nThe key organizational elem ent for the Bibliography \nand Index Database is the Master Bibl. Number (>NO \n1 http://www.nisc.comfield). Each biblio graphical title is manually assigned a \nunique number which places it in a particular order in \nrelation to all the other titles.  This resulting \narrangement provides a list of editions arranged in \nchronological order, sub-arranged alphabetically.  The \nMaster Bibl. Number also provides the link between the \nbibliography title and its index entries.  \nEvery name (composer, poet, editor) is subject to \nauthority control and is established using the fullest \nform of name, along with birth and death dates.  Every \nname also has its own unique identifying number.\nBelow is an example of a bibliography record and two \nindex records of works contained in it.\n>B\n>NO B5166\n>NA Berlioz, Louis-Hector, |d 1803-1869. [B5163]\n>TI  Werke;\n>ED hrsg. C. Malherbe und F. Weingartner.\n>IM Leip zig:|b Breitkopf & H<8>artel, |d[1900-07].\n>PD 20 v.\n>NT Issued on microfiche: New York: University\n          Mu sic  Editions, [1968]\n>NT Reprint: Wiesbaden: Breitkopf & H<8>artel.\n>NT Reprint: New York: Edwin F. Kalmus, [1970], \n          mi niature score (44 v.)\n>AN Malherbe, Charles-Th<2>eodore, |d1853-\n          1 911.|red.       [M2 4955]\n>AN Weingartner, Felix  Paul, Edler von\n          M< 8>unzberg,      |d 1863-1942.|d red. [W42355]\n!\n>I\n>NO B5166-001, 3-150\n>NA Berlioz, Louis-Hector, |d 1803-1869. [B5163]\n>UT Symphonie fantastique\n>TI <2>Episode de la vie d’un artiste\n>OP op.14\n>TN H.48\n>FT f\n>ED Malherbe, Charles-Th<2>eodore, |d1853-\n       1 911.|red.      [M2 4955]\n>ED Weingartner, Felix  Paul, Edler von \n       M< 8>unzberg,  |d1863-1942.|d red. [W42355]\n>GE Symphony\n>IN |e flt2,  obo2, ehn2, cla2, E<9> cla, bsn4, hrn4, crn2,\n       t rp2, trb3, tba2, oph2, tmp2, per, bel, hrp4, vln2,\n       vl a, vlc, dbs\n>PE f DOV 0-486-24657-4\n>PE m KAL no.144\n>PE f DOV 0-486-29890-6\n!\n>I\n>NO B5166-015, 121-25\n>NA Berlioz, Louis-Hector, |d 1803-1869. [B5163]\n>UT4 Les nuits d’<2>et<2>e. |p Absence\n>OP op. 7 no. 4\n>TN H.85B\n>ED Malherbe, Charles-Th<2>eodore, |d1853-        19 11.|red.   [M 24955]\n>ED  Weingartner, Felix  Paul, Edler von\n        M<8 >unzberg,  |d1863-1942.|d red. [W42355]\n>LA  fregerent\n>LI Gautier, Th<2>eophile, |d1811-1872. [G27775]\n>GE Song, secular\n>TE Reviens, ma bien  aim<2>ee\n>IN |s mez sop/ten, |e flt2,  obo, A cla2, A crn, D hrn,\n       vln 2, vla, vlc, dbs\n>PE m KAL no. 1229\n>PE f DOV-0-486-42665-3\n!\n5.   FUTURE DIRECTI ONS\nThe next step is to make it possible for users to click \nthrough to the digitiz ed music score and/or the recording \nthrough a link in the index record.  We are in the \nprocess of identifying digit al collections available on the \nweb  with which to partner.  Two projects we’ve \nidentified thus far are the Variations2 Project at Indiana \nUniversity1 and the Neue Mozart-Ausgabe/Digital \nMozart Edition2   being carried out by the Internationale \nStiftung Mozarteum in cooperation with the Packard \nHum anities Institute.  \nWe are holding preliminary conversations with  other \npubli shers who have digita l collections of music for \nwhich we have indexing.  We are also in discussion for \na partnership with a vendor which is providing online \naccess to  digit al scores. \n \n6.   REFERENCES\n[1]Charles, Sydney Robinson, et al., ‘Editions, \nhistorical,’ Grove  Music  Online, ed. L. Macy \n(accessed  30/03/2007) \nhttp://www. grovem usic.co m\n[2]Hill, George R., Norris L. Stephens, Collected \nEditions,  Historical  Serie s & Sets & Monuments \nof Music:  A Bibliography. Berkeley: Fallen \nLeaf Press, 1997. [Available through \nScarecrow Press (Rowman & Littlefield \nPublishing Group).]\n1 http://variations2.indiana.edu/\n2 http://nma.redhost24-001.com/mambo/index.php."
    },
    {
        "title": "Bayesian Aggregation for Hierarchical Genre Classification.",
        "author": [
            "Christopher DeCoro",
            "Zafer Barutçuoglu",
            "Rebecca Fiebrink"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416012",
        "url": "https://doi.org/10.5281/zenodo.1416012",
        "ee": "https://zenodo.org/records/1416012/files/DeCoroBF07.pdf",
        "abstract": "Hierarchical taxonomies of classes arise in the analysis of many types of musical information, including genre, as a means of organizing overlapping categories at varying levels of generality. However, incorporating hierarchical structure into conventional machine learning systems presents a challenge: the use of independent binary classifiers for each class in the hierarchy can produce hierarchically inconsistent predictions. That is, an example may be assigned to a class, and not assigned to the parent of that class. This paper applies a Bayesian framework to combine, or aggregate, a hierarchy of multiple binary classifiers in a principled manner, and consequently improves performance over the hierarchy as a whole. Furthermore, such an approach allows for an arbitrarily complex hierarchy, and does not suffer from classes that are too broad or too refined. Experiments on the MIREX 2005 symbolic genre classification dataset show that our Bayesian Aggregation algorithm provides significant improvement over independent classifiers, and demonstrates superior performance compared to previous work. Our method also improves similarity search by ranking songs by similarity of hierarchical predictions to those of a query song. 1 INTRODUCTION Many musical concepts are inherently hierarchical. Some notion of hierarchy is implicitly or explicitly at play in concepts such as genre and mood (which have overlapping coarse and fine categories), instrument timbre (which is grouped by instrument families), and meter. When hierarchy has been accommodated in automatic classification of these concepts, it has generally been in quite simple ways (such as the top-down approach of [9]) or in ways that are acutely tailored to the learning task and/or classification method at hand (e.g. [8, 14]). We have developed a technique called Bayesian Aggregation, which uses the output predictions of arbitrary independent classifiers (such as k-nearest neighbor, support vector machines, etc.) which we refer to as the base classifiers, and aggregates them in such a way as to take advantage of the hierarchical nature of the predictions to improve classification accuracy. We demonstrate performance of this method on genre c⃝2007 Austrian Computer Society (OCG). classification, a popular classification task in music information retrieval. Genre is a culturally relevant and practically useful concept, and genre classification systems have the potential to be quite useful in organizing and allowing efficient access to music databases as shown by McKay in [12]. Further, the same work argued that multiple class assignments and user-specified ontological structure are beneficial in principle; both of these are inherently supported by Bayesian Aggregation. We describe the genre dataset and features used in Section 2.1, and provide background on classification algorithms in Section 2.2. We describe our algorithm in Section 3, and demonstrate in Section 4 that the use of Bayesian Aggregation allows for a significant improvement in genre classification accuracy when compared either to existing methods, or to the predictions of independent classifiers without aggregation. Further, it does so in a way that guarantees that the predictions will be consistent with the constraints of the hierarchy; that is, once an instance is assigned to a leaf class, it will also be assigned to the leaf’s parent classes. We also demonstrate that the outputs of such a classification system may offer improvements to similarity search systems built on genre classifiers. 2 BACKGROUND",
        "zenodo_id": 1416012,
        "dblp_key": "conf/ismir/DeCoroBF07",
        "keywords": [
            "hierarchical taxonomies",
            "musical information",
            "genre classification",
            "Bayesian framework",
            "aggregation",
            "machine learning systems",
            "independent binary classifiers",
            "arbitrarily complex hierarchy",
            "similarity search",
            "query song"
        ],
        "content": "BAYESIAN AGGREGATION FOR\nHIERARCHICAL GENRE CLASSIFICATION\nChristopher DeCoro Zafer Barutcuoglu Rebecca Fiebrink\nPrinceton University\nDepartment of Computer Science\nABSTRACT\nHierarchical taxonomies of classes arise in the analysis\nof many types of musical information, including genre,\nas a means of organizing overlapping categories at vary-\ning levels of generality. However, incorporating hierarch i-\ncal structure into conventional machine learning systems\npresents a challenge: the use of independent binary classi-\nﬁers for each class in the hierarchy can produce hierarchi-\ncally inconsistent predictions. That is, an example may be\nassigned to a class, and not assigned to the parent of that\nclass. This paper applies a Bayesian framework to com-\nbine, or aggregate , a hierarchy of multiple binary classi-\nﬁers in a principled manner, and consequently improves\nperformance over the hierarchy as a whole. Furthermore,\nsuch an approach allows for an arbitrarily complex hier-\narchy, and does not suffer from classes that are too broad\nor too reﬁned. Experiments on the MIREX 2005 symbolic\ngenre classiﬁcation dataset show that our Bayesian Aggre-\ngation algorithm provides signiﬁcant improvement over\nindependent classiﬁers, and demonstrates superior perfor -\nmance compared to previous work. Our method also im-\nproves similarity search by ranking songs by similarity of\nhierarchical predictions to those of a query song.\n1 INTRODUCTION\nMany musical concepts are inherently hierarchical. Some\nnotion of hierarchy is implicitly or explicitly at play in\nconcepts such as genre and mood (which have overlapping\ncoarse and ﬁne categories), instrument timbre (which is\ngrouped by instrument families), and meter. When hierar-\nchy has been accommodated in automatic classiﬁcation of\nthese concepts, it has generally been in quite simple ways\n(such as the top-down approach of [9]) or in ways that are\nacutely tailored to the learning task and/or classiﬁcation\nmethod at hand (e.g. [8, 14]).\nWe have developed a technique called Bayesian Ag-\ngregation, which uses the output predictions of arbitrary\nindependent classiﬁers (such as k-nearest neighbor, sup-\nport vector machines, etc.) which we refer to as the base\nclassiﬁers , and aggregates them in such a way as to take\nadvantage of the hierarchical nature of the predictions to\nimprove classiﬁcation accuracy.\nWe demonstrate performance of this method on genre\nc/circlecopyrt2007 Austrian Computer Society (OCG).classiﬁcation, a popular classiﬁcation task in music infor -\nmation retrieval. Genre is a culturally relevant and practi -\ncally useful concept, and genre classiﬁcation systems have\nthe potential to be quite useful in organizing and allowing\nefﬁcient access to music databases as shown by McKay\nin [12]. Further, the same work argued that multiple class\nassignments and user-speciﬁed ontological structure are\nbeneﬁcial in principle; both of these are inherently sup-\nported by Bayesian Aggregation.\nWe describe the genre dataset and features used in Sec-\ntion 2.1, and provide background on classiﬁcation algo-\nrithms in Section 2.2. We describe our algorithm in Sec-\ntion 3, and demonstrate in Section 4 that the use of Bayes-\nian Aggregation allows for a signiﬁcant improvement in\ngenre classiﬁcation accuracy when compared either to ex-\nisting methods, or to the predictions of independent clas-\nsiﬁers without aggregation. Further, it does so in a way\nthat guarantees that the predictions will be consistent wit h\nthe constraints of the hierarchy; that is, once an instance\nis assigned to a leaf class, it will also be assigned to the\nleaf’s parent classes. We also demonstrate that the outputs\nof such a classiﬁcation system may offer improvements to\nsimilarity search systems built on genre classiﬁers.\n2 BACKGROUND\n2.1 Genre classiﬁcation\nGenre classiﬁcation has been a popular task in music in-\nformation retrieval since originally posed by Tzanetakis\n[15]. In 2005, the MIREX contest featured both audio\nand symbolic musical genre classiﬁcation tasks [11]. The\nwinning participants of the symbolic genre classiﬁcation\ntask employed the Bodhidharma MIDI classiﬁcation sys-\ntem (presented in [10] and expanded in [9]) that extracts\n111 high-level features related to instrumentation, rhyth m,\ndynamics, and chords. Bodhidharma also considers hi-\nerarchical relationships via a top-down classiﬁcation ap-\nproach, wherein classiﬁer outputs at each branch of a hi-\nerarchy determine which child classiﬁers will be used to\nfurther reﬁne the classiﬁcation. In our evaluation, we used\nthe Bodhidharma features from the MIREX 2005 38-leaf\nclass hierarchy and 950-item symbolic genre dataset.\n2.2 Classiﬁcation\nClassiﬁcation is a well-studied problem in the machine\nlearning literature. One popular choice of classiﬁer is thesupport vector machine (SVM), for which an accessible\noverview is given in [4]. Given a labeled training set,\nwhich includes both positive (members of the genre) and\nnegative example vectors, the SVM ﬁnds the maximally-\nseparating hyperplane in a kernel-transformed vector space\nbetween the two subsets. To classify a novel, unlabeled\nexample, we can compute its distance to the maximally-\nseparating hyperplane. SVMs have been used extensively,\nand there exists signiﬁcant theoretical and empirical evi-\ndence of their classiﬁcation efﬁcacy.\nIn this work we exclusively use support vector ma-\nchines as a base classiﬁer, due to its ability to support\nwide ranges of data without assumptions on the distribu-\ntion of values (kNN classiﬁers, by contrast, expect that the\nEuclidean L2norm is meaningfully deﬁned over the fea-\nture vector space, which is not the case for these features).\nHowever, our method can be used to improve the results\nof any type of classiﬁer; we have shown its utility with\nkNN classiﬁers in a previous work [1].\n3 ALGORITHM DESCRIPTION\nAs mentioned, we perform hierarchical classiﬁcation of\nmusic examples by aggregating the results of multiple in-\ndependent classiﬁers according to a Bayesian framework,\nto perform collaborative error correction over their possi bly-\ninconsistent predictions. We build the framework in a\ntraining phase using labeled example data, and subsequentl y\nuse the pre-computed framework to assign labels to novel\ndata in a classiﬁcation phase. The reader is referred to [3]\nfor details and analyses beyond those presented here.\n3.1 Training Phase\nThe ﬁrst step to our algorithm is to train a set of individ-\nual base classiﬁers to predict memberships for each class\nin the hierarchy. We call this initial set of predictions the\nbase classiﬁcation . We use base classiﬁcations from the\ntraining set in order to generate a Bayesian network that\ndescribes the hierarchy. A Bayesian network involves a\nnumber of random variables, some of which are observed\ndirectly, while others are hidden [6]. Of these variables,\nsome are assumed to be conditionally dependent on oth-\ners. We can visually represent this as a graph, as in Fig-\nure 1. Nodes represent variables, and edges represent con-\nditional dependence. Given values for observed nodes,\nBayesian inference algorithms use this network to calcu-\nlate probabilities for hidden node values, or ﬁnd the most\nprobable conﬁguration of hidden node value assignments\nconsistent with the observations.\nFor a given example, let yidenote the actual binary\nmembership to class i,ˆyidenote the base classiﬁer predic-\ntion for that class, and /vector yparents (i)denote the actual mem-\nbership to superclasses of i. For example, yimight rep-\nresent membership in the Smooth Jazz genre, with ˆyithe\nprediction of the base classiﬁer trained to recognize that\ngenre, and /vector yparents would then represent membership in\nthe Modern Pop and Fusion genres.\nAfter obtaining a set of (possibly inconsistent) ˆypre-\ndictions from all base classiﬁers, we wish to ﬁnd the mostC1\nC2 C5\nC3 C4y1 ˆy1\ny2 ˆy2 y5 ˆy5\ny3ˆy3 y4 ˆy4\n(a) Hierarchy of classes (b) Bayesian network\nFigure 1 .The class hierarchy (a) is transformed into a Bayesian net-\nwork (b). The ynodes are the binary-valued hidden nodes representing\nactual membership to the class, and the corresponding ˆynodes are the\nobserved classiﬁer outputs.\nprobable set of consistent ylabels that may be underly-\ning them. Therefore, for Nnodes we need to ﬁnd the\nlabels y1... y Nthat maximize the conditional probability\nP(y1... y N|ˆy1...ˆyN), which by Bayes rule equals\nP(ˆy1...ˆyN|y1... y N)P(y1... y N)\nZ, (1)\nwhere Zis a constant normalization factor. We propose a\nBayesian network structure for this problem, as illustrate d\nby Figure 1. The class hierarchy shown at left is trans-\nformed into a Bayesian network by adding extra nodes\nthat correspond to the observed classiﬁer outputs. The y-\nnodes are probabilistically dependent on their parent clas ses,\nand the ˆy-nodes are probabilistically dependent on their\ncorresponding labels y.\nWe enforce hierarchical consistency of labels using the\nedges among the y-nodes. The edges encode the condi-\ntional dependencies P(yi|/vector yparents (i)), where /vector yparents (i)\nis used to denote all parent y-nodes of node yi, which we\nset to ensure that a label must be 0 if any of its parents is 0.\nThe remaining entries P(yi|/vector yparents (i)= 1) are inferred\nfrom the training set. We condition each y-node only on\nits parents, thereby limiting the complexity of the Bayes\nnet to one that is both tractable to infer and constrained to\navoid overﬁtting, producing the following simpliﬁcation:\nP(y1... y N) =N/productdisplay\ni=1P(yi|/vector yparents (i)). (2)\nThe edges from ytoˆyreﬂect an important observation:\nfor a given example, a classiﬁer prediction ˆyiis condi-\ntionally independent of all other classiﬁers’ predictions ˆyj\nand labels yj(i/negationslash=j) given its true label yi. This simpliﬁes\nEquation 1, since we can write\nP(ˆy1...ˆyN|y1... y N) =N/productdisplay\ni=1P(ˆyi|yi). (3)\nP(ˆyi|yi)consists of P(ˆyi|yi= 1) andP(ˆyi|yi= 0) ,\nwhich are the distributions for the base classiﬁer predic-\ntions, and can be estimated during training by validation.\nWe obtain a sampling of the distributions P(ˆyi|yi= 1)\nandP(ˆyi|yi= 0) , using cross validation on the training\ndata. We use the continuous SVM outputs, without thresh-\nolding at zero, as the observed ˆyivalues, and model these\ntwo distributions as Gaussians, computing their means and\nvariances for each class over the SVM outputs for positive\nand negative examples, respectively.3.2 Classiﬁcation Phase\nThe ﬁrst step to assigning a genre to a novel example\nis to compute a base classiﬁcation using the trained per-\nclass SVMs. In our Bayesian network, this corresponds\nto observing values for the ˆyinodes. A Bayesian infer-\nence algorithm will then ﬁnd the most likely conﬁgura-\ntion of (consistent) hidden ylabels for the given ˆypre-\ndictions, or the marginal distribution P(yi|ˆy1...ˆyN)for\neach class separately. We use the marginal probabilities\nP(yi= 1|ˆy1...ˆyN)in our results so that we retain real-\nvalued membership probabilities and can threshold them\nat different levels as desired. Among the many Bayesian\ninference algorithms available, in our experiments we used\nthejunction tree algorithm for exact inference, although\napproximate inference with Monte Carlo methods such as\nGibbs sampling may be more practical for more complex\nhierarchies. Descriptions and detailed references for the se\nand other inference algorithms are available in [13].\n4 EXPERIMENTAL RESULTS\n4.1 Genre Classiﬁcation\nFor class hierarchies in general, and in particular for the\nthe MIREX 2005 dataset, each training example belongs\nto very few nodes, in comparison to all the other nodes for\nwhich it counts as a negative example. In consequence,\nfor each node the number of negative examples is dispro-\nportionately larger than the number of positives, a charac-\nteristic known as skew . Since machine learning algorithms\nfavor simpler models, when possible, to avoid overﬁtting\nthe training data, failing to take skew into account can\nproduce classiﬁers that unconditionally predict negative ,\nas this is an ultimately simple model with seemingly very\nhigh accuracy. Instead, a skew-insensitive performance\nmeasure is necessary, both for optimizing during training\nand for analyzing results, which will penalize errors on the\nfew positive examples proportionally higher than errors on\nthe ample negatives. The standard skew-insensitive accu-\nracy is the average of sensitivity (accuracy on positive ex-\namples) and speciﬁcity (accuracy on negative examples)\n0.5true positives\ntrue pos. +false neg.+ 0.5true negatives\ntrue neg. +false pos..(4)\nOn the MIREX 2005 symbolic genre classiﬁcation data-\nset, we trained linear SVMs using the SVMlight software\n[7] with the appropriate cost factors to compensate for\nclass skew. We used 3-fold cross-validation, obtaining\nthree SVMs for each class. Each example is used as train-\ning for two, and a class prediction is given for that ex-\nample by the third. The Bayesian network was then con-\nstructed using these distributions as previously describe d,\nand marginal probabilities were computed for the consis-\ntent hidden labels using Bayesian inference.\nAverage skew-insensitive accuracy over all 55classes\nwas 76.8% for independent SVMs, and 85.1% after Bayes-\nian Aggregation thresholded at p >0.5. Figure 2 shows a\nscatterplot of each class before and after aggregation.0.5 0.6 0.7 0.8 0.9 10.50.550.60.650.70.750.80.850.90.951\nIndependent SVM AccuraciesAccuracies after Bayesian Aggregation\nFigure 2 .Scatterplot of skew-insensitive accuracies for each class\nbefore vs.after Bayesian Aggregation.\nTo provide a fair comparison of our results directly\nagainst previous work using this dataset, we also com-\nputed the “raw accuracy” statistic reported in the MIREX\n2005 contest results, which relies on the one-leaf-only na-\nture of song labels in this dataset and picks as the genre\nprediction the leaf node with the highest output. Under\nthis multi-class single-label criterion, 56.0%of all exam-\nples were labeled correctly by independent SVMs, and\n60.1%after Bayesian Aggregation, compared to the MIREX\n2005 contest entries in Table 1. Considering that this re-\nquires the choice of one correct class out of 38, where a\nrandom guess would have less than 3% accuracy, both the\nimprovement over previous results and the improvement\nof Bayesian Aggregation over independent SVMs are sig-\nniﬁcant.\nAlgorithm Raw Accuracy\nBayesian Aggregation 60.1%\nIndependent SVMs 56.0%\nBodhidharma 46.1%\nBasili et al. (NB) 45.0%\nBasili et al. (J48) 41.0%\nLi 39.8%\nPonce de Leon & Inesta 15.3%\nTable 1 .Independent SVMs and Bayesian Aggregation compared\nto MIREX 2005 symbolic genre classiﬁcation contest entries b y single-\nlabel multi-class “raw accuracy.”\n4.2 Similarity Search\nA related application is to search for songs “similar” to\na query song by genre, or equivalently, rank all songs in\na database by genre similarity to the given song. Again\nit is assumed that only a small part of the available data\nis manually labeled, so the query song as well as the re-\ntrieved songs might have no certain genre label. While\none could ﬁrst classify each song as above into a discrete\ngenre and then retrieve the other songs in that genre as the\nmost similar, the hierarchy provides a means of deﬁning\ninter-class similarity as well.For our experiments, we deﬁned similarity of two songs\nas the number of their equal binary labels in the hierarchy,\nwhich decreases as the path distance of their classes in the\nhierarchy increases. We computed the “true similarity” of\nevery pair of songs using the actual labels, and the pre-\ndicted similarities from independent SVMs outputs and\nthen for the Bayes-aggregated predictions. To avoid se-\nlecting an arbitrary threshold, the classes along the branc h\nof the maximum-conﬁdence leaf were selected as the posi-\ntive labels for each example. Using each song as the query,\nall other songs were sorted by similarity, and the top pre-\ndicted results were compared to the top results as given\nby true similarity. Across all examples, of the 100 most-\nsimilar songs an average of 52% were retrieved by inde-\npendent SVMs, while the aggregated predictions retrieved\n62%. Similarly, of the top 50, SVMs retrieved 46%, com-\npared to 52% after Bayesian Aggregation.\nWest and Lamere have used Euclidean distance between\ngenre classiﬁer soft outputs to perform similarity compu-\ntation for playlist generation and collection visualizati on,\nbut without regard to hierarchical class organization [16] .\nOur results suggest that applying Bayesian Aggregation to\nsuch a system could improve on their approach.\n5 CONCLUSIONS AND FUTURE WORK\nBayesian Aggregation offers an elegant and generalizable\nmeans of taking hierarchy into account in performing a\nclassiﬁcation task, and thus offers several beneﬁts to mu-\nsic information retrieval researchers. Our experiments de -\nmonstrate that Bayesian Aggregation is able to signiﬁ-\ncantly improve genre classiﬁcation accuracy, both com-\npared to the base classiﬁers used alone, as well as to pre-\nvious work. This is done at the minor cost of training and\nevaluation of parent-node classiﬁers in addition to leaf-\nnode classiﬁers; the time required for additional evalua-\ntions is on the order of milliseconds. By allowing for a\nsoft assignment to related classes, our algorithm is able to\neffectively expand the training set for each class, which\nis particularly important for datasets such as this with a\nlimited number of examples.\nThis has the important consequence that our method\nenables arbitrarily reﬁned or broad taxonomies; it remains\nrobust even when considering classes for which the indi-\nvidual classiﬁers may be inaccurate, thereby decoupling\ntaxonomy design from concerns of system performance.\nThe algorithm does this by discovering inaccuracies dur-\ning the training phase and implicitly discounts such classi -\nﬁers’ predictions on new data. Furthermore, this approach\nis superior to heuristic top-down or bottom-up approaches\nthat place excessive responsibility on the root or leaf clas -\nsiﬁers, respectively, without providing any means to cor-\nrect bad predictions at those levels. Most importantly,\nBayesian Aggregation is able to improve accuracy for any\ntype of base classiﬁer, as suited to the task. Although we\nused SVMs for this application, the system is independent\nof this choice, and one can use any other classiﬁcation al-\ngorithm as applicable, such as neural networks or kNN.\nGiven these characteristics of our algorithm, there existmany avenues of future work, which we intend to explore.\nOur algorithm is currently capable of handling multi-label\nclassiﬁcation problems, such as genre or mood, where a\nsong can be labeled to multiple classes at various lev-\nels. We therefore intend to empirically demonstrate its\nefﬁcacy on such tasks. Additionally, our algorithm can\nbe specialized to make single-leaf predictions, potential ly\nfurther improving performance. We are excited about the\npotentials that hierarchy-aware methods present, and have\npresented this work as a signiﬁcant ﬁrst step.\n6 ACKNOWLEDGMENTS\nWe would like to thank Cory McKay for sharing the Bod-\nhidharma MIDI dataset used for evaluation of this work,\nand for his helpful advice and opinions. This work was\npartially supported by an ATI/AMD Technologies Fellow-\nship and NSF grant IIS-0513552.\n7 REFERENCES\n[1] Barutcuoglu, Z. and C. DeCoro. “Hierarchical Shape Clas siﬁcation\nUsing Bayesian Aggregation,” Proc. Shape Modeling International ,\n2006.\n[2] Barutcuoglu, Z., R.E. Schapire and O.G. Troyanskaya. “H ierarchi-\ncal Multi-label Prediction of Gene Function,” Bioinformatics , Jan-\nuary 2006.\n[3] Barutcuoglu, Z., C. DeCoro, R.E. Schapire and O.G. Troya nskaya.\n“Bayesian Aggregation for Hierarchical Classiﬁcation,” Technical\nReport TR-785-07 , Princeton University, Department of Computer\nScience, 2007\n[4] Burges, C.J.C. “A Tutorial on Support Vector Machines fo r Pattern\nRecognition,” Data Mining and Knowledge Discovery , 1998.\n[5] Flach, P.A. “The geometry of ROC space: understanding mach ine\nlearning metrics through ROC isometrics,” Proc. 20th International\nConference on Machine Learning, 2003.\n[6] Heckerman, D. “A Tutorial on Learning with Bayesian Netwo rks,”\nLearning in Graphical Models, MIT Press, Cambridge, MA, 1999.\n[7] Joachims, T. “Making large-Scale SVM Learning Practical ,”Ad-\nvances in Kernel Methods - Support Vector Learning , MIT-Press,\nCambridge, MA, 1999.\n[8] Klapuri, A., A. Eronen and J. Astola. “Analysis of the mete r of\nacoustic musical signals,” IEEE Trans. Speech and Audio Process-\ning14(1), 2006.\n[9] McKay, C. “Automatic Genre Classiﬁcation of MIDI Recordi ngs,”\nM.A. Thesis , McGill University, Canada, 2004.\n[10] McKay, C. and I. Fujinaga. “Automatic Genre Classiﬁcati on Using\nLarge High-Level Musical Feature Sets,” Proc. ISMIR , 2004.\n[11] McKay, C. and I. Fujinaga. “The Bodhidharma system and th e re-\nsults of the MIREX 2005 symbolic genre classiﬁcation contest ,”\nProc. ISMIR , 2005.\n[12] McKay, C. and I. Fujinaga. “Musical genre classiﬁcatio n: Is it worth\npursuing and how can it be improved?” Proc. ISMIR , 2006.\n[13] Murphy, K. “The Bayes Net Toolbox for MATLAB,” Computing\nScience and Statistics, 2001.\n[14] Rauber, A., E. Pampalk and D. Merkl. “Using pyscho-acous tic mod-\nels and self-organizing maps to create a hierarchical struct uring of\nmusic by sound similarity,” Proc. ISMIR, 2002\n[15] Tzanetakis, G., G. Essl and P. Cook. “Automatic musical ge nre clas-\nsiﬁcation of audio signals,” Proc. ISMIR , 2001.\n[16] West, K. and P. Lamere. “A Model-Based Approach to Constr uct-\ning Music Similarity Functions,” EURASIP Journal on Advances in\nSignal Processing , 2007."
    },
    {
        "title": "Fuzzy Song Sets for Music Warehouses.",
        "author": [
            "François Deliège",
            "Torben Bach Pedersen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415770",
        "url": "https://doi.org/10.5281/zenodo.1415770",
        "ee": "https://zenodo.org/records/1415770/files/DeliegeP07.pdf",
        "abstract": "The emergence of music recommendation systems calls for the development of new data management technologies able to query vast music collections. In this paper, we define fuzzy song sets and an algebra to manipulate them. We present a music warehouse prototype able to perform efficient nearest neighbor searches in an arbitrary song similarity space. Using fuzzy song sets, the music warehouse offers a practical solution to the all musical data management scenarios provided: song comparisons, user musical preferences and user feedback. We investigate three practical approaches to tackle the storage issues of fuzzy song sets: tables, arrays and bitmaps. Finally, we confront theoretical estimates to concrete implementation results and prove that, from a storage perspective, arrays and bitmaps are both effective data structure solutions. 1 INTRODUCTION Music recommendation systems have recently gained a tremendous popularity. Music lovers discover new ways of searching and sharing their favorite music. However, at such growing speed, the database element of any recommendation systems will soon become a bottleneck. Hence, appropriate musical data management tools are needed. Music Warehouses (MWs) are dedicated data warehouses optimized for the storage and analysis of music content. They are currently developed to respond to this is call. The contributions of this paper are threefold. First, motivated by a case study [2], we propose three generic usage scenarios illustrating the current demands in musical data management. To answer these demands, we define fuzzy song sets and develop an algebra. Second, to demonstrate the usefulness of fuzzy song sets, a prototypical MW composed of two multidimensional cubes is presented. For each cube, concrete examples of queries inspired by the usage scenarios are provided. Fuzzy song sets prove to be an adequate data structure to manipulate musical information. Third, we discuss three solutions for storing fuzzy song sets and we construct theoretical estimates. A practical implementation shows that the structure overhead represents a major part of the storage consumption and that two solutions are viable for very large music collections. A lot of attention was drawn to enable music lovers to explore individual music collections [6, 7]. Within this context, several research projects have been conducted in c⃝2007 Austrian Computer Society (OCG). order to pursue a suitable similarity measure for music [8, 9]. A music data model, an algebra and a query language are introduced by Wang et al. [10]. However, the model lacks an adequate framework to perform similarity searches. Jensen et al. address this issue and offer a model that supports dimension hierarchies [5]. This paper tackles the storage issues when the scalability does not remain limited to a few hundred thousands songs. Nearest neighbor searches are a popular topic in the database community for their usage in content based retrieval and similarity searches. Work on both high and low dimensional spaces can be found in the literature. However, existing indexing techniques do not apply to high dimensional musical features due to the subjective nature of musical perception, i.e., similarities do not form a metric. Work on indexes for non-metric space is presented in the literature [12]. Though the similarity function is nonmetric, it remains confined in a pair of lower and upper bounds specifically constructed. MWs, however, should not be tightened to any similarity function. The use of bitmaps in multidimensional databases is frequent. Different compression schemes exist to reduce the storage consumption of bitmaps. The Word Aligned Hybrid [11], WAH, and the Byte aligned Bitmap Compression [1], BBC, are two very common compression algorithms. BBC offers a very good compression ratio and performs bitwise logical operations efficiently. WAH performs bitwise operations much faster than BBC but consumes more storage space. The remainder of this paper is organized as follows. Section 2 presents three search for information scenarios that could be treated by music recommendation systems. We proceed in Section 3 by defining fuzzy song sets and an algebra. In Section 4, two prototypical multidimensional cubes are presented and use of the algebra is illustrated through queries examples. Storage solutions are discussed in Section 5 and implementation results are shown in Section 6. Finally, we conclude in Section 7. 2 USAGE SCENARIO Three examples of data obtained from music recommendations system are presented below. The User Feedback The user’s opinion about the previously suggested songs is a valuable piece of information. For each song played, the user can grade if the suggestion was wise based on the criteria provided, referred to as the query context. The query context can be the artist similarity, the genre similarity, the beat similarity, or any other similarity measure available to the system to perform a selection. The grading reflects if a proposed song was relevant in the given query context. For example, it is possible to retrieve the list of songs John liked when he asked for a list of rock songs or the ten songs Alice liked the most when she asked for songs similar to “U2: Where the streets have no name”. Typically, the data obtained should contain: (i) a reference to the profile of a registered user in the system, (ii) a reference to a query context provided by the user, and (iii) a list of songs and marks so that for each song proposed, the user can grade how much he liked a particular song being part of the proposition. Grades are given on a per song basis, they reflect if the user believes the song deserved its place among the suggested list of songs: strongly disagrees, neutral, likes, and loves. While the grade must not be a numerical value, we assume in the rest of the article that a mapping function to [0, 1] has to be provided. When a user believes a song definitely deserves its place in the list, a high mark should be given. The User Preferences Some songs should never be proposed to the user independently of the query context. On the contrary, some songs should be proposed more often as they are marked as the user’s favorites. Therefore, a user should be able to grade any song on a fan-scale ranging from “I love it” to “I hate it” depending if he likes the song or not. For example, the music recommendation system database should be able to retrieve the list of songs Maria likes, and the songs she hates the most. The User Musical Preferences contains two different pieces of information: (i) a reference to a user registered, and (ii) a list of songs associated with their respective grades on the fan-scale. If Rico hates a song, a low value should be used; if he loves it, a value close to 1 should be used instead. Musical profiles modify the frequency a given song appears in a music recommendation list. The Songs Comparisons Finally, the music recommendation system should be able to compare any pair of songs. For each pair of songs, the system is able to provide a similarity value with respect to a given aspect of the song. The similarity values should indicate if two songs are “very different”, “different”, “somewhat similar”, or “very similar” from the perspective of an given aspect of the song. For example, the song “We will rock you” by Queen is “very different” from the song “Twinkle, twinkle little star” with respect to their genre similarity aspect. To compare songs, three pieces of information are necessary: (i) a reference to the first song of the pair being compared, (ii) a reference to the second song of the pair, (iii) a reference to the definition of a similarity function that maps to any pair of songs to a similarity value, and (iv) the similarity value reflecting how similar the two songs are. If two songs are very different, a value close to 0 should be used, if they are very similar, a value close to 1 should be used instead. To keep the scenario as generic as possible, very few assumptions are made about the properties of the functions used to compute the similarity values. In particular, the similarity functions do not have to fulfill the mathematical properties of a metric, e.g., the non-negativity, the identity of indiscernibles, the triangular inequality, and the symmetry properties. 3 AN ALGEBRA FOR FUZZY SONG SETS In this section, we introduce song sets as well as operators and functions to manipulate them. Let X be the set of all songs. Then, a fuzzy song set, A, is a fuzzy set defined over X such that A = {µA(x)/x : x ∈X, µA(x) ∈[0, 1]} (1) and is defined as a set of pairs µA(x)/x, where x is a song, µA(x), referred to as the membership degree of x, is a real number belonging to [0, 1], and / denotes the association of the two values as commonly expressed in the fuzzy logic literature [3]. µA(x) = 0 when song x does not belong to A, and µA(x) = 1 when x completely belongs to A.",
        "zenodo_id": 1415770,
        "dblp_key": "conf/ismir/DeliegeP07",
        "keywords": [
            "music recommendation systems",
            "fuzzy song sets",
            "algebra",
            "music warehouse",
            "song comparisons",
            "user feedback",
            "user preferences",
            "user musical preferences",
            "song similarities",
            "music data management"
        ],
        "content": "FUZZY SONG SETS FOR MUSIC WAREHOUSES\nFranc ¸ois Deli `ege and Torben Bach Pedersen\nAalborg University\nDepartment of Computer Science\nABSTRACT\nThe emergence of music recommendation systems calls\nfor the development of new data management technolo-\ngies able to query vast music collections. In this paper,\nwe deﬁne fuzzy song sets and an algebra to manipulate\nthem. We present a music warehouse prototype able to\nperform efﬁcient nearest neighbor searches in an arbitrary\nsong similarity space. Using fuzzy song sets, the music\nwarehouse offers a practical solution to the all musical\ndata management scenarios provided: song comparisons ,\nuser musical preferences anduser feedback . We investi-\ngate three practical approaches to tackle the storage issues\nof fuzzy song sets: tables ,arrays andbitmaps . Finally, we\nconfront theoretical estimates to concrete implementation\nresults and prove that, from a storage perspective, arrays\nand bitmaps are both effective data structure solutions.\n1 INTRODUCTION\nMusic recommendation systems have recently gained a\ntremendous popularity. Music lovers discover new ways\nof searching and sharing their favorite music. However, at\nsuch growing speed, the database element of any recom-\nmendation systems will soon become a bottleneck. Hence,\nappropriate musical data management tools are needed.\nMusic Warehouses (MWs) are dedicated data warehouses\noptimized for the storage and analysis of music content.\nThey are currently developed to respond to this is call.\nThe contributions of this paper are threefold. First, mo-\ntivated by a case study [2], we propose three generic usage\nscenarios illustrating the current demands in musical data\nmanagement. To answer these demands, we deﬁne fuzzy\nsong sets and develop an algebra. Second, to demonstrate\nthe usefulness of fuzzy song sets, a prototypical MW com-\nposed of two multidimensional cubes is presented. For\neach cube, concrete examples of queries inspired by the\nusage scenarios are provided. Fuzzy song sets prove to be\nan adequate data structure to manipulate musical informa-\ntion. Third, we discuss three solutions for storing fuzzy\nsong sets and we construct theoretical estimates. A practi-\ncal implementation shows that the structure overhead rep-\nresents a major part of the storage consumption and that\ntwo solutions are viable for very large music collections.\nA lot of attention was drawn to enable music lovers to\nexplore individual music collections [6, 7]. Within this\ncontext, several research projects have been conducted in\nc\r2007 Austrian Computer Society (OCG).order to pursue a suitable similarity measure for music [8,\n9]. A music data model, an algebra and a query lan-\nguage are introduced by Wang et al. [10]. However, the\nmodel lacks an adequate framework to perform similarity\nsearches. Jensen et al. address this issue and offer a model\nthat supports dimension hierarchies [5]. This paper tack-\nles the storage issues when the scalability does not remain\nlimited to a few hundred thousands songs.\nNearest neighbor searches are a popular topic in the\ndatabase community for their usage in content based re-\ntrieval and similarity searches. Work on both high and low\ndimensional spaces can be found in the literature. How-\never, existing indexing techniques do not apply to high\ndimensional musical features due to the subjective nature\nof musical perception, i.e., similarities do not form a met-\nric. Work on indexes for non-metric space is presented in\nthe literature [12]. Though the similarity function is non-\nmetric, it remains conﬁned in a pair of lower and upper\nbounds speciﬁcally constructed. MWs, however, should\nnot be tightened to any similarity function.\nThe use of bitmaps in multidimensional databases is\nfrequent. Different compression schemes exist to reduce\nthe storage consumption of bitmaps. The Word Aligned\nHybrid [11], WAH, and the Byte aligned Bitmap Com-\npression [1], BBC, are two very common compression al-\ngorithms. BBC offers a very good compression ratio and\nperforms bitwise logical operations efﬁciently. WAH per-\nforms bitwise operations much faster than BBC but con-\nsumes more storage space.\nThe remainder of this paper is organized as follows.\nSection 2 presents three search for information scenar-\nios that could be treated by music recommendation sys-\ntems. We proceed in Section 3 by deﬁning fuzzy song\nsets and an algebra. In Section 4, two prototypical mul-\ntidimensional cubes are presented and use of the algebra\nis illustrated through queries examples. Storage solutions\nare discussed in Section 5 and implementation results are\nshown in Section 6. Finally, we conclude in Section 7.\n2 USAGE SCENARIO\nThree examples of data obtained from music recommen-\ndations system are presented below.\nThe User Feedback The user’s opinion about the previ-\nously suggested songs is a valuable piece of information.\nFor each song played, the user can grade if the sugges-\ntion was wise based on the criteria provided, referred to\nas the query context. The query context can be the artist\nsimilarity, the genre similarity, the beat similarity, or anyother similarity measure available to the system to per-\nform a selection. The grading reﬂects if a proposed song\nwas relevant in the given query context. For example, it\nis possible to retrieve the list of songs John liked when he\nasked for a list of rock songs or the ten songs Alice liked\nthe most when she asked for songs similar to “U2: Where\nthe streets have no name”.\nTypically, the data obtained should contain: (i)a refer-\nence to the proﬁle of a registered user in the system, (ii)a\nreference to a query context provided by the user, and (iii)\na list of songs and marks so that for each song proposed,\nthe user can grade how much he liked a particular song be-\ning part of the proposition. Grades are given on a per song\nbasis, they reﬂect if the user believes the song deserved\nits place among the suggested list of songs: strongly dis-\nagrees, neutral, likes, and loves. While the grade must not\nbe a numerical value, we assume in the rest of the article\nthat a mapping function to [0;1]has to be provided. When\na user believes a song deﬁnitely deserves its place in the\nlist, a high mark should be given.\nThe User Preferences Some songs should never be pro-\nposed to the user independently of the query context. On\nthe contrary, some songs should be proposed more often\nas they are marked as the user’s favorites. Therefore, a\nuser should be able to grade any song on a fan-scale rang-\ning from “I love it” to “I hate it” depending if he likes the\nsong or not. For example, the music recommendation sys-\ntem database should be able to retrieve the list of songs\nMaria likes, and the songs she hates the most.\nThe User Musical Preferences contains two different\npieces of information: (i) a reference to a user registered,\nand (ii) a list of songs associated with their respective\ngrades on the fan-scale. If Rico hates a song, a low value\nshould be used; if he loves it, a value close to 1should\nbe used instead. Musical proﬁles modify the frequency a\ngiven song appears in a music recommendation list .\nThe Songs Comparisons Finally, the music recommen-\ndation system should be able to compare any pair of songs.\nFor each pair of songs, the system is able to provide a sim-\nilarity value with respect to a given aspect of the song. The\nsimilarity values should indicate if two songs are “very\ndifferent”, “different”, “somewhat similar”, or “very sim-\nilar” from the perspective of an given aspect of the song.\nFor example, the song “We will rock you” by Queen is\n“very different” from the song “Twinkle, twinkle little star”\nwith respect to their genre similarity aspect .\nTo compare songs, three pieces of information are nec-\nessary: (i)a reference to the ﬁrst song of the pair being\ncompared, (ii)a reference to the second song of the pair,\n(iii)a reference to the deﬁnition of a similarity function\nthat maps to any pair of songs to a similarity value, and\n(iv)the similarity value reﬂecting how similar the two\nsongs are. If two songs are very different, a value close\nto0should be used, if they are very similar, a value close\nto1should be used instead.\nTo keep the scenario as generic as possible, very few\nassumptions are made about the properties of the func-\ntions used to compute the similarity values. In particular,the similarity functions do not have to fulﬁll the mathe-\nmatical properties of a metric, e.g., the non-negativity, the\nidentity of indiscernibles, the triangular inequality, and the\nsymmetry properties.\n3 AN ALGEBRA FOR FUZZY SONG SETS\nIn this section, we introduce song sets as well as operators\nand functions to manipulate them.\nLetXbe the set of all songs. Then, a fuzzy song set,\nA, is a fuzzy set deﬁned over Xsuch that\nA=f\u0016A(x)=x:x2X;\u0016A(x)2[0;1]g (1)\nand is deﬁned as a set of pairs \u0016A(x)=x, wherexis a\nsong,\u0016A(x), referred to as the membership degree of x,\nis a real number belonging to [0;1], and=denotes the as-\nsociation of the two values as commonly expressed in the\nfuzzy logic literature [3]. \u0016A(x) = 0 when songxdoes\nnot belong to A, and\u0016A(x) = 1 whenxcompletely be-\nlongs toA.\n3.1 Operators\nThe following operators are classically used in order to\nmanipulate song sets. They form a closed algebra.\nequality: LetAandBbe two fuzzy song sets. Ais equal\ntoBiff for all song the membership degree of a song in A\nis equal to the membership degree of the same song in B.\nA=B,8x2X;\u0016A(x) =\u0016B(x) (2)\nsubset: LetAandBbe two fuzzy song sets. Ais included\ninBiff for all song, the membership degree a song in A\nis lower than the membership degree of the same song in\nB.\nA\u0012B,8x2X;\u0016A(x)\u0014\u0016B(x) (3)\nNote that the empty fuzzy song set deﬁned with the null\nmembership function, i.e., 8x2X;\u0016(x) = 0 , is a subset\nof all fuzzy sets.\nunion: LetAandBbe two fuzzy song sets over X. The\nunion ofAandBis a fuzzy song set with, for each song,\na membership degree equal to the maximum membership\ndegree associated to that song in AandB.\nA[B=f\u0016(A[B)(x)=xg\n\u0016(A[B)(x) =max(\u0016A(x);\u0016B(x))(4)\nintersection: LetAandBbe two fuzzy sets over X. The\nintersection of AandBis a fuzzy song set with, for each\nsong, a membership degree equal to the minimum mem-\nbership degree associated to that song in AandB.\nA\\B=f\u0016(A\\B)(x)=xg\n\u0016(A\\B)(x) =min(\u0016A(x);\u0016B(x))(5)\nnegation: LetAbe a fuzzy sets over X. The negation of\nA is a fuzzy song set with the membership degree of each\nsong equal to its symmetric value on the interval [0;1].\n:A=f1\u0000\u0016A(x)=xg (6)The following new operators are introduced speciﬁ-\ncally to manipulate song sets.\nreduction: LetAbe a fuzzy set over X. The reduction of\nAis a subset of Asuch that membership degrees smaller\nthan\u000bare set to 0.\nReduce\u000b(A) =f\u0016A\u000b(x)=xg\n\u0016A\u000b(x) =\u001a\n\u0016A(x)if\u0016A(x)\u0015\u000b;\n0 if\u0016A(x)<\u000b(7)\nThe reduction operator changes the membership degree of\nsongs below a given threshold to 0. It allows the construc-\ntion of more complex operators that allow the reducing\nthe membership degree granularity over ranges of mem-\nbership degrees.\nTopk:LetAbe a fuzzy set over X. TheTopksubset\nofAis a fuzzy song with the membership degree of all\nelements not having the k highest membership degree set\nto0and the membership degree of the k highest elements\nofAset to their respective membership degree in A.\nTopk(A) =f\u0016cAk(xi)=xij\n8xi;xj2X;1\u0014i<j;\u0016A(xi)\u0015\u0016A(xj)g\n\u0016cAk(xi) =\u001a\n\u0016A(xi)ifi\u0014k;\n0 otherwise(8)\nNote that the Topksubset ofAis not unique, e.g., when\nall elements have an identical membership degree. The\nTopkoperator returns a fuzzy song set with all member-\nship degrees set to zero except for k elements with the\nhighest membership degrees that remain unchanged. Topk\nis a cornerstone for the development of complex operators\nbased on relative ordering of the membership degrees.\naverage: LetA1;:::;Aibeifuzzy song sets. The average\nofA1;:::;Aiis a fuzzy song set that assigns to each song\na membership degree equal to the arithmetic mean of the\nmembership degrees of that song in the given sets.\nAvg(A1;:::;Ai) =f\u0016avg(A1;:::;Ai)(x)=xg\n\u0016Avg (A1;:::;Ai)(x) =iX\nj=1\u0016Aj(x)\ni(9)\nThe average operator in fuzzy sets is the pendant of the\ncommon average operator and is very useful to aggregate\ndata, a very common operation in data warehousing in or-\nder to gain some overview over large datasets.\n3.2 Defuziﬁcation Functions\nThe following functions are deﬁned on song sets. They\nextract information from the song sets to real values or\ncrisp sets.\nsupport: The support of Ais the crisp subset of Xthat\nincludes all the elements having a non-zero membership\ndegree inA.\nSupport (A) =fx2X:\u0016A(x)>0g (10)\nAll songs \nArtistPublication decade \nSong ID Publisher\nPublication year Subgenre Genre \nAlbum All similarity functions \nSimilarity function ID Similarity function group Similarity dimension Song dimension \nBeatFigure 1 . Closest Songs Cube Dimensions\ncardinality: The cardinality of Ais the sum of the mem-\nbership degrees of all its elements.\n#A=X\nx2X\u0016A(x) (11)\n4 THE MUSIC WAREHOUSE CUBES\nIn this section, we present two data cubes to store the in-\nformation presented in the scenarios. For each cube, the\nfuzzy song sets are used conformingly to Section 3.\n4.1 The Closest Songs Cube\nThe closest songs cube provides a set of the closest songs\nfrom a seed song with respect to a similarity function. For\neach seed song and for each similarity function, the closest\nsongs are stored using a fuzzy song set. The notion of\nsimilarity is represented by the fuzzy song set membership\ndegree. The closest songs take a high membership degree\nwhile the farthest songs have a low membership degree.\nThe data cube has a song dimension and a similarity\ndimension. Both dimensions have a hierarchy as illus-\ntrated in Figure 1. The cube is composed of references to\neach dimension and a fuzzy song set. Typical queries are\nmaking use of the intersection, union, and reduction op-\nerators. The queries can be performed on the song seeds\nusing pieces of information such as the artist or the cre-\nation year. Closest Songs Cube usage examples based on\ndata in Table 1 are presented below.\nIn the following examples, we assume that fuzzy song\nsets and the algebra have been implemented in an abstract\nSQL datatype, called FSSET, using object-relational ex-\ntensibility functionality like found in PostgreSQL [4].\nExample 1 The3 closest songs to song: “One” by U2\nwith respect the beat or the rock similarity:\nSELECT SUPPORT(TOP 3(UNION(closest songs)))\nFROM song AS a, similarity AS b, closest songs AS c\nWHERE a.title = ’One’ AND a.artist = ’U2’\nAND (b.sim func = ’beat’ OR b.sim func = ’rock’)\nAND b.sim id = c.sim id\nAND a.song id = c.song id;\nThe use of hierarchies facilitates the use of aggregate\nfunctions. The following example illustrates the usage of\nthe song dimension hierarchy to ﬁnd songs similar to the\nones sung by a given artist.\nExample 2 The songs that are very similar with respect\nto their beat to the songs sung by U2:\nSELECT SUPPORT(REDUCE 0.9(AVG(closest songs)))\nFROM song AS a, similarity AS b, closest songs AS c\nWHERE a.artist = ’U2’ AND b.sim func = ’beat’\nAND b.sim id = c.sim id AND a.song id = c.song id;Song dimension\nsong id title artist Jazz Rock Beat\n1 We will rock you Queen Low High Medium\n2 One U2 Low Medium Medium\n3 Hips Don’t Lie Shakira Low Low High\nSimilarity function dimension\nsim id sim func sim group\n1 Rock Acoustic\n2 Jazz Acoustic\n3 Beat Acoustic\n4 Artist Editorial\nClosest songs fact\nsong id sim id closest songs\n1 1f1/1; 0.5/2; 0/3g\n2 1f1/2; 0.3/1; 0.2/3g\n3 1f1/3; 0.6/2; 0.4/1g\n1 2f1/1; 0.2/2; 0.1/3g\n2 2f1/2; 0.9/1; 0.2/3g\n3 2f1/3; 0.8/2; 0.7/1g\nTable 1 . Song Comparisons Cube DataUser dimension\nuser country age favorite songs\nJohn USA 52f0.8/1; 0.6/2; 0.3/3g\nAlice Spain 41f0.9/2; 0.5/1; 0.3/3g\nMaria Greece 28f0.6/1; 0.3/2; 0.1/3g\nBob Denmark 22f0.1/1; 0.7/2; 0.7/3g\nQuery dimension\nquery id query\n1 return some rock music\n2 return some traditional music\n3 return some latin american music\nUser Feedback fact\nuser query id feedback\nJohn 1f1/1; 0.5/2; 0/3g\nJohn 2f1/2; 0.3/1; 0.2/3g\nAlice 1f1/3; 0.6/2; 0.4/1g\nAlice 3f1/1; 0.2/2; 0.1/3g\nMaria 1f1/2; 0.9/1; 0.2/3g\nBob 2f1/3; 0.8/2; 0.7/1g\nTable 2 . Feedback Cube\nAll queries \nQuery ID Query group Query dimension \nAll users \nLanguage Age group \nUser ID Favorite songs \nAge \nCountry Continent\nGenderUser dimension \nRegion \nFigure 2 . User Feedback Cube Dimensions\n4.2 User Feedback Cube\nThe User Feedback Cube gathers relevance statistics about\nthe songs proposed to users by the music recommendation\nsystem. As illustrated by Figure 2, the user feedback cube\nis composed of the user dimension and the query dimen-\nsion. For each user and query, the user feedback is stored.\nThe feedback given for a particular played song is stored\nas a membership degree representing how the proposed\nsong is relevant in the context of the query. A very low\nmembership degree is given when the users believes the\nsong should not have been proposed. The Feedback and\nthe Favorite Songs attribute are both deﬁned using the FS-\nSET abstract datatype.\nAs with the Closest Songs Cube, it is possible to use\naggregate functions along the dimension hierarchies.\nExample 3 The 10 songs users living in Denmark liked\nthe most when they asked for traditional music:\nSELECT SUPPORT(TOP 10(AVG(c.feedback))\nFROM user AS a, query AS b, user feedback AS c\nWHERE a.country = ’Denmark’ AND a.user = c.user\nAND b.query = ’return some traditional music’\nAND b.query id = c.query id;\nFurthermore, it is possible to perform aggregation along\nfuzzy song sets deﬁned in a dimension such as the favorite\nsongs attribute in the user dimension.\nExample 4 The 10 songs that users who dislike Shakira\nlike the most when they ask for “Latin american music”:SELECT SUPPORT(TOP 10(AVG(c.feedback)))\nFROM user AS a, query AS b, user feedback AS c\nWHERE ARRAY(REDUCE 0.6(NEG(a.favorite songs))) &&\nARRAY(SELECT song id FROM song WHERE artist = ’Shakira’)\nAND b.query = ’return some latin american music’\nAND b.query id = c.query id;\n5 STORAGE\nIn this section, three different storage options for repre-\nsenting song sets in the MW are presented: tables, arrays,\nand bitmaps. To illustrate the discussion, a prototypical\nMW where songs are uniquely identiﬁed using 24 bits and\nmembership degrees are stored using 8 bits. The proposed\nMW can reach a size of over 16 million songs and use up\nto 256 different membership degrees.\n5.1 Table\nThe ﬁrst solution is to represent the fuzzy song sets using\na table with three columns: (seed song, song, membership\ndegree) . Only the k closest songs are physically stored\nin the table. The selection is performed using the Topk\noperator. Let sbe the size of the seed song set, ethe size\nof the song set, mthe size of the set of all the values the\nmembership degree can take, and fthe number of fuzzy\nsong sets associated to a given song seed. The size of the\npayload, i.e., the size of the data when not considering the\noverhead due to the DBMS, denoted p, can be calculated\nas follows.\np=s\u0001k\u0001(log2s+f\u0001(log2e+ log2m))b (12)\nwhere log2s,log2e,log2mare the minimum number of\nbits required to store respectively a seed song, a song, and\na membership degree. Using a default membership degree\nand a Top1000 operator reduces the theoretical size of the\ntable to 109GB when each of the 16 million song seeds is\nassociated to one fuzzy song set attribute.\n5.2 Array\nA second approach is to use one dimensional arrays con-\ntaining the songs and their associated membership degree0100 200 300 400 500 600 700 800 \n500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500kp (GB) \nArray \nWAH Bitmap \nTable (a) 1 attribute per seed\n03000 6000 9000 12000 \n500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500kp (GB) \nArray \nWAH Bitmap \nTable (b) 15 attributes per seed\nFigure 3 . Estimates Comparisonfor representing song sets. The\ndata is stored in a table with\ntwo columns: (seed song, array) .\nAs with tables, only the k clos-\nest songs should be physically\nstored. The size of the payload\ncan be calculated as follows.\np=s\u0001(log2s+f\u0001k\n\u0001(log2e+ log2m))b(13)\nSo, when storing the 1000 clos-\nest songs with respect to one at-\ntribute, the size of the payload is\nreduced to 63GB. However, since the probability of hav-\ning no songs for a particular membership degree is small,\nordering the fuzzy song set by membership degrees allows\nmembership degrees to be stored using one bit relatively\nto each other: a bit set means to move to the next lower\nmembership degree, a bit unset means to keep the same\nmembership degree. In the unlikely case of a gap in the\nsequence of membership degrees, a dummy element, re-\nferred to as the empty element, is used to jump to the next\nmembership degree. For large gaps, successive empty el-\nements are used. The compression ratio, r, obtained is as\nfollows.\nr=k\u0001(log2m+ log2e)\n(k+x)(log2e+ 1)(14)\nIn order to be efﬁcient, i.e., r >1, the number of empty\nelements in the data set has to remain limited.\nx<k\u0001log2m\u00001\nlog2e+ 1(15)\nUsing a granularity of 8 bits for the membership degree,\nthe compression is effective for x <280. This is always\nveriﬁed as 256different membership degrees exist. The\ncompression ratio in the best and worst case scenarios are:\nr\u0000=k\u0001(log2m+ log2e)\n(k+m\u00001)\u0001(log2e+ 1)\nr+=log2m+ log2e\nlog2e+ 1(16)\nIn our example, using 8 bits for storing the membership\ndegree, the pessimistic ratio is 1:10while the optimistic\ncompression ratio reaches 1:28.\n5.3 Bitmap\nA third option is to use bitmaps to represent fuzzy song\nsets. Each bit indicates if a song belongs or not to the set\nof the k closest songs to a given song seed.\np=s\u0001(log2s+f\u0001e)b (17)\nThe bitmap size can be dramatically reduced using com-\npression algorithms. The WAH compression offers good\ncompression on sparse bitmaps while preserving query\nperformance.text in red\nIn the worst bit distribution, i.e., a random bitmap, the\nWAH algorithm reduces the size of the bitmap as follows.\npWAH(n;d;w )\u0019w\u0001n\nw\u00001\u0000\n1\u0000(1\u0000d)2w\u00002\u0000d2w\u00002\u0001\nb\n(18)\nwherenis the size of the bitmap in bits, dis the bit density,\ni.e., the fraction of bits set, and wis the word length, 32 on\nmost computers. Using the Topkoperator, the bit density\nisd=k\ne. On a fuzzy song set of 224songs where only\n1000 closest songs are physically stored, n= 224b, and\nd=1000\n224. The size of each bitmap reaches 63883 b. To\nrepresent the membership degree, a bitmap is constructed\nfor the membership degree each of the song could possibly\ntake.\np\u0019s\u0001\u0012\nlog2e+f\u0001pWAH(e\u0001m;k\ne\u0001m;w)\u0013\nb (19)\nThe size of the compressed bitmap for each song seed is\nonly slightly increased to 63999 b. Therefore, in an MW\nof224song seeds with one fuzzy song set attribute, the\nsize of the database reaches 140GB.\n5.4 Estimates Comparison\nFigures 3(a) and 3(b) show the expected size for stor-\ning 1 and 15 Fuzzy Song Set Attribute (FSSA), respec-\ntively, for each of the 224song seeds and for different val-\nues of k. The linearity of the WAH bitmap curves is ex-\nplained by consideringk\nn<<1and applying a binomial\ndecomposition, the payload can then be approximated by\npWAH\u00192\u0001w\u0001k.\nFor one FSSA per song seed, arrays consume half the\nstorage space of bitmaps. This difference vanishes when\nthe number of fuzzy song set attributes per song seed in-\ncreases. With respect to the storage requirements, WAH\nbitmaps seem to be a poor choice when the number of\nFSSA per song seed increases.\n6 IMPLEMENTATION\nThis section describes the Closest Song Cube fact table\nimplementation under PostgreSQL 8.2, well-known for\nits scalability. Therefore, some parts of the following are\nDBMS dependent. As explained in Section 5, songs can1 att. 15 att.TablePayload est. (MB) 345 3115\nOverhead est. (MB) 1777 1783\nTotal est. (MB) 2226 4898\nReal size (MB) 2779 21500\nB-tree index size(MB) 1557 1557ArrayPayload est. (MB) 197 2961\nOverhead est. (MB) 4 4\nTotal est. (MB) 201 2965\nReal size (MB) 729 21504\nReal size + LZ (MB) 256 2975\nB-tree index size(MB) 2 2WAH BitmapPayload est. (MB) 395 5923\nOverhead est. (MB) 4 4\nTotal est. (MB) 399 5927\nReal size (MB) 514 7696\nReal size + LZ (MB) 202 2923\nB-tree index size(MB) 2 2\nTable 3 . Storage comparison\nbe uniquely identiﬁed using 24 bits. The dataset used\nfor the implementation consists of 51574 songs that were\nselected from two different sources: ﬁrst, the Intelligent\nSound1database that has extracted the genre information\nof over 150000 music tracks, second, the 5 million songs\nof the Music Brainz2database. Each of the 51574 songs\nis described using 15 attributes that represent the genre.\nThe expected overhead in PostgreSQL can be estimated to\n32 bytes per row and 20 bytes per page, where each page\nhas a ﬁxed size of 8 KB [4]. Since tuples are not allowed\nto span over multiple pages, PostgreSQL uses secondary\nstorage tables, referred to as TOAST tables, to store large\nﬁeld values. TOAST tables can use a Lempel-Ziv, brieﬂy\nLZ, compression technique to reduce their size.\nThe implementation results are shown in Table 3. For\none FSSA per song seed, arrays and WAH bitmaps appear\nto be a better solution than tables when comparing their\nreal size. The results also show that the theoretical over-\nhead estimates were wrong. In Figure 3(a), the theoretical\nestimates showed that the size of arrays should be half the\nsize of WAH bitmaps. This is far from being the case. To\nthe contrary, WAH bitmaps are smaller than arrays.\nFor 15 FSSA per song seed, arrays and WAH bitmaps\nare again a better solution than tables. Again, the expecta-\ntions shown in Figure 3(b) were different. First, the over-\nhead is considerable compared to the data size. This con-\nforms to the theoretical result that tables are a bad choice\nsince an considerable overhead per row exists. However,\nwhen comparing arrays versus bitmaps, the implementa-\ntion and theoretical results are again contradictory. WAH\nbitmaps only take a third of the storage space of uncom-\npressed arrays, due to the overhead of the array data struc-\nture. LZ compressed WAH bitmaps in a TOAST table are\nreduced to 2923 MB, while LZ compressed arrays are re-\nduced to 2975 MB, thus making the two solutions compa-\nrable. However, using LZ compression on WAH bitmaps\nrequires further investigations. On one hand it will reduce\n1http://www.intelligentsound.org\n2http://www.musicbrainz.orgthe number of IOs. On the other hand, it will slow down\nthe bitwise operations and therefore might reduce the ad-\nvantages of the WAH compression scheme.\n7 CONCLUSION\nMWs are the practical answer to the need for efﬁcient con-\ntent management tools over vast music collections. The\napproach chosen in this paper is to treat song similari-\nties in a relative space where songs are deﬁned by com-\nparison to each others. We have deﬁned fuzzy song sets\nand presented an algebra to manipulate them. We have\ndemonstrated the usefulness of fuzzy song sets and their\noperators to handle various information management sce-\nnarios in the context of a MW for which we have created\ntwo multidimensional cubes. Furthermore, the practical\nimplementations solutions from a theoretical perspective\nwere discussed. Finally, confronting theoretical estimates\nto practical implementation makes clear that the DMBS\ndata overhead represents a major part of the storage re-\nquirements. Further study should be conducted on the\nperformance aspects and optimized operators should be\nimplemented with respect to the data structure choices.\n8 REFERENCES\n[1] G. Antoshenkov, Byte aligned data compression. United\nStates Patent , 5363098, 1994.\n[2] F. Deli `ege and T. B. Pedersen. Music warehouses: Chal-\nlenges for the next generation of music search engines. In\nProc. of LSAS , 2006.\n[3] J. Galindo, M. Piattini, and A. Urrutia. Fuzzy Databases:\nModeling, Design and Implementation . Idea Group Pub,\n2005.\n[4] The PostgreSQL Global Development Group. PostgreSQL\n8.2.0 documentation. At http://www.postgresql.org/docs/\nmanuals/ , 2006.\n[5] C. A. Jensen, E. M. Mungure, T. B. Pedersen, and\nK. Sørensen. A data and query model for dynamic playlist\ngeneration. In Proc. of IEEE-MDDM , 2007.\n[6] D. L ¨ubbers. SoniXplorer: Combining visualization and au-\nralization for content-based exploration of music collec-\ntions. In Proc. of ISMIR , 2005.\n[7] M. Mandel and D. Ellis. Song-level features and support\nvector machines for music classiﬁcation. In Proc. of ISMIR ,\n2005.\n[8] R. Neumayer, M. Dittenbach, and A. Rauber. PlaySOM and\nPocketSOMPlayer, alternative interfaces to large music col-\nlections. In Proc. of ISMIR , 2005.\n[9] E. Pampalk. Speeding up music similarity. In Proc. of\nMIREX , 2005.\n[10] C. Wang, J. Li, and S. Shi. A music data model and its ap-\nplication. In Proc. of MMM , 2004.\n[11] K. Wu, E. J. Otoo, and A. Shoshani. Optimizing bitmap\nindices with efﬁcient compression. ACM Trans. Database\nSyst., 31(1), 2006.\n[12] B.-K. Yi, H. V . Jagadish, and C. Faloutsos. Efﬁcient retrieval\nof similar time sequences under time warping. In Proc. of\nICDE , 1998."
    },
    {
        "title": "The Probado Music Repository at the Bavarian State Library.",
        "author": [
            "Jürgen Diet",
            "Frank Kurth"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417527",
        "url": "https://doi.org/10.5281/zenodo.1417527",
        "ee": "https://zenodo.org/records/1417527/files/DietK07.pdf",
        "abstract": "In this paper, we describe the Probado music repository which is currently set up at the Bavarian State Library, Munich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized audio recordings. The repository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing.",
        "zenodo_id": 1417527,
        "dblp_key": "conf/ismir/DietK07",
        "keywords": [
            "Probado music repository",
            "Bavarian State Library",
            "German Probado digital library initiative",
            "FRBR approach",
            "work-centric metadata model",
            "scanned sheet music",
            "digitized audio recordings",
            "searching mechanisms",
            "automated content-based document analysis",
            "indexing"
        ],
        "content": "THE PROBADO MUSIC REPOSITORY AT THE \nBAVARIAN STATE LIBRARY  \nJürgen Diet Frank Kurth \nBavarian State Library \nLudwigstraße 16 \n80539 Munich, Germany \njuergen.diet@bsb-muenchen.deUniversity of Bonn \nRömerstraße 164 \n53117 Bonn, Germany \nfrank@iai.uni-bonn.de  \nABSTRACT \nIn this paper, we describe the Probado music repository \nwhich is currently set up at the Bavarian State Library, \nMunich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized  audio recordings. The \nrepository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing. \n1. INTRODUCTION \nThe vast majority of content in today’s digital libraries \nthat are in productive use consist of textual documents. Even though a lot of research has been done on how to manage, retrieve, and present multimedia documents (the research presented at the ISMIR conferences shows the progress in case of music documents), there is still the need for integrating multimedia documents in existing library work-flows. User-friendly tools must be \ndeveloped so that the management of multimedia documents for librarians and the user access to these documents (both content-based and in the conventional way of searching the metadata) become possible.  In this paper, as a concrete step in this direction, we describe an ongoing project in itiative that aims at setting \nup a digital music repository at the Bavarian State Library (“Bayerische Staatsb ibliothek”, BSB), Munich. \nIn a cooperative approach with MIR researchers, the project aims at developing an integrated work-flow for both document handling and cataloging according to the \nclassical librarian workflow and content-based \ndocument processing, i.e., making the collection accessible through content-based retrieval, the latter involving automatic content-based document analysis and indexing. As key contributions, we \n• propose a generic structure of a music \nrepository for storing relevant data of real-world digital music libraries that is suitable for both conventional and c ontent-based access to \nmusic documents, \n• propose a general metadata scheme for  music \nrepositories based on the FRBR model [1], • describe a concrete design of a corresponding \nmusic repository that is currently set up at BSB and in particular describe a prototypical workflow for automatic content-based document analysis, annotation, and access, \n• sketch how the proposed repository will be \nintegrated into the larger-scale Probado-framework that facilitates access to general document types stored in digital libraries. \nAfter summarizing relevant related work in Section 2, Section 3 describes the Probado digital library project. Sections 4 and 5 are devoted to the Probado music repository currently set up at BSB. \n2. RELATED WORK \nThe Variations2 project at  Indiana University in \nBloomington [2] provides a user-friendly access to \nmusic in form of scanned sheet music and audio. Its metadata model is also based on the FRBR model. Intended users are students in music and in musicology.  Two research projects in the area of digital music \nlibraries and archives have been started recently and are \nfunded by the European Union: The EASAIER project [3] which is coordinated by Queen Mary University of London will create new and innovative methods of access to sound archives. In the DISMARC project [4], \na consortium of 10 project partners, will make audio collections searchable, discoverable and, where the content owner provides access to audio, listenable. \n3. THE PROBADO-PROJECT \nProbado is a cooperative German digital library project \nfunded by Deutsche Forsc hungsgemeinschaft (DFG). \nThe project started in February 2006 and has a tentative duration of 5 years\n1. \nThe main goal of Probado is to integrate general (in particular non-textual) multim edia documents into the \nwork-flow of existing libraries. Important subtasks of the project are to develop and implement (a) methods to support automatic processing of general documents in the library processing chain of document acquisition, annotation, search & delivery, and storage, (b) local Probado repositories which are located at particular libraries and in which documents and annotation data for single subject areas (e.g ., music) can be organized, \nand (c) a common Probado platform serving as  a web-\n                                                          \n \n1 DFG Grant  554975 (1) Oldenburg, BIB48 OLof 01-02 © 2007 Austrian Computer Society (OCG).   \n \n based access point for searching and accessing \ndocuments stored in the connected repositories. Rather than being a pure res earch project, it is a special \nfocus of Probado to achieve long-term usage of the developed systems and work-flows at the cooperating libraries. To achieve the above goals, project partners from different universities, each having expertise in \ndistinct areas of multimedia document analysis and retrieval, are cooperating with partners of two large German libraries (TIB, Hannover, and BSB, Munich).  \nDispatcherStandard Search\nUser Layer Interface\nRepository\n(Music)Probado System Layer\nRepository InterfaceContent-based Search\nRepository\n(eLearning)Repository\n(3D)Session & User\nManagerRepository \nManagerRepository\n(Global \nMetadata)\nRepository\n...User Interface Layer\nRepository Layer  \nFigure 1 . Overall system architecture of the Probado \nframework.  \nAn overview of the Probado system architecture, which \nis currently under development, is depicted in Figure 1. It consists of three layers. The user interface layer enables a user to perform both classical text-based \nqueries as well-as content-ba sed queries for the specific \nsubject areas (e.g., query-by-humming for the music subject area). The main component of the Probado system layer is a task dispatcher that is us ed to assign incoming queries \nto the respective query engine s located at the connected \nrepositories. As a special feature, global Dublin-Core-style metadata for all documents contained in the connected repositories are managed in a central metadata repository that is hosted at the Probado system.  The repository layer consists of an arbitrary number of Probado repositories, each holding documents and \nassociated data for a specific subject area. In the current \nstage of the project, Probado repositories are set up for the particular subject areas of music, 3D graphical models, and e-learning. In this paper, we will describe the Probado repository for the subject area of music, which is currently set up at BSB in cooperation with the Multimedia Signal Processing Group at the University \nof Bonn [8]. Each repository offers a specific number of query engines which are re gistered at the central \nrepository manager located in the system layer. Besides a mandatory query engine for descriptive metadata, a Probado repository may contain an arbitrary number of query engines for content-based queries. 4. THE METADATA MODEL FOR THE \nPROBADO MUSIC SUBJECT AREA \nThe Probado-model is based on the Functional \nRequirements for Bibliographic Records (FRBR). FRBR is a suggestion of the International Federation of Library Associations and Institutions for an improved structure of bibliographic metadata. The FRBR document [1] has already been published in 1998, and received a lot of attention dur ing the last years. The new \ncataloguing  standard RDA (“Resource Description and Access”) currently developed by the Joint Steering Committee for the Revision of AACR and scheduled for release in early 2009 [5] will incorporate the FRBR \nconcepts. The FRBR model is very suitable for music because it is work-centered and distinguishes between a work and its expressions. Therefore, several innovative projects in the music library area are based on the FRBR model, like the Variations2 project [2] and MusicAustralia at the National Library of Australia [6]. \nExpr essi on\nbased_upon Mani festati on\nIte mwork_exp\nex p_man\nman_i t em\nite m _ filepart_of\nw_start,\nw_end0,1\n0,*\n1,1\n0,*\n1,*\n0,*\n1,1\n0,* 1,10,1\n0,*\n0,*0,10,*\n0,*\nF ilecontr_work\ncontr_expr\ncontr_mani fCont r i but orWor k\n0,*\n0,*0,*0,*\n0,*\n0,*\n \nFigure 2 . Metadata Model of  BSB Music Repository \n \nThe FRBR document uses an entity-relationship model for describing the proposed data structure of the bibliographic records. The entities are divided into three groups, where group 1 describes intellectual and bibliographic units, group 2 describes the involved agents, and group 3 describes different subjects. In the remainder of this paper, we will only deal with the group 1 entities “work” (denoting a musical work as an abstract entity independent of any performance or recording, such as “J.S. Bach’s six suites for unaccompanied cello”), “expression” (denoting particu-lar performances of a work as abstract entities, e.g. “the Bach suites as performed by Janos Starker and  recorded \nin 1963 and 1965”), “manifestation” (denoting particular physical realizations of an expression such as “a recording of the Starker performance released on 33 \n1/3 rpm sound discs in 1965 by Mercury”) and “item”   \n \n (denoting particular locations where manifestations are \navailable, e.g. a CD available at BSB).  Figure 2 depicts the metadata model that is used for the music subject area of the Probado project. The model is shown as an entity-relationship diagram where entities are depicted by rectangles and relationships by diamonds. The cardinalities of the relationships are included in (min,max)-notation. “min” stands for the minimal number of relationships that an entity takes part in and “max” for the maximal number (“*” allows for an arbitrary number of relationships). E.g., the cardinality (0,*) on the “work”-side of the relationship “work_exp” means that one work relates to at least 0 and at most arbitrarily many expressions. The Probado model in Figure 2 is very similar to the FRBR model. All entities in the Probado-model also appear as entities in the FRBR model except for the “file” entity. We introduced th e file-entity because we \nneed the information about the physical files that store our music documents. The information contained in the item-entity is not sufficient for modelling digital documents because an item can consist of several files. The Probado-model also extends the FRBR model by two new relationships: “part_of” and “w_start, w_end”. In “part_of” we store the information which work is part of which other work and can therefore include the structure of a musical work (e.g., the individual “Lieder” that are part of Schubert’s “Winterreise” or the movements that make up a piano sonata). Using this mechanism of a “part_of”-relation hence allows us to capture the structure of a work by both modeling possible subdivisions into subparts (like movements) as well as allowing to group several works to be subparts of a superordinate work entity (like the Winterreise cycle being superordinate to the constituting Lieder). Therefore, from a technical point of view, the work \nentity is extended to not only include complete works but also parts and unions ther eof. Note that, as for each \nsuch work entity we additionally store a type informa-tion (e.g., “piece of music”, “movement”, “lied cycle”), \nthis is a proper generalization of the FRBR model. In particular, we do not lose any expressibility of the FRBR approach. The second new relationship compared to FRBR is “w_start” and indicates the offset at which a work or a work part starts in a particular manifestation. In case of audio, “w_start” contains the time offset, and \nin case of sheet music or scanned sheet music it contains the page number (the same applies to “w_end”, indicating the offset where a work ends). \n5. THE MUSIC REPOSITORY AT THE BSB \nA prototypical repository for collections of music data is \ncurrently set up at the BSB in Munich. The BSB is a \ngood candidate for a music repository because its music department owns a large document collection including 347,000 sheet music documents, 36,000 music manuscripts, 78,000 audio recordings, and 132,000 books and  journals (on music and musicology). Query Engine 1\nLyrics Search Query Engine 3\nAudio Matching\nScanned\nScoresLyrics\nIndex\nOCR\nAudio \nRecordingsFeature\nIndex\nFeature\nIndexer \nAudioQuery Engine 4\nMetadata Search\nRepository \nDatabase\n(Metadata)Symbolic\nMusicMelody \nIndex\nLyrics\nOMRQuery Engine 2\nMelody Search \nMelody\nIndexer Feature\nIndexer \nSymbolic\nMusicLyrics\nIndexer \nDocument collectionsInterface to Probado System Layer \n \n \nFigure 3 . Components of the Probado Music Repository  \n5.1. Data Collections and Digitization \nFigure 3 shows an overview of the components of the \nrepository. The repository consists of two document collections, one containing audio recordings, the other one containing scanned images of musical scores. The underlying music material consists of classical and romantic piano sonatas (Haydn, Mozart, Beethoven, Schubert, Schumann, Chopin, Liszt, Brahms) as well as a collection of German 19th centuries piano songs. This collection amounts to approximately 6.000 images of scanned sheet music and about 1.200 musical works in total. The score material has been scanned at the BSB in \nhigh quality (600 dpi) and stor ed in TIFF-format with \nloss-less compression. For each musical work, audio recordings are currently dig itized at the BSB and stored \nin WAV- and MP3-format within the music repository. \n5.2. Obtaining the Metadata \nA goal of the Probado project is to minimize the manual \ncataloging work and to automatically generate the appropriate metadata wherever possible. The main source for the Probado metadata is the BSB cataloging database. It contains metadata for the whole library collection in the MAB-format, the German standard corresponding to the Anglo-American MARC-format. The MAB-records describing the Probado collections have to be transformed into the FRBR-based Probado repository database, a process called “FRBRization” [7]. It turns out that this process can not be fully automated. Especially the extraction of the metadata on the work and expression levels needs manual intervention. Although it will be possible to do this cataloging work manually for the initial Probado collections, further automation of this process is required for the future augmentation of the Probado collections. \n5.3. Searching the Music Repository \n \nThe user can search in th e repositories connected to \nProbado in different ways: when no specific subject area is selected, the user enters text in a Google-like   \n \n basic search form and the search is done on global \nProbado metadata which is a subset of the metadata of all connected Probado repositories (similar to Dublin Core). For music-specific searches, the Probado system allows (a) searching  in the metadata of Probado music repo-sitories and (b) content-based search. \n(a) The Probado Music OPAC  \nThe search in the metadata of the Probado music \nrepositories is similar to the traditional search in OPACs of large music libraries. It is  a text-based search with a \nbasic search form (with just one input field) and an advanced form. In the adva nced case, the user can \nspecify entities and attributes whose values must match the text that the user enters into the input fields. For the presentation of the search results, it is not sufficient to provide the user with an unstructured list of documents. Instead, the FRBR-structure of the metadata should be preserved. E.g., the Probado music OPAC should differentiate the identification of a musical work from the identification of a particular recording.\n \n(b) Content-based Search \nFor content-based search, the repository will offer three \nquery engines (see Figure 3). Those query engines facilitate text-based search in the textual transcriptions of the piece's vocal tracks (lyr ics), melody-based search \n(Query-by-humming) in the musical notes from the scores, and audio retrieval (Query-by-Example) based on the audio matching approach. For technical details on the corresponding retrieval problems, we refer to the corresponding technical papers [9][10][11]. For efficient retrieval, the query engines employ particular search indexes which are created from the document collections by suitable indexing applications in an offline-operation.  \n5.4. Content-based Document Analysis and Indexing \nBecause of the unavailability of comprehensive (i.e., \ncomplete and consistent) lyrics- and score-material in a symbolic digital format, the scanned scores are processed by helper applications (Figure 3, light boxes) for optical character recognition (OCR) and optical music recognition (OMR). Although the extracted lyrics- and score-material deviates from the correct data because of \nOCR- and OMR-errors, the material is nevertheless useful for the search and retrieval process. Using suitable indexer applications, lyrics- and melody-indexes are created from the OMR- and OCR-data. The audio recordings are processed by an audio indexer that extracts chroma-based audio features representing the rough harmonic progression of a piece of audio [9]. From those \nfeatures, a feature index is created. Without going into technical details, we mention that it is also possible to extract a chroma-based repr esentation from the symbolic \n(OMR-) data. Adding the resulting features into the audio feature index, audio matching is possible, at the same time, on the audio recordings and the score material. \nTechnical details will be reported elsewhere. 6. CONCLUSIONS AND FUTURE DIRECTIONS \nIn this paper, we presented the Probado music \nrepository for organizi ng and making accessible \ncollections of digital music documents which is currently set up at the Bavari an State Library, Munich, \nas a component of the larger Probado framework. The next steps in setting up the music repository will consist of  finishing the meta-data acquisition for the initial Probado music collection and subsequently increasing the number of music documents considerably. Concerning content-based document analysis, the next steps consist of evaluating and completing the workflow for automatic OMR/OCR-based indexing and annotation. For content-base d retrieval, existing query \nengines [11] have to be adapted and integrated into the repository. For presenting retrieval results to the user and for facilitating content-based browsing within the music collection, we will adopt components of the existing SyncPlayer framework [10] for multimodal browsing, retrieval, and presentation of music data.  Finally, the repository will be  connected to the overall \nProbado framework and the Probado service will be integrated into the conventional library services. For information on the Probado project, we refer to [12]. \n7. REFERENCES \n[1] IFLA Study Group on the Functional Requirements \nof Bibliographic Records. Functional Requirements \nfor Bibliographic Records; Final Report . Saur, \nMunich, 1998. available at www.ifla.org/VII/s13/frbr/frbr.pdf \n[2] Dunn, J. W., Byrd, D., No tess, M., Riley, J., and \nScherle, R. Variations2: Retrieving and Using \nMusic in an Academic Setting. Comm. ACM, 49 (8), \n2006, pp. 53-58. \n[3] http://www.easaier.org/ \n[4] http://www.dismarc.org/ \n[5] http://www.collectionscanada.ca/jsc/rda.html \n[6] http://www.musicaustralia.org/ \n[7] Yee, M. M. FRBRization: A Method for Turning \nOnline Public Finding Lists into Online Public Catalogs . Information Technology and Libraries, 24 \n(2), 2005, pp. 77-95 \n[8] http://www-mmdb.iai.uni-bonn.de/ \n[9] Müller, M., Kurth, F., Cl ausen, M.: Audio Matching \nvia Chroma-based Statistical Features. In: ISMIR, London, GB., 2005. \n[10] Kurth, F., Müller, M., Damm, D., Fremerey, C., \nRibbrock, A., Clausen, M.:  Sync Player — An  \nAdvanced System for Multimodal Music Access. In: ISMIR, London, GB, 2005. \n[11] Clausen, M., Kurth, F.: A Unified Approach to \nContent-Based and Fault Tolerant Music Recognition, IEEE Trans. Multimedia 6 (5), 2004. \n[12] http://www.probado.de/"
    },
    {
        "title": "Music Recommendation Mapping and Interface Based on Structural Network Entropy.",
        "author": [
            "Justin Donaldson",
            "Ian Knopke"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417329",
        "url": "https://doi.org/10.5281/zenodo.1417329",
        "ee": "https://zenodo.org/records/1417329/files/DonaldsonK07.pdf",
        "abstract": "Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists are often obscure, measured strength of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a dynamic plot interaction method as a recommendation browsing utility. A novel “Recommendation Map” web application implements both the ZMDS algorithm and the plot interface and are offered as an example of both components working together. 1 INTRODUCTION Item based recommendation systems such as the music recommendation services offered by MyStrands 1 are based on networks of associations between items. These associations are formed from aggregate observations or records of sets that occur between the items. In the context of the MyStrands music recommendation network, these sets are playlists. A network can be constructed from this data where each item is a node and the connection weights between the items are the number of times that the two items appear in any set. Recommendation for music in this context involves providing one or more songs as “input” into the system. The system resolves the songs against its list of known songs, and then retrieves a set of recommended songs that have strong associations for the provided songs. For the purposes of this project, a dataset of 1.6 million songs from the MyStrands recommendation database was used. The data was constructed from hundreds of thousands of playlists, which in turn formed millions of links between the individual songs. Recommendation results were retrieved through a modified network neighborhood 1 http://www.mystrands.com c⃝2007 Austrian Computer Society (OCG). extraction routine. This process resolves the input songs in the network, and then retrieves each of the direct neighbors of these songs. 2 MUSIC NETWORK VISUALIZATIONS Since the recommendation data is a network, it is possible to visualize the prominent structure of the network in two or three dimensions using conventional network visualization techniques. Many recent projects have attempted to visualize music relationships in a low dimensional space according to association or content based similarity metrics [2, 1]. Visualizations such as these are extremely valuable for understanding the various relationships in music. 3 ZMDS AND STRUCTURAL NETWORK ENTROPY ZMDS is a novel modification of the standard Euclidean distance based multidimensional scaling method. It involves the following steps: (1) Construction of a modified association matrix from the recommendation result, (2) row-wise z-score normalization of node edge weights (The Z in ZMDS is taken from this step.), (3) row-wise Euclidean distance calculation for dissimilarity matrix, and (4) classic multidimensional scaling method on dissimilarity matrix for required number of dimensions. According to this method, in step 1 a matrix of associations from a recommendation result is constructed. This matrix is similar to a Laplacian matrix, where each nondiagonal entry is the association weight between two of the songs in the recommendation result, and each diagonal entry is the global edge weight for the song. Each song’s connection weights in the matrix are modified by a function of the songs total participation in the network in step 2. In order to view the resulting structure in Euclidean space, it is necessary to convert the asymmetric weighted matrix into a symmetric version. This can be done by calculating row-wise Euclidean distances and constructing a symmetric dissimilarity matrix. Applying a multidimensional reduction algorithm on the dissimilarity matrix will provide a low dimensional representation fit for visualization. A z-score normalization method is preferred which uses variance (unit standard deviation) as Figure 1. Artist, genre, and popularity clustering in ZMDS graph the basis for edge weighting: wi,j = ki,j −¯k σ (1) With wi,j being the node weight between nodes i and j, ki,j , being the co-occurrence weight, and σ being the standard deviation for row k. By encoding the size of the node by its global edge degree, it is clear to see how the large hub-pop songs are marginalized on the left side of the distribution. The right side of the distribution describes the two tail structures for the recommendation set. In the context of the recommendation set retrieved, these tails correspond to songs by the artist Bruce Springsteen and Tori Amos (See Figure 1). These artists have long histories and loyal fan bases. As a result, they have a catalog of songs that share more intracatalog edges than extra-catalog edges. However, the associations do not form a cluster in the traditional sense. Instead, the tail structures can be thought of as a “gradient” cluster feature. Nodes along the ends of the tails will include regions of songs that are less widely popular for a given artist. However, while most of these songs are only associated with other Springsteen and Amos songs, there are songs that act as bridges or gateways into the rest of the music network. Logically, these songs are the most widely popular songs for both artists. Near the center of the distribution where these tails start is the “low entropy hub” which ties together the tails into the larger high entropy fan region on the left. It is important to note that one or more of these features may be missing from a ZMDS distribution. If there is no significant recommendation set structure, there will be no tails. Likewise, low entropy hubs may be missing as well. 4 INTERACTIVE VISUALIZATION A node repulsion technique was used to handle the node occlusion that occurs in such representations. This technique is very similar to a class of techniques collectively known as liquid browsing [3]. The motivation for the interactive implementation is to allow a user to recognize Figure 2. Recommendation mapping applet and investigate occlusion as it occurs in the low dimensional representation, as well as to retrieve non-dimensional information about the node, such as artist or track title. The visual interface was created in Macromedia Flash as a lightweight web application, suitable for visualizing and investigating music recommendation sets of around two hundred elements. The actual ZMDS process is performed using the PDL matrix data language 2 , and passed to the Flash applet as an XML data set (See Figure 2). The overall effect of this behavior is easy to perceive in the context of the web interface itself, which is available online 3 . 5 REFERENCES [1] F. J. Igo, M. Brand, K. Wittenburg, D. Wong, and S. Azuma. Multidimensional visualization for collaborative filtering recommender systems, 2002. [2] E. Pampalk. Islands of music: Analysis, organization, and visualization of music archives. Journal of the Austrian Soc. for Artificial Intelligence, 22:20–23, 2003. [3] C. Waldeck, D. Balfanz, C. G. Center, and G. ZGDV. Mobile liquid 2d scatter space (ml2dss). Information Visualisation, 2004. IV 2004. Proceedings. Eighth International Conference on, pages 494–498, 2004. 2 PDL: The Perl Data Language: http://pdl.perl.org/ 3 Recommendation Mapping Applet: http://labs.mystrands.com/cgibin/recmap.cgi",
        "zenodo_id": 1417329,
        "dblp_key": "conf/ismir/DonaldsonK07",
        "content": "MUSIC RECOMMENDATION MAPPING AND INTERFACE BASED ON\nSTRUCTURAL NETWORK ENTROPY\nJustin Donaldson\nIndiana University School of Informatics\nHuman Computer Interaction/DesignIan Knopke\nIndiana University School of Informatics\nMusic Informatics\nABSTRACT\nRecommendation systems generally produce the results of\ntheir output to their users in the form of an ordinal list.\nIn the interest of simplicity, these lists are often obscure,\nabstract, or omit many relevant metrics pertaining to the\nmeasured strength of the recommendations or the rela-\ntionships the recommended items share with each other.\nThis information is often useful for coming to a better\nunderstanding of the nature of how the items are struc-\ntured according to the recommendation data. This paper\ndescribes the ZMDS algorithm, a novel way of analyzing\nthe fundamental network structure of recommendation re-\nsults. Furthermore, it also describes a dynamic plot in-\nteraction method as a recommendation browsing utility.\nA novel “Recommendation Map” web application imple-\nments both the ZMDS algorithm and the plot interface and\nare offered as an example of both components working to-\ngether.\n1 INTRODUCTION\nItem based recommendation systems such as the music\nrecommendation services offered by MyStrands1are based\non networks of associations between items. These associ-\nations are formed from aggregate observations or records\nof sets that occur between the items. In the context of\nthe MyStrands music recommendation network, these sets\nare playlists. A network can be constructed from this data\nwhere each item is a node and the connection weights be-\ntween the items are the number of times that the two items\nappear in any set. Recommendation for music in this con-\ntext involves providing one or more songs as “input” into\nthe system. The system resolves the songs against its list\nof known songs, and then retrieves a set of recommended\nsongs that have strong associations for the provided songs.\nFor the purposes of this project, a dataset of 1.6 million\nsongs from the MyStrands recommendation database was\nused. The data was constructed from hundreds of thou-\nsands of playlists, which in turn formed millions of links\nbetween the individual songs. Recommendation results\nwere retrieved through a modiﬁed network neighborhood\n1http://www.mystrands.com\nc\r2007 Austrian Computer Society (OCG).extraction routine. This process resolves the input songs\nin the network, and then retrieves each of the direct neigh-\nbors of these songs.\n2 MUSIC NETWORK VISUALIZATIONS\nSince the recommendation data is a network, it is possible\nto visualize the prominent structure of the network in two\nor three dimensions using conventional network visualiza-\ntion techniques. Many recent projects have attempted to\nvisualize music relationships in a low dimensional space\naccording to association or content based similarity met-\nrics [2, 1]. Visualizations such as these are extremely valu-\nable for understanding the various relationships in music.\n3 ZMDS AND STRUCTURAL NETWORK\nENTROPY\nZMDS is a novel modiﬁcation of the standard Euclidean\ndistance based multidimensional scaling method. It in-\nvolves the following steps: (1) Construction of a modi-\nﬁed association matrix from the recommendation result,\n(2) row-wise z-score normalization of node edge weights\n(The Z in ZMDS is taken from this step.), (3) row-wise\nEuclidean distance calculation for dissimilarity matrix, and\n(4) classic multidimensional scaling method on dissimi-\nlarity matrix for required number of dimensions.\nAccording to this method, in step 1 a matrix of associ-\nations from a recommendation result is constructed. This\nmatrix is similar to a Laplacian matrix, where each non-\ndiagonal entry is the association weight between two of\nthe songs in the recommendation result, and each diago-\nnal entry is the global edge weight for the song.\nEach song’s connection weights in the matrix are mod-\niﬁed by a function of the songs total participation in the\nnetwork in step 2. In order to view the resulting structure\nin Euclidean space, it is necessary to convert the asymmet-\nric weighted matrix into a symmetric version. This can\nbe done by calculating row-wise Euclidean distances and\nconstructing a symmetric dissimilarity matrix. Applying\na multidimensional reduction algorithm on the dissimilar-\nity matrix will provide a low dimensional representation\nﬁt for visualization. A z-score normalization method is\npreferred which uses variance (unit standard deviation) asFigure 1 . Artist, genre, and popularity clustering in\nZMDS graph\nthe basis for edge weighting:\nwi;j=ki;j\u0000\u0016k\n\u001b(1)\nWithwi;jbeing the node weight between nodes iand\nj,ki;j, being the co-occurrence weight, and \u001bbeing the\nstandard deviation for row k.\nBy encoding the size of the node by its global edge de-\ngree, it is clear to see how the large hub-pop songs are\nmarginalized on the left side of the distribution. The right\nside of the distribution describes the two tail structures for\nthe recommendation set. In the context of the recommen-\ndation set retrieved, these tails correspond to songs by the\nartist Bruce Springsteen and Tori Amos (See Figure 1).\nThese artists have long histories and loyal fan bases. As a\nresult, they have a catalog of songs that share more intra-\ncatalog edges than extra-catalog edges. However, the as-\nsociations do not form a cluster in the traditional sense.\nInstead, the tail structures can be thought of as a “gradi-\nent” cluster feature. Nodes along the ends of the tails will\ninclude regions of songs that are less widely popular for a\ngiven artist. However, while most of these songs are only\nassociated with other Springsteen and Amos songs, there\nare songs that act as bridges or gateways into the rest of\nthe music network. Logically, these songs are the most\nwidely popular songs for both artists. Near the center of\nthe distribution where these tails start is the “low entropy\nhub” which ties together the tails into the larger high en-\ntropy fan region on the left. It is important to note that one\nor more of these features may be missing from a ZMDS\ndistribution. If there is no signiﬁcant recommendation set\nstructure, there will be no tails. Likewise, low entropy\nhubs may be missing as well.\n4 INTERACTIVE VISUALIZATION\nA node repulsion technique was used to handle the node\nocclusion that occurs in such representations. This tech-\nnique is very similar to a class of techniques collectively\nknown as liquid browsing [3]. The motivation for the in-\nteractive implementation is to allow a user to recognize\nFigure 2 . Recommendation mapping applet\nand investigate occlusion as it occurs in the low dimen-\nsional representation, as well as to retrieve non-dimensional\ninformation about the node, such as artist or track title.\nThe visual interface was created in Macromedia Flash as\na lightweight web application, suitable for visualizing and\ninvestigating music recommendation sets of around two\nhundred elements. The actual ZMDS process is performed\nusing the PDL matrix data language2, and passed to the\nFlash applet as an XML data set (See Figure 2). The over-\nall effect of this behavior is easy to perceive in the context\nof the web interface itself, which is available online3.\n5 REFERENCES\n[1] F. J. Igo, M. Brand, K. Wittenburg, D. Wong, and\nS. Azuma. Multidimensional visualization for collab-\norative ﬁltering recommender systems, 2002.\n[2] E. Pampalk. Islands of music: Analysis, organiza-\ntion, and visualization of music archives. Journal of\nthe Austrian Soc. for Artiﬁcial Intelligence , 22:20–23,\n2003.\n[3] C. Waldeck, D. Balfanz, C. G. Center, and G. ZGDV .\nMobile liquid 2d scatter space (ml2dss). Information\nVisualisation, 2004. IV 2004. Proceedings. Eighth In-\nternational Conference on , pages 494–498, 2004.\n2PDL: The Perl Data Language: http://pdl.perl.org/\n3Recommendation Mapping Applet: http://labs.mystrands.com/cgi-\nbin/recmap.cgi"
    },
    {
        "title": "Tuning Frequency Estimation Using Circular Statistics.",
        "author": [
            "Karin Dressler",
            "Sebastian Streich"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416294",
        "url": "https://doi.org/10.5281/zenodo.1416294",
        "ee": "https://zenodo.org/records/1416294/files/DresslerS07.pdf",
        "abstract": "In this document a new approach on tuning frequency estimation based on circular statistics is presented. Two",
        "zenodo_id": 1416294,
        "dblp_key": "conf/ismir/DresslerS07",
        "keywords": [
            "new",
            "approach",
            "frequency",
            "estimation",
            "circular",
            "statistics",
            "tuning",
            "circular",
            "statistics",
            "approach"
        ],
        "content": "TUNING FREQUENCY ESTIMATION USING CIRCULAR STATISTICS\nKarin Dressler\nFraunhofer IDMT\nLangewiesener Str. 22\n98693 Ilmenau, Germany\ndresslkn@idmt.fraunhofer.deSebastian Streich\nMusic Technology Group\nPompeu Fabra University\nBarcelona, Spain\nsstreich@beat.yamaha.co.jp\nABSTRACT\nIn this document a new approach on tuning frequency\nestimation based on circular statistics is presented. Two\nmethods are introduced: the calculation of the tuning fre-\nquency over an entire audio piece, and the estimation of\nan adapting reference frequency for a single voice.\nThe results for the tuning frequency estimation look\nvery good for audio pieces where the dominant voices are\ntuned close to the equal-temperament scale and exhibit\nonly moderate frequency dynamics. For the analysis of\npopular western music, the method does not achieve very\nrobust results due to the strong frequency dynamics of the\nhuman singing voice. Nevertheless, the method could be\nimproved by excluding the singing voice from the calcu-\nlation taking only the accompaniment into account.\nThe main advantage of the proposed method lies espe-\ncially in the easy computation of an adaptive reference fre-\nquency using an exponential moving average. This adap-\ntive reference can for example be used in the quantization\nof the singing voice into a note representation.\n1 INTRODUCTION\nThe tuning frequency estimation for musical audio signals\nby itself is not a very popular research topic in the MIR do-\nmain. If addressed at all, it is mostly regarded as a minor\npreprocessing step in a larger system performing for ex-\nample key or chord detection. However, results from key\ndetection experiments with real world recordings show [1]\nthat the tuning frequency estimation might have some im-\npact on the overall system performance. In general, any\nsystem that at some point quantizes frequencies from the\ncontinuous scale into pitch classes or semitone intervals\nneeds a speciﬁed reference frequency. Since in most prac-\ntical cases it is not possible to make a safe assumption\nabout the underlying tuning frequency of a music record-\ning the only possibility is to estimate this value from the\ndata itself.\nSeveral methods reported in the literature make use of\npitch histograms (e.g. [2],[3] and [4]). These approaches\nshare the disadvantage that the pitch values already need\nto be quantized for the preprocessing in order to construct\nc/circlecopyrt2007 Austrian Computer Society (OCG).the histograms. Others rely on iterative optimization mech-\nanisms (e.g. [5]). Here we want to propose two differ-\nent approaches (either as a global or as an adaptive esti-\nmation) originating from the domain of circular statistics\nthat we consider more elegant and advantageous in several\nways. The methods do not require a quantization, they\nare efﬁciently computable, consume only minimal stor-\nage, and produce as a byproduct a conﬁdence measure for\nthe reference frequency estimate.\n2 METHOD\nIf we assume a twelve-tone equal temperament scale, each\nsemitone lies exactly on a 100 cent grid. The cent is a log-\narithmic unit of measure used for musical intervals. One\noctave equals 1200 cents. For the frequency to cent con-\nversion, we compute cent values relative to the standard\ntuning frequency of 440 Hz:\nc= 1200 ·log2/parenleftbiggf\n440Hz/parenrightbigg\n. (1)\nWhen frequencies have to be quantized to the nearest semi-\ntone, all frequencies within ±50cent distance are assigned\nto the same tone. The absolute cent deviation may not be-\ncome bigger than 50 cent, because in this case simply the\nnext semitone will be considered.\nFor the estimation of the tuning frequency only the cent\ndeviation from the 100 cent grid is of importance, no mat-\nter to which semitone in particular a frequency is assigned.\nIf a quantity ”wraps around” after reaching a certain value\nwe may speak of circular data. The simple calculation of\nthe arithmetic mean is not an appropriate statistic for such\ndata. In this case histogram techniques can be used to ex-\nploit a certain trend in the data.\nAnother way to deal with circular or directional quanti-\nties is circular statistics, a subdiscipline of statistics that is\nfor example applied when directions or periodic time mea-\nsurements (e.g. day, week, month) have to be evaluated[6].\nSuch data are often best handled not as scalars, but as\n(unit) vectors in the complex plane. Each cent value is\ntreated as a unit vector ˆuwhose angle φis the appropriate\nfraction of a full circle:\nˆu= 1·ejφ(2)with\nφ=2π\n100·c.\n2.1 Overall Estimate\nIn order to determine the tuning frequency of the entire\naudio piece the sum of all ”cent” vectors is computed and\nthen divided by the number of values Nto get a mean ¯z\nof the circular quantities:\nRe(¯z) =/summationtext\nicos (φi)\nN\nIm(¯z) =/summationtext\nisin (φi)\nN.(3)\nThe mean ¯zis again a vector in the complex plane which\nmeans it can be decomposed into a magnitude and a phase\nvalue. The magnitude |¯z|lies in the interval [0,1]. It de-\npends on the amount of variation in the vectors that are av-\neraged. The more the cent values are scattered, the smaller\nthe magnitude of the complex number ¯zwill be, whereas\nif¯zhas a magnitude close to 1 that would imply a signif-\nicant tendency in the data. We can therefore see |¯z|as a\nconﬁdence measure for the tuning frequency estimate.\nThe phase angle ¯φof¯zis converted back into cents to\nobtain the deviation from the standard tuning frequency of\n440 Hz in cents:\n∆c=100\n2πarg(¯z). (4)\nWith help of the estimated cent deviation ∆cthe reference\nfrequency can be calculated as\nfref= 2∆c/1200·440Hz. (5)\nThe results of the calculation can be improved if each\ncent vector is weighted by a certain weighting factor ri:\nRe(¯z) =/summationtext\niricos (φi)/summationtext\niri\nIm(¯z) =/summationtext\nirisin (φi)/summationtext\niri(6)\nThree different approaches using equation (6) were ex-\namined:\nSpectral peaks: The instantaneous frequencies and mag-\nnitudes of salient spectral peaks were computed us-\ning the spectral analysis frontend described in [7].\nThe cent vectors calculated from the instantaneous\nfrequencies are weighted with the respective peak\nmagnitudes to avoid a high impact of noise peaks.\nMelody pitch: In the second approach, only the estimated\npitch frequencies of the melody voice are taken into\naccount. The resulting cent vectors are weighted\nby the pitch magnitude. The pitch contour and the\npitch magnitude are computed by the melody ex-\ntraction algorithm described in [8].Stable melody pitch: There are many instruments that al-\nlow a substantial frequency variation within one sin-\ngle tone. The human singing voice is a prominent\nexample – a sung vibrato can easily exceed more\nthan one semitone in either direction from the note\nitself. Nevertheless the perceived tone height is to\na certain degree stable. In our third approach, we\nsuccessively assign stable tone heights to the iden-\ntiﬁed melody tone in order to get a more reliable\nreference frequency for the melody voice.\n2.2 Exponential Moving Average\nThe exponential moving average (EMA) is a technique\nused to analyze time series data. The EMA applies weight-\ning factors which decrease exponentially in time – giving\nmore importance to recent observations while still not dis-\ncarding older observations entirely. If the pitch of a tone\nneeds to be quantized into the equal-temperament scale,\nit has to be evaluated relative to the current reference fre-\nquency, which might change over time. At each time in-\nstance a reference frequency estimate is computed by the\nfollowing recursive formula:\nz/prime\ni= (1−α)·z/prime\ni−1+α·riejφiwith z/prime\n0= 0.(7)\nEach cent vector ejφiis again weighted with a magnitude\nmeasure ri. The decay of older values is determined by a\nconstant smoothing factor α\nα= 1−0.5∆t/T1/2. (8)\nThe parameter ∆tspeciﬁes the time advance between two\nobservations, the parameter T1/2is the half-life period. In\ncase of an STFT spectrogram ∆tis given by\n∆t=L\nfs, (9)\nwhere Lis the hop-size in samples and fsdetermines the\nsampling rate of the audio data.\nFinally, z/prime\niis normalized, so that the magnitude of z/prime\ni\nlies in the interval [0,1]:\nz/prime\ni=z/prime\ni\nr/prime\ni(10)\nwith\nr/prime\ni= (1−α)·r/prime\ni−1+α·ri\nFor each frame, the argument of z/prime\nicorresponds to a\nreference frequency estimate which can be calculated as\ngiven in equations (4) and (5). Again the magnitude of the\nnormalized complex z/prime\nimay be used as a conﬁdence mea-\nsure for the estimate. Of course, meaningful results can\nbe expected only after an initial settling phase has passed.\nTwo different values of the half-life period T1/2are used\nfor the evaluation: T1/2= 2s gives more stable results,\nwhile T1/2= 0.5s allows for very quick adaption.1.) spectral peaks 2.) melody pitch 3.) stable melody pitch\nﬁle |¯z| ∆c fref |¯z| ∆c fref |¯z| ∆c fref\ndaisy1 0.54 1.7 440.4 0.77 0.8 440.2 0.83 0.7 440.2\ndaisy4 0.70 0.2 440.1 0.87 0.5 440.1 0.95 0.5 440.1\njazz1 0.43 -2.1 439.5 0.67 -2.2 439.5 0.71 -1.1 439.7\njazz4 0.37 -0.5 439.9 0.83 0.3 440.1 0.84 1.4 440.4\nmidi3 0.82 -4.4 438.9 0.96 -4.7 438.8 0.99 -4.7 438.8\nmidi4 0.39 -2.9 439.3 0.93 -0.7 439.8 0.94 -0.8 439.8\nopera4 0.28 8.5 442.3 0.12 12.8 443.3 0.30 -6.9 438.3\nopera5 0.13 15.4 443.9 0.11 10.9 442.8 0.48 25.8 446.6\npop2 0.31 -4.4 438.9 0.35 -10.5 437.3 0.45 -13.1 436.7\npop3 0.21 -2.4 439.4 0.41 3.2 440.8 0.50 3.2 440.8\nTable 1 . Overall tuning frequency estimation of the ISMIR2004 data set\n3 RESULTS\n3.1 Data Set\nFor the ISMIR 2004 Audio Description Contest, the Mu-\nsic Technology Group of the Pompeu Fabra University\nassembled a diverse set of 20 polyphonic musical audio\npieces and corresponding melody transcriptions including\nMIDI, Jazz, Pop and Opera music as well as audio pieces\nwith a synthesized voice (daisy examples)1. The small\ndata set allows only a rather limited statistical analysis.\nYet, it was intentionally chosen in order to demonstrate\nthe signiﬁcant inﬂuence of the music style on the tuning\nfrequency estimation. We would also like to point out that\nit is almost impossible to obtain valid ground truth data\nfor a time-varying tuning frequency other than for com-\npletely synthesized music. We therefore focus on a quali-\ntative analysis here rather than providing actual detection\naccuracies.\n3.2 Overall Estimate\nTable 1 shows the evaluation results of the overall tun-\ning frequency calculation for each audio piece. A very\ngood algorithm performance with high conﬁdence mea-\nsures can be noted with artiﬁcially generated data (midi,\ndaisy) and pieces with a rather stable frequency through-\nout a tone like in the jazz examples. This is especially\ntrue for the third approach using stable melody pitches.\nThe ﬁrst approach (spectral peaks) has a lower conﬁdence\nmeasure for these examples, because noisy peaks and the\ndeviation from the equal-temperament scale of some higher\nharmonics affect the result[4].\nClearly, the results look very different for the pop and\nopera examples. The dominant human voice with high\nfrequency dynamics has a strong impact on the tuning fre-\nquency estimation. This fact becomes obvious especially\nin the opera examples, which contain a voice with a very\nstrong vibrato. The expressivity of the voice lays in the\nfundamental frequency variations, eg. glissandi and vi-\nbrato.\n1The data set can be found on the contest web page http://\nismir2004.ismir.net/melody_contest/results.htmlWe tried to compensate such effects by assigning stable\npitch estimates before calculating the reference frequency\n(stable melody pitch approach). However, the singer is not\nexactly in tune with the accompaniment and the micro-\ntuning of the voice slightly changes during the perfor-\nmance. That is why the spectral peak approach surpris-\ningly yields comparable conﬁdence values in the opera\nexamples, because the harmonics of the accompaniment\nare taken into account.\nEvidently, the proposed method struggles to accurately\nestimate the overall tuning frequency as long as the human\nsinging voice ”corrupts” the data. We expect to markedly\nincrease reliability by isolating the instruments that have\nstable pitches.\nWe conclude that an evaluation of tuning frequency es-\ntimators on artiﬁcially generated audio does not necessar-\nily show the efﬁciency of the method with e.g. popular\nwestern music.\n3.3 Exponential Moving Average\nThe ﬁgures 1 and 2 show two examples for an adapting\nreference frequency calculated as exponential moving av-\nerage. We see the computed cent deviation and the con-\nﬁdence measure over time for two different values of the\nhalf-life period parameter T1/2. The short-term estimates\nwith T1/2= 0.5s adapt very quickly to a shifting refer-\nence frequency, while the long-term reference frequency\nestimate has a more stable progression. Of course one will\nnot expect the reference frequency to shift that quickly\nin a professional recording – otherwise it would not be a\nreference. Still, even the short-term estimate may be of\nvalue if you have to analyze a very mistuned voice of an\nuntalented singer. In this case, if the pitch being quan-\ntized repeatedly ﬁts badly to the grid deﬁned by the long-\nterm reference frequency, the algorithm could switch to\nthe short-term reference.\nEven if the overall tuning frequency estimate of the\njazz1 example does not differ much from the standard tun-\ning, the local reference frequency of the saxophone de-\nviates slightly with the played melody because the saxo-\nphone is not tuned in equal-temperament.\nThe reference frequency of the male voice in the opera5\nrecording cannot be determined with a high conﬁdence0 2 4 6 8 10 12 14 16−50050\ntime (s)cent deviation2s\n0.5s\n0 2 4 6 8 10 12 14 1600.51\ntime (s)confidenceFigure 1 . Short-term and long-term estimates of the cent deviation and the conﬁdence for the jazz1 recording\n0 5 10 15 20−50050\ntime (s)cent deviation2s\n0.5s\n0 5 10 15 2000.51\ntime (s)confidence\nFigure 2 . Short-term and long-term estimates of the cent deviation and the conﬁdence for the opera5 recording\nmeasure, as there is no ”ground truth” tuning frequency\nin the human voice. It can be noted that the reference fre-\nquency changes over time: Towards the end of the tune the\nopera singer is singing sharp while increasing the volume\nof his voice.\n4 CONCLUSION\nIn this document, we presented a new approach to tun-\ning frequency estimation based on circular statistics. Two\nmethods have been introduced: The calculation of the tun-\ning frequency over an entire audio piece, and the estima-\ntion of an adapting reference frequency for a single voice.\nThe estimation of the tuning frequency over the entire\naudio piece is a convenient way to estimate the tuning fre-\nquency, as long as one can assume that the tuning stays the\nsame throughout the music piece and the dominant voices\nexhibit only moderate frequency modulation. Yet, espe-\ncially the human singing voice often does not fulﬁll these\nrequirements. The method could be improved by the iden-\ntiﬁcation of the accompaniment with stable pitch and the\nexclusion of the human voice from the calculation.\nThe estimation of the adaptive reference frequency can\nbe used to study the evolution of the tuning over time,\nwhich may be useful to monitor the tuning of instruments\nor singers/choirs singing a capella. The EMA approach\nalso proved useful for the quantization of a sung melody\ninto a note representation, for example in a query by hum-\nming system or a melody extraction algorithm. In the easy\ncomputation of an adapting reference frequency in differ-\nent time scales, we see a decisive advantage of the pro-posed method over histogram techniques.\n5 REFERENCES\n[1] A. Lerch, “On the requirement of automatic tuning\nfrequency estimation,” in Proc. of the 7th Int. Conf.\non Music Information Retrieval , 2006, pp. 212–215.\n[2] M. P. Ryyn ¨anen, “Probabilistic modelling of note\nevents in the transcription of monophonic melodies,”\nM.S. thesis, Tampere University of Technology, 2004.\n[3] C. A. Harte and M. B. Sandler, “Automatic chord iden-\ntiﬁcation using a quantised chromagram,” in Proc. of\nthe 118th AES Convention , 2005, number 6412.\n[4] E. G ´omez, Tonal Description of Music Audio Signals ,\nPh.D. thesis, UPF, Barcelona, Spain, 2006.\n[5] S. Dixon, “Multiphonic note identiﬁcation,” in Aus-\ntralian Computer Science Communications, 18, 1,\nProc. of the 19th Australasian Comp. Sc. Conf. , 1996,\npp. 318–323.\n[6] E. Batschelet, Circular Statistics in Biology , Mathe-\nmatics in Biology. Academic Press, 1981.\n[7] K. Dressler, “Sinusoidal extraction using an efﬁcient\nimplementation of a multi-resolution FFT,” in Proc.\nof DAFx-06 , 2006, pp. 247–252.\n[8] K. Dressler, “An auditory streaming approach on\nmelody extraction,” in 2nd Music Information Re-\ntrieval Evaluation eXchange (MIREX) , 2006"
    },
    {
        "title": "Towards Query by Singing/Humming on Audio Databases.",
        "author": [
            "Alexander Duda",
            "Andreas Nürnberger",
            "Sebastian Stober"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414804",
        "url": "https://doi.org/10.5281/zenodo.1414804",
        "ee": "https://zenodo.org/records/1414804/files/DudaNS07.pdf",
        "abstract": "Current work on Query-by-Singing/Humming (QBSH) focusses mainly on databases that contain MIDI files. Here, we present an approach that works on real audio recordings that bring up additional challenges. To tackle the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular “karaoke effect” exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process. A series of experiments are conducted to assess the discrimination and robustness of this representation. They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement. 1 INTRODUCTION Query-by-Singing/Humming (QBSH) as introduced in [6] is a popular content-based music retrieval method where the user enters a search query by singing, humming, or whistling it into a microphone. So far, work on QBSH systems has mainly focused on databases containing pieces of music in a symbolic description, usually MIDI. We describe an approach for QBSH on audio recordings instead. From these recordings, descriptive features are extracted and aggregated in symbolic strings which allow using distance measures for string pattern matching. However, music in general consists of several instruments or voices playing harmonically or in opposition to each other at the same time. In MIDI, each instrument usually has its own track, allowing straightforward separation of the individual voices. In real audio recordings, however, audio information of all instruments and voices is mixed and stored in all channels. Nevertheless, users of a QBSH system usually want to query the melody sung by the lead voice or played by a solo instrument. Consequently, the recordings need to be reduced to a somewhat more precise representation of components related to melody or lyrics. Before we introduce our methodology in Section 3, we c⃝2007 Austrian Computer Society (OCG). briefly discuss related work. Section 4 describes the experiments conducted to evaluate our approach. The results of the experiments are presented in Section 5. 2 RELATED WORK In [17] a retrieval method for audio is proposed that restricts the frequency range to 22 semitones. Furthermore, the songs need to be manually segmented into semantically meaningful phrases. The authors argue that usually a query starts at the beginning of such a phrase and thus confine query comparison to the beginning of phrases without further local alignment. We make a similar assumption to improve performance but matching is not restricted to that case. In [14] an automated way for segmentation is presented using a normal CD recording as well as a karaoke track of the same song, which is usually not available and thus, we avoid using such additional information. Instead, we try to infer higher-level representations directly. Current techniques of melody extraction from polyphonic recordings as [3, 5, 13] are still vulnerable to interferences from instruments. Here, source-separation approaches such as [4, 15] could help but still have many limitations: The method proposed in [15] works well on all single-note harmonic instruments including voice but has problems when drums are present or multiple instruments play the same note or an octave. The approach presented in [4] only works for vocals and up to three voices. In contrast to these approaches, we do not aim to achieve a perfect separation into individual voices. Instead, we are mainly interested in characteristics that are reproducable by a human singer. In this work we focus on the lead vocals or solo instruments. 3 METHODOLOGY",
        "zenodo_id": 1414804,
        "dblp_key": "conf/ismir/DudaNS07",
        "keywords": [
            "Query-by-Singing/Humming",
            "MIDI files",
            "karaoke effect",
            "stereo mix",
            "melody extraction",
            "distance measures",
            "symbolic strings",
            "string pattern matching",
            "experiments",
            "results"
        ],
        "content": "TOWARDS QUERY BYSINGING/HUMMING ONAUDIO DATABASES\nAlexanderDuda,AndreasN ¨urnbergerandSebastian Stober\nFacultyofComputer Science\nOtto-von-Guericke-University Magdeburg, Germany\n{nuernb,stober }@iws.cs.uni-magdeburg.de\nABSTRACT\nCurrentworkonQuery-by-Singing/Humming(QBSH)fo-\ncussesmainlyondatabases thatcontainMIDIﬁles. Here,\nwe present an approach that works on real audio record-\nings that bring up additional challenges. To tackle the\nproblem of extracting the melody of the lead vocals from\nrecordings,weintroduceamethodinspiredbythepopular\n“karaoke effect” exploiting information about the spatial\narrangement of voices and instruments in the stereo mix.\nThe extracted signal time series are aggregated into sym-\nbolic strings preserving the local approximated values of\nafeatureand revealing higher-level context patterns. Thi s\nallowsdistancemeasuresforstringpatternmatchingtobe\napplied in the matching process. A series of experiments\nare conducted to assess the discrimination and robustness\nof this representation. They show that the proposed ap-\nproachprovidesaviablebaselineforfurtherdevelopment\nand point out several possibilitiesforimprovement.\n1 INTRODUCTION\nQuery-by-Singing/Humming(QBSH)asintroducedin[6]\nis a popular content-based music retrieval method where\nthe user enters a search query by singing, humming, or\nwhistlingitintoamicrophone. Sofar,workonQBSHsys-\ntems has mainly focused on databases containing pieces\nofmusicinasymbolicdescription,usuallyMIDI.Wede-\nscribeanapproachforQBSHonaudiorecordingsinstead.\nFrom these recordings, descriptive features are extracted\nandaggregatedinsymbolicstringswhichallowusingdis-\ntancemeasuresforstringpatternmatching. However,mu-\nsic in general consists of several instruments or voices\nplaying harmonically or in opposition to each other at the\nsame time. In MIDI, each instrument usually has its own\ntrack, allowing straightforward separation of the individ -\nualvoices. Inrealaudiorecordings,however,audioinfor-\nmation of all instruments and voices is mixed and stored\nin all channels. Nevertheless, users of a QBSH system\nusually want to query the melody sung by the lead voice\nor played by a solo instrument. Consequently, the record-\nings need to be reduced to a somewhat more precise rep-\nresentation of components relatedtomelody or lyrics.\nBeforeweintroduceourmethodologyinSection3,we\nc/circlecopyrt2007 AustrianComputer Society (OCG).brieﬂy discuss related work. Section 4 describes the ex-\nperimentsconductedtoevaluateourapproach. Theresults\nof theexperiments arepresented inSection 5.\n2 RELATED WORK\nIn [17] a retrieval method for audio is proposed that re-\nstricts the frequency range to 22 semitones. Furthermore,\nthe songs need to be manually segmented into semanti-\ncallymeaningfulphrases. Theauthorsarguethatusuallya\nquerystartsatthebeginningofsuchaphraseandthuscon-\nﬁnequerycomparisontothebeginningofphraseswithout\nfurther local alignment. We make a similar assumption to\nimproveperformancebutmatchingisnotrestrictedtothat\ncase. In [14] an automated way for segmentation is pre-\nsented using a normal CD recording as well as a karaoke\ntrack of the same song, which is usually not available and\nthus,weavoidusingsuchadditionalinformation. Instead,\nwetrytoinferhigher-level representations directly.\nCurrenttechniquesofmelodyextractionfrompolypho-\nnic recordings as [3, 5, 13] are still vulnerable to inter-\nferences from instruments. Here, source-separation ap-\nproaches such as [4, 15] could help but still have many\nlimitations: The method proposed in [15] works well on\nall single-note harmonic instruments including voice but\nhas problems when drums are present or multiple instru-\nmentsplaythesamenoteoranoctave. Theapproachpre-\nsentedin[4]onlyworksforvocalsanduptothreevoices.\nIn contrast to these approaches, we do not aim to achieve\naperfectseparationintoindividualvoices. Instead,wear e\nmainly interested in characteristics that are reproducabl e\nby a human singer. In this work we focus on the lead vo-\ncals or soloinstruments.\n3 METHODOLOGY\n3.1 Voice Separation\nWeapplyatwo-stepﬁlteringtoreducetheimpactofback-\ning instruments and voices in the audio recordings. First,\naband-path-ﬁlter from 300Hz to 3000Hz is used to re-\nmove some instrument components, while keeping most\nof the lead voice. The second step exploits the spatial ar-\nrangementofinstrumentsandvoicesinthestereomixand\ncouldbedescribedas inversekaraoke effect. Itisinspired\nbythecenterpanremoval techniqueusedbymostkaraoke\nmachinestoremovetheleadvoicefromatypicalrock/popsong: One stereo channel is inverted and mixed with the\nother one into a mono signal. The lead voice and solo in-\nstruments are usually centered in the stereo mix whereas\nmost instruments and backing vocals are out of center.\nApplying the above mentioned transformation drastically\nreduces the power level of the centered signals and thus\ncan be used toremove the leadvoice.\nNow the idea is to invert this effect, so that the pre-\nprocessed version of a song yields a high portion of the\nlead voice, while most other instruments are ﬁltered out.\nUnfortunately, there is no simple way to keep the cen-\nter pan. Inverting the karaoke result and mixing it with a\nmono version of the original will not work. The Audacity\naudioeditor1thatweusedforthepre-processingprovides\nafunctioncalled noiseproﬁle . Itderivesapowerspectrum\nof frequencies from a noise track deﬁning a noise signal\nwhichcanthenberemovedfromanyothertrack. Thisﬁl-\nter works well for removing monotonic noise, e.g. white\nnoise or growling. However, deﬁning all the background\nasnoise,thenoiseproﬁlebecomesratherimpreciseresult-\ning in removal of foreground parts which yields warbling\nartefacts. To reduce this effect, a local noise proﬁle is\ndeﬁned as the noise proﬁle of a narrow 2s time window,\nwhich ismoved along the trackwith1s overlap.\n3.2 Feature Selection and Extraction\nInthis work, we concentrate on thefollowing small set of\nmanually selected features (as well as their 1stand2nd\nderivatives) that seem to be promising in terms of dis-\ncrimination and robustness: audio power (AP) [8],au-\ndio fundamental frequency (AFF) [8],chroma[12],mel\nfrequencycepstralcoefﬁcients(MFCC) [11],and formant\nfrequencies (FF) [2]. For these features we empirically\ndeterminedaframesizeandhopsize(i.ethespanbetween\nthe starting times of two succeeding frames) of 30ms in\npreliminaryexperimentsasoptimalfortheextraction. Thi s\ntime span roughly corresponds to the length of a1\n32note\nassuming a common tempo of 70–120 beats-per-minute.\nItshouldbetheﬁnestresolutionforamelodytrackandis\na natural factor of the more likely note lengths,1\n4,1\n8and\n1\n16, allowing a natural aggregation factor in the following\naggregation stepand thus avoiding interpolation.\n3.3 Aggregation and Discretization\nThe extracted features are converted into a symbolic rep-\nresentation by reducing the number of possible values to\nreveal identiﬁable and repeating patterns. We extend the\nsymbolic aggregate approximation (SAX) approach [10]\nthat uses piecewise aggregate approximation (PAA) [16]\nfor decomposing the time series into ﬁxed length inter-\nvals.2The aggregated vector Cof length wfor a time\nseries Coflength niscomputed as [10]\n1http://audacity.sourceforge.net/\n2Werestrictourselvestoﬁxedintervallengthsaswedonotex pectan\nimprovementbyallowinga varyingandadaptiveinterval lengt h.ci=w\nnn\nwi/summationdisplay\nj=n\nw(i−1)+1cj,0< i≤n (1)\nand normalized with respect to mean and standard devi-\nation. SAX uses a lookup table for discretization that\ncontains a symbol for each quantile of the value distri-\nbutionofthefeaturetoguarantyanequallydistributedal-\nphabet. This lookup table is highly dependent on the dis-\ntributions type of the feature to discretize. The values of\naudio power, MFCC, fundamental and formant frequen-\ncies are exponentially distributed whereas the values of\nchroma bins are lognormal distributed. As the original\nSAX only allowed Gaussian distributions, we added fur-\nther lookup tables and the distribution type as additional\nparameter for the discretization process. Further, in SAX\nthe(edit)distancebetweensymbolsisdeﬁnedastheabso-\nlute difference between the centroids of the area they rep-\nresent, approximating the absolutedistance between the\nvaluestheyhaveaggregated. However,asa relative,qual-\nitybased distance was desiredin thecontext of thiswork,\nwedeﬁnedconsecutivesymbolstobeequidistantinstead.\nFollowing parameters have an impact in this step: the\naggregation factor, the alphabet size and the distribution\ntype. As mentioned in Section 3.2, the former is a natu-\nral number which results in a time span per symbol that\ncorrespondsroughlytothecommonnotelengths. Though\nthe PAA is capable of aggregating time series by rational\nfactornumbers,doingsowouldimplysomeinterpolation.\nThe feature values are mapped to an alphabet, assuming\nthere is an optimal alphabet size for each feature which\nhas to be determined experimentally. The alphabet size\naffects the resolution of the representation. Increasing t he\nalphabet size may veil patterns, whereas decreasing it re-\nduces its discriminatory power. Finally, from the distri-\nbution type the respective quantiles can be derived to be\nused in for the symbol lookup table. Here, the global dis-\ntributionofthefeaturevalues onallsongsofthedatabase\nor a local one referring to an indivual song can be taken\ninto account. In the latter case, the symbols of the alpha-\nbet for a speciﬁc feature would correspond to (slightly)\ndifferentvalueintervalsdependingontheparticularsong .\nThis could be a disadvantage but on the other hand it en-\nsuresthat thealphabet isoptimallyutilizedforeach song.\nBothoptions were testedinour experiments.\n3.4 High-Level Patterns\nFor demonstration purposes, we also tried to derive high-\nlevel patterns, describing generic shapes of a time series.\nAsthereisnoautomaticwayofidentifyingsuchpatterns,\nwe restrict ourselves to the audio power since some pat-\nterns can be deﬁned intuitively. A Gaussian de-noising\nﬁlter with length twelve is applied, while each value de-\nscribes a frame of 30ms. From observation, four types of\npatterns can be deﬁned: Flat patterns refer to sections of\nsilence or quiet background that have a low mean value\nandalowvariance,orliebetweentwoelevations. Smooth\nelevations have a mean signal above a certain thresholdwith only one peak, probably describing single syllables\norprimaryfeaturesthatlastforsomeframes. Toothystruc-\ntures, are elevations with a mean signal above a certain\nthreshold and more than one peak, that probably occur,\nwhentwoormoresmoothelevationsareveryclosetoeach\nother, so there is no ﬂat section between them. All other\nsections are classiﬁed as undeﬁned or noisy regions that\ncan result from quiet singing or ﬁltered out instruments.\nEach pattern is stored along with its length. The distance\noftwopatternsisagaindeterminedbyalookuptable,that\nhas been manually deﬁned, but could also be optimized\nby some machine learning technique for further improve-\nment.\n3.5 Distance Computation\nTo compute the distance of the query and a song, well\nestablished distance measures for string pattern matching\nwere used: edit distance (Levenshtein Distance) [9],con-\ntinuouseditdistance [7]andn-grams[1]. However,some\nconsiderations had to be made in order to assure pitch in-\nvariance, which is necessary because users will most of-\nten not sing or hum with the right pitch. Here, the only\naffected features are the fundamental frequency and the\nchroma. For the former this is not a problem because the\nsymbols of the transformed signal do not refer to speciﬁc\npitches anymore but to frequency bands derived from a\npitch distribution. For the latter the problem can be over-\ncomebyrotatingthechromabinsduringthedistancecom-\nputation which isanalogous toatransposition.\n4 EXPERIMENTS\nFrom a private collection, stereo recordings of 200 well-\nknown songs of pop, rock, beat and soul were selected to\nform the test database.3Six experiments have been con-\nducted on that database toassesstheproposed approach:\n(1)Queryingwith150 MIDIﬁles ,eachcontainingsole-\nlythe melody of asinglesong from thetest database.\n(2)Queryingwith150 humanizedMIDIﬁles ,obtained\nby alteringtempo, pitch and pauses of theﬁles from (1).\n(3) Querying with 150 hummed queries – each for a\ndifferent song –from anon-professional singer.\n(4)Queryingwith130 sungqueries from7non-profes-\nsional singers (multiplequeries forseveral songs).\n(5) Querying 10 songs with modiﬁed recording snip-\npetsas in(2)and anextra snippet from a liverecording .\n(6) Querying with the sung queries and additinal in-\nformation on whether the query refers to the beginning,\nthe chorus or “anything else” of asong forboosting.\nThequerieswere10slongandtranscribedasdescribed\nin Section 3. For each test scenario and parameter com-\nbination two performance measures on all queries were\ncomputed. We deﬁne the Mean of Accuracy (MoA) as:\nMoA =1\nnn/summationdisplay\ni=1/parenleftbiggn−rank(ti)\nn−1/parenrightbigg\n(2)\n3The extracted features can be made available. For a song list, see\nhttp://irgroup.cs.uni-magdeburg.de/mirMIDI Queriesground truth humanized\nMoA MRR MoA MRR\nsimple features\n1st MFCC (MFCC1) 0.5965 0.0338 0.5678 0.0289\n2nd-5th MFCC 0.6096 0.0735 0.5677 0.0252\nAudio Power (AP) 0.6262 0.0574 0.5522 0.0495\nFundamental Freq. 0.6098 0.0494 0.5678 0.0289\n1st Formant (FF1) 0.5398 0.0344 -- --\nChroma 0.6328 0.1242 0.6380 0.0618\n1st derivatives\ndAP 0.6237 0.0924 0.6189 0.0872\ndChroma 0.5490 0.0320 -- --\nhigh-level patterns \nHLP(AP) 0.5390 0.0350 -- --\nfeature combinations\n(AP, dAP) 0.6708 0.0764 0.6085 0.0826\n(AP, dAP, Chroma) 0.6979 0.1426 0.6439 0.0820\nHuman Querieshummed sung\nMoA MRR MoA MRR\nsimple features\n1st MFCC (MFCC1) 0.5867 0.0393 0.7325 0.1950\n2nd-5th MFCC 0.5361 0.0376 0.6325 0.1307\nAudio Power (AP) 0.6127 0.0549 0.6794 0.1558\nFundamental Freq. 0.5113 0.0362 0.5586 0.0585\n1st Formant (FF1) 0.5696 0.0251 0.6351 0.0821\nChroma 0.6095 0.0312 0.5879 0.1288\n1st derivatives\ndMFCC1 0.5574 0.0641 0.6062 0.1032\nhigh-level patterns \nHLP(AP) 0.5970 0.0588 0.6925 0.1454\nfeature combinations\n(MFCC1, dMFCC1) 0.5796 0.0456 0.7552 0.2164\n(MFCC1, FF1) -- -- 0.7447 0.2457\n(MFCC1, dMFCC1, FF1) -- -- 0.7635 0.2329\nTable 1. Selected results for experiments (1)–(4). Re-\nmarkable values are highlighted. In case of missing val-\nues, the feature or combination was not further examined\nbecause no improvement was expected.\nIt depicts the average rank at which the target was found\nforeachquerywithavalueof 0.5describingrandom, 0.57\nto0.67mediocre, and above 0.67good accuracy. The\nMeanReciprocalRank(MRR) asusedintheMIREX2006\nQBSH known item retrieval task4isdeﬁned as:\nMRR =1\nnn/summationdisplay\ni=1/parenleftbigg1\nrank(ti)/parenrightbigg\n(3)\nand gives a hint of how often the target reaches one of\nthe ﬁrst ranks. It is highly depending on the size of the\ndatabase. AgoodMRRon200songswouldbeabove 0.2.\n5 RESULTS\nGenerally, the continuous edit distance, an aggregation\nlevel of 4 and an alphabet size of 12 showed the best re-\nsults,exceptforchroma,withonly3symbolsperbin. Us-\ning a local feature distribution for each song to compute\nthe symbol lookup table yielded better results than using\naglobalone. Forthebestparametercombination,Table1\nshows the average performance values fortests(1)–(4).\nMIDI queries: Audio power and chroma were ade-\nquateworkingfeaturesforthesetests. Especially,chroma\ncompensatedthedifﬁcultiesintroducedintest(2). Acom-\nbinationwiththe 1stderivativeoftheaudiopowerledtoa\nremarkable performance gain, probably because of added\n4http://www.music-ir.org/mirex2006/context information. However, the higher-level patterns\nadding even more context failed but since they are de-\nsigned for real recordings, this result is no setback. Simi-\nlarly, the formant frequencies performed as poorly as ex-\npected, forthey arebound tovowels and consonants.\nHummed and sung queries: The1stMFCC proved\nto be the best standalone feature and performed for sung\nqueries even better than the audio power. This is not sur-\nprisingsincesingerstrytoreproducewhattheyhaveheard.\nCombiningitwithitsderivateandthe 1stformantyielded\nthebestresults. TheremainingMFCCsarenotsodiscrim-\ninative as they describe the shape of the spectrum which\ncontains more artefacts from instruments. Likewise, only\nthe1stformant frequency yields acceptable results. Es-\npeciallyremarkableistheperformanceofthehigher-level\npatterns that is equal or slightly better than the simple au-\ndiopowerfeatureandcanstillbeimprovedbyreﬁnedpat-\nterns (see also 3.4). On the other hand, the fundamental\nfrequency was only slightly better than random, possibly\nbecause of extraction errors.\nControl Condition: Transposition and levelling (rais-\ning the audio power for quieter while keeping the same\nlevel for louder sections) had no impact. For these and\nunmodiﬁed snippets the target song was always ranked\nhighest. Tempo changes by up to 10%had only a negli-\ngible impact of less than 1%. Only shortening sections of\nsilenceto200msand100mscausedadropoftheMoAto\n0.89and0.76respectively. For the live version the MoA\nwas0.79. Thus, the proposed approach shows robustness\nincase of reasonable distortionsof thequery.\nUsing Additional Information: Boosting beginnings\nby10%and each possible chorus by 5%increased the\nMoA to 0.79 and the MRR to 0.3. In 23.3%of all queries\nthe target song was ranked ﬁrst, for 30%it was amongst\nthe top 3 and for 41.1%in the top 10. This could be sig-\nniﬁcantlyimprovedbyusingmoresophisticatedmethods,\ne.g. forsegmentation or chorus detection.\n6 CONCLUSION\nIn this paper we presented an approach for QBSH on real\naudio recordings that showed some promising results in\nﬁrst experiments and still leaves much room for improve-\nmentssuchasﬁlteringoutdrumbeatsfromthecenterpan\n– as tracks with a high beat portion tended to be harder\nto ﬁnd – or using more robust and elaborate melody ex-\ntractionalgorithms[3]. Wedemonstratedthathigher-leve l\npatterns perform at least equally well in comparison to\nsimple feature values. A method for automatically learn-\ning the characteristic higher-level patterns from a set of\ntime series for a feature would allow to further apply and\ninvestigate higher-level patterns that might not only im-\nproveretrievalperformancebutalsosigniﬁcantlyspeedup\nprocessing as the signal information is even further com-\npressed. ThiswouldnotonlybeinterestingforQBSH,but\nfor the whole pattern matching domain. This way, QBSH\nonrealaudiodatahasahighchancetobecomeastandard\napplication inmany multimedia retrieval scenarios.References\n[1] J.-M. Batke, G. Eisenberg, P. Weishaupt, and\nT. Sikora. Evaluation of distance measures for\nMPEG-7melodycontours. In Int.WorkshoponMul-\ntimedia Signal Processing , 2004.\n[2] P.R.Cook. RealSoundSynthesisforInteractiveAp-\nplications . A. K. Peters, Ltd., 2002.\n[3] K. Dressler. Sinusoidal extraction using an efﬁcient\nimplementation of a multi-resolution FFT. In Proc.\noftheInt.Conf.onDigitalAudioEffects(DAFx’06) ,\n2006.\n[4] S. Dubnov, J. Tabrikian, and M. Arnon-Targan.\nSpeech source separation in convolutive envi-\nronments using space-time-frequency analysis.\nEURASIP J. on Applied Signal Processing , 2006.\n[5] D. Gerhard. Pitch track target deviation in natural\nsinging. In Proc. of ISMIR’05 , 2005.\n[6] A.Ghias,J.Logan,D.Chamberlin,andB.C.Smith.\nQuery by humming: Musical information retrieval\ninan audio database. In ACM Multimedia , 1995.\n[7] N. Jacobs, F. V. den Borre, L. Smeets, E. Schoofs,\nand H. Blockeel. A symbolic approach to music\nrecognition, 2003.\n[8] H.-G. Kim, N. Moreau, and T. Sikora. MPEG-7 Au-\ndio and Beyond: Audio Content Indexing and Re-\ntrieval. John Wiley& Sons, 2005.\n[9] V.I.Levenshtein.Binarycodescapableofcorrecting\nspurious insertions and deletions of ones. Problems\nof Information Transmission , 1:8–17, 1965.\n[10] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A sym-\nbolicrepresentationoftimeseries,withimplications\nfor streaming algorithms. In Proc. of the 8th ACM\nSIGMODworkshoponResearchissuesindatamin-\ning and knowledge discovery (DMKD’03) , 2003.\n[11] B. Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. In Proc. of ISMIR’00 , 2000.\n[12] M. M ¨uller, F. Kurth, and M. Clausen. Audio match-\ningviachroma-basedstatisticalfeatures. In Proc.of\nISMIR’05 , 2005.\n[13] R. P. Paiva. On the detection of melody notes in\npolyphonic audio. In Proc. of ISMIR’05 , 2005.\n[14] W.-H. Tsai, H.-M. Yu, and H.-M. Wang. Query-by-\nexample technique for retrieving cover versions of\npopular songs with similar melodies. In Proc. of IS-\nMIR’05, 2005.\n[15] J. Woodruff and B. Pardo. Using pitch, ampli-\ntude modulation, and spatial cues for separation of\nharmonic instrumentsfromstereomusicrecordings.\nEURASIP J. on Adv. inSignal Processing , 2007.\n[16] B.-K.YiandC.Faloutsos. Fasttimesequenceindex-\ning for arbitrary lp norms. In Proc. of the 26th Int.\nConf. on Very Large Data Bases (VLDB’00) , 2000.\n[17] H.-M.Yu,W.-H.Tsai,andH.-M.Wang.Aquery-by-\nsinging technique for retrieving polyphonic objects\nof popular music. In Proc. of the 2nd Asia Informa-\ntion Retrieval Symposium (AIRS’05) , 2005."
    },
    {
        "title": "Autotagging Music Using Supervised Machine Learning.",
        "author": [
            "Douglas Eck",
            "Thierry Bertin-Mahieux",
            "Paul Lamere"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417869",
        "url": "https://doi.org/10.5281/zenodo.1417869",
        "ee": "https://zenodo.org/records/1417869/files/EckBL07.pdf",
        "abstract": "Social tags are an important component of “Web2.0” music recommendation websites. In this paper we propose a method for predicting social tags using audio features and supervised learning. These automatically-generated tags (or “autotags”) can furnish information about music that is untagged or poorly tagged. The tags can also serve to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. 1 INTRODUCTION In this paper we investigate the automatic generation of tags with properties similar to those generated by social taggers. Specifically we introduce a machine learning algorithm that takes as input acoustic features and predicts social tags mined from the web (in our case, Last.fm). The model can then be used to tag new or otherwise untagged music, thus providing a (partial) solution to the cold-start problem. We believe these autotags might also serve to dampen feedback loops which occur when certain songs in a social recommender become over-popular and thus over-tagged. Recently, there has been increasing interest in social tagging including the social tagging of music. Music tagging sites such as QLoud (www.qloud.com) and Last.fm (www.last.fm) and allow music listeners to apply free-text labels (tags) to songs, albums or artists. The real strength of a tagging system is seen when the tags of many users are aggregated. When the tags created by thousands of different listeners are combined, a rich and complex view of the song or artist emerges. Table 1 show the top 20 tags and frequencies of tags applied to the band “The Shins.” From these tags and their frequencies we learn much more about “The Shins” than we would from a traditional single genre assignment of “Indie Rock”. Additionally, in previous work [3] it was shown that social tags (in this case from the freedb CD track listing service at www.freedb.org) can predict canonical music-industry genre with good accuracy. Thus we lose little and gain a lot by moving from genres to tags. c⃝2007 Austrian Computer Society (OCG). Tag Freq Tag Freq Indie 2375 Mellow 85 Indie rock 1138 Folk 85 Indie pop 841 Alternative rock 83 Alternative 653 Acoustic 54 Rock 512 Punk 49 Seen Live 298 Chill 45 Pop 231 Singer-songwriter 41 The Shins 190 Garden State 39 Favorites 138 Favorite 37 Emo 113 Electronic 36 Table 1. Top 20 tags applied to The Shins 2 AN AUTOTAGGING ALGORITHM We now describe a machine learning model which uses the meta-learning algorithm AdaBoost [4] to predict tags from acoustic features. This model is an extension of a previous model [2] which performed well at predicting music attributes from acoustic features: at MIREX 2005 (ISMIR conference, London, 2005) the model won the Genre Prediction Contest and was the 2nd place performer in the Artist Identification Contest. The model has two principal advantages. First it performs automatic feature selection based on a feature’s ability to minimize empirical error. Thus we can use the model to eliminate useless feature sets. Second, it’s performance is linear in the number of inputs. Thus it has the potential to scale well to large datasets. Both of these properties are general to AdaBoost and are not explored further in this short paper. See [4] for more. Acoustic feature extraction: We obtained MP3s from a subset of the tagged artists described above. From these MP3s we extracted several popular acoustic features. Due to space limitations, we do not cover feature extraction in depth here. Please see [2] for details. The features used included 20 Mel-Frequency Cepstral Coefficients, 176 autocorrelation coefficients computed for lags spanning from 250msec to 2000msec at 10ms intervals, and 85 spectrogram coefficients sampled by constant-Q (or log-scaled) frequency. We also tried 12 chromagram coefficients but discarded them because they contributed very little to the final result. For those not familiar with these standard acoustic features, please see [5]. The features were extracted with high temporal precision to preserve spectral and timbral information. Following the strategy of [2] coarser “aggregate” features were generated by taking means and standard deviations of high-temporal precision features over longer timescales, here 5 sec. Tagging as a classification problem: Intuitively, automatic labeling would be a regression task where a learner would try to predict tag frequencies for artists or songs. However, because tags are sparse (many artist are not tagged at all) this proves to be too difficult using our current Last.fm / Audioscrobbler dataset. Instead we chose to treat the task as a classification one. Specifically, for each tag we try to predict if a particular artist has “none”, “some” or “a lot” of a particular tag relative to other tags. We label training examples as being in one of these three classes based on the relative number of times that tag has been applied. Tag prediction with AdaBoost: Using MultiBoost.MH a booster is trained to predict the tag (“none”, “some”, “a lot”) directly from the aggregate feature values. The value for a song is taken by voting over the predictions for each aggregate feature. Voting can take place in two ways: we can choose segment winners and then select as global winner the class receiving the most segment votes or we can sum the weak learner values over segments and then take the class with the maximum sum. 3 EXPERIMENTS To test our model we extracted tags and tag frequencies for more than 50, 000 artists from the social music website Last.fm using the Audioscrobbler web service [1]. From the full set of tags we selected 13 tags corresponding to popular genres. We selected these particular tags to be relatively easy to analyze (i.e. it’s not clear how to analyze the performance of a predictor of “fun” or “mellow”). Results: We compare our results against a baseline computed using the one-versus-all boosted model from [2]. Unlike our current approach, this model assumes that one and only one tag can be applied to a single song. In order to train and test the model we needed to select a winner. We simply chose the most frequent tag. Mean classification error rate over all classes except Classical was 42% (std=2.22) error by segment and 39% (std=2.32) by song. The classical was not counted because only two significant classes “some/all” and “none” could be generated using available data, yielding error of 13%. These results compare favorably to the baseline one-versus-all results of 62% error by segment and 59% error by song. As an example of model performance see Table 2 where we compare the nearest neighbors for our predicted tags to those for the original tags. In general it seems that our predicted tags are comparable in quality to the original tags. That is, our tags have some surprising errors (Marvin Gaye as a near neighbor to the Beatles?) yet so do the original tags (John Williams as a near neighbor to Mozart?). Overall, these preliminary results suggest that autotagging helps solve the cold start problem seen in social-tag-based music recommenders. Near-neighbor artists Seed Artist Last.fm Tags Our Prediction The Prodigy Chemical Brothers Fatboy Slim Basement Jaxx Apollo 440 Apollo 440 Beck John Lennon Eric Clapton The Beatles The Beach Boys Marvin Gaye The Doors The Rolling Stones Bach Schubert Mozart Beethoven Haydn John Williams Brahms Table 2. A comparison of 3 nearest neighbors for Last.fm tags versus our model predictions. Euclidean distance was used. 4 CONCLUSIONS With these preliminary results conclude that a supervised learning approach to autotagging has merit. Our predictions are noisy and lead to sometimes-counterintuitive near artist predictions. However social tags themselves share these properties. There is much future work to do. One next step (already underway) is to learn a much larger number of tags and combine them using a second stage of learning for similarity prediction. A second step is to compare the performance of our boosted model to other approaches such as SVMs and neural networks. Most importantly, the machine-generated autotags need to be tested in a social recommender. It is only in such a context that we can explore whether autotags, when blended with real social tags, will in fact yield improved recommendations. 5 REFERENCES [1] Audioscrobbler. Web Services described at http://www.audioscrobbler.net/data/webservices/. [2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. K´egl. Aggregate features and AdaBoost for music classification. Machine Learning, 65(2-3):473–484, 2006. [3] J. Bergstra, A. Lacoste, and D. Eck. Predicting genre labels for artists using freedb. In Proceedings of the 7th International Conference on Music Information Retrieval (ISMIR 2006), 2006. [4] Y. Freund and R.E. Shapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156, 1996. [5] B. Gold and N. Morgan. Speech and Audio Signal Processing: Processing and Perception of Speech and Music. Wiley, Berkeley, California., 2000.",
        "zenodo_id": 1417869,
        "dblp_key": "conf/ismir/EckBL07",
        "content": "AUTOTAGGING MUSIC USING SUPERVISED MACHINE LEARNING\nDouglas Eck\nSun Labs\nSun Microsystems\nBurlington, Mass, USA\ndouglas.eck@umontreal.caThierry Bertin-Mahieux\nUniv. of Montreal\nDept. of Comp. Sci.\nMontreal, QC, Canada\nbertinmt@iro.umontreal.caPaul Lamere\nSun Labs\nSun Microsystems\nBurlington, Mass, USA\npaul.lamere@sun.com\nABSTRACT\nSocial tags are an important component of “Web2.0” mu-\nsic recommendation websites. In this paper we propose\na method for predicting social tags using audio features\nand supervised learning. These automatically-generated\ntags (or “autotags”) can furnish information about music\nthat is untagged or poorly tagged. The tags can also serve\nto smooth the tag space from which similarities and rec-\nommendations are made by providing a set of comparable\nbaseline tags for all tracks in a recommender system.\n1 INTRODUCTION\nIn this paper we investigate the automatic generation of\ntags with properties similar to those generated by social\ntaggers. Speciﬁcally we introduce a machine learning al-\ngorithm that takes as input acoustic features and predicts\nsocial tags mined from the web (in our case, Last.fm). The\nmodel can then be used to tag new or otherwise untagged\nmusic, thus providing a (partial) solution to the cold-star t\nproblem. We believe these autotags might also serve to\ndampen feedback loops which occur when certain songs\nin a social recommender become over-popular and thus\nover-tagged.\nRecently, there has been increasing interest in social\ntagging including the social tagging of music. Music tag-\nging sites such as QLoud (www.qloud.com) and Last.fm\n(www.last.fm) and allow music listeners to apply free-text\nlabels (tags) to songs, albums or artists.\nThe real strength of a tagging system is seen when the\ntags of many users are aggregated. When the tags cre-\nated by thousands of different listeners are combined, a\nrich and complex view of the song or artist emerges. Ta-\nble 1 show the top 20 tags and frequencies of tags applied\nto the band “The Shins.” From these tags and their fre-\nquencies we learn much more about “The Shins” than we\nwould from a traditional single genre assignment of “Indie\nRock”. Additionally, in previous work [3] it was shown\nthat social tags (in this case from the freedb CD track list-\ning service at www.freedb.org ) can predict canonical\nmusic-industry genre with good accuracy. Thus we lose\nlittle and gain a lot by moving from genres to tags.\nc/circlecopyrt2007 Austrian Computer Society (OCG).Tag Freq Tag Freq\nIndie 2375 Mellow 85\nIndie rock 1138 Folk 85\nIndie pop 841 Alternative rock 83\nAlternative 653 Acoustic 54\nRock 512 Punk 49\nSeen Live 298 Chill 45\nPop 231 Singer-songwriter 41\nThe Shins 190 Garden State 39\nFavorites 138 Favorite 37\nEmo 113 Electronic 36\nTable 1 . Top 20 tags applied to The Shins\n2 AN AUTOTAGGING ALGORITHM\nWe now describe a machine learning model which uses\nthemeta-learning algorithm AdaBoost [4] to predict tags\nfrom acoustic features. This model is an extension of a\nprevious model [2] which performed well at predicting\nmusic attributes from acoustic features: at MIREX 2005\n(ISMIR conference, London, 2005) the model won the\nGenre Prediction Contest and was the 2nd place performer\nin the Artist Identiﬁcation Contest. The model has two\nprincipal advantages. First it performs automatic feature\nselection based on a feature’s ability to minimize empir-\nical error. Thus we can use the model to eliminate use-\nless feature sets. Second, it’s performance is linear in the\nnumber of inputs. Thus it has the potential to scale well\nto large datasets. Both of these properties are general to\nAdaBoost and are not explored further in this short paper.\nSee [4] for more.\nAcoustic feature extraction: We obtained MP3s from\na subset of the tagged artists described above. From these\nMP3s we extracted several popular acoustic features. Due\nto space limitations, we do not cover feature extraction in\ndepth here. Please see [2] for details. The features used in-\ncluded 20 Mel-Frequency Cepstral Coefﬁcients, 176 auto-\ncorrelation coefﬁcients computed for lags spanning from\n250msec to 2000msec at 10ms intervals, and 85 spectro-\ngram coefﬁcients sampled by constant-Q (or log-scaled)\nfrequency. We also tried 12 chromagram coefﬁcients but\ndiscarded them because they contributed very little to the\nﬁnal result. For those not familiar with these standardacoustic features, please see [5]. The features were ex-\ntracted with high temporal precision to preserve spectral\nand timbral information. Following the strategy of [2]\ncoarser “aggregate” features were generated by taking mean s\nand standard deviations of high-temporal precision fea-\ntures over longer timescales, here 5 sec.\nTagging as a classiﬁcation problem: Intuitively, auto-\nmatic labeling would be a regression task where a learner\nwould try to predict tag frequencies for artists or songs.\nHowever, because tags are sparse (many artist are not tagged\nat all) this proves to be too difﬁcult using our current Last. fm\n/ Audioscrobbler dataset. Instead we chose to treat the\ntask as a classiﬁcation one. Speciﬁcally, for each tag we\ntry to predict if a particular artist has “none”, “some” or\n“a lot” of a particular tag relative to other tags. We label\ntraining examples as being in one of these three classes\nbased on the relative number of times that tag has been\napplied.\nTag prediction with AdaBoost: Using MultiBoost.MH\na booster is trained to predict the tag (“none”, “some”, “a\nlot”) directly from the aggregate feature values. The value\nfor a song is taken by voting over the predictions for each\naggregate feature. V oting can take place in two ways: we\ncan choose segment winners and then select as global win-\nner the class receiving the most segment votes or we can\nsum the weak learner values over segments and then take\nthe class with the maximum sum.\n3 EXPERIMENTS\nTo test our model we extracted tags and tag frequencies\nfor more than 50,000artists from the social music website\nLast.fm using the Audioscrobbler web service [1]. From\nthe full set of tags we selected 13 tags corresponding to\npopular genres. We selected these particular tags to be\nrelatively easy to analyze (i.e. it’s not clear how to analyz e\nthe performance of a predictor of “fun” or “mellow”).\nResults: We compare our results against a baseline\ncomputed using the one-versus-all boosted model from\n[2]. Unlike our current approach, this model assumes that\none and only one tag can be applied to a single song. In\norder to train and test the model we needed to select a win-\nner. We simply chose the most frequent tag. Mean clas-\nsiﬁcation error rate over all classes except Classical was\n42% (std=2.22) error by segment and 39% (std=2.32) by\nsong. The classical was not counted because only two sig-\nniﬁcant classes “some/all” and “none” could be generated\nusing available data, yielding error of 13%. These results\ncompare favorably to the baseline one-versus-all results o f\n62% error by segment and 59% error by song.\nAs an example of model performance see Table 2 where\nwe compare the nearest neighbors for our predicted tags\nto those for the original tags. In general it seems that our\npredicted tags are comparable in quality to the original\ntags. That is, our tags have some surprising errors (Marvin\nGaye as a near neighbor to the Beatles?) yet so do the orig-\ninal tags (John Williams as a near neighbor to Mozart?).\nOverall, these preliminary results suggest that autotaggi nghelps solve the cold start problem seen in social-tag-based\nmusic recommenders.\nNear-neighbor artists\nSeed Artist Last.fm Tags Our Prediction\nThe Prodigy Chemical Brothers\nFatboy Slim Basement Jaxx Apollo 440\nApollo 440 Beck\nJohn Lennon Eric Clapton\nThe Beatles The Beach Boys Marvin Gaye\nThe Doors The Rolling Stones\nBach Schubert\nMozart Beethoven Haydn\nJohn Williams Brahms\nTable 2 .A comparison of 3 nearest neighbors for Last.fm tags\nversus our model predictions. Euclidean distance was used.\n4 CONCLUSIONS\nWith these preliminary results conclude that a supervised\nlearning approach to autotagging has merit. Our predic-\ntions are noisy and lead to sometimes-counterintuitive nea r\nartist predictions. However social tags themselves share\nthese properties. There is much future work to do. One\nnext step (already underway) is to learn a much larger\nnumber of tags and combine them using a second stage\nof learning for similarity prediction. A second step is to\ncompare the performance of our boosted model to other\napproaches such as SVMs and neural networks. Most\nimportantly, the machine-generated autotags need to be\ntested in a social recommender. It is only in such a con-\ntext that we can explore whether autotags, when blended\nwith real social tags, will in fact yield improved recom-\nmendations.\n5 REFERENCES\n[1] Audioscrobbler. Web Services described at\nhttp://www.audioscrobbler.net/data/webservices/.\n[2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. K ´egl. Aggregate features and AdaBoost for music\nclassiﬁcation. Machine Learning , 65(2-3):473–484,\n2006.\n[3] J. Bergstra, A. Lacoste, and D. Eck. Predicting genre\nlabels for artists using freedb. In Proceedings of the\n7th International Conference on Music Information\nRetrieval (ISMIR 2006) , 2006.\n[4] Y . Freund and R.E. Shapire. Experiments with a new\nboosting algorithm. In Machine Learning: Proceed-\nings of the Thirteenth International Conference , pages\n148–156, 1996.\n[5] B. Gold and N. Morgan. Speech and Audio Signal\nProcessing: Processing and Perception of Speech and\nMusic . Wiley, Berkeley, California., 2000."
    },
    {
        "title": "The Music Information Retrieval Evaluation Exchange &quot;Do-It-Yourself&quot; Web Service.",
        "author": [
            "Andreas F. Ehmann",
            "J. Stephen Downie",
            "M. Cameron Jones"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417949",
        "url": "https://doi.org/10.5281/zenodo.1417949",
        "ee": "https://zenodo.org/records/1417949/files/EhmannDJ07.pdf",
        "abstract": "The Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherwise freely distributable. Since its inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and validation of submitted code. The goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evaluations of algorithms yearround, as opposed to annual exchanges.",
        "zenodo_id": 1417949,
        "dblp_key": "conf/ismir/EhmannDJ07",
        "keywords": [
            "Music Information Retrieval (MIR)",
            "Music Information Retrieval Evaluation eXchange (MIREX)",
            "Do-It-Yourself (DIY)",
            "Remote submission",
            "Execution and evaluation",
            "Standardized datasets",
            "Freely distributable",
            "Heavy interaction by IMIRSEL team members",
            "Yearround evaluations",
            "Submitters responsibilities"
        ],
        "content": "THE MUSIC INFORMATION RETRIEVAL EVALUATION \nEXCHANGE “DO-IT-YOURSELF” WEB SERVICE\nAndreas F. Ehmann J. Stephen Downie M. Cameron Jones \nInternational Music Information Retrieval Systems Evaluation Laboratory \nThe Graduate School of Library and Information Science \nUniversity of Illinois at Urbana-Champaign  \n{aehmann, jdownie, mjones2}@uiuc.edu \nABSTRACT \nThe Do-It-Yourself (DIY) web service of the Music \nInformation Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherw ise freely distributable. Since \nits inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and valid ation of submitted code. \nThe goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evalua tions of algorithms year-\nround, as opposed to annual exchanges. \n1. INTRODUCTION \nMIREX [1] represents a comm unity-based effort for the \nstandardization of datasets and metrics used in the evaluation of MIR algorithms. Due to the litigious nature of copyright protected materials, MIREX has adopted a paradigm whereby community-developed algorithms are submitted to a single, central location (IMIRSEL) where they are evaluated against common, \nnon-distributable datasets. In previous iterations of MIREX, the execution and evaluation of submitted code was handled by IMIRSEL team members. The goal of the MIREX DIY web service is  to allow researchers to submit, debug, execute, and evaluate their algorithms \nremotely via a web interface. \nThe MIREX DIY service is largely built upon the \nData-to-Knowledge web service (D2KWS) [3] and Music-to-Knowledge (M2K) [2] libraries. As with the majority of past MIREX evaluations, the necessary file input-output, algorithm execution, and algorithm evaluations are carried out  within D2K/M2K programs \nreferred to as itineraries. A typical itinerary used in MIREX evaluations can be seen in Figure 1. \nEach component of an itinerary is referred to as a \nmodule. In general, the itiner aries used for the execution \nand evaluation of algorithms are comprised of four types of modules. Input modules specify the locations of a dataset’s audio or MIDI files, and their corresponding ground truths. File reader modules are task specific modules for reading a task’s specified file format. External Integration modules are responsible for executing external code (namely the submitted algorithm). Finally, evaluation modules are task specific modules that measure the performance of an algorithm’s output compared to the pre-established ground truth. \nThe MIREX DIY web interface is largely responsible \nfor generating and populating the itineraries and their \nmodules with pertinent information, thus absolving the user of this burden. The D2KWS queues and distributes jobs to sandboxed and firewalled D2K servers for execution. Real-time status and debugging information is displayed to the user, and upon successful completion, evaluation results are made available\n. \n© 2007 Austrian Computer Society (OCG). \nFigure 1 . An itinerary used in MIREX evaluati ons, in this case, music key finding.    \n \n 2. MIREX DIY FRAMEWORK1 \nAn overview of the general MIREX DIY framework \ncan be seen in Figure 2.  \n \nFigure 2 . MIREX DIY framework overview.  \nUpon logging in, a user can either upload a new \nalgorithm, or select one of his or her previously uploaded algorithms for testing. Next, the user specifies essential information for the execution of the algorithm: \n(A) The user must specify a task that the algorithm \nis designed for (e.g., audio onset detection, audio key finding, etc.). This information is used to generate an itinerary with the task-specific file reader and evaluation modules. \n(B) The user selects from the available datasets \nappropriate for the task se lected in (A). For each \ntask, there also exists a small test/validation dataset so the user can verify that a new upload functions correctly. The dataset selection information is used to populate the evaluation itinerary’s input modules \nwith the local paths to a dataset’s audio or MIDI files and their ground truths.  \n(C) Information detailing the calling format and \nparameters is specified by the user, which subsequently populates the necessary fields in the \nM2K external code integration module. \nAfter specifying the above information, the \nalgorithm can be run. The job is queued by the D2KWS, and real-time information displaying the progress is shown to the user. Upon successful completion, the evaluation results are made available, and appropriate entries are made into a database. The user has the option to publish the results of the run. If the results are to be published, a web page containing the results of all published runs is updated. \n                                                          \n \n1 See http://music-ir.org/mirexdiy  to access a limited demo \nversion of the framework.  3. CHALLENGES \n3.1. Datasets \nThe ability to evaluate al gorithms year round raises \nsome concern over the possibility of over-fitting to specific datasets. It may be beneficial to maintain separate datasets that are only made available for once-per-year evaluations. However, facilities for constructing unique datasets ‘on the fly’ from IMIRSEL’s databases will be incorporated into the web service. This could prove useful for debugging purposes, where, for example, an algorithm fails on a single file. \n3.2. Security \nAlthough the D2KWS contains many safeguards, the \nsubmission of malicious code, whether intentional or not, must be guarded against. In addition, the protection of copyrighted content against theft must be addressed. \n3.3. Language and Platform Support \nSupporting multiple OS platforms and programming \nlanguages poses significant problems. In the earliest phases of the web service, it is likely that LINUX based MATLAB and Java submissions will be favored, as remotely compiling binaries or supporting precompiled binaries is difficult. \n4. FUTURE WORK \nExpanding platform and programming language support \nwill be top priority. The likely approach will be to set up a sandboxed machine with identical architecture to the DIY cluster for users to compile their algorithms. In addition, means to gather user feedback for improvement of the system should be implemented. Ways to distribute web service capabilities to other locations outside of IMIRSEL will be explored. \n5. ACKNOWLEDGMENTS \nThis project is funded by the National Science \nFoundation (IIS-0327371) and The Mellon Foundation. \n6. \nREFERENCES  \n[1] Downie, J. S. “The Music Information Retrieval \nEvaluation eXchange (MIREX),” D-Lib Magazine , \n12 (12), 2006. \n[2] Downie, J. S., Ehmann, A. F., Tcheng, D. K. \n“Music-to-Knowledge (M2K): A prototyping and evaluation environment for music information retrieval research,” SIGIR , 676, 2005. \n[3] Shirk, A. “D2K Web Service Design & \nImplementation,” presented at NCSA CyberArchitecture Working Group, Available at \nhttp://algdocs.ncsa.uiuc.edu/PR-20040828-1.ppt , 2004."
    },
    {
        "title": "Classifying Music Audio with Timbral and Chroma Features.",
        "author": [
            "Daniel P. W. Ellis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416906",
        "url": "https://doi.org/10.5281/zenodo.1416906",
        "ee": "https://zenodo.org/records/1416906/files/Ellis07.pdf",
        "abstract": "Music audio classification has most often been addressed by modeling the statistics of broad spectral features, which, by design, exclude pitch information and reflect mainly instrumentation. We investigate using instead beat-synchronous chroma features, designed to reflect melodic and harmonic content and be invariant to instrumentation. Chroma features are less informative for classes such as artist, but contain information that is almost entirely independent of the spectral features, and hence the two can be profitably combined: Using a simple Gaussian classifier on a 20-way pop music artist identification task, we achieve 54% accuracy with MFCCs, 30% with chroma vectors, and 57% by combining the two. All the data and Matlab code to obtain these results are available. 1 1 INTRODUCTION Classifying music audio (for instance by genre or artist) has been most successful when using coarse spectral features (e.g. Mel-Frequency Cepstral Coefficients or MFCCs [3]), which has been dubbed “timbral similarity” [1]. Such features reflect mainly the instruments and arrangements of the music rather than the melodic or harmonic content. Although a wide range of more musically-relevant features has been proposed, they have rarely afforded much improvement on the overall system performance. This paper describes the results of using beat-synchronous chroma features in place of frame-level MFCCs in statistical classification of artist identification. This feature representation was developed for matching “cover songs” (alternative performances of the same musical piece), and thus reflects harmonic and melodic content with the minimum of influence from instrumentation and tempo [2]. Chroma features consist of a twelve-element vector with each dimension representing the intensity associated with a particular semitone, regardless of octave; our implementation uses instantaneous frequency to improve frequency resolution and rejection of non-tonal energy. By storing just one chroma vector for each beat-length segment of the 1 http://labrosa.ee.columbia.edu/projects/ timbrechroma/ c⃝2007 Austrian Computer Society (OCG). original audio (as determined by a beat tracker), the representation is both compact and somewhat tempo invariant – yet still sufficient, since chroma content rarely changes within a single beat, provided the beats are chosen near the bottom of the metrical hierarchy. Using chroma features directly in place of MFCCs means that we are looking only at the global distribution of chroma use within each piece, and assuming this is somewhat consistent within classes. A chroma vector reflects all current notes and is thus roughly correlated with a particular chord (such as Bmin7). Thus, the kind of regularity that such a model might capture would be if a given artist was particularly fond of a certain subset of chords. The model in this form would not learn characteristic chord sequences, and would also be defeated by simple transposition. To overcome these problems we tried (a) modeling sequences of chroma vectors for several beats, and (b) using a key-normalization front-end that attempts to align every piece to a common chroma transposition. Although we believe that beat-chroma distribution models can capture something specific about individual composition style, we do not expect them to perform as well as timbral features, since artists are likely to vary their melodic-harmonic palette more readily than their instrumentation. However, we expect the information from these two sources to be complementary, since harmonic “signatures”, to the extent they exist, have no reason to be associated with instrumentation. For this reason, we experiment with fusing the two results in final classification. 2 CLASSIFIER We adopt an artist identification task to illustrate the utility of beat-chroma features. We opt for the simple approach of fitting the distributions of random subsets of pooled frame-level features for each artist with either a single full-covariance Gaussian, or a mixture of diagonalcovariance Gaussians. Classification of an unknown track is achieved by finding the model that gives the best total likelihood for a random subset of the features from that track (treated as independent). Although previous work has shown that the SVM classifier applied to track-level features is a more powerful approach to this problem [3], our interest here is in comparing the usefulness of the different features. The simple and quick Gaussian models are adequate for this purpose. 3 KEY NORMALIZATION The goal of key normalization was to find a per-track rotation of the circular, 12-bin chroma representation that made all the data within each training class as similar as possible. To do this we first fit a single, full-covariance Gaussian to the chroma representation of first track in the class. Then the likelihood of each subsequent track was evaluated using this model under each of the 12 possible chroma rotations; the most likely rotation was retained. After one pass through the entire set, a new Gaussian was fit to all the tracks after applying the best rotations from the first pass. The search for the best rotations was repeated based on this new model, and the process iterated until no changes in rotations occurred. The final, global model was also used on test tracks to choose the best rotation for them prior to classification. 4 DATA We collected and used a set of 1412 tracks, composed of six albums from each of 20 artists. It is based on the 18 artist set used on [3] (drawn from “uspop2002”) with some additions and enhancements. We used 6-fold testing, with five albums from each artist used for training and one for testing in each fold. For this test set, statistical significance at 5% requires a difference of just over 3% in classification accuracy (one-tailed binomial). Guessing the most common class gives a baseline accuracy of 6%. 5 EXPERIMENTS For both MFCC and beat-chroma features, we experimented with varying the number of frames used to train and test the models, and the number of Gaussians used to model the distributions. We used 20 MFCC coefficients including the zero’th, based on a 20-bin Mel spectrum extending to 8 kHz. For the beat-chroma features, we varied the number of temporally-adjacent frames modeled from 1 to 4, and with using key normalization. The results are summarized in table 1. We notice that MFCCs are intrinsically more useful than chroma features (as expected, since the instrumentation captured by MFCCs is well correlated with artist), that Gaussian mixture models are preferable for the chroma features (which are likely to be multimodal) but not for MFCCs, that key normalization gives a small but significant improvement, but concatenating multiple beats into a single feature vector does not. Fusing the best MFCC and Chroma systems (shown in bold in the table) based on a weighted sum of separate model likelihoods tuned on a small tuning subset, we see a substantial but statistically insignificant improvement that results from including chroma information. 6 CONCLUSIONS We have shown that the simple approach of modeling the distribution of per-beat chroma vectors very much as cepFeature Model T win Acc Exec time MFCC20 FullCov 1 54% 181 s MFCC20 64 GMM 1 54% 1282 s Chroma FullCov 1 15% 31 s Chroma FullCov 4 15% 50 s Chroma 64GMM 1 25% 581 s Chroma 64GMM 4 16% 1501 s ChromaKN FullCov 1 23% 113 s ChromaKN FullCov 4 23% 255 s ChromaKN 64GMM 1 30% 957 s ChromaKN 64GMM 4 23% 1717 s MFCC + Chroma fusion 57% Table 1. Artist identification accuracy. “T win” is the size of temporal context window. “FullCov” designates single, full-covariance Gaussian models. “ChromaKN” refers to per-track key-normalized chroma data. Execution times are per fold on a 2.8 GHz Xeon CPU. stral vectors have been modeled in the past is a viable approach for artist identification. Rotating the chroma vectors in order to transpose all pieces to a common tonal framework is necessary to realize the full benefits. Our future plans are to pursue the idea of modeling small fragments of beat-chroma representation to identify the most distinctive and discriminative fragments characteristic of each composer/artist. 7 ACKNOWLEDGEMENTS This work was supported by the Columbia Academic Quality Fund, and by the NSF under Grant No. IIS-0238301. Any opinions, findings, etc. are those of the authors and do not necessarily reflect the views of the NSF. 8 REFERENCES [1] Jean-Julien Aucouturier and Francois Pachet. Music similarity measures: What’s the use? In Proc. 3rd International Symposium on Music Information Retrieval ISMIR, Paris, 2002. [2] D. P. W. Ellis and G. Poliner. Identifying cover songs with chroma features and dynamic programming beat tracking. In Proc. ICASSP, pages IV–1429– 1432, Hawai’i, 2007. [3] M. I. Mandel and D. P. W. Ellis. Song-level features and support vector machines for music classification. In Proc. International Conference on Music Information Retrieval ISMIR, pages 594–599, London, Sep 2005.",
        "zenodo_id": 1416906,
        "dblp_key": "conf/ismir/Ellis07",
        "keywords": [
            "beat-synchronous chroma features",
            "melodic and harmonic content",
            "invariant to instrumentation",
            "artist identification task",
            "MFCCs",
            "chroma vectors",
            "20-way pop music",
            "Gaussian classifier",
            "combined features",
            "spectral features"
        ],
        "content": "CLASSIFYING MUSIC AUDIO\nWITH TIMBRAL AND CHROMA FEATURES\nDaniel P. W. Ellis\nLabROSA, Dept. Elec. Eng.\nColumbia University\nABSTRACT\nMusic audio classiﬁcation has most often been addressed\nby modeling the statistics of broad spectral features, which,\nby design, exclude pitch information and reﬂect mainly in-\nstrumentation. We investigate using instead beat-synchronous\nchroma features, designed to reﬂect melodic and harmonic\ncontent and be invariant to instrumentation. Chroma fea-\ntures are less informative for classes such as artist, but\ncontain information that is almost entirely independent of\nthe spectral features, and hence the two can be proﬁtably\ncombined: Using a simple Gaussian classiﬁer on a 20-way\npop music artist identiﬁcation task, we achieve 54% accu-\nracy with MFCCs, 30% with chroma vectors, and 57% by\ncombining the two. All the data and Matlab code to obtain\nthese results are available.1\n1 INTRODUCTION\nClassifying music audio (for instance by genre or artist)\nhas been most successful when using coarse spectral fea-\ntures (e.g. Mel-Frequency Cepstral Coefﬁcients or MFCCs\n[3]), which has been dubbed “timbral similarity” [1]. Such\nfeatures reﬂect mainly the instruments and arrangements\nof the music rather than the melodic or harmonic content.\nAlthough a wide range of more musically-relevant fea-\ntures has been proposed, they have rarely afforded much\nimprovement on the overall system performance.\nThis paper describes the results of using beat-synchronous\nchroma features in place of frame-level MFCCs in sta-\ntistical classiﬁcation of artist identiﬁcation. This feature\nrepresentation was developed for matching “cover songs”\n(alternative performances of the same musical piece), and\nthus reﬂects harmonic and melodic content with the min-\nimum of inﬂuence from instrumentation and tempo [2].\nChroma features consist of a twelve-element vector with\neach dimension representing the intensity associated with\na particular semitone, regardless of octave; our implemen-\ntation uses instantaneous frequency to improve frequency\nresolution and rejection of non-tonal energy. By storing\njust one chroma vector for each beat-length segment of the\n1http://labrosa.ee.columbia.edu/projects/\ntimbrechroma/\nc/circlecopyrt2007 Austrian Computer Society (OCG).original audio (as determined by a beat tracker), the repre-\nsentation is both compact and somewhat tempo invariant\n– yet still sufﬁcient, since chroma content rarely changes\nwithin a single beat, provided the beats are chosen near\nthe bottom of the metrical hierarchy.\nUsing chroma features directly in place of MFCCs means\nthat we are looking only at the global distribution of chroma\nuse within each piece, and assuming this is somewhat con-\nsistent within classes. A chroma vector reﬂects all current\nnotes and is thus roughly correlated with a particular chord\n(such as Bmin7). Thus, the kind of regularity that such a\nmodel might capture would be if a given artist was par-\nticularly fond of a certain subset of chords. The model in\nthis form would not learn characteristic chord sequences,\nand would also be defeated by simple transposition.\nTo overcome these problems we tried (a) modeling se-\nquences of chroma vectors for several beats, and (b) using\na key-normalization front-end that attempts to align every\npiece to a common chroma transposition.\nAlthough we believe that beat-chroma distribution mod-\nels can capture something speciﬁc about individual com-\nposition style, we do not expect them to perform as well\nas timbral features, since artists are likely to vary their\nmelodic-harmonic palette more readily than their instru-\nmentation. However, we expect the information from these\ntwo sources to be complementary, since harmonic “signa-\ntures”, to the extent they exist, have no reason to be associ-\nated with instrumentation. For this reason, we experiment\nwith fusing the two results in ﬁnal classiﬁcation.\n2 CLASSIFIER\nWe adopt an artist identiﬁcation task to illustrate the util-\nity of beat-chroma features. We opt for the simple ap-\nproach of ﬁtting the distributions of random subsets of\npooled frame-level features for each artist with either a\nsingle full-covariance Gaussian, or a mixture of diagonal-\ncovariance Gaussians. Classiﬁcation of an unknown track\nis achieved by ﬁnding the model that gives the best total\nlikelihood for a random subset of the features from that\ntrack (treated as independent). Although previous work\nhas shown that the SVM classiﬁer applied to track-level\nfeatures is a more powerful approach to this problem [3],\nour interest here is in comparing the usefulness of the dif-\nferent features. The simple and quick Gaussian models\nare adequate for this purpose.3 KEY NORMALIZATION\nThe goal of key normalization was to ﬁnd a per-track ro-\ntation of the circular, 12-bin chroma representation that\nmade all the data within each training class as similar as\npossible. To do this we ﬁrst ﬁt a single, full-covariance\nGaussian to the chroma representation of ﬁrst track in the\nclass. Then the likelihood of each subsequent track was\nevaluated using this model under each of the 12 possible\nchroma rotations; the most likely rotation was retained.\nAfter one pass through the entire set, a new Gaussian was\nﬁt to all the tracks after applying the best rotations from\nthe ﬁrst pass. The search for the best rotations was re-\npeated based on this new model, and the process iterated\nuntil no changes in rotations occurred. The ﬁnal, global\nmodel was also used on test tracks to choose the best ro-\ntation for them prior to classiﬁcation.\n4 DATA\nWe collected and used a set of 1412 tracks, composed\nof six albums from each of 20 artists. It is based on the\n18 artist set used on [3] (drawn from “uspop2002”) with\nsome additions and enhancements. We used 6-fold test-\ning, with ﬁve albums from each artist used for training\nand one for testing in each fold. For this test set, statistical\nsigniﬁcance at 5% requires a difference of just over 3%\nin classiﬁcation accuracy (one-tailed binomial). Guessing\nthe most common class gives a baseline accuracy of 6%.\n5 EXPERIMENTS\nFor both MFCC and beat-chroma features, we experimented\nwith varying the number of frames used to train and test\nthe models, and the number of Gaussians used to model\nthe distributions. We used 20 MFCC coefﬁcients includ-\ning the zero’th, based on a 20-bin Mel spectrum extend-\ning to 8 kHz. For the beat-chroma features, we varied the\nnumber of temporally-adjacent frames modeled from 1 to\n4, and with using key normalization.\nThe results are summarized in table 1. We notice that\nMFCCs are intrinsically more useful than chroma features\n(as expected, since the instrumentation captured by MFCCs\nis well correlated with artist), that Gaussian mixture mod-\nels are preferable for the chroma features (which are likely\nto be multimodal) but not for MFCCs, that key normaliza-\ntion gives a small but signiﬁcant improvement, but con-\ncatenating multiple beats into a single feature vector does\nnot. Fusing the best MFCC and Chroma systems (shown\nin bold in the table) based on a weighted sum of separate\nmodel likelihoods tuned on a small tuning subset, we see a\nsubstantial but statistically insigniﬁcant improvement that\nresults from including chroma information.\n6 CONCLUSIONS\nWe have shown that the simple approach of modeling the\ndistribution of per-beat chroma vectors very much as cep-Feature Model T win Acc Exec time\nMFCC20 FullCov 1 54% 181 s\nMFCC20 64 GMM 1 54% 1282 s\nChroma FullCov 1 15% 31 s\nChroma FullCov 4 15% 50 s\nChroma 64GMM 1 25% 581 s\nChroma 64GMM 4 16% 1501 s\nChromaKN FullCov 1 23% 113 s\nChromaKN FullCov 4 23% 255 s\nChromaKN 64GMM 1 30% 957 s\nChromaKN 64GMM 4 23% 1717 s\nMFCC + Chroma fusion 57%\nTable 1 . Artist identiﬁcation accuracy. “T win” is the size\nof temporal context window. “FullCov” designates single,\nfull-covariance Gaussian models. “ChromaKN” refers to\nper-track key-normalized chroma data. Execution times\nare per fold on a 2.8 GHz Xeon CPU.\nstral vectors have been modeled in the past is a viable ap-\nproach for artist identiﬁcation. Rotating the chroma vec-\ntors in order to transpose all pieces to a common tonal\nframework is necessary to realize the full beneﬁts.\nOur future plans are to pursue the idea of modeling\nsmall fragments of beat-chroma representation to identify\nthe most distinctive and discriminative fragments charac-\nteristic of each composer/artist.\n7 ACKNOWLEDGEMENTS\nThis work was supported by the Columbia Academic Qual-\nity Fund, and by the NSF under Grant No. IIS-0238301.\nAny opinions, ﬁndings, etc. are those of the authors and\ndo not necessarily reﬂect the views of the NSF.\n8 REFERENCES\n[1] Jean-Julien Aucouturier and Francois Pachet. Music\nsimilarity measures: What’s the use? In Proc. 3rd\nInternational Symposium on Music Information Re-\ntrieval ISMIR , Paris, 2002.\n[2] D. P. W. Ellis and G. Poliner. Identifying cover\nsongs with chroma features and dynamic program-\nming beat tracking. In Proc. ICASSP , pages IV–1429–\n1432, Hawai’i, 2007.\n[3] M. I. Mandel and D. P. W. Ellis. Song-level features\nand support vector machines for music classiﬁcation.\nInProc. International Conference on Music Informa-\ntion Retrieval ISMIR , pages 594–599, London, Sep\n2005."
    },
    {
        "title": "A Closer Look on Artist Filters for Musical Genre Classification.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415668",
        "url": "https://doi.org/10.5281/zenodo.1415668",
        "ee": "https://zenodo.org/records/1415668/files/Flexer07.pdf",
        "abstract": "Musical genre classification is the automatic classification of audio signals into user defined labels describing pieces of music. A problem inherent to genre classification experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to overoptimistic accuracy results but also selectively favours particular classification approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist filter. The same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files. 1 INTRODUCTION Music information retrieval (MIR) is the science of extracting information from music. Probably the most popular form of such information is musical genre (see [2] and [15] for comprehensive overviews). Genre information can be used to describe music in interpersonal communication, in publications about music as well as to structure music databases, libraries and music stores. Although musical genre is a somewhat poorly defined concept, automation of the genre classification process remains an important topic in MIR [9]. Besides being a goal in its own right, genre classification results are often used as a means to quantify success in modelling musical similarity. A problem inherent to genre classification experiments in MIR research is the use of songs from the same artist in both training and test sets. It can be argued that in such a scenario one is doing artist classification rather than genre classification. Specific mastering and production effects could also play a role in such a scenario. In [14] the use of a so-called “artist filter” ensuring that all songs from an artist are in either the training or the test set is proposed. The authors found that the use of such an artist filter can lower the classification results quite considerably (with one of their music collection even from 71% down to 27%). These over-optimistic accuracy results due to not c⃝2007 Austrian Computer Society (OCG). using an artist filter have been confirmed in other studies [12] [5]. In extending these results, we show that the failure to use an artist filter also selectively favours particular genre classification approaches. In two genre classification experiments we are able to show that: (i) the advantage of using models of songs rather than models of genres vanishes when applying an artist filter; (ii) the same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files. 2 DATA For our experiments we used a data set of the ISMIR 2004 genre classification contest. The data base consist of S = 729 songs from A = 128 artists belonging to G = 6 genres. The different genres plus the numbers of artists and songs belonging to each genre are given in Table 2. Genre No. artists No. songs % of songs Classical 40 320 43.9 Electronic 30 115 15.8 Jazz Blues 5 26",
        "zenodo_id": 1415668,
        "dblp_key": "conf/ismir/Flexer07",
        "keywords": [
            "genre classification",
            "audio signals",
            "music information retrieval",
            "songs from the same artist",
            "artist filter",
            "genre classification process",
            "music databases",
            "music stores",
            "music similarity",
            "genre information"
        ],
        "content": "A CLOSER LOOK ON ARTIST FILTERS\nFOR MUSICAL GENRE CLASSIFICATION\nArthur Flexer\nInstitute of Medical Cybernetics and Artiﬁcial Intelligenc e\nCenter for Brain Research, Medical University of Vienna, Austr ia\nFreyung 6/2, A-1010 Vienna, Austria\narthur.flexer@meduniwien.ac.at\nABSTRACT\nMusical genre classiﬁcation is the automatic classiﬁca-\ntion of audio signals into user deﬁned labels describing\npieces of music. A problem inherent to genre classiﬁca-\ntion experiments in music information retrieval research\nis the use of songs from the same artist in both training\nand test sets. We show that this does not only lead to over-\noptimistic accuracy results but also selectively favours p ar-\nticular classiﬁcation approaches. The advantage of using\nmodels of songs rather than models of genres vanishes\nwhen applying an artist ﬁlter. The same holds true for\nthe use of spectral features versus ﬂuctuation patterns for\npreprocessing of the audio ﬁles.\n1 INTRODUCTION\nMusic information retrieval (MIR) is the science of ex-\ntracting information from music. Probably the most pop-\nular form of such information is musical genre (see [2] and\n[15] for comprehensive overviews). Genre information\ncan be used to describe music in interpersonal commu-\nnication, in publications about music as well as to struc-\nture music databases, libraries and music stores. Although\nmusical genre is a somewhat poorly deﬁned concept, au-\ntomation of the genre classiﬁcation process remains an\nimportant topic in MIR [9]. Besides being a goal in its\nown right, genre classiﬁcation results are often used as a\nmeans to quantify success in modelling musical similarity.\nA problem inherent to genre classiﬁcation experiments\nin MIR research is the use of songs from the same artist in\nboth training and test sets. It can be argued that in such a\nscenario one is doing artist classiﬁcation rather than genr e\nclassiﬁcation. Speciﬁc mastering and production effects\ncould also play a role in such a scenario. In [14] the use\nof a so-called “artist ﬁlter” ensuring that all songs from\nan artist are in either the training or the test set is pro-\nposed. The authors found that the use of such an artist ﬁl-\nter can lower the classiﬁcation results quite considerably\n(with one of their music collection even from 71% down\nto 27%). These over-optimistic accuracy results due to not\nc/circlecopyrt2007 Austrian Computer Society (OCG).using an artist ﬁlter have been conﬁrmed in other studies\n[12] [5].\nIn extending these results, we show that the failure to\nuse an artist ﬁlter also selectively favours particular gen re\nclassiﬁcation approaches. In two genre classiﬁcation ex-\nperiments we are able to show that: (i) the advantage of\nusing models of songs rather than models of genres van-\nishes when applying an artist ﬁlter; (ii) the same holds true\nfor the use of spectral features versus ﬂuctuation patterns\nfor preprocessing of the audio ﬁles.\n2 DATA\nFor our experiments we used a data set of the ISMIR 2004\ngenre classiﬁcation contest. The data base consist of S=\n729songs from A= 128 artists belonging to G= 6gen-\nres. The different genres plus the numbers of artists and\nsongs belonging to each genre are given in Table 2.\nGenre No. artists No. songs %of songs\nClassical 40 320 43.9\nElectronic 30 115 15.8\nJazz Blues 5 26 3.6\nMetal Punk 8 45 6.2\nPop Rock 26 101 13.9\nWorld 19 122 16.7\nSum 128 729 100.0\nTable 1 . ISMIR 2004 contest data base (Genre, number\nof artists, number of songs, percentage of songs).\n3 METHODS\nWe performed two genre classiﬁcation experiments to show\nthe inﬂuence of the use of artist ﬁlters. The ﬁrst shows the\neffect of artist ﬁlters on the usage of models of songs ver-\nsus models of genres. The second shows the effect of artist\nﬁlters on the choice of features used for genre classiﬁca-\ntion.3.1 Experiment 1: one model per song versus one model\nper genre\nThe following approach based on spectral similarity is\nnow seen as one of the standard approaches in genre clas-\nsiﬁcation (see [8] and [1] for early references). For a given\nmusic collection of Ssongs, divided into Strain training\nandStesttest songs, each belonging to one of Gmusic\ngenres, it consists of the following basic steps:\nOne model per song (GMMsong)\n1. for each song, compute Mel Frequency Cepstrum\nCoefﬁcients (MFCCs) for short overlapping frames\n2. train a Gaussian Mixture Model (GMM) for each of\ntheStrain training songs\n3. compute an Strain×Stestdistance matrix using the\nlikelihood of each test song given all GMMs esti-\nmated on the training songs (Equ. 2, see below)\n4. based on the genre information, do nearest neigh-\nbour classiﬁcation for all test songs using the dis-\ntance matrix\nTo be more precise, step number four means that for\neach test song, we ﬁnd its closest neighbour amongst the\nStrain GMMs (where likelihood of test song is maximal)\nand assign the label of this nearest neighbour to the test\nsong. This is actually a version of an earlier approach [16]\nwhich used one GMM per genre and not per song1:\nOne model per genre (GMMgenre)\n1. for each song, compute Mel Frequency Cepstrum\nCoefﬁcients (MFCCs) for short overlapping frames\n2. train a Gaussian Mixture Model (GMM) for each of\nthegenres\n3. compute a G×Stestdistance matrix using the like-\nlihood of each test song given all GMMs estimated\non genres (Equ. 2, see below)\n4. based on the genre information, do nearest neigh-\nbour classiﬁcation for all test songs using the dis-\ntance matrix\nStep number four means that for each test song, we ﬁnd\nits closest neighbour amongst the GGMMs (where like-\nlihood of test song is maximal) and assign the respective\nlabel of this GMM to the test song.\nFor step number one, we divide the raw audio data into\noverlapping frames of short duration and use Mel Fre-\nquency Cepstrum Coefﬁcients (MFCC) to represent the\nspectrum of each frame. MFCCs are a perceptually mean-\ningful and spectrally smoothed representation of audio sig -\nnals. MFCCs are now a standard technique for computa-\ntion of spectral similarity in music analysis (see e.g. [7]) .\nThe frame size for computation of MFCCs for our ex-\nperiments was 23.2ms(512 samples), with a hop-size of\n11.6ms(256 samples) for the overlap of frames. We used\n1The authors used more than just MFCCs as features.the ﬁrst 8 MFCCs for all our experiments. The MA Tool-\nbox [11] was used for computation of the MFCCs.\nFor step number two, we use Gaussian Mixture Models\n(GMM) to model the density of the input data by a mixture\nmodel of the form\np(x) =M/summationdisplay\nm=1PmN[x,µm,Um] (1)\nwhere Pmis the mixture coefﬁcient for the m-th compo-\nnent,Nis the Normal density and µmandUmare the\nmean vector and covariance matrix of the m-th mixture.\nFor a data set Xicontaining Tdata points given a\nGMM trained on a song (GMMsong) or genre (GMM-\ngenre) j, the negative log-likelihood function is given by\nL(Xi|GMM j) =−1\nTT/summationdisplay\nt=1log(pj(xi\nt)) (2)\nFor learning a GMM for a song or genre i,L(Xi|GMM i)\nis minimised both with respect to the mixing coefﬁcients\nPmand with respect to the parameters of the Gaussian\nbasis functions using Expectation-Maximisation (see e.g.\n[4]). For all our experiments we used M= 10 compo-\nnents and diagonal covariances. For GMMsong, we used\nall MFCCs from the whole duration of a song for training\nof a GMM, as well as for evaluation of L(Xi|GMM j).\nFor GMMgenre, we used only a total of 5000 frames from\nall training songs belonging to a genre for training of a\nGMM, as well as for evaluation of L(Xi|GMM j). This\ncorresponds to only about one minute of music to repre-\nsent a genre. The share of frames taken from each song\nbelonging to a genre was taken randomly from the middle\nminute of the songs. The amount of frames and corre-\nsponding MFCCs taken from each song was equal irre-\nspective of the song’s total length.\n3.2 Experiment 2: Mel Frequency Cepstrum Coefﬁ-\ncients versus Fluctuation Patterns\nThis experiment compares the results obtained with the\nGMMsong approach described above to those obtained by\nsubstituting the MFCC features with Fluctuation Patterns:\nOne Fluctuation Pattern per song (FPsong)\n1. for each song, compute a Fluctuation Pattern (FP)\n2. compute an Strain×Stestdistance matrix using the\nEuclidean distance of each test song to all training\nsongs\n3. based on the genre information, do nearest neigh-\nbour classiﬁcation for all test songs using the dis-\ntance matrix\nFluctuation Patterns (FP) [10] [13] describe the ampli-\ntude modulation of the loudness per frequency band and\nare based on ideas developed in [6]. Closely following\nthe implementation outlined in [12], an FP is computed\nby: (i) cutting an MFCC spectrogram into three secondsegments, (ii) using an FFT to compute amplitude modu-\nlation frequencies of loudness (range 0−10Hz) for each\nsegment and frequency band, (iii) weighting the modula-\ntion frequencies based on a model of perceived ﬂuctuation\nstrength, (iv) applying ﬁlters to emphasise certain patter ns\nand smooth the result. The resulting FP is a 20 (frequency\nbands according to 20 critical bands of the Bark scale\n[17]) times 60 (modulation frequencies, ranging from 0\nto10Hz) matrix for each song. The distance between two\nFPsiandjis computed as the Euclidean distance:\nD(FPi,FPj) =20/summationdisplay\nk=160/summationdisplay\nl=1(FPi\nk,l−FPj\nk,l)2(3)\n4 RESULTS\nFor experiment 1, we computed two 10-fold cross-validation s\nto compare approaches GMMsong and GMMgenre: one\nwith and one without the use of an artist ﬁlter. During\ncross-validation without artist ﬁlter, assignment of song s\nto training and test sets was totally random irrespective\nof its association with an artist. During cross-validation\nwith artist ﬁlter it was ensured that all songs from an artist\nwere either in the training or test set. All other assign-\nments were again done randomly. Average accuracy rates\n(i.e. percentage correctly classiﬁed test songs) and stan-\ndard deviations are given in Table 4.\nMethod no AF with AF\nGMMsong 75.72±3.3558.50±10.29\nGMMgenre 69.00±3.3661.22±10.42\nTable 2 . Experiment 1: average accuracies ±standard de-\nviations for GMMsong and GMMgenre without and with\nartist ﬁlter (AF).\nThe difference in genre classiﬁcation accuracy between\nGMMsong and GMMgenre without artist ﬁlter is signif-\nicant:|t|=| −4.1650|> t(95,d f=9)= 2.26. GMM-\nsong outperforms GMMgenre by about seven percentage\npoints (76% versus 69%). The difference in genre classiﬁ-\ncation accuracy between GMMsong and GMMgenre with\nartist ﬁlter is however not signiﬁcant: |t|=|1.1523|<\nt(95,d f=9)= 2.26. GMMsong and GMMgenre perform at\nthe same level of around 60%. Whereas both approaches\ndecrease in performance due to the use of the artist ﬁlter,\nthis decrease is more severe for GMMsong.\nFor experiment 2, we compared the results from exper-\niment 1 obtained for GMMsong with those obtained with\nFPsong. We used the identical cross-validation folds from\nexperiment 1 to compare FPsong to GMMsong both with\nand without artist ﬁlter. Average accuracy rates (i.e. per-\ncentage correctly classiﬁed test songs) and standard devi-\nations are given in Table 4.\nThe difference in genre classiﬁcation accuracy between\nGMMsong and FPsong without artist ﬁlter is signiﬁcant:Method no AF with AF\nGMMsong 75.72±3.3558.50±10.29\nFPsong 63.10±2.3855.63±8.61\nTable 3 . Experiment 2: average accuracies ±standard\ndeviations for GMMsong and FPsong without and with\nartist ﬁlter (AF).\n|t|=|8.5779|> t(95,d f=9)= 2.26. GMMsong outper-\nforms FPsong by about thirteen percentage points (76%\nversus 63%). The difference in genre classiﬁcation accu-\nracy between GMMsong and FPsong with artist ﬁlter is\nhowever not signiﬁcant: |t|=|0.8514|< t(95,d f=9)=\n2.26. GMMsong and FPsong perform at the same level of\naround 55to58%. Whereas both approaches decrease in\nperformance due to the use of the artist ﬁlter, this decrease\nis more severe for GMMsong.\nLooking at Tables 4 and 4, it is noticeable that the stan-\ndard deviations of the accuracy results increase for all ex-\nperiments when using artist ﬁlters. Since some of the gen-\nres have fewer artists (e.g. ﬁve artists in “Jazz Blues” or\neight in “Metal Punk”) than the number of cross-validation\nfolds (ten), some of the test folds inevitably do not contain\nsongs from these genres. Since some of the genres are\nharder to classify than others, the fact that not all genres\nare present in all of the test folds introduces additional\nvariance in the results. Please note that this increased\nvariance does not change any of our results. Even with\nstandard deviations at the level of the “no artist ﬁlter”-\nresults, performance differences when using artist ﬁlters\nwould still not have been signiﬁcant, i.e. our conclusions\nwould not be different.\n5 DISCUSSION\nIn experiment 1 we examined the effect of an artist ﬁlter\non a popular and successful genre classiﬁcation approach:\nusing statistical models of MFCC representations of indi-\nvidual songs plus nearest neighbour classiﬁcation. In par-\nticular we compared it to building models of whole gen-\nres (GMMgenre) instead of individual songs (GMMsong).\nWhereas GMMsong is signiﬁcantly better than GMMgenre\nwithout the use of an artist ﬁlter, both approaches show\nreduced but similar performance with an artist ﬁlter em-\nployed. Our explanation is that comparing models of indi-\nvidual songs is prone to ﬁnding songs from the same artist\nduring nearest neighbour search. With GMMsong and no\nartist ﬁlter, this is the case in 48.84% of all test songs.\nInstead of actually learning the spectral characteristics of\na certain genre due to its preferred instrumentation and\nrespective sound, the peculiar style of individual artists\n(production effects, vocal characteristics, etc.) might b e\nmodelled.\nIn experiment 2 we examined the effect of an artist ﬁl-\nter on the choice of features used for preprocessing the\naudio ﬁles. In particular, Mel Frequency Cepstrum Coef-\nﬁcients (MFCCs) were compared to Fluctuation Patterns(FPs). Whereas MFCCs are a quite direct representation\nof the spectral information of a signal and therefore of the\nspeciﬁc “sound” or “timbre” of a song, FPs are a more\nabstract kind of feature describing the amplitude modu-\nlation of the loudness per frequency band. During near-\nest neighbour search of FPsong with no artist ﬁlter, songs\nfrom the same artist are found in only 24.69% of all test\nsongs (compared to 48.84% with GMMsong). This also\nexplains why classiﬁcation based on MFCCs (GMMsong)\ndegrades more than classiﬁcation based on FPs (FPsong)\nwhen employing an artist ﬁlter. The advantage of using\nMFCCs vanishes when modelling sound characteristics of\nindividual songs is no longer the main focus.\nAs with any empirical research, our results are limited\nto the data sets and algorithms used in the experiments.\nTherefore it remains an open question whether our results\ncan be replicated when other classiﬁcation algorithms are\nbeing employed, other parametrizations of the data or dif-\nferent, probably larger data sets are being used. Neverthe-\nless it is our belief that there is enough evidence to dis-\ncourage any further research on genre classiﬁcation with-\nout the use of artist ﬁlters since the results obtained with\nand without such ﬁlters might be quite different.\n6 CONCLUSION\nOur work is concerned with a major problem of musical\ngenre classiﬁcation experiments: the use of songs from\nthe same artist in both training and test sets. Our results\nsuggest that use of an artist ﬁlter not only lowers genre\nclassiﬁcation accuracy but may also erode the differences\nin accuracies between different techniques. As a conse-\nquence it seems advisable to reconsider all results on mu-\nsic classiﬁcation obtained without the use of artist ﬁlters .\n7 ACKNOWLEDGMENT\nParts of the MA Toolbox [11] and the Netlab Toolbox\n(http://www.ncrg.aston.ac.uk/netlab ) have\nbeen used for this work.\n8 REFERENCES\n[1] Aucouturier J.-J., Pachet F.: Music Similarity Mea-\nsures: What’s the Use?, in Proc. of the 3rd Int. Conf.\non Music Information Retrieval, pp. 157-163, 2002.\n[2] Aucouturier J.-J., Pachet F.: Representing Musical\nGenre: A State of the Art, Journal of New Music Re-\nsearch, V ol. 32, No. 1, pp.83-93, 2003.\n[3] Aucouturier, J.-J., Pachet F.: Improving Timbre Simi-\nlarity: How high is the sky?. Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n[4] Bishop C.M.: Neural Networks for Pattern Recogni-\ntion, Clarendon Press, Oxford, 1995.[5] Flexer A.: Statistical Evaluation of Music Informa-\ntion Retrieval Experiments, Journal of New Music Re-\nsearch, V ol. 35, No. 2, pp.113-120, 2006.\n[6] Fruehwirt M., Rauber A.: Self-Organizing Maps for\nContent-Based Music Clustering, Proceedings of the\nTwelth Italian Workshop on Neural Nets, IIAS, 2001.\n[7] Logan B.: Mel Frequency Cepstral Coefﬁcients for\nMusic Modeling, Proceedings of the International\nSymposium on Music Information Retrieval (IS-\nMIR’00), 2000.\n[8] Logan B., Salomon A.: A music similarity function\nbased on signal analysis, IEEE International Conf. on\nMultimedia and Expo, Tokyo, Japan, 2001.\n[9] McKay C., Fujinaga I.: Musical genre classiﬁcation:\nIs it worth pursuing and how can it be improved?,\nProceedings of the 7th International Conference on\nMusic Information Retrieval (ISMIR 2006), Victoria,\nCanada, October 8-12, 2006.\n[10] Pampalk E.: Islands of Music: Analysis, Organiza-\ntion, and Visualization of Music Archives, MSc The-\nsis, Technical University of Vienna, 2001.\n[11] Pampalk E.: A Matlab Toolbox to compute music\nsimilarity from audio, in Proceedings of the 5th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’04), Universitat Pompeu Fabra, Barcelona,\nSpain, pp.254-257,2004.\n[12] Pampalk E.: Computational Models of Music Sim-\nilarity and their Application to Music Information\nRetrieval, Vienna University of Technology, Austria,\nDoctoral Thesis, 2006.\n[13] Pampalk E., Rauber A., Merkl D.: Content-based or-\nganization and visualization of music archives, Pro-\nceedings of the 10th ACM International Conference\non Multimedia, Juan les Pins, France, pp. 570-579,\n2002.\n[14] Pampalk E., Flexer A., and Widmer G.: Im-\nprovements of Audio-Based Music Similarity and\nGenre Classiﬁcation , Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’05), London, UK, September 11-15, pp.628-\n633, 2005.\n[15] Scaringella N., Zoia G., Mlynek D.: Automatic genre\nclassiﬁcation of music content: a survey, IEEE Signal\nProcessing Magazine, V ol. 23, Issue 2, pp. 133-141,\n2006.\n[16] Tzanetakis G., Cook P.: Musical genre classiﬁcation\nof audio signals, IEEE Transactions on Speech and\nAudio Processing, V ol. 10, Issue 5, pp. 293-302, 2002.\n[17] Zwicker E., Fastl H.: Psychoaccoustics, Facts and\nModels, Springer Series of Information Sciences, V ol-\nume 22, 2nd edition, 1999."
    },
    {
        "title": "A Demonstration of the SyncPlayer System.",
        "author": [
            "Christian Fremerey",
            "Frank Kurth",
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415586",
        "url": "https://doi.org/10.5281/zenodo.1415586",
        "ee": "https://zenodo.org/records/1415586/files/FremereyKMC07.pdf",
        "abstract": "The SyncPlayer system is an advanced audio player for multimodal presentation, browsing, and retrieval of music data. The system has been extended significantly in the last few years. In this contribution, we describe the current state of the system and demonstrate the functionalities and interactions of the novel SyncPlayer components including combined interand intra-document music browsing. 1 SYNCPLAYER OVERVIEW The SyncPlayer is a client-server based software framework integrating various MIR-techniques such as music synchronization, content-based retrieval, and multimodal presentation of audio recordings and associated annotations [3]. The framework basically consists of three components: a server component, a client component, and a toolset for data administration (see also Figure 1): • The user operates the client component, which in its basic mode acts like a standard software audio player for *.mp3 and *.wav files. Additional interfaces, e.g., for performing content-based queries as well as various visualization tools, are provided through plug-ins (see Figure 2). • A remote computer system runs the server component, which is capable of identifying audio recordings played by the client and which supplies the client with metadata and annotations for those recordings. Furthermore, the server comprises several types of music search engines. • Several server-side administration tools are used for maintaining the databases and indexes underlying the SyncPlayer system. In a typical SyncPlayer scenario we assume that, on the server-side, there exists a large collection of music documents. Here, for each given piece of music, various digital representations (e. g., audio, MIDI, MusicXML, scanned images of sheet music) as well as associated metadata should be accessible by the server. In the following, this kind of data will be referred to as raw data. The raw data is further processed to generate what we refer to as derived c⃝2007 Austrian Computer Society (OCG). Raw Data Audio Data MIDI Data Lyrics Metadata · · · Derived Data Features Annotations Alignment Data · · · SyncPlayer Framework Administration Data Conversion Annotation & Data Management Search Indexes · · · Server Audio Identification Delivery of Metadata & Annotations Search Engines (e. g. Lyrics, MIDI, Score, Metadata) · · · Client Audio Playback Visualization Navigation Search Interfaces · · · | {z } | {z } Offline (Preprocessing) Online (Runtime) Figure 1. Overview of the SyncPlayer framework. data. The derived data comprises high-level audio features, various kinds of synchronization and linking data, or structural data, which reveals musically relevant characteristics as well as existing relations within the underlying raw data. Such data may be generated efficiently in a purely automatic fashion by means of MIR techniques. Other types of derived data may include textual annotations of audio recordings aligning the lyrics to a corresponding recorded song or synchronization data linking scanned sheet music with a corresponding audio recording. Using the SyncPlayer administration tools, the raw data as well as the derived data are indexed and stored in databases, which can then be efficiently accessed by the SyncPlayer server. The generation of the derived data as well as the data organization and indexing can be done offline in some preprocessing step (Figure 1). For further technical details concerning the data administration and the SyncPlayer implementation, we refer to [2, 3]. The SyncPlayer framework offers two basic modes for accessing audio documents and corresponding contentbased data such as annotations. First, a user operating the client system may choose locally available audio recordings for playback. The client then extracts features from the audio recordings which are sent to the remote SyncPlayer server. The server subsequently attempts to identify the audio recording based on the submitted features. Upon success, the server searches its database for available annotations (such as lyrics or notes) which are then sent back to the client. The client system offers the user several visualization types for the available annotations. Examples are indicated by Figure 2, which shows from top to bottom the SyncPlayer client, a visualization plug-in for a karaokelike display for lyrics information, a piano-roll style display for note (MIDI) information, and a display for the repetitive audio structure. Further plug-ins are available for displaying the waveform, the spectogram, or scanned Figure 2. The SyncPlayer with three visualization plugins showing different annotations (lyrics, piano roll, structure) for Schubert’s Winterreise D911 No. 11. images of sheet music synchronously to audio playback. The plug-in displaying the audio structure allows the user to instantly switch between blocks of musically similar content within a single music document. For example, musically similar blocks might be repetitions or variations of a musical theme. We call this application intradocument browsing because the user browses through particular blocks of a single music document. In addition, we have developed the Audio Switcher plug-in (see Figure 3), which offers inter-document browsing similar to the MATCH System by Dixon et al. [1]. It allows the user to open several synchronized interpretations of the same piece of music. The user may listen to one of the selected interpretations and then, at any time during playback, switch to another interpretation. The playback in the target interpretation will continue at the position that musically corresponds to the position inside the previously selected interpretation. The second method for accessing audio documents using the SyncPlayer is by means of appropriate query enFigure 3. The SyncPlayer on top of the Audio Switcher plug-in. Here, five different interpretations of Beethoven’s Fifth Symphony are selected in the Audio Switcher. gines. In this scenario, the user operates a query plugin offered by the client. Queries are submitted to the SyncPlayer server which, depending on the query type, schedules the queries to an appropriate query engine. A ranked list of retrieval results is returned to the client and displayed to the user. The user may then select particular query results for playback which are subsequently streamed from the server along with available annotation data. Currently, query engines for lyricsand melodybased retrieval exists for performing symbolic queries. For searching audio recordings, a query engine for audio matching [4] has been implemented. For detailed information on the SyncPlayer framework we refer to [2, 3]. A demo version of the SyncPlayer is available for download at the SyncPlayer Homepage [5]. 2 REFERENCES [1] S. DIXON AND G. WIDMER, MATCH: A Music Alignment Tool Chest, in Proc. ISMIR, London, GB, 2005. [2] C. FREMEREY, SyncPlayer – a Framework for ContentBased Music Navigation. Master Thesis, Dept. of Computer Science, University of Bonn, 2006. [3] F. KURTH, M. M ¨ULLER, D. DAMM, C. FREMEREY, A. RIBBROCK, AND M. CLAUSEN, Syncplayer—an advanced system for content-based audio access, in Proc. ISMIR, London, GB, 2005. [4] M. M ¨ULLER, F. KURTH, AND M. CLAUSEN, Audio matching via chroma-based statistical features, in Proc. ISMIR, London, GB, 2005. [5] SYNCPLAYER, An advanced system for multmodal music access. Website, January 2007. http://www-mmdb. iai.uni-bonn.de/projects/syncplayer.",
        "zenodo_id": 1415586,
        "dblp_key": "conf/ismir/FremereyKMC07",
        "keywords": [
            "audio synchronization",
            "content-based retrieval",
            "multimodal presentation",
            "music data browsing",
            "metadata and annotations",
            "raw data processing",
            "derived data generation",
            "administration tools",
            "administration of databases",
            "data indexing"
        ],
        "content": "A DEMONSTRATION OF THE SYNCPLAYER SYSTEM\nChristian Fremerey, Frank Kurth, Meinard M ¨uller, and Michael Clausen\nBonn University\nDepartment of Computer Science III\nABSTRACT\nThe SyncPlayer system is an advanced audio player for\nmultimodal presentation, browsing, and retrieval of music\ndata. The system has been extended signiﬁcantly in the\nlast few years. In this contribution, we describe the curren t\nstate of the system and demonstrate the functionalities and\ninteractions of the novel SyncPlayer components includ-\ning combined inter- and intra-document music browsing.\n1 SYNCPLAYER OVERVIEW\nThe SyncPlayer is a client-server based software frame-\nwork integrating various MIR-techniques such as music\nsynchronization, content-based retrieval, and multimoda l\npresentation of audio recordings and associated annota-\ntions [3]. The framework basically consists of three com-\nponents: a server component, a client component, and a\ntoolset for data administration (see also Figure 1):\n•The user operates the client component , which in\nits basic mode acts like a standard software audio\nplayer for*.mp3 and*.wav ﬁles. Additional in-\nterfaces, e.g., for performing content-based queries\nas well as various visualization tools, are provided\nthrough plug-ins (see Figure 2).\n•A remote computer system runs the server compo-\nnent, which is capable of identifying audio record-\nings played by the client and which supplies the\nclient with metadata and annotations for those\nrecordings. Furthermore, the server comprises sev-\neral types of music search engines.\n•Several server-side administration tools are used for\nmaintaining the databases and indexes underlying\nthe SyncPlayer system.\nIn a typical SyncPlayer scenario we assume that, on the\nserver-side, there exists a large collection of music docu-\nments. Here, for each given piece of music, various digital\nrepresentations (e. g., audio, MIDI, MusicXML, scanned\nimages of sheet music) as well as associated metadata\nshould be accessible by the server. In the following, this\nkind of data will be referred to as raw data . The raw data\nis further processed to generate what we refer to as derived\nc/circlecopyrt2007 Austrian Computer Society (OCG).Raw Data\n- Audio Data\n- MIDI Data\n- Lyrics\n- Metadata\n· · ·\nDerived Data\n- Features\n- Annotations\n- Alignment\nData\n· · ·SyncPlayer Framework\nAdministration\n- Data Conversion\n- Annotation\n& Data Management\n- Search Indexes\n· · ·Server\n- Audio Identiﬁcation\n- Delivery of Metadata\n& Annotations\n- Search Engines\n(e. g. Lyrics, MIDI,\nScore, Metadata)\n· · ·Client\n- Audio Playback\n- Visualization\n- Navigation\n- Search Interfaces\n· · ·\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright /bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\nOfﬂine (Preprocessing) Online (Runtime)\nFigure 1 . Overview of the SyncPlayer framework.\ndata. The derived data comprises high-level audio fea-\ntures, various kinds of synchronization and linking data,\nor structural data, which reveals musically relevant char-\nacteristics as well as existing relations within the under-\nlying raw data. Such data may be generated efﬁciently in\na purely automatic fashion by means of MIR techniques.\nOther types of derived data may include textual annota-\ntions of audio recordings aligning the lyrics to a corre-\nsponding recorded song or synchronization data linking\nscanned sheet music with a corresponding audio record-\ning. Using the SyncPlayer administration tools, the raw\ndata as well as the derived data are indexed and stored in\ndatabases, which can then be efﬁciently accessed by the\nSyncPlayer server. The generation of the derived data as\nwell as the data organization and indexing can be done of-\nﬂine in some preprocessing step (Figure 1). For further\ntechnical details concerning the data administration and\nthe SyncPlayer implementation, we refer to [2, 3].\nThe SyncPlayer framework offers two basic modes for\naccessing audio documents and corresponding content-\nbased data such as annotations. First, a user operating the\nclient system may choose locally available audio record-\nings for playback. The client then extracts features from\nthe audio recordings which are sent to the remote Sync-\nPlayer server. The server subsequently attempts to identify\nthe audio recording based on the submitted features. Upon\nsuccess, the server searches its database for available an-\nnotations (such as lyrics or notes) which are then sent back\nto the client. The client system offers the user several visu -\nalization types for the available annotations. Examples ar e\nindicated by Figure 2, which shows from top to bottom the\nSyncPlayer client, a visualization plug-in for a karaoke-\nlike display for lyrics information, a piano-roll style dis -\nplay for note (MIDI) information, and a display for the\nrepetitive audio structure. Further plug-ins are availabl e\nfor displaying the waveform, the spectogram, or scannedFigure 2 . The SyncPlayer with three visualization plug-\nins showing different annotations (lyrics, piano roll, str uc-\nture) for Schubert’s Winterreise D911 No. 11.\nimages of sheet music synchronously to audio playback.\nThe plug-in displaying the audio structure allows the\nuser to instantly switch between blocks of musically sim-\nilar content within a single music document. For exam-\nple, musically similar blocks might be repetitions or vari-\nations of a musical theme. We call this application intra-\ndocument browsing because the user browses through par-\nticular blocks of a single music document. In addition,\nwe have developed the Audio Switcher plug-in (see Fig-\nure 3), which offers inter-document browsing similar to\nthe MATCH System by Dixon et al. [1]. It allows the\nuser to open several synchronized interpretations of the\nsame piece of music. The user may listen to one of the\nselected interpretations and then, at any time during play-\nback, switch to another interpretation. The playback in the\ntarget interpretation will continue at the position that mu -\nsically corresponds to the position inside the previously\nselected interpretation.\nThe second method for accessing audio documents us-\ning the SyncPlayer is by means of appropriate query en-\nFigure 3 . The SyncPlayer on top of the Audio Switcher\nplug-in. Here, ﬁve different interpretations of Beethoven ’s\nFifth Symphony are selected in the Audio Switcher.\ngines. In this scenario, the user operates a query plug-\nin offered by the client. Queries are submitted to the\nSyncPlayer server which, depending on the query type,\nschedules the queries to an appropriate query engine. A\nranked list of retrieval results is returned to the client an d\ndisplayed to the user. The user may then select partic-\nular query results for playback which are subsequently\nstreamed from the server along with available annotation\ndata. Currently, query engines for lyrics- and melody-\nbased retrieval exists for performing symbolic queries.\nFor searching audio recordings, a query engine for audio\nmatching [4] has been implemented.\nFor detailed information on the SyncPlayer framework\nwe refer to [2, 3]. A demo version of the SyncPlayer is\navailable for download at the SyncPlayer Homepage [5].\n2 REFERENCES\n[1] S. D IXON AND G. W IDMER ,MATCH: A Music Alignment\nTool Chest , in Proc. ISMIR, London, GB, 2005.\n[2] C. F REMEREY ,SyncPlayer – a Framework for Content-\nBased Music Navigation . Master Thesis, Dept. of Computer\nScience, University of Bonn, 2006.\n[3] F. K URTH , M. M ¨ULLER , D. D AMM , C. F REMEREY ,\nA. R IBBROCK ,AND M. C LAUSEN ,Syncplayer—an ad-\nvanced system for content-based audio access , in Proc. IS-\nMIR, London, GB, 2005.\n[4] M. M ¨ULLER , F. K URTH ,AND M. C LAUSEN ,Audio match-\ning via chroma-based statistical features , in Proc. ISMIR,\nLondon, GB, 2005.\n[5] S YNCPLAYER ,An advanced system for multmodal music\naccess . Website, January 2007. http://www-mmdb.\niai.uni-bonn.de/projects/syncplayer ."
    },
    {
        "title": "Visualizing Music on the Metrical Circle.",
        "author": [
            "Klaus Frieler"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416798",
        "url": "https://doi.org/10.5281/zenodo.1416798",
        "ee": "https://zenodo.org/records/1416798/files/Frieler07.pdf",
        "abstract": "In this paper we propose a novel method, called Metrical Circle Map, for exploring the cyclic aspects of musical time. To this end, we give a short formalization introducing the notion of Metrical Markov Chains as transition probabilities of segments on the metrical circle. As an illustration we present a compact visualization of the zerothand first order metrical Markov transitions of 61 Irish folk songs. 1 INTRODUCTION One important and distinctive feature of metrically-bound music is the double nature of its musical time, linear on one hand, cyclic on the other. However, in most of musicological and other music-related research the focus is laid on linear aspects of musical time. The cyclic nature of musical time is mainly investigated in the context of genuine meter and rhythm research, where rhythm and meters are occasionally visualized or operationalized using a circle representation ([3], [4], [5]). In this paper, we will extend this approach by introducing the so-called Metrical Circle Map and Metrical Markov Chains, opening up several interesting posibilities of visualizing and analyzing metrically bound music, including single monophonic or polyphonic pieces as well as entire corpora. 2 METRICAL CIRCLE MAP The onsets of metrically-bound music are organized around underlying beats (or pulses), which are grouped into higher level units. This is even true for un-quantized music with tempo and meter variations, as for the concept of meter only a set of discrete time-points is needed along with a grouping prescription. We restrict ourselves here to the bar as the main metrical unit, and futhermore to the simplest form of a bar as a constantly-recurring time-span. For a rhythm conceived as a sequence of time-points ti, and a bar time T possibly inferred from the original sequence by some beat and meter induction algorithm (e.g. [2]), or given by annotation the Metrical Circle Map MT is defined as a mapping from the reals into the complex unit circle S1: MT (ti) = zi = e2πi ti T (1) c⃝2007 Austrian Computer Society (OCG). Figure 1. Metrical circle map of two Irish melodies. The left one (Essen Folksong Collection, I0501) shows 2/4 time, the right one (Essen Folksong Collection, I0504) 3/4 time. Thickness of lines corresponds to the frequencies of the transitions. Time is running counter-clockwise and the downbeat is located an the point (1,0) in the complex plane (three o’clock) In this form we already normalized the map, so that time is running in the mathematical direction counter-clockwise and the beginning of bars always lie at three o’clock. In Figure 1 two simple examples of the Metrical Circle Map (MCM) for two Irish folk songs can be seen, one of them being in 2/4 and one in 3/4 time.",
        "zenodo_id": 1416798,
        "dblp_key": "conf/ismir/Frieler07",
        "keywords": [
            "Metrical Circle Map",
            "Metrical Markov Chains",
            "metrically-bound music",
            "metrically-bound time",
            "cyclic nature",
            "Metrical Circle Map MT",
            "reals into the complex unit circle",
            "complex plane",
            "time-points ti",
            "bar time T"
        ],
        "content": "VISUALIZING MUSIC ONTHE METRICAL CIRCLE\nKlaus Frieler\nUniversity ofHamburg\nInstitute for Musicology\nABSTRACT\nIn this paper we propose a novel method, called Metrical\nCircle Map, for exploring the cyclic aspects of musical\ntime. To this end, we give a short formalization intro-\nducing the notion of Metrical Markov Chains as transi-\ntion probabilities of segments on the metrical circle. As\nan illustration we present a compact visualization of the\nzeroth- and ﬁrst order metrical Markov transitions of 61\nIrishfolksongs.\n1 INTRODUCTION\nOneimportantanddistinctivefeatureofmetrically-bound\nmusic is the double nature of its musical time, linear on\none hand, cyclic on the other. However, in most of mu-\nsicological and other music-related research the focus is\nlaidonlinearaspectsofmusicaltime. Thecyclicnatureof\nmusical time is mainly investigated in the context of gen-\nuinemeterandrhythmresearch,whererhythmandmeters\nare occasionally visualized or operationalized using a cir -\ncle representation ([3], [4], [5]). In this paper, we will\nextend this approach by introducing the so-called Metri-\ncal Circle Map and Metrical Markov Chains, opening up\nseveral interesting posibilities of visualizing and analy z-\ningmetricallyboundmusic,includingsinglemonophonic\nor polyphonic pieces as well as entirecorpora.\n2 METRICAL CIRCLE MAP\nTheonsetsofmetrically-boundmusicareorganizedaround\nunderlyingbeats(orpulses),whicharegroupedintohigher\nlevel units. This is even true for un-quantized music with\ntempo and meter variations, as for the concept of meter\nonly a set of discrete time-points is needed along with a\ngrouping prescription. We restrict ourselves here to the\nbar as the main metrical unit, and futhermore to the sim-\nplest form of a bar as a constantly-recurring time-span.\nFor a rhythm conceived as a sequence of time-points ti,\nand a bar time T- possibly inferred from the original se-\nquence by some beat and meter induction algorithm (e.g.\n[2]),orgivenbyannotation-the MetricalCircleMap MT\nis deﬁned as a mapping from the reals into the complex\nunit circle S1:\nMT(ti) =zi=e2πiti\nT (1)\nc/circlecopyrt2007 AustrianComputer Society (OCG).\nFigure 1. Metrical circle map of two Irish melodies. The\nleft one (Essen Folksong Collection, I0501) shows 2/4\ntime,therightone(EssenFolksongCollection,I0504)3/4\ntime. Thickness of lines corresponds to the frequencies\nof the transitions. Time is running counter-clockwise and\nthe downbeat is located an the point (1,0) in the complex\nplane (threeo’clock)\nInthisformwealreadynormalizedthemap,sothattimeis\nrunning in the mathematical direction counter-clockwise\nand the beginning of bars always lie at three o’clock. In\nFigure 1 two simple examples of the Metrical Circle Map\n(MCM) for two Irish folk songs can be seen, one of them\nbeing in2/4and one in3/4time.\n2.1 Metrical Markov Chains\nThe Metrical Circle Map (MCM) lends itself quite natu-\nrallytodeﬁningtransitionprobabilitiesbetweensegment s\non the metrical circle. To this end, we deﬁne Nintervals\nonS1according to\nIk={z∈S1|z=e2πiφk\nN,φk∈[k−1\n2,k+1\n2]}(2)\nwith0≤k < N. With these intervals and the MCM\nwe can map a sequence of time-points onto a sequence\nof intervals on the metrical circle. For these sequences\nof interval indices we deﬁne the usual Markov transitions\nprobabilities. Particularly,wewillbeinterestedinzero th-\nand ﬁrst-ordertransitions\np(k) =p(z∈Ik), p(k|j) =p(zi∈Ik|zi−1∈Ij)\n(3)\nFor calculating transition probabilities, the choice of Nis\nof course crucial, as it controls the time resolution. If, fo r\nexample, one chooses N= 2, the ﬁrst order probabilities\nindicate the probabilities of transitions between the twoSignature 2/43/44/46/46/89/8\nCount 720202102\nTable 1. Distribution of signatures in the collection of 61\nIrishfolksongs\nhalves of a bar. For more detailed and complete compar-\nisions, particularly for whole corpora with a full range of\nmeters, achoice of N= 48seems appropriate, represent-\ninge.g. a4/4meterwitharesolutionofthirty-secondnote\nsextuplets.\n3 EXAMPLES: ZEROTH- AND FIRST-ORDER\nMARKOV CHAINS FOR IRISH FOLK SONGS\nInordertoillustrateourmethod,wetransformed61(mono-\nphonic) Irish folk songs taken from the Essen Folksong\nDatabase with the MCM, and accumulated zeroth- and\nﬁrst-order transition probabilities over all songs, using an\nN= 48segmentation of the circle. These probabili-\nties are jointly depicted in Figure 2, where the thickness\nof a line connecting two metric positions is proportional\nto the ﬁrst-order transition probability (transitions wit h\np <0.001left out). The size of the smaller circles at the\nmetricpositionsareproportionaltothelogarithmofoccu-\npation probability. One clearly identiﬁes the square from\nduple,thetrianglefromtriple,andthehexagonfromcom-\npoundduplemeters. Notsurprisingly,thedownbeatisthe\nmost frequent position (21%), with the most arrows end-\ning at this point, reached from 6 different positions, noti-\ncably from points right before the downbeat, the upbeats,\nwhich playanimportant roleinestablishingand reinforc-\ning the meter. The next most frequent position, the half-\nbar position from duple and ternary compound meter, is\noccupiedin13%ofthecases,with3arrowsendingthere.\nFurthermore, a nearly complete absence of syncopations\ncan be read off from the graph, notably, the vertical axis\nfrom the transition 12→36is missing, as well as synco-\npational transitions from positions 6, 18, 30, 42 in duple\nmeter,orfrom8,20,40incompounddupleandtripleme-\nter. In contrast to this, examination of a set of pop songs\nshowed a considerably more chaotic picture (not shown\nhere for the lack of space), with frequent syncopations, a\nhigher diversity of possible transitions and less emphasis\non thedown- and half-bar-beats.\n4 DISCUSSION & OUTLOOK\nWe propose the method of the Metrical Circle Map. Sin-\ngle or entire corpora of melodies, as well as polyphonic\nmusic can be visualized on the metrical circle using met-\nricaltransitionprobabilities,givingnewanalyticalins ight\nin the cyclic aspects of musical time and in the metrical\npeculiarities of different genres, which will be explored\nfurther inthefuture.012\n24\n366 18\n304239 15\n21\n27\n33 3945\nFigure 2. Zeroth- and ﬁrst-order metric transitions prob-\nabilities of 61 Irish folk songs from the Essen collection.\nThe thickness of lines is proportional to the frequency of\nﬁrst-order transitions; transitions with p <0.01were left\nout. Thesizeofthecirclesisproportionaltothelogarithm\nof occupation probabilities.\n5 ACKNOWLEDGEMENTS\nWeliketothankStephanRoters(http://www.brandhotel.de )\nforhis help withassembling Figure 2.\n6 REFERENCES\n[1] Anku, Willie. “Circles and Time: A The-\nory of Structural Organization of Ryhthm in\nAfrican Music”, Music Theory Online , 6(1),\nhttp://www.societymusictheory.org/mto, 2004.\n[2] Frieler, Klaus. “Beat and meter extraction using gaus-\nsiﬁed onsets.” In Proc. 5th International Conference\non Music Information Retrieval , Universitat Pompeu\nFabra, Barcelona, Spain, 2004.\n[3] London, Justin Hearing in Time . Oxford University\nPress,Oxford, England, 2004.\n[4] Toussaint, Godfried T. “A comparison of rhythmic\nsimilarity measures.” In Proc. 5th International Con-\nference on Music Information Retrieval , Universitat\nPompeu Fabra, Barcelona, Spain, 2004.\n[5] Taslakian, P. and Toussaint, Godfried T. “Geomet-\nric properties of musical rhythm”, Proceedings of the\n16th Fall Workshop on Computational and Combina-\ntorial Geometry , Smith College, Northampton, Mas-\nsachussetts, November 10-11, 2006."
    },
    {
        "title": "A Music Information Retrieval System Based on Singing Voice Timbre.",
        "author": [
            "Hiromasa Fujihara",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416228",
        "url": "https://doi.org/10.5281/zenodo.1416228",
        "ee": "https://zenodo.org/records/1416228/files/FujiharaG07.pdf",
        "abstract": "We developed a music information retrieval system based on singing voice timbre, i.e., a system that can search for songs in a database that have similar vocal timbres. To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors. We operated the system using 75 songs and confirmed that the system worked appropriately. According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method finds more appropriate songs that have similar vocal timbres. 1 INTRODUCTION This paper describes a music information retrieval (MIR) system that searches for songs that have similar voice timbres of vocals to a query song presented by a user. By using this system, we can find a song by using its musical content in addition to traditional bibliographic information. This kind of retrieval is called content-based MIR, and our system, which focuses on singing voices as content, falls into this category. There is a growing demand for such content-based MIR. Because of rapid and widespread diffusion of portable audio players and online music stores, many users can have an access to a large amount of music tracks and listen to any music they want anytime, anywhere. This trend has triggered a demand for an MIR that takes favorite songs from a large amount of music and uses them to discover new songs that the user has never heard before. When the query of the target song is not known and only vague information such as ”preference” is available, the conventional method of searching for music that only uses bibliographic information is useless. Although a number of studies on content-based MIR have been undertaken [2, 8, 1, 3, 9, 12, 10, 4, 11], they use low-level acoustic features, such as MFCC, spectral centroid, rolloff, and flux, for expressing musical content and do not use higher-level features such as the timbre of vocals. We therefore developed an MIR system that can retrieve songs by using vocal timbres. To achieve this sysc⃝2007 Austrian Computer Society (OCG). Vocal analysis Similarity calculation Vocal feature vectors Database construction Operation Query Retrieval result Database Target songs Vocal feature vectors Vocal feature vectors (Ranked list of songs) Vocal analysis Vocal analysis Vocal analysis Figure 1. Overview of our MIR system. tem, we have to extract, from the polyphonic audio signal of a song, vocal-based feature vector that represent the vocal characteristics and to calculate the vocal-timbre similarity between two songs by using the feature vectors. The vocal-based feature vectors were also used in our singer identification method proposed earlier [5]. We used the mutual information content as their similarity measure. 2 ARCHITECTURE OF THE SYSTEM Among many songs registered in a database, our system searches for songs that have similar singing voice timbres to a song query given by a user. The overview of this system is shown in Figure 1. The system consists of a database construction (audio analysis) part and an operation (song retrieval) part. In the database construction part, target songs are stored in the database after being ripped or downloaded, and then each song is analyzed to extract feature vectors that represent vocal characteristics. When the user enters a (favorite) song as a query to the system, the system analyses the query song and extracts feature vectors that also represent the query’s vocal characteristics. The system then calculates the vocal-timbre similarity between the query song and each song in the database and shows the list of songs with high similarity. 3 IMPLEMENTATION OF THE SYSTEM To implement the MIR system described in Section 2, we have to define the vocal-based feature vector and the vocal-timbre similarity measure.",
        "zenodo_id": 1416228,
        "dblp_key": "conf/ismir/FujiharaG07",
        "keywords": [
            "music information retrieval",
            "singing voice timbre",
            "feature vectors",
            "vocal-timbre similarity",
            "database construction",
            "operation",
            "subjective experiment",
            "conventional method",
            "content-based MIR",
            "rapid and widespread diffusion"
        ],
        "content": "A MUSIC INFORMATION RETRIEVAL SYSTEM\nBASED ON SINGING VOICE TIMBRE\nHiromasa Fujihara andMasataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST)\nTsukuba, Ibaraki 305-8568, Japan\n{h.fujihara, m.goto }[at] aist.go.jp\nABSTRACT\nWe developed a music information retrieval system based\non singing voice timbre, i.e., a system that can search for\nsongs in a database that have similar vocal timbres. To\nachieve this, we developed a method for extracting fea-ture vectors that represent characteristics of singing voices\nand calculating the vocal-timbre similarity between two\nsongs by using a mutual information content of their fea-ture vectors. We operated the system using 75 songs and\nconﬁrmed that the system worked appropriately. Accord-\ning to the results of a subjective experiment, 80% of sub-jects judged that compared with a conventional method\nusing MFCC, our method ﬁnds more appropriate songs\nthat have similar vocal timbres.\n1 INTRODUCTION\nThis paper describes a music information retrieval\n(MIR) system that searches for songs that have similarvoice timbres of vocals to a query song presented by a\nuser. By using this system, we can ﬁnd a song by using\nits musical content in addition to traditional bibliographicinformation. This kind of retrieval is called content-based\nMIR, and our system, which focuses on singing voices as\ncontent, falls into this category.\nThere is a growing demand for such content-based\nMIR. Because of rapid and widespread diffusion of\nportable audio players and online music stores, many\nusers can have an access to a large amount of music tracks\nand listen to any music they want anytime, anywhere. Thistrend has triggered a demand for an MIR that takes fa-\nvorite songs from a large amount of music and uses them\nto discover new songs that the user has never heard be-fore. When the query of the target song is not known and\nonly vague information such as ”preference” is available,\nthe conventional method of searching for music that onlyuses bibliographic information is useless.\nAlthough a number of studies on content-based MIR\nhave been undertaken [2, 8, 1, 3, 9, 12, 10, 4, 11], theyuse low-level acoustic features, such as MFCC, spectral\ncentroid, rolloff, and ﬂux, for expressing musical content\nand do not use higher-level features such as the timbre ofvocals.\nWe therefore developed an MIR system that can re-\ntrieve songs by using vocal timbres. To achieve this sys-\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nVocal\nanalysis\nSimilarity calculationVocal\nfeature \nvectorsDatabase construction\nOperation\nQuery Retrieval result\nDatabaseTarget songs\nVocal\nfeature \nvectorsVocal\nfeature \nvectors\n(Ranked list of songs)Vocal\nanalysisVocalanalysis\nVocal\nanalysis\nFigure 1 . Overview of our MIR system.\ntem, we have to extract, from the polyphonic audio signal\nof a song, vocal-based feature vector that represent the vo-\ncal characteristics and to calculate the vocal-timbre simi-\nlarity between two songs by using the feature vectors. Thevocal-based feature vectors were also used in our singer\nidentiﬁcation method proposed earlier [5]. We used the\nmutual information content as their similarity measure.\n2 ARCHITECTURE OF THE SYSTEM\nAmong many songs registered in a database, our sys-\ntem searches for songs that have similar singing voice tim-bres to a song query given by a user. The overview of this\nsystem is shown in Figure 1. The system consists of a\ndatabase construction (audio analysis) part and an oper-ation (song retrieval) part. In the database construction\npart, target songs are stored in the database after being\nripped or downloaded, and then each song is analyzed toextract feature vectors that represent vocal characteristics.\nWhen the user enters a (favorite) song as a query to the\nsystem, the system analyses the query song and extractsfeature vectors that also represent the query’s vocal char-\nacteristics. The system then calculates the vocal-timbre\nsimilarity between the query song and each song in thedatabase and shows the list of songs with high similarity.\n3 IMPLEMENTATION OF THE SYSTEM\nTo implement the MIR system described in Section 2,\nwe have to deﬁne the vocal-based feature vector and the\nvocal-timbre similarity measure.3.1 Feature Extraction\nTo calculate feature vectors that represent vocal char-\nacteristics, we use a feature extraction method used in our\nsinger identiﬁcation method [5]. This method can reduce\na negative inﬂuence of accompaniment sounds which aremixed with the singing voice in a song. This feature vec-\ntor has a better representation of vocal characteristics than\na feature vector like MFCC that just represents a mixtureof accompaniment sounds and the singing voice.\nThis method consists of the following three parts: ac-\ncompaniment sound reduction, feature extraction, and re-\nliable frame selection. To reduce the negative inﬂuenceof accompaniment sounds, the accompaniment sound re-\nduction part ﬁrst segregates and resynthesizes the singing\nvoice from polyphonic audio signals on the basis of itsharmonic structure. The feature extraction part then cal-\nculates feature vectors from the segregated singing voice.\nThe reliable frame selection part selects reliable vocal re-\ngions (frames) from the feature vectors and removes un-\nreliable regions that do not contain vocals or are highlyinﬂuenced by accompaniment sounds.\n3.1.1 Accompaniment Sound Reduction\nFor the accompaniment sound reduction part, we use a\nmelody resynthesis technique that consists of the follow-\ning three steps:\n1. Estimating the fundamental frequency (F0) of the vo-\ncal melody using Goto’s PreFEst [6].\n2. Extracting the harmonic structure corresponding to\nthe melody.\n3. Resynthesizing the audio signal corresponding to the\nmelody using sinusoidal synthesis.\nWe use Goto’s PreFEst [6] for estimating the F0 of\nthe melody. PreFEst estimates the most predominant F0in frequency-range-limited sound mixtures. Since the\nmelody line tends to have the most predominant harmonic\nstructure in middle- and high-frequency regions, we canestimate the F0 of the melody by applying PreFEst with\nappropriate frequency-range limitation.\nBy using the estimated F0, we then extract the ampli-\ntude of the fundamental frequency component and har-monic components. For each component, we allow rcent\nerror and extract the local maximum amplitude in the al-\nlowed area. The frequency F\n(t)\nland amplitude A(t)\nlof the\nlth overtone (l=1,...,L )at time (t)can be represented\nas\nF(t)\nl= argmax\nF|S(t)(F)|\n(lF(t)(1−2r\n1200)≤F≤lF(t)(1 + 2r\n1200)),\n(1)\nA(t)\nl=|S(t)(Fl)|, (2)\nwhere S(t)(F)denotes the complex spectrum, and F(t)\ndenotes F0 estimated by the PreFEst. In our experiments,\nwe set rto 20.\nFinally, we use a sinusoidal model to resynthesize the\naudio signal of the melody by using the extracted har-\nmonic structure, F(t)\nlandA(t)\nl. Changes in phase are ap-\nproximated using a quadratic function so that a frequencycan change linearly. Changes in amplitude are also ap-\nproximated using a linear function. The resynthesized au-\ndio signals, s(k), are expressed as\ns(k)=L/summationdisplay\nl=1sl(k), (3)\nsl(k)=/braceleftbigg\n(A(t+1)\nl−A(t)\nl)k\nK+A(t)\nl/bracerightbigg\nsin (θl(k)),\n(4)\nθl(k)=π(F(t+1)\nl−F(t)\nl)\nKk2+2πF(t)\nlk+θl,0,(5)\nwhere krepresents a time in units of seconds and k=\n0corresponds to the time (t),Krepresents the duration\nbetween (t)and(t+1) in units of seconds, and θl,0means\nthe initial phase.\n3.1.2 Feature Extraction\nFrom the resynthesized audio signals, we calculate fea-\nture vectors consisting of two features.\n•LPC-derived mel cepstral coefﬁcients (LPMCCs)\nIt is known that the individual characteristics of\nspeech signals are expressed in their spectral en-\nvelopes. We use LPMCCs as spectral feature because\nwe have reported that, in the context of singer identi-\nﬁcation, LPMCCs represent vocal characteristics bet-ter than mel-frequency cepstral coefﬁcients (MFCCs),\nwhich are widely used for music modeling [5].\n•ΔF0s\nWe use ΔF0s which represent the dynamics of F0’s\ntrajectory, because a singing voice tends to have tem-\nporal variations in its F0 in consequence of vibratoand such temporal information is expected to express\nthe singer’s characteristics.\n3.1.3 Reliable Frame Selection\nBecause the F0 of the melody is simply estimated as\nthe most predominant F0 in each frame [6], the resynthe-sized audio signals may contain both the vocal sound in\nsinging sections and other instrument sounds in interlude\nsections. The feature vectors obtained from them there-fore include unreliable regions (frames) where other ac-\ncompaniment sounds are predominant. The reliable frame\nselection part removes such unreliable regions and usesonly the reliable regions for calculating similarity. In or-\nder to achieve this, we introduce two kinds of Gaussian\nmixture models (GMMs), a vocal GMM λ\nVand a non-\nvocal GMM λN. The vocal GMM λVis trained on feature\nvectors extracted from singing sections, and the non-vocal\nGMM λNis trained on those extracted from interlude sec-\ntions. Given a feature vector x, the likelihoods for the\ntwo GMMs, p(x|λV)andp(x|λN), represent how the fea-\nture vector xis like a vocal or a (non-vocal) instrument,\nrespectively. We therefore determine whether the feature\nvector xis reliable or not by using the following equation:\nlogp(x|λV)−logp(x|λN)reliable\n≥\n<\nnot-reliableη, (6)\nwhere ηis a threshold.It is difﬁcult to decide a universal constant threshold\nfor a variety of songs because if a threshold is too high for\nsome songs, there are too few reliable frames to appro-\npriately calculate the similarity. We therefore determinethe threshold dependent on songs so that α%of the whole\nframes in the song are selected as reliable frames. Note\nthat most of the non-vocal frames are rejected in this se-lection step.\n3.2 Similarity Calculation\nWe choose mutual information content to be the simi-\nlarity measure between two songs. First, we model a prob-\nability distribution of the feature vectors for a song using\nGMM and estimate the parameters of the GMM for eachsong by using the EM algorithm. Then, we calculate the\nsimilarity between song X and song Y, d\nCE(X,Y), by us-\ning the following equation:\ndCE(X,Y) = log/productdisplay\niNGMM(xi;θX)\nNGMM(xi;θY)\n+ log/productdisplay\njNGMM(yj;θY)\nNGMM(yj;θX)(7)\nwhere xiandyjrepresent feature vectors of reliable\nframes in song X and song Y, respectively, θXandθY\nrepresent GMM parameters of song X and song Y, re-\nspectively, and NGMM(x;θ)represents the likelihood of\nGMM with parameter θ.\n4 OPERATION OF THE SYSTEM\nFigure 2 shows a screenshot of the system. For con-\nstructing the vocal and the non-vocal GMMs, we se-\nlected 25 songs from ”RWC Music Database: PopularMusic“ (RWC-MDB-P-2001) [7]. In the system database,\nwe registered other 75 songs from the RWC-MDB-P-\n2001, which were not used for constructing those GMMs.In Figure 2, a song “PROLOGUE” (RWC-MDB-P-2001\nNo.7) sung by a female singer “Tomomi Ogata” is given\nas a query. Given a query song, it took about 20 seconds tocalculate similarities and output a ranked list of retrieved\nsongs. As shown in Figure 2, the retrieval result consists\nof the rank, song title, artist name, and similarity.\nIn most of songs retrieved given various queries, vocal\ntimbres of the top 10 songs were generally similar to that\nof each query song in our experience. For example, inFigure 2, the top 21 songs were sung by female singers,\nand vocal timbres of the top 15 songs shown in this ﬁgure\nwere similar to the query song. Note that four songs from“Tomomi Ogata” who was the singer of the query took\nﬁrst, second, 10th, and 12th places. This is because the\nsinging style of the 10th and 12th songs were differentfrom that of the ﬁrst and second songs and the query.\n5 SUBJECTIVE EXPERIMENT\nWe conducted a subjective experiment to compare our\nsystem using the proposed vocal-based feature vector with\na baseline system using the traditional MFCCs.\n5.1 Experimental Procedure\nSix university students (two male and four female) par-\nticipated in this experiment. Those subjects had not re-\nFigure 2 . Screenshot of the system.\nceived any professional training in music. They ﬁrst lis-\ntened to a set of three songs — a query song (song X), thetop ranked song retrieved by our system (song A/B), and\nthe top ranked song retrieved by the baseline system (song\nB/A) —, and then judged which song was more similarto the query song (Figure 3). The subjects did not know\nwhich song was retrieved by our system and the song or-\nder of A and B is randomized. We allowed them to listento those songs in any order as much as they like.\nWe selected ten query songs from the system database\nconsidering that these songs have a variety of genders and\ngenres. For each query song, we asked the subjects thefollowing questions:\nQuestion 1 When comparing the singing voice timbres of\nsong A and B, which song resembles song X?\nQuestion 2 When comparing the overall timbres of song\nA and B, which song resembles song X?\n5.2 Results and Discussion\nFigures 4 and 5 show the results of the experiment.\n80% of the responses for 10 songs judged that the singing\nvoice timbre by our method was more similar to that ofthe query song (Figure 4). On the other hand, 70% of the\nresponses judged that the overall timbre by the baseline\nmethod was more similar to that of the query song (Figure5). Accordingly, we conﬁrmed that our method can reduce\nthe inﬂuence of accompaniment sounds and ﬁnd songs by\nusing vocal timbres. We also found that our method canretrieve not only songs with similar vocal timbres (or by\nsame singer) but also songs with similar singing styles.Table 1 . Query songs and the corresponding retrieved\nsongs used for the subjective experiment: The three-digit\nnumber indicates the piece number of the RWC MusicDatabase (RWC-MDB-P-2001). Given each query song,\nthe top ranked song by the baseline method (MFCC) and\nthe top ranked song by our method are shown on the sameline.\nQuery song Retrieved (top ranked) song\nPiece # Gender Language MFCC Our method\n004 M Japanese 031 082\n010 F Japanese 016 054\n029 M Japanese 017 012\n035 F Japanese 036 094\n045 M Japanese 090 042\n053 F Japanese 062 014\n072 M Japanese 071 076\n077 F Japanese 071 067\n092 F English 024 086\n098 M English 009 085\nSong X\nSong A Song BListen to the following three songs and answer the questions.\nQuestion 1: When comparing the singing voice timbres of the song A\nand B, which song resembles song X?\nQuestion 2: When comparing the overall timbres of the song A and B,\nwhich song resembles song X?\nFigure 3 . Interface used for the subjective experiment.\nFor example, when the song RWC-MDB-P-2001 No.53\nwas used as a query, both our method and the baselinemethod retrieved the top ranked songs by the singer same\nwith the query, but 5 out of 6 subjects judged that the song\nby our method was more similar to the query song in terms\nof the singing voice timbre.\n6 CONCLUSION\nWe have described a vocal-timbre-based MIR system\nthat uses a method for extracting vocal-based feature vec-\ntors from polyphonic audio signals and a method for mea-\nsuring the vocal-timbre similarity between two songs. Wetested the system on 75 songs and found that the system\nwas useful for retrieving similar-vocal songs. Our exper-\nimental results with six subjects showed the effectivenessof our similarity measure.\nWe used the mutual information content as our similar-\nity measure because of its symmetry and statistical sim-\nplicity. Although it was effective, it requires a high com-\nputational cost and a large storage because we have to useall the feature vectors to compute it. In the future, we\nplan to try other similarity measures, including the earth\nmover’s distance (EMD) [3], to lower the computationalcost. We also plan to integrate this system with other\ncontent-based MIR methods to give users a wider variety\n0%10%20%30%40%50%60%70%80%90%100%\n004 010 029 035 045 053 072 077 092 098 T otal\nMFCC\nOur Method\nFigure 4 . Evaluation result: Question 1: singing voice\ntimbre.\n0%10%20%30%40%50%60%70%80%90%100%\n004 010 029 035 045 053 072 077 092 098 Total\nMFCC\nOur Method\nFigure 5 . Evaluation result: Question 2: overall timbre.\nof retrieval methods.\nAcknowledgments: This work was supported by Crest-\nMuse, CREST, JST. We thank Takeshi Saito (AIST) and Tomoy-\nasu Nakano (Univ. of Tsukuba) for their valuable discussions.\n7 REFERENCES\n[1] E. Allamanche, J. Herre, O. Hellmuth, T. Kastner, and C. Ertel.\nA multiple feature model for musical similarity retrieval. In Proc.\nISMIR2003 , pages 217–218, 2003\n[2] J.-J. Aucouturier and F. Pachet. Music similarity measures: What’s\nthe use? In Proc. ISMIR2002 , pages 157–163, 2002.\n[3] A. Berenzweig, B. Logan, D. P. W. Ellis, and B. Whitman. A\nlarge-scale evaluation of acoustic and subjective music similarity\nmeasures. In Proc. ISMIR2003 , pages 99–105, 2003.\n[4] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer. Probabilistic com-\nbination of features for music classiﬁcation. In Proc. ISMIR2006 ,\npages 111–114, 2006.\n[5] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata, and\nH. G. Okuno. Singer identiﬁcation based on accompaniment sound\nreduction and reliable frame selection. In Proc. ISMIR2005 , pages\n329–336, 2005.\n[6] M. Goto. A real-time music-scene-description system:\npredominant-F0 estimation for detecting melody and bass lines in\nreal-world audio signals. Speech Communication , 43(4):311–329,\n2004.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWC Music\nDatabase: Popular, classical, and jazz music databases. In Proc.\nISMIR2002 , pages 287–288, 2002.\n[8] B. Logan. Content-based playlist generation: Exploratory experi-\nments. In Proc. ISMIR2002 , pages 295–296, 2002.\n[9] M.F. McKinney and J. Breebaart, Features for audio and music\nclassiﬁcation. In Proc. ISMIR2003 , pages 151–158, 2003.\n[10] E. Pampalk, A. Flexer, and G. Widmer. Improvements of audio-\nbased music similarity and genre classiﬁcation. In Proc. IS-\nMIR2005 , pages 628–633, 2005.\n[11] T. Pohle, P. Knees, M. Schedl, and G. Widmer. Independent com-\nponent analysis for music similarity computation. In Proc. IS-\nMIR2006 , pages 228–233, 2006.\n[12] G. Tanetakis, J. Gao, and P. Steenkiste. A scalable peer-to-peer\nsystem for music content and information retrieval. In Proc. IS-\nMIR2003 , pages 209–214, 2003."
    },
    {
        "title": "Using Pitch Stability Among a Group of Aligned Query Melodies to Retrieve Unidentified Variant Melodies.",
        "author": [
            "Jörg Garbers",
            "Peter van Kranenburg",
            "Anja Volk",
            "Frans Wiering",
            "Remco C. Veltkamp",
            "Louis P. Grijp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418191",
        "url": "https://doi.org/10.5281/zenodo.1418191",
        "ee": "https://zenodo.org/records/1418191/files/GarbersKVWVG07.pdf",
        "abstract": "Melody identification is an important task in folk song variation research. In this paper we develop methods and tools that support researchers in finding melodies in a database that belong to the same variant group as a set of given melodies. The basic approach is to derive from the pitches of the known variants per onset a weighted pitch distribution, which quantifies pitch stability. We allow for partial matching and AND and OR queries. Technically we do so by defining a distance measure between weighted pitch distribution sequences. It is based on two applications of the Earth Mover’s Distance, which is a distribution distance. We set up a distance framework and discuss musically meaningful parameterizations for two tasks: a) Study the inner-group distances between the group as a whole and single members of the group. b) Use the group’s weighted pitch distribution sequence to query for variant melodies. The first experimental results seem very promising: a) The inner-group distances correlate to expert assigned subgroups. b) For variant retrieval our method works better than last year’s MIREX winner. 1 INTRODUCTION In folk song variation research, collection items are associated with each other and grouped by different methods. These include text analysis, meta-information analysis and score-analysis. In the WITCHCRAFT project (What Is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes) we try to aid this process with computational methods based on the musical content. Our initial approach was to use a melody query in – ranked melodies out (short: M2M, Melody to Melody) search engine, based on a transportation distance (EMD) in the real valued onset-pitch domain. The actual M2M system, described in [7], proved to be the most effective retrieval system in the 2006 MIREX competition. We hoped that the result list would inspire folk song researchers by presenting items of the collection in new, similarity driven orderings. However, the users were rather disturbed by the amount of false positives (items c⃝2007 Austrian Computer Society (OCG). on top of the ranking list that should not be there), so we tried to find more informed ways to query the database. An important task in folksong research is the grouping of related melodies, generally because they might derive from a common melodic ancestor. To do so, one employs features that are stable accross sets of melodies. However, already in 1951, Bronson [1] pointed out, that finding and describing these stable aspects can be musicologically quite challenging. He proposes to use punch cards and a sorting machine to quantify stability and to describe the relationships between variants. In a similar approach [2] we have developed computer aided methods and tools to study pitch and harmonic stability. In the current paper we follow up on this approach by making use of the by-products of such kind of studies, namely sets of (manually) aligned melodies: we formally develop the idea to query a database for unclassified variants with a group of known melodic variants (short: G2M, Group to Melody). In this paper we assume that a set of melodic variants or variant phrases is aligned with respect to the temporal axis and that musically reasonable transformations of the material have been applied. The latter includes translations to a common key, selection of suitable bars and reduction to a particular metrical level, as done in a semi-automatic way in [2]. For a given onset time we thus get the pitches and rests that occur in any of the melodies. By querying ual variations that resulted in the different melodies but still maintain stable pitch requirements. In section 2 we first set up a mathematical and computational framework and define the mapping of musical notes to mathematical values. In section 3 we discuss musically reasonable parameterizations of the framework and in section 4 we conduct and evaluate computational experiments with selected parameterizations and compare the results with previous approaches. 2 THE COMPUTATIONAL FRAMEWORK This research builds on the Earth Mover’s Distance (EMD), whose musical application was studied in [7]. It is also an important building block in Typke’s M2M search engine, which we used in our first approach to folk song variation retrieval. However, instead of combining the onset and pitch dimensions into one Euclidean ground distance, as done in Typke’s contour matching, we compute the EMD on the onset and pitch dimensions separately. In section 2.1 we introduce the reader into the parameters of the EMD. In the following sections we define musically and task specific values.",
        "zenodo_id": 1418191,
        "dblp_key": "conf/ismir/GarbersKVWVG07",
        "keywords": [
            "Melody identification",
            "Folk song variation research",
            "Database matching",
            "Weighted pitch distribution",
            "Earth Movers Distance",
            "Transportation distance",
            "Group to Melody",
            "Alignment",
            "Musical stability",
            "MIREX competition"
        ],
        "content": "USING PITCH STABILITY AMONG A GROUP OF ALIGNED QUERY\nMELODIES TO RETRIEVE UNIDENTIFIED V ARIANT MELODIES\nJ¨org Garbers, Peter van Kranenburg, Anja Volk, Frans Wiering, Remco C. Veltkamp, Louis P. Grijp\u0003\nDepartment of Information and Computing Sciences\nUtrecht University and\u0003Meertens Institute, Amsterdam\ngarbers@cs.uu.nl\nABSTRACT\nMelody identiﬁcation is an important task in folk song\nvariation research. In this paper we develop methods and\ntools that support researchers in ﬁnding melodies in a\ndatabase that belong to the same variant group as a set of\ngiven melodies. The basic approach is to derive from the\npitches of the known variants per onset a weighted pitch\ndistribution, which quantiﬁes pitch stability . We allow for\npartial matching and AND and OR queries.\nTechnically we do so by deﬁning a distance measure\nbetween weighted pitch distribution sequences. It is based\non two applications of the Earth Mover’s Distance , which\nis a distribution distance. We set up a distance framework\nand discuss musically meaningful parameterizations for\ntwo tasks: a) Study the inner-group distances between the\ngroup as a whole and single members of the group. b) Use\nthe group’s weighted pitch distribution sequence to query\nfor variant melodies.\nThe ﬁrst experimental results seem very promising:\na) The inner-group distances correlate to expert assigned\nsubgroups. b) For variant retrieval our method works bet-\nter than last year’s MIREX winner.\n1 INTRODUCTION\nIn folk song variation research, collection items are as-\nsociated with each other and grouped by different meth-\nods. These include text analysis, meta-information anal-\nysis and score-analysis. In the WITCHCRAFT project\n(What Is Topical in Cultural Heritage: Content-based Re-\ntrieval Among Folksong Tunes ) we try to aid this process\nwith computational methods based on the musical content.\nOur initial approach was to use a melody query in –\nranked melodies out (short: M2M, Melody to Melody)\nsearch engine, based on a transportation distance (EMD)\nin the real valued onset-pitch domain. The actual M2M\nsystem, described in [7], proved to be the most effec-\ntive retrieval system in the 2006 MIREX competition.\nWe hoped that the result list would inspire folk song re-\nsearchers by presenting items of the collection in new,\nsimilarity driven orderings. However, the users were\nrather disturbed by the amount of false positives (items\nc\r2007 Austrian Computer Society (OCG).on top of the ranking list that should not be there), so we\ntried to ﬁnd more informed ways to query the database.\nAn important task in folksong research is the grouping\nof related melodies, generally because they might derive\nfrom a common melodic ancestor. To do so, one employs\nfeatures that are stable accross sets of melodies. How-\never, already in 1951, Bronson [1] pointed out, that ﬁnding\nand describing these stable aspects can be musicologically\nquite challenging. He proposes to use punch cards and a\nsorting machine to quantify stability and to describe the\nrelationships between variants.\nIn a similar approach [2] we have developed computer\naided methods and tools to study pitch and harmonic sta-\nbility. In the current paper we follow up on this approach\nby making use of the by-products of such kind of studies,\nnamely sets of (manually) aligned melodies: we formally\ndevelop the idea to query a database for unclassiﬁed vari-\nants with a group of known melodic variants (short: G2M,\nGroup to Melody).\nIn this paper we assume that a set of melodic variants or\nvariant phrases is aligned with respect to the temporal axis\nand that musically reasonable transformations of the ma-\nterial have been applied. The latter includes translations\nto a common key, selection of suitable bars and reduction\nto a particular metrical level, as done in a semi-automatic\nway in [2]. For a given onset time we thus get the pitches\nand rests that occur in any of the melodies. By querying\nwith sequences of pitch sets, we abstract from the individ-\nual variations that resulted in the different melodies but\nstill maintain stable pitch requirements.\nIn section 2 we ﬁrst set up a mathematical and com-\nputational framework and deﬁne the mapping of musical\nnotes to mathematical values. In section 3 we discuss mu-\nsically reasonable parameterizations of the framework and\nin section 4 we conduct and evaluate computational exper-\niments with selected parameterizations and compare the\nresults with previous approaches.\n2 THE COMPUTATIONAL FRAMEWORK\nThis research builds on the Earth Mover’s Distance\n(EMD), whose musical application was studied in [7]. It is\nalso an important building block in Typke’s M2M search\nengine, which we used in our ﬁrst approach to folk song\nvariation retrieval. However, instead of combining the on-set and pitch dimensions into one Euclidean ground dis-\ntance, as done in Typke’s contour matching, we compute\nthe EMD on the onset and pitch dimensions separately.\nIn section 2.1 we introduce the reader into the parame-\nters of the EMD. In the following sections we deﬁne mu-\nsically and task speciﬁc values.\n2.1 The Earth Mover’s Distance\nThe EMD is a geometrically motivated transportation dis-\ntance, which deﬁnes a distance between node distribu-\ntions (weighted node sets) where a ground distance is\ngiven. To apply it we must identify two sets of nodes\nand node weights, which for later usage we call query\nsetQ=f(qj;wqj)gandcandidate setP=f(pi;wpi)g:\nBetween the query nodes qjand the candidate nodes pi\nwe draw edges for which we have to deﬁne a reasonable\nground distance dij. (See section 2.2.)\nThe EMD then computes an edge-ﬂow F= [fij]with\nfijbeing the weight ﬂow between between piandqjthat\nminimizes the overall cost\nWORK (P;Q) =mX\ni=1nX\nj=1dijfij; (1)\nwith the constraint that either all weight ﬂows from Pto\nQor vice versa. To get a distance, this work is divided by\nthe total weight ﬂow:\nEMD (P;Q) =WORK (P;Q)Pm\ni=1Pn\nj=1fij: (2)\nWhen the two distributions have unequal weight, the\nEMD might nonetheless be zero. Therefore, the EMD has\nthe property to allow for partial matches . For mathemati-\ncal and computational details see [6].\n2.2 The EMD for Pitch Distribution Sequences\nFor musical distance modelling we make use of the EMD\ntwice and later reason about the parameters. But ﬁrst we\ndeﬁne the term pitch distance in a way that includes also\ndistances between notes and rests.\nLetPbe the combined set of all possible pitches given\nin some representation and rests. To simplify the formu-\nlation in the following we call any element of Pa pitch,\neven if it is a rest. We assume that there is a ground dis-\ntancedpbetween all the elements in P, e.g. the MIDI-\npitch difference and a penalty distance drbetween rests\nand non-rests. Let furthermore be dta distance in musical\ntime, e.g. in quarter notes or seconds.\nApitch distribution is a setWP =f(ewi;pi)gof\nweighted pitches (same onset time). We deﬁne the dis-\ntancedwpbetween two pitch distributions as the EMD be-\ntween the weighted pitches with the ground distance dp.\nAweighted pitch distribution sequence is a sequence\nWPDS =f(bwj;WPj;tj)gof externally weighted ( bwj)\npitch distributions ( WPj) given per onset ( tj). We de-\nﬁne the distance dwpds between two WPDS as theirEMD. We therefore combine the pitch distribution dis-\ntancedwpand the time distance dtinto a ground dis-\ntancedtwp=dwp\bdt. Here\bcan be freely de-\nﬁned: we may scale the single distances and compute the\nEuclidean (p\n(\u000bdwp)2+ (\fdt)2) or manhattan distance\n(\u000bdwp+\fdt). (See section 4.3 for \u000band\fchoices.)\n2.3 From Weighted Melodies to Pitch Distribution Se-\nquences\nIn this section we describe how we can turn a set of\nmelodiesM=f(Mk)ginto a weighted pitch distribu-\ntion sequence. We additionally assume that the melodies\nare given weights W=f(Wk)g, which affect the abso-\nlute and relative weights of the pitches in the pitch dis-\ntributions. We will use Wto formulate different types of\nqueries (AND vs. OR, see section 3.1) and to manually re-\nﬁne a query by putting different emphasis on more stable\ncore variants than on peripheral variants of a query group.\n(We do not make use of different Wkin this paper.)\nFigure 1 . Creating common onsets for a set of melodies:\nLong notes in melodies m1andm2are split wherever an\nonset occurs.\nTo get to collective pitch distributions, we compute the\nset of all time positions at which a note-on or note-off\nevent occurs. (See ﬁgure 1.) For the time span \u0001tjbe-\ntween adjacent time positions we compute a pitch distri-\nbution containing the pitches of the melodies at that po-\nsition.1We construct the pitch distribution weights ewi\nby summing the weights Wkthat each melody Mkcon-\ntributes to a particular pitch i. The external weight bwj\nequals the time span \u0001tj. (See section 3.1.)\nA special case is the construction of a weighted pitch\ndistribution sequence from a single melody. Here each\nnote turns into a pitch distribution that contains just the\nnote’s pitch with weight ew= 1and the external weight of\nthe pitch distribution bwjequals the duration of that note.\n2.4 Inner Group Distances\nAs an application of a concrete distance measure between\na group of melodies and a single melody, we can apply this\nmeasure to single members of the group and the group as\na whole. If the distances of the melodies to the group are\nsmall, then we have a quite narrow query. If on the other\n1Note that by doing this for each position where an event occurs in\nany of the melodies, we get shifts in the weight conﬁguration depending\non the set of all events. The M2M search engine does not do this but al-\nlows to split long notes during preprocessing, which also leads to slightly\ndifferent results. We omit to investigate these issues in section 4.5 be-\ncause we generally project all query notes to the eighth notes metrical\nlevel.hand the distances are quite big, then we either want to\nlook for rather distant variations, or the group is ill deﬁned\nin terms of the distance measure. (See section 4.4 for an\nexample.)\nTo check if a group member belongs to the core of a\ngroup or to its outskirts, we can either directly compute\nthe distance between the member and the whole group or\nremove the member from the group before computing the\ndistance. If we want to get the melody that is most char-\nacteristic for the group, we can take the melody with the\nminimum distance to the whole group. We can also re-\npeatedly remove the member that lies at most at the out-\nskirts until we are left with only melodies that have the\nsame distance to the group.\nIn any case it would be useful to present to the user of\na search engine the resulting group-internal ranking list,\nenabling him to understand the working of the distance\nmeasure and allowing him to modify the query melody\ngroup, e.g. by removing all group melodies above a dis-\ntance threshold.\n3 INSTANCIATIONS OF THE COMPUTATIONAL\nFRAMEWORK\nIn this section we discuss free parameters within the com-\nputational framework in ways that make musical sense\nand help to solve particular tasks.\n3.1 Weights\nIn our application we have the special situation that\nwe match pitch distributions that stem from the query\nmelodies with single pitches of a variant candidate melody\nfrom the database. Without loss of generality we turn a\npitch into a one element pitch distribution with weight\new= 1, as described in section 2.3. Given this normal-\nization, how should the query melodies contribute weight\nto the pitch distribution?\nIf we set the pitch weights ewi\u00151, we make sure in\nthe ﬁrst application of the EMD that the candidate pitch\nis matched with the closest pitch. This is like an OR-\nquery. If we let the ewisum up to 1, we match all pitches at\nthe same time, thus an AND -query. In addition we might\nweight the different query melodies by their importance\nfor the melody group. (See Wkin section 2.3.)\nThe WPDS weights bwjare more restricted: Since we\ntypically want to match the same amount of musical time\n(or small multiples thereof), bwjshould be proportional to\nthe inter-onset-intervals of the WPDS events. This way a\nhalf note in the query can match two quarter notes in the\ncandidate melody.\n3.2 Pitch Distances\nAt the core of the algorithm are pitch distance calcula-\ntions.\nFor the MIDI pitch to MIDI pitch distance we want to\ndiscuss the following setups: 1) dp1is the absolute dif-\nference of the pitch values. 2) dp2is 0 if the pitches arethe same and 1 otherwise. 3) dp3is the octave invariant\nMIDI pitch distance (which has 6 semitones as its max-\nimum value). Expressing dpas the distance within the\nmusical scale would be another option, which we do not\ndiscuss here.\nWe expect that option dp1coupled with an AND-query\nreturns lower distances for candidates that have the same\ncontour as the average melody. dp2counts the pitches that\nmismatch and thus realizes a boolean notion of absolute\npitch stability .dp3ignores octave variations that result\nfrom a too small voice range of the singer whose perfor-\nmance was recorded in the transcription. However it does\nnot catch smaller steps that the singer eventually made to\nreturn to his or her favorite pitch region.\n3.3 Scaling Pitch versus Time\nIn how we combine dwpanddtintodtwp, we deﬁne the\ntime window used for pitch matching. If we put more\nemphasis on dwp, then we match candidate melodies that\nhave similar pitch material with respect to a rather large\ntemporal segment. We thus consider e.g. pitch sequence\n(A B C D) similar to (A C B D).\nIf we put more emphasis on dt, then we match only\ntemporally very close pitches. Variations like turning half\nnotes into two quarter notes or changing from 4/4 to 6/8\nmeasure will not be matched so easily.\nIn the original M2M search engine setup, we used\npitches given in MIDI-pitch and time given in seconds\nor quarter notes. There multiplying the time distance\nwith a factor of \f= 4 and taking the Euclidean distancep\n(\u000bdwp)2+ (\fdt)2did a good job for contour similarity\nsearches. This value might be slightly too large when we\ncompute OR-queries on pitches, because we take the min-\nimum distance, which is generally smaller than the pitch\nto pitch distance in the M2M setup.\n3.4 A Simple Example\nFigure 2 and table 1 illustrate the effects of \u000band\ffor the\nOR-queries in a very simple case. Time is given in quarter\nnotes and pitches in semitones. Each quarter note’s weight\nis 1 and all note duration must ﬂow from the upper melody\nto the chords, with both chords to be completely matched\nwith duration 2.\nFigure 2 . Matching the pitch distribution from the accent\nnotes of 3 melodies (lower staff) with a single melody (up-\nper staff). Purely by chance the query melodies form ma-\njor chords. All ewiare 1 and the bwjare proportional to the\nduration of the notes (OR-query). Different \u000band\fcan\nresult in different matches for upper notes FandC.TheGﬂows in any case to the C major chord and the A\nto the F major chord. The rest depends on the onset scal-\ning factor: If it is large then Fprefers a ‘close mismatch’\nboth in time and pich and matches with the remaining du-\nration of the C major chord. Note Cmatches perfectly. If\nthe pitch scaling factor is high, then Fmatches with the F\nfrom the F major chord and the Csatisﬁes the remaining\nduration of the C major chord. (See table 1.)\nDistance g f c a\ndwp(ceg;x )hq:0 1 hq:0 2\ndwp(cfa;x ) 2 hq:0 0 hq:0\ndt(1;x) hq:0 1 2 3\ndt(2;x) 1 hq:0 1 2\ndt(3;x) 2 1 hq:0 1\ndt(4;x) 3 2 1 hq:0\nd1m1(1;x)hq:0 h:2 2 5\nd1m1(2;x) 1 q:1 1 4\nd1m1(3;x) 4 1 hq:0 h:1\nd1m1(4;x) 5 2 1 q:0\nd4m1(1;x)hq:0 5 h:2 11\nd4m1(2;x) 1 4 q:1 10\nd4m1(3;x) 10 hq:1 0 h:1\nd4m1(4;x) 11 2 1 q:0\nTable 1 . Distances between the melody notes and the\nchords.d\u000bm\f(n;x)denotes Manhatten distance, with\npitch distance factor \u000band time distance factor \fbetween\nchord at onset position nand the melody tone given in\nthe column header. Minimum WORK is achieved when\nchoosing the marked entries to create a complete match. A\nhorqin front of a distance value marks inexpensive edges\nthat the EMD chooses to move weight from the melody to\nthe chords. Marker his valid for the case that we put the\nhalf-note chord weight completely onto the strong beats.\nMarker qis valid for the case that we split each chord into\ntwo chords with quarter-note duration.\n3.5 Optional Transformations\nAs usual when computing distances between musical\nitems, we have to consider additional requirements and\nwhether we can implement them with the parameters of\nthe framework or with additional transformations. Octave\ninvariance could be implemented by adjusting dpor by\nprojecting all notes to the same octave before applying\nany of the pitch dependent distances. Transposition in-\nvariance can be achieved in a brute force manner if the\nnumber of considered transpositions is reasonable limited,\ne.g. when sticking to the set of MIDI pitches. Then we\nsimply transpose the candidate melody as often as neces-\nsary and compute the minimum over the distances of the\ntransposed candidate and the query group. In a similar\nmanner we can handle onset translations when we want to\nﬁnd the best partial match of a query pattern within a can-\ndidate melody. (See [4] for a more explained description\nof how to implement this efﬁciently.)Another option is to abstract from or extend the candi-\ndate melody and/or the query melodies. By splitting large\nnotes into smaller notes to a common grid, as done in the\nq-case of table 1, we emphasize less the exact onset of\nnotes but more its duration, resulting in an equivalence\nclass of queries or melodies that all project to the same\ngrid-values. We can also replace metrically or otherwise\nless important notes with rests or previously occuring im-\nportant notes. Thereby we make sure that the less impor-\ntant mismatches do not count as much as the important\nones. (See [2] for examples on this issue.)\n4 EXPERIMENTS\nIn this section we make use of the computational frame-\nwork and the parameterization discussions from the pre-\nvious section to set up and conduct experiments. We ﬁrst\ndescribe a musicological task and a computational goal.\nThen we analyse an example data set and derive a rea-\nsonable parameterization of the computational framework\nfor this case. We calculate the inner group distances (see\nsection 2.4) and query the database.\n4.1 The Task and Goal\nWe want to develop a tool for folk song researchers for the\npurpose of variation research. Folk song researchers want\nto know which melodies of a collection stem from each\nother. To ﬁnd genetically related items the researchers\nmay ﬁrst group musically related melodies and then check\nby means of other information sources, like historical\nrecords or geographic information, whether inheritance is\nplausible.\nWe translate the ﬁrst part of this research process into\na computational task: for a given group of musically re-\nlated melodies we want to retrieve and present melodies\nfrom a database that are good additional candidate mem-\nbers for that group. Since musicologists (sub-)consciously\nconsider a large, not fully described set of musical fea-\ntures when comparing and classifying melodies, we state\na more modest goal: we want to ﬁnd melodies that follow\nthe notion of stable notes as described in [2] and by this\ncriterion are good group member candidates.\n4.2 Example\nFigure 3 shows the beginnings of six melodies from the\ndatabase, which were properly aligned to study the differ-\nences between these rather similar melodies. When we\nfollow the lyrics further, we see that variants 1, 3 and\n5 (OGL41101m, OGL36012m, OGL33006m) start each\nodd verse with the same text, like the ﬁrst verse in bars\n1–2 and continue in the even verses differently.\nMusically we can identify a core subgroup consisting\nof items 1, 3, 5 and maybe 4. In this subgroup each\nverse starts quite similar in its ﬁrst bar and shows more\ndifferences in the second bar. Also we note that metri-\ncally strong notes are typically more stable accross vari-\nants (see [2] for more evidence on this statement). Theremaining two melodies share fewer pitches with the core\ngroup but are metrically, harmonically and textually very\nsimilar. But from just the pitch stability point of view we\nmight want to exclude these melodies from the group.\n4.3 Chosen Parameterizations\nIn a pre-experiment we compute the inner-group distances\nas described in section 2.4. We express musical time in\nquarter notes and pitches in MIDI pitch numbers. We de-\nﬁne the distance between a MIDI pitch and a rest as 1 and\ndeﬁne the distance between MIDI pitches as the absolute\ndifferencedp1.\nTo compare results, we project all melodies to the 8th-\nnote metrical level and thus abstract from mainly textu-\nally motivated rhythmic variations. (See [5], ﬁle Moeder-\nAligned.pdf.) For the weighting between pitch and time\ndimensions we choose \u000band\f(see section 2.2) from\nf0;1;4g. For\f= 1 the results are most intuitive (see\nsection 3.4). 4 allows us to compare our search results\nwith those of our previous M2M search engine.\nIn section 4.4 we compare the different inner-group\ndistance results for different \u000band\fvalues. For each\nsetting we compute 4 distances: a) With or without the\ncandidate included in the query group. b) Using AND- or\nOR scheme pitch weights. (For results see [5], ﬁle Re-\nsultsMIDIPitchDistance.)\n4.4 Inner-Group Distances\nIn this section we discuss the results for different distance\nmeasures and \u000band\fvalues. We use the following ter-\nminology: When a melody scores high, its distance to the\ngroup is rather low and we call it a good match . When a\npitch does not match it means that no other member of the\ngroup has the same pitch at that onset position.\nSetting\u000b= 0 and\f >= 0 ignores pitch. Because of\nthe splitting of notes according to the 8th-note grid dura-\ntion, there are no rhythmical differences, so the results are\nnot meaningful (all zero).\nFigure 3 . Beginnings of six manually aligned melodies\nfrom the collection Onder de Groene Linde . [3] The lyrics\ncorrespond to the staves. Please see the remaining bars at\n[5], ﬁle MoederLargeOGLFirst.pdf.For\u000b > 0and\f= 0 we actually compare\npitch histograms for the entire melodies. OGL36012m\nand OGL41101m score high in OR-queries, while\nOGL35003m is rather distant, probably because of un-\nmatched ( D,E,F# )-phrases in the beginning of odd\nverses. In AND-queries, OGL33006 performs best, which\nmeans, it is most average in pitch.\nFor\u000b= 1 and\f= 1 OGL36012m scores best\nin OR-queries and can be called the most representa-\ntive melody. OGL41101m is closest to the average pitch\ncontour. OGL28602m is most distant in both cases,\nwhich corresponds nicely with an expert’s decision to\nput OGL28602m into a different subgroup than the other\nmelodies.\nFor\u000b= 1and\f= 4we penalize pitch distances much\nmore. As a result in AND- and OR-queries, OGL37102m\nis more distant than OGL28602m, which might result\nfrom unmatched pitches in the end of bar 1 and begin-\nning of bar 3, which are 7 semitones apart from the other\nmelodies. We think that this is overweighted or should\nbe corrected with a pitch distance measure that takes the\nharmonic context into account.\nFor\u000b= 4 and\f= 1 pitch differences are\nweighted high, but we allow to match with neighbor\npitches. OGL33006m and OGL36012m are close to\nthe group in OR-queries, additionally OGL41101m again\nscores best in AND-queries. Worst are OGL35003m and\nOGL28602m, where the former scores over the latter in\nOR-queries and vice versa in AND-queries, again proba-\nbly because of the unmatched ( D,E,F# )-phrases.\n4.5 Finding Other Candidates\nIn this section we compare the performance of the group-\ndistance approach with our previous setup, which allowed\na M2M query. In a recently established ‘ground truth’\nby our expert, there is a so called melody norm2Moeder\nwhich includes our group members and additional items\nfrom the database. By constructing ranking lists, we eval-\nuate qualitatively, how useful our method is in showing\nthese items in top positions (small rank numbers).\nIn a short experiment with the database we restrict\nourselves to OR-queries with the following setup: \u000b=\n1;\f= 1;dp=dp1. We do not perform transpositions and\ntime stretching, because the general guideline of the tran-\nscriptions of this database was to transcribe it in tonality\nG (any mode).\nOur test collection contains 1443Dutch folk songs\nfrom [3], and 68 unrelated additional popular ring tone\nmelodies. The database contains both complete MIDI ﬁles\nand each of the verses of the folk songs separately, be-\ncause we look also for melodies that match just in one\nverse with the query. These kind of partial matches are\nsupported by the EMD. In total there are 212 melodies\n2Within the Meertens Institute the concept of melody norm is used\nto group historically or ‘genetically’ related melodies. If there is not\nenough historical evidence, such as in this case, then melodies may be\nattached to a melody norm on the basis of convincing melodic similarity.\n3Three more than used in other ISMIR 2007 publications.broken into 804 items. Independent from our system our\nexpert assigned 13 melodies to the melody norm Moeder ,\nincluding the 6 query melodies. She created a subgroup\n‘1’ with 11 of the melodies and put the remaining two\nmelodies (OGL28602 and OGL33112) into subgroup ‘2’.\nOur query results (see [5], ﬁles MoederRanksShort and\nMoederResults) are very promising:\nThe ﬁrst 8 items found in the ranking list, including\nthe 6 query melodies, are actual members of the melody\nnorm. The next members are found on ranks 10, 14 and\n15. OGL33112, which is part of subgroup 2, is found at\nrank 22 (of 212). We did not ﬁnd OGL25309 at the top\nof the ranking list. It is also part of the melody norm but\nnotated in quarter notes instead of eighth notes.\nWe compare this ﬁndings with the result of a single\nM2M query with OGL36012m as the best representative\nof the query group. Here besides OGL25309 we miss 3\nitems (including OGL33112) in the top ranks. So our\nG2M appears to work much better than M2M. (See [5],\nﬁles OGL36012mRanksShort and OGL36012mResults\nand ﬁgure 4.)\nFigure 4 . Number of found melodies by category (query\nitem, melody norm item or false hit) counted up to a\ngiven rank, shown for M2M and G2M separately (on\ndifferent scales). Only the ﬁrst hit of an item is con-\nsidered in the ranking list. Only melody norm items\nthat are not part of the query group are included in the\nmelody norm category. M2M: 6 melody norm melodies\nare not found among the ﬁrst 20 ranks (melodies at\nranks 195 and 200 are not shown). G2M: 2 melody\nnorm melodies are not found among the ﬁrst 20 ranks\n(melody at rank 87 is not shown). The complete data\ncan be found at [5], ﬁles MoederResultCategoryCount and\nOGL36012mResultCategoryCount.\n5 CONCLUSION AND FUTURE WORK\nWe have implemented and tested the method G2M for\nﬁnding in a database (unclassiﬁed) variants, given a group\nof melodies that are known variants of each other. In our\nevaluation we found with this method a more complete set\nof true positives and smaller number of disturbing false\npositives in the top of a ranking list than with our previous\nsearch method M2M, with which Typke won the MIREX\n2006 Symbolic Similarity Contest. The results are partic-\nularly very promising, because we got there only by rea-\nsoning about the free parameters and without manual or\nautomatic ﬁne-tuning.For the future, a more thorough quantitative evaluation\nof different cases and different parameterization options\nremains to be done. We will also compare the group-query\nresults with the ranking that we get with a linear combi-\nnation of the distances between any member of the group\nand the candidates. We expect that the former works bet-\nter with harmonically stable aspects while the latter works\nbetter when contour is important in the variant group. To\nfurther improve on harmonic generalization from pitches,\nwe consider to apply automatic harmonic analysis. [2]\nFrom the user interface point of view we will study,\nwhether a sort of relevance feedback approach is possible\nto support the incremental interactive shaping of variant\ngroups. There may be an initial M2M query, for which\na ranked list is returned. In this list, the user indicates\ngood hits, which we (semi-)automatically align and use as\na G2M query, with new group candidates. Finally, we will\nelaborate a query formulation and reﬁnement process and\nallow users to express pitch and timing wildcards.\n6 ACKNOWLEDGEMENTS\nThis work was kindly supported by the Nether-\nlands Organization for Scientiﬁc Research within the\nWITCHCRAFT project NWO 640-003-501, which is part\nof the program Continuous Access to Cultural Heritage .\nWe thank Ellen van der Grijn for the melody norm ground\ntruth.\n7 REFERENCES\n[1] Bronson, B.H., ”Melodic Stability in Oral Transmis-\nsion”, In: Journal of the International Folk Music\nCouncil, V ol. 3, 1951, pp. 50-55, 1951.\n[2] Garbers, J., V olk, A., van Kranenburg, P., Wiering, F.,\nGrijp, L., Veltkamp, R. C., ”On pitch and chord stabil-\nity in folk song variation retrieval”, In: Proceedings of\nthe First International Conference of the Society for\nMathematics and Computation in Music (to be pub-\nlished, [5], ﬁle GarbersMCM2007.pdf), 2007.\n[3] ”Onder de groene linde. Verhalende liederen uit de\nmondelinge overlevering”, Uitgeverij Uniepers, Am-\nsterdam, 1987-1991.\n[4] Lubiw, A., Tanur, L. ”Pattern Matching in Poly-\nphonic Music as a Weighted Geometric Translation\nProblem”, In: Proceedings of the 5th International\nConference on Music Information Retrieval, 2004.\nAlso http://ismir2004.ismir.net/proceedings/p054-\npage-289-paper154.pdf.\n[5] Garbers, J., ”Additional Resources for ISMIR 2007”,\nhttp://www.cs.uu.nl/people/garbers/ISMIR2007/.\n[6] Rubner, Y ., Tomasi, C., Guibas, L. J. ”The Earth\nMover’s Distance as a Metric for Image Retrieval”, In:\nInternational Journal of Computer Vision, vol. 40, no.\n2, pp. 99–121, 2000.\n[7] Typke, R. Music Retrieval Based on Melodic Similar-\nity. PhD thesis, Utrecht University, 2007."
    },
    {
        "title": "A Symmetry Based Approach for Musical Tonality Analysis.",
        "author": [
            "Gabriel Gatzsche",
            "Markus Mehnert",
            "David Gatzsche",
            "Karlheinz Brandenburg"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416214",
        "url": "https://doi.org/10.5281/zenodo.1416214",
        "ee": "https://zenodo.org/records/1416214/files/GatzscheMGB07.pdf",
        "abstract": "We present a geometric approach for tonality analysis called symmetry model. To derive the symmetry model, Carol L. Krumhansl and E.J. Kessler’s toroidal Multi Dimensional Scaling (MDS) solution is separated into a key spanning and a key related component. While the key spanning component represents relationships between different keys, the key related component is suitable for the analysis of inner relationships of diatonic keys, for example tension or resolution tendencies, or functional relationships. These features are directly related to the symmetric organisation of tones around the tonal center, which is particularly visualized by the key related component. 1 INTRODUCTION Tonality is the basis of western tonal music. Tonality comprises several aspects like stability of tones within a given tonal context, aspects of consonance or dissonance, aesthetic properties of a given musical piece, or the prediction of tensions or resolution. Tonality also helps to explain the development and usage of chords and keys. To this day a plenty of theories have been developed to explain these different aspects of tonality. An intuitive and unified theory of tonality is required that not only supports the development of extended music information retrieval methods, e.g. chord and key recognition, transcription and similarity estimation, but also helps to understand the way the human auditory system processes tonal information. 2 RELATED WORK The Analysis of musical tonality generally operates at three stages: 1. Frequency analysis, signal transformation, 2. Complexity, irrelevancy, redundancy reduction, 3. the analysis of the preprocessed audio signal by menans of a tonality model. The present publication is exclusively devoted to tonality models. For this aspects of frequency transformation and c⃝2007 Austrian Computer Society (OCG). preprocessing (e.g. transient location, noise reduction, consonance filtering, pitch tracking, horizontal segmentation, dimensionality reduction) shall not be regarded in more detail. The Description of musical tonality with geometric tonality models has a long tradition. Early approaches are for example Heinichen’s (1728) or Kellner’s (1737) regional circles, the harmonic network proposed by Leonhard Euler (1739), and Weber’s (1767) regional chart [6, p.43]. Known as cirle of fifth (Kellner’s regional circle), Riemann’s “Tonnetz” (Euler’s harmonic network) or Schönberg’s chart of key regions (Webers regional chart), these models are of great interest till this day. In the meantime, advanced geometric tonality models have been developed. Roger Shepard [12] proposes several helix models, which primarily describe aspects of octave equivalence or fifth and chroma proximity. Elaine Chew [2] proposes a so called Spiral Array. The model’s core is the, harmonic network inspired, geometric arrangement of pitches on a spiral. The great breakthrough of Chew’s model is an unified description of the relationship between tones, chords and keys within one model and the observation of functional relationships that build a tonal center. Fred Lerdahl’s [6] “diatonic space” consists of “basic space”, “chordal space” and “regional space”. These spaces help to model different aspects of tonality. While “basic space” describes the relationships between different tonal hierarchies (octave, fifth, triadic, diatonic and chromatic), “chordal space” is specialized to model tonal relationships between chords (chord proximity, chord progressions, ...) and “regional space” helps to describe tonal relationships between keys (e.g. aspects of modulation). Dmitri Tymoczko [14] represents musical chords in a geometric space called “orbifold space”. The mapping of the notes from one chord to those of another are represented with the help of line segments . The similarity of chords is represented by the length of these line segments. Aline Honingh [3] introduces the property of star convexity to describe and visualize the principle of shapeliness of tonal pitch structures. Another relevant geometric tonality model is Krumhansl and Kessler’s [5] four dimensional MDS solution which is subject of the next chapter in which the symmetry model is derived. 3 THE SYMMETRY MODEL Like Elaine Chew’s Spiral Array [2], the symmetry model is a geometric tonality model. A particular feature is the organisation of tones in a way that tonal symmetries within western tonal music become apparent. The model differentiates between key related 1 and key spanning tonal phenomena 2 and additionally reveals the relationship between tones, major and minor chords and keys. There are different ways to derive the symmetry model 3 , but to provide evidence for the close relationship between the symmetry model’s output and the psychological reality, we derive the symmetry model from Carol L. Krumhansl and E.J.Kessler’s pure cognition inspired MDS solution [5].",
        "zenodo_id": 1416214,
        "dblp_key": "conf/ismir/GatzscheMGB07",
        "keywords": [
            "tonality",
            "symmetry model",
            "tonal symmetries",
            "Carol L. Krumhansl",
            "E.J. Kessler",
            "toroidal Multi Dimensional Scaling (MDS)",
            "key spanning component",
            "key related component",
            "inner relationships of diatonic keys",
            "functional relationships"
        ],
        "content": "A SYMMETRY BASED APPROACH FOR MUSICAL TONALITY\nANALYSIS\nG. Gatzsche\nFraunhofer IDMT\nIlmenauM. Mehnert\nTechnische Universität\nIlmenauD. Gatzsche\nHochschule für Musik\nFranz Liszt WeimarK. Brandenburg\nTechnische Universität\nIlmenau\nABSTRACT\nWe present a geometric approach for tonality analy-\nsis called symmetry model. To derive the symmetry\nmodel, Carol L. Krumhansl and E.J. Kessler’s toroidal\nMulti Dimensional Scaling (MDS) solution is sepa-\nrated into a key spanning and a key related com-\nponent. While the key spanning component repre-\nsents relationships between di ﬀerent keys, the key re-\nlated component is suitable for the analysis of inner\nrelationships of diatonic keys, for example tension\nor resolution tendencies, or functional relationships.\nThese features are directly related to the symmetric\norganisation of tones around the tonal center, which\nis particularly visualized by the key related compo-\nnent.\n1 INTRODUCTION\nTonality is the basis of western tonal music. Tonal-\nity comprises several aspects like stability of tones\nwithin a given tonal context, aspects of consonance\nor dissonance, aesthetic properties of a given musi-\ncal piece, or the prediction of tensions or resolution.\nTonality also helps to explain the development and\nusage of chords and keys. To this day a plenty of\ntheories have been developed to explain these dif-\nferent aspects of tonality. An intuitive and uniﬁed\ntheory of tonality is required that not only supports\nthe development of extended music information re-\ntrieval methods, e.g. chord and key recognition, tran-\nscription and similarity estimation, but also helps to\nunderstand the way the human auditory system pro-\ncesses tonal information.\n2 RELATED WORK\nThe Analysis of musical tonality generally operates\nat three stages: 1. Frequency analysis, signal trans-\nformation, 2. Complexity, irrelevancy, redundancy\nreduction, 3. the analysis of the preprocessed audio\nsignal by menans of a tonality model. The present\npublication is exclusively devoted to tonality mod-\nels. For this aspects of frequency transformation and\nc/circlecopyrt2007 Austrian Computer Society (OCG).preprocessing (e.g. transient location, noise reduc-\ntion, consonance ﬁltering, pitch tracking, horizontal\nsegmentation, dimensionality reduction) shall not be\nregarded in more detail. The Description of mu-\nsical tonality with geometric tonality models has a\nlong tradition. Early approaches are for example\nHeinichen’s (1728) or Kellner’s (1737) regional cir-\ncles, the harmonic network proposed by Leonhard\nEuler (1739), and Weber’s (1767) regional chart [ 6,\np.43]. Known as cirle of ﬁfth (Kellner’s regional\ncircle), Riemann’s “Tonnetz” (Euler’s harmonic net-\nwork) or Schönberg’s chart of key regions (Webers\nregional chart), these models are of great interest till\nthis day. In the meantime, advanced geometric tonal-\nity models have been developed. Roger Shepard\n[12] proposes several helix models, which primarily\ndescribe aspects of octave equivalence or ﬁfth and\nchroma proximity. Elaine Chew [ 2] proposes a so\ncalled Spiral Array. The model’s core is the, har-\nmonic network inspired, geometric arrangement of\npitches on a spiral. The great breakthrough of Chew’s\nmodel is an uniﬁed description of the relationship be-\ntween tones, chords and keys within one model and\nthe observation of functional relationships that build\na tonal center. Fred Lerdahl’s [ 6] “diatonic space”\nconsists of “basic space”, “chordal space” and “re-\ngional space”. These spaces help to model di ﬀerent\naspects of tonality. While “basic space” describes the\nrelationships between di ﬀerent tonal hierarchies (oc-\ntave, ﬁfth, triadic, diatonic and chromatic), “chordal\nspace” is specialized to model tonal relationships be-\ntween chords (chord proximity, chord progressions,\n...) and “regional space” helps to describe tonal rela-\ntionships between keys (e.g. aspects of modulation).\nDmitri Tymoczko [ 14] represents musical chords in\na geometric space called “orbifold space”. The map-\nping of the notes from one chord to those of another\nare represented with the help of line segments . The\nsimilarity of chords is represented by the length of\nthese line segments. Aline Honingh [ 3] introduces\nthe property of star convexity to describe and visu-\nalize the principle of shapeliness of tonal pitch struc-\ntures. Another relevant geometric tonality model is\nKrumhansl and Kessler’s [ 5] four dimensional MDS\nsolution which is subject of the next chapter in which\nthe symmetry model is derived.3 THE SYMMETRY MODEL\nLike Elaine Chew’s Spiral Array [ 2], the symmetry\nmodel is a geometric tonality model. A particular\nfeature is the organisation of tones in a way that to-\nnal symmetries within western tonal music become\napparent. The model di ﬀerentiates between key re-\nlated1and key spanning tonal phenomena2and\nadditionally reveals the relationship between tones,\nmajor and minor chords and keys. There are dif-\nferent ways to derive the symmetry model3, but to\nprovide evidence for the close relationship between\nthe symmetry model’s output and the psychological\nreality, we derive the symmetry model from Carol L.\nKrumhansl and E.J.Kessler’s pure cognition inspired\nMDS solution [ 5].\n3.1 Cognition inspired derivation\nC.L. Krumhansl and E.J. Kessler [ 4] correlated the\nprobe tone ratings4of 12 major and 12 minor keys\nand derived a similarity measure for every possible\npair of keys. These data again had been fed into\na Multi Dimensional Scaling analysis, resulting in a\nfour dimensional spacial arrangement of keys (Fig-\nure1). The distance between two objects within the\nMDS space represents the perceived similarity of two\nkeys.\na)\ncGgG\nE\nAfGFB\nbgBIcEIFGDI\neGAI\nDdG bI\naf\nCdb)\ngze@IDMT000841cG\nafgG\nE\nA\nfGdFDICe\nB\nbG\ng\nBIc AI\nEI\nDdG\nbI FG\nFigure 1 . a) Dimensions 1 and 2 and b) dimensions 3\nand 4 of Krumhansl and Kessler’s four dimensional\nMDS solution [ 5, p.43].\nWithin the ﬁrst two dimensions (Figure 1a) two\ncircles of ﬁfth result, one representing the major and\nanother representing the minor keys. Within the sec-\nond two dimensions (Figure 1b) 4 groups of 3 major\nand 4 groups of 3 minor keys are generated, primar-\nily representing parallel and relative relationships be-\ntween major and minor keys. The ﬁrst step in deriv-\ning the symmetry model from the MDS solution is the\n1e.g. the resolution tendencies of the dominant seventh\n2e.g. the parallel relationship between major and minor keys\n3e.g. from music theory (Hugo Riemanns Harmonic Network [ 10])\nor from Hendrik Purwins SOM based tonal representation [ 9]\n4The probe tone ratings provide a measure how good each of the\n12 chromatic pitch classes ﬁts into a given key.development of the spiral of thirds : The angles of the\nkeys within the ﬁrst two dimensions are represented\non the z-axis and the circle within the last two di-\nmensions is represented on the xy-plane5. The two\nresulting spirals of ﬁfth are conﬂated in such a way,\nthat a single spiral composed of major and minor\nthirds evolves (Figure 2a). Cutting out one winding\nof the spiral of thirds results in a subspace containing\n8 keys. The roots of these keys again form a diatonic\nset (Figure 2 a). This set contains one pitch class twice,\nthat is the pitch class forming the start (d)6and the\nend (D). Furthermore it can be denoted that also the\ngeometric center of the spiral winding (that is be-\ntween C and e) is occupied by the same pitch class\nforming the start and the end of the spiral winding.\nWe call this “invisible” – for reference added – pitch\nclass symmetry tone (˜d) because the whole diatonic set\nis symmetrically arranged around that tone7(Figure\n2b,c).\na\ndFC e\nbG\nDdF aC\nbeGD\ngze@IDMT000600a) b) d~\nC e\na G\nF b\ndDc) d~\nFigure 2 . The key related circle of thirds evolves\nfrom dimensions 3 and 4 of the MDS solution. An\ninteresting ﬁnding is the symmetric organisation of\nthe diatonic set around the symmetry tone ˜d.\nFigure 2 c shows a spiral winding where both ends\nare closed. This conﬁguration is called key related\ncircle of thirds TR . Within dimensions 1 and 2 the con-\nﬂation of the two spiral of ﬁfth results in a conﬁgura-\ntion of alternating major and minor thirds, called key\nspanning circle of thirds8T(Figure 3 ). This conﬁgura-\ntion represents the major-minor relationship between\nkeys in a more interpretable way than the original so-\nlution does: It is di ﬃcult to interpret the generation\nof major-minor key pairs like C-Major and d-Minor\nwithin dimensions 1 and 2 of the original solution.\nWithin the key related circle of thirds, the very impor-\ntant relative major /minor relationship is emphasized\n(e.g. C-Major /a-Minor).\n5Within dimensions 1,2 and 3,4 the keys are arranged on a circle\nwhich makes it possible to identify every key by a angle related to\nthe circles origin.\n6Capital letters like “D” represent major keys and chords, small\nletters like “d” represent minor keys and chords, letters with a\ntilde like “ ˜d” represent symmetry tones.\n7The semitone distance between the symmetry tone and the tones\nto right of the symmetry tone are the same like that’s of the tones\nto the left.\n8Also known as Hauptmann’s line-of-thirds[ 7].3.2 Interpreting the symmetry model\nAccording to Albert S. Bregman [ 1, p.124], the cru-\ncial issue in dealing with MDS solutions is to assign\n“meaningful properties” to the “mathematically de-\nrived dimensions”. A ﬁrst step towards that direc-\ntion shall be done here: Actually the MDS solution\nis a low dimensional space composed of two circu-\nlar dimensions. The ﬁrst circular dimension ( Fig-\nure 1 a) can be interpreted as a key spanning compo-\nnent. Within the symmetry model it is represented\nby the key spanning circle of thirds T(Figure 3 ) which\ncontains the two circles of ﬁfth of the original MDS\nsolution in a slightly contorted way. The second cir-\ncular dimension ( Figure 1 b) can be interpreted as a\nkey related component , represented by the key related\ncircle of thirds TR(Figure 2 c). The key related cir-\ncle of thirds geometrically outlines functional relation-\nships : Tones to the left of the symmetry axis form sub-\ndominant9chords (d-F-a, F-a-C), those to the right\ndominant chords (e-G-b, G-b-D) and those in the cen-\nter tonic chords (a-C-e, C-e-G)(Figure 2b). Opposed\nto the symmetry tone, the diminished chord (b-D/d-f)\nappears, whereas a counter clockwise extension re-\nsults in the dominant seventh chord (G-b-d /D-f) and\na clockwise extension lets the sixte ajoutée (d-F-a-b)\nevolve. The resolution tendencies towards the tonic\nof that chords can be explained with the fact that\nthese chords unambiguously deﬁne a spiral winding\nand therefore a tonal center.\na) b)\ngze@IDMT000599D/c98E/c98gB/c98dFC eG\nb\nD\nf/c35\nA\nc/c35\nE\ng/c35\nBd/c35F/c35b/c98fA/c98c\nG/c98e/c98a\ncG\nafgG\nE\nA\nfGdFDICe\nB\nbG\ng\nBIcAI\nEI\nDdG\nbI\nFG\na/c35C/c35\nFigure 3 . A part of the key spanning circle of thirds.\n3.3 A ﬁrst application\nThe angle of a tone within the key related circle of\nthirds αTRcan be taken as a measure for a tone’s\nstability in a given tonal context. TRis composed\nof an underlying grid of 24 semi tones. To every of\nthese semitones we assign a base index m :\n−24\n2≤m<24\n2,m∈Z (1)\n9In 1726 Rameau introduced the terms tonic, dominant and sub-\ndominant to describe emotional aspects of modulations. Dom-\ninantic motions where linked to warmth, light, happiness, sub-\ndominant motions where characterized to be cold, dark, sad [ 11,\np.68]. Rameau’s descriptive framework became the basis of Hugo\nRiemann’s “Funktionstheorie” [ 10].The base index mis chosen such that it numerically\nrepresents real interval distances of tones and addi-\ntionally outlines the symmetric organisation of the\ndiatonic set around the symmetry tone ( Figure 2 ):\nThe base index m=0 is assigned to the symmetry\ntone ( ˜d), values m<0 are assigned to tones to the left\nof the symmetry tone, values m>0 are assigned to\ntones right of the symmetry tone ( Table 1 ).\nd F a C e G b D\nm -12 -9 -5 -2+2+5+9+12\nTable 1 . The assignment of the base index mto the\ndiatonic set of C-Major\nSo the angle αTRof a tone within the key related\ncircle of thirds can be calculated as follows:\nαTR(m)=2π∗m\n24(2)\nWith the help of these equations the stability sof an\ngiven real tone can be calculated as follows:\ns=1.0− |αTR(m)\nπ| (3)\nFigure 4 shows the result of this prediction in com-\nparison to Krumhansl and Kessler’s probe tone rat-\nings [ 5, Table 2.1]. It is important to denote, that the\nsymmetry model to this point considers the major\nand minor mode simultaneously10. To allow a com-\nparison of the probe tone proﬁles with the symmetry\nmodel’s prediction, the proﬁles have to be summed.\nFurthermore the calculated stability curve is slightly\nshifted to the right. This can be explained due to the\nfact that the symmetry model totally eliminates the\nroot relationship.\n4 RESULTS\nThe spiral of thirds has been shown as a three dimen-\nsional representation of Krumhansl and Kessler’s\nfour dimensional MDS space. With the spiral’s help,\nthekey related and key spanning circle of thirds have\nbeen identiﬁed as important subspaces of the MDS\nsolution. It has been outlined that the generation of\natonal center can be explained with the symmetric or-\nganisation of tonal objects around a so called symmetry\ntone.Functional relationships between tones and the\ntonal stability of a tone within a given tonal context\ncan be explained with the tone’s relation to the to-\nnal center, which is not – like traditionally done –\nrepresented by the key’s root but by it’s symmetry\ntone. The dominant seventh chord has been shown as\nevolving from the 2D representation of one wind-\ning of the spiral of thirds. The chord’s key deﬁning\n10To predict the exact tension and resolution values of distinct\nmodes, the symmetry model has to be extended by laws that\ndescribe the polarisation of music towards major or minor.g\nbdfac\neg\nbdf\ngbdfa\nce\ngbdfg\nbdfac\ne\ng\nbdfg\nb\ndface\ng\nb\ndf\nProbe tone ratings for C-Major\nProbe tone ratings for a-Minor\nSummation of the major/minor ratings\nTonal stability s predicted by the symmetry model\ngze@IDMT000448Probe tone ratings for C-Major\nProbe tone ratings for a-Minor\nSummation of the major/minor ratings\nThe symmetry model’s prediction (s)Figure 4 . Krumhansl and Kessler’s probe tone rat-\nings in comparison with the symmetry model’s pre-\ndiction.\nfunction and it’s resolution tendencies towards the\ntonic haven been explained with the fact, that the di-\nminished seventh unambiguously deﬁnes a “spiral\nwinding” which again deﬁnes a tonal center.\n5 CONCLUSION\nKrumhansl and Kessler’s pure cognition inspired\nMDS solution does not only represent relationships\nbetween keys. It also provides strong evidence, that\nthe generation of major as well minor chords, the\nfactors that generate a tonal center, aspects of musi-\ncal consonance and dissonance, as well as functional\nrelationships have it’s origin in the neuronal self or-\nganisation of tonal events. The symmetry model pro-\nvides an analytic description of these relationships\nand separates the MDS solution in semantic relevant\nsubspaces. Therefore the symmetry model is predes-\ntinated for musical feature analysis, which can com-\nprise of chord and key ﬁnding, tension and resolution\nanalysis or tonal ﬁngerprint estimation. The close\nrelationship to Krumhansl and Kessler’s pure cog-\nnition inspired MDS solution shows, that the model\ndescribes important aspects of the psychological re-\nality. Next steps will be to investigate, if there are\nmore semantic properties encoded within the sym-\nmetry model than regarded to date. Algorithms have\nto be developed, that extract the structural relation-\nships – represented by the symmetry model – from\nreal audio. Another necessary step is to combine\nthe pure “pitch chroma” based model with approa-\nches that incorporate aspects of “pitch height”, e.g.\nthe principle of root relationship. This can be for\nexample by incorporating psychoacoustical approa-\nches like Ernst Terhardt’s virtual pitch theory [ 13] or\nRichard Parncutt’s theory of harmony [ 8].6 REFERENCES\n[1] B , Albert S.: Auditory scene analysis .\nCambridge Mass. : MIT Press, 2001. – ISBN\n0–2620–2297–4\n[2] C, Elaine: Towards a Mathematical Model of\nTonality , Massachusetts Institute of Technology,\nDiss., 2000\n[3] H , Aline: The Origin and Well-Formedness\nof Tonal Pitch Structures , Institue for Logic, Lan-\nguage and Computation, Diss., 2006\n[4] K , C. ; K  , E.: Tracing the Dy-\nnamic Changes in Perceived Tonal Organization\nin a Spatial Representation of Musical Keys. In:\nPsychological Review (1982), Nr. 89, S. 334–368\n[5] K , Carol L.: Cognitive foundations of\nmusical pitch . Oxford psychology series; no.17.\nOxford University Press, 1990. – ISBN 0–19–\n505475–X\n[6] L , Fred: Tonal pitch space . Oxford : Ox-\nford University Press, 2001. – ISBN 0–1950–\n5834–8\n[7] M , Jose-Antonio: Organization of pitch-\nclass space according to the medieval notion of\nintervallic a ﬃnities. In: MCM2007 – First Inter-\nnational Conference of the Society for Mathematics\nand Computation in Music , 2007\n[8] P , Richard: Harmony: a psychoacousti-\ncal approach . Springer series in information sci-\nences. Berlin : Springer-Verl., 1989. – ISBN\n3540512799\n[9] P , Hendrik: Proﬁles of Pitch Classes; Cir-\ncularity of Relative Pitch and Key - Experiments,\nModels, Computational; Music Analysis, and Per-\nspectives , Technical University of Berlin, Diss.,\n2005\n[10] R , Hugo: Handbuch der Harmonielehre . 4.\nAuﬂage. Breitkopf und Härtel Leipzig, 1906\n[11] S¨ , Arnold: Harmonielehre . Fotomech-\nanischer Nachdruck der 3. Auﬂage Wien 1922.\nEdition Peters, 1977\n[12] S , Roger N.: Geometrical approxima-\ntions to the structure of musical pitch. In: Psy-\nchological Review (1982), Nr. 89, S. 305–333\n[13] T , Ernst: Akustische Kommunikation .\nBerlin : Springer, 1998. – ISBN 3540634088\n[14] T , Dmitri: The Geometry of Musical\nChords. In: Science (2006), Nr. 313, S. 72–74"
    },
    {
        "title": "Tool Play Live: Dealing with Ambiguity in Artist Similarity Mining from the Web.",
        "author": [
            "Gijs Geleijnse",
            "Jan H. M. Korst"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416430",
        "url": "https://doi.org/10.5281/zenodo.1416430",
        "ee": "https://zenodo.org/records/1416430/files/GeleijnseK07.pdf",
        "abstract": "As methods in artist similarity identification using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set. We notice that ambiguous artist names lead to unsatisfying results. We present a simple, efficient and unsupervised method to deal with ambiguous artist names. 1 INTRODUCTION Web Music Information Retrieval (Web-MIR) is a novel and promising field of research. Recently, excellent performances (with precision rates around 90%) are reported on artist genre classification and artist similarity identification using dedicated test sets [2, 1, 3]. In this work, we focus on the task of identifying and scoring artist similarities using web data. Although our method performs well on two commonly used test sets, the results on a more realistic collection of artists are less encouraging. Contrary to the two common benchmark sets for Web-MIR [2, 3], this set of artists contains ambiguous names of less famous artists. When querying for artist names, within the search results the ambiguous name Nirvana can be expected to refer to the band. However, the meaning of Play – also a band in our collection – is less obvious. We observe that such ambiguous artist names tend to be found similar to a large number of artists. Especially for lesser known artists, this leads to unsatisfying results. We therefore present an unsupervised method to deal with the phenomenon of ambiguous artist names, making the method more robust and suited for realistic tasks. 2 FINDING ARTIST SIMILARITIES Using two efficient methods introduced in earlier work [1], we compute the measure of similarity T(a, b) of artist a to another artist b using co-occurrences on the web. With PM, we extract artist names from search engine snippets after querying with relation patterns, where with DM we scan full documents for artist names. We use a set of 224 c⃝2007 Austrian Computer Society (OCG). 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 5 10 15 20 25 30 precision k Precision for k-NN Artist Similarity pm-224 pm-1995 dm-224 dm-1995 Figure 1. Precision for the sets of 224 and 1995 artists. artists, equally divided over 14 genres [2] and a large set of 1995 artists divided over 9 genres [3]. We consider two artists to be similar, if they share a genre in the test set. Figure 1 shows the average precision of the similarity of the artists and their k-NN. We can conclude that the pattern based method gives good results and outperforms DM in both sets. The experiments show that PM both outperforms DM and is less time consuming. Hence, we applied PM to a collection of 1732 artists, used in music recommender experiments. Since no ground truth is available for this collection of artist, we cannot evaluate the precision. We observe that the names that often not refer to the intended artist frequently occur amidst the most related artists. Although the queried expressions contain an artist name by construction, the retrieved snippets do not always handle musical artists and their relations. 3 DEALING WITH AMBIGUITY Ideally, for each occurrence of an artist name in a text we want to observe whether the occurrence indeed reflects the intended artist. However, the automatic parsing of sentences and term recognition is troublesome as the snippets contain broken sentences and may be multilingual. Moreover if an artist name is identified as a subject or object within a sentence, then we still do not know whether the term indeed reflects the artist. We therefore aim for an unsupervised method where we estimate the probability that a term a indeed reflects the intended artist named a. If we know the probability p(a) that a reflects the artist, we can redefine the artist similarity function T as follows. T ′(a, b) = co′(a, b) P c,c̸=b co′(c, b) (1) with co′(a, b) = co(a, b) · p(a) · p(b) (2) Note that for p(a) = p(b) = 1, we have the baseline function [1] as applied in Section 2. We use the define functionality in Google to obtain the number of senses of a term. For example, by querying define: Tool, we obtain a list of 31 definitions for the term Tool, collected from various online dictionaries and encyclopedias. This indicates that Tool is an ambiguous term. On the contrary, terms such as Daft Punk, Fatboy Slim and Johannes Brahms lead to precisely one definition. We define n(a) to be the number of definitions for a to be returned by Google. If no definitions are returned, we consider n(a) to be 1. Based on the number of definitions n(a) returned by Google, we investigate the following two alternatives to estimate p(a). linear. As we do not know anything about the distributions of the use of the definitions for term a, we estimate that each definition has a equal probability to be used and that only one of the definitions reflects the artist. plin(a) = 1 n(a) (3) sqrt. Especially for terms with many definitions, we observe some overlap between the definitions. Moreover, two distinct definitions can be close related. For example, Red Hot Chili Peppers is the name of a band and the name of their self-titled debut album. We therefore investigate a second method to estimate p(a) by using the square root of the number of definitions found. psqrt(a) = 1 p n(a) (4) 4 EXPERIMENTAL RESULTS For both the sets of 224 and 1995 artists, we collected the numbers of definition for all the artist names. We recomputed the artist similarities using the linear approach (3) and the sqrt approach (4) and compared the two with the baseline as applied in Section 2. We present the results for the 1995 artists in Figure 2. For the set of 224 the performance of the methods using disambiguation is slightly less than that of the baseline approach. For the set of 1995 artists however, the results improve using either the linear or the sqrt approach. We note that contrary to the set of 224 artists, the 1995 set does contain ‘difficult’ ambiguous names such Autograph, Gamma Ray and Hypocrisy. For the set of 1732 artists in our own collection, we compare the number of times that ambiguous artist names occur among the 5 nearest neighbors for the other artists 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0 5 10 15 20 25 30 35 40 45 50 precision k Precision for k-NN Artist Similarity linear sqrt baseline Figure 2. Precision for the sets of 1995 artists using the three ambiguity estimators. (Table 1). We note that for the term Juli only one definition is found. Although the distribution of ambiguous names is quite different for plin and psqrt, we cannot draw conclusions which approach is better suited as currently no ground truth for artist similarity ranking is available. Artist baseline plin psqrt Live 1227 1 54 Tool 1334 0 642 Fish 724 0 7 Juli 691 1251 1207 Table 1. Number of times an ambiguous artist name occurs among the top 5 nearest neighbors of the 1731 other artists. 5 CONCLUSIONS AND FUTURE WORK We have shown that a method that performs well on a ‘clean’ set with few ambiguous names leads to unsatisfying results on a more realistic data set. Terms that do not have the intended artist as dominant meaning (e.g. Boston, Play and Live) are likely to be found similar to other artists. We observe this problem especially for lesser known artists, where the data collected is more sparse. We have shown that a simple, efficient and unsupervised method using the number of definitions of a term can compensate for this phenomenon. 6 REFERENCES [1] G. Geleijnse and J. Korst. Web-based artist categorization. In Proceedings of ISMIR’06, pages 266 – 271, 2006. [2] P. Knees, E. Pampalk, and G. Widmer. Artist classification with web-based data. In Proceedings of ISMIR’04, pages 517 – 524, 2004. [3] M. Schedl, T. Pohle, P. Knees, and G. Widmer. Assigning and visualizing music genres by web-based co-occurrence analysis. In Proceedings of ISMIR’06, pages 260 – 265, 2006.",
        "zenodo_id": 1416430,
        "dblp_key": "conf/ismir/GeleijnseK07",
        "content": "TOOL PLA YLIVE: DEALING WITH AMBIGUITY IN\nARTIST SIMILARITY MINING FROM THE WEB\nGijs Geleijnse JanKorst\nPhilips Research\nEindho ven(theNetherlands)\nfgijs.geleijnse,jan.k orstg@philips.com\nABSTRA CT\nAsmethods inartist similarity identi\u0002cation using Web\nMusic Information Retrie valperform well onknowneval-\nuation sets, weinvestig atetheapplication ofsuch amethod\ntoamore realistic data set. Wenotice that ambiguous\nartist names lead tounsatisfying results. Wepresent a\nsimple, ef\u0002cient andunsupervised method todeal with\nambiguous artist names.\n1INTR ODUCTION\nWebMusic Information Retrie val(Web-MIR) isanovel\nandpromising \u0002eld ofresearch. Recently ,excellent per-\nformances (with precision rates around 90%) arereported\nonartist genre classi\u0002cation andartist similarity identi\u0002-\ncation using dedicated testsets[2,1,3].\nInthiswork, wefocus onthetask ofidentifying and\nscoring artist similarities using web data. Although our\nmethod performs well ontwocommonly used testsets, the\nresults onamore realistic collection ofartists arelessen-\ncouraging. Contrary tothetwocommon benchmark sets\nforWeb-MIR [2,3],thissetofartists contains ambigu-\nousnames oflessfamous artists. When querying forartist\nnames, within thesearch results theambiguous name Nir-\nvanacanbeexpected torefer totheband. However,the\nmeaning ofPlay also aband inourcollection isless\nobvious. Weobserv ethat such ambiguous artist names\ntend tobefound similar toalargenumber ofartists. Es-\npecially forlesser knownartists, thisleads tounsatisfy-\ningresults. Wetherefore present anunsupervised method\ntodeal with thephenomenon ofambiguous artist names,\nmaking themethod more robustandsuited forrealistic\ntasks.\n2FINDING ARTIST SIMILARITIES\nUsing twoef\u0002cient methods introduced inearlier work\n[1],wecompute themeasure ofsimilarity T(a;b)ofartist\natoanother artistbusing co-occurrences ontheweb.With\nPM,weextract artist names from search engine snippets\nafter querying with relation patterns, where with DMwe\nscan fulldocuments forartist names. Weuseasetof224\nc\r2007 Austrian Computer Society (OCG). 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n 0  5  10  15  20  25  30precision\nkPrecision for k-NN Artist Similarity\npm-224\npm-1995\ndm-224\ndm-1995\nFigur e1.Precision forthesetsof224and1995 artists.\nartists, equally divided over14genres [2]andalargeset\nof1995 artists divided over9genres [3].Weconsider two\nartists tobesimilar ,iftheyshare agenre inthetestset.\nFigure 1showstheaverage precision ofthesimilarity of\ntheartists andtheirk-NN. Wecanconclude thatthepat-\nternbased method givesgood results andoutperforms DM\ninboth sets.\nThe experiments showthat PMboth outperforms DM\nandisless time consuming. Hence, weapplied PMto\nacollection of1732 artists, used inmusic recommender\nexperiments. Since noground truth isavailable forthis\ncollection ofartist, wecannot evaluate theprecision. We\nobserv ethatthenames thatoften notrefer totheintended\nartist frequently occur amidst themost related artists. Al-\nthough thequeried expressions contain anartist name by\nconstruction, theretrie vedsnippets donotalwayshandle\nmusical artists andtheir relations.\n3DEALING WITH AMBIGUITY\nIdeally ,foreach occurrence ofanartist name inatextwe\nwanttoobserv ewhether theoccurrence indeed re\u0003ects the\nintended artist. However,theautomatic parsing ofsen-\ntences andterm recognition istroublesome asthesnippets\ncontain brokensentences andmay bemultilingual. More-\noverifanartist name isidenti\u0002ed asasubject orobject\nwithin asentence, then westilldonotknowwhether the\nterm indeed re\u0003ects theartist.\nWetherefore aimforanunsupervised method where\nweestimate theprobability thatatermaindeed re\u0003ectstheintended artist named a.Ifweknowtheprobability\np(a)thatare\u0003ects theartist, wecanrede\u0002ne theartist\nsimilarity function Tasfollows.\nT0(a;b)=co0(a;b)P\nc;c 6=bco0(c;b)(1)\nwith\nco0(a;b)=co(a;b)\u0001p(a)\u0001p(b) (2)\nNote thatforp(a)=p(b)=1,wehavethebaseline\nfunction [1]asapplied inSection 2.\nWeusethede\u0002ne functionality inGoogle toobtain the\nnumber ofsenses ofaterm. Forexample, byquerying\ndefine: Tool ,weobtain alistof31de\u0002nitions forthe\nterm Tool,collected from various online dictionaries and\nencyclopedias. This indicates thatToolisanambiguous\nterm. Onthecontrary ,terms such asDaft Punk ,Fatboy\nSlim andJohannes Brahms lead toprecisely onede\u0002ni-\ntion. Wede\u0002ne n(a)tobethenumber ofde\u0002nitions for\natobereturned byGoogle. Ifnode\u0002nitions arereturned,\nweconsider n(a)tobe1.\nBased onthenumber ofde\u0002nitions n(a)returned by\nGoogle, weinvestig atethefollowing twoalternati vesto\nestimate p(a).\nlinear .Aswedonotknowanything about thedistrib u-\ntions oftheuseofthede\u0002nitions forterma,weestimate\nthateach de\u0002nition hasaequal probability tobeused and\nthatonly oneofthede\u0002nitions re\u0003ects theartist.\nplin(a)=1\nn(a)(3)\nsqrt. Especially forterms with manyde\u0002nitions, weob-\nservesome overlap between thede\u0002nitions. Moreo ver,\ntwodistinct de\u0002nitions canbeclose related. Forexample,\nRedHotChili Peppers isthename ofaband andthename\noftheir self-titled debutalbum.Wetherefore investig atea\nsecond method toestimate p(a)byusing thesquare root\nofthenumber ofde\u0002nitions found.\npsqrt(a)=1p\nn(a)(4)\n4EXPERIMENT ALRESUL TS\nForboth thesetsof224and1995 artists, wecollected the\nnumbers ofde\u0002nition foralltheartist names. Werecom-\nputed theartist similarities using thelinear approach (3)\nandthesqrtapproach (4)andcompared thetwowith the\nbaseline asapplied inSection 2.Wepresent theresults\nforthe1995 artists inFigure 2.Forthesetof224theper-\nformance ofthemethods using disambiguation isslightly\nlessthan thatofthebaseline approach. Forthesetof1995\nartists however,theresults impro veusing either thelinear\northesqrtapproach. Wenote thatcontrary tothesetof\n224artists, the1995 setdoes contain `dif\u0002cult' ambiguous\nnames such Autograph ,Gamma RayandHypocrisy .\nForthesetof1732 artists inourowncollection, we\ncompare thenumber oftimes thatambiguous artist names\noccur among the5nearest neighbors fortheother artists 0.6 0.65 0.7 0.75 0.8 0.85 0.9\n 0  5  10  15  20  25  30  35  40  45  50precision\nkPrecision for k-NN Artist Similarity\nlinear\nsqrt\nbaseline\nFigur e2.Precision forthesetsof1995 artists using the\nthree ambiguity estimators.\n(Table 1).Wenote thatfortheterm Julionly onedef-\ninition isfound. Although thedistrib ution ofambiguous\nnames isquite different forplinandpsqrt,wecannot draw\nconclusions which approach isbetter suited ascurrently\nnoground truth forartist similarity ranking isavailable.\nArtist baseline plinpsqrt\nLive 1227 1 54\nTool 1334 0 642\nFish 724 0 7\nJuli 691 1251 1207\nTable 1.Number oftimes anambiguous artist name oc-\ncurs among thetop5nearest neighbors ofthe1731 other\nartists.\n5CONCLUSIONS AND FUTURE WORK\nWehaveshownthat amethod that performs well ona\n`clean' setwith fewambiguous names leads tounsatis-\nfying results onamore realistic data set. Terms thatdo\nnothavetheintended artist asdominant meaning (e.g.\nBoston ,Play andLive)arelikelytobefound similar to\nother artists. Weobserv ethisproblem especially forlesser\nknownartists, where thedata collected ismore sparse.\nWehaveshownthatasimple, ef\u0002cient andunsupervised\nmethod using thenumber ofde\u0002nitions ofaterm cancom-\npensate forthisphenomenon.\n6REFERENCES\n[1]G.Geleijnse andJ.Korst. Web-based artist categorization.\nInProceedings ofISMIR'06 ,pages 266271, 2006.\n[2]P.Knees, E.Pampalk, andG.Widmer .Artist classi\u0002cation\nwith web-based data. InProceedings ofISMIR'04 ,pages\n517524, 2004.\n[3]M.Schedl, T.Pohle, P.Knees, andG.Widmer .Assigning\nandvisualizing music genres byweb-based co-occurrence\nanalysis. InProceedings ofISMIR'06 ,pages 260 265,\n2006."
    },
    {
        "title": "The Quest for Ground Truth in Musical Artist Tagging in the Social Web Era.",
        "author": [
            "Gijs Geleijnse",
            "Markus Schedl",
            "Peter Knees"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416582",
        "url": "https://doi.org/10.5281/zenodo.1416582",
        "ee": "https://zenodo.org/records/1416582/files/GeleijnseSK07.pdf",
        "abstract": "Research in Web music information retrieval traditionally focuses on the classification, clustering or categorizing of music into genres or other subdivisions. However, current community-based web sites provide richer descriptors (i.e. tags) for all kinds of products. Although tags have no well-defined semantics, they have proven to be an effective mechanism to label and retrieve items. Moreover, these tags are community-based and hence give a description of a product through the eyes of a community rather than an expert opinion. In this work we focus on Last.fm, which is currently the largest music community web service. We investigate whether the tagging of artists is consistent with the artist similarities found with collaborative filtering techniques. As the Last.fm data shows to be both consistent and descriptive, we propose a method to use this community-based data to create a ground truth for artist tagging and artist similarity. 1 INTRODUCTION Researchers in music information retrieval widely consider musical genre to be an ill-defined concept [2, 14, 9]. Several studies also showed that there is no consensus on genre taxonomies [1, 10]. However, automatic genre classification is a popular topic of research in music information retrieval (e.g. [3, 17, 8, 11, 16, 6]). In their 2006 paper [9], McKay and Fujinaga conclude that musical genre classification is worth pursuing. One of their suggestions is to abandon the idea that only one genre is applicable to a recording. Hence, multiple genres can be applicable to one recording and a ranked list of genres should be computed per recording. Today, the content of web sites such as del.icio.us, flickr.com and youtube.com is generated by their users. Such sites use community-based tags to describe the available items (photos, films, music, (scientific) literature, etc.). Although tags has proven to be suitable descriptors for items, no clear semantics are defined. Users can label an item with any term. The more an item is lac⃝2007 Austrian Computer Society (OCG). beled with a tag, the more the tag is assumed to be relevant to the item. Last.fm is a popular internet radio station where users are invited to tag the music and artists they listen to. Moreover, for each artist, a list of similar artists is given based on the listening behavior of the users. In [4], Ellis et al. propose a community-based approach to create a ground truth in musical artist similarity. The research question was whether artist similarities as perceived by a large community can be predicted using data from All Music Guide and from shared folders for peer-to-peer networks. Now, with the Last.fm data available for downloading, such community-based data is freely available for non-commercial use. In Last.fm, tags are terms provided by users to describe music. They “are simply opinion and can be whatever you want them to be” 1 . For example, Madonna’s music is perceived as pop, glamrock and dance as well as 80s and camp. When we are interested in describing music in order to serve a community (e.g. in a recommender system), community-created descriptors can be valuable features. In this work we investigate whether the Last.fm data can be used to generate a ground truth to describing musical artists. Although we abandon the idea of characterizing music with labels with defined semantics (i.e. genres), we follow the suggestion in [9] to characterize music with a ranked list of labels. We focus on the way listeners perceive artists and their music, and propose to create a ground truth using community data rather than to define one by experts. In line with the ideas of Ellis et al. [4], we use artist similarities as identified by a community to create a ground truth in artist similarity. As tastes and opinions change over time, a ground truth for music characterization should be dynamic. We therefore present an algorithm to create a ground truth from the dynamically changing Last.fm data instead of defining it once and for all. This paper is organized as follows. In the next section we investigate whether the data from Last.fm can indeed be used as a ground truth for describing artists and artist similarities. As the experiments in Section 2 give rise to further research, we present a method to create a ground 1 http://www.Last.fm/help/faq/?category=Tags truth for a set of artists in Section 3. Using this ground truth, we propose an approach to evaluate the performance of arbitrary methods for finding artist similarities and tags. Finally, we draw conclusions and indicate directions for future work in Section 4. 2 ANALYZING THE LAST.FM META-DATA Last.fm users are invited to tag artists, albums, and individual tracks. The 100 top-ranked tags (with respect to the frequency a tag is assigned) for these three categories are easily accessible via the Audioscrobbler web services API 2 . By analyzing the listening behavior of its users, Last.fm also provides artist similarities via Audioscrobbler 3 . Per artist, a list of the 100 most similar artists is presented. We analyze tags for artists in the next subsection. As the lists of the top-ranked tags tend to contain noise, we propose a simple mechanism to filter out such noise (Section 2.2). In order to check the consistency of the tags, we inspect, in Section 2.3, whether users label similar artists with the same tags. In Section 2.4, we compare the performance of the Last.fm data with results from previous work on a traditional genre classification task. Section 2 ends with conclusions on the suitability of Last.fm data to create a ground truth in artist tagging and similarity.",
        "zenodo_id": 1416582,
        "dblp_key": "conf/ismir/GeleijnseSK07",
        "keywords": [
            "music",
            "genre",
            "community-based",
            "tags",
            "Last.fm",
            "artist similarities",
            "ground truth",
            "collaborative filtering",
            "community-created",
            "recommender system"
        ],
        "content": "THE QUEST FOR GROUND TRUTH INMUSICAL ARTIST TAGGING\nINTHE SOCIAL WEB ERA\nGijs Geleijnse\nPhilips Research\nHigh TechCampus 34\nEindho ven(theNetherlands)\ngijs.geleijnse@philips.comMarkus Schedl Peter Knees\nDept. ofComputational Perception\nJohannes Kepler University\nLinz (Austria)\nfmarkus.schedl,peter .kneesg@jku.at\nABSTRA CT\nResearch inWebmusic information retrie valtraditionally\nfocuses ontheclassi\u0002cation, clustering orcategorizing of\nmusic into genres orother subdi visions. However,cur-\nrent community-based web sites provide richer descrip-\ntors (i.e. tags) forallkinds ofproducts. Although tags\nhavenowell-de\u0002ned semantics, theyhaveproventobe\naneffectivemechanism tolabel andretrie veitems. More-\nover,these tags arecommunity-based andhence givea\ndescription ofaproduct through theeyesofacommunity\nrather than anexpert opinion. Inthisworkwefocus on\nLast.fm, which iscurrently thelargest music community\nweb service. Weinvestig atewhether thetagging ofartists\nisconsistent with theartist similarities found with collab-\norative\u0002ltering techniques. AstheLast.fm data showsto\nbeboth consistent anddescripti ve,wepropose amethod\ntousethiscommunity-based data tocreate aground truth\nforartist tagging andartist similarity .\n1INTR ODUCTION\nResearchers inmusic information retrie valwidely con-\nsider musical genre tobeanill-de\u0002ned concept [2,14,9].\nSeveralstudies also showed thatthere isnoconsensus on\ngenre taxonomies [1,10].However,automatic genre clas-\nsi\u0002cation isapopular topic ofresearch inmusic informa-\ntionretrie val(e.g. [3,17,8,11,16,6]).\nIntheir 2006 paper [9],McKay andFujinag aconclude\nthatmusical genre classi\u0002cation isworth pursuing. One\noftheir suggestions istoabandon theidea thatonly one\ngenre isapplicable toarecording. Hence, multiple genres\ncanbeapplicable toonerecording andarankedlistof\ngenres should becomputed perrecording.\nToday,thecontent ofweb sites such asdel.icio.us ,\nflickr.com andyoutube.com isgenerated bytheir\nusers. Such sites usecommunity-based tags todescribe\ntheavailable items (photos, \u0002lms, music, (scienti\u0002c) liter-\nature, etc.). Although tags hasproventobesuitable de-\nscriptors foritems, noclear semantics arede\u0002ned. Users\ncanlabel anitem with anyterm. Themore anitem isla-\nc\r2007 Austrian Computer Society (OCG).beled with atag,themore thetagisassumed toberelevant\ntotheitem.\nLast.fm isapopular internet radio station where users\nareinvited totagthemusic andartists theylisten to.More-\nover,foreach artist, alistofsimilar artists isgivenbased\nonthelistening behavior oftheusers. In[4],Ellis etal.\npropose acommunity-based approach tocreate aground\ntruth inmusical artist similarity .The research question\nwaswhether artist similarities aspercei vedbyalargecom-\nmunity canbepredicted using data from AllMusic Guide\nandfrom shared folders forpeer-to-peer netw orks. Now,\nwith theLast.fm dataavailable fordownloading, such com-\nmunity-based data isfreely available fornon-commercial\nuse.\nInLast.fm, tagsareterms provided byusers todescribe\nmusic. Theyare simply opinion andcanbewhate veryou\nwantthem tobe1.Forexample, Madonna' smusic is\npercei vedaspop,glamrock anddance aswell as80sand\ncamp .When weareinterested indescribing music inor-\ndertoserveacommunity (e.g. inarecommender system),\ncommunity-created descriptors canbevaluable features.\nInthisworkweinvestig atewhether theLast.fm data\ncanbeused togenerate aground truth todescribing mu-\nsical artists. Although weabandon theidea ofcharacter -\nizing music with labels with de\u0002ned semantics (i.e.gen-\nres), wefollowthesuggestion in[9]tocharacterize music\nwith arankedlistoflabels. Wefocus onthewaylisten-\nerspercei veartists andtheir music, andpropose tocreate\naground truth using community data rather than tode-\n\u0002neonebyexperts. Inlinewith theideas ofEllis etal.\n[4],weuseartist similarities asidenti\u0002ed byacommunity\ntocreate aground truth inartist similarity .Astastes and\nopinions change overtime, aground truth formusic char-\nacterization should bedynamic. Wetherefore present an\nalgorithm tocreate aground truth from thedynamically\nchanging Last.fm data instead ofde\u0002ning itonce andfor\nall.\nThis paper isorganized asfollows.Inthenextsection\nweinvestig atewhether thedata from Last.fm canindeed\nbeused asaground truth fordescribing artists andartist\nsimilarities. Astheexperiments inSection 2giveriseto\nfurther research, wepresent amethod tocreate aground\n1http://www.Last.fm/help/faq/?category=Tagstruth forasetofartists inSection 3.Using thisground\ntruth, wepropose anapproach toevaluate theperformance\nofarbitrary methods for\u0002nding artist similarities andtags.\nFinally ,wedrawconclusions andindicate directions for\nfuture workinSection 4.\n2ANALYZING THE LAST .FM MET A-DATA\nLast.fm users areinvited totagartists, albums, andindi-\nvidual tracks. The 100top-rank edtags (with respect to\nthefrequenc yatagisassigned) forthese three categories\nareeasily accessible viatheAudioscrobbler web services\nAPI2.Byanalyzing thelistening behavior ofitsusers,\nLast.fm also provides artist similarities viaAudioscrob-\nbler3.Perartist, alistofthe100most similar artists is\npresented.\nWeanalyze tags forartists inthenextsubsection. As\nthelists ofthetop-rank edtags tend tocontain noise, we\npropose asimple mechanism to\u0002lter outsuch noise (Sec-\ntion2.2). Inorder tocheck theconsistenc yofthetags, we\ninspect, inSection 2.3,whether users label similar artists\nwith thesame tags. InSection 2.4,wecompare theper-\nformance oftheLast.fm data with results from previous\nworkonatraditional genre classi\u0002cation task. Section 2\nends with conclusions onthesuitability ofLast.fm data to\ncreate aground truth inartist tagging andsimilarity .\n2.1Tagging ofArtists\nInTable1,the20top-rank edtagsfortheartist Eminem are\ngiven,asfound with theAudioscrobbler web service. The\nterms rap,hiphop anddetroit canbeseen asdescripti vefor\ntheartist andhismusic. Eminem istagged with multiple\nterms thatre\u0003ect agenre butthetagrapismore signi\u0002cant\nthan metal .\nWithout questioning thequality orapplicability ofthe\nterms inthelistinTable 1,weobserv esome noise inthe\ntagging ofthisartist. Whether weconsider Eminem tobe\nahip-hop artist ornot,after encountering thesecond high-\nestrankedtagHip-Hop ,thetags hiphop,hiphop donot\nprovide anynewinformation. Moreo ver,thetagEminem\ndoes notprovide anynewinformation with respect tothe\ncatalogue meta-data. The tags favorite andgood donot\nseem verydiscriminati ve.\nToinvestig atewhether thetags areindeed descripti ve\nforaparticular artist, wecollected thetagsapplied toaset\nofartists. In[16]4,alistof1,995 artists wasderivedfrom\nAllMusic Guide. Wecalculated thenumber ofartists that\narelabeled with each ofthetags. The most frequently\noccurring tags overallartists aregiveninTable 2.Ta-\nble3contains some ofthetags thatareapplied only to\noneartist. Forthe1,995 artists, weencountered 14,146\nunique tags.\n2http://ws.audioscrobbler.com\n3e.g.http://ws.audioscrobbler.com/1.0/\nartist/Madonna/similar.xml\n4http://www.cp.jku.at/people/schedl/\nmusic/C1995a artists genres.txtrap\nHip-Hop\nhiphop\nEminem\nhiphop\npop\nrock\nalternative\ndetroit\nseenliveGangsta Rap\nAftermath\nfavorites\nmetal\nFavorite\nrnb\ndance\namerican\nclassic rock\nrandb\nTable 1.Top20tagsforEminem.\njazz:809\nseenlive:658\nrock:633\n60s:623\nblues:497\nsoul:423\nclassic rock:415\nalternative:397\nfunk:388\npop:381\nfavorites:349\namerican:345\nmetal:334\nelectronic:310\nindie:309country:308\nhardrock:294\nsingersongwriter:291\noldies:289\nfemalevocalists:285\npunk:282\nfolk:281\nheavymetal:277\nhip-hop:267\ninstrumental:233\nrnb:231\nprogressive rock:229\nelectronica:215\ndance:209\nalternative rock:208\nTable 2.The30most popular tags andtheir frequencies\nforthesetof1995 artists.\nIfatagisapplied tomanydiverse artists, itcannot be\nconsidered tobediscriminati ve.Weobserv ethat there\narenotags that areapplied toamajority oftheartists.\nThe high number ofartists labeled with jazz canbeex-\nplained bythefactthatthe1,995-artist-set contains 810\njazz artists. Allfrequent tags seem relevantcharacteriza-\ntions formusical artists orfortherelation oftheusers to\ntheartists (e.g. seen live).\nThe most debatable tagamong thebest scoring ones\nmay befavorites .Table 4contains alistofthetopartists\nforthistag,asextracted from audioscrobbler .Wenotice\nthatnomainstream dance orpopartists areamong thelist\nof100topartists forfavorites .The100topartists forseen\nliveareartists thattoured inthe00s.\nTagsthatareapplied toonly one, oronly afewartists\narenotinformati veeither .Since wedonotconsider the\nsemantics ofthetags, uniquely occurring tags cannot be\nused tocompute artist similarities.\nWeobserv ethatthetags thatareonly applied once to\nartists inthissetaremore prosaic, areinadifferent lan-\nguage, orsimply contain typos (cf.electro techo inTa-\nble3).Itisnotable thatintotal 7,981 tags (56%) areap-\nplied toonly oneartist. Only 207tags areapplied toat\nleast 50outofthe1,995 artists.\nTocheck whether the7,981 tags aredescripti vefora\nlargersetofartists, wecomputed thetopcount. That\nis,thetotal number oftimes each tagisapplied toits\natmost 100topartists5.Table 5contains examples of\n5e.g. http://ws.audioscrobbler.com/1.0/tag/\npost-hardcore/topartists.xmlcrappygirlsingers:1\nstuffthatneedsfurther exploration:1\ndisconoir:1\nknarz:1\nlektroluv compilation:1\ngdo02:1\nelectro techo:1\n808state:1\niiiii:1\ngrimy:1\nmussikk:1\ngrimey:1\ngoodgymmusic:1\ntechnomanchester electronic acidhouse:1\nmusicitriedbutdidntlike:1\nricherbadrappers havenotexisted:1\namerican virginfestival:1\nTable 3.Some oftheleast used tagsforthe1995 artists.\nRadiohead\nTheDecemberists\nDeathCabforCutie\nTheBeatles\nTheShinsColdplay\nPinkFloyd\nThePostalService\nBrightEyes\nElliotSmith\nTable 4.The10topartists forthetag'favorites'.\ntags applied once inthe1995 artist collection andtheir\ntopcounts. Ifthissum isone, only oneuser hastagged\noneartist with thistag. Hence, thelargerthetopcount,\nthemore people will haveused thetagtodescribe their\nmusic. Outofthese 7,981 tags, 7,238 haveatopcount\nofatmost 100. Forcomparison, thetag'rock' hasatop\ncount of150,519. Hence, wecanconclude thatthetags\nthatarefound only once inacollection ofartists arein\ngeneral uncommon descriptors foranartist.\nBased onthese small experiments, weconclude thatall\nfrequently used tags arerelevanttocharacterize anartist.\nMoreo ver,although users cantaganartist with anyterm,\nthelistoffrequently used tagsisrelati velysmall. Wecon-\nclude thatthenumber oftagsthatdescribe multiple artists\nisintheorder ofthousands. Ifweselect thetagsthatapply\nto5%oftheartists, thenumber isintheorder ofhundreds.\n2.2Filtering theTags\nAsindicated above,notalltags provide suf\u0002cient infor -\nmation forourtask since tags occur with small spelling\nvariations andcatalogue meta-data isused astagaswell.\nMoreo ver,tags thatareonly applied tofewartists cannot\nbeused todiscriminate between artists. Suppose thatwe\nhaveacollection Aofartists. Wepresent asimple method\npost-hardcore:8134\ntwee:4036\nfuturepop:3162\nmathcore:2865\npianorock:2558fagzzz:0\nwhensomebody lovesyou:0\nravensmusic:0\nbandsimet:0\nmostdefinitely abamf:1\nTable 5.Examples oftags occurring only once with the\nhigh andlowtopcounts.hiphop\nEminem\nhiphop\nAftermathalternative\nseenlive\nmetal\nclassic rock\nTable 6.Tagsremo vedforEminem after normalization\n(l.)andtrack-\u0002ltering (r.).\nto\u0002lter outsuch meaningless tags.\nNormalizing Tags. Aswewanttagstobedescripti ve,we\n\u0002lter outtagsattached toa2Aasfollows.\nIfatagisequal tothename oftheartist, weremo veit.\nWecompute anormalized form foralltagsby\nturning them intolowercase,\ncomputing thestem ofallwords inthetags using\nPorter' sstemming algorithm [13],and\nremo ving allnon-letter -or-digit characters inthetags.\nIftwotags havethesame normalized form, weremo ve\nthesecond oneinthelist.\nWeremo veeverytagthatisapplied tolessthan 5%of\ntheartists inA.\nAswewantthetagstore\u0003ect themusic oftheartist, we\npropose anext\u0002ltering step based onthetags applied to\nthebest scoring tracks oftheartist. Audioscrobbler pro-\nvides themost popular tracks perartist, based onthelis-\ntening behavior oftheLast.fm users. Astracks canalso\nbetagged individually ,wecancompare thetags applied\ntotheartist with thetags applied tothetoptracks. Inthe\nTrack Filtering step, we\u0002lter outtagsapplied totheartist,\nthatarenotapplied tohistoptracks.\nTrack Filtering .Byremo ving thetagsthatdonotre\u0003ect\nthemusic ofanartist, weperform asecond \u0002ltering step.\n-Wecollect the10top-rank edtracks according toLast.fm\nforeveryartist inthelist.\n-Foreach ofthese, weretrie vethemost popular tags.\n-Wecompute anormalized form forthetags foreach\ntrack.\n-Forthelistofthenormalized tags ofa2A,weretain\nonly those whose normalized form isapplied toatleast 3\noutofthe10top-rank edtracks fortherespecti veartist.\nThe tags from Table 1forEminem thatareremo ved\nafter normalization andtrack \u0002ltering aregiveninTable 6.\n2.3Checking theConsistency oftheTags\nThe artist similarities asprovided byLast.fm arebased\nonthelistening behavior oftheusers, andthus computed\nindependently ofthetags applied totheartists. Since we\nwanttousetheLast.fm dataasground truth inmusic char-\nacterization, thetagging should beconsistent, i.e.similar\nartists should share alargenumber oftags.\nToensure thiscriterion, weselected thesetof224artists\nused in[7]6,where theartists were originally chosen to\nberepresentati vesof16different genres. Foreach ofthe\n6http://www.cp.jku.at/people/knees/\npublications/artistlist224.html224artists, wecollected the100most similar artists ac-\ncording toLast.fm. Fortheresulting setofthe224artists\nandtheir 100nearest neighbors, wedownloaded thelists\nofthe100most popular tags. Perartist inthelistof224,\nwe\u0002rstcompared thelistoftagsofthemost similar artist.\nWedidthesame forthefollowing (less) similar artists in\nthelist.\nWecomputed theaverage number ofoverlapping tags\nforthe224artists andtheirknearest neighbors anddis-\nplay theresults inFigure 1.Asespecially after track\n\u0002ltering often lessthan 100tags areassigned toeach\nartist, wealso computed thesimilarity scoreforeach of\nthe224artists andtheirknearest neighbors bytaking the\naverage number oftagsrelati vetothetotal number oftags\nforthenearest neighbors. Forexample, ifanartist shares\n34outof40tags with anartist inthelistof224, therela-\ntivetagsimilarity score forthisartist is34=40.Theaver-\nagesimilarity scores aregiveninFigure 2.Thescores are\ncomputed using un\u0002ltered, normalized andtrack-\u0002ltered\nLast.fm data.\nTheaverage number andscore ofoverlapping tags de-\ncreases only slightly fortheun\u0002ltered andnormalized data\nwith increasing k.Forthetrack-\u0002ltered data, weeven\nnote asmall increase intherelati veamount oftagsshared\n(starting fromk=25).This canbeexplained bythesmall\nnumber oftags thatremain after track-\u0002ltering, ascanbe\nfound inTable 1.\n 0 10 20 30 40 50 60 70\n 0  10  20  30  40  50  60  70  80  90  100average number of shared tags\nkAverage number of shared tags for k-NN\nnormalized data\nlast.fm data\ntrack-filtered data\nFigur e1.Average number ofshared tags forthe224\nartists.\nUsing theun\u0002ltered Last.fm tagsofallretrie vedartists,\nweestimate theexpected number oftags shared bytwo\nrandomly chosen artists as29:8andtherelati venumber\nofshared tagsas0:58.When we\u0002lter thetagsbynormal-\nization andcompare thenormalized forms ofthetags, we\nobtain anaverage of29:8shared tags, with arelati venum-\nberof0:62.Forthetrack \u0002ltering, these numbers are3:87\nand0:64respecti vely.Hence, thenumber oftags shared\nbysimilar artists isindeed much largerthan thatshared by\nrandomly chosen artists.\n2.4Using Last.fm inGenr eClassi\u0002cation\nTogetfurther assess theusefulness oftheLast.fm tags,\nweinvestig atewhether thisdata canbeused ina`tradi- 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84\n 0  10  20  30  40  50  60  70  80  90  100relative tag similarity\nkRelative tag similarity for k-NN\ntrack-filtered data\nnormalized data\nlast.fm data\nFigur e2.Relati vetagsimilarity score forthe224artists\nandtheir kNearest Neighbors\ntional' genre classi\u0002cation task. Weevaluate whether the\nLast.fm data canbeused toclassify theearlier mentioned\n224artists into the16genres (with 14artists pergenre)\nasde\u0002ned in[7].Using data from web pages found with\nasearch engine, earlier workreports precision values of\naround 88% [7,6,15].\nWerepeated theexperiment asdescribed in[6]using\nthedatafrom Last.fm. Foreach artist, weinitially selected\nthegenre thatgetsthehighest score. Inthiscase, wethus\nselect thegenre thatismentioned asthehighest ranked\ntag. Ifnogenre ismentioned foranartist, initially no\ngenre isassigned.\nWecompare thegenre classi\u0002cation using theLast.fm\ndata with themethods PMand DMasdiscussed in[6].\nWithPMweusepatterns to\u0002nd relevantsearch engine\nsnippets. Using DM,wescan fulldocuments thatareob-\ntained byquerying theterm music together with either a\ngenre name oranartist name.\nAstheexperiments in[6]showed, theuseofsimilar\nartists canimpro vetheclassi\u0002cation results. Fortheex-\nperiment with theLast.fm data wetherefore retrie ved,for\neach artist ofthe224artist set,thelistofthe(atmost) 100\nmost similar artists. Onaverage, 14artists from the224-\nlistwere mentioned among the100most similar artists of\nanarbitrarily chosen artist from the224-set.\nHaving obtained aninitial mapping between each of\nthe224artists andagenre, weusethenearest neighbors\ntocompute a\u0002nal mapping. Alike[6],wecompute ama-\njority voting among theinitial genre foreach artist andits\nknearest neighbors using PM,DMandtheLast.fm data.\nWecompare theresults oftheLast.fm-based artist clas-\nsi\u0002cation with thebest tworesults from [6]inFigure 3.\nForthemethod DMco-occurrences between artists and\ngenres within fullweb documents areused tocompute the\ninitial mapping. Tocompute artist similarity using DM,\nweuseco-occurrences ofartist names within documents.\nThe method PMuses co-occurrences within phrases that\nexpress therelations ofinterest.\nThe results forartist classi\u0002cation using theLast.fm\ndata aresimilar totheones gained using web-data col-\nlected with asearch engine. The results forLast.fm arebest when incorporating thetags ofthe3nearest neigh-\nbors ofeveryartist. Since anaverage number of14similar\nartists (outofthesetof224) isidenti\u0002ed, theperformance\ndeteriorates forlargervalues ofk.\nItisnotable thatforallthree methods most misclassi-\n\u0002cations were made intheFolk,HeavyandRock 'nRoll\ngenres, where often thegenre Indie/Alternati vewasas-\nsigned totheartist.\nWhen weclassify theartists using theLast.fm data af-\ntertrack \u0002ltering, theinitial mapping (k=0)impro ves\nslightly asCubby Check erisnowcorrectly classi\u0002ed. For\nvalues ofklargerthan 1,theperformance using thetrack\n\u0002ltered data isequal totheoneusing either theraworthe\nnormalized Last.fm data.\n 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88 0.9\n 0  5  10  15  20  25  30  35  40precision\nkPrecision for Artist Categorization of 224 test set\nlast.fm data\ndm\npm\nFigur e3.Precision ofthe224artist categorization fork\nnearest neighbors.\nAstheresults ofthegenre classi\u0002cation using theLast.fm\ndata areequally good asthose gained with thebest, high\nperformance methods using arbitrary web-data, wecon-\nclude thatthetagsfrom Last.fm areareliable data source\nforthisclassi\u0002cation task.\n2.5Conclusions\nWehaveseen thatthebestscoring tagsperartist arechar-\nacterizations oftheartist, themusic, andtherelation of\ntheuser totheartist. Allthree categories areconsidered\ninformati veanduseful fore.g.arecommender system.\nThe experiments with the224artists andtheir 100near-\nestneighbors ofeach indicate thatsimilar artists share a\nhigh amount oftags. Onagenre-classi\u0002cation task, using\nLast.fm data gavesimilar results asmethods using data\nretrie vedwith aweb search engine.\nDue tospelling variations, notalltags contain valu-\nable information. Moreo ver,wecannot deduce informa-\ntionfrom rarely used tags, asnosemantics aregivenfor\nthetags. Hence, ifatagonly occurs once inacollection,\nthistagisnotdescripti veandcanneither beused to\u0002nd\nsimilar artists ormusic.\nWeproposed anormalization method toremo vemean-\ningless tags. Tagsthatdonotre\u0003ect themusic canbe\u0002l-\ntered outbycomparing thetags oftheartist with thetags\noftheartist' stoptracks.From theexperiments, weconclude thatfrequently used\ntags givefaithful descriptions oftheartists. Using asim-\nplenormalization method, wecan\u0002lter out'double' tags,\ni.e.tags with small variations inspelling. The \u0002ltering\nstep using thetags applied tothetracks, however,seems\nlessuseful asthere islittle overlap between thetags for\nthetoptracks andtheones applied totheartist.\n3EVALU ATING WITH LAST .FM DATA\nInearlier work(e.g. [16,12,5])computed artist similar -\nities were evaluated using theassumption thattwoartists\naresimilar when theyshare agenre. Toourbest knowl-\nedge, only thetagging ofartists with asingle tag,usually\nagenre name, hasbeen addressed inliterature.\nAstheLast.fm data showstobereliable, wepropose\ntouseitasaground truth forevaluating algorithms that\nidentify tags forartists tagging andcompute artist sim-\nilarity .The useofsuch arich, user-based ground truth\ngivesbetter insights intheperformance ofthealgorithm\nandprovides possibilities tostudy theautomatic labeling\nofartists with multiple tags.\n3.1ADynamic Ground Truth Extraction Algorithm\nAstheperception ofusers changes overtime, wedonot\npropose tocreate aconstant setofartists andrankedtags,\nbutanalgorithm toextract theground truth from Last.fm7.\nForcomparison purposes, wepropose thesetsof224and\n1,995 artists used inearlier experiments forevaluation of\nmethods inautomatic artist tagging.\nFortheevaluation ofartist similarity ,weusethesimilar\nartists fortheartists inthesetUasprovided byLast.fm.\nForthelists ofsimilar artists, wediscard theartists that\narenotinU.\nTocreate aground truth fortherankedtags applica-\nbletotheartists inU,wedownload thetoptags foreach\nartist andcompute theNormalized Tagsasdescribed in\nSection 2.2. Using allnormalized tags applied totheU\nartists, weidentify asetTofknowntags.\n3.2Proposed Evaluation Measur es\nForatagoranartisttigivenbytheground truth,ga(ti)\ndenotes therank oftiwith respect toartista.Hence,\nga(ti)\u00001tags orartists areconsidered tobemore ap-\nplicable (orsimilar) toa.Incontrast, withra(ti)wede-\nnote therank oftiforaascomputed bythemethod tobe\nevaluated.\nWepropose twoevaluation measures. The\u0002rstfocuses\nonthetraditional information retrie valmeasures precision\nandrecall, thesecond evaluates theranking.\nPrecision and Recall. Weselect thesetSofthetopn\ntagsforartistaintheground truth andevaluate precision\nandrecall ofthecomputed ordered listLmofthemmost\n7The algorithm is available online via\nhttp://gijsg.dse.nl/ismir07/ .applicable tags according tothetagging approach tobe\nevaluated.\nRanking .Wedonotonly consider important theretrie val\nofthentop-rank edtagsintheground truth, wealsowant\ntoevaluate theranking itself, hence thecorrelation be-\ntween theranking intheground truthga(ti)andthecom-\nputed ranking ra(ti).Weevaluate theranking foreach\nartist using Pearson' sCorrelation Coef \u0002cient.\nHaving proposed both atestsetofartist, analgorithm\ntodynamically create aground truth andevaluation mea-\nsures, weaimtofacilitate research inautomatic tagging of\nartist similarity .\n4CONCLUSIONS AND FUTURE WORK\nWehaveproposed toadopt theconcept ofcommunity-\nbased tagging inmusic information retrie valresearch [4].\nContrary toexpert-de\u0002ned ground truth setsforgenre\nclassi\u0002cation, theLast.fm data iscomposed byalarge\ncommunity ofusers. Asartists aredescribed byranked\nlistsrather than single terms, thecharacterization isricher\nthan asingle genre [9].\nTagsarevaluable characterizations ofartists andtheir\nmusic. Inthesocial web, theuser-input tags haveproven\ntobeavaluable method tocharacterize products. Webe-\nlievethat thecharacterization ofmusic byalargeuser-\ncommunity deserv estobeaddressed bytheMIR commu-\nnity.\nOurexperiments indicate thatthetagsapplied toartists\ninLast.fm areindeed consistent with respect toartist simi-\nlarities. Tagsshowedtobedescripti veandusing theLast.fm\ndata good results were achie vedina'traditional' genre-\nclassi\u0002cation task.\nByproviding amethod tocreate aground truth for\nartist tags andsimilarities, wewanttofacilitate research\ninapromising novelarea inmusic information retrie val\nfrom theweb.\n5REFERENCES\n[1]Z.Alekso vski, W.tenKate, andF.vanHarmelen. Approx-\nimate semantic matching ofmusic classes ontheinternet.\nInIntellig entAlgorithms inAmbient andBiomedical Com-\nputing ,Philips Research Book Series, pages 133 148.\nSpringer ,2006.\n[2]J.-J.Aucouturier andF.Pachet. Representing musical genre:\nAstate oftheart.Journal ofNewMusic Resear ch,32(1):83\n93,2003.\n[3]R.Basili, A.Sera\u0002ni, andA.Stellato. Classi\u0002cation ofmu-\nsical genre: amachine learning approach. InProceedings\nof5thInternational Confer ence onMusic Information Re-\ntrieval(ISMIR'04) ,October 2004.\n[4]D.P.Ellis, B.Whitman, A.Berenzweig, andS.Lawrence.\nThe quest forground truth inmusical artist similarity .In\nProceedings oftheThirdInternational Confer ence onMusic\nInformation Retrie val(ISMIR'02) ,pages 170177, Paris,\nFrance, 2002.\n[5]G.Geleijnse and J.Korst. Tagging artists using co-\noccurrences ontheweb.InW.Verhae gh,E.Aarts, W.tenKate, J.Korst, andS.Pauws, editors, Proceedings Third\nPhilips Symposium onIntellig entAlgorithms (SOIA 2006) ,\npages 171 182, Eindho ven,theNetherlands, December\n2006.\n[6]G.Geleijnse andJ.Korst. Web-based artist categorization.\nInProceedings oftheSeventh International Confer ence on\nMusic Information Retrie val(ISMIR'06) ,pages 266271,\nVictoria, Canada, October 2006.\n[7]P.Knees, E.Pampalk, andG.Widmer .Artist classi\u0002cation\nwith web-based data. InProceedings of5thInternational\nConfer ence onMusic Information Retrie val(ISMIR'04) ,\npages 517524, Barcelona, Spain, October 2004.\n[8]T.Li,M.Ogihara, and Q.Li.Acomparati vestudy on\ncontent-based music genre classi\u0002cation. InSIGIR '03: Pro-\nceedings ofthe26th annual international ACMSIGIR con-\nference onResear chand development ininformaion re-\ntrieval,pages 282289, NewYork,NY,USA, 2003. ACM\nPress.\n[9]C.McKay andI.Fujinag a.Musical genre classi\u0002cation: Is\nitworth pursuing andhowcanitbeimpro ved?InProceed-\nings oftheSeventh International Confer ence onMusic In-\nformation Retrie val(ISMIR'06) ,pages 101106, Victoria,\nCanada, October 2006.\n[10] F.Pachet andD.Cazaly .Ataxonomy ofmusical genres. In\nContent-Based Multimedia Information Access Confer ence\n(RIAO),Paris, France, 2000.\n[11] E.Pampalk, A.Flexer,andG.Widmer .Impro vements of\naudio-based music similarity andgenre classi\u0002caton. InPro-\nceedings oftheSixth International Confer ence onMusic In-\nformation Retrie val(ISMIR'05) ,pages 628633, London,\nUK, September 2005.\n[12] T.Pohle, P.Knees, M.Schedl, andG.Widmer .Building an\ninteracti venext-generation artist recommender based onau-\ntomatically derivedhigher -levelconcepts. InProceedings of\ntheFifthInternational Workshop onContent-Based Multi-\nmedia Indexing (CBMI'07) ,Bordeaux, France, 2007.\n[13] M.F.Porter .Analgorithm forsuf\u0002xstripping. InReadings\nininformation retrieval,pages 313 316. MorganKauf-\nmann Publishers Inc., SanFrancisco, CA, 1997.\n[14] N.Scaringella, G.Zoia, andD.Mlynek. Automatic genre\nclassi\u0002cation ofmusic content. IEEE Signal Processing\nMagazine :Special Issue onSemantic Retrie valofMulti-\nmedia ,23(2):133 141, 2006.\n[15] M.Schedl, P.Knees, andG.Widmer .AWeb-Based Ap-\nproach toAssessing Artist Similarity using Co-Occurrences.\nInProceedings ofthe Fourth International Workshop\nonContent-Based Multimedia Indexing (CBMI'05) ,Riga,\nLatvia, June 2005.\n[16] M.Schedl, T.Pohle, P.Knees, andG.Widmer .Assigning\nandvisualizing music genres byweb-based co-occurrence\nanalysis. InProceedings oftheSeventh International Con-\nference onMusic Information Retrie val(ISMIR'06) ,pages\n260265, Victoria, Canada, October 2006.\n[17] G.Tzanetakis andP.Cook. Musical genre classi\u0002cation of\naudio signals. IEEE Transactions onSpeec handAudio Pro-\ncessing ,10(5), 2002."
    },
    {
        "title": "Supervised and Unsupervised Sequence Modelling for Drum Transcription.",
        "author": [
            "Olivier Gillet",
            "Gaël Richard"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417237",
        "url": "https://doi.org/10.5281/zenodo.1417237",
        "ee": "https://zenodo.org/records/1417237/files/GilletR07.pdf",
        "abstract": "We discuss in this paper two post-processings for drum transcription systems, which aim to model typical properties of drum sequences. Both methods operate on a symbolic representation of the sequence, which is obtained by quantizing the onsets of drum strokes on an optimal tatum grid, and by fusing the posterior probabilities produced by the drum transcription system. The first proposed method is a generalization of the N-gram model. We discuss several training and recognition strategies (style-dependent models, local models) in order to maximize the reliability and the specificity of the trained models. Alternatively, we introduce a novel unsupervised algorithm based on a complexity criterion, which finds the most regular and wellstructured sequence compatible with the acoustic scores produced by the transcription system. Both approaches are evaluated on a subset of the ENST-drums corpus, and yield performance improvements. 1 INTRODUCTION Many useful applications can be derived from the knowledge of a semantic description of music signals. As a result, the field of music information retrieval (MIR) is receiving a continuously growing interest from the scientific community. MIR has primarily focused on the extraction of melodic and tonal information, though it is now acknowledged that the rhythmic content, and the drum track in particular, is of primary importance for a number of applications such as drum track remixing, automatic genre recognition, automatic DJing or query by beatboxing. The problem of drum transcription has already been addressed in several studies (see [1] for a review of existing systems). However, most of the studies on drum track transcription only use short-term acoustic information (as carried in acoustic features, or for example, in the coefficients of a non-redundant decomposition), and thus consider each drum event independently from the other adjacent events. Nevertheless, by analogy with speech, where a sequence of random phonemes does not constitute a syntactically correct sentence, most of the sequences of drum events do not represent musically interesting drum tracks. c⃝2007 Austrian Computer Society (OCG). In fact, as already shown in previous works [2, 3, 4], the acoustic clues should be combined with sequence models to take into account the structural specificities of drum sequences. Some of these specificities are listed here: some drum subsequences may never be played (either because they are musically irrelevant or because they are too complex to be played by a drummer), some subsequences are frequent, independently of the style (a tom fill for example), and some subsequences are typical of a given style (for example a disco rhythm has the bass drum played on each beat). Furthermore, a drum sequence may contain repetitive patterns that span several hierarchical levels. For instance, a simple one-bar-long pattern may be repeated to create a musical phrase, this phrase may be repeated several times during the chorus, which itself is played several times during the piece. The aim of this paper is to present two strategies to include such information in drum transcription systems: a supervised strategy, based on a generalization of N-gram models (described in section 3); and an unsupervised strategy (described in section 4) which aims to find the drum sequence that exhibits the largest degree of repetitivity and structure, while still being compatible with the acoustic scores. Both these methods operate on a symbolic representation of the drum sequence. We therefore briefly discuss in section 2 how to obtain such a representation from a list of unquantized onsets and posterior probabilities produced by a drum transcription system. Finally, experimental results are given in section 5, and some conclusions are suggested in the last section. 2 SYMBOLIC REPRESENTATION EXTRACTION Drum transcription systems output a sequence of pairs (ti, πij)1≤i≤N where πij expresses the probability that the drum instrument Ij is played at time ti 1 . In this study, we focus on three drum instruments: I1 = bass drum, I2 = snare drum and I3 = hi-hat. 1 In some drum transcription systems, the (ti) are obtained by an onset detector, and the posterior probabilities (πij) by probabilistic models trained for each class of drum instrument (bass drum, snare drum...) to detect. Some other systems may require additional post-processings to output a probability score, for example from a detected amplitude or a distance to a template.",
        "zenodo_id": 1417237,
        "dblp_key": "conf/ismir/GilletR07",
        "keywords": [
            "drum transcription systems",
            "symbolic representation",
            "quantizing onsets",
            "fusing posterior probabilities",
            "N-gram model",
            "unsupervised algorithm",
            "complexity criterion",
            "musical phrase",
            "repetitive patterns",
            "musical genre recognition"
        ],
        "content": "SUPERVISED AND UNSUPERVISED SEQUENCE MODELLING FOR\nDRUM TRANSCRIPTION\nOlivier Gillet, Ga ¨el Richard\nGET / T ´el´ecom Paris (ENST)\nCNRS LTCI\n37 rue Dareau, 75014 Paris, France\nABSTRACT\nWe discuss in this paper two post-processings for drum\ntranscription systems, which aim to model typical proper-\nties of drum sequences. Both methods operate on a sym-\nbolic representation of the sequence, which is obtained by\nquantizing the onsets of drum strokes on an optimal tatum\ngrid, and by fusing the posterior probabilities produced by\nthe drum transcription system. The ﬁrst proposed method\nis a generalization of the N-gram model. We discuss sev-\neral training and recognition strategies (style-dependent\nmodels, local models) in order to maximize the reliability\nand the speciﬁcity of the trained models. Alternatively, we\nintroduce a novel unsupervised algorithm based on a com-\nplexity criterion, which ﬁnds the most regular and well-\nstructured sequence compatible with the acoustic scores\nproduced by the transcription system. Both approaches\nare evaluated on a subset of the ENST-drums corpus, and\nyield performance improvements.\n1 INTRODUCTION\nMany useful applications can be derived from the knowl-\nedge of a semantic description of music signals. As a re-\nsult, the ﬁeld of music information retrieval (MIR) is re-\nceiving a continuously growing interest from the scientiﬁc\ncommunity. MIR has primarily focused on the extraction\nof melodic and tonal information, though it is now ac-\nknowledged that the rhythmic content, and the drum track\nin particular, is of primary importance for a number of ap-\nplications such as drum track remixing, automatic genre\nrecognition, automatic DJing or query by beatboxing.\nThe problem of drum transcription has already been\naddressed in several studies (see [1] for a review of exist-\ning systems). However, most of the studies on drum track\ntranscription only use short-term acoustic information (as\ncarried in acoustic features, or for example, in the coefﬁ-\ncients of a non-redundant decomposition), and thus con-\nsider each drum event independently from the other adja-\ncent events. Nevertheless, by analogy with speech, where\na sequence of random phonemes does not constitute a syn-\ntactically correct sentence, most of the sequences of drum\nevents do not represent musically interesting drum tracks.\nc/circlecopyrt2007 Austrian Computer Society (OCG).In fact, as already shown in previous works [2, 3, 4], the\nacoustic clues should be combined with sequence models\nto take into account the structural speciﬁcities of drum se-\nquences. Some of these speciﬁcities are listed here: some\ndrum subsequences may never be played (either because\nthey are musically irrelevant or because they are too com-\nplex to be played by a drummer), some subsequences are\nfrequent, independently of the style (a tom ﬁll for exam-\nple), and some subsequences are typical of a given style\n(for example a disco rhythm has the bass drum played\non each beat). Furthermore, a drum sequence may con-\ntain repetitive patterns that span several hierarchical lev-\nels. For instance, a simple one-bar-long pattern may be\nrepeated to create a musical phrase, this phrase may be\nrepeated several times during the chorus, which itself is\nplayed several times during the piece.\nThe aim of this paper is to present two strategies to in-\nclude such information in drum transcription systems: a\nsupervised strategy, based on a generalization of N-gram\nmodels (described in section 3); and an unsupervised strat-\negy (described in section 4) which aims to ﬁnd the drum\nsequence that exhibits the largest degree of repetitivity and\nstructure, while still being compatible with the acoustic\nscores. Both these methods operate on a symbolic rep-\nresentation of the drum sequence. We therefore brieﬂy\ndiscuss in section 2 how to obtain such a representation\nfrom a list of unquantized onsets and posterior probabil-\nities produced by a drum transcription system. Finally,\nexperimental results are given in section 5, and some con-\nclusions are suggested in the last section.\n2 SYMBOLIC REPRESENTATION EXTRACTION\nDrum transcription systems output a sequence of pairs\n(ti, πij)1≤i≤Nwhere πijexpresses the probability that\nthe drum instrument Ijis played at time ti1. In this study,\nwe focus on three drum instruments: I1=bass drum,\nI2=snare drum and I3=hi-hat.\n1In some drum transcription systems, the (ti)are obtained by an on-\nset detector, and the posterior probabilities (πij)by probabilistic models\ntrained for each class of drum instrument (bass drum, snare drum...) to\ndetect. Some other systems may require additional post-processings to\noutput a probability score, for example from a detected amplitude or a\ndistance to a template.2.1 Temporal quantization\nFor further processing, the detected events should be aligned\non the same quantized time basis or grid. An ideal time\nbasis for this alignment is the tatum , which is deﬁned as\nthe pulsation that most highly coincides with all note on-\nsets. Several approaches have been proposed to extract the\ntatum directly from an audio signal or from the inter-onset\nintervals (IOI). In this work, we used a histogram-based\nmethod similar to the one described in [5]. A smoothed\nhistogram of the observed IOIs is considered, and each\nsubdivision of the most frequent IOI is a tatum candidate.\nAmong the candidates, the one whose multiples coincide\nthe most with peaks in the IOI histogram is selected.\nOnce the tatum τis estimated, a tatum grid G(φ) =\n{φ+iτ,0≤i≤L\nτ}can be considered to quantize events.\nThe phase parameter φcan be obtained to maximize the\ncoincidences of the grid with note onsets. However, it\nis also important to adjust each point of the quantization\ngrid to take into account slight tempo changes (ritardandi,\naccelerandi), swing, and the limited temporal resolution\nof the tatum estimation algorithm. In this paper, a simple\napproach based on dynamic programming is followed, to\noptimize the individual position of each tatum event, in an\ninterval around the initial position. We further denote τn\nthe position of the n-th point on the tatum grid.\n2.2 Temporally aligned acoustic scores\nThe next step consists in representing the rhythmic se-\nquence as a set of symbols Sn∈A, each of them mapped\nbyφto a combination of drum instruments played at time\nτnof the tatum grid. φ(Sn)can be any subset of the\nsetA={bass drum ,snare drum ,hi-hat}of drum instru-\nments of interest.\nLetTnbe the set of indices of the onsets tiwhose clos-\nest tatum point is τn(i.e.Tn={i, n= arg min k|τk−\nti|}). For a given symbol Sdenoting a combination φ(S)⊂\nAof drum instruments, the probability that this combina-\ntion is played at τnis given by:\nP(Sn=S|t, π) =/productdisplay\nj/braceleftbigg1−/producttext\ni∈TnπijifIj∈φ(S)/producttext\ni∈TnπijifIj/∈φ(S)\n(1)\nwhere πij= (1−πij). For example, the probability that\nthe symbol denoting the combination {sd, hh }is played\nat time τnis calculated as the probability that there is at\nleast one snare drum and one hi-hat strokes detected in the\ninterval related to τnand that there is no bass drum stroke\ndetected in this same interval.\nAs a result of the processes described in this section,\nthe output of the drum transcription system can be repre-\nsented as a sequence of tatum pulsations τn, and proba-\nbilities P(Sn=S); a candidate transcription being rep-\nresented as a sequence sn. Using only the acoustic clues,\nthe most likely transcription is simply:\ns∗\nn= arg max\ns∈AP(Sn=s)In the next section, we propose several means of combin-\ning these acoustic cues with sequence models. We also\nconsider, in section 4, a different decision rule which in-\ncludes a regularization term penalizing complex transcrip-\ntions.\n3 SUPERVISED SEQUENCE MODELLING\n3.1 Generalized N-gram models\nSeveral techniques have been proposed in the past to de-\nscribe the dependencies of symbols Snin a drum sequence.\nThe most straightforward approach (as followed in [6]) is\nthe traditional N-gram model, where the probability to\nobserve a sequence s= (sn)1≤n≤Lis:\nP(s) =L/productdisplay\nn=1P(sn|sn−1. . . s n−N+1) (2)\nA different approach was proposed in [3] where the de-\npendencies are tracked at the level of successive bars. In\nthis periodic N-gram model, the probability to observe a\nsequence sis given by:\nP(s) =L/productdisplay\nn=1P(sn|sn−M. . . s n−(N−1)M) (3)\nIn this paper we propose a generalization of the previ-\nous approaches by considering dependencies at different\nscales. Let Tbe the sequence model time support – in\nother words, the temporal levels at which the dependen-\ncies are considered. Following this model, the probability\nto observe a sequence sis then equal to:\nP(s) =L/productdisplay\nn=1P(sn|sn−T1. . . s n−TN) (4)\nNote that the traditional N-gram model corresponds to\nthe case where T= (1,2, . . . , N −1)and that the periodic\nN-gram to the case where T= (M,2M, . . . , (N−1)M).\nAs such, our model allows the use of a time support T\nthat achieves a trade-off between the observation length\nand the number of probabilities to estimate. For instance,\nwhen the tatum corresponds to a sixteenth note, with a/parenleftbig4\n4/parenrightbig\ntime signature, the choice T= (1,4,16)allows the model\nto learn dependencies between successive bars, beats, and\ntatum points, while keeping the overall complexity and the\nnumber of degrees of freedom of the model low.\n3.2 Probabilities estimation\nThe learning of sequence models consists in estimating\nthe probabilities of observing a given symbol snknowing\nits context. These probabilities can simply be estimated\nby counting in a training corpus the number of times snis\nobserved in a given context normalized by the number of\ntimes this context is encountered. This simple approach\nis unable to deal with infrequent or unseen substrings in\nthe training corpus. In this work, we used Witten-Bell\nsmoothing [7] to estimate the probabilities in such cases.3.3 Most likely sequence\nGiven the probabilities P(Sn=S|t, π)(noted P(S|t, π)\nfor sake of clarity), and a generalized N-gram model of\ntime support T, the most likely sequence is:\narg max\ns/productdisplay\n1≤n≤LP(sn|t, π)P(sn|sn−T1. . . s n−TN)(5)\nThis optimal sequence can be approximated with a causal\ngreedy search ( O(L|A|)complexity) or the non-causal Viterbi\nalgorithm ( O(L|A|2)complexity). It can also be found\nexactly by a Viterbi search through the space of all con-\ntexts (O(L|A|TN+1)complexity), where |A|is the num-\nber of rhythmic symbols.\n3.4 Training sequence models: What to learn?\nSo far, we considered that a training corpus is available\nto estimate the probabilities ˆP(sn|sn−T1. . . s n−TN). An\nopen question is the choice of this training corpus. Here,\nwe discuss several strategies, and evaluate the predictive\npower of the models that can be obtained by following\nthem.\nGeneric model The training corpus consists in a set of\nheterogenous sequences of different styles played by dif-\nferent drummers. This approach is the simplest to follow:\none unique model has to be trained once and for all.\nDrummer-dependent model The training corpus contains\nonly sequences played by the same drummer as the se-\nquences to transcribe. Such an approach is only feasible\nfor a few limited applications – where the system can be\ncalibrated to a given performer.\nStyle-dependent model The training corpus contains only\nsequences of a given style (e.g. salsa, reggae or rock)\nplayed by various drummers. Thus, a different model is\ntrained for each style. In order to select the model to\nbe used for the recognition, several approaches are pos-\nsible: an independent style classiﬁcation system can be\nused (hierarchical classiﬁcation), a human user with in-\nfallible skills can select the model corresponding to the\nsequence (classiﬁcation with style oracle), or the recogni-\ntion can be performed in parallel by each model, the ﬁnal\nresult being the one produced by the model having the\nhighest likelihood.\nIndividual sequence model We assume, in this case, that\nthe sequence to be transcribed is known in advance, and\nthat we can estimate the probabilities from this sequence.\nThis approach is of limited interest, except for applica-\ntions like computer aided teaching of drumming, or score\nfollowing, where the score is known in advance. A more\nfeasible solution consists in using an initial model (for ex-\nample, a generic model) to obtain a ﬁrst transcription; and\nthen to train a local individual sequence model on the rec-\nognized sequence. Assuming that the errors made by the\ntranscription system are independent of the context, the\nprobabilities estimated from the recognized sequence willT Generic Drummer Style Sequence\nGeneralized trigrams\n-2,-1 0.153 0.237 0.357 0.405\n-4,-1 0.157 0.237 0.347 0.396\n-8,-1 0.192 0.262 0.359 0.403\n-16,-1 0.185 0.254 0.348 0.391\n-4,-2 0.179 0.253 0.356 0.398\n-8,-2 0.204 0.265 0.353 0.390\n-16,-2 0.213 0.273 0.370 0.407\n-8,-4 0.219 0.279 0.354 0.392\n-16,-4 0.196 0.254 0.344 0.380\n-16,-8 0.229 0.283 0.348 0.379\n-32,-16 0.208 0.264 0.325 0.361\nGeneralized quadrigrams\n-3,-2,-1 0.281 0.414 0.523 0.552\n-4,-2,-1 0.297 0.429 0.528 0.555\n-8,-2,-1 0.307 0.429 0.531 0.558\n-16,-8,-1 0.311 0.423 0.517 0.546\n-8,-4,-2 0.318 0.428 0.515 0.540\n-16,-4,-2 0.308 0.418 0.525 0.551\n-16,-8,-2 0.322 0.423 0.514 0.541\n-16,-8,-4 0.312 0.408 0.500 0.526\n-48,-32,-16 0.309 0.403 0.470 0.504\nTable 1 . Predictive power of the sequence model, mea-\nsured by the mutual information between a symbol and its\ncontext I(C, S), for different time supports Tand training\ncorpora (all of them subsets of the ENST-drums database).\nbe close to those estimated on the correct sequence. The\nrecognition is then performed using this local model.\nThese four approaches are evaluated by comparing the\npredictive power of the learned sequence models, which is\nmeasured by the mutual information between a rhythmic\nsymbol sand its context c(this context being deﬁned by\nthe support T):\nI(C, S) =/summationdisplay\nc∈AN−1/summationdisplay\ns∈AP(cs) log|A|P(cs)\nP(c)P(s)(6)\nSince I(C, S) =H(S)−H(S|C), the mutual infor-\nmation measures the certainty with which a symbol is de-\ntermined, knowing its context. A null value implies that\nthe context has no predictive power on the observed sym-\nbol. The results obtained on the different training corpora\n(Refer to section 5 for a description of the database) with\ndifferent time supports Tare summarized in table 1.\nFirstly, the results show that drummer dependent mod-\nels are only slightly more efﬁcient than generic models.\nSuch models have a lower predictive power than style-\ndependent models. Secondly, the efﬁciency of style-dependent\nmodels, and their predictive power close to the one of in-\ndividual sequence models, suggests that sequences played\nin a speciﬁc style are rather homogeneous and played with\na limited degree of variability. Finally the results highlight\nthe usefulness of the generalized N-grams introduced: in\nfact, they are, in most cases, more efﬁcient than traditional\nN-grams and pure periodic N-grams since they can simul-\ntaneously track short and long term dependencies. How-ever, it is worth noting that we have only evaluated the\npredictive power of the sequence model here. Such mod-\nels might not offer any improvement in a full transcription\nsystem, as their efﬁciency depends on the reliability of the\nacoustic scores.\n4 UNSUPERVISED SEQUENCE MODEL\nAlthough powerful, the previously described supervised\napproach suffers from two main drawbacks. On the one\nhand, it needs a training phase for which a trade-off be-\ntween genericity and predictive power needs to be found.\nOn the other hand, if higher performances can be obtained\nby a generalized N-gram approach, the choice of the time\nsupport Trequires prior knowledge of the duration of a\nbar or a sequence; and when N is large, the accuracy of\nthe estimated probabilities is poor. As a consequence, a\nnovel alternative approach, entirely unsupervised, is pro-\nposed in this section.\nThis approach is based on the following assumption:\ndrum sequences are built with rather regular and repetitive\npatterns at different time scales. The basic idea for our ap-\nproach is thus to correct (or rather simplify) the drum se-\nquence obtained by the transcription system so that it can\nbe more easily described in terms of hierarchical, repeti-\ntive patterns.\n4.1 Complexity criterion\nThe Kolmogorov complexity of a sequence K(S)is de-\nﬁned as the length of the shortest program, represented\nwith a binary alphabet for a given model of computation\n(for example, a universal Turing Machine), which outputs\nthe sequence S.K(S)is not computable, but can be ap-\nproximated by compression algorithms. In this case, the\nshortest program generating Sis a compressed version of\nSfollowed by a program decompressing it.\nComplexity criteria have been used in [8] and [9] to\nmeasure the similarity between melodies; and in [10] to\ndetect the melody in a polyphonic piece (the main melody\nis considered to be the part of maximal complexity). All\nthese studies use the LZ77 or LZ78 [11] compression al-\ngorithms as an approximation of Kolmogorov complexity.\nHere, we propose to use a different compression algo-\nrithm to measure the complexity of rhythmic sequence:\nthe S EQUITUR algorithm [12]. First, this algorithm out-\nperforms LZ78 for various text compression tasks, and\nthus, gives a better approximation of the minimal descrip-\ntion length. Moreover, this algorithm infers from the ob-\nserved sequence, not a dictionary of frequent preﬁxes (as\nis the case with LZ78), but a context-free grammar, which\nthus takes into account the hierarchical and recursive struc-\nture of the sequence. Finally, this algorithm can be easily\nmodiﬁed to include domain-speciﬁc transformation oper-\nators (transposition, time-reversal, etc.) in the inferred\ngrammar.4.1.1 Inferring a grammar from a sequence\nWe recall here the principle of the S EQUITUR algorithm,\nwhich processes the sequence, symbol by symbol, from\nleft to right, to update its representation as a context-free\ngrammar Gverifying the following two properties:\nBi-gram uniqueness a bigram should not appear more\nthan once2in the right member of a production rule. Two\ncases are possible:\n•when Gcontains the rules A→XabY andB→\nZabT , a new rule C→abis created and original\nrules are modiﬁed as A→XCY andB→ZCT .\n•when Gcontains the rules A→XabY andB→\nab, the ﬁrst rule is modiﬁed into A→XBY .\nUsefulness of a rule . Each production rule should be used\nat least twice. Thus, if the grammar contains A→XBY\nandB→ZT, and if the non-terminal Bonly appears in\nthe ﬁrst rule, the second rule is deleted and the ﬁrst rule\nbecomes A→XZTY .\nAs an example, the following grammar will be inferred\nfrom the sequence abcbcabcbc :\nS→AA A→aBB B→bc\n4.1.2 Complexity from SEQUITUR\nA context-free grammar is entirely deﬁned by the string\nobtained by joining the right members of the production\nrules with a separation marker noted #. For example, the\ngrammar given in the previous example is entirely deﬁned\nby the string AA#aBB #bc. If an entropic code (e.g.\na Huffman code) is used to compress this sequence, an\napproximation of the length of the corresponding binary\nmessage is given by:\nl(G)≈/summationdisplay\na∈Ω−C(a) log2C(a)\nN(7)\nwhereC(a)\nNis the frequency of symbol ain the se-\nquence, Nthe length of the sequence and Ωthe alphabet\nof symbols. The whole procedure for the approximation\nof the complexity of a rhythmic sequence is summarized\nbelow:\n1. Inference of a context-free grammar G(s)from the\nsequence swith S EQUITUR .\n2. Reduction of the grammar production rules into a\nstring.\n3. Compression of this string into a binary message of\nlength l(G(s))with an entropic code.\n2Although it is not used in this work, it is possible to generalize this\nrule to include bijective transformations in the production rules (such\nrules would be of the form A→ϕ(B)C, where ϕis the transforma-\ntion). In the context of note sequences, useful transformations that can\nbe considered include transposition or reversal. In the context of drum\nsequences, a useful transformation is the substitution of one cymbal by\nanother – for example, a sequence can be repeated with the ride cymbal\nplayed instead of the closed hi-hat. In this work, we only consider the\nhi-hat, and thus do not use such rules.We observed in our database that the average complex-\nity of the original (ground-truth annotation) drum sequences\nis 984 bits per sequence, while the average complexity\nof the corresponding transcriptions3is 1179 bits per se-\nquence. This supports our initial assumption that the er-\nrors made by the transcription system break the structure\nand repetitiveness of the drum sequences.\n4.2 Penalized likelihood for sequences\nWe propose the following penalized likelihood criterion in\norder to ﬁnd the sequence that is both simple, according\nto the previously deﬁned complexity measure, and com-\npatible with the acoustic scores:\ns∗= arg max\nsF(s) (8)\nF(s) =L/summationdisplay\nn=1logP(Sn=sn|t, π)−αl(G(s))(9)\nThe ﬁrst term requires the sequence to be compatible\nwith the acoustic score, and the second term penalizes\ncomplex sequences. Unfortunately, there exists to our knowl-\nedge no deterministic algorithm to ﬁnd s∗(above all, con-\ntrary to the model we described in section 3, dynamic\nprogramming cannot be used), and a search in the space\nof all possible sequences is obviously intractable. Ge-\nnetic algorithms appear as an interesting approach to solve\nthis problem, especially because in our case, there exists\na trivial representation of the parameter to optimize as\n“chromosomes”, for which the cross-over operator makes\nsense: a good transcription is likely to be obtained by\ncombining parts of good transcriptions. The procedure is\ndescribed below :\nInitialization of a population of Npop= 200 sequences\n(si). This population is initialized with mutations of the\nbest sequence obtained without the complexity penaliza-\ntion term, i.e. arg max s/summationtextL\nn=1logP(Sn=sn|t, π).\nReproduction Nexp= 4Npopchildren sequences are pro-\nduced by the following procedure:\n1. Random choice of two parents s1ands2amongst\nthe current population.\n2. Crossing-over. A recombination point p∈[1, L]is\nrandomly chosen. The child sequence is then deter-\nmined by sc(n) =s1(n),∀n∈[1, p]andsc(n) =\ns2(n),∀n∈[p+ 1, L].\n3. Mutation. A mutation position p∈[1, L]is ran-\ndomly chosen. The probability that the symbol at\nposition pmutates into ais given by the acoustic\nscore P(Sp=a|t, π).\nSelection . A population of Npopsequences survives. This\npopulation contains the 0.9Npopsequences for which the\ncriterion Fis the largest and 0.1Npopsequences randomly\nselected amongst the remaining sequences.\n3Produced by a system described in yet unpublished work.BD SD HH BD SD HH\nBaseline Unsupervised\n79.4 59.6 76.7 81.3 61.7 80.4\nIndiv. sequence Style-dep. model\nT with sequence oracle with style oracle\n-1 82.6 63.3 79.2 79.4 60.4 78.0\n-2,-1 82.0 67.0 80.6 80.2 60.9 79.7\n-4,-1 81.7 64.6 80.9 80.5 61.2 79.5\n-8,-1 82.3 63.8 80.3 81.2 61.9 79.1\n-16,-1 81.0 63.2 80.2 80.2 60.2 78.8\n-3,-2,-1 80.9 66.7 81.5 78.1 61.8 80.3\n-4,-2,-1 82.2 65.7 82.5 78.5 60.1 79.3\n-8,-2,-1 81.2 65.4 81.2 78.7 59.8 79.3\n-16,-2,-1 82.4 66.0 82.1 78.8 59.2 78.6\n-4,-3,-2,-1 78.7 66.0 82.9 76.4 61.5 80.3\n-16,-8,-2,-1 81.4 64.7 81.3 79.3 59.0 79.4\nwith local model with most likely style\n-1 80.8 60.2 77.9 79.4 60.4 78.0\n-2,-1 81.3 60.6 78.2 80.2 60.8 79.6\n-4,-1 81.2 61.2 77.6 80.9 60.9 78.7\n-8,-1 81.0 60.9 77.8 81.2 61.4 78.8\n-16,-1 81.2 60.1 77.7 80.1 60.1 78.6\n-3,-2,-1 81.3 60.8 77.2 78.1 61.8 80.3\n-4,-2,-1 81.6 61.1 77.6 77.4 59.8 78.8\n-8,-2,-1 81.5 61.1 77.7 77.2 59.9 78.6\n-16,-2,-1 81.6 60.8 77.2 78.5 59.1 78.2\n-4,-3,-2,-1 81.1 61.0 77.5 75.4 61.6 80.0\n-16,-8,-2,-1 81.5 60.1 76.5 79.3 59.0 79.1\nTable 2 . Performance of the supervised sequence mod-\nels for several contexts and training strategies, and of the\nunsupervised error correction method\nIteration onN= 50 generations.\nA speciﬁcity of this implementation is the control of\nthe mutation probabilities. This trick avoids the explo-\nration of regions of the solution space where the likelihood\nterm is too low. Actually, we observed that even when\nα >> 1(i.e., when the regularization term dominates the\nlikelihood term), the solutions have a high likelihood. We\nusedα= 0.5in the following experiments.\n5 EXPERIMENTAL RESULTS\nThe experiments were conducted on the minus one se-\nquences of the ENST-Drums corpus [13], with a balanced\nmix between drums and musical accompaniment to recre-\nate realistic use conditions. This corpus consists of 17\nsequences, of various genre, played by 3 drummers – we\nselected for these experiments the sequences played by the\nsecond and third drummers. For each instrument, the per-\nformance is measured by the F-measure. Results are given\nin table 2. The baseline corresponds to the raw output of\nthe drum transcription system, without sequence model-\ning.\nFirst of all and not surprisingly, we observe that the\ngreater gains are obtained by using an individual sequence\nmodel with oracle – that is to say, by using a sequence\nmodel trained in advance on the sequence to be recog-nized. Interestingly, the best performances for each in-\nstrument are achieved by considering a different context:\nthe hi-hat requires long contexts, while the snare drum and\nbass drum require shorter context. The best results are ob-\ntained with generalized quadrigrams taking into account\nthe local context, and the event played 4 or 8 tatum points\nearlier. The performance gain offered by the local model\nis lower – this is probably due to the lack of training data\nfor the adapted model.\nThe performance gains offered by the style-dependent\nmodels (be it with automatic style detection, or with an\noracle indicating the style in which the sequence is per-\nformed) are very similar. Actually, the performance of\nthe style identiﬁcation stage (which consists in selecting\nthe style-dependent model with the largest likelihood) is\nrather satisfying – the model corresponding to the style in\nwhich the sequence was played was selected 70% of the\ntime. We observed that even when the wrong model was\nselected, some errors were eliminated in the transcription,\nas a model from a different style still carried general prop-\nerties of drum sequences.\nThe unsupervised method based on complexity mini-\nmization performed similarly as style-dependent models.\nHowever, its usefulness is hampered by its large computa-\ntional cost.\n6 CONCLUSION AND FUTURE WORK\nWe described in this work two methods to improve the\noutput of drum transcription systems by modeling sev-\neral typical properties of drum sequences. Firstly, a su-\npervised method based on a generalization of the N-gram\nmodels is described. We particularly focused on the se-\nlection of the corpus on which such models should be\ntrained, and we proposed several strategies and classiﬁca-\ntion schemes to efﬁciently use such models, since we ob-\nserved that their predictive power is limited on too diverse\ncorpora. Secondly, an unsupervised method based on a\ncomplexity criterion is introduced. This criterion favors\nsequences which exhibit a well-deﬁned structure – more\nprecisely, which can be described by a compact context-\nfree grammar. However, there is, to our knowledge, no\nefﬁcient way to maximize this criterion. In this work,\nwe used genetic algorithms with controlled mutation rates\nas a heuristic to efﬁciently explore the space of possible\ndrum sequences. Future work should address the prob-\nlem of ﬁnding more efﬁcient algorithms or heuristics to\nminimize this criterion. Even if we are pessimistic about\nthe existence of a solution of polynomial complexity, the\nuse of such complexity penalization terms could beneﬁt to\nother MIR applications such as music transcription.\nOur experimental results showed performance gains for\nthe two methods. However, these gains remain modest.\nWe suggest that this result is not due to the sequence mod-\nels themselves, but rather to the lack of reliability of the\nacoustic scores produced by the transcription system. Our\ninitial assumption that classiﬁers or detectors produce pos-\nterior probabilities close to the decision boundary, but onthe wrong side, when they encounter a difﬁcult case, is\nwrong: we rather observed that most of the mistakes made\nby classiﬁers indeed corresponded to posterior probabili-\nties far from the decision boundary. It is thus very likely\nthat the performance of sequence models is bounded by\nthe performance of the detection or classiﬁcation module.\n7 REFERENCES\n[1] D. FitzGerald and J. Paulus. Unpitched percussion\ntranscription. In A. Klapuri and M. Davy, editors,\nSignal processing methods for the automatic tran-\nscription of music , pages 131–162. Springer, 2006.\n[2] O. Gillet and G. Richard. Automatic labelling of\nTabla signals. In Proc. ISMIR’03 , October 2003.\n[3] J. Paulus and A. Klapuri. Conventional and periodic\nn-grams in the transcription of drum sequences. In\nProc. ICME’2003 , 2003.\n[4] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and\nH. Okuno. An error correction framework based on\ndrum pattern periodicity for improving drum sound\ndetection. In Proc. ICASSP’06 , May 2006.\n[5] C. Uhle and J. Herre. Estimation of tempo, micro\ntime and time signature from percussive music. In\nProc. DAFX’03 , September 2003.\n[6] O. Gillet and G. Richard. Automatic transcription of\ndrum loops. In Proc. ICASSP’04 , May 2004.\n[7] I. H. Witten and T. C. Bell. The zero-frequency prob-\nlem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Trans. Inf. Theory ,\n37(4):1085–1094, 1991.\n[8] R. Cilibrasi, P. Vitanyi, and R. De Wolf. Algorith-\nmic clustering of music based on string compression.\nComputer Music Journal , 28(4):49–67, 2004.\n[9] M. Li and R. Sleep. Melody classiﬁcation using a\nsimilarity metric based on kolmogorov complexity.\nInProc. of the 2nd Conf. on Sound and Music Com-\nputing , 2005.\n[10] S. T. Madsen and Gerhard Widmer. Music complex-\nity measures predicting the listening experience. In\nProc. 9th Int. Conf. Music Perception and Cognition ,\n2006.\n[11] J. Ziv and A. Lempel. Compression of individual\nsequences via variable-rate coding. IEEE Trans. Inf.\nTheory , 24(5):530–536, September 1978.\n[12] C. G. Nevill-Manning, I. H. Witten, and D. L.\nMaulsby. Compression by induction of hierarchical\ngrammars. In Proc. of the Data Compression Conf. ,\npages 244–253, 1994.\n[13] O. Gillet and G. Richard. ENST-drums: an extensive\naudio-visual database for drum signals processing.\nInProc. ISMIR’06 , 2006."
    },
    {
        "title": "An Analysis of the Mongeau-Sankoff Algorithm for Music Information Retrieval.",
        "author": [
            "Carlos Gómez",
            "Soraya Abad-Mota",
            "Edna Ruckhaus"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417931",
        "url": "https://doi.org/10.5281/zenodo.1417931",
        "ee": "https://zenodo.org/records/1417931/files/GomezAR07.pdf",
        "abstract": "An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure. In this work we implemented a modified version of the Mongeau-Sankoff measure. We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition. The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures. 1 PROPOSED APPROACH Throughout the development of music information retrieval systems, several melodic similarity measures have been proposed. A melodic similarity measure is a function that given two melodies, estimates the degree of identity that can be established between them, expressing it as a real number. This similarity measure is ideally a “continuous” function that is expressed in musical terms. This work studies the Mongeau-Sankoff measure [2] in the context of music information retrieval, and compares it with some of the existing measures. The MongeauSankoff measure is based on the alignment of musical sequences, where the elements of the sequences are notes with a specific pitch and duration. The similarity between the sequences is expressed in terms of the edit distance between them, which depends on the series of edit operations of least cost that is needed to transform one sequence into the other. In addition to the traditional edit operations (insertion, deletion and substitution), the MongeauSankoff measure defines two operations that apply specifically to music: fragmentation and consolidation. The measure was designed in 1990, and even though it has been applied previously to music information retrieval systems, to the best of our knowledge, no recent implementation of the measure has been developed with the specific purpose of comparing it to the algorithms evaluated c⃝2007 Austrian Computer Society (OCG). in a Music Information Retrieval Evaluation eXchange (MIREX) edition, a specific goal of our project. In this work we propose variants to the measure that are related to several aspects: achieving transposition invariance when the key of queries and the key of documents is not known, computing the interval weight according to its distance, and representing the relative cost of the operations. Mongeau and Sankoff originally applied their measure to the study of musical variation; therefore, the measure was not conceived specifically for retrieving musical documents in a collection. The original measure assumes that the key of the two melodies that are being compared is known; based on this knowledge, the measure attains transposition invariance. In the general case of a music information retrieval system, both the key of the query and the key of the documents could be unknown, therefore, it is necessary to adapt the measure in this aspect. In this work, we adopt the approach of making all the possible transpositions of the query with respect to the target, and consider the similarity result as the best value between all the transpositions. Other researchers in this area have applied this approach to Mongeau-Sankoff, and also to other measures. Another variant of the measure pertains to the weight of the operations that involve the substitution of a note. In the original measure, this weight depends partially on the interval between the note that is being substituted and the new note, and it is defined based on the consonance of the interval. In this work we adopt another approach, which consists of assigning a weight to the interval in terms of the distance between the two notes. This approach has been applied in previous works [1]. Finally, a change was made to the cost functions of the measure. The weight of an operation is the sum of two quantities, that represent the weight associated to the pitch difference between the notes involved in the operation, and that of the duration difference. A parameter of the measure determines the proportion in which the duration difference contributes to the weight, versus that of pitch difference. This constant was originally the same for all operations. In our work, we associate a separate constant to the weight formula of insertions and deletions with respect to the other operations. This allows to vary the relative weight of insertions and deletions with respect to these other operations. Thus, in this work we propose a Alignment ADR NRGB Average Prec. R-Prec. Total time Local∗",
        "zenodo_id": 1417931,
        "dblp_key": "conf/ismir/GomezAR07",
        "keywords": [
            "melodic similarity measures",
            "Mongeau-Sankoff measure",
            "music information retrieval",
            "edit distance",
            "fragmentation",
            "consolidation",
            "MIREX symbolic melodic similarity competition",
            "experimental study",
            "implementation",
            "effectiveness"
        ],
        "content": "AN ANALYSIS OF THE MONGEAU-SANKOFF ALGORITHM FOR\nMUSIC INFORMATION RETRIEVAL\nCarlos G ´omez Soraya Abad-Mota Edna Ruckhaus\nDept. of Computing and Information Technology\nUniversidad Sim ´on Bol ´ıvar, Venezuela\nABSTRACT\nAn essential problem in music information retrieval is to\ndetermine the similarity between two given melodies; there\nare several melodic similarity measures that have been\nproposed, among others, the Mongeau-Sankoff measure.\nIn this work we implemented a modiﬁed version of the\nMongeau-Sankoff measure. We conducted an experimen-\ntal study to compare the implemented measure with other\nsimilarity measures; this evaluation was done in the con-\ntext of the 2005 edition of the MIREX symbolic melodic\nsimilarity competition. The most relevant result of our\nwork is an implementation of the Mongeau-Sankoff mea-\nsure that presents greater effectiveness when compared to\nother current melodic similarity measures.\n1 PROPOSED APPROACH\nThroughout the development of music information retrie-\nval systems, several melodic similarity measures have been\nproposed. A melodic similarity measure is a function that\ngiven two melodies, estimates the degree of identity that\ncan be established between them, expressing it as a real\nnumber. This similarity measure is ideally a “continuous”\nfunction that is expressed in musical terms.\nThis work studies the Mongeau-Sankoff measure [2] in\nthe context of music information retrieval, and compares\nit with some of the existing measures. The Mongeau-\nSankoff measure is based on the alignment of musical se-\nquences, where the elements of the sequences are notes\nwith a speciﬁc pitch and duration. The similarity between\nthe sequences is expressed in terms of the edit distance\nbetween them, which depends on the series of edit opera-\ntions of least cost that is needed to transform one sequence\ninto the other. In addition to the traditional edit opera-\ntions (insertion, deletion and substitution), the Mongeau-\nSankoff measure deﬁnes two operations that apply specif-\nically to music: fragmentation and consolidation . The\nmeasure was designed in 1990, and even though it has\nbeen applied previously to music information retrieval sys-\ntems, to the best of our knowledge, no recent implemen-\ntation of the measure has been developed with the spe-\nciﬁc purpose of comparing it to the algorithms evaluated\nc/circlecopyrt2007 Austrian Computer Society (OCG).in a Music Information Retrieval Evaluation eXchange\n(MIREX) edition, a speciﬁc goal of our project.\nIn this work we propose variants to the measure that are\nrelated to several aspects: achieving transposition invari-\nance when the key of queries and the key of documents is\nnot known, computing the interval weight according to its\ndistance, and representing the relative cost of the opera-\ntions.\nMongeau and Sankoff originally applied their measure\nto the study of musical variation; therefore, the measure\nwas not conceived speciﬁcally for retrieving musical doc-\numents in a collection.\nThe original measure assumes that the key of the two\nmelodies that are being compared is known; based on this\nknowledge, the measure attains transposition invariance.\nIn the general case of a music information retrieval sys-\ntem, both the key of the query and the key of the doc-\numents could be unknown, therefore, it is necessary to\nadapt the measure in this aspect. In this work, we adopt\nthe approach of making all the possible transpositions of\nthe query with respect to the target, and consider the sim-\nilarity result as the best value between all the transposi-\ntions. Other researchers in this area have applied this ap-\nproach to Mongeau-Sankoff, and also to other measures.\nAnother variant of the measure pertains to the weight\nof the operations that involve the substitution of a note. In\nthe original measure, this weight depends partially on the\ninterval between the note that is being substituted and the\nnew note, and it is deﬁned based on the consonance of the\ninterval. In this work we adopt another approach, which\nconsists of assigning a weight to the interval in terms of\nthe distance between the two notes. This approach has\nbeen applied in previous works [1].\nFinally, a change was made to the cost functions of\nthe measure. The weight of an operation is the sum of\ntwo quantities, that represent the weight associated to the\npitch difference between the notes involved in the opera-\ntion, and that of the duration difference. A parameter of\nthe measure determines the proportion in which the du-\nration difference contributes to the weight, versus that of\npitch difference. This constant was originally the same\nfor all operations. In our work, we associate a separate\nconstant to the weight formula of insertions and deletions\nwith respect to the other operations. This allows to vary\nthe relative weight of insertions and deletions with respect\nto these other operations. Thus, in this work we propose aAlignment ADR NRGBAverage\nPrec.R-Prec.Total\ntime\nLocal∗0.7337 0.6422 0.5298 0.5361 6.882\nAll-to-some∗0.6940 0.6044 0.4734 0.4817 5.893\nLocal 0.6763 0.6046 0.4601 0.4800 6.251\nAll-to-some 0.6720 0.5771 0.4420 0.4501 5.444\nGlobal 0.6486 0.5586 0.4364 0.4369 5.494\nTable 1 . M-S with absolute representation.∗These con-\nﬁgurations implement the proposed variation on the rela-\ntive weight of the operations.\nmore general version of the cost formulas that could result\nin an improvement on the quality of the algorithm.\nIn our work, we implemented global and local align-\nment, as well as the additional alignment types described\nby Meek [1].\n2 EXPERIMENTAL STUDY\nWe conducted experiments to evaluate the effectiveness\nof the Mongeau-Sankoff algorithm as a similarity mea-\nsure for music retrieval, using the document collections\nand the ground truth from the symbolic melodic similarity\ntask of MIREX 2005. The experiments were divided into\na parameter optimization stage, and an algorithm evalu-\nation stage. The goal of the ﬁrst stage was to ﬁnd a set\nof parameters that maximizes the quality of the algorithm,\nusing the test collection from MIREX 2005. The goal of\nthe evaluation stage was to assess the quality of the al-\ngorithm using the evaluation collection of the contest and\nthe parameters determined in the ﬁrst stage. The purpose\nof separating parameter optimization from evaluation was\nto emulate the conditions of the 2005 contest, in which\nonly the test collection was known by the participants be-\nfore the contest, so as to avoid algorithm overﬁtting with\nrespect to the evaluation collection.\nWe studied several possible conﬁgurations of the algo-\nrithm according to the alignment type, and the kind of rep-\nresentation (absolute or modulo-12). Experiments were\nconducted with three types of alignment: global, local,\nandall-to-some .\nFor each conﬁguration, the optimal value of the al-\ngorithm parameters (the parameters of the weight func-\ntions) was determined using exhaustive search. The ob-\njective function to be maximized was the average of the\nfour quality measures used to evaluate the algorithms in\nthe melodic similarity task of MIREX 2005; a previous\nuse of this objective function can be found in one of the\nalgorithms participating in this contest.\nThe Mongeau-Sankoff algorithm was then applied to\nthe evaluation collection from MIREX 2005 with 11 que-\nries. The collection was composed of 558 documents.\nTwo of the applied quality measures take into account the\norder of the results in the ground truth for the computa-\ntion of the quality of the algorithm (ADR and NRGB),\nwhereas the other two correspond to traditional quality\nmeasures and consider all the documents in the ground\ntruth as equally relevant for a query.Alignment ADR NRGBAverage\nPrec.R-Prec.Total\ntime\nLocal 0.6219 0.5393 0.3919 0.4135 3.869\nAll-to-some 0.6030 0.5098 0.3728 0.3709 3.233\nTable 2 . M-S with modulo-12 representation\n 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75\nADR NRGB Avg. precision R-PrecisionM-S\nGrachten\nOrio\nSuyoto\nTypke\nLemström (P3)\nLemström (DP)\nFrieler\nFigure 1 . M-S vs. algorithms in MIREX 2005\nTables 1 and 2 show the results of the evaluation. In the\nfollowing discussion, the ADR was used as a criterion to\ncompare the results. The best quality was obtained with\na local alignment and an absolute representation, for the\nalgorithm variant with the proposed formula change; the\nADR was 0.7337 . In the MIREX 2005 evaluation, the al-\ngorithms obtained an ADR between 0.52 and 0.66. Thus,\nthe results of Mongeau-Sankoff with an absolute repre-\nsentation, for the basic algorithm and also for the version\nwith the proposed formula change, are superior to all the\nresults obtained in MIREX 2005 by the participating sim-\nilarity measures.\nFigure 1 shows the comparison between the best Mon-\ngeau-Sankoff conﬁguration, and the results obtained by\nthe algorithms in the contest.\nTables 1 and 2 report the total time (in seconds) that\nwas required by the Mongeau-Sankoff algorithm to an-\nswer the 11 queries (with an AMD Opteron of 2.2 GHz).\nThe time results show that the algorithm can have an ac-\nceptable response time for collections of similar size than\nthe one used. For larger collections, it would be possible\nto combine the algorithm with an indexing method.\nOur implementation with the proposed variants is more\neffective than all the other similarity measures evaluated\nin MIREX 2005. Thus, it can be concluded that the Mon-\ngeau-Sankoff measure is among the best existing melodic\nsimilarity measures.\n3 REFERENCES\n[1]Meek, C. Modelling error in query-by-humming ap-\nplications , Ph.D. dissertation, University of Michigan,\n2004.\n[2]Mongeau, M. and Sankoff, D. “Comparison of mu-\nsical sequences”, Computer and the Humanities , vol.\n24, pp. 161–175, 1990."
    },
    {
        "title": "Mood-ex-Machina: Towards Automation of Moody Tunes.",
        "author": [
            "Sten Govaerts",
            "Nik Corthaut",
            "Erik Duval"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415918",
        "url": "https://doi.org/10.5281/zenodo.1415918",
        "ee": "https://zenodo.org/records/1415918/files/GovaertsCD07.pdf",
        "abstract": "In 2006, the rockanango system was developed for music annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with lower level metadata like mode and key. The other case study concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher then laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata.",
        "zenodo_id": 1415918,
        "dblp_key": "conf/ismir/GovaertsCD07",
        "keywords": [
            "rockanango system",
            "music annotation",
            "expert creation",
            "flat data structure",
            "commercial environment",
            "manual annotation",
            "automatic metadata generation",
            "case studies",
            "association rules",
            "lower level metadata"
        ],
        "content": "MOOD-EX-MACHINA: TOWARDS AUTOMATION OF \nMOODY TUNES\nSten Govaerts, Nik Corthaut, Erik Duval \nKatholieke Universiteit Leuven \nDepartment of Computer Science \nCelestijnenlaan 200A \nB-3001 Heverlee, Belgium \n{Nik.Corthaut, Sten.Govaerts, Erik.Duval}@cs.kuleuven.be  \nABSTRACT \nIn 2006, the rockanango system was developed for \nmusic annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with \nlower level metadata like mode and key. The other case \nstudy concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher then laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata. \n1. INTRODUCTION \nLast year, we proposed a platform for music annotation, \nthe rockanango project [1]. This project is a co-\noperation between the Katholieke Universiteit Leuven \nand the company Aristo Music (http://www.aristomusic.com). In this project, we developed a context-based music player for the horeca (hotels, restaurants and cafés). The main idea behind this music player is a musical context. This is a description of a situation, based on atmospheres and musical parameters. The bartender or restaurant owner can describe a desired musical context by selecting different atmospheres and musical parameters, like genre, instrument, dance style, rhythm... With this \ndescription, a matching playlist is generated. This \nsystem is commercialized and currently running at around 400 pl aces in the Benelux and France. The music \ncollection consists of around 30,000 songs. \nThe system relies heavily on good qua lity metadata. \nEach song is manually annotated for each of the 22 \nmusical parameters by a team of music experts (dance teachers, music teachers, DJ's...) with a tool [1] that we created for this purpose. The two main problems are the quality and cost of annotating. \nThe cost of classifying wrongly is considerable: \nimagine playing a mood-killing song on a Saturday night party and the effect on the dance floor... To counter this, the metadata is annotated in multiple iterations. Furthermore, the indexers look at each other's work in order to refine the metadata. Regarding quality as fitness for purpose, our purpose is making the acquired experience of music experts in creating moods while rendering music available to pub owners. For that reason we cannot discard the music experts. \nWhen a new song enters the system, it has only trivial \nmetadata. Therefore, the playlist generator will not \nselect it. It can take days if not weeks to actually reach a full annotation. Not only is this process slow, it is also expensive, as it involved manual labour by the indexers. \nTo assist in this task, different complementary \napproaches are possible. The metadata can be computationally generated through various artificial intelligence techniques utilising our annotated database as ground truth. The vast knowledge pool of the Internet can be mined and automated classifiers used on the mined results [2].  \nStudies [14] show that many have an interest in \nmusic. Their knowledge can be used to assist our music \nexperts. Another approach is collaborative filtering, as used by for instance Last.fm (http://www.last.fm). Using human algorithms [3] could be another possibility. This is ideal for metadata that is typically hard to compute.  \nWe want to bundle the power of computational \ntechniques, the knowledge of the masses and the experience of our music experts. The data from the computational and collaborative approach can be combined and used to suggest values for atmospheres and musical parameters during manual annotation to the \nexperts, while they are annotating music. The music \nexperts can then decide based on their knowledge and context-awareness. \n2. TOOLS OF THE TRADE \nWe will look at two different paths we have taken: first \nwe will look at an approach to assist with the annotation of the topic of a song, then we will look at the extraction of key, mode and chords, and last we derive association rules out of the annotated music. \n© 2007 Austrian Computer Society (OCG).   \n \n 2.1. With lyrics towards topic markers \nThe musical experts planned the introduction of a new \ndata field in the metadata scheme, a field to mark the subject of a song. This should allow people to select songs based on their spoken content, for example “love songs” or “songs about a dog”. To introduce this new data field manually for 30,000 songs would take much time and thus be very costly. \nPlenty of tag-based systems are available, like \nLast.fm, AllMusic (http://www.allmusic.com), Plurn \n(http://www.plurn.com). For example, here is a sample of the tags of “Paint it Black” of the Rolling Stones on Last.fm: “60s”, “favourites”, “guitar”, “psychedelic rock”, “rock”, “seen live”. The problem is that tags mostly indicate genres, sometimes moods and favourites. Even if there would be subjects in the tag cloud of a song, it would be hard to select the valid subject candidates from genres, favourites and moods. \nIt is clear that having the lyrics can assist with this \ntask. This transforms the retrieval of topic from an acoustic problem to a text-retrieval problem. Adding \nlyrics to our database isn’t new and can assist in solving \nanother problem. When people search for a song, they often know parts of recurring words in the song and not the title. It is not uncommon that the title doesn’t occur in the lyrics at all. If this is the case the user needs to know the title song to find it, e.g. Queen’s ‘Bohemian Rhapsody’. Enriching our search engine with lyrics would solve this. Whether to search on the complete text or on the most important parts of the lyrics, for example the intro and chorus, to limit the search results, should be evaluated. But we think that there definitely is an \nadvantage to be able to search on ‘Galileo’ or \n‘Scaramouche’ and find ‘Bohemian Rhapsody’. \nThe Internet contains many sites dedicated to song \ntexts, mostly entered by fans. In order to harvest the lyrics, there are in general two ways: through an API or by web crawling and screen scraping. We decided to use a source that supports the first approach.  \nLyricWiki (http://www.lyricwiki.org) is a free source to search for or add lyrics and it has a web services API \navailable. \n2.1.1.  Lyrics Keyword Extraction \nThe importance of a keyword in a given lyric makes a \nreasonable metric for the quality of a generated topic. It is very hard to extract information about the context the song was written for. Our approach to suggesting a topic is using keyword extraction on the lyrics. \nSimple keyword extraction is based on term \nfrequency while complex approaches rely on statistical \ntechniques [7] or natural language processing [8], sometimes supported by domain specific ontologies [9]. There are plenty of keyword extraction techniques in IR literature, most are either proprietary or experimental; hence there are not much freely available products that can be used. The limited options are search engine optimisation (SEO) keyword analyser tools, Kea [10], or the Yahoo API term extractor [11]. \nKea requires extensive training in a specific domain \nto generate reasonable results. SEO tools mostly look for popular search terms in a webpage while extracting \nkeywords and the techniques used are very basic, e.g. \nword frequency/count. Yahoo uses a context-based technique to extract keywords [12]; this means that they can generate results based on the context of a document, so no training is needed. Yahoo can generate multi-word keywords by recognizing, with NLP, parts of sentences. The Yahoo term extractor seems most suited for lyrics. \n2.1.2.  Evaluation \nWe evaluated whether the generated keywords are \ngood topic markers for a song. This is done by measuring the difference in perception between the \nmusic experts, who will be assisted by the keywords \nduring their work, and ordinary people. As a significant sample set, 5 out of 7 music experts and 10 laymen performed a survey on 10 songs out of a pool of 40. The laymen are people working on non-music related research at the department of Computer Science at the \nFigure 1. Bar chart with average percentage of relevant keywords of laymen and experts   \n \n K.U. Leuven. We made two musical contexts, one with \nvery popular songs and one with unpopular songs, harvested the lyrics and generated 474 keywords for the 40 songs, selected evenly amongst the two contexts. For each s ong we made a survey, containing the artist and \ntitle of the song, the lyrics, the keywords and some questions. We asked whether the evaluator knew the song, whether a given keyword was relevant for the song and their name. We distributed the surveys \nrandomly over laymen, assuring that every song was at \nleast evaluated twice and for the experts at least once. This gives 150 samples of keyword validation data, this is sufficient for statistical relevance.  \nFigure 1 shows a chart with the (ordered) average \npercentage of relevant keywords for each s ong for the \nexperts and the laymen. We observe that there were relevant topic markers for all of the songs. Further more, \nexperts value the keywords higher \u0001 on average. \nThis can be formalised by studying the mean values \nof the experts (40.3%) and the laymen (26.9%).  We performed a two-sample t-test with equal variances not assumed (see Table 1), since each song was considered multiple times. This gives us a t-value of 3.8 with significance 1.14e-4. The large difference of 13.4% in mean between the laymen and the experts is supported \nby a 95% significance interval of [0.06,0.21]. Lyrics \nhave a special textual structure, separated in verses and chorus. Choruses are likely to contain good pointers to the topic of the song. It is possible that the retrieved lyrics contain a chorus that gets repeated, or that marginally changes over time. Due to the high similarity of these words, they could be considered as stop words by the algorithm. There has been some research on finding complete lyrics [13], but a lyrics aware keyword extractor would be needed too.  \nMusical exclamations, e.g. \u0002Yeah, baby \u0003, \u0002Oh, \nno, no, no \u0003, \u0002Na, na, na, na \u0003, also showed up \nbetween the keywords. For this reason it is not \nsurprisingly that about 2/3 of the keywords are irrelevant.  Some participants asked us why we did not treat them as stop-words. We are considering doing so in our future work. \nTable 1:  statistical results of keyword evaluation. 2.2.\n Data mining and association rules \nThe systematic approach of the experts posed the \nquestion whether association rules could be retrieved from the existing metadata. To test this hypothesis, we used the a priori association rule algorithm from the WEKA [5] toolkit on the data. We made a selection of subjective parameters: genre, subgenre, mood, dancing style and party factor. These parameters are the most time consuming for the music annotators and we know \nfrom experience that these parameters are key in \ngenerating musical contexts. \n2.2.1.  Observations \nNot much to our surprise, we could identify the genre \ngiven the main subgenre with remarkable confidence level: 100%. More interesting however is the lack of association rules containing mood, dancing style or rhythm style. Table 2 shows the derived rules with confidence level above 75%. We used the a priori algorithm for its memory efficiency due to the large dataset (30000 songs). Remarkable, but not surprising is the association of a low danceability with a low party \nfactor (86%). \nIf we limit to pop songs according to the first \ngeneration metadata scheme (3.2.2), other parameters appear on the right hand side of the rules. Subgenre, dancing style and rhythm style with respective values Pure Pop, Pop Rhythm 4/4 and Slow interact with one another and with danceability and party factor. These rules make actually sense: Music that is not danceable will most likely not drive the dance floor wild and hence will not make a good party song. \nTable 2.  Rules with a priori, confidence level > 75%. \nThe grey boxes show the number of songs, the black ones the confidence level of the rules. \nClearly, subjective parameters alone do not have \nsufficient predicting power for parameter automation. In an attempt to counter this problem, we introduced 2 additional parameters: the key of the song and what we call the mode of a song. These parameters are generated from the chords generated by the ChordsExtractor [15] \nembedded in the CLAM Annotator framework \n(http://clam.iua.upf.edu/). Histograms were used to determine the main key. The mode of a song is determined by counting the occurrences of minor and major chords. The value for the mode is either minor or major. It was expected that those parameters are relevant for the mood related parameters on the idea of chord quality [6]. Unfortunately, they are by far not expressive enough to predict any subjective parameter \n  \n \n by means of a classifier for instance. To accomplish this \nmore predictive parameters are needed. \n2.2.2.  Metadata scheme changes \nThe rockanango metadata annotation methodology [1] \nallows for changes in the metadata scheme. The musical experts claim to have found new insights in the classification of mood and other musical parameters like rhythm style. Additionally, they suggest new parameters that can be used for association rules. \nInstead of using flat labels for music annotation, the \nexperts have developed a tendency towards categories with a limited set of related values. This allows for a more granular approach. The values for the mood parameter are now grouped in four categories. The values in category are ordered, allowing for a more convenient comparison. \nThe reason behind the granularity is the gap in \nmusical knowledge between experts and (most) end users. The typical bartender cannot distinguish a cumbia from a rumba; he just wants ‘this Latin flavour’. A Latin dance teacher, however, needs the difference. \nAdditionally, it is expected that classifiers will perform \nbetter with bigger grained metadata because of reduced value spaces for algorithms. \n3. CONCLUSION AND FUTURE WORK \nIn conclusion of the two presented case studies we can \nstate that metadata generation by means of MIR techniques or collaborative filtering is a valid option in the process of contextualising music. Furthermore, data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata. The experience of the \nmusic experts is the binding factor that ensures the \nquality of the metadata. Their context- and purpose-awareness should be reflected in the metadata and only they can validate this. \nCurrent work includes a conceptual design stage of \nhuman algorithms for mood classification in the form of a collaborative, competitive game [4]. As a starting point, we are thinking of experiments to map a musical mood to the mood of something else, e.g. a picture. We are also looking for closer integration of metadata generation, data mining techniques and music experts. \n4. ACKNOWLEDGEMENT \nWe gratefully acknowledge the financial support of the \nIWT Vlaanderen (http:///www.iwt.be), through the project: Music Metadata Generation IWT-060237. \n5. REFERENCES \n[1] S. Govaerts, N. Corthaut, E. Duval, “Moody Tunes: \nThe rockanango project.”, Proc. of the 7th Int. \nConf. on Music Information Retrieval , 2006. \n[2] Schedl M., Pohle T., Knees P., Widmer G., \n“Assigning and visualising Music Genres by Web-based Co-Occurrence Analysis”, Proc. of the 7th \nInt. Conf. on Music Information Retrieval , 2006,. \n[3] L. von Ahn, L. Dabbish, “Labelling images with a \nComputer Game”, CHI 2004 , 24-29 April 2004. \n[4] Raph Koster, A Theory of Fun for Game Design , \nParaglyph, 1st. edition, Nov. 2004. \n[5] Ian H. Witten and Eibe Frank, Data Mining: \nPractical machine learning tools and techniques , \n2nd Edition, 2005. \n[6] Mark Levine, The Jazz Piano Book , Sher Music, \nJune 1, 2005 \n[7] Matsuo, Y. and M. Ishizuka, “Keyword Extraction \nfrom a Single Document using Word Co-occurrence Statistical Information.”, Int. Journ. on \nArtificial Intelligence Tools , 13(1): 157-169, 2004. \n[8] Sado, W. N., et al., “A Linguistic and Statistical \nApproach for Extracting Knowledge from Documents”, Proc. of the 15th Int. Workshop on \nDatabase and Expert Systems Applications (DEXA’04) , 2004. \n[9] Hulth, A., et al., “Automatic Keyword Extraction \nUsing Domain Knowledge”, Proc. of the 2nd Int. \nConf. on Computational Linguistics and Intelligent Text Processing , 2001. \n[10] Witten I.H., Paynter G.W., Frank E., Gutwin C. \nand Nevill-Manning C.G., \"KEA: Practical automatic keyphrase extraction.\" Proc. DL '99 , pp. \n254-256. \n[11]\n Yahoo Web Services API Term Extraction, \nhttp://developer.yahoo.com/search/content/V1/termExtraction.html \n[12] Kraft, R. et al., “Y!Q: Contextual Search at the \nPoint of Inspiration”, ACM Conf. on Information \nand Knowledge Management (CIKM’05) , 2005. \n[13] G. Geleijnse, J. Korst, “Efficient Lyrics Extraction \nfrom the Web”, Proc. of the 7th Int. Conf. on Music \nInformation Retrieval , 2006.  \n[14] Stevens, F., Van Den Broek, A., &  Vandeweyer, \nJ., “Time use of adolescents in Belgium and the Netherlands”, 25th IATUR Conf. on Time Use \nResearch  \n[15] Amatriain, X., de Boer, M., Robledo, E. & Garcia, \nD., “CLAM: an OO framework for developing \naudio and music applications”, Companion of the \n17th annual ACM SIGPLAN Conf. on Object-oriented Programming Systems, Languages, and Applications , 2002."
    },
    {
        "title": "Phoneme Recognition in Popular Music.",
        "author": [
            "Matthias Gruhne",
            "Christian Dittmar",
            "Konstantin Schmidt"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417111",
        "url": "https://doi.org/10.5281/zenodo.1417111",
        "ee": "https://zenodo.org/records/1417111/files/GruhneDS07.pdf",
        "abstract": "Automatic lyrics synchronization for karaoke applications is a major challenge in the field of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the vocal part of polyphonic music. This paper describes a system, which detects the phonemes based on a state-of-the-art audio information retrieval system with harmonics extraction and synthesizing as pre-processing method. The extraction algorithm is based on common speech recognition low-level features, such as MFCC and LPC. In order to distinguish phonemes, three different classification techniques (SVM, GMM and MLP) have been used and their results are depicted in the paper. 1 INTRODUCTION During the last years, users of personal computers have aquired huge amounts of digital music due to efficient audio compression techniques. One of the most fascinating leisure activities, especially in asian countries is the karaoke application. It is however, difficult and timeconsuming to create karaoke files at the moment. Since a manual tagging of the song is required, in order to convey the information, when a certain word is played to display and mark it on the screen. It might be commercially interesting to create karaoke applications for the home user, that is able to create karaoke files automatically from the personal music collection and corresponding texts from the internet. A basis of such system is the automatic synchronisation between music and corresponding lyrics. There have been scientific papers in the past, which described such methods [5] [1], but there is still room to improve the results significantly. The authors think, that phoneme recognition is a basis for synchronizing lyrics and corresponting text, since the most prominent phonemes give a good indication of the time label within a currently sung word. Thus, the system presented in this paper describes a novel approach for automatically detecting pho-nemes in music. Section 2 describes the setup of the overall system, illustrates the state of the art and shows c⃝2007 Austrian Computer Society (OCG). own extensions. Subsequently, the test setup is described and results are depicted. The paper finishes with conclusions and an explanation of the future work. 2 PROPOSED SYSTEM The proposed system uses techniques of a common state of the art information retrieval system, but makes additionally use by a harmonics extraction algorithm at the beginning. The overall design has been inspired by the method used by Fujihara [3], who described a singer identification method based on a music information retrieval system with a previous harmonics extraction. Since singer identification addresses a similar task, the results of detecting phonemes from polyphonic music have been expected to increase as well. The proposed method starts with a fundamental frequency estimation as described in [2] in order to improve the harmonic extraction results. Dressler uses a Multi-Resolution Fast Fourier Transform (MRFFT) to compute the spectra in different time-frequency resolutions efficiently. In order to discriminate frequencies, an instantaneous frequency (IF) is estimated from successive phase spectra. Due to the fact, that sinusoidal components of the audio signal contain the most relevant melody information, the harmonics are identified using a psychoacoustic model under distinction of spectral features. After estimating the fundamental frequency, the partials are retrieved from a spectrogram. The final sinusoidal resynthesizing of the audio signal is determined by transforming the spectrum into the time domain by using an Inverse Discrete Fourier Transform (IDFT). Only the previously calculated harmonic components of the spectrum are considered for a resynthesis. After constructing the signal, common speech recognition features had been extracted and assembled. The applied features are Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Coefficients (LPC), Perceptual Linear Prediction (PLP) and Warped Linear Prediction Coefficients (WarpedLPC) [4]. Before the actual classification the dimensions are reduced by using a linear discriminant analysis. The resulting feature matrix is classified with common classifier techniques, namely Gaussian Mixture Models (GMM), Support Vector Machines (SVM) and Multilayer Perceptron (MLP). Classifier Pr. Rc. CCI Results with harmonics analysis MLP",
        "zenodo_id": 1417111,
        "dblp_key": "conf/ismir/GruhneDS07",
        "keywords": [
            "Automatic lyrics synchronization",
            "karaoke applications",
            "music information retrieval",
            "phoneme detection",
            "state-of-the-art audio",
            "harmonics extraction",
            "pre-processing method",
            "classification techniques",
            "phoneme recognition",
            "karaoke files"
        ],
        "content": "PHONEME RECOGNITION IN POPULAR MUSIC\nMatthias Gruhne, Konstantin Schmidt and Christian Dittmar\nFraunhofer IDMT\nLangewiesener Str. 22\n98693 Ilmenau\nfghe,schmkn,dmrg@idmt.fraunhofer.de\nABSTRACT\nAutomatic lyrics synchronization for karaoke applications\nis a major challenge in the ﬁeld of music information re-\ntrieval. An important pre-requisite in order to precisely\nsynchronize the music and corresponding text is the de-\ntection of single phonemes in the vocal part of polyphonic\nmusic. This paper describes a system, which detects the\nphonemes based on a state-of-the-art audio information\nretrieval system with harmonics extraction and synthesiz-\ning as pre-processing method. The extraction algorithm\nis based on common speech recognition low-level fea-\ntures, such as MFCC and LPC. In order to distinguish\nphonemes, three different classiﬁcation techniques (SVM,\nGMM and MLP) have been used and their results are de-\npicted in the paper.\n1 INTRODUCTION\nDuring the last years, users of personal computers have\naquired huge amounts of digital music due to efﬁcient\naudio compression techniques. One of the most fasci-\nnating leisure activities, especially in asian countries is\nthe karaoke application. It is however, difﬁcult and time-\nconsuming to create karaoke ﬁles at the moment. Since a\nmanual tagging of the song is required, in order to con-\nvey the information, when a certain word is played to\ndisplay and mark it on the screen. It might be commer-\ncially interesting to create karaoke applications for the\nhome user, that is able to create karaoke ﬁles automati-\ncally from the personal music collection and correspond-\ning texts from the internet. A basis of such system is\nthe automatic synchronisation between music and corre-\nsponding lyrics. There have been scientiﬁc papers in the\npast, which described such methods [5] [1], but there is\nstill room to improve the results signiﬁcantly. The authors\nthink, that phoneme recognition is a basis for synchroniz-\ning lyrics and corresponting text, since the most prominent\nphonemes give a good indication of the time label within\na currently sung word. Thus, the system presented in this\npaper describes a novel approach for automatically detect-\ning pho-nemes in music. Section 2 describes the setup of\nthe overall system, illustrates the state of the art and shows\nc\r2007 Austrian Computer Society (OCG).own extensions. Subsequently, the test setup is described\nand results are depicted. The paper ﬁnishes with conclu-\nsions and an explanation of the future work.\n2 PROPOSED SYSTEM\nThe proposed system uses techniques of a common state\nof the art information retrieval system, but makes ad-\nditionally use by a harmonics extraction algorithm at\nthe beginning. The overall design has been inspired\nby the method used by Fujihara [3], who described a\nsinger identiﬁcation method based on a music informa-\ntion retrieval system with a previous harmonics extrac-\ntion. Since singer identiﬁcation addresses a similar task,\nthe results of detecting phonemes from polyphonic mu-\nsic have been expected to increase as well. The proposed\nmethod starts with a fundamental frequency estimation as\ndescribed in [2] in order to improve the harmonic extrac-\ntion results. Dressler uses a Multi-Resolution Fast Fourier\nTransform (MRFFT) to compute the spectra in different\ntime-frequency resolutions efﬁciently. In order to discrim-\ninate frequencies, an instantaneous frequency (IF) is es-\ntimated from successive phase spectra. Due to the fact,\nthat sinusoidal components of the audio signal contain\nthe most relevant melody information, the harmonics are\nidentiﬁed using a psychoacoustic model under distinction\nof spectral features. After estimating the fundamental fre-\nquency, the partials are retrieved from a spectrogram. The\nﬁnal sinusoidal resynthesizing of the audio signal is deter-\nmined by transforming the spectrum into the time domain\nby using an Inverse Discrete Fourier Transform (IDFT).\nOnly the previously calculated harmonic components of\nthe spectrum are considered for a resynthesis. After con-\nstructing the signal, common speech recognition features\nhad been extracted and assembled. The applied features\nare Mel Frequency Cepstral Coefﬁcients (MFCC), Lin-\near Prediction Coefﬁcients (LPC), Perceptual Linear Pre-\ndiction (PLP) and Warped Linear Prediction Coefﬁcients\n(WarpedLPC) [4]. Before the actual classiﬁcation the\ndimensions are reduced by using a linear discriminant\nanalysis. The resulting feature matrix is classiﬁed with\ncommon classiﬁer techniques, namely Gaussian Mixture\nModels (GMM), Support Vector Machines (SVM) and\nMultilayer Perceptron (MLP).Classiﬁer Pr. Rc. CCI\nResults with harmonics analysis\nMLP 0.335 0.338 54.42 %\nSVM 0.333 0.340 57.68 %\nGMM 0.309 0.300 49.13 %\nResults without harmonics analysis\nMLP 0.186 0.187 34.16 %\nSVM 0.167 0.184 28.34 %\nGMM 0.178 0.191 31.45 %\nTable 1 . Accuracy of GMM, SVM and MLP with and\nwithout harmonics analysis.\n3 EVALUATION\nIn order to estimate the phonemes in the vocal parts of\nthe popular music, an extensive database has been estab-\nlished. Overall, 2244 phonemes have been manually la-\nbeled from vocal parts of popular music. Since the genre\nof the most songs in karaoke applications concentrate on\npopular music, the testset in the proposed system uses\nonly audio items in the genre Pop from the last ﬁfty years.\nDue to the fact, that the vocal part of music contains a\nlarge amount of residual ”distortion” besides the plain\nvoice, this paper concentrates only on extracting the 15\nmost discriminative voiced phonemes. These phonemes\nhave been labeled from 37 popular music songs, 21 songs\nperformed by male singers and 16 songs performed by fe-\nmale singers. The items have been split into training (51\npercent) and test set (49 percent). The occurrence ratio of\nthe phonemes between test und training set is equal.\nThe considered phonemes can be divided into three dif-\nferent classes (approximants, nasals and vowels). Some\nof the usually distinguished vowels have been combined,\nbecause they refer to the same character and are some-\ntimes so similar, that they even confound by non-native\nspeakers. Concerning the parameters of the feature and\nharmonic extraction algorithm, besides the in Dressler[2]\ndescribed fundamental frequency analysis and the in\nFujihara[3] described synthesis, eight LPC features have\nbeen used, because they turned out to deliver most reli-\nable results. Furthermore eight WLPC coefﬁcients and\nnine PLP coefﬁcients have been used. The frequency of\nMFCC features ranged between 50 Hz and 5 kHz, 13 co-\nefﬁcients were utilized.\n4 TEST RESULTS\nThe test results, that have been received during the tests\nand which are described in this section more in detail. Ta-\nble 1 shows precision (Pr), recall(Rc) and percentage of\ncorrect classiﬁed instances (CCI) of the tested classiﬁers.\nTable1 is divided in two main blocks. The upper block\nshows the performance of the system by performing a pre-\nvious harmonics analysis and the lower block depicts the\nresults without harmonics analysis, showing the results of\nthe GMM, SVM and MLP classiﬁers.The best result could be obtained by performing a har-\nmonics analysis and using an SVM classiﬁer. With this\nconﬁguration, the proposed system reached an average\nprecision of 0.33 and an average recall of 0.34 (58% CCI).\nBy not executing a previous harmonics analysis, the MLP\nclassiﬁer performed better than SVM and GMM (34%\nCCI). The difference between the results with resynthe-\nsis and without are signiﬁcant, especially by considering\nthe fact, that the fundamental frequency analysis reaches\nat the moment only an accuracy of about 70%.\n5 CONCLUSIONS AND FUTURE WORK\nThis paper described a novel approach for detecting pho-\nnemes in vocal parts of polyphonic music. The described\nmethod incorporates state of the art techniques in feature\nextraction and classiﬁcation used in music and speech re-\ncognition and performs melody detection algorithms for\nreducing inﬂuences from accompanying sounds. The re-\nsults show, that the best classiﬁer with an overall perfor-\nmance (with harmony extraction) of 58% (CCI). The re-\nsults with resynthesis by fundamental frequency and with-\nout are not signiﬁcant, but due to the accuracy of 70 per-\ncent of the fundamental frequency estimation, the results\ncould improve enormously with an improvement of pre-\nprocessing algorithms. In order to improve the perfor-\nmance of the system, it is planned to extend the test set\nby manually labelling more songs as well as using addi-\ntional features and to test different classiﬁer.\n6 REFERENCES\n[1] K. Chen, S. Gao, Y . Zhu, and Q. Sun. Popular song\nand lyrics synchronisation and it’s application to mu-\nsic. In Proceedings of the 13th Annual Conference\non Multimedia Computing and Networking (MMCN) ,\n2006.\n[2] K. Dressler. Sinusoidal extraction using an efﬁcient\nimplementation of a multi-resolution fft. In Proceed-\nings of the International Conference on Digital Audio\nEffects (DAFx) , 2006.\n[3] H. Fujihara, T. Kitahara, M. Goto, K. Komatani,\nT. Ogata, and H. G. Okuno. Singer identiﬁcation\nbased on accompaniment sound reduction and reli-\nable frame selection. In Proceedings of the 6th Inter-\nnational Symposium on Music Information Retrieval\n(ISMIR) , pages 329–336, 2005.\n[4] A. Harma and U. Laine. A comparison of warped\nand conventional linear predictive coding. In IEEE\nTransaction on Acoustics, Speech and Signal Process-\ning Vol. 9 No. 5 , pages 579 – 588, 2001.\n[5] Y . Wang, M. Y . Kan, T. L. Nwe, A. Shenoy, and\nY . Yin. Lyrically: Automatic synchronization of\nacoustic musical signals and textual lyrics. In Pro-\nceedings of the 12th annual ACM international con-\nference on Multimedia , 2004."
    },
    {
        "title": "ATTA: Implementing GTTM on a Computer.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418269",
        "url": "https://doi.org/10.5281/zenodo.1418269",
        "ee": "https://zenodo.org/records/1418269/files/HamanakaHT07.pdf",
        "abstract": "We have been discussing the design principle for the implementation of GTTM and presented the semiautomatic generation techniques of grouping structure, metrical structure, and time-span tree, and the searching method for the optimal parameter value assignments. In ISMIR2007, we organize a tutorial session on the techniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference.  Since the time of the tutorial session is not enough, we demonstrate a working automatic timespan tree analyzer ATTA in a demo session.  ATTA is an integration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail.",
        "zenodo_id": 1418269,
        "dblp_key": "conf/ismir/HamanakaHT07",
        "keywords": [
            "GTTM",
            "design principle",
            "grouping structure",
            "metrical structure",
            "time-span tree",
            "searching method",
            "tutorial session",
            "automatic timespan tree analyzer",
            "demo session",
            "understanding GTTM"
        ],
        "content": "ATTA: IMPLEMENTING GTTM ON A COMPUTER\nMasatoshi Hamanaka  Keiji Hirata Satoshi Tojo \nUniversity of Tsukuba \nhamanaka@iit.tsukuba.ac.jp  NTT Communication Sience \nLaboratories  Japan Advanced Institute of \nScience and Technology \nABSTRACT \nWe have been discussing the design principle for the \nimplementation of GTTM and presented the semi-automatic generation techniques of grouping structure, metrical structure, and time- span tree, and the searching \nmethod for the optimal parameter value assignments. In \nISMIR2007, we organize a tutorial session on the \ntechniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference.  Since the time of the tutorial session is \nnot enough, we demonstrate a working automatic time-span tree analyzer ATTA in a demo session.  ATTA is an \nintegration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail.   \n1. INTRODUCTION \nMusic theory, in particular for a piece written as a score, \ngives us a methodology for analyzing and understanding a piece, and it explains d eep structure, musical \nknowledge, experience, and skills in a comprehensive way.  We believe that implementing such a music theory on a computer would bring great benefits to the MIR research.   Since analysis  results by a music theory \ncan be considered as the expressions of the musical semantics of a piece, there is  a possibility that musical \nsemantic computing at a symbol level can be made more \nprecise and consistent.  Therefore, the advances in musical semantic computing will be beneficial to those who are working on various kinds of music applications. Formalizing and mechanizing music theories is one of the primary concerns in computational musicology, and the researchers in the fiel d are recently paying more \nattention to the elaborating music theories through experiences of implementing a working system based on the theories.  Furthermore,  since the analyzer based \non our techniques may be helpful for educational use in music courses for understanding and composing music.   We believe that the generative theory of tonal music (GTTM) [1] is the most promising theory for mechanizing and developing a support system, thanks to three of its features. The first feature is that it uses rules \nto describe musical insight and knowledge obtained by investigating the musical structures and relations occurring in a piece of music. The second feature is that GTTM is designed to consistently represent multiple aspects of music in a single framework. The third feature is that GTTM has been developed based on the concept of reduction. We argue that the proper way of taking into account deep structures is to adopt the concept of reduction [2, 3]. 2. EXGTTM \nTo get music theory GTTM to operate on a computer, \nhowever, we must overcome some widely recognized fundamental difficulties. One is giving an ambiguous concept a firm definition, and the other is supplementing the lack of necessary concepts (e xternalization). Here, note that \nwe distinguish two kinds of ambiguity in music analysis: one involves the musical understanding of humans, and the other concerns the representation of a music theory.   In our approach, we extend GTTM by full externalization and parameterization and propose a machine-excutable extension of GTTM, exGTTM. This externalization in mechanizing GTTM includes introducing an algorithm for generating a hierarchical structure of the time-span \ntree in the mixed manner of top-down and bottom-up. Such an algorithm has not b een presented for GTTM. The \nparameterization includes introducing a parameter controlling the priorities of rules to avoid a conflict among the rules as well as parameters for controlling the shape of the hierarchi cal time-span tree.  \n The significance of full externalization and parameterization is twofold: precise controllability and coverage of the correct results. Whenever we find a correct result that exGTTM cannot generate, we introduce new parameters for exGTTM and give them proper values so that it can generate the correct result. In this way, we repeatedly externalize and in troduce new parameters until \nwe can obtain all of the results that humans consider \ncorrect. In total, we have introduced 15 parameters for grouping-structure analysis, 18 for metrical-structure analysis, and 13 for time-span reduction. We think that full externalization and parameterization is an important intermediate step toward a fu lly automatic GTTM analyzer. \nBased on the above consideration, we have presented the semi-automatic generation techniques of grouping structure [4], metrical stru cture [5], and time-span tree \n[6], and the searching method for the optimal parameter value assignments [7]. \n3. ATTA \nWe developed a working automatic time-span tree \nanalyzer, called ATTA, which realizes exGTTM.  \nFigure 1 is the overview of the ATTA, which consists of a grouping structure analyzer, a metrical structure analyzer, and a time-span tree analyzer. The features of \nATTA contain: an XML-based data structure, programming in Perl, and a Java-based GUI (Figure 2).  \n4. ANALISYS RESULTS BY ATTA  \nFigure 3 shows the analysis results of two pieces, which \n© 2007 Austrian Computer Society (OCG).   \n \n  \n  \n \n     \n \n      are generated with the same parameter values, \nBeethoven’s Turkish March, and English Traditional, Green Sleeves. The numbers shown around the \nbranches of the trees in the figure indicate the rule \nnumbers that hold. By co mparing the two pieces, we \ninterestingly find the similarity between the two, where \nthe same rules hold. At present, the parameters are configured by hand, because the optimal values of the \nparameters depend on a piece of music. \n5. CONCLUSION  \nWe briefly present a machin e-executable extention of \nGTTM, exGTTM, and a worki ng automatic time-span tree \nanalyzer, ATTA, which reali zes exGTTM.  We would like \nto contribute to the MIR research through mechanizing a music theory.  In the demo session of ISMIR2007, we look forward to demonstrating ATTA and discussing the techniques for implementing GTTM.   \n        \n     \n  \n \n     \nREFERENCES \n[1] Lerdahl, F. and Jackendoff, R. A Generative \nTheory of Tonal Music , The MIT Press, \nCambridge, 1983. \n[2] Selfridge-Field, E. ''Conceptual and \nRepresentational Issues in Melodic Comparison'', Computing in Musicology 11 , The MIT Press, \npp. 3-64, 1998. \n[3] Hirata, K. and Aoyagi, T. ''Computational Music Representation on the Generative Theory of Tonal Music and the Deductive Object-Oriented Database'', Computer Music \nJournal , Vol. 27, No. 3, pp. 73-89, 2003.\n  \n[4] Hamanaka, M., Hirata, K.  and Tojo, S. ''Automatic \nGeneration of Grouping Structure Based on The \nGTTM '', In Proc. of ICMC 2004 , pp.141-144, 2004.  \n[5] Hamanaka, M., Hirata, K.  and Tojo, S. ''Automatic \nGeneration of Metrical Structure Based On GTTM '', \nIn Proc. of ICMC 2005 , pp.53-56, 2005.  \n[6] Hamanaka, M., Hirata, K.  and Tojo, S. ''ATTA: \nAutomatic Time-Span Analyzer Based On Extended GTTM\n'', In Proc. of ISMIR 2005 , pp.358-365, 2005.  \n[7] Oka, Y., Hamanaka, M., Hirata, K. and Tojo, S. \n''Optimal Parameter Set Acquisition for exGTTM '', \nIn Proc. of Music-AI 2007 , 2007.  \n \n  \n \n \n \n   \n \n   =\n==\n=\n=\nMetrical structure \nGrou ping structure \nTime-span tree \n(a) Beethoven, Turkish March\n(b) English Traditional, Grenn Sleeves1\n11\n3b1\n1\n1\n11, 3b\n1111, 3b\n1, 3b1, 3b1, 3b\n1, 3b\n11\n1, 3b\n1\n11, 3b\n1, 3b13b\n3b\n1 111\n1, 3b\n3b 1, 3b\n1\n11\n11\n11\n1, 3b\n1, 3b11\n11\n1Figure 1 . Processing flow of ATTA. \nFigure 2 . ATTA: automatic time- span tree analyzer.  \nFigure 3 . Analysis results of two pieces having the same parameter sets."
    },
    {
        "title": "Desoloing Monaural Audio Using Mixture Models.",
        "author": [
            "Yushen Han",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417507",
        "url": "https://doi.org/10.5281/zenodo.1417507",
        "ee": "https://zenodo.org/records/1417507/files/HanR07.pdf",
        "abstract": "We describe a new approach to the “desoloing” problem, in which one tries to isolate the accompanying instruments from a monaural recording of a soloist with accompaniment. Our approach is based on explicit knowledge of the audio in the form of a score match – a correspondence between a symbolic score and the music audio, giving the times of all musical events. We employ the familiar idea of masking the short time Fourier transform to eliminate the solo part. The ideal mask is estimated by fitting a model to the data, whose note-based components are derived from the score match. The parameters for our probabilistic model are estimated using the EM algorithm. 1 INTRODUCTION We focus here on the problem of isolating an accompanying instruments from a monaural recording of music for soloist and accompaniment. We call this problem “desoloing.” The primary application for desoloing, at least in terms of numbers, would likely be karaoke. Desoloing would produce an accompaniment for any song of interest, thus increasing the range of music on which both singers and listeners could enjoy karaoke. Our interest in this problem, however, stems from our work with musical accompaniment systems. The idea here seems, at first, painfully close to karaoke, except that the accompanying instruments must follow the soloist, rather than the other way around. This change adds a great deal of complexity to the problem, while also making it attractive to “classical” musicians. Our preferred method of orchestral resynthesis is from actual audio. While commercial orchestral accompaniments are available for some of the solo literature, they tend to be poorly recorded with variable playing. A successful desoloing algorithm would harvest a wide world of beautifully played and expertly recorded orchestras for the accompaniment system. Desoloing serves an MIR need by allowing one to access the “sources” of an audio file independently. The desoloing problem takes an asymmetric view of the familiar source separation idea, which has received much attention in the signal processing community over c⃝2007 Austrian Computer Society (OCG). Figure 1. Spectrogram of opening of Samuel Barber Violin Concerto. Vertical lines mark the solo note onset. the last decade. Much of this work is called “blind” source separation, meaning that one tries to separate the sources with little or no knowledge of their contents [1] [2], [3], [4]. The general area of blind source separation includes several efforts that are explicitly devoted to music audio [5] and [6]. Our framing of the problem is distinct from most work in source separation due to the explicit knowledge we have of the audio — we assume that we are given a symbolic score to the piece of music, giving the pitches and instruments of all notes in the music, as well as a score match, giving a precise correspondence between these notes and the audio file. The present work is enabled by our previous work in orchestral score following with very minor adjustment done manually in case of mismatch. Figure 1 demonstrates this correspondence between score and audio that forms the basis of our approach. A similar problem statement was defined in [7]. While this problem is, no doubt, highly challenging, our particular needs make the goal somewhat more attainable. Any desoloing procedure will almost certainly result in the loss or disfigurement of certain aspects of the orchestral audio we wish to isolate. However, in our accompaniment application, the live soloist will be playing at the precise time-frequency regions where our desoloing procedure does the most damage. Thus, much of the harm done by desoloing will be masked by the live soloist. This brings our desoloing into the realm of tractable problems. 2 MASKING IN A TIME-FREQUENCY DOMAIN Our approach operates on the short time Fourier transform (STFT) X of the audio signal x in the time domain [8]. We use binary masking to decompose X into X = ˆ Xs + ˆ Xa. ˆ Xs and ˆ Xs are our estimates for the solo and the accompaniment. We denote this by ˆ Xs ≈ 1SX ˆ Xa ≈ 1AX where 1C(t, k) = ½ 1 if (t, k) ∈C 0 otherwise We define ˆA to be the complement of ˆS. Convincing audio signals ˆxs and ˆxa in the time domain are reconstructed by STFT−1 ˆ Xs and STFT−1 ˆ Xs. We include Hann window in STFT with L = N/H = 4 hops per FFT length, as it fulfills the constant overlap-add (COLA) constraint for perfect recovery of x from X [13]. It is well-known that perceptually good results can be constructed using the ideal mask that can be computed when the sets, S and A are known, as when x is artificially constructed by adding together two known signals xs and xa. For instance, see [9]. Moreover, with known x, xs and xa, we can derive a percentage that describes how much binary masking we can estimate correctly. This will serve as a quantitative evaluation other than the audible result. Roweis [14] described the idea of binary masking from a filter bank point of view. 3 MODEL-BASED DECOMPOSITION",
        "zenodo_id": 1417507,
        "dblp_key": "conf/ismir/HanR07",
        "keywords": [
            "desoloing",
            "monaural recording",
            "score match",
            "short time Fourier transform",
            "EM algorithm",
            "probabilistic model",
            "orchestral resynthesis",
            "source separation",
            "MIR need",
            "tractable problems"
        ],
        "content": "DESOLOING MONAURAL AUDIO USING MIXTURE MODELS\nYushen Han\nSchool of Informaitcs\nIndiana Univ.\nyushan@indiana.eduChristopher Raphael\nSchool of Informatics\nIndiana Univ.\ncraphael@indiana.edu\nABSTRACT\nWe describe a new approach to the “desoloing” prob-\nlem, in which one tries to isolate the accompanying in-\nstruments from a monaural recording of a soloist with ac-\ncompaniment. Our approach is based on explicit knowl-\nedge of the audio in the form of a score match – a corre-\nspondence between a symbolic score and the music audio,\ngiving the times of all musical events. We employ the fa-\nmiliar idea of masking the short time Fourier transform\nto eliminate the solo part. The ideal mask is estimated\nby ﬁtting a model to the data, whose note-based compo-\nnents are derived from the score match. The parameters\nfor our probabilistic model are estimated using the EM al-\ngorithm.\n1 INTRODUCTION\nWe focus here on the problem of isolating an accompany-\ning instruments from a monaural recording of music for\nsoloist and accompaniment. We call this problem “des-\noloing.” The primary application for desoloing, at least\nin terms of numbers, would likely be karaoke. Desoloing\nwould produce an accompaniment for any song of interest,\nthus increasing the range of music on which both singers\nand listeners could enjoy karaoke.\nOur interest in this problem, however, stems from our\nwork with musical accompaniment systems. The idea here\nseems, at ﬁrst, painfully close to karaoke, except that the\naccompanying instruments must follow the soloist, rather\nthan the other way around. This change adds a great deal\nof complexity to the problem, while also making it attrac-\ntive to “classical” musicians. Our preferred method of or-\nchestral resynthesis is from actual audio. While commer-\ncial orchestral accompaniments are available for some of\nthe solo literature, they tend to be poorly recorded with\nvariable playing. A successful desoloing algorithm would\nharvest a wide world of beautifully played and expertly\nrecorded orchestras for the accompaniment system. Des-\noloing serves an MIR need by allowing one to access the\n“sources” of an audio ﬁle independently.\nThe desoloing problem takes an asymmetric view of\nthe familiar source separation idea, which has received\nmuch attention in the signal processing community over\nc°2007 Austrian Computer Society (OCG).\nFigure 1 . Spectrogram of opening of Samuel Barber Vio-\nlin Concerto. Vertical lines mark the solo note onset.\nthe last decade. Much of this work is called “blind” source\nseparation, meaning that one tries to separate the sources\nwith little or no knowledge of their contents [1] [2], [3],\n[4]. The general area of blind source separation includes\nseveral efforts that are explicitly devoted to music audio\n[5] and [6]. Our framing of the problem is distinct from\nmost work in source separation due to the explicit knowl-\nedge we have of the audio — we assume that we are given\na symbolic score to the piece of music, giving the pitches\nand instruments of all notes in the music, as well as a score\nmatch, giving a precise correspondence between these notes\nand the audio ﬁle. The present work is enabled by our pre-\nvious work in orchestral score following with very minor\nadjustment done manually in case of mismatch. Figure 1\ndemonstrates this correspondence between score and au-\ndio that forms the basis of our approach. A similar prob-\nlem statement was deﬁned in [7].\nWhile this problem is, no doubt, highly challenging,\nour particular needs make the goal somewhat more attain-\nable. Any desoloing procedure will almost certainly re-\nsult in the loss or disﬁgurement of certain aspects of the\norchestral audio we wish to isolate. However, in our ac-\ncompaniment application, the live soloist will be playing\nat the precise time-frequency regions where our desoloing\nprocedure does the most damage. Thus, much of the harm\ndone by desoloing will be masked by the live soloist. This\nbrings our desoloing into the realm of tractable problems.\n2 MASKING IN A TIME-FREQUENCY DOMAIN\nOur approach operates on the short time Fourier transform\n(STFT) Xof the audio signal xin the time domain [8].\nWe use binary masking to decompose XintoX=^Xs+^Xa.^Xsand ^Xsare our estimates for the solo and the\naccompaniment. We denote this by\n^Xs¼1SX\n^Xa¼1AX\nwhere\n1C(t; k) =½\n1 if(t; k)2C\n0 otherwise\nWe deﬁne ^Ato be the complement of ^S.\nConvincing audio signals ^xsand^xain the time do-\nmain are reconstructed by STFT¡1^XsandSTFT¡1^Xs.\nWe include Hann window in STFT with L=N=H = 4\nhops per FFT length, as it fulﬁlls the constant overlap-add\n(COLA) constraint for perfect recovery of xfromX[13].\nIt is well-known that perceptually good results can be\nconstructed using the ideal mask that can be computed\nwhen the sets, SandAareknown , as when xis artiﬁcially\nconstructed by adding together two known signals xsand\nxa. For instance, see [9].\nMoreover, with known x,xsandxa, we can derive a\npercentage that describes how much binary masking we\ncan estimate correctly. This will serve as a quantitative\nevaluation other than the audible result.\nRoweis [14] described the idea of binary masking from\na ﬁlter bank point of view.\n3 MODEL-BASED DECOMPOSITION\n3.1 Note-based models\nSimilar to that of [11], our approach to desoloing relies\non a note-based probabilistic model for the magnitude of\nthe STFT, jX(t; k)j. Through this model, we decompose\nthe magnitude into two components, one for the soloist\nand one for the orchestra, via parameter estimation for the\nmodel. We then move easily to the classiﬁcation of each\ntime-frequency point using our decomposition.\nSuppose we have a collection of models ,M, that de-\nscribes all of the known contributions to our data jX(t; k)j.\nEach model is described by a “template” function qm, sup-\nported on a subset of time-frequency space, Dm, with\nX\n(t;k)2Dmq(t; k) = 1\nThe note models will describe the contribution of a given\nnote over a range of frames t, in which the associated qm\nwould be supported on the frequency bins, k, near the har-\nmonics of the note over the relevant range of frames, t.\nOne could also create models for various other contribu-\ntions such as the attack and reverberation of a note, etc.\n3.2 Statistical assumptions for EM\nTo employ the expectation-maximization(EM) algorithm,\nwe assume that the magnitude contribution to the spec-\ntrogram for each model is given by a collection of inde-\npendent Poisson random variables fZm(t; k)gfor(t; k)2Dm, as the hidden variable in [12], with means ®mqm(t; k)\nfor some ®m¸0. Thus, ®mdescribes the extent to which\nthe contributing event is active, while the average contri-\nbution proﬁle, qm(t; k), is ﬁxed for the model. Further-\nmore, we assume that\njX(t; k)j=X\nm2MZm(t; k) (1)\nFor this model to make sense, the units of jX(t; k)jare\nscaled so that no signiﬁcant loss is incurred by regard-\ning the jX(t; k)jas integers, which is consistent with the\nPoisson assumptions. Strictly speaking, Eqn. 1 cannot be\ncompletely correct since, for complex numbers, the sum\nof the magnitudes is not equal to the magnitude of the\nsum. However, the assumption is approximately true in\nthe very common case in which one magnitude is much\ngreater than all others. Ellis [10] gives a discussion of this\nassumption.\n3.3 EM algorithm\nWith the assumptions above, we decompose our spectrum\njX(t; m)jby estimating the ®mandqmparameters using\nthe EM. This algorithm is based on estimating the col-\nlection of hidden variables, fZm(t; k)g, using the current\nparameter conﬁguration, and re-estimating the parameters\nusing these estimates. Suppose that ®r\nmandqr\nmare the\nestimates we have after the rth iteration of the algorithm.\nThe E-step of the EM algorithm computes\nCr\nm(t; k) = E[Zm(t; k)j jXj]\n=®r\nmqr\nm(t; k)jX(t; k)jP\n¹2M®r¹qr¹(t; k)\nCr\nm(t; k)is the estimated contribution to time-frequency\npoint (t; k)given by model m, using our current parame-\nters.\nThe M-step of the EM algorithm will vary depending\non the parameters we are estimating. For the ®mparame-\nters, the M-step would be\n®r+1\nm=X\n(t;k)2DmCr\nm(t; k)\nThis is not surprising, since ®mrepresents our estimate of\nthe total spectral magnitude contribution of model m.\nSome of our model templates, qm, are ﬁxed through the\nEM iterations. For those that are re-estimated, the M-step\nwill depend on the parametric form of the model.\n3.4 Parameters to be estimated\nWe have experimented with a variety of different models,\nbut, at present, get the best results with a relatively sim-\nple conﬁguration. For each note, min the score, we let\nI(m)be the range of frames that span the inter-onset in-\nterval beginning with the onset of note mand ending with\nthe onset of the following note. This information follows\ndirectly from our score match. For each note b, in bothsolo and orchestra, we create a model, m, for each frame\nt2Ibwith domain Dm=f(t;0): : : ;(t; N¡1)g\nqm(t; k) =X\nhphN(k;¹h; ¾2\nh)\nwhereP\nhph= 1and\nN(k;¹; ¾2) =P(k¡1=2< Y < k + 1=2)\nwhere Yis a normally distributed random variable with\nmean ¹and variance ¾2. For our note models we couple\nall of the mean values by ¹h=h¹1(m)where ¹1(m)\nis the not-necessarily-integral frequency bin for the fun-\ndamental frequency of note m. While the peak widths\nappear to be constant over harmonic number, we achieved\nbetter results by allowing the variances ¾2\nhto increase some-\nwhat with frequency. Finally, the phconstants were taken\nto be representative of the characteristic frequency proﬁle\nfor the particular instruments.\nWe have tried to estimate different combinations of these\nparameters, sharing the parameters in different ways across\nthe entire collection of models. At present, the best as-\nsumptions involve estimating only the ¹1parameter of a\nsolo note for each individual frame in the M-step\n¹r+1\n1(m) =P\n(t;k)2Dmk\nhm(k)Cr\nm(t; k)\nP\n(t;k)2DmCrm(t; k)\nwhere hm(k)is the harmonic number in f1;2; : : : ;gas-\nsociated with bin kand the pitch of model m. We do\nnot estimate any parameters for the orchestra note models\nother than the f®mg.\nThe other events we capture are the reverberation of\neach solo note. The domain of reverberation model mis\nDm=f(t; k) :t2tend: : : ; t end+Lreverb ; k20; : : : ; N ¡1g\nwhere tenddenotes the last frame of the solo note, with\nqm(t; k) =X\nhphN(k;¹h; ¾2\nh)e¸(t¡tend)\nWe only estimate the ®mparameter for these models.\n4 EXPERIMENTS\nBefore the work of this paper, the attack or transient phase\nof a note that distributes spectral energy widely is captured\nand removed by our ad hoc recognizer.\nAs described in the previous section, a note model, m,\ncan be viewed as a combination of hharmonic compo-\nnents, each of which has its mean and variance. The coef-\nﬁcient phis the normalized weight associated with the hth\nharmonic component of note m. It is not surprising that\nthe conﬁguration of phdepends on the instrumentation,\npitch, and dynamic level of the note. Failure to specify the\nphconﬁguration accordingly leads to a dubious descrip-\ntion of the magnitude contribution of the various harmon-\nics. In our experiments, we initialize the conﬁguration of\nFigure 2 . Harmonic weight distribution.\nthe solo model from an instrument spectrum library using\ntemplates trained from a subset of the University of Iowa\nmusical instrument samples. See 4.\nIn the case that the reverberation of a previous solo\nnote contributes, but does not mask the following note, we\napproximate the decay pattern of the reverberating solo\nnote with an exponential over time. Without knowing\nthe acoustic conditions of the recording, we have cho-\nsen the parameters experimentally through trial and error.\nThe essence of the “desoloing” problem dictates that we\nshould be generous in our labeling of solo points, since\ncontributions from the solo instrument are readily appar-\nent and undesirable in our results To effect this bias, we\nemploy the following 3 mechanisms:\nInitialization The EM algorithm will converge to a lo-\ncal maximum that splits the magnitude of the STFT,\nX(t; k), into solo and orchestra contributions. We\ninitialize the EM algorithm to expect signiﬁcantly\nlarger contributions from the solo note s (a ratio of\n3:1). Similarly, since the solo reverberation model\nis longer than the other models in terms of frames,\nwe expect it to consume more spectral energy and\nassign a larger initial contribution value accordingly.\nHarmonic pre-masking Most often when there is a “col-\nlision” between a solo harmonic and an orchestra\nharmonic, most of the spectral data will be due to\nthe solo. In such a case the orchestra model ends\nup using its free parameters mostly to explain solo\nenergy. To avoid this problem, we identify such col-\nlisions using our score and omit the orchestra model\nfor these harmonics.\nMasking bias With the ﬁnal results from the EM algo-\nrithm ®¤\nmandq¤\nm. Denote the solo and orchestraFigure 3 . Top: The original spectrogram after transient\nremoval. Bot: spectrogram after desoloing.\nmodels by MsandMa. Our solo and orchestra pro-\nﬁle estimates are then\nj^Xs(t; k)j=X\nm2Ms®¤\nmq¤\nm(t; k)\nj^Xa(t; k)j=X\nm2Ma®¤\nmq¤\nm(t; k)\nWe then estimate Sby\n^S=f(t; k) :j^Xs(t; k)j ¸Bj^Xa(t; k)jg (2)\nwhere B(0< B < 1) is the constant describing our\n“generosity” in labling points as solo points, and ^A\nis the complement of ^S.\nOur experiments focus on an excerpt from the 2nd move-\nment of Oboe Concerto in C major, K. 314, by Mozart. A\ndesoloed spectrogram is presented in contrast to the orig-\ninal one in Figure 3. The blackened area is corresponding\ntoS=f(t; k) :jXs(t; k)j ¸BjXa(t; k)jg. This audio\ncan be heard at http://xavier.informatics.indiana.\nedu/˜yushan/desolo_examples.html in contrast\nto the original. (A cutoff frequency of 5000Hz is set in or-\nder to accelerate the processing.)\nA record of a live soloist playing on top of the ”des-\noloed” orchestra is present to demonstrate how the solo\nwill mask the damage done by desoloing.\n5 REFERENCES\n[1]Bregman, A., Auditory Scene Analysis. MIT\nPress, 1990.\n[2]Cardoso, J., “Blind signal separation: statisti-\ncal principles,” Proceedings of the IEEE, spe-\ncial issue on blind identiﬁcation and estima-\ntion, vol. 9, no. 10, pp. 2009–2025, 1998.[3]Ellis, D., “Prediction-driven computational\nauditory scene analysis,” Ph.D. Dissertation,\nMIT Department of Electrical Engineering\nand Computer Science, 1996.\n[4]Bell, A. J., and Sejnowski, T. J., “An\nInformation-Maximization Approach to Blind\nSeparation and Blind Deconvolution,” Neural\nComputation , vol. 7, no. 6, pp. 1129–1159,\n1995.\n[5]Maher, R. C. “Evaluation of a Method for Sep-\narating Digitized Duet Signals” J. Audio Eng.\nSoc., vol. 38, no. 12, pp. 956–979, 1990.\n[6]Vincent, E., “Musical Source Separation Us-\ning Time-Frequency Source Priors,” IEEE\nTransactions on Speech and Audio Processing\nV olume 14, Issue 1, Jan. 2006 Page(s): 91 - 98\n[7]Ben-Shalom, A., Shalev-Shwartz, S., Wer-\nman, M., Dubnov, S. “Optimal Filtering of an\nInstrument Sound in a Mixed Recording Using\nHarmonic Model and Score Alignment,” Pro-\nceedings of the ICMC , 2004.\n[8]Zolzer, U. Editor, DAFX - Digital Audio Ef-\nfects. John Wiley and Sons, 2002.\n[9]Li Y ., and Wang D., “Singing V oice Separa-\ntion from Monaural Recordings,” Proceedings\nof the 7th International Conference on Music\nInformation Retrieval, Ed. Roger Dannenberg,\nKjell Lemstr ¨om and Adam Tindale, Victoria,\nBC, Canada, 176-179, 2006.\n[10] Ellis D., Chapter 4 of Computational Auditory\nScene Analysis: Principles and Algorithms, D.\nWang and G. Brown eds., Wiley/IEEE Press,\npp.115-146, 2006.\n[11] Raj B., Smaragdis Ellis D., “Latent Variable\nDecomposition of Spectrograms for Single\nChannel Speaker Separation,” IEEE Workshop\non Applications of Signal Processing to Audio\nand Acoustics pp. 17-20, Oct. 2005.\n[12] Bilmes, J.A., “A Gentle Tutorial of the EM\nAlgorithm and its Application to Parameter\nEstimation for Gaussian Mixture and Hidden\nMarkov Models,” International Computer Sci-\nence Institute, Berkeley, CA, Tech. Rep. TR-\n97-021, April 1998\n[13] Smith, Julius O., Spectral Audio Signal Pro-\ncessing, March 2007 Version. Center for\nComputer Research in Music and Acoustics\n(CCRMA), Stanford University.\n[14] Sam T. Roweis., “One microphone source sep-\naration,” Advances in Neural Information Pro-\ncessing Systems 13. 2001."
    },
    {
        "title": "Music Browsing Using a Tabletop Display.",
        "author": [
            "Stephen Hitchner",
            "Jennifer Murdoch",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415896",
        "url": "https://doi.org/10.5281/zenodo.1415896",
        "ee": "https://zenodo.org/records/1415896/files/HitchnerMT07.pdf",
        "abstract": "The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present findings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/selection process. 1 INTRODUCTION Similarity and clustering algorithms may be used to automatically incorporate meaningful structure into large, diverse, and disorganized collections of music. In particular, Self-Organizing Maps (SOMs) [3] are sought after for their unsupervised dimensionality-reduction applications. A growing body of research aims to enhance the effectiveness of automatic content-based analysis algorithms via intuitive user interface (UI) design. We believe the solution to truly effective music collection browsing combines the use of automatic techniques for structuring collections, with a framework for interaction that facilitates systematic and satisfying exploration of music collections. SOMs of music clusters are particularly amenable to satisfying visualization [4], but may also facilitate purely auditory exploration of the cluster map via tangible UIs such as the Radio Drum and Kaoss pad [5]. In particular, we incorporate use of a SOM for clustering of audio-based features, described in [9], automatically extracted from each track in a digital audio collection. Our primary aim is to support both the systematic exploration of music collections, and collaborative interaction between individuals with potentially diverse musical tastes The perception and appreciation of music is an interactive process that we feel is complimented by rich interaction among multiple individuals within social contexts. Music browsing is also an inherently interactive activity, requiring constant interplay between the user and the sysc⃝2007 Austrian Computer Society (OCG). Figure 1. User study participants interacting with the tabletop interface. tem. We believe that the (collaborative) browsing of music collections via existing systems is limited to a large extent by the use of the often cumbersome monitor / keyboard / mouse UI paradigm. The Jukola, a collaborative system for democratic music selection tested in natural communal setting, is presented in [6]. The use of large horizontal displays has been established as a means of supporting collaborative interaction and social cohesion [7]. As such, tabletop UIs are beginning to be explored for a number of music-related applications [2, 8]. We use the multi-touch front-projected DiamondTouch table developed in 2001 by the Mitsubishi Electric Research Laboratories [1]. Through qualitative user studies, we assess the relative effectiveness of two novel music-collection-browsing paradigms. We investigate consensus-reaching and processes whereby individuals tend to guide each other during exploration of a music collection. 2 SYSTEM OVERVIEW The MarGrid application uses the Marsyas 0.2 1 software framework to extract audio features[9] from a collection of digital audio tracks, and a SOM[3] is employed to organize the tracks within a 2D grid. A user can browse the 2D grid using a mouse and keyboard, MIDI controller pad, or tabletop UI (described below). Music collections 1 http://marsyas.sourceforge.net are imported via parsing of iTunes (XML) library files. Serveral UIs, providing different approaches to browsing/ interacting with music collections, were explored.",
        "zenodo_id": 1415896,
        "dblp_key": "conf/ismir/HitchnerMT07",
        "keywords": [
            "Music Information Retrieval (MIR)",
            "search/retrieval paradigm",
            "browsing as an interaction paradigm",
            "novel interfaces",
            "graphical tabletop interface",
            "content-aware browsing",
            "qualitative user studies",
            "collaborative interaction",
            "music collection browsing",
            "collaborative music selection"
        ],
        "content": "MUSIC BROWSING USING A TABLETOP DISPLAY\nStephen Hitchner\nUniveristy of Victoria\nComputer Engineering\nhitchner@engr.uvic.caJennifer Murdoch\nUniversity of Victoria\nComputer Science\njmurdoch@cs.uvic.caGeorge Tzanetakis\nUniversity of Victoria\nComputer Science\ngtzan@cs.uvic.ca\nABSTRACT\nThe majority of work in Music Information Retrieval\n(MIR) follows a search/retrieval paradigm. More recently,\nthe importance of browsing as an interaction paradigm has\nbeen realized, and several novel interfaces have been pro-\nposed. In this paper, we describe two novel interaction\nschemes for content-aware browsing of music collections\nthat use a graphical tabletop interface. We further present\nﬁndings from qualitative user studies. We describe our\nwork in the context of two primary themes: music col-\nlection browsing, and collaborative (multiple simultane-\nous users) interaction and involvement during the brows-\ning/selection process.\n1 INTRODUCTION\nSimilarity and clustering algorithms may be used to auto-\nmatically incorporate meaningful structure into large, di-\nverse, and disorganized collections of music. In particu-\nlar, Self-Organizing Maps (SOMs) [3] are sought after for\ntheir unsupervised dimensionality-reduction applications.\nA growing body of research aims to enhance the effec-\ntiveness of automatic content-based analysis algorithms\nvia intuitive user interface (UI) design. We believe the\nsolution to truly effective music collection browsing com-\nbines the use of automatic techniques for structuring col-\nlections, with a framework for interaction that facilitates\nsystematic and satisfying exploration of music collections.\nSOMs of music clusters are particularly amenable to sat-\nisfying visualization [4], but may also facilitate purely au-\nditory exploration of the cluster map via tangible UIs such\nas the Radio Drum and Kaoss pad [5]. In particular, we\nincorporate use of a SOM for clustering of audio-based\nfeatures, described in [9], automatically extracted from\neach track in a digital audio collection. Our primary aim\nis to support both the systematic exploration of music col-\nlections, and collaborative interaction between individuals\nwith potentially diverse musical tastes\nThe perception and appreciation of music is an inter-\nactive process that we feel is complimented by rich inter-\naction among multiple individuals within social contexts.\nMusic browsing is also an inherently interactive activity,\nrequiring constant interplay between the user and the sys-\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nFigure 1 .User study participants interacting with the\ntabletop interface.\ntem. We believe that the (collaborative) browsing of music\ncollections via existing systems is limited to a large extent\nby the use of the often cumbersome monitor / keyboard /\nmouse UI paradigm. The Jukola, a collaborative system\nfor democratic music selection tested in natural commu-\nnal setting, is presented in [6]. The use of large horizontal\ndisplays has been established as a means of supporting\ncollaborative interaction and social cohesion [7]. As such,\ntabletop UIs are beginning to be explored for a number of\nmusic-related applications [2, 8]. We use the multi-touch\nfront-projected DiamondTouch table developed in 2001\nby the Mitsubishi Electric Research Laboratories [1].\nThrough qualitative user studies, we assess the rela-\ntive effectiveness of two novel music-collection-browsing\nparadigms. We investigate consensus-reaching and pro-\ncesses whereby individuals tend to guide each other dur-\ning exploration of a music collection.\n2 SYSTEM OVERVIEW\nThe MarGrid application uses the Marsyas 0.21software\nframework to extract audio features[9] from a collection\nof digital audio tracks, and a SOM[3] is employed to or-\nganize the tracks within a 2D grid. A user can browse\nthe 2D grid using a mouse and keyboard, MIDI controller\npad, or tabletop UI (described below). Music collections\n1http://marsyas.sourceforge.netare imported via parsing of iTunes (XML) library ﬁles.\nServeral UIs, providing different approaches to browsing/\ninteracting with music collections, were explored.\n2.1 DiamondTouch Tabletop User Interface\nThe front-projected DiamondTouch Tabletop UI allows a\nuser to interact with MarGrid in a manner analogous to the\nsurface of a table. Both single-user mouse emulation and\nmulti-user multi-touch capability is supported.\nGraphical User Interface – Users interact with a graph-\nical rendering of the 2D SOM of music tracks. Once a col-\nlection has been imported, the user is given a list of their\nentire collection organized by both tracks and playlists.\nThe user is able to personalize the SOM training by drag-\nging and dropping selected tracks and playlists to speciﬁc\nlocations on the grid. Each grid square may contain more\nthan one track, and the density of a grid square is indicated\nby its colour. Track meta-data is displayed if available.\nUser Interaction – There are two supported interac-\ntion schemes: “continuous playback” and “click-to-play”.\nDuring “continuous playback”, moving the cursor into a\nnew square immediately starts playback (useful for brows-\ning for a speciﬁc song in a collection). During “click-to-\nplay” browsing, a square must be clicked to start play-\nback (useful for listening to an entire track without in-\nterruption while continuing to browse). Consecutive user\nclicks within a single square cycle through multiple tracks\nmapped to that square.\n2.2 Alternative Interfaces\nA developer can easily create a new UI by inheriting from\na simple abstract class. This class provides drag-and-drop,\nMIDI, and other common functionality. In addition to the\nGUI described above, several additional GUIs were devel-\noped that were not included in the user study.\nKeypad – Motivated by popular handheld electronic\ndevices, the Keypad UI emulates a typical numeric key-\npad. The Keypad UI can be thought of as a tree structure,\nwith each node (key) containing eight children; users can\ngenerate a personalized hierarchy of music by dragging-\nand-dropping tracks into a node. A user navigates the hier-\narchy by clicking a child node (all keys except 5) to move\ndown the tree, and the center node (key 5) to move up.\nThe tree structure optimizes use of the limited UI hard-\nware available on handheld devices.\nKAOSS MIDI Controller – The SOM is mapped to the\npad of a KAOSS MIDI controller, and thus, the need for\na graphical display is removed. When the user touches\nthe pad, the system plays the track corresponding to the\nposition the user touched.\n3 CONCLUSIONS AND FUTURE WORK\nOverall reaction to the MarGrid application was positive.\nUsers found it easy to adapt to the tabletop interface, and\nthey indicated the experience was generally enjoyable.While the “continuous playback” paradigm produced\nthe most animated, collaborative, and enjoyable user ex-\nperience, it did not facilitate the most effective music col-\nlection browsing. The “click-to-play” paradigm showed\ngreater potential for effective and efﬁcient browsing; inter-\naction between users was facilitated, and consensus was\nmet more frequently. Multi-touch capability, and the dis-\nplay of track meta-data, were cited by users as beneﬁcial\nfeatures of the music collection browsing system.\nOngoing research involving several rounds of user stud-\nies, carried out in both lab and natural public settings, will\nbe conducted to further enhance the user experience dur-\ning music collection browsing. We intent to incorporate\nuse of more complex hand gestures together with a more\nadvanced graphical rendering (OpenGL).\n4 REFERENCES\n[1] P. Dietz and D. Leigh. Diamondtouch: a multi-user\ntouch technology. In Proc. ACM Symposium on User\nInterface Software and Technology (UIST) , pages\n219–226, 2001.\n[2] M. Kaltenbrunner, S. Jorda, G. Geiger, and M. Alonso.\nThe reactable: A collaborative musical instrument.\nInIEEE Workshops on Enabling Technologies (WET-\nICE) , 2006.\n[3] T. Kohonen. The self-organizing map. In Proc. of the\nIEEE , volume 78, no. 9, pages 1464–1480, 1990.\n[4] F. Morchen, A. Ultsch, M. Nocker, and S. Christian.\nDatabionic visualization of music collections acoord-\ning to perceptual distance. In Proc. Int. Conf. on Music\nInformation Retrieval (ISMIR) , 2005.\n[5] J. Murdoch and G. Tzanetakis. Interactive content-\naware music browsing using the radio drum. In Proc.\nInt. Conf. on Multimedia Expo. (ICME) , 2006.\n[6] K. O’Hara, M. Lipson, M. Jansen, A. Unger, H. Jef-\nfries, and P. Macer. Jukola: Democratic music choice\nin a public space. In Proc. Conf. on Designing Inter-\nactive Systems , pages 145–154, 2004.\n[7] Y . Rogers and S. E. Lindley. Collaborating around ver-\ntical and horizontal large interactive displays: which\nway is best. In Interacting with Computers, vol. 16,\nno. 6 , pages 1133–1152, 2004.\n[8] I. Stavness, J. Gluck, L. Vilhan, and S. Fels. The\nmusictable: A map-based ubiquitous system for so-\ncial interaction with a digital music collection. In Int.\nConf. Entertainment Computing (ICEC) , pages 291–\n302, 2005.\n[9] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. In IEEE Transactions on Speech\nand Audio Processing , volume 10, no. 5, 2002."
    },
    {
        "title": "Content-Based Music Retrieval Using Query Integration for Users with Diverse Preferences.",
        "author": [
            "Keiichiro Hoashi",
            "Hiromi Ishizaki",
            "Kazunori Matsumoto",
            "Fumiaki Sugaya"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416004",
        "url": "https://doi.org/10.5281/zenodo.1416004",
        "ee": "https://zenodo.org/records/1416004/files/HoashiIMS07.pdf",
        "abstract": "This paper proposes content-based music information retrieval (MIR) methods based on user preferences, which aim to improve the accuracy of MIR for users with “diverse” preferences, i.e., users whose preferences range in songs with a wide variety of features. The proposed MIR method dynamically generates an optimal set of query vectors from the sample set of songs submitted by the user to express their preferences, based on the similarity of the songs in the sample set. Experiments conducted on a music collection with subjective user ratings verify that our proposal is effective to improve the accuracy of contentbased MIR. Furthermore, by implementing a two-step MIR algorithm which utilizes song clustering results, the efficiency of the proposed MIR method is significantly improved. 1 INTRODUCTION Recent popularity of online music distribution services have provided the opportunity to access to millions of songs, and also have enabled common users to accumulate a largescaled music collection. This rapid growth of both online and personal music collections has made it increasingly difficult for users to efficiently find songs which they want to listen to. Development of an effective music information retrieval (MIR) system is, therefore, essential to realize satisfactory music distribution services, and improve usability of music applications. Due to these recent developments, various research have been conducted in the area of content-based MIR based on user preferences. Logan has proposed a content-based MIR method which extracts the acoustic features from a set of songs, e.g., an album, which is defined as an expression of user preferences[8]. Grimaldi et al. have extended feature extraction techniques that have been proved effective for music genre classification, to conduct retrieval of music based on user preferences[5]. Furthermore, the authors have proposed a content-based MIR method, which generates a vector expression of user preferences from a sample set of songs submitted by the user[6][7]. c⃝2007 Austrian Computer Society (OCG). Although the above content-based MIR methods have been proved to be reasonably effective, the effectiveness of the methods are limited on conditions that the preferences of the users are focused. For example, the evaluations in [8] have utilized individual albums, which typically consist of similar songs, as an expression of user preferences. Furthermore, evaluations in [5] conclude that their MIR method is ineffective when user preferences are not focused on a specific genre. The main objective of this paper is to propose a method which is capable of providing accurate content-based MIR results to users with diverse preferences. Namely, this paper proposes a method which automatically generates an optimal number of queries from a sample set of songs submitted by the user, based on the similarity between the songs in the set. Effectiveness of the proposed method is verified by experiments conducted on a music collection with subjective user ratings. 2 CONVENTIONAL MIR METHODS In [6], the authors have proposed a content-based MIR method, which retrieves songs that fit music preferences based on a small set of example songs (hereafter referred as the “sample set”) provided by the user. Our MIR method applies the tree-based vector quantization (TreeQ) algorithm developed by Foote[2]. Furthermore, in [7], the authors have proposed a feature space modification (FSM) method, which utilizes song vector clustering results to automatically generate a training set for TreeQ, from any music collection. By implementing this method, the MIR system can build a feature space optimized to the songs in the music collection. By utilizing the tree-based vector quantizer (hereafter referred to as the “VQ tree”) generated by the above methods, our system is able to retrieve a list of songs which fit user preferences, based on the sample set of “good” songs submitted by the user. The system first generates a vector expression of the users’ preferences (hereafter referred to as the “user profile”), by calculating the vector sum of all vectors of the songs in the sample set. Scores of all songs in the collection are calculated based on the cosine similarity between the user profile and song vectors, and songs with high similarity to the query are presented to the user as the MIR result. 3 PROBLEMS As previously described, existing content-based MIR methods are able to achieve reasonable success in retrieving songs which fit user preferences. However, the accuracy of existing MIR methods, including our methods in Section 2, are dependent on the information submitted by the user. For instance, if the user inputs his/her preferences to the MIR system by specifying a single song or musical genre, it is fairly easy to derive accurate MIR results, since the user preference is well-expressed by the submitted query. It is obvious, though, that the preferences of users are not always focused on a specific genre and/or song. If anything, user preferences are usually diverged in multiple genres. In such cases, the accuracy of existing content-based MIR methods are expected to be degraded. For example, consider a situation where a noisy rock song and a soothing classical song are both included as “good” songs in the sample set. As mentioned in Section 1, most existing MIR methods are not capable to output accurate MIR results for users with such diverse preferences. Our previously proposed methods in [6] and [7] are capable of conducting MIR, even from such a diverse sample set. However, the user profile generated from the sample set is the sum vector of the two songs, meaning that the user profile points to the area in the middle of the two songs. Naturally, the MIR result based on the query is expected to consist of songs located in that area. In other words, the system will not be able to retrieve songs that are similar to either rock or classical songs, which are assumably a better representation of the user preferences than the songs that will be retrieved by these methods. A naive way to solve this problem is to utilize the vectors of all songs in the sample set as independent queries, and merge all MIR results obtained from each query. However, this approach obviously will increase the computational cost to conduct MIR. 4 QUERY INTEGRATION In order to solve the previously described problems, we propose the query integration method. The objective of this method is to automatically determine an optimal number of queries to be generated from the sample set, based on the similarity of the features of the songs in the set. A conceptual illustration of the proposed method is shown in Figure 1. Figure 1 illustrates a situation where the sample set consists of six songs, S1, · · · , S6. There are two sets of songs in the sample set that are highly similar to each other, {S1, S2, S3}, and {S4, S5}. In this case, the intuitionally optimal set of queries can be generated by integrating the songs in these two sets to generate a single query, which represents each set respectively, and utilizing S6 as an independent query. The proposed query integration method is implemented by conducting hierarchical clustering of the song vectors included in the sample set. First, the similarity between all song vectors in the sample set are calculated. Next, the Music feature space S1 S2 S3 S4 S5 S6 : song vector : integrated query Figure 1. Conceptual illustration of query integration two songs with the highest similarity are extracted. If the similarity between these two songs exceed a predefined threshold τq, the songs are integrated to a single vector, by summing the vectors of the two songs. This procedure is repeated until all song vectors in the sample set are integrated to a single vector, or when the maximum similarity fails to exceed τq. Query integration is expected to solve the diverse user preference problem, since the generated queries are able to represent the features of the songs in the sample set if they are diverse, and, simultaneously, can generate a focused query whenever appropriate. Furthermore, by integrating vectors of highly similar songs, this method can reduce the increase of computational cost, compared to conducting MIR individually for all songs in the sample set. 5 EXPERIMENTS In order to evaluate the accuracy and efficiency of the proposed method, an experiment is conducted based on a large music collection with subjective user ratings. Details of the experiments are as follows.",
        "zenodo_id": 1416004,
        "dblp_key": "conf/ismir/HoashiIMS07",
        "keywords": [
            "content-based music information retrieval",
            "user preferences",
            "diverse preferences",
            "dynamic query vector generation",
            "song clustering",
            "user ratings",
            "effective accuracy improvement",
            "two-step MIR algorithm",
            "music distribution services",
            "music applications"
        ],
        "content": "CONTENT -BASED MUSIC RETRIEV ALUSING QUER YINTEGRA TION\nFOR USERS WITH DIVERSE PREFERENCES\nKeiichir oHoashi HiromiIshizaki Kazunori Matsumoto Fumiaki Sugaya\nKDDI R&D Laboratories, Inc.\n2-1-15 Ohara Fujimino-shi, Saitama 356-8502, JAPAN\ne-mail: fhoashi, ishizaki, matsu, fsugayag@kddilabs.jp\nABSTRA CT\nThis paper proposes content-based music information re-\ntrieval(MIR) methods based onuser preferences, which\naimtoimpro vetheaccurac yofMIR forusers with di-\nverse preferences, i.e.,users whose preferences range in\nsongs with awide variety offeatures. Theproposed MIR\nmethod dynamically generates anoptimal setofquery vec-\ntorsfrom thesample setofsongs submitted bytheuser to\nexpress their preferences, based onthesimilarity ofthe\nsongs inthesample set.Experiments conducted onamu-\nsiccollection with subjecti veuser ratings verify thatour\nproposal iseffectivetoimpro vetheaccurac yofcontent-\nbased MIR. Furthermore, byimplementing atwo-step MIR\nalgorithm which utilizes song clustering results, theef\u0002-\ncienc yoftheproposed MIR method issigni\u0002cantly im-\nproved.\n1INTR ODUCTION\nRecent popularity ofonline music distrib ution services\nhaveprovided theopportunity toaccess tomillions ofsongs,\nandalsohaveenabled common users toaccumulate alarge-\nscaled music collection. This rapid growthofboth online\nandpersonal music collections hasmade itincreasingly\ndif\u0002cult forusers toef\u0002ciently \u0002ndsongs which theywant\ntolisten to.Development ofaneffectivemusic informa-\ntionretrie val(MIR) system is,therefore, essential toreal-\nizesatisf actory music distrib ution services, andimpro ve\nusability ofmusic applications.\nDuetothese recent developments, various research have\nbeen conducted inthearea ofcontent-based MIR based\nonuser preferences. Loganhasproposed acontent-based\nMIR method which extracts theacoustic features from a\nsetofsongs, e.g.,analbum,which isde\u0002ned asanexpres-\nsion ofuser preferences[8 ].Grimaldi etal.haveextended\nfeature extraction techniques thathavebeen provedeffec-\ntiveformusic genre classi\u0002cation, toconduct retrie valof\nmusic based onuser preferences[5 ].Furthermore, theau-\nthors haveproposed acontent-based MIR method, which\ngenerates avector expression ofuser preferences from a\nsample setofsongs submitted bytheuser[6 ][7].\nc°2007 Austrian Computer Society (OCG).Although theabovecontent-based MIR methods have\nbeen provedtobereasonably effective,theeffectiveness\nofthemethods arelimited onconditions thattheprefer -\nences oftheusers arefocused. Forexample, theevalu-\nations in[8]haveutilized individual albums, which typ-\nically consist ofsimilar songs, asanexpression ofuser\npreferences. Furthermore, evaluations in[5]conclude that\ntheir MIR method isineffectivewhen user preferences are\nnotfocused onaspeci\u0002c genre.\nThemain objecti veofthispaper istopropose amethod\nwhich iscapable ofproviding accurate content-based MIR\nresults tousers with diversepreferences. Namely ,thispa-\nperproposes amethod which automatically generates an\noptimal number ofqueries from asample setofsongs sub-\nmitted bytheuser,based onthesimilarity between the\nsongs intheset.Effectiveness oftheproposed method is\nveri\u0002ed byexperiments conducted onamusic collection\nwith subjecti veuser ratings.\n2CONVENTION ALMIR METHODS\nIn[6],theauthors haveproposed acontent-based MIR\nmethod, which retrie vessongs that\u0002tmusic preferences\nbased onasmall setofexample songs (hereafter referred\nasthesample set) provided bytheuser.OurMIR method\napplies thetree-based vector quantization (TreeQ) algo-\nrithm developed byFoote[2 ].Furthermore, in[7],theau-\nthors haveproposed afeature space modi\u0002cation (FSM)\nmethod, which utilizes song vector clustering results to\nautomatically generate atraining setforTreeQ, from any\nmusic collection. Byimplementing thismethod, theMIR\nsystem canbuildafeature space optimized tothesongs in\nthemusic collection.\nByutilizing thetree-based vector quantizer (hereafter\nreferred toastheVQ tree) generated bytheabovemeth-\nods, oursystem isable toretrie vealistofsongs which \u0002t\nuser preferences, based onthesample setofgood songs\nsubmitted bytheuser.Thesystem \u0002rstgenerates avector\nexpression oftheusers' preferences (hereafter referred to\nastheuser pro\u0002le), bycalculating thevector sum ofall\nvectors ofthesongs inthesample set.Scores ofallsongs\ninthecollection arecalculated based onthecosine simi-\nlarity between theuser pro\u0002le andsong vectors, andsongs\nwith high similarity tothequery arepresented totheuser\nastheMIR result.3PROBLEMS\nAspreviously described, existing content-based MIR meth-\nodsareable toachie vereasonable success inretrie ving\nsongs which \u0002tuser preferences. However,theaccurac y\nofexisting MIR methods, including ourmethods inSec-\ntion2,aredependent ontheinformation submitted bythe\nuser.Forinstance, iftheuser inputs his/her preferences\ntotheMIR system byspecifying asingle song ormusi-\ncalgenre, itisfairly easy toderiveaccurate MIR results,\nsince theuser preference iswell-e xpressed bythesubmit-\ntedquery .Itisobvious, though, thatthepreferences of\nusers arenotalwaysfocused onaspeci\u0002c genre and/or\nsong. Ifanything, user preferences areusually diverged\ninmultiple genres. Insuch cases, theaccurac yofexisting\ncontent-based MIR methods areexpected tobedegraded.\nForexample, consider asituation where anoisy rock\nsong andasoothing classical song areboth included as\ngood songs inthesample set.Asmentioned inSection\n1,most existing MIR methods arenotcapable tooutput\naccurate MIR results forusers with such diverse prefer -\nences. Our previously proposed methods in[6]and[7]\narecapable ofconducting MIR, evenfrom such adiverse\nsample set.However,theuser pro\u0002le generated from the\nsample setisthesum vector ofthetwosongs, meaning\nthattheuser pro\u0002le points tothearea inthemiddle ofthe\ntwosongs. Naturally ,theMIR result based onthequery\nisexpected toconsist ofsongs located inthat area. In\nother words, thesystem willnotbeable toretrie vesongs\nthataresimilar toeither rock orclassical songs, which are\nassumably abetter representation oftheuser preferences\nthan thesongs thatwillberetrie vedbythese methods.\nAnaivewaytosolvethisproblem istoutilize thevec-\ntorsofallsongs inthesample setasindependent queries,\nandmergeallMIR results obtained from each query .How-\never,thisapproach obviously will increase thecomputa-\ntional costtoconduct MIR.\n4QUER YINTEGRA TION\nInorder tosolvethepreviously described problems, we\npropose thequery integration method. The objecti veof\nthismethod istoautomatically determine anoptimal num-\nberofqueries tobegenerated from thesample set,based\nonthesimilarity ofthefeatures ofthesongs intheset.\nAconceptual illustration oftheproposed method is\nshowninFigure 1.Figure 1illustrates asituation where\nthesample setconsists ofsixsongs,S1;¢¢¢;S6.There\naretwosetsofsongs inthesample setthatarehighly sim-\nilartoeach other ,fS1;S2;S3g,andfS4;S5g.Inthiscase,\ntheintuitionally optimal setofqueries canbegenerated by\nintegrating thesongs inthese twosetstogenerate asingle\nquery ,which represents each setrespecti vely,andutilizing\nS6asanindependent query .\nTheproposed query integration method isimplemented\nbyconducting hierarchical clustering ofthesong vectors\nincluded inthesample set. First, thesimilarity between\nallsong vectors inthesample setarecalculated. Next,theMusic feature space\nS1\nS2\nS3S4\nS5\nS6\n: song vector : integrated query\nFigur e1.Conceptual illustration ofquery integration\ntwosongs with thehighest similarity areextracted. Ifthe\nsimilarity between these twosongs exceed aprede\u0002ned\nthreshold ¿q,thesongs areintegrated toasingle vector ,\nbysumming thevectors ofthetwosongs. This procedure\nisrepeated until allsong vectors inthesample setareinte-\ngrated toasingle vector ,orwhen themaximum similarity\nfailstoexceed¿q.\nQuery integration isexpected tosolvethediverse user\npreference problem, since thegenerated queries areable\ntorepresent thefeatures ofthesongs inthesample setif\ntheyarediverse, and, simultaneously ,cangenerate afo-\ncused query whene verappropriate. Furthermore, byinte-\ngrating vectors ofhighly similar songs, thismethod can\nreduce theincrease ofcomputational cost, compared to\nconducting MIR individually forallsongs inthesample\nset.\n5EXPERIMENTS\nInorder toevaluate theaccurac yandef\u0002cienc yofthe\nproposed method, anexperiment isconducted based on\nalargemusic collection with subjecti veuser ratings. De-\ntails oftheexperiments areasfollows.\n5.1Experimental setup\n5.1.1 Data\nThemusic collection used forourexperiments isthesame\nasthecollection used intheexperiments in[7],which is\nconstructed bycombining theexperiment data setused in\ntheexperiments in[6],with songs included intheCDs\nlisted intheuspop2002 music datasetconstructed byEllis[1 ].\nThetotal number ofsongs inthemusic collection is6863.\nRating data forourMIR experiments, which areused\nfortheevaluation ofMIR accurac y,arecollected byinvit-\ning20subjects toapply subjecti veratings tothesongs\nintheabovemusic data set,ranging from 1to5(Bad:1\n»Good:5). The rated data isthen classi\u0002ed into three\ncategories, according totheratings applied toeach song.\nThethree categories are:good songs (Cg),bad songs\n(Cb),andfairsongs (Cf).CategoriesCg,Cb,andCf\nconsist ofsongs rated (4or5),(1or2),and3,respec-\ntively.Table 1showstheaverage ratio ofsongs perrating\nandcategory forallsubjects.Table 1.Summary ofuser rating data\nCategory Rating Ratio(%)\nCg 5 16.9\n4 20.1\nCf 3 31.3\nCb 2 20.4\n1 11.2\n5.1.2 VQtreeconstruction\nTheVQtree, which isused inthefollowing experiments,\nisconstructed based ontheFSM method described in[7].\nFirst, theinitial feature space isgenerated byusing the\nRWC Genre Database [4]astraining data. Next,thefea-\nturespace ismodi\u0002ed bycluster -based FSM. Thenumber\nofclusters, andthenumber ofsongs thatareextracted as\ntraining data from each cluster forFSM, aresetto10and\n3,respecti vely.These values aredetermined empirically ,\nbased ontheresults ofpreliminary experiments.\n5.2Method\nInthisexperiment, asample setofsongs foreach subject\nis\u0002rstgenerated byrandomly extracting N=5songs that\nbelong toCg.This sample setisutilized togenerate two\nuserpro\u0002les, onebased onquery integration, andtheother\nbytheconventional MIR method, where thesum vector of\nallNsongs inthesample setisutilized astheuser pro\u0002le.\nNext,MIR isconducted using thetwouser pro\u0002les. Ifthe\nquery integration method generates multiple queries from\nthesample set,theMIR result isgenerated bymerging the\nresults from each generated query ,bysorting songs based\nontheir score ineach MIR result. The\u0002nal result isob-\ntained byextracting thetopNumsongs based ontheir\nscore. Inorder tocompensate therandomness ofthesam-\nplesetgeneration process, thisquery generation process\nisrepeated \u0002vetimes persubject.\nThethreshold forthequery integration method, ¿q,is\ndetermined bythefollowing formula: ¿q=¹sim+®¢\n¾sim,where¹simand¾simdenote theaverage, andstan-\ndard deviation ofallsong-to-centroid similarity values, re-\nspecti vely.®isacoef\u0002cient which isde\u0002ned toadjust the\nvalue of¿q.Inthefollowing experiments, thevalue of®\nissetas®=f0:5;1:0g.\n5.3Evaluation measur es\nInorder toevaluate MIR accurac y,wecalculate theratio\nofsongs intheMIR result which belong tocategoriesCg,\nCf,andCb,andusetheaverage ratio ofallexperiments as\nthe\u0002nal evaluation measure. Ahigh ratio ofsongs inCg\nindicate thatmore preferable songs havebeen retrie ved,\nthus isconsidered asasuperior result.\nFurthermore, inorder tomeasure MIR ef\u0002cienc y,we\ncount thenumber ofsimilarity calculations executed in\neach experiment, andcalculate theratio tothenumber\nofsongs inthemusic collection (hereafter referred toas\nCalcRate ).Note that, inthefollowing experiments, theTable 2.Average MIR accurac yandef\u0002cienc yofquery\nintegration andconventional methods (Num=50)\nMethod CgCfCb CalcRate\nQI(®=0:5)0.668 0.195 0.137 291.3%\nQI(®=1:0)0.683 0.187 0.130 364.3%\nConv 0.585 0.244 0.170 100.0%\nNo-QI 0.702 0.176 0.122 500.0%\nnumber ofqueries apply aneffecttoCalcRate .Namely ,\nwhenK>1queries aregenerated asaresult ofquery in-\ntegration, theCalcRate oftheMIR process isK£100% .\n5.4Results\nComparison oftheMIR accurac yoftheproposed query\nintegration method (QI)andtheconventional method (Conv)\nisshowninTable 2,where theaverage ratio ofsong cate-\ngories inallMIR results whenNum=50islisted. For\nadditional comparison, wealso present theresults ofex-\nperiments when noquery integration isconducted (No-\nQI),i.e.,where allN=5songs inthesample setare\ntreated asindividual queries. Furthermore, theaverage\nCalcRate ofallexperiments arealsowritten inthisTable.\nResults inTable 2indicate thatallmethods haveac-\ncurately retrie vedsongs that\u0002tuser preferences, since the\nratio ofsongs inCgintheMIR results allexceed theover-\nallratio ofsongs inCg(=37.0%, from Table 1).Itisalso\nclear thattheQImethods aresuperior toConv,based on\ncomparison oftheCgratio ofthetwomethods. Interms\nofMIR accurac y,theNo-QI method hasachie vedthebest\nresults among ourexperiments. However,theCalcRate\nresults indicate thattheQImethods arecapable ofimpro v-\ningaccurac y,without increasing theamount ofcomputa-\ntionasmuch asNo-QI .Under extreme conditions where\nthequery consists ofhundreds ofsongs, thecost ofthe\nquery integration process may apply asigni\u0002cant effectto\ntheoverall performance oftheMIR process. However,in\narealistic situation whereNisasmall number ,wecan\nconclude thattheQImethod iscapable ofachie ving accu-\nrateMIR, while limiting increase ofcomputational cost.\nNext,weconduct analysis tocon\u0002rm theeffectiveness\nofthequery integration method forusers with diversepref-\nerences, asassumed inourhypotheses. First, wede\u0002ne\nthediversity ofaquery astheaverage similarity be-\ntween thesongs included inthequery (hereafter referred\ntoasAvgQSim ).IftheAvgQSim ofaquery islow,thepref-\nerence expressed bythequery isconsidered asdiverse. On\nthecontrary ,queries with high AvgQSim areconsidered to\nbefocused, since allsongs inthequery aresimilar toeach\nother .Inthefollowing analysis, queries arecategorized to\nthefollowing three classes: LowSim ,MidSim ,andHiSim ,\nwhich consist ofqueries whose AvgQSim isbelow0.3,be-\ntween 0.3and0.5,andover0.5,respecti vely.\nTable 3lists theaverage rate ofsongs inCgforthe\nConvandQIresults. These results indicate thattheMIR\naccurac yforqueries intheLowSim andMidSim classes\nhavebeen signi\u0002cantly impro vedbyQI,verifying that, asTable 3.Average ratio ofsongs inCgforeach query class\n(Num=50)\nQuery class ConvQI(®=0:5)QI(®=1:0)\nLowSim 0.562 0.699 0.715\nMidSim 0.598 0.678 0.702\nHiSim 0.558 0.565 0.564\nhypothesized, QIisespecially effectiveforgenerating ac-\ncurate MIR results forusers with diversepreferences.\nGenerally ,theexperimental results indicate thatahigh\n¿qisbene\u0002cial forimpro vement ofMIR accurac y.How-\never,utilization ofa\u0002xed¿qdoes notnecessarily impro ve\nMIR results forallqueries, ascanbeobserv edfrom the\nlowimpro vement rateofHiSim queries inTable 3.There-\nfore, amethod which candetermine anoptimal threshold\nforquery integration, based onthefeatures ofthesongs\ninthesample set,isassumed tobenecessary forfurther\nimpro vement ofMIR accurac y.\n6CLUSTER SELECTION MIR\nWhile theprevious experiment hasprovedtheeffective-\nness ofquery integration, itisalso true thatthemethod\nincreases thecomputational cost oftheMIR process, as\nindicated bythehigh CalcRate values inTable 2.Inorder\ntoimpro veMIR ef\u0002cienc y,wepropose aselecti veMIR\nmethod, which utilizes theclustering results ofthesongs\ninthecollection.\nThecluster selection MIR method utilizes thecluster -\ningresults oftheVQtreeconstruction phase described in\nSection 5.1.2. Allsongs inthecollection areassociated\ntoacluster ,byselecting thecluster whose centroid has\nthehighest similarity tothesong vector .Next,inorder\ntoselect thetargetcluster setforagivenquery ,wecal-\nculate thesimilarity between thequery andeach cluster\ncentroid, andselect thetopmclusters based onthequery-\ncentroid similarity .MIR isthen conducted only forthe\nsongs which belong totheclusters inthetargetset.The\ncombination ofcluster selection MIR with query integra-\ntionisimplemented byselecting thetargetsetofclusters\nforeach query generated bythequery integration process.\nAsimple experiment isconducted toevaluate theac-\ncurac yandef\u0002cienc yofcombining cluster selection MIR\nwith query integration. Table 4lists theaverage accu-\nracyandCalcRate ofallconducted experiments. Note that\nCSinTable 4expresses thecluster selection MIR runs.\nBycomparison oftheresults inTables 2and4,itis\nclear thatMIR accurac ysimilar tothatoftheoriginal QI\nmethod canbeachie vedwhenmissetto4forcluster se-\nlection MIR, while signi\u0002cantly reducing CalcRate .Fur-\nthermore, eveniftheconditions ofcluster selection are\nextreme, e.g.,whenm=1,theMIR accurac ystillout-\nperforms thatofConv,while reducing CalcRate downto\n31.3%. These results indicate thattheincrease ofcompu-\ntational costnecessary forquery integration canbeeasily\nreduced byimplementing cluster selection MIR.Table 4.Average accurac yandef\u0002cienc yofquery inte-\ngration combined with cluster selection MIR\nMethod CgCfCb CalcRate\nQI+CS (®=0:5,m=1)0.606 0.221 0.173 31.3%\nQI+CS (®=0:5,m=2)0.645 0.199 0.156 69.1%\nQI+CS (®=0:5,m=3)0.661 0.198 0.141 109.7%\nQI+CS (®=0:5,m=4)0.666 0.195 0.139 148.7%\nQI+CS (®=1:0,m=1)0.609 0.221 0.171 40.9%\nQI+CS (®=1:0,m=2)0.658 0.193 0.149 89.6%\nQI+CS (®=1:0,m=3)0.674 0.190 0.136 141.6%\nQI+CS (®=1:0,m=4)0.680 0.187 0.133 190.3%\n7CONCLUSION\nInthispaper ,wehaveproposed aquery integration method,\nwhich aims toachie veaccurate MIR forusers who have\ndiverse preferences. Theoverall results ofevaluation ex-\nperiments conducted onamusic collection with subjecti ve\nuser ratings haveprovedthattheproposed method isca-\npable toprovide accurate MIR results forsuch users. Fur-\nthermore, theimplementation ofcluster selection MIR has\nprovedtobeeffectivetoreduce computational cost with\nminimal sacri\u0002ce ofMIR accurac y.\n8ACKNO WLEDGMENTS\nThis research used theTreeQ package[3 ]developed by\nJonathan T.Foote.\n9REFERENCES\n[1]D.Ellis: The uspop2002 PopMusic data set, Listavail-\nable athttp://www .ee.columbia.edu/%7Edpwe/research/\nmusicsim/uspop.html, 2003.\n[2]J.Foote: Content-based retrie valofmusic andaudio,\nProceedings ofSPIE, Vol3229, pp138-147, 1997.\n[3]J.Foote: TreeQ softw are,http://treeq.sourcefor ge.net/\n[4]M.Goto, H.Hashiguchi, T.Nishimura, R.Oka: RWC\nMusic Database: Music Genre Database andMusical In-\nstrument Sound Database, Proc.ofISMIR ,pp.229-230,\nBaltimore, MD, USA, 2003.\n[5]M.Grimaldi, P.Cunningham: Experimenting with mu-\nsictaste prediction byuser pro\u0002ling, Proc.of6thACM\nSIGMM Int'l Workshop onMultimedia Information Re-\ntrieval,pp173-180, 2004.\n[6]K.Hoashi, K.Matsumoto, N.Inoue: Personalization of\nuser pro\u0002les forcontent-based music retrie valbased on\nrelevance feedback, Proc. ofACMMultimedia 2003, pp.\n110-119, 2003.\n[7]K.Hoashi, K.Matsumoto, F.Sugaya,H.Ishizaki, J.Katto:\nFeature space modi\u0002cation forcontent-based music re-\ntrievalbased onuser preferences, Proc. ofICASSP 2006,\npp.517-520, 2006.\n[8]B.Logan:Music recommendation from song sets, Pro-\nceedings ofISMIR ,Barcelona, Spain, 2004."
    },
    {
        "title": "Creating a Simplified Music Mood Classification Ground-Truth Set.",
        "author": [
            "Xiao Hu 0001",
            "Mert Bay",
            "J. Stephen Downie"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416920",
        "url": "https://doi.org/10.5281/zenodo.1416920",
        "ee": "https://zenodo.org/records/1416920/files/HuBD07.pdf",
        "abstract": "A standardized mood classification testbed is needed for formal cross-algorithm comparison and evaluation. In this poster, we present a simplification of the problems associated with developing a ground-truth set for the evaluation of mood-based Music Information Retrieval (MIR) systems. Using a dataset derived from Last.fm tags and the USPOP audio collection, we have applied a K-means clustering method to create a simple yet meaningful cluster-based set of high-level mood categories as well as a ground-truth dataset. 1",
        "zenodo_id": 1416920,
        "dblp_key": "conf/ismir/HuBD07",
        "keywords": [
            "standardized mood classification testbed",
            "formal cross-algorithm comparison",
            "evaluation of mood-based Music Information Retrieval (MIR) systems",
            "dataset derived from Last.fm tags",
            "USPOP audio collection",
            "K-means clustering method",
            "simple yet meaningful cluster-based set of high-level mood categories",
            "ground-truth dataset",
            "MIR systems",
            "evaluation of mood-based Music Information Retrieval (MIR) systems"
        ],
        "content": "CREATING A SIMPLIFIED MUSIC MOOD \nCLASSIFICATION GROUND-TRUTH SET \nXiao Hu Mert Bay J. Stephen Downie \nInternational Music Information Retrieval Systems E valuation Laboratory \nThe Graduate School of Library and Information Scie nce \nUniversity of Illinois at Urbana-Champaign \n{xiaohu, mertbay, jdownie}@uiuc.edu \nABSTRACT \nA standardized mood classification testbed is neede d for \nformal cross-algorithm comparison and evaluation. I n \nthis poster, we present a simplification of the pro blems \nassociated with developing a ground-truth set for t he \nevaluation of mood-based Music Information Retrieva l \n(MIR) systems. Using a dataset derived from Last.fm  \ntags and the USPOP audio collection, we have applie d a \nK-means clustering method to create a simple yet \nmeaningful cluster-based set of high-level mood \ncategories as well as a ground-truth dataset. \n1 INTRODUCTION  \nThe emotional component of music has attracted inte rest \nin the MIR community, and experiments have been \nconducted to classify music by mood (e.g., [3]). \nHowever, the lack of a standardized mood term set a nd \nassociated audio datasets that are accessible to th e \ncommunity impedes comparisons among approaches. \nThis research strives to construct a highly simplif ied, yet \nreasonable, music mood term set, and to associate t his \nterm set with a commonly used audio test collection  that \ncould be used as ground truth for a proposed “Audio  \nMusic Mood Classification” task in Music Informatio n \nRetrieval Evaluation eXchange (MIREX)1. \nDeriving a mood term set from the real world \npractice of music information services (i.e. popula r \nmusic websites and software) has the advantage of \ngrounding analyses in the realistic social contexts  of \nmusic seeking and consumption. Besides, such servic es \nusually associate tracks with mood related labels o r tags \nwhich can be used to obtain a ground-truth set. \nTherefore, we take this approach, basing our analys es on \na dataset derived from Last.fm  and the USPOP \ncollection. \n2 USPOP and Last.fm \nUSPOP is a collection of audio tracks from ~700 CDs  \ncollected in 2002 by Dan Ellis of Columbia Universi ty. \nIt has been used as in previous MIREX [1] evaluatio ns \n(e.g., Audio Genre Classification). In order to use  the \nUSPOP collection for an Audio Mood Classification \n                                                           \n MIREX task, its tracks must be tagged with a well \ngrounded set of mood labels.  \nLast.fm  is a website collecting public users’ input \nabout music such as playlists, favourite artists, a nd free- \ntext tags. Many of these tags describe feelings ins pired \nby the music pieces, and thus can be used as mood l abel \ncandidates. Furthermore, user tags on Last.fm  are \navailable to the public through their web services API.2 \nThus, the Last.fm  tags, when brought together with the \nUSPOP tracks, provide an opportunity for MIR \nresearchers to derive a mood classification categor y set \nassociated with an audio dataset.   \n3 BRINGING TAGS AND AUDIO TOGETHER \n3.1 Top Tags on USPOP Tracks \nThrough the Last.fm  web services, we obtained tags \nfor tracks in USPOP for which entries on Last.fm  exist. \nBecause of the popularity of the USPOP tracks, they  are \ncovered quite well by the Last.fm  system. Information \nregarding 8333 USPOP tracks (out of the total 8764)  is \navailable at Last.fm , with 6747 USPOP tracks having \nat least one tag. We collected the top 100 tags for  each \nof these 6747 songs, and resulting in 10178 unique tags.  \nAdj. # Adj. #  Adj. # \nelectronic  27949 instrumental  3664  political 1446  \nmellow*  15609 progressive  3563 aggressive*  1368  \nindustrial  14002 dark* 3271  powerful*  1365  \nsad*  9967 vocal 3157  male 1304  \nawesome   6692 heavy 2701  emotional*  1304  \nclassic 5841 cool* 2246  soft* 1278  \nrelaxing*  4562 slow 2222  sleek 1132  \nsexy* 4327 experimental  2207  energetic*  1128  \nupbeat*  4318 melancholy*  2172  classical 1061  \nromantic*  3828 funny* 1669  calm* 860  \nhappy*  3683 angry* 1496  depressing*  804  \n \nTable 1. Most popular adjectives for USPOP tracks \n3.2 Adjective Tags on USPOP Tracks  \nAs mood is usually described by adjectives, we used  an \nEnglish part-of-speech (POS) tagger 3 to filter the tags \nassociated with the USPOP tracks. A total of 782 si ngle \n                                                           \n1http://music-ir.org/mirexwiki  \n2 http://www.audioscrobbler.net/data/webservices  \n3 http://search.cpan.org/dist/Lingua-EN-Tagger/  © 2007 Austrian Computer Society (OCG). \n   \n \nword adjective tags remained. Table 1 shows the mos t \npopular adjectives and their counts in the dataset.  \nAmong them, some are genre terms (e.g., “electronic ”, \n“classic”) and others are non-mood related (e.g., \n“instrumental”, “female”). We selected 19 terms fro m \nthe list that we deemed to be associated with music  \nmoods (marked with * in Table 1).   \n3.3 Track Clustering Based on Selected Adjectives \nThere are 2554 USPOP tracks associated with at leas t \none of the 19 adjectives. Each of the 2554 tracks t hen \nwas represented by a 19-dimensional binary tag vect or. \nEach dimension corresponds to one of the 19 adjecti ves \nand has value 1 if the track is tagged by that adje ctive or \n0 otherwise. K-means clustering [2] using Hamming \ndistance was performed on this space varying the \nnumber of clusters from 3 to 12. The resulting clus ters \npartitioned the 2554 tracks into mutually exclusive  \ngroups that can serve as a ground-truth set with mo od \nlabels. To determine the optimal number of clusters , K-\nmeans was performed with 100 random seeds for each \nnumber of clusters and the maximum silhouette value  \nfor each case was calculated [2]. Among the \nexperiments, the 3-cluster solution resulted in the  \nhighest maximum silhouette value, which convinced u s \nto choose 3 clusters (Table 2). Basic statistics of  the \nindividual clusters are given in Table 3. \n# of clusters 3 4 5 6 \nMax. silhouette value  0.348  0.316  0.324  0.326  \nTable 2.  Silhouette values for varying cluster numbers  \n C.1  C. 2  C.3  \n# of tracks  1219  853  482  \nIntra-cluster variance  8.002  10.402  3.155  \nAve. Intra-cluster distance 0.067  0.078  0.069  \nTable 3. Statistics of the 3 clusters of USPOP tracks \nC.1 C. 2 C.3 \naggressive  90.7%  mellow 98.4%  upbeat  90.0%  \nangry 89.2%  calm 62.9% happy  79.3%  \nTable 4. Adjectives with the highest ratios  \n3.4 Representative Adjectives in Clusters \nAfter separating the 2554 tracks into 3 clusters, f or each \nadjective we calculated the percentage of the track s \ntagged with that adjective in each cluster. The rea son we \nused such percentages instead of raw counts is that  the \ndataset is highly unbalanced such that some adjecti ves \nwere tagged to nearly 900 tracks whereas some other s \nwere tagged to only 70 tracks. Table 4 shows the to p \nadjectives in 3 clusters and their percentages.  \nThe term combinations shown in Table 4 seem \nreasonable, and these top adjectives in each cluste r \ntogether can well define the mood nature of the clu ster. \nTo further verify the closeness of such top terms i n this \nsample space, we performed Principal Component \nAnalysis (PCA) on the 19 mood adjectives using the 2554 tracks as observations. Figure 1 is the plot o f these \nadjectives based on the first two principal compone nts \nof this dataset.  As it can be seen, top adjectives  in each \nof the 3 clusters are close to each other, which fu rther \nconfirms the clustering results are reasonable.   \n-0.5 -0.4 -0.3 -0.2 -0.1 00.1 0.2 0.3 0.4 -0.5 -0.4 -0.3 -0.2 -0.1 00.1 0.2 0.3 0.4 0.5 \nPCA1 PCA2 \naggressive \nangry calm \ncool\ndark depressing emotionalenergetic \nfunny happy \nmelancholy mellow \npowerfulrelaxing \nromantic \nsad sexy soft upbeat \nFigure 1.  First two principal components of the dataset  \n4 CONCLUSION AND FUTURE WORK \nThe 3 mood clusters we derived from the association s \nbetween the Last.fm  user tags and the USPOP audio \ncollection provide a simplified mood ground-truth s et \nthat is rooted in the social context of the real wo rld. This \nset is arguably over-simplified but it is a practic al set \nnonetheless. As such, it should be seen as a starti ng \npoint for further debates on the construction of mo od-\nbased evaluation tasks.  \nAs future work, we will continue  to examine datase ts \nfrom other influential music services such as \nAllMusicGuide. We will also explore different \nclustering techniques and will investigate how our \napproach is generalizable to other contexts. \n5 ACKNOWLEDGEMENT \nThis project is funded by the National Science \nFoundation (IIS-0327371) and the Mellon Foundation.  \nWe also thank the anonymous reviewers and Dr. David  \nDubin for his statistical consultations.  \n6 REFERENCES  \n[1] Downie, J. S. “The Music Information Retrieval \nEvaluation eXchange (MIREX)”, D-Lib Magazine \n2006, Vol. 12(12) .  \n[2] Kaufman, L. and Rousseeuw, P. J.  Finding Groups \nin Data: An Introduction to Cluster Analysis, \nWiley, 1990. \n[3] Li, T. and Ogihara, M. “Detecting emotion in \nmusic”, Proceedings of the 4th International. \nConference on Music Information Retrieval  ISMIR \n2003 , Washington, D.C."
    },
    {
        "title": "Exploring Mood Metadata: Relationships with Genre, Artist and Usage Metadata.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415126",
        "url": "https://doi.org/10.5281/zenodo.1415126",
        "ee": "https://zenodo.org/records/1415126/files/HuD07.pdf",
        "abstract": "There is a growing interest in developing and then evaluating Music Information Retrieval (MIR) systems that can provide automated access to the mood dimension of music. Mood as a music access feature, however, is not well understood in that the terms used to describe it are not standardized and their application can be highly idiosyncratic. To better understand how we might develop methods for comprehensively developing and formally evaluating useful automated mood access techniques, we explore the relationships that mood has with genre, artist and usage metadata. Statistical analyses of term interactions across three metadata collections (AllMusicGuide.com, epinions.com and Last.fm) reveal important consistencies within the genre-mood and artist-mood relationships. These consistencies lead us to recommend a cluster-based approach that overcomes specific term-related problems by creating a relatively small set of data-derived “mood spaces” that could form the ground-truth for a proposed MIREX “Automated Mood Classification” task. 1",
        "zenodo_id": 1415126,
        "dblp_key": "conf/ismir/HuD07",
        "keywords": [
            "Music Information Retrieval",
            "Automated access",
            "Mood dimension",
            "Standardized terms",
            "Idiosyncratic application",
            "Statistical analyses",
            "Genre-mood relationship",
            "Artist-mood relationship",
            "Cluster-based approach",
            "MIREX task"
        ],
        "content": "EXPLORING MOOD METADATA: RELATIONSHIPS W ITH GENRE, \nARTIST AND USAGE METADATA \nXiao Hu J. Stephen Downie \nInternational Music Information Retrieval Systems E valuation Laboratory \nThe Graduate School of Library and Information Scie nce  \nUniversity of Illinois at Urbana-Champaign \n{xiaohu, jdownie}@uiuc.edu  \nABSTRACT \nThere is a growing interest in developing and then \nevaluating Music Information Retrieval (MIR) system s \nthat can provide automated access to the mood \ndimension of music. Mood as a music access feature,  \nhowever, is not well understood in that the terms u sed to \ndescribe it are not standardized and their applicat ion can \nbe highly idiosyncratic. To better understand how w e \nmight develop methods for comprehensively developin g \nand formally evaluating useful automated mood acces s \ntechniques, we explore the relationships that mood has \nwith genre, artist and usage metadata. Statistical analyses \nof term interactions across three metadata collecti ons \n(AllMusicGuide.com , epinions.com  and Last.fm ) \nreveal important consistencies within the genre-moo d \nand artist-mood relationships. These consistencies lead \nus to recommend a cluster-based approach that \novercomes specific term-related problems by creatin g a \nrelatively small set of data-derived “mood spaces” that \ncould form the ground-truth for a proposed MIREX \n“Automated Mood Classification” task. \n1 INTRODUCTION \n1.1 Music Moods and MIR Development \nIn music psychology and education, the emotional \ncomponent of music has been recognized as the most \nstrongly associated with music expressivity [6]. Mu sic \ninformation behaviour studies (e.g., [10]) have als o \nidentified music mood as an important criterion use d by \npeople in music seeking and organization. Several \nexperiments have been conducted to classify music b y \nmood (e.g., [7][8][9]). However, a consistent and \ncomprehensive understanding of the implications, \nopportunities and impacts of music mood as both \nmetadata and content-based access points still elud es the \nMIR community. Since mood is a very subjective noti on, \nthere has yet to emerge a generally accepted mood \ntaxonomy that is used within the MIR research and \ndevelopment community. For example, each of \naforementioned studies used different mood categori es, \nmaking meaningful comparisons between them difficul t.  \nNotwithstanding that there is a growing interest in  \ntackling mood issues in the MIR community--as evidenced by the ongoing discussions to establish a  \n“Audio Mood Classification” (AMC) task at the Music  \nInformation Retrieval Evaluation eXchange (MIREX) 1 \n[3], this lack of common understanding is inhibitin g \nprogress in developing and evaluating mood-related \naccess mechanisms. In fact, it was the MIREX \ndiscussions that inspired this study. Thus, this pa per is \nintended to contribute our general understanding of  \nmusic mood issues by formally exploring the \nrelationships between: 1) mood and genre; 2) mood a nd \nartist; and, 3) mood and recommended usage (see \nbelow). It is also intended to contribute more \nspecifically to the MIREX community by providing \nrecommendations on how to proceed in constructing a  \npossible method for conducting an “AMC” task. \nOur primary dataset is derived from metadata found \nwithin the AllMusicGuide.com  (AMG) site, a popular \nmusic database that provides professional reviews a nd \nmetadata for albums, songs and artists. Secondary d ata \nsets were derived from epinions.com  and Last.fm , \nthemselves both popular music information services.  \nThe fact that real world users engage with these se rvices \nallows us to ground our analyses and conclusions wi thin \nrealistic social contexts of music seeking and \nconsumption.  \nIn a previous study [5], we examined a relatively \nnovel music metadata type: “recommended usage”. We \nexplored the relationships between usages and genre s as \nwell as usages and artists using a set of 11 user \nrecommended usages provided by epinons.com , a \nwebsite specializing in product reviews written by \ncustomers. Because both music moods and usages \ninvolve subjective reflections on music, they can v ary \ngreatly both among, and within, individuals. It is \ntherefore interesting to see whether there is any s table \nrelationship between these two metadata types. We \nexplore this question by examining the set of album s \ncommon to the AMG mood dataset and our \nepinions.com usage dataset [5].  \nThe rest of the paper is organized as follows: Sect ion \n2 describes how we derived the mood categories used  in \nthe analyses. Sampling and testing method is descri bed \nin Section 3.  Sections 4 to 6 report analyses of the \nrelationships between mood and genre, artist and us age \nrespectively. In Section 7, the results from Sectio ns 4-6 \n                                                           \n1 http://music-ir.org/mirexwiki  © 2007 Austrian Computer Society (OCG). \n   \n \n undergo a corroboration analysis using an independe nt \ndataset from Last.fm . Section 8 concludes the paper \nand provides recommendations for a possible MIREX \n“Audio Mood Classification” task. \n2 MOOD CATEGORIES \n2.1 Mood Labels on AMG \nAMG claims to be “the most comprehensive music \nreference source on the planet” 1 and supports access to \nmusic information by mood label. There are 179 mood  \nlabels in AMG where moods are defined as “adjective s \nthat describe the sound and feel of a song, album, or \noverall body of work” 2and include such terms as \n“happy”, “sad”, “aggressive”, “stylish”, “cheerful” , etc. \nThese mood labels are created and assigned to music  \nworks by professional editors. Each mood label has its \nown list of representative “Top Albums” and its own  list \nof “Top Songs”. The distribution of albums and song s \nacross these mood lists is very uneven. Some moods are \nassociated with more than 100 albums and songs whil e \nothers have as few as 3 albums or songs. This creat es a \ndata sparseness problem when analysing all 179 mood  \nlabels. To alleviate this problem, we designed thre e \nalternative AMG datasets:  \n1. Whole Set : Comprises the entire 179 AMG mood \nlabel set. Its “Top Album” lists include 7134 album -\nmood pairs. Its “Top Song” lists include 8288 song-\nmood pairs. \n2. Popular Set : Comprises those moods associated with \nmore than 50 albums and 50 songs. This resulted in \n40 mood labels and 2748 album-mood and 3260 \nsong-mood pairs. \n3. Cluster Set : Many albums and songs appear in \nmultiple mood label lists. This overlap can be \nexploited to group similar mood labels into several  \nmood clusters. Clustering condenses the data \ndistribution and gives us a more concise, higher-\nlevel view of the mood “space”. The set of albums \nand songs assigned to the mood labels in the mood \nclusters forms our third dataset (described below).   \n2.2  Mood Clustering on Top Albums and Top Songs \nIn order to obtain robust and more meaningful clust ering \nresults, it is advantageous to use more than one vi ew of \nthe available data. The AMG dataset provides two vi ews: \n“Top Albums” and “Top Songs”. Thus, we performed \nthe following clustering methods independently on b oth \nthe “Top Albums” and the “Top Songs” mood list data  \nof the Popular Set . \n First, a co-occurrence matrix was formed such that  \neach cell of the matrix was the number of albums (o r \nsongs) shared by two of the 40 “popular” mood label s \nspecified by the coordinates of the cell. Pearson’s  \ncorrelation was calculated for each pair of rows (o r \n                                                           \n1AllMusicGuide.com : “About Us”.  \n2AllMusicGuide.com : “Site Glossary”.  columns) as the similarity measure between each pai r of \nmood labels. Second, an agglomerative hierarchical \nclustering procedure using Ward’s criterion [1] was  \napplied to the similarity data. Third, the resultan t two \ncluster sets (derived from album-mood and song-mood  \npairs respectively) were examined and found to have  29 \nmood labels out of the original 40 that were consis tently \ngrouped into 5 clusters at a similar distance level . Table \n1 presents the resultant 5 mood clusters along with  their \nconstituent mood terms ranked by the number of \nassociated albums. \nCluster1  Cluster2  Cluster3  Cluster4  Cluster5  \nRowdy  Literate Witty Volatile \nRousing  Amiable/ \nGood natured  Wistful Humorous  Fiery \nConfident  Sweet  Bittersweet  Whimsical  Visceral \nBoisterous  Fun Autumnal  Wry Aggressive  \nPassionate  Rollicking  Brooding  Campy Tense/anxious  \n Cheerful Poignant  Quirky Intense \n   Silly  \nTable 1. Popular Set  mood label clustering results \n Note the high level of synonymy within each cluste r \nand the low level of synonymy across the clusters. This \nstate of affairs suggests that the clusters are bot h \nreasonable and potentially useful. The high level o f \nsynonymy found within each cluster helps to define and \nclarify the nature of the mood being captured bette r than \na single term label could (i.e., lessens ambiguity) . For \nthis reason, we are NOT going to assign a term labe l to \nany of these clusters in order to stress that the “ mood \nspaces” associated with each cluster is really the \naggregation of the mood terms represented within ea ch \ncolumn. \n3 SAMPLING AND TESTING METHOD \nIn each of the following sections, we analyse the \nrelationship of mood to genre, artist and usage usi ng our \nthree datasets. We focus on the “Top Album” lists f rom \neach of these sets rather than their “Top Song” lis ts \nbecause the album is the unit of analysis on \nepinions.com  to which we will turn in Section 6 when \nlooking at usage-mood interactions. \nAt the heads of Sections 4-6, you will find \ninformation about the specific (and slightly varyin g) \nsampling methods used for each of the relationships  \nexplored. In general, the procedure is one of gathe ring \nup the albums associated with a set of mood labels and \ntheir genre, artist or usage information and then \ncounting the number of [genre|artist|usage]-mood la bel \npairs that occur for each album. The overall sample  \nspace is the total number of [genre|artist|usage]-m ood \nlabel pairs across all relevant albums. \nTo test for significant [genre|artist|usage]-mood l abel \npairs, we chose the Fisher’s Exact Test (FET) [2]. FET \nis used to examine the significance of the \nassociation/dependency between two variables (in ou r \ncase [genre|artist|usage]-mood), regardless of whet her \nthe sample sizes are small, or the data are very \nunequally distributed. All of our significance test s were \nperformed using FET.   \n \n 4 MUSIC MOODS AND GENRES \nEach album in each individual “Top Album” list is \nassociated with only one genre label. However, an \nalbum can be assigned to multiple “Top Album” mood \nlists. Thus, our genre-mood sample space is all exi sting \ncombinations of genre and mood labels with each \nsample being the pairing of one genre and one mood \nlabel.  \n4.1 All Moods and Genres \nThere are 3903 unique albums in 22 genres in the Whole \nSet . This set contains 7134 genre-mood pairs, but thei r \ndistribution across the 22 genres is very skewed wi th \n4564 of them involving the “Rock” genre. In order t o \ncompensate for this “Rock” bias, we conducted our \nassociation tests on the whole dataset as well as o n a \ndataset excluding Rock albums. Table 2 shows the ba sic \nstatistics of the two datasets. The mood labels “Hu ngry”, \n“Snide” and “Sugary” were exclusively involved with  \n“Rock” which resulted in a “non-Rock” mood set of 1 76 \nlabels. \n Samples  Moods  Genres  Unique Albums  \n+Rock  7134 179 22 3903 \n- Rock  2570 176 21 1715 \nTable 2.  Whole Set  counts (+/- Rock genre)  \nThe FET results on the Whole Set  with “Rock” \nalbums gives 262 genre-mood pairs whose association s \nare significant at p < 0.05. Analysis of the “non-Rock” \nsubset yielded 205 significant genre-mood pairs. 17 0 of \nthese pairs are significant in both subsets and inv olve 17 \ngenres. Table 3 presents these 17 genres and the to p-\nranked (by frequency) associated moods.  \nGenre  Mood #  Genre Mood #  \nR & B  Sensual 51  Folk Earnest 8  \nRap Street Smart  29  Latin Spicy 5  \nJazz Fiery 28  World Hypnotic  4 \nElectronica  Hypnotic 20  Reggae Outraged  3 \nBlues Gritty 16  Soundtrack  Atmospheric   3 \nVocal Sentimental  15  Easy Listening  Soothing   2 \nCountry  Sentimental  15  New Age Soothing  2 \nGospel  Spiritual 11  Avant-Garde  Cold 3  \nComedy  Silly 8     \nTable 3.  Whole Set  top-ranked genre-mood pairs \nWhile it is interesting to note the reasonableness of \nthese significant pairings, it is more important to  note \nthat each genre is associated with 10 significant m oods \non average and that the mood labels cut across the genre \ncategories. This is strong evidence that genre and mood \nare independent of each other and that both provide  \ndifferent modes of access to music items. \n4.2 Popular Moods and Genres  \nThe 40 mood labels in the Popular Set  involve 2748 \ngenre-mood pairs. Again, many of the pairs are in t he \n“Rock” genre, and thus we performed FET on both set s \nwith and without “Rock”. Table 4 presents the stati stics \nof the two sets. There are 70 genre-mood pairs with  significant relations at p < 0.05 in the “with Rock” set \nand 54 pairs in the “non-Rock” set. 41 pairs involv ing \n16 genres are significant in both sets. Table 5 pre sents \nthe top (by frequency) 16 genre-mood pairs.  \n Samples  Moods  Genres  Unique albums  \n+ Rock  2748 40 21 1900 \n- Rock  927 40 20 714 \nTable 4. Popular Set  counts (+/- Rock genre) \nGenre  Mood #  Genre Mood #  \nR & B  Sensual 51  Electronica  Fun 6  \nJazz Fiery 28  Gospel Joyous 5  \nVocal  Sentimental  15  Latin Rousing  5 \nCountry  Sentimental  15  Soundtrack  Theatrical  3 \nRap Witty 14  Reggae Druggy  3 \nComedy  Silly 8  World Confident  2 \nBlues  Rollicking  8 Easy Listening  Fun 2  \nFolk Wistful 8  Avant-Garde  Volatile  2 \nTable 5. Popular Set  top-ranked genre-mood pairs \nBecause of the exclusion of less popular moods, \nsome genres are shown to be significantly related t o \ndifferent moods than those presented in Table 3 (e. g., \n“Blues”, “Electronic”, “Rap”, “Gospel”, etc.). Note  that \nthese term changes are not contradictory but rather  are \nsuggestive of an added dimension to describing a mo re \ngeneral “mood space”. For example, in the case of \n“Folk” the two significant mood terms are “Earnest”  and \n“Wistful”. Similarly, the combination of “Joyous” a nd \n“Spiritual” mood terms better describes “Gospel” th an \neither term alone. See also “Latin” (“Spicy”, “Rous ing”) \nand “Reggae” (“Outraged”, “Druggy”).  \n4.3 Mood Clusters and Genres \nIn the Cluster Set , there are 1991 genre-mood cluster \ncombinations, covering 20 genres. Among them, \n“Rock” albums again occupy a large portion of sampl es, \nand thus we made an additional “non-Rock” subset \n(Table 6). The FET significant results (at p < 0.05) on \nthe “with Rock” set contain 20 genre-mood pairs and  \nthose on the “non-Rock” set contain 15 pairs. “Rock ” \nwas significantly related to Cluster 4 and 5 at p < 0.001. \nThe 14 pairs significant in both sets are shown in Table \n7.  \n Samples  Clusters  Genres  Unique Albums  \n+Rock  1991 5 20 1446 \n- Rock  619 5 19 507 \nTable 6.  Cluster Set  counts (+/- Rock genre) \nGenre  Mood  # Genre Mood  # \nR & B  Cluster1  71  Vocal Cluster3  18  \nJazz Cluster5  57  Vocal Cluster2  17  \nRap Cluster4  32  Comedy Cluster4  12  \nRap Cluster5  30  Latin Cluster1  7 \nFolk Cluster3  28  World Cluster1  6 \nCountry  Cluster3  24  Avant-Garde Cluster5  4 \nBlues  Cluster1  20  Easy Listening  Cluster2  4 \nTable 7. Cluster Set  top-ranked genre-mood pairs    \n \n It is noteworthy that “R&B” and “Blues” are both \nassociated with Cluster1 which might reflect their \ncommon heritage. Similarly, “Country” and “Folk” ar e \nboth associated with Cluster3. \n5 MUSIC MOODS AND ARTISTS \nEach album on AMG has a “Title” and an “Artist” fie ld. \nFor albums combining tracks by multiple artists, th e \n“Artist” field is filled with “Various Artists”. In  the \nfollowing analyses, we eliminated “Various Artists”  as \nthis label does not signify a unique analytic unit.    \n5.1 All Moods and Artists \nThere are 2091 unique artists in our Whole Set . Some \nartists contribute as many as over 30 artist-mood p airs \neach while 871 artists only occur once in the datas et and \nthus each of them only relates to one mood. We limi ted \nthis analysis to artists who have at least 10 artis t-mood \npairs, which gave us 142 artists, 175 mood labels a nd \n2241 artist-mood pairs. There are 623 significant a rtist-\nmood pairs at p < 0.05. Table 8 presents the top 14 (by \nfrequency) pair associations. Those familiar with t hese \nartists will find these results reasonable.  \nArtist Mood  Artist Mood \nDavid Bowie  Theatrical  The Grateful Dead  Trippy \nWire Fractured  The Small Faces  Whimsical \nWire Cold Randy Newman  Cynical/Sarcastic  \nT. Rex Campy  Randy Newman  Literate \nThe Beatles  Whimsical  Miles Davis Uncompromising  \nThe Kinks  Witty Thelonious Monk  Quirky \nBrian Eno  Detached  Talking Heads  Literate \nTable 8. Whole Set  top  significant artist-mood pairs  \n5.2 Popular Moods and Artists \nThe Popular Set  contains 1142 unique artists. 29 of \nthem appear in at least 9 artist-mood pairs, and to gether \ncontribute 372 artist-mood pairs that form the test ing \nsample space. The results contain 68 significantly \nassociated artist-mood pairs at p < 0.05. Table 9 \npresents the top 16 (by frequency) pair association s. \nArtist Mood  Artist Mood \nDavid Bowie  Theatrical  The Small Faces  Whimsical \nDavid Bowie  Campy  The Small Faces  Trippy \nTalking Heads  Wry Randy Newman  Literate \nTalking Heads  Literate  Randy Newman  Cynical/Sarcastic  \nThe Beatles  Whimsical  Hüsker Dü Fiery \nThe Beatles  Trippy  \nElton John  Wistful  The Jesus & Mary \nChain Tense/Anxious  \nT. Rex Campy  \nThe_Kinks  Witty The Velvet \nUnderground Literate \n Table 9. Popular Set  top significant artist-mood pairs \nLike we discussed in Section 4.2, it is important t o \nnote in Tables 8 and 9 the application of multiple \nsignificant terms to individual artists. For exampl e, \nRandy Newman is associated with “Cynical/Sarcastic”  \nand “Literate” and Wire is associated with “Fractured” \nand “Cold”. Again, we see that it is the “sum” of t hese mood terms that evokes a more robust sense of the \ngeneral mood evoked by these artists.  \n5.3 Mood Clusters and Artists \nThe Cluster Set  contains albums by 920 unique artists. \nAmong them, 24 artists who have no less than 8 arti st-\nmood pairs form a testing space of 248 artist-mood pairs. \nTable 10 presents the 17 significant artist-mood cl uster \nassociations at p < 0.05.  \nArtist Mood  # Artist Mood  # \nThe Kinks  Cluster4  13  Miles Davis Cluster5  7 \nHüsker Dü  Cluster5  12  Leonard Cohen  Cluster3  7 \nXTC Cluster4  9 Paul Simon Cluster3  7 \nBob Dylan  Cluster3  9 \nElvis Presley  Cluster1  8 John Coltrane w/ \nJohnny Hartma  Cluster3  6 \nElton John  Cluster3  8 David Bowie  Cluster4  6 \nHarry Nilsson  Cluster4  8 The Beatles Cluster2  4 \nThe Who Cluster5  8 The Beach Boys  Cluster2  4 \nX Cluster5  7 Nick_Lowe Cluster2  4 \nTable 10.  Cluster Set  significant artist-mood pairs \nThe associations presented in Table 10 are again \nquite reasonable. For example, The Beatles  and The \nBeach Boys  are both related to Cluster2. The four artists \nrelated to Cluster5 are all famous for their \n“uncompromising” styles. It is noteworthy that Clus ter5 \nmembers represent both the “Rock” (e.g., Hüsker Dü ) \nand “Jazz” (Miles Davis) genres further indicating the \nindependence of genre and mood to describe music. \nSimilarly, Cluster3’s members of John, Cohen, Coltr ane, \nand Simon also cut across genres. \n6 MUSIC MOODS AND USAGES \nIn each of the user-generated reviews of music CDs \npresented on epinions.com , there is a field called \n“Great Music to Play While” where the reviewer sele cts \na usage suggestion for the reviewed piece from a re ady-\nmade list of recommended usages prepared by the \neditors. Each album (CD) can have multiple reviews but \neach review can be associated with at most one \nrecommended usage. Hu et al. [5] identified interes ting \nrelations between the recommended usage labels and \nmusic genres and artists as well as relations among  the \nusages themselves. In this section, we explore poss ible \nrelations between mood and usage. The following \nusage-mood analyses are based on intersections betw een \nour three AMG datasets and our earlier epinions.com  \ndataset which contains 2800 unique albums and 5691 \nalbum-usage combinations [5].  \n6.1 All Moods and Usages \nBy matching the title and artist name of each album  in \nour Whole Set  and the epinions.com  dataset, 149 \nalbums were found common to both sets. As each albu m \nmay have more than one mood label and more than one  \nusage label, we count each combination of existing \nmood and usage labels of each album as one usage-\nmood sample. There were 1440 usage-mood samples \ninvolving 140 mood labels. 64 significant usage-moo d \npairs are identified by FET at p < 0.05. Table 11   \n \n presents the most frequent usage-mood associations for \neach of the 11 usage categories  1. \nUsage  Mood # Artist Mood  # \nGo to sleep  Bittersweet 12  Hang w/friends  Fierce  5 \nDriving Menacing 11  Waking up  Cathartic  4 \nListening  Epic 9  Exercising  Angry  4 \nReading  Provocative  7 At work Menacing  3 \nGo out Party/Celebratory  5 House clean  Carefree  2 \nRomancing  Delicate 5     \nTable 11. Whole Set  top significant usage-mood pairs \n6.2 Popular Moods and Usages \nThere are 84 common albums in the Popular Set  and the \nepinions.com  dataset, which yields 527 usage-mood \npairs. There are 16 pairs with 7 usages identified as \nsignificant at p < 0.05. Table 12 presents the most \nfrequent usage-mood associations for each of the us age \ncategories . \nUsage Mood #  Artist Mood  # \nGo to sleep  Bittersweet  12  Go out Fun 5  \nDriving Visceral  7 Exercising  Volatile  3 \nListening  Theatrical  7 House clean  Sexy 2  \nRomancing  Sensual  5    \nTable 12.  Popular Set  top significant usage-mood pairs  \n6.3 Mood Clusters and Usages \nThere are 66 albums included in both the Cluster Set  \nand the epinions.com  dataset, yielding 358 usage-\nmood pairs. Table 13 presents the 6 significant pai rs ( p \n< 0.05). \nUsage Mood  # Usage Mood  # \nGo to sleep  Cluster3  44  Romancing  Cluster3  17  \nDriving Cluster5  20  Exercising Cluster5  13  \nHang w/friends  Cluster4  19  Go out Cluster2  6 \nTable 13. Cluster Set  significant usage-mood pairs \nThe usage-mood relationship appears to be much less  \nstable than the genre-mood and artist-mood \nrelationships.  Only 6 of the 11 usages have signif icant \ncluster relationships. We believe this instability is a \nresult of the specific terms and phrases used to de note \nthe usage activities (also see Section 7.3).   \n7 EXTERNAL CORROBORATION  \nIt is always desirable to analyse multiple independ ent \ndata sources whenever conducting analyses of \nrelationships.  In this section we take our relatio nship \nfindings from Sections 4-6 and attempt to re-find t hem \nusing sets of data from Last.fm . Note that we are only \nlooking for corroboration, not definite “proof” whe ther \nthe AMG findings are “true” or “false”. That is, we  are \nexploring the Last.fm  data sets to see whether, or not, \nour approach is sound and whether it merits further  \ndevelopment.  \n                                                           \n1 Usage labels modified for space reasons. See [5] fo r original labels.  Last.fm  is a website collecting music related \ninformation from the general public, including play lists, \nand variety of tags associated with albums, tracks and \nartists, etc. The Last.fm  tag set includes genre-related, \nmood-related and sometimes usage-related tags that can \nbe used to analyse genre-mood, artist-mood and usag e-\nmood relationships. \n7.1 Corroboration of Mood and Genre Associations \nLast.fm  provides webservices 2 through which the \ngeneral public can obtain lists of “Top Tracks”, “T op \nAlbums” and “Top Artists” for each user tag. As we are \ninterested in corroborating the significance of the  genre-\nmood pairs uncovered in the AMG datasets, we \nobtained the 3 Last.fm  “top lists” for tags named by \nthe genre-mood pairs shown in Tables 3 and 5. From \nthese lists, we constructed three sample sets by \ncollecting albums, tracks and artists with at least  one \ngenre tag and one mood tag. The three sample sets \npresent three different “views” with regard to the \nassociations between genre and mood. A FET was \nperformed on each of the three sample sets. 21 of t he 28 \nsignificant pairs presented in Tables 3 and 5 are a lso \nsignificantly associated in at least one of the Last.fm  \nsample sets ( p < 0.05). The 7 non-corroborated pairs are: \n“Electronica”–“Fun”, “Latin”–“Rousing”, “Reggae”–\n“Druggy”, “Reggae”–“Outraged”, “Jazz”–“Fiery”, \n“Rap”–“Street  Smart”, and “World”–“Hypnotic”. \nThe same method was applied to the corroboration of  \ngenre-mood cluster pairs. 12 of the 14 pairs in Tab le 7 \ntested to be significantly associated at  p < 0.05. The 2 \nnon-corroborated pairs are: “Jazz”–Cluster5 and  \n“Latin”–Cluster1.  \n7.2  Corroboration of Mood and Artist Associations \nLast.fm  provides a “Top Artists” list for each user tag \nand a “Top Tags” list for each artist in its system . We \nretrieved the “Top Artists” list for each of the mo od \nlabels in Table 8 and 9, as well as the “Top Tags” list \nfor each of the artists. 17 of the 22 artist-mood p airs in \nTables 8 and 9 were corroborated either by successf ully \nidentifying the artists in the “Top Artists” lists of the \ncorresponding tags (10 pairs) or by identifying the  tags \nin the “Top Tags” lists of the corresponding artist s (7 \npairs). The 5 non-corroborated artist-mood pairs in clude: \nThe Beatles –“Whimsical”, The Grateful Dead –“Trippy”, \nMiles Davis–“Uncompromising”, Thelonious Monk –\n“Quirky”, and David Bowie–“Campy”. \nTo corroborate artist-mood cluster pairs, we \ncombined the “Top artists” lists of all the mood la bels in \neach cluster. By the same method, 15 of the 17 pair s in \nTable 10 (except for Miles Davis–Cluster5 and John \nColtrane with Johnny Hartma–Cluster3) were \ncorroborated. \n7.3 Corroboration of Mood and Usage Associations \nUsing the same method as in Section 7.1, we built t hree \nsample sets based on top albums, tracks and artists  with \n                                                           \n2 http://www.audioscrobbler.net/data/webservices    \n \n at least one usage tag and one mood tag that appear ed in \nTables 11 and 12. Please note that some of the usag e \ntags are not available in Last.fm  such as “Hanging out \nwith friends”, and “Romancing”. Others have very fe w \noccurrences, such as “Cleaning the house”. We tried  to \nlocate tags similar to these phrases (e.g., “hangin g out”, \n“cleaning”). Thus, results from this dataset disclo se \nquite different associations than those from the AM G \nsets. The only 3 pairs corroborated are ( p < 0.01): \n“Going to sleep”–“Bittersweet”, “Driving”–“Menacing ”, \nand “Listening”–“Epic”.  \nBy combining the albums/tracks/artists lists with a ll \nthe mood labels in each cluster, we corroborated on ly 2 \nusage-mood cluster pairs found in Table 13: “Going to \nsleep”–Cluster3 ( p = 0.001), “Driving”–Cluster5 ( p < \n0.015). Again, these observations indicate that the  \nrelationship between usage and mood is not stable a nd is \nmost likely dependent on the specific vocabularies \npresent in the datasets they are derived from.  \n8 RECOMMENDATIONS  \nThe usage-mood relationships are not stable enough to \nwarrant further consideration. However, the genre-m ood \nand artist-mood relationships explored in this stud y \nshow great promise in helping construct a meaningfu l \nMIREX “AMC” task. The corroborative analyses using \nthe Last.fm  data sets provide additional evidence that \nthe nature of these two relationships is generalize able \nbeyond our original AMG data source.   \nMood term vocabulary size (and its uneven \ndistribution across items) is a huge impediment to the \nconstruction of useable ground-truth sets (e.g., AM G’s \n179 mood terms). Throughout this study we saw that \nmany of the individual mood terms were highly \nsynonymous or described aspects of the same \nunderlying, more general, “mood space”. Thus, we \nfound that decreasing mood vocabulary size in some \nways actually clarified the underlying mood of the items \nbeing described. We therefore recommend that MIREX \nmembers consider constructing an “AMC” task based \nupon a set of “mood space” clusters rather than \nindividual mood terms. The clusters themselves need  \nnot be those presented here but should be relativel y \nsmall in number. As Table 14 shows, a cluster-based  \napproach also improves the distribution of albums a nd \nartists in AMG across the clusters. \n Cluster1  Cluster2  Cluster3  Cluster4  Cluster5  \nAlbums  355 285 486 493 372 \nArtist  14 16 85 87 46 \nTable 14. AMG sample distributions across mood clusters \nUnder a fully automated scenario (i.e., no human \nevaluation), ground-truth sets could be constructed  by \nlocating those works, across both artists and genre s, \nwhich are represented in each cluster by mapping th e \nconstituent mood terms back to those artists and ge nres \nto which they have statistically significant relati onships.  \nUnder a human evaluation scenario (e.g. [4]), train ing \nsets would be similarly constructed. However, for evaluation itself, the human evaluators would be gi ven \nexemplars from each of the 5 (or so) clusters to gi ve \nthem an understanding of their “nature”. The limite d \nnumber of clusters increases the probability of eva luator \nconsistency. Scoring would be based on the agreemen t \nbetween system and evaluator assigned cluster \nmemberships. \n9 ACKNOWLEDGEMENTS \nThis project is funded by the National Science \nFoundation (IIS-0327371) and the Andrew W. Mellon \nFoundation.  \n10 REFERENCES \n[1] Berkhin, P. Survey of Clustering Data Mining \nTechniques,  Accrue Software, 2002. \n[2] Buntinas, M. and Funk, G. M.  Statistics for the \nSciences , Brooks/Cole/Duxbury, 2005.  \n[3] Downie, J. S. “The Music Information Retrieval \nEvaluation eXchange (MIREX)”, D-Lib Magazine \n2006, Vol. 12(12) .   \n[4] Gruzd, A. A., Downie J. S., Jones, M. C. and Le e, \nJ. H. “Evalutron 6000: collecting music relevance \njudgments”, Proceedings of the Joint Conference \non Digital Libraries (JCDL 2007) .  \n[5] Hu, X., Downie, J. S. and Ehmann, A. F. \n“Exploiting recommended usage metadata: \nExploratory analyses”, Proceedings of the 7th \nInternational  Conference on Music Information \nRetrieval , ISMIR'06, Victoria, Canada. \n[6] Juslin, P.N., Karlsson, J., Lindström E., Fribe rg, A. \nand Schoonderwaldt, E. “Play it again with feeling:  \ncomputer feedback in musical communication of \nemotions”, Journal of Experimental Psychology: \nApplied 2006, Vol.12(1) . \n[7] Li, T. and Ogihara, M. “Detecting emotion in \nmusic”, Proceedings of the 4th International. \nConference on Music Information Retrieval  ISMIR \n2003 , Washington, D.C. \n[8] Lu, L., Liu, D. and Zhang, H. “Automatic Mood \nDetection and Tracking of Music Audio Signals”, \nIEEE  Transaction on Audio, Speech, and \nLanguage Processing 2006,  Vol.14(1) . \n[9] Mandel, M. Poliner, G. and Ellis, D. “Support \nvector  machine  active  learning  for  music retrieval”,  \nMultimedia  Systems  2006, Vol.12(1) . \n[10]  Vignoli, F. “Digital Music Interaction concep ts: a \nuser study”, Proceedings of the 5th Int. Conference \non Music Information Retrieval, ISMIR 2004 , \nBarcelona, Spain."
    },
    {
        "title": "Performance of Philips Audio Fingerprinting under Desynchronisation.",
        "author": [
            "Neil J. Hurley",
            "Félix Balado",
            "Elizabeth P. McCarthy",
            "Guenole C. M. Silvestre"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416068",
        "url": "https://doi.org/10.5281/zenodo.1416068",
        "ee": "https://zenodo.org/records/1416068/files/HurleyBMS07.pdf",
        "abstract": "An audio fingerprint is a compact representation (robust hash) of an audio signal which is linked to its perceptual content. Perceptually equivalent instances of the signal must lead to the same hash value. Fingerprinting finds application in efficient indexing of music databases. We present a theoretical analysis of the Philips audio fingerprinting method under desynchronisation for correlated stationary Gaussian sources. 1 INTRODUCTION There are inevitable trade-offs between the size and the properties of a robust hash or fingerprint. An audio fingerprinting scheme which has proved to be remarkably robust is the Philips method, proposed by Haitsma et al [1] based on quantizing differences of energy measures from overlapped short-term power spectra. In this paper we examine the theoretical performance of the Philips method under desychonisation through a statistical model. This approach allows the influence of the system parameters to be studied and optimization strategies to minimize the probability of bit error of the hash to be tackled. Some previous work has tackled performance analysis of the Philips method through a statistical model. For example a model was proposed by Doets and Lagendijk [2], for the case in which the signal to be hashed is uncorrelated Gaussian noise. This was used to evaluate the performance of the fingerprinting method under distortion, but the results only apply to i.i.d. sources. The important issue of performance analysis under desynchronization, which to our knowledge has not been previously tackled, constitutes the main contribution of this paper. 2 DESYNCHRONIZATION ERROR ANALYSIS In the Philips method, the length N input signal x is divided into overlapped frames before hashing. Let L be the number of samples in a single frame, ∆be the number of non-overlapping samples between two frames and xn be the input signal corresponding to the nth frame. We define the degree of overlap as θ ≜1 −∆/L, where θ ∈(0, 1), and higher θ corresponds to greater overlap. c⃝2007 Austrian Computer Society (OCG). A window w is applied to xn before computing the power spectrum. The spectrum is divided into 32 frequency bands on a logarithmic scale. Denoting by En(m) the energy of frequency band m for input frame xn, an unquantised hash value is given by Dn(m) ≜[En(m) − En(m + 1)] −[En−1(m) −En−1(m + 1)], with m = 0, 1, · · · , 31 and frames n = 0, 1, 2, · · ·. The variables Dn(m) completely determine the system, as the binary hash value Fn(m) ∈{0, 1} corresponding to frame n and band m is computed as Fn(m) ≜u (Dn(m)) , with u(·) the unit step function. This method can be expressed as a quadratic form on the extended vector ˜xn ≜(x[(n −1) · ∆+ 1], · · · , x[n · ∆+ L])T for n = 0, 1, 2, . . ., which includes all the components of the overlapping vectors xn and xn−1 and which is of length M ≜L + ∆, such that Dn(m) = ˜xT n Q(m) ˜xn, for a band and window dependent matrix Q(m), whose derivation is described in [3]. Desynchronization is the potential lack of alignment between the original framing used in the acquisition stage and the framing that takes place in the identification stage. The Philips algorithm has a high degree of overlapping in order to counteract desynchronization. Nevertheless, this strategy has a cost of generating a long hash sequence, which may be costly to store and compare. Consider a situation in which the signal fed to the system is desynchronized by k samples, with k ∈{−∆/2+1, · · · , ∆/2} and assuming ∆/2 integer for simplicity. It is sufficient to consider this range since a desynchronization of ∆just shifts all the fingerprint bits one position. A desynchronization by k samples results in a distorted hash value D′ n(m) and then a certain probability of bit error. It is convenient to write Dn(m) and D′ n(m) as quadratic forms in the same extended Gaussian vector xn ≜(x[(n −1)∆−∆/2 −1], · · · , x[n∆+ L + ∆/2])T , (1) of length M + ∆−1, which we assume is distributed as xn ∼N(0, Z). We write these quadratic forms as Dn(m) = xT n Q0(m) xn and D′ n(m) = xT n Qk(m) xn. Letting S ≜Dn(m) and V ≜D′ n(m), we note that stationarity implies that Z is Toeplitz and hence the mean of S and V is zero. Assume that S and V can be jointly modeled as a bivariate normal distribution centered at the origin with correlation coeficient ρ, which can be written as ρk(m) = tr \u0002 Z Q0(m) Z Qk(m) \u0003 tr [(Z Q(m))2] . (2) Defining ǫk n(m) ≜{F ′ n(m) ̸= Fn(m) | k} we have that Pr[ǫk n(m)] = 1 2(Pr[S > 0|V ≤0] + Pr[S ≤0|V > 0]) = 1 π arccos(ρk(m)) In order to average over k, we assume that k is uniformly distributed. An upper bound is based on assuming that ρk(m) ≥0 which holds as long as Pr \u0002 ǫk n(m) \u0003 ≤1/2. Hence, arccos(·) is a concave function and we may apply Jensen’s inequality [4] to upper bound the probability of bit error at frame n and band m as Pr[ǫn(m)] = E \u0014 1 π arccos(ρ(m)) \u0015 ≤1 π arccos(E[ρ(m)]). (3)",
        "zenodo_id": 1416068,
        "dblp_key": "conf/ismir/HurleyBMS07",
        "keywords": [
            "audio fingerprint",
            "robust hash",
            "perceptual content",
            "application in music databases",
            "Philips method",
            "statistical model",
            "desynchronization",
            "performance analysis",
            "trade-offs between size and properties",
            "system parameters"
        ],
        "content": "PERFORMANCE OF PHILIPSAUDIO FINGERPRINTINGUNDER\nDESYNCHRONISATION\nNeilJ. Hurley,F ´elixBalado, Elizabeth P. McCarthy,Gu ´enol´e C.M.Silvestre\nSchool ofComputerScience andInformatics\nUniversityCollegeDublin,Ireland\nABSTRACT\nAn audio ﬁngerprint is a compact representation (robust\nhash) of an audio signal which is linked to its perceptual\ncontent. Perceptually equivalent instances of the signal\nmust lead to the same hash value. Fingerprinting ﬁnds\napplication in efﬁcient indexing of music databases. We\npresent a theoretical analysis of the Philips audio ﬁnger-\nprinting method under desynchronisation for correlated\nstationaryGaussiansources.\n1 INTRODUCTION\nThere are inevitable trade-offs between the size and the\npropertiesofarobusthashorﬁngerprint. Anaudioﬁnger-\nprinting scheme which has proved to be remarkably ro-\nbust is the Philips method, proposed by Haitsma et al[1]\nbased on quantizingdifferencesof energymeasures from\noverlappedshort-termpowerspectra. Inthispaperweex-\namine the theoretical performance of the Philips method\nunder desychonisation through a statistical model. This\napproach allows the inﬂuence of the system parameters\nto be studied and optimization strategies to minimize the\nprobability of bit error of the hash to be tackled. Some\nprevious work has tackled performance analysis of the\nPhilipsmethodthroughastatisticalmodel. Forexamplea\nmodel was proposed by Doets and Lagendijk [2], for the\ncaseinwhichthesignaltobehashedisuncorrelatedGaus-\nsian noise. This was used to evaluate the performanceof\nthe ﬁngerprintingmethod underdistortion,but the results\nonly apply to i.i.d. sources. The important issue of per-\nformanceanalysis under desynchronization,which to our\nknowledgehasnotbeenpreviouslytackled,constitutesthe\nmaincontributionofthispaper.\n2 DESYNCHRONIZATIONERRORANALYSIS\nIn the Philips method, the length Ninput signal xis di-\nvidedintooverlappedframesbeforehashing. Let Lbethe\nnumberof samplesin a single frame, ∆be the numberof\nnon-overlappingsamples between two frames and xnbe\ntheinputsignalcorrespondingtothe nthframe. Wedeﬁne\nthe degreeof overlapas θ/defines1−∆/L,where θ∈(0,1),\nandhigher θcorrespondsto greateroverlap.\nc/circlecopyrt2007AustrianComputerSociety(OCG).A window wis applied to xnbefore computing the\npower spectrum. The spectrum is divided into 32fre-\nquencybandsonalogarithmicscale. Denotingby En(m)\nthe energy of frequency band mfor input frame xn, an\nunquantised hash value is given by Dn(m)/defines[En(m)−\nEn(m+ 1)]−[En−1(m)−En−1(m+ 1)], with m=\n0,1,···,31and frames n= 0,1,2,···. The variables\nDn(m)completely determine the system, as the binary\nhashvalue Fn(m)∈ {0,1}correspondingtoframe nand\nbandmis computed as Fn(m)/definesu(Dn(m)),withu(·)\nthe unit step function. This methodcan be expressedas a\nquadratic form on the extendedvector ˜ xn/defines(x[(n−1)·\n∆ + 1] ,···, x[n·∆ +L])Tforn= 0,1,2, . . ., which\nincludesallthecomponentsoftheoverlappingvectors xn\nandxn−1and which is of length M/definesL+ ∆, such that\nDn(m) =˜ xT\nnQ(m)˜ xn,for a band and window depen-\ndentmatrix Q(m), whosederivationisdescribedin [3].\nDesynchronization is the potential lack of alignment\nbetweentheoriginalframingusedintheacquisitionstage\nandtheframingthattakesplaceintheidentiﬁcationstage.\nThePhilipsalgorithmhasa highdegreeofoverlappingin\norder to counteract desynchronization. Nevertheless, thi s\nstrategy has a cost of generating a long hash sequence,\nwhich may be costly to store and compare. Consider a\nsituation in which the signal fed to the system is desyn-\nchronizedby ksamples,with k∈ {−∆/2+1,···,∆/2}\nand assuming ∆/2integer for simplicity. It is sufﬁcient\nto considerthisrangesince a desynchronizationof ∆just\nshifts all the ﬁngerprint bits one position. A desynchro-\nnization by ksamples results in a distorted hash value\nD′\nn(m)andthenacertainprobabilityofbiterror. Itiscon-\nvenienttowrite Dn(m)andD′\nn(m)asquadraticformsin\nthesame extendedGaussianvector\nxn/defines(x[(n−1)∆−∆/2−1],···, x[n∆ +L+ ∆/2])T,\n(1)\nof length M+ ∆−1, which we assume is distributed\nas xn∼ N(0,Z). We write these quadratic forms as\nDn(m) =xT\nnQ0(m)xnandD′\nn(m) =xT\nnQk(m)xn.\nLetting S/definesDn(m)andV/definesD′\nn(m), we note that\nstationarity implies that Zis Toeplitz and hence the mean\nofSandVis zero. Assume that SandVcan be jointly\nmodeledas a bivariate normaldistributioncenteredat the\norigin with correlation coeﬁcient ρ, which can be written\nas\nρk(m) =tr/bracketleftbig\nZQ0(m)ZQk(m)/bracketrightbig\ntr[(Z Q( m))2].(2)Deﬁning ǫk\nn(m)/defines{F′\nn(m)/ne}ationslash=Fn(m)|k}wehavethat\nPr[ǫk\nn(m)] =1\n2(Pr[S >0|V≤0] + Pr[ S≤0|V >0])\n=1\nπarccos( ρk(m))\nIn orderto averageover k, we assume that kis uniformly\ndistributed. An upper bound is based on assuming that\nρk(m)≥0which holds as long as Pr/bracketleftbig\nǫk\nn(m)/bracketrightbig\n≤1/2.\nHence, arccos( ·)is a concavefunctionandwe mayapply\nJensen’s inequality [4] to upper bound the probability of\nbiterrorat frame nandband mas\nPr[ǫn(m)] =E/bracketleftbigg1\nπarccos( ρ(m))/bracketrightbigg\n≤1\nπarccos( E[ρ(m)]).\n(3)\n2.1 OptimalWindow andAsymptoticPerformance\nNoticethat(2)impliesthat,evenfori.i.d.input,theboun d\n(3)ontheprobabilityofbiterrorisdependentonthewin-\ndoww(throughthe matrices Qi(m)) and onthe band m.\nIt is possible to minimize (3) with respect to the window\nwin the i.i.d. case. In [3], we show that the optimalwin-\ndowsatisﬁes a non-linearsystem of equationsthat can be\nsolved numericallyusing a generalised eigenvaluesolver.\nFurthermore,thisoptimisationcanbeexploitedtoobtaina\nclosed-formboundon Pesolelydependentontheoverlap\nlevelθ, thatholdsas L→ ∞andθ→1,namely,\nPe≤1\nπarccos/parenleftbiggsin((1 −θ)π)\n(1−θ)π/parenrightbigg\n.(4)\n3 EXPERIMENTAL RESULTS\nFirstly,weobtainresultsonzero-meanGaussiani.i.d. sig -\nnals. For a range of values of overlap θ,Peis averaged\nover all desynchronizationlevels kin the range −∆/2 +\n1, . . .,∆/2and over all bands. In Figure 1, this is illus-\ntrated both for the von Hann window and for a window\nobtained by averaging the band-dependent optimal win-\ndowsoverallbands. Theempiricalvaluesareobtainedby\naveragingover 2×105frames. Weseethatwith θ= 0.945\nandtheoptimizedwindowwecangetthesame Peaswith\nθ= 0.955and the von Hann window. This overlap de-\ncrease accounts for a reduction of approximately 20% in\nthe hash size. In anycase, these resultsshow that the von\nHann window is very close to optimal. In Figure 2, we\nalso apply our analysis to 5-second excerpts of three real\naudio signals used in [1]. We observe that the empirical\nresults are very similar to each other and very similar to\nthe i.i.d. Gaussian case. The performance of the i.i.d.\ncase acts as a natural upper boundfor desynchronization.\nThis bound is tight due to the weak dependence of the\nresults on the autocovariance matrix. Therefore, we can\nuse (4) to predict accurately performance for any signal,\nespeciallywhenframessizeshaverealistic (large)values .\nNotice that this expression has been obtained for the best\npossible window in the i.i.d. case, and for this reason the0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.9910−2Hann Window\nAverage of Band Optimal Windows\nTheoretical I\nTheoretical IIPe\nθ\nFigure1.Probabilityofbiterrorunderuniformdesynchroniza-\ntion versus overlap level, using an i.i.d. Gaussian hashed s ignal.\nEmpiricalresultscorrespondtothevonHannwindow andtoth e\naveraged band-optimal windows, respectively. Frame durat ion,\nTf≈0.3seconds. Theoretical I and II refer to two theoretical\nupper bounds applicable tothis situation(see [3]).\n0.80.820.840.860.88 0.90.920.940.960.98 110−310−210−1100\niidgauss\ntexas\nacdc\nofortuna\ntheoreticalPe\nθ\nFigure 2.Probability of bit error under uniform desynchro-\nnization versus overlap level, using 5-second excerpts of t hree\nreal audio signals and i.i.d. Gaussian signal. Frame durati on\nTf= 0.3seconds, von Hann window. The theoretical result is\nthe asymptotic performance for anoptimal window.\nplot lies below the i.i.d. Gaussian empiricalvalues which\ncorrespondtothevonHannwindow.\n4 REFERENCES\n[1] J. Haitsma, T. Kalker, and J. Oostven, “Robust audio hash -\ning for content identiﬁcation,” in Procs. of the International\nWorkshop onContent-BasedMultimediaIndexing , (Brescia,\nItaly), October 2001.\n[2] P.J. O. Doets, “Modelling a robust audio ﬁngerprinting s ys-\ntem,” inTechnical Report, Delft University of Technology ,\nJune 2004.\n[3] F. Balado, N. Hurley, E. McCarthy, and G. Silvestre, “Per -\nformance analysis of robust audio hashing,” IEEETrans. on\nInformation Forensics and Security , vol. 2, pp. 1556–6013,\nJune 2007.\n[4] T.M.CoverandJ.A.Thomas, ElementsofInformationThe-\nory. John Wiley& Sons, 1991."
    },
    {
        "title": "Localized Key Finding from Audio Using Nonnegative Matrix Factorization for Segmentation.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417197",
        "url": "https://doi.org/10.5281/zenodo.1417197",
        "ee": "https://zenodo.org/records/1417197/files/Izmirli07.pdf",
        "abstract": "A model for localized key finding from audio is proposed. Besides being able to estimate the key in which a piece starts, the model can also identify points of modulation and label multiple sections with their key names throughout a single piece. The front-end employs an adaptive tuning stage prior to spectral analysis and calculation of chroma features. The segmentation stage uses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Nonnegative matrix factorization with additional sparsity constraints and additive updates is used for segmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters.",
        "zenodo_id": 1417197,
        "dblp_key": "conf/ismir/Izmirli07",
        "keywords": [
            "adaptive tuning",
            "spectral analysis",
            "chroma features",
            "segmentation stage",
            "groups of contiguous chroma vectors",
            "nonnegative matrix factorization",
            "sparsity constraints",
            "additive updates",
            "candidate segments",
            "local keys"
        ],
        "content": "LOCALIZED KEY FINDING FROM AUDIO USING NON-\nNEGATIVE MATRIX FACTORIZATION FOR \nSEGMENTATION  \n \nÖzgür İzmirli  \n Center for Arts and Technology \nComputer Science \nConnecticut College \n  \nABSTRACT \nA model for localized key finding from audio is \nproposed. Besides being able to estimate the key in which a piece starts, the mode l can also identify points \nof modulation and label multiple sections with their key names throughout a single pi ece. The front-end employs \nan adaptive tuning stage prio r to spectral analysis and \ncalculation of chroma featur es. The segmentation stage \nuses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Non-negative matrix factorization with additional sparsity \nconstraints and additive updates is used for \nsegmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters. \n1. INTRODUCTION \nMusic being bought on digital media, aired through radio broadcasts, streamed or downloaded from Internet sites is almost exclusively in audio format and generally not accompanied by metadata that would be useful for \nmusic information retrieval (MIR). On the other hand, \nmany retrieval tasks require the acquisition, playback, browsing or content analysis to be in audio format. This emphasizes the importance of audio content analysis tools that operate at the front-end and become the eyes \nand ears of higher level and general MIR tools. Many \ncategories in the MIREX competitions aim at extracting \nstructural information from a udio. In this regard, key \nfinding is one of these areas that adds considerably to \nthe structural knowledge that can be extracted from a musical piece. Being able to reliably detect the key of a tonal piece (in the context of Western music) remains an \nimportant step in content analysis for MIR. Tonal music constitutes a significant portion of the music consumed today. Hence, models of key finding are applicable to a wide range and large portion of available music. In the same vein, localized key finding is essential for other methods in MIR research to work reliably. \nMany audio key finding models exist in the literature. Most of these deal with identifying the main key in a musical piece. Although this is an important task, it does not provide useful information for structural analysis. That is, by knowing the main key of a piece we cannot infer any additional information regarding the time evolution of its harmonic structure. On the other hand, modulation through one or multiple keys is \nvery common in classical music and is utilized in \npopular music quite often.  \nFrom the listener’s viewpoint a musical fragment in a \nsingle key implies a most stable pitch, the tonic, and a musical scale associated with that key. Throughout this fragment, if the music has a well-formed tonal structure, a change in key center will not be sensed. Secondary functions and tonicizations are heard as short deviations from the well-grounded key in which they appear - although the boundary between modulation and tonicization is not clear cut. A modulation unambiguously instigates a shift in the key center. \nStructure discovery aims at providing high-level representations of music. It deals with problems such as similarity, repetition and thumbnailing. Segmentation is used for identifying points of structural change and it can be based on a multitude of features. In this paper, we investigate segmentation from a tonality perspective. The presented method aims to identify points of modulation, the names of the key centers and their corresponding modes without attempting to perform transcription or chord recognition. It also performs this in an unsupervised manner. \nIn order to infer key from audio input, the music needs to be observed for a certain duration to ensure all necessary elements have been encountered. In other words, one might develop an initial estimate of the key after hearing the first chord of a piece. However, at this point there are multiple competing estimates and one cannot arrive at a reliabl e decision until subsequent \nmusical events have been heard. Every new musical \nevent works in the direc tion of weakening some \nestimates and disambiguati ng and strengthening others. \n© 2007 Austrian Computer Society (OCG).   \n \nThe optimal duration of key locality depends on musical \ncontext. The model presented here works on this premise and aims to group and segment an appropriate duration of music that belongs to and characterizes a key. This is done with non-negative matrix factorization with chroma features as input. This approach flies in the face of sliding window key center tracking techniques \nwhich need the window duration to be fixed and predetermined.  \n2. RELATED WORK \nSymbolic and audio key finding differ in their methods and accuracy. On the symbolic end Chew [5] and Temperley [23] have addressed the problem of modulation. Shmulevic and Yli-Harja [21] employ sliding windows to find local key estimates. Although most researchers working on key finding allude to modulation detection, very little systematic research on \nperformance of these algorithms has been reported.  On the audio end Purwins et al. [19] used a fuzzy distance measure between constant-Q profiles and reference constant-Q sets to track key centers. Operation of the \nmethod is demonstrated on a single piece of piano \nmusic. Chai and Vercoe [4] used a Hidden Markov Model to detect key changes from audio. They used 10 classical pieces to test their method. G\nómez [7] uses a \nspecialized form of PCP feature and a sliding window method to track tonality. Izmirli [13] used symbolic representations to perform efficient comparisons of tonal evolution between different renditions of the same piece, in turn proposing a meas ure of similarity between \nentire pieces. Harte et al. [10] proposed a harmonic \nchange detection function to detect transitions between tonal regions which were de fined by chords in their \ncase. Chord segmentation and recognition are akin to \nkey finding in that common methods are employed for \nsolutions to these problems. For example, among the \nmany models, Sheh & Ellis [20] used a Hidden Markov Model to perform segmentation of chords and chord recognition on Beatles songs.   \nSegmentation has been an ac tive research topic in the \nfield of MIR. We refer to recent work that relates to \nlocal key finding and tonality. Chai [3] proposed models \nfor analysis of musical form and recurrent structure as well as harmony analysis. Ong [17] studied audio-based music structural analysis and used tonal features in measuring similarity of cover songs. \nNon-negative matrix factorization (NMF) was initially proposed by Lee and Seung [16] for part-based learning of images. Smaragdis and Brown [22] demonstrated the application of NMF to polyphonic music transcription. Abdallah and Plumbley [1] used a similar method for transcription and demonstrated its performance on piano music. Cont [6] used NMF to learn spectral note templates off line and then used NMF with sparsity constraints to perform real time note recognition.  The reader is referred to Iz mirli [14] for work in the \nfield of audio key finding. \n3. TUNING FRONT END \nA tuning front end is used to adjust the frequency reference of the system to each input file. Factors such \nas transcoding effects and intentional tuning preferences may result in different tunings for each piece. For example, Peeters [18] and Harte and Sandler [9] have proposed methods for tuning adjustment. In order to find the reference tuning of the input audio file, our method analyzes the first 10-15 seconds of the music and compares it to synthetically generated spectral key templates. A detailed description of spectral templates from audio samples is given in [12, 14] and a summary for line spectra is provided below. The frequency that \nmaximizes the integral of the product between the \ntemplates and the spectrum serves as the tuning frequency estimate of the input audio: \n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛\n=∫∑−\n=max\nmin1\n0, ,\n,) ( ) ( max arg ) , (f\nfR\nik i c i\nk cdf F f X f Y k c    (1) \nY(f) represents the mean of the short-time amplitude \nspectra over the first 10-15 seconds of the piece. X i,c(f) \nrepresents the line spectra  of note i (a Dirac comb with \ndecaying weights) with its fundamental frequency \ncalculated with respect to the reference frequency c . For \nexample, c=442 Hz. would mean the fundamental \nfrequency of note A4 is at that frequency and all other notes are determined according to equally tempered intervals. Each X is constructed using 20 harmonics \nwith amplitudes decaying at  12 dB per octave. The \nlimits on the integration are chosen to be in the range 55 - 1250 Hz. R is the total number of notes used for the \nsynthetic spectral templates, typically spanning 5 \noctaves. Eq. 1 can be directly implemented with a high resolution FFT with zero padding applied to the input signal and a compatible discrete representation for the line spectra. \nProfiles are incorporated into  the calculation of spectral \ntemplates to approximate the distribution of pitch classes in the spectrum. In Eq. 1 F\ni,k are the composite \nprofile weights rotated k steps for note i within each mode. \n⎪⎩⎪⎨⎧\n≤ ≤ + −≤ ≤ + −\n=\n23 12 ) 12 mod ) 24 ( (11 0 ) 12 mod ) 12 ( (\n,\nk if k i Pk if k i P\nF\nmM\nk i (2) \nThe composite profile P is given by the elementwise \nproduct of the diatonic (D) and Temperley (T) profiles: P\ne(k)=D e(k)T e(k). The index e is either M for major or \nm for the minor mode.  \nNot surprisingly, the second index (k) over which the product is maximized in Eq. 1 gives an estimate of the key of the initial section of the piece. However, at this \nstage no segmentation has been done nor has any   \n \nattempt been made to capture the most relevant parts of \nthe music for key estimation. Therefore, this estimate is treated as a by product of the optimization. The results of this estimation are given in the evaluation section.  \n4.   SEGMENTATION \n4.1. Non-negative Matrix Factorization \nNon-negative matrix factorization aims to decompose a matrix V with n rows and m columns into a product of two matrices W and H. An internal dimension p is chosen such that W has n rows and p columns and H has p rows and m columns. As implied in its name the main constraint that separates this decomposition from other similar ones is its non-negativity constraint on all three matrices. The usefulness of this method originates from its summarization property that when p is chosen \nto be smaller than n, the columns of V are summarized \nin columns of W. Hence, W can now be interpreted as a compressed collection of basis vectors that could be \nused to reconstruct an approximation to the original input in V. Due to the smaller internal dimension p the reconstruction WH will not be exact. Thus, a distance measure between WH and V is used as the cost function to be minimized during the factorization. \nNMF which was originally proposed for decomposition of images has also been applied to the problem of polyphonic transcription as mentioned above. This method is suitable for transc ription because the method \ntries to find the sparse additive constituents, i.e. notes,  of the observed polyphonic frequency spectrum. It also does not allow for negative contributions of components that would lead to reconstruction through cancellation. In this case, the basis functions in W represent approximations to note spectra and the corresponding weights (in H) can be viewed as the mixing matrix. Parallel to this, the problem of segmentation based on \ntonal features, the topic of this paper, aims to reveal \nadditive contributions of tonal elements in the analyzed piece. \nIn the original formulation of NMF by Lee and Seung [16] multiplicative updates were used for the factorization. Later, Hoyer [11] proposed a formulation that used additive updates with  sparsity constraints that \ncould be imposed independently on W and H. In this context, sparsity is a measure that quantifies the distribution of energy in a vector. By definition, if the total energy is in a single component the sparsity measure is equal to 1. Similarly, if the energy is spread \nequally among the components then the measure is \nequal to 0.  \n4.2. Segmentation \nIn this work, NMF is used for segmentation. The \ncolumns of V are composed of grouped chroma vectors obtained from the entire length of the musical piece. A group is found by taking the mean of consecutive chroma vectors. The calcu lation of chroma vectors comprises the following steps: the audio is downsampled to 11025 Hz. The spectrum is calculated with a Hann windowed 2048-point FFT. The 12-element chroma vector is obtained from the spectrum in \nthe range 50 Hz. to 2000 Hz. The tuning frequency, c, \nfound in Section 3 is used as the frequency reference while calculating the chroma representation. The details regarding the calculation of the chroma vectors can be \nfound in [12, 14]. The grouped chroma vectors are found by averaging the chroma vectors over a span of s \nseconds. The value of the parameter s is on the order of \n5-15 seconds. Groups are heavily overlapped. The factorization is performed using the Euclidean norm as the cost function. Finally, the maxima in columns of H are found and all existing segments are identified with each segment defined as a consecutive sequence of maxima with the same row index.  \nThe sparsity constraint is only imposed on the H matrix. Forcing the columns of H to  be sparse affects the \nresultant structure of W. As a result, the input matrix is factored such that the ba sis vectors learned in W \nrepresent the best approximati on to the specific clusters \nof chroma vectors where each cluster approximates a chroma pattern particular to one or a group of keys. This makes the factorization function similar to vector quantization because the columns of W will mimic \nglobal representations rather than capturing local \nfeatures or sparse basis functions as in the case of \ntranscription. The work reported here is part of ongoing \nresearch in optimal representations for tonality and key finding. In this context, NMF is preferred over other clustering methods to maintain the flexibility of global vs. local representations regarding chroma and as a means to explore the possibility of lower dimensional representations for tonality. \nThe clustering behavior may need some more clarification. For exampl e, if there is a single \nmodulation in an input piece then it should suffice for H \nto have two rows. In order to minimize the cost function, the performed factorization will result in a summarization of the two keys in columns of W. An example of a factorization for a pop piece that contains \nthree keys is shown in Figure 1.a. The horizontal axis is time and each column represents a grouped chroma vector. Figure 1.b. shows the W matrix with internal dimension p=2 and chroma group window of approximately 7 seconds. Figure 1.c. shows the H \nmatrix factored with a sp arseness value of 0.3. The \nground truth is given at the bottom of the plot. This is an \nexample where the number of keys was underestimated. It can be seen in part c that the first two key regions would be segmented such that they map to the same basis vector. This demonstrates an undesirable situation where a new segment boundary is missed. Nevertheless, the detection of the segment boundary would not be a problem if two closely related keys were interleaved by a distant key. A remedy to this situation would be to increase the internal dimension p to attain less summarization. Figure 2 shows W and H for p=3 using   \n \nthe same song. In this case, the three key regions are \nclearly detectable. The method gives satisfactory results for a simple case like this, however, in general, it is not realistic to assume that the number of modulations would be known a priori. Therefore, one approach might be to consistently overestimate the number of keys in the input. The down side  of this is that there will \nbe more jumps between smaller size segments. This idea is considered in the evaluation using the different data sets.  \n \n \n \nFigure 1.  (a) Top plot. The input matrix V for Shania \nTwain’s Come on Over. The summary chroma vectors are the columns of this matrix. (b) Middle plot. The W matrix (p=2). (c) Bottom pl ot. The H matrix (p=2).  \n \n \nFigure 2. (a) Top plot. W with p=3. (b) Bottom plot. H \nwith p=3. \n5. LOCALIZED KEY ESTIMATION \nLocalized key finding is important for structural segmentation methods in MIR research. For example when using a method of chromagram matching to detect verse or chorus repetitions [2] it is desirable to detect the repetitions regardless of any modulation. Goto [8] addresses this problem by rotating the chroma in all possible keys to account for the possibility of modulation in repetitions of the music. Reliable localized key finding would be helpful in converting all single-key regions to a reference key enabling the \nexisting similarity algorithms to be employed.  \n5.1. Single Key Estimation \nKey finding is generally understood to be the estimation \nof the main key of a piece. Some models only look at the beginning of a piece. This was also part of the specification in the MIREX 2005 audio key finding competition in which our model ranked first, but only with a slight margin ahead  of the other competing \nalgorithms. Some other models look at different parts of entire pieces: beginning, middle and end. Given the performance of the model (model I) in [14] we maintain that analyzing approximately the first half minute of the piece suffices to produce a reliable key estimate of that section. The reason for this, in the case of common \npractice classical music, is that the main key of a piece \nis almost always introduced at the beginning of that piece. Furthermore, the key name is spelled out in the name of the piece conveniently saving the researcher some annotation time. \nAlthough this approach works fairly well, the question of optimal segment length for reliable estimation still remains open. A short segment may put too much focus on a particular chord and an excessively long segment may extend into a section where the piece modulates into another key. In [14] this issue was circumvented by using progressive overlapping windows, all pivoting off the starting point of the piece. For each window the key was estimated and an associated confidence was \ncalculated. The final estimate was determined by \nselecting the key with the overall highest confidence. \nThe approach presented here for single key estimation \nuses the segmentation step described in Section 4 to find the length of one segment at the beginning of the piece, on which a key estimation algorithm is run, and consequently render a key estimate. The correlational model in [12] is used to estimate the key.  \n5.2. Multiple Key Estimation \nA key finding algorithm is applied to the entire span of every segment determined by the segmentation method discussed in the previous section. The key is estimated assuming that the optimal key locality has been correctly delineated by th e segmentation step. Figure \n3.a. shows an example of key estimation on segments found in H for p=3 using a fragment of classical music. The dark letters at the bottom of the figure (above the time axis) are the ground truth and the light letters \nindicate the key estimates at the beginnings of the   \n \nrespective segments. The audio is taken from one of the \ndata sets used in the evaluation. Figure 3.b. shows segmented key estimation on another pop piece. Note that the key estimates are correct but not continuous in the first section. This is fortunately not a problem for the frame based evaluation explained in the next section. If continuous segments were required a simple algorithm for stitching neighboring segments with the same key could be implemented.    \n \n \nFigure 3. (a) Top plot. Segmented key estimation \nshown on H for audio from a Bach Choral. (b) Bottom plot. Segmented key estimation for Abba’s Money Money Money. \n \n6. EVALUATION \nSeveral types of evaluation were carried out to test the performance of the model. There were three data sets. The first set was a collection of 17 complete pop songs that contained at least one modulation. All pieces were \ncarefully annotated with all keys and modulation points. The second set was the initial fragments of 152 classical music pieces from the Naxos set (www.naxos.com). The ground truth was obtained from the names of the pieces. The third set consisted of short fragments of classical \nmusic with modulations. The music was taken from the \nTonal Harmony textbook by Kostka and Payne [15]. \nThis data set (K&P) also had 17 short fragments. The \nrecordings on the accompanying CDs were used. The ground truth was obtained from the accompanying instructor’s manual. \nIn all evaluations a raw measure of accuracy was accompanied by a composite score. In order to partially reward closely related key estimates the following (MIREX) fractional allocations were used while calculating the composite score: correct key, 1 point; perfect fifth, 0.5; relative major/minor, 0.3; parallel major/minor, 0.2 points. In the following, the composite score follows the raw figure in parentheses. \nInitially, we report on the key finding accuracy of the front end tuning stage. As this stage generates a single estimate from the beginning of each piece, that value was compared to the first key in the annotation. The results were as follows: pop set 58.8 % (74.1%), Naxos set 51.3% (62.9%) and K&P set 76.5% (79.4%). Note however, that this method is not intended for key finding. \nThree different types of es timates were calculated for \neach data set with a chroma group window size of 7.4 seconds. Note that a much smaller window size will focus the chromagram on individual chords and much larger window will degenerate the model to a sliding window implementation at the frame level – but even then, it will be useful at the global level and for visualization. Several window durations have been tested and this length has been determined to be a good compromise. It should also be noted that using longer windows will cause the segmentation boundaries to \nblur, however, simply picking the maximum element in \nH will suffice to identify the modulation point. Method ‘I’ is an unweighted correlation estimate with elements of the chromagram raised to the power of 0.5. All frames within a segment are averaged and correlated to the 24 chroma templates. The index of the template with the highest correlation is the key estimate of that segment. The accuracy is cal culated for all available \nframes in the input piece. Method I is a frame based multiple key estimation measure for the pop and K&P sets and a frame based single key estimation measure for the Naxos set as only the main key data is available as ground truth. It is the frame accuracy of the beginning section that ends on the earliest segment boundary between 10 and 30 seconds of each piece. Method ‘II’ denotes a single key estimation accuracy measure. It uses a confidence weighted estimate, as explained in Section 5.1, only for the first segment. Method ‘III’ denotes confidence weighted estimates for all segments in the piece. The accuracy for the three \nmethods and three values of p are given in Table 1. \n \n p=2  (%) p=3  (%) p=4  (%) \nPop I  79.6 (83.9) 82.4 ( 87.0) 76.6 (83.5) \nPop II 64.7 (72.6) 70.6 ( 78.2) 58.8 (70.6) \nPop III 71.8 (76.0) 73.5 ( 79.2) 67.8 (75.6) \nNaxos I 75.1 (80.9) 78.8 ( 83.5) 72.8 (78.5) \nNaxos II 80.9 (85.8) 78.9 ( 84.1) 78.3 (83.5) \nNaxos III 77.1 (83.2) 74.2 ( 80.2) 74.0 (79.4) \nK&P I 69.7 (77.2) 71.5 ( 77.4) 72.5 (78.2) \nK&P II 94.1 (97.1) 94.1 ( 97.1) 82.4 (85.3) \nK&P III 67.6 (74.5) 64.2 ( 70.9) 68.0 (73.7) \nTable 1.  Evaluation results for the three different \ndata sets. \n \nThese preliminary results are en couraging. It can be seen \nthat the accuracy figures are relatively stable over values of p. This shows that the method is not too sensitive to the internal dimension parameter p, and overestimating the number of modulations does not drastically degrade the performance. For comparison, the results of two evaluations are given: the frame accuracies with unweighted key estimates using ground truth   \n \nsegmentation are 88.3% (92.8%) for the pop set and \n76.9% (84.1%) for the K&P set. The unsegmented frame accuracies are pop set 66.49% (75.6%); Naxos set 70.6% (77.1%); K&P set 71.3% (76.1%). This shows that segmentation has improved the frame accuracy for the pop and Naxos sets but not for the K&P set. This is mainly due to the short audio length in the K&P examples and particularly due  to insufficient time span \nin the last modulated key in these recordings. The very high accuracy of model II on this set also supports this point. On the Naxos set, the unsegmented evaluation is \ndone on the segmentation used in method I to make the \nnumber of frames equal. The actual difference is probably greater. The accuracy  of the modulation points \ndepend on the group window duration and the nature of the modulation.   Overall, the proposed method is able to identify modulations and estimate all local key labels in a given piece as seen by the evaluation.  \n7. CONCLUSIONS \nA modulation detection and local key labeling model with a preprocessing stage for tuning adjustment and non-negative matrix factorization for segmentation has been proposed.  The model identifies segments that are candidates for unique local keys in relation to the neighboring key centers. A correlational key finding model is run on every segment in order to label each one with a key center. Encouraging results are obtained on three different data sets and it has been shown that the model does not necessarily have to be tuned to the number of keys for the piece of interest, although a slight drop in performance is experienced as a penalty \nfor overestimating the number of keys.  \n \n8. REFERENCES \n[1] Abdallah S. A. and Plum bley M. D. “Polyphonic \nTranscription by Non-negative Sparse Coding of Power \nSpectra.” Proceedings of the Inte rnational Conference on \nMusic Information Retrieval , Barcelona, Spain, 2004. \n[2] Bartsch, M. and Wakefield, G. H. “To Catch a Chorus: Using Chroma-Based Representations For Audio Thumbnailing,” Proceedings of the Workshop on \nApplications of Signal Processing to Audio and Acoustics , New Paltz, NY, USA, 2001. \n[3] Chai, W., “Automated Analysis of Musical Structure,” Ph.D. Dissertation , MIT, 2005.  \n[4] Chai, W. and Vercoe, B. “D etection of Key Change in \nClassical Piano Music,” Proceedings of the International Conference on Music Information Retrieval , London, \nUK, 2005. \n[5] Chew, E. “The Spiral Array: An Algorithm for Determining Key Boundaries,” Proceedings of the \nSecond International Conference , ICMAI, Edinburgh, \nScotland, UK, 2002. \n[6] Cont, A. “Realtime Multiple Pitch Observation using Sparse Non-negative Constraints,” International \nSymposium on Music Information Retrieval , Victoria, \nCanada, 2006. [7] Gómez, E. “Tonal Descripti on of Music Audio Signals,” \nPh.D. Dissertation , Pompeu Fabra University, Barcelona, \n2006. \n[8] Goto, M. “ Music Scene Description ,” in Anssi Klapuri \nand Manuel Davy, eds., Signal Processing Methods for \nMusic Transcription , pp.327-359, Springer, 2006.  \n[9] Harte, C. and Sandler, M. “Automatic Chord \nIdentification using a Quantised Chromagram,” AES \n118th Convention , Barcelona, Spain, 2005. \n[10] Harte, C., Sandler M., and Gasser, M. “Detecting Harmonic Change in Musical Audio,” Proceedings of \nAMCMM’06 , Santa Barbara, California, USA, 2006.  \n[11] Hoyer, P. O., “Non-negative Matrix Factorization with Sparseness Constraints,” Journal of Machine Learning Research , vol. 5, pp. 1457–1469, 2004. \n[12] İzmirli, Ö. “Template Base d Key Finding from Audio,” \nProceedings of the Inter national Computer Music \nConference , Barcelona, Spain, 2005. \n[13] İzmirli, Ö. “Tonal Similarity from Audio Using a Template Based Attractor Model,” Proceedings of the \nInternational Symposium on Music Information Retrieval, London, UK, 2005. \n[14] İzmirli, Ö. “Audio Key Finding Using Low-Dimensional \nSpaces,” Proceedings of the Inte rnational Conference on \nMusic Information Retrieval , Victoria, Canada, 2006. \n[15] Kostka S. and Payne D. Tonal Harmony  (4th edition), \nBoston: McGraw Hill, 1999. \n[16] Lee, D. D. and Seung, H. S. “Learning the Parts of \nObjects by Non-negative Matrix Factorization”. Nature \n401, pp. 788-791, (1999). \n[17] Ong, B. “Structural Analysis  and Segmentation of Music \nSignals,” Ph.D. Dissertation . Pompeu Fabra University, \nBarcelona, 2007. \n[18] Peeters, G. “Musical Key Estimation of Audio Signal \nBased on HMM Modeling of Chroma Vectors,” Proceedings of DAFX, McGill, Montreal, Canada, 2006. \n[19]  Purwins, H., Blankertz, B. and. Obermayer, K. \n“Constant Q Profiles for Tracking Modulations in Audio Data,” Proceedings of the Inter national Computer Music \nConference , Havana, Cuba, 2001. \n[20] Sheh A. and Ellis, D.  “Chord Segmentation and Recognition using EM-trained Hidden Markov Models,” Proceedings of the Inter national Conference on Music \nInformation Retrieval, Baltimore, Maryland, USA, 2003.  \n[21] Shmulevich, I. and Yli-Harja, O. “Localized Key Finding: Algorithms and Applications.” Music \nPerception, Special Issue in Tonality Induction , \n17(4):531–544, 2000. \n[22] Smaragdis, P. and Brown,  J. “Non-negative Matrix \nFactorization for Polyphonic Music Transcription,” Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics , New Paltz, NY, \nUSA, 2003. \n[23] Temperley, D. The Cognition of Basic Musical Structures , Cambridge, MA: MIT Press, 2001."
    },
    {
        "title": "Evaluation of Distance Measures Between Gaussian Mixture Models of MFCCs.",
        "author": [
            "Jesper Højvang Jensen",
            "Daniel P. W. Ellis",
            "Mads Græsbøll Christensen",
            "Søren Holdt Jensen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415752",
        "url": "https://doi.org/10.5281/zenodo.1415752",
        "ee": "https://zenodo.org/records/1415752/files/JensenECJ07.pdf",
        "abstract": "In music similarity and in the related task of genre classification, a distance measure between Gaussian mixture models is frequently needed. We present a comparison of the Kullback-Leibler distance, the earth movers distance and the normalized L2 distance for this application. Although the normalized L2 distance was slightly inferior to the Kullback-Leibler distance with respect to classification performance, it has the advantage of obeying the triangle inequality, which allows for efficient searching. 1 INTRODUCTION A common approach in computational music similarity is to extract mel-frequency cepstral coefficients (MFCCs) from a song, model them by a Gaussian mixture model (GMM) and use a distance measure between the GMMs as a measure of the musical distance between the songs [2, 3, 5]. Through the years, a number of distance measures between GMMs have been suggested, such as the Kullback-Leibler (KL) distance [2], optionally combined with the earth movers distance (EMD) [3]. In this article, we evaluate the performance of these two distance measures between GMMs together with the normalized L2 distance, which to our knowledge has not previously been used for this application. 2 MEASURING MUSICAL DISTANCE In the following, we shortly describe the Gaussian mixture model and the three distance measures between GMMs we have tested. Note that if a distance measure satisfies the triangle inequality, i.e., d(p1, p3) ≤d(p1, p2) + d(p2, p3) for all values of p1, p2 and p3, then a nearest neighbor search can be speeded up by precomputing some distances. Assume we are searching for the nearest neighbor to p, and that we have just computed the distance to p1. If we already know the distance between p1 and p2, then the distance to p2 is bounded by d(p, p2) ≥d(p1, p2) − This research was supported by the Intelligent Sound project, Danish Technical Research Council grant no. 26–04–0092, and the Parametric Audio Processing project, Danish Research Council for Technology and Production Sciences grant no. 274–06–0521. c⃝2007 Austrian Computer Society (OCG). d(p1, p). If the distance to the currently best candidate is smaller than d(p1, p2) −d(p1, p), we can discard p2 without computing d(p, p2).",
        "zenodo_id": 1415752,
        "dblp_key": "conf/ismir/JensenECJ07",
        "keywords": [
            "Gaussian mixture model",
            "Kullback-Leibler distance",
            "earth movers distance",
            "normalized L2 distance",
            "triangle inequality",
            "nearest neighbor search",
            "computational music similarity",
            "genre classification",
            "mel-frequency cepstral coefficients",
            "distance measure"
        ],
        "content": "EVALUATION OF DISTANCE MEASURES BETWEEN GAUSSIAN\nMIXTURE MODELS OF MFCCS\nJesper Højvang Jensen Daniel P.W. Ellis Mads G. Christensen Søren Holdt Jensen\nAalborg University Columbia University Aalborg University Aalborg University\nDept. Electron. Syst. LabROSA Dept. Electron. Syst. Dept. Electron. Syst.\nABSTRACT\nIn music similarity and in the related task of genre clas-\nsiﬁcation, a distance measure between Gaussian mixture\nmodels is frequently needed. We present a comparison of\nthe Kullback-Leibler distance, the earth movers distance\nand the normalized L2 distance for this application. Al-\nthough the normalized L2 distance was slightly inferior\nto the Kullback-Leibler distance with respect to classiﬁ-\ncation performance, it has the advantage of obeying the\ntriangle inequality, which allows for efﬁcient searching.\n1 INTRODUCTION\nA common approach in computational music similarity\nis to extract mel-frequency cepstral coefﬁcients (MFCCs)\nfrom a song, model them by a Gaussian mixture model\n(GMM) and use a distance measure between the GMMs\nas a measure of the musical distance between the songs\n[2, 3, 5]. Through the years, a number of distance mea-\nsures between GMMs have been suggested, such as the\nKullback-Leibler (KL) distance [2], optionally combined\nwith the earth movers distance (EMD) [3]. In this arti-\ncle, we evaluate the performance of these two distance\nmeasures between GMMs together with the normalized\nL2 distance, which to our knowledge has not previously\nbeen used for this application.\n2 MEASURING MUSICAL DISTANCE\nIn the following, we shortly describe the Gaussian mixture\nmodel and the three distance measures between GMMs\nwe have tested. Note that if a distance measure satis-\nﬁes the triangle inequality, i.e., d(p1, p3)≤d(p1, p2) +\nd(p2, p3)for all values of p1, p2andp3, then a nearest\nneighbor search can be speeded up by precomputing some\ndistances. Assume we are searching for the nearest neigh-\nbor to p, and that we have just computed the distance to p1.\nIf we already know the distance between p1andp2, then\nthe distance to p2is bounded by d(p, p2)≥d(p1, p2)−\nThis research was supported by the Intelligent Sound project, Dan-\nish Technical Research Council grant no. 26–04–0092, and the Paramet-\nric Audio Processing project, Danish Research Council for Technology\nand Production Sciences grant no. 274–06–0521.\nc/circlecopyrt2007 Austrian Computer Society (OCG).d(p1, p). If the distance to the currently best candidate is\nsmaller than d(p1, p2)−d(p1, p), we can discard p2with-\nout computing d(p, p2).\n2.1 Gaussian Mixture Models\nDue to intractability, the MFCCs extracted from a song are\ntypically not stored but are instead modelled by a GMM.\nA GMM is a weighted sum of multivariate Gaussians:\np(x)=K/summationdisplay\nk=1ck1/radicalbig\n|2πΣk|exp/parenleftBig\n−1\n2(x−µk)TΣ−1\nk(x−µk)/parenrightBig\n,\nwhere Kis the number of mixtures. For K= 1, a simple\nclosed-form expression exists for the maximum-likelihood\nestimate of the parameters. For K > 1, the k-means algo-\nrithm and optionally the expectation-maximization algo-\nrithm are used to estimate the parameters.\n2.2 Kullback-Leibler Distance\nThe KL distance is an information-theoretic distance mea-\nsure between probability density functions. It is given by\ndKL(p1, p2) =/integraltext\np1(x) logp1(x)\np2(x)dx. As the KL distance\nis not symmetric, a symmetrized version, dsKL(p1, p2) =\ndKL(p1, p2) +dKL(p2, p1), is usually used in music infor-\nmation retrieval. For Gaussian mixtures, a closed form\nexpression for dKL(p1, p2)only exists for K= 1. For\nK > 1,dKL(p1, p2)is estimated using stochastic integra-\ntion or the approximation in [4]. The KL distance does\nnot obey the triangle inequality.\n2.3 Earth Movers Distance\nIn this context the EMD is the minimum cost of changing\none mixture into another when the cost of moving proba-\nbility mass from component min the ﬁrst mixture to com-\nponent nin the second mixture is given [3]. A common\nchoice of cost is the symmetrized KL distance between\nthe individual Gaussian components. With this cost, the\nEMD does not obey the triangle inequality.\n2.4 Normalized L2 Distance\nLetp/prime\ni(x) =pi(x)//radicalBig/integraltext\npi(x)2dx, i.e., pi(x)scaled to\nunit L2-norm. We then deﬁne the normalized L2 distance\nbydnL2(p1, p2) =/integraltext\n(p/prime\n1(x)−p/prime\n2(x))2dx. Since the ordi-Figure 1 . Instrument recognition results. Labels on the\nx-axis denotes the number of MFCCs retained, i.e. 0:10\nmeans retaining the ﬁrst 11 coefﬁcients including the 0th.\n“Fluid” and “SGM” denotes the Fluid R3 and SGM 180\nsound fonts, respectively.\nnary L2 distance obeys the triangle inequality, and since\nwe can simply prescale all GMMs to have unit L2-norm\nand then consider the ordinary L2 distance between the\nscaled GMMs, the normalized L2 distance will also obey\nthe triangle inequality. Also note that dnL2(p1, p2)is noth-\ning but a continuous version of the cosine distance [6],\nsince dnL2(p1, p2) = 2(1 −/integraltext\np/prime\n1(x)p/prime\n2(x)dx). For GMMs,\nclosed form expressions for the normalized L2 distance\ncan be derived for any Kfrom [1, Eq. (5.1) and (5.2)].\n3 EVALUATION\nWe have evaluated the symmetrized KL distance com-\nputed by stochastic integration using 100 samples, EMD\nwith the exact, symmetrized KL distance as cost, and the\nnormalized L2 distance. We extract the MFCCs with the\nISP toolbox R1 using default options1. To model the\nMFCCs we have both used a single Gaussian with full\ncovariance matrix and a mixture of ten Gaussians with di-\nagonal covariance matrices. With a single Gaussian, the\nEMD reduces to the exact, symmetrized KL distance. Fur-\nthermore, we have used different numbers of MFCCs. As\nthe MFCCs are timbral features and therefore are expected\nto model instrumentation rather than melody or rhythm,\nwe have evaluated the distance measures in a synthetic\nnearest neighbor instrument classiﬁcation task using 900\nsynthesized MIDI songs with 30 different melodies and\n30 different instruments. In Figure 1, results for using\na single sound font and results where the query song is\nsynthesized by a different sound font than the songs it is\ncompared to are shown. The former test can be considered\na sanity test, and the latter test reﬂects generalization be-\nhaviour. Moreover, we have evaluated the distance mea-\nsures using 30 s excerpts of the training songs from the\nMIREX 2004 genre classiﬁcation contest, which consists\nof 729 songs from 6 genres. Results for genre classiﬁca-\ntion, artist identiﬁcation and genre classiﬁcation with an\nartist ﬁlter (see [5]) are shown in Figure 2.\n1http://isound.kom.auc.dk/\nFigure 2 . Genre and artist classiﬁcation results for the\nMIREX 2004 database.\n4 DISCUSSION\nAs the results show, all three distance measures perform\napproximately equal when using a single Gaussian with\nfull covariance matrix, except that the normalized L2 dis-\ntance performs a little worse when mixing instruments\nfrom different sound fonts. Using a mixture of ten diago-\nnal Gaussians generally decrease recognition rates slightly,\nalthough it should be noted that [2] recommends using\nmore than ten mixtures. For ten mixtures, the recognition\nrate for the Kullback-Leibler distance seems to decrease\nless than for the EMD and the normalized L2 distance.\nFrom these results we conclude that the cosine distance\nperforms slightly worse than the Kullback-Leibler distance\nin terms of accuracy. However, with a single Gaussian\nhaving full covariance matrix this difference is negligible,\nand since the cosine distance obeys the triangle inequality,\nit might be preferable in applications with large datasets.\n5 REFERENCES\n[1] P. Ahrendt, “The multivariate gaussian probability dis-\ntribution,” Technical University of Denmark, Tech.\nRep., 2005.\n[2] J.-J. Aucouturier, “Ten experiments on the modelling\nof polyphonic timbre,” Ph.D. dissertation, University\nof Paris 6, France, 2006.\n[3] B. Logan and A. Salomon, “A music similarity func-\ntion based on signal analysis,” in Proc. IEEE Int. Conf.\nMultimedia Expo , 2001, pp. 745 – 748.\n[4] E. Pampalk, “Speeding up music similarity,” in 2nd\nAnnual Music Information Retrieval eXchange , Lon-\ndon, 2005.\n[5] ——, “Computational models of music similarity and\ntheir application to music information retrieval,” Ph.D.\ndissertation, Vienna University of Technology, Aus-\ntria, 2006.\n[6] J. R. Smith, “Integrated spatial and feature image sys-\ntems: Retrieval, analysis and compression,” Ph.D. dis-\nsertation, Columbia University, New York, 1997."
    },
    {
        "title": "Human Similarity Judgments: Implications for the Design of Formal Evaluations.",
        "author": [
            "M. Cameron Jones",
            "J. Stephen Downie",
            "Andreas F. Ehmann"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416126",
        "url": "https://doi.org/10.5281/zenodo.1416126",
        "ee": "https://zenodo.org/records/1416126/files/JonesDE07.pdf",
        "abstract": "This paper presents findings of a series of analyses of human similarity judgments from the Symbolic Melodic Similarity, and Audio Music Similarity tasks from the Music Information Retrieval Evaluation Exchange (MIREX) 2006. The categorical judgment data generated by the evaluators is analyzed with regard to judgment stability, inter-grader reliability, and patterns of disagreement, both within and between the two tasks. An exploration of this space yields implications for the design of MIREX-like evaluations.",
        "zenodo_id": 1416126,
        "dblp_key": "conf/ismir/JonesDE07",
        "keywords": [
            "Symbolic Melodic Similarity",
            "Audio Music Similarity",
            "Music Information Retrieval Evaluation Exchange (MIREX)",
            "2006",
            "categorical judgment data",
            "evaluators",
            "judgment stability",
            "inter-grader reliability",
            "patterns of disagreement",
            "design of MIREX-like evaluations"
        ],
        "content": "HUMAN SIM ILARITY JUDGMENTS: I MPLICATIONS \nFOR THE DESIGN OF FO RMAL EVALUATIONS\nM. Cameron Jones  J. Stephen Downie  Andreas F. Ehmann  \nInternational Music Information Retrieval Systems Evaluation Laboratory  \nGraduate School of Library and Information Science  \nUniversity of Illinois at Urbana -Champaign  \nABSTRACT  \nThis paper presents findings of a series of analyses of \nhuman similarity judgments from the Symbolic Melodic \nSimilarity, and Audio Music Similarity tasks from the \nMusic Information Retrieval Evaluation Exchange \n(MIREX ) 2006.  The categorical judgment data \ngenerated by the evaluators is analyzed with regard to \njudgment stability , inter -grader reliability, and patterns \nof disagreement, both within and between the two tasks. \nAn exploration of th is space yields implications  for the \ndesign of MIREX -like evaluation s. \n1. INTRODUCTION  \nThe International Music Information Retrieval Systems \nEvaluation Laboratory (IMIRSEL) at the University of \nIllinois at Urbana -Champaign has been hosting and \nrunning the Annual Music Information Retrieval \nEvaluation eXchange (MIREX) since 2005. Inspired by \nTREC, the goal of MIREX is to formally evaluate state -\nof-the-art algorithms for Music Information Retrieval \n(MIR) systems [ 2]. \nMIREX 2006 comprised nine separate evaluation \ntasks which were defi ned by community input [ 5]. Two \nof these tasks, “Symbolic Melodic Similarity” (SMS) \nand “Audio Music Similarity and Retrieval” (AMS), \ncalled for human judgments of similarity in order to \nestablish ground truth for the evaluation of the \nsubmitted algorithms . In order to capture these \nsimilarity judgments we created a new web -based tool \ncalled the “Evalutron 6000” (E6K).   \nIn this paper, we present findings from our analysis \nof categorical human similarity judgment data  collected \nusing the E6K. We explore the consistency of the \ngraders’ scoring, measuring the amount of disagreement \namong graders. We discuss the implications of our \nfindings for the design of future tasks which utilize \nhuman judgements of similarity in the MIR domain.  \n2. DATA CAPTURE: EVALUT RON 6000  \nThe SMS and AMS tasks shared a common structure. \nEach task participant’s algorithm was run against a \ncollection of either symbolic or audio music files. For \neach query, each algorithm returned a list of  the top-\nranked “candidate” songs , the length n of the candidate \nlists was ten for SMS and five for AMS . All resulting \ncandidates  for each query were merged  and then evaluated by graders using the E6K.  \nIn the E6K, graders score the anonymized set of \ncandidates for each query  anonymously. Individual \ngraders are tracked, but their scores are kept \nindependent of their identities. This tracking allowed us \nto log each grader’s interactions with the E6K. Events \nlogged include: score inputs, score modifications, \nauditions, etc. Table 1 provides descriptive statisti cs for \neach the two evaluation tasks.  \n \n SMS  AMS \nNo. of events logged  \nNo. of submitted algorithms  \nTotal no. of queries  \nTotal no. of query -candidate pairs  \nNo. of graders  \nNo. of queries per grader  \nAvg. size of candidate lists  \nAvg. no. of evaluations per grader  23,491  \n8 \n17  \n905 \n21 \n15 \n15 \n225 46,254  \n6 \n60 \n1,629  \n24 \n7-8 \n27 \n205 \nTable 1 . Summary of Evalutron 6000 statistics.  \nAfter listening to each query -candidate pair, graders \nwere asked to rate the degree of similarity of the \ncandidate to the query in two ways: 1) by selecting one \nof the three BROAD categories of similarity: Not \nSimilar (NS), Somewhat Similar (SS), and Very Similar \n(VS); and, 2) by assigning a FINE score between 0.0 \n(Least similar) and 10.0 (Most similar). Each query -\ncandidate pair was eval uated by three different graders. \nData were collected between 5 Sept. and 20 Sept., 2006, \nfrom volunteer graders from the MIR/MDL research \ncommunity, representing 11 different countries.   \n3. MEASURING DISAGREEME NT \nUnderstanding the consistency of the graders’ \njudgments is essential to interpreting human judgments \nof similarity in contexts such as MIREX. Previous \nstudies [ 1,6] have analyzed the consistency of \njudgments between BROAD scores and FINE scores. \nFigure 1 s hows the consistency of assignment of FINE \nscores within BROAD categories for both SMS and \nAMS tasks. The variation of FINE scores within the \nBROAD SS category for AMS is particularly \ninteresting, indicating that graders were not very \nconsistent in assigni ng FINE scores to items they had \nmarked as Somewhat Similar (SS) for this task. The \ndifferences between the tasks  and of the consistency of \nFINE scores  are discussed in more detail in [ 1].  \n© 2007 Austrian Computer Society (OCG).  \n   \n \n  \n \nFigure  1. Distribution of FINE scores within BROAD score \ncatego ries for SMS and AMS tasks.  \nThe open, web -based nature of the E6K allowed \ngraders to enter and leave the system at their \nconvenience. This had the consequence of allowing \ngraders to revisit  and revise  their similarity judgments. \nIn assigning BROAD judgment s to query -candidate \npairs, graders did not tend to change their judgments \n(see Table 2). The majority of graders in both SMS and \nAMS made only a single BROAD category judgment, \nnot changing it. Overall, only 5.86% of SMS graders \nand 9.04% of AMS graders c hanged their judgment at \nsome point during the grading process . \n \n # Grading  \nOpportunities  # Grading  \nEvents  Mean  Max  Mode  \nSMS  2715  2907  1.07 5 1 \n(94.14 %)  \nAMS  4887  5450  1.12 14 1 \n(90.96 %) \nTable 2 . Description of grader judgment changes.  \n3.1. Inter -grader Reliability  \nInter -grader reliability measures the relative objectivity \n(or inter -subjectivity) of judgments; a necessary \ncondition for measuring the validity of the E6K \nframework and, more generally, the design of future \nprojects which will utilize similar  mechanisms for \nevaluation.  Several metrics have been developed to \nmeasure inter -grader reliabi lity.  \nFleiss’s Kappa is a measure of inter -grader reliability \nfor nominal data, and is based on Cohen’s two -grader \nreliability Kappa, but measures reliability among an \narbitrary number of graders [3]. The equation for \nFleiss’s Kappa is given in (3.1.1).  \n \n𝜅=𝑃 −𝑃𝑒 \n1−𝑃𝑒  (3.1.1)  \nwhere:  𝑃 =1\n𝑁𝑛 𝑛−1   𝑛𝑖𝑗 𝑛𝑖𝑗−1 𝑘\n𝑗=1𝑁\n𝑖=1  (3.1.2)  \n𝑃𝑒 =  1\n𝑁𝑛 𝑛𝑖𝑗𝑁\n𝑖=1 2 𝑘\n𝑗=1 (3.1.3)  \n \nIn the Fleiss Kappa equation, N is total number of \nquery -candidate pairs to be graded ; n is the number of \njudgments per query -candidate pair; k is the number of \nresponse categories; and nij is the number of graders \nwho assigned the i-th query -candidate pair to the j-th \ncategory.  \nWe measured the inter -grader reliability of the E6K \njudgments from the SMS and AMS tasks. In addition to \ncomput ing the reliability score using all three BROAD \njudgment scores (VS, SS, NS), we also combined the \n(VS, SS)  categories to create a  general “Similar ” (S) \ncategory and measured inter -grader reliability using the \nresulting 2 -level judgment scores (S, NS). The resulting \nKappa scores for the 3 -level and 2 -level judgments for \nboth SMS and AMS are given in Table 3. \n \n 3-level (VS, SS, NS)  2-level (S, NS)  \nSMS  0.3664  0.3201  \nAMS  0.2141  0.2989  \nTable 3. Fleiss’s Kappa for AMS and SMS contests at 3 -levels \nand 2 -levels  of judgment.  \nFleiss’s Kappa scores can range  from 0.0 (no \nagreement) to 1.0 (perfect agreement). Landis and Koch \n[4] studied the consistency of physician diagnoses of \npatient illnesses and derived a scale for interpreting the \nstreng th of agreement indicat ed by  Fleiss’s Kappa score . \nAccording to this scale, t he resulting Kappa scores \nreported in Table 3 indicate a “Fair” level of agreement  \n(within the range of 0.21 – 0.40)  for all tasks at both 3 - \nand 2 -levels of judgment . While greater levels of \nagreement are possible, and indeed desirable, the \nassessment of a “fair” level of agreement is encouraging \ngiven that the graders in the E6K are drawn from a \nheterogeneous pool of candidates with a wide  variety of \nskills and background s compared to  the relatively hi ghly \ntrained physicians studied  by Landis and Koch.  \nIn order to better understand the nature of the \ndisagreement indicated by the Kappa scores, we \nconducted further analysis of the data. To start, we \nlooked at the distribution of scores given by each grade r, \ni.e., how many VS, SS, and NS scores did each grader \nassign globally. A Chi -squared analysis  (Table 4) shows \nthat the re is a significant difference between  graders \nwith respect to  each grader’s  frequency distribution of  \nVS, SS, and NS (or S, NS) scores . This might be a result \nof the fact that each grader evaluated a different subset \nof queries from the total set.  \nLooking at each query -candidate pair, the set of \nscores assigned by the three graders were counted \n(Tables 5 and 6). Within the set, triple assignment of the \nsame judgment (same score given by each grader) \nindicates total agreement. Cases of partial agreement  are \n  \n \n counted as “doubles”  (i.e., where two graders agreed \nand a third dissented ). No agreement is indicated by \nthree different scores.  \n \n3-level (VS, SS, NS)  \n χ2 df P \nAMS  765.98  46 0* \nSMS  414.01  38 0* \n2-level (S, NS)  \n χ2 df P \nAMS  413.22  23 0* \nSMS  323.89  19 0* \n* denotes significanc e \nTable 4. Chi -Squared statistics for the variation of agreement \nacross queries in AMS and SMS contests at 3-levels and 2 -\nlevels of judgment.  \nComparing grader judgments at 3-levels of similarity  \n(Table 5), only 2.2% of query -candidate pairs in SMS \n(7.1% in AMS) resulted in no agreement.  There was \npartial disagreement  in a majority of the cases (51.9% \nfor SMS , 62.8% for AMS).   \n \nJudgments  3-level SMS  3-level AMS  \nVS,VS,VS  114 12.6%  61 3.7%  \nSS,SS,SS  38 4.2%  137 8.4%  \nNS,NS,NS  263 29.1%  293 18.0%  \nTotal triples  415 45.9%  491 30.1%  \nVS,VS  24 2.7%  150 9.2%  \nSS,SS  158 17.5%  469 28.8%  \nNS,NS  288 31.8%  404 24.8%  \nTotal doubles  470 51.9%  1023  62.8%  \nVS,SS,NS  20 2.2%  115 7.1%  \nTotal  905 100.0%  1629  100.0%  \nTable 5. Distribution of 3 -level judgment triples (total \nagreement), doubles  (partial agreement) , and cases of no \nagreement.  \nWhen the SS and VS categories are combined into a \nbroader “Similar”  category (S) the amount of total \nagreement  increase d, as expected (Table 6). \n \nJudgments  2-level SMS  2-level AMS  \nS,S,S  188 20.8%  494 30.3%  \nNS,NS,NS  263 29.1%  293 18.0%  \nTotal triples  451 49.8%  787 48.3%  \nS,S 166 18.3%  438 26.9%  \nNS,NS  288 31.8%  404 24.8%  \nTotal doubles  454 50.2%  842 51.7%  \nTotal  905 100.0%  1629  100.0%  \nTable 6. Distribution of 2 -level judgment triples  (total \nagreement)  and doubles  (partial agreement).  \nWithin each query, the total number of cases of \npartial agreement and no agreement were counted and \ncompared. A Chi -Squared test was used to measure the \ndifferences in the variance  of this disagreement between \nqueries. At 3 -levels of judgment, there were no \nsignificant differences in the variance of disagreement \nacross queries, meaning that graders tended to disagree \nfairly consistently across queries. When VS and SS \nwere combined, the variance of disagreement across \nqueries did reach significance for both  AMS and SMS, \nindicating that there were some queries which had significantly more or less amounts of disagreement than \nothers (Table 7). \n \n3-level (VS, SS, NS)  \n χ2 df P \nAMS  44.25  59 0.92 \nSMS -Mixed  10.92  5 0.053  \n  SMS-RISM  7.94 5 0.159  \n  SMS-Karaoke  1.03 4 0.905  \n2-level (S, NS)  \n χ2 df P \nAMS  124.98  59 0* \nSMS -Mixed  11.3 5 0.046*  \n  SMS-RISM  10.09  5 0.073  \n  SMS-Karaoke  1.17 4 0.882  \n* denotes significanc e \nTable 7. Chi -Squared statistics for the variation of agreement \nacross queries in AMS and SMS contests at 3 -levels and 2 -\nlevels of judgment.  \n4. DISCUSSION  \nGraders’ judgm ents tended to be fairly stable . The raw \nevent log counts presented in Table 2 represent the total \nnumber of times BROAD score categories were \nchanged. In SMS 39 (20.3%)  of the 192  changes and in \nAMS  125 (22.2%) of the 563  changes to BROAD score \nresulted in the grader reverting to their original \njudgment, meaning that the final score  recorded  was the \nsame as the initial score  given. This may represent an \nartifact of the logging mecha nism whereby the grader \nmay have clicked on the interface, generating an event, \nwhich was not an intentional judgment action, or was \nessentially a duplicate event for the same judgment. \nHowever, ev en considering this possibility the majority \nof graders  only made a single judgment  for each query -\ncandidate pair.  \nAdditionally, t he amount of disagreement observed \namong graders differ ed between the SMS and AMS \ntasks  when considering 3 -level judgments  (Table 5) . \nThe raw proportions of cases resulting in no agreem ent \nwere different, with AMS having slightly more \ndisagreement among graders than SMS. This difference \nbetween the tasks may be attributable to the nature of \nthe task and data being analyzed or  it may be a \nconsequence of the task definition. In SMS, grader s \nwere asked to evaluate the melodic similarity of two \nworks and were explicitly instructed to look beyond \ndifferences in timbre and instrumentation [ 5]. In AMS, \nhowever, the task was less well -defined, asking graders \nto evaluate the musical similarity of two works, only \nspecifying that the works should “sound” similar. [ 5]. \nThis highly subjective definition of musical similarity \nmay explain the observed differences between the tasks.  \nThe likelihood of graders to assign a particular score \ndid differ signifi cantly across graders. This tells us that \nmultiple graders are needed to temper the peculiarities \nof any individual. For example, one grader in the AMS \nevaluation did not assign a single query -candidate pair a \nVery Similar (VS) rating; another grader gave over 78% \nof judgments a Somewhat Similar (SS) rating. These \nvery different judgment profiles imply that relying on a   \n \n single grader may skew the results significantly. \nHowever, the relatively small number of cases with no \nagreement may indicate that only tw o graders are \nneeded per query -candidate pair, instead of three.  \nInterpreting the amount of consistency between \ngraders is subjective. When considering three levels of \nresponses, there is greater consistency between queries \nthan when considering only two. Greater consistency is \ndesired as it reflects a more objective measure of \nsimilarity. However, as these judgments are ultimately \nbeing used to evaluate the performance of algorithms \nand improve their  design, the greater variation in \nagreement afforded by t he two -level judgments may \nprovide greater discrimi nation between queries . \nIdentifying particular queries which caused unusual \namounts of agreement or disagreement may help \nbackground the results, allowing MIREX organizers to \nweight those judgments accordi ngly, and help \ndevelopers identify limitations of their algorithms.  \n5. CONCLUSION S AND RECOMMENDATIONS  \nIn this paper we have tried to unpack the issues of \nhuman judgment in evaluating music similarity systems. \nThese analyses can be used to improve the design of \nfuture MIREX tasks, and inform the design of other \nprojects which seek to use human judgments for \nevaluation. Towards this goal, we present several \nrecommendations based on our understanding of the \ndata.  \nThe differences between the AMS and SMS tasks did  \nresult in differences in the judgments of graders. \nBuilding on the discussion started in [ 1], we advance \nthat many of the differences observed in the reliability \nand consistency of the graders can be attributed to \nproblems in the definition of the evaluat ion tasks. The \nlesser -specified instructions given to graders in the \nAMS task are likely contributing to the greater observer \nvariation in the scores. To the extent that Inter -grader \nreliability tests measure the consistency of grader \njudgments, they also reflect the degree graders \nunderstood the instructions given to them. Tasks like \nAMS may need to be more narrowly defined to include \nmore objective features, such as melody, \ninstrumentation, style, genre, etc.  \nThe number of response categories presented to  \ngraders does have an effect on their ability to reach \nconsensus; fewer categories yields greater consensus. \nThis might lead to the conclusion that only two \ncategories should be used for evaluation (Similar or Not \nSimilar). Indeed, this is a model which wa s discussed in \nthe design of the AMS task, and reflects the user \nbehavior of managing a playlist (i.e., songs are either \nincluded in the list or not). However, the addition al \ncosts of collecting the slightly more nuanced judgments \nis negligible, and based on anecdotal feedback, graders \nappear to prefer having three options.  \nAs mentioned above, three graders may not be \nneeded to achieve consensus. The amount of partial and \ntotal agreement observed indicates that two graders may \nsuffice where three were previously used. Furthermore, the amount of disagreement is consistent acro ss queries. \nThus, a more advanced E6K system may adaptively \nallocate graders to queries, only assigning a third grader \nto queries which deviate from the expected level of \nagreement, or to query -candidate pairs which have not \nachieved consensus. This would allow for a greater \nnumber of query -candidate pairs to be evaluated by the \nsame number of graders.  \nThe major limiting factor in the design of evaluation \nschemes, like that employed by AMS and SMS, is the \navailability of graders.  Ultimately, evaluating mor e \nqueries and more candidates per query would more \ngreatly benefit algorithm developers. We argue that the \nnumber of queries be increased in future iterations of \nMIREX.  \nThe analyses presented in this paper have only \nscratched the surface of understanding t he judgment \ndata collected with the E6K. In the future we intend to \ndo similar analyses of the FINE scores collected in the \nE6K, as well as explore the temporal dimension of the \ngrader judgments  in order to understand how presumed \nindependent judgments may  be interacting or \ninterfering. Furthermore, f uture iterations of MIREX \nand use of the E6K will generate more data to be \nanalyzed and compared towards the production of a \nmodel of human evaluation and interaction in the E6K.  \n6. ACKNOWLEDGEMENTS  \nSpecial thanks  to: The Andrew W. Mellon Foundation, \nthe National Science Foundation ( #IIS-0327371) , Dr. \nEllen Vo orhees, and  the MIREX 2006 graders.  \n7. REFERENCES  \n \n[1] Downie, J. S., Lee, J. H., Gruzd, A. A., Jones, M. C. \n(2007). Toward an Understanding of Similarity \nJudgments  for Music Digital Library Evaluation. In \nthe Proceedings of the ACM/IEEE Joint Conference \non Digital Libraries.  \n[2] Downie, J. S., West, K., Ehmann, A., and Vincent, \nE. The 2005 Music Information Retrieval \nEvaluation eXchange (MIREX 2005): Preliminary \novervie w. In Proceedings of the 6th International \nConference on Music Information Retrieval (ISMIR \n2005), Queen Mary, UK, 2005, 320 -323. \n[3] Fleiss, Joseph L. Measuring nominal scale \nagreement among many raters. Psychological \nBulletin. 76(5):378 -382, November 1971.  \n[4] Landis, J. R. and Koch, G. G. (1977) \"The \nmeasurement of observer agreement for categorical \ndata\" in Biometrics. Vol. 33, pp. 159 -174. \n[5] MIREX Wiki. Available at: http://music -\nir.org/mirexwiki/ . \n[6] Pampalk, E. (2006).  Audio -Based Music Similarity \nand Retrieval: Combining a Spectral Similarity \nModel with Information Extracted from Fluctuation \nPatterns. Technical Report.  \nhttp://staff.aist.go.jp/el ias.pampalk/papers/pam_mire\nx06.pdf"
    },
    {
        "title": "Pedagogical Transcription for Multimodal Sitar Performance.",
        "author": [
            "Ajay Kapur",
            "Graham Percival",
            "Mathieu Lagrange",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417127",
        "url": "https://doi.org/10.5281/zenodo.1417127",
        "ee": "https://zenodo.org/records/1417127/files/KapurPLT07.pdf",
        "abstract": "Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modified traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar. 1 INTRODUCTION Historically, musical traditions were preserved via oral transmission. With the invention of written music, audio recordings, and video, more information can be retained. However, valuable performance data must still be passed by oral means. There will never be a technological replacement for face-to-face teaching, but new methods for archiving performance data will let us retain and disseminate more information. Automatic music transcription is a well-researched area [3, 4, 5]. The novelty of our work is that we look beyond the audio data by using sensors to avoid octave errors and problems caused from polyphonic transcription. In addition, our work does not share the bias of most research that focuses only on Western music. This paper describes a music transcription system for sitar performance. The sitar is a fretted stringed instrument from North India. Unlike many Western fretted stringed instruments (classical guitar, viola de gamba, etc) sitar performers pull (or “bend”) their strings to produce higher pitches. In normal performance, the bending of a string will produce notes as much as a fifth higher than the same fret-position played without bending. In addition to simply showing which notes were audible, our framework also provides information about how to produce such notes. A musician working from an audio recording (or transcription of an audio recording) alone will need to determine which fret they should begin pulling from. This can be challenging for a skilled performer, let alone a beginner. By representing the fret information on the sheet music, sitar musicians may overcome these problems. c⃝2007 Austrian Computer Society (OCG). Figure 1. Block diagram of the system 2 METHOD The Electronic Sitar (ESitar) [2] is an instrument which gathers gesture data from a performing artist. For the research described in this paper, fret data was captured by a network of resistors connecting each fret. The fret sensor is translated into MIDI pitch values based on equivalent resistance induced by left hand placement on the neck of the instrument. Each fret has a “bucket” of values, converting raw sensor data into discrete pitch values seen in figure 2. Data was recorded at a sampling rate of (44100 ÷ 512) Hz. The synchronized audio signal was recorded with a Shure Beta-57 microphone at 44100 Hz. The entire system is displayed in Figure 1.",
        "zenodo_id": 1417127,
        "dblp_key": "conf/ismir/KapurPLT07",
        "keywords": [
            "Automatic music transcription",
            "sheet music from audio signal",
            "performance data",
            "performance techniques",
            "instrument performance",
            "annotated guidelines",
            "sensors",
            "sitar",
            "fretted stringed instrument",
            "traditional instruments"
        ],
        "content": "PED AGOGICAL TRANSCRIPTION FOR MUL TIMOD ALSITAR\nPERFORMANCE\nAjay Kapur ,Graham Percival,Mathieu Lagrange, Geor geTzanetakis\nDepartment ofComputer Science\nUni versity ofVictoria\nBritish Columbia, Canada\nABSTRA CT\nMost automatic music transcription research isconcerned\nwith producing sheet music from the audio signal alone.\nHowever,the audio data does not include certain perfor -\nmance data which isvital for the preserv ation ofinstru-\nment performance techniques and the creation ofanno-\ntated guidelines for students. Wepropose the use ofmod-\niﬁed traditional instruments enhanced with sensors which\ncan obtain such data; asacase study we examine the sitar .\n1INTR ODUCTION\nHistorically ,musical traditions were preserv ed via oral\ntransmission. With the invention ofwritt enmusic, audio\nrecordings, and video, more information can beretained.\nHowever,valuable performance data must still bepassed\nby oral means. There will neverbeatechnological re-\nplacement for face-to-f ace teaching, butnewmethods for\narchi ving performance data will letusretain and dissemi-\nnate more information.\nAutomatic music transcription isawell-researched area\n[3, 4,5]. The novelty ofour work isthat we look beyond\nthe audio data byusing sensors toavoid octa veerrors and\nproblems caused from polyphonic transcription. Inaddi-\ntion, our work does not share the bias ofmost research\nthat focuses only onWestern music.\nThis paper describes amusic transcription system for\nsitar performance. The sitar isafretted stringed instru-\nment from North India. Unlik eman yWestern fretted stringed\ninstruments (classical guitar ,viola de gamba, etc) sitar\nperformers pull (or “bend”) their strings toproduce higher\npitches. Innormal perf ormance, the bending ofastring\nwill produce notes asmuch asaﬁfth higher than the same\nfret-position played without bending. Inaddition tosim-\nply sho wing whic hnotes were audible, our frame work\nalso provides information about howtoproduce such notes.\nAmusician working from an audio recording (or tran-\nscription ofanaudio recording) alone will need todeter -\nmine which fret theyshould begin pulling from. This can\nbechallenging for askilled performer ,letaloneabegin-\nner.By representing the fret information onthe sheet mu-\nsic, sitar musicians may overcome these problems.\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nFigur e1.Block diagram ofthe system\n2METHOD\nThe Electronic Sitar (ESitar) [2] isaninstrument which\ngathers gesture data from aperforming artist. Forthe re-\nsearch described inthis paper ,fret data wascaptured bya\nnetw ork ofresistors connecting each fret.\nThe fret sensor istransla ted into MIDI pitch values\nbased onequi valent resistance induced byleft hand place-\nment onthe neck ofthe instrument. Each fret has a“buck et”\nofvalues, con verting rawsensor data into discrete pitch\nvalues seen inﬁgure 2.Data wasrecorded atasampling\nrate of(44100 ÷512)Hz. The synchronized audio signal\nwasrecorded with aShure Beta-57 microphone at44100\nHz. The entire system isdisplayed inFigure 1.\n2.1 Audio Signal Chain and Pitch Extraction\nAcompressor/limiter wasused onthe audio signal togen-\nerate abalanced amplitude for all frequencies. Without\nthis step, our experiments yielded poor results for notes\nplayed atlower frequencies.\nToautomatically determine the pitch animplementa-\ntion ofthe method described in[1] wasused. Weutilize\nthe autocorrelation function toefﬁciently estimate the fun-\ndamental frequenc y(f0).Foratime signal s(n)that is\nstationary ,the autocorrelation rs(τ)asafunction ofthe\nlag τisdeﬁned as\nrs(τ)=1/Nt+N/summationdisplay\nj=ts(t)s(t+τ) (1)\nThis function has aglobal maximum for τ=0.Ifthere\nare also additional global maxima, the signal iscalled pe-\nriodic and there exists alag τ0,the period, sothat allthese 60 62 64 66 68 70 72 74\n 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800MIDI note value\nTimepitch datafret datadecided note\nFigur e2.Fret data, audio pitches, and the resulting de-\ntected notes. The ﬁnal three notes were pulled.\nmaxima are placed atthe lags nτ0,for every inte ger n,\nwith rs(nτ0)=rs(0).\nThe inverse ofthe lag τ0provides anestimation ofthe\nfundamental frequenc yf0.The period isdetermined by\nscanning rt(τ),starting atzero, and stopping atthe ﬁrst\nglobal maximum with non-zero abscissa. Quadratic in-\nterpolation isused tofurther impro vethe frequenc yesti-\nmation. Inpractical cases, the relati veamplitude ofthose\nmaxima may change and some others maxima may appear\ndue tosmall aperiodicities ofthe signal. The issue isthen\ntorele vantly select which maximum corresponds tothe f0\nbyconsidering several candidates under aplausible range\nand pick the one with the highest conﬁdence, see [1] for\nfurther references onthe algorithm.\n2.2 Fusion with FretSignal Chain\nTocompensate for noisy fret data, we smoothed the fret\ndata samples comparing the median value ofthe previous\nten samples with the median ofthe nextten samples (in-\ncluding the current sample). Ifthe median values differed\nbymore than acertain amount, we mark edthat sample as\nbeing anote boundary .\nToget anaccurate ﬁnal result, pitch information from\nthe audio signal chain isfused with onset and pitch bound-\naries calculated from the fret signal chain. The fret pro-\nvided con venient lower and upper bounds onthe pitch: a\nnote cannot belower than the fret, nor higher than aﬁfth\n(ie 7MIDI notes) abo vethe fret. Using the note bound-\naries deri vedfrom the fret data, we ﬁnd the median value\nofthe pitches inside the boundaries supplied by the fret\ndata. These are represented by the vertical lines inFig-\nure 2,and are the note pitches inthe ﬁnal output.\n3SHEET MUSIC\nAlthough there are man ycomputer notation programs for\nWestern music, there are nosuch counterparts for Indian\nmusic. India nnotation isnot standardized and there isno\nwaytonotate both frets and audible notes, sowe inventedScorecreatedfrom/Users/gperciva/Desktop/vsens.notes.txt\n1Figur e3.Sheet music ofsitar performance, generated\nfrom the data used inFigure 2. The top swar aare the\naudible notes, while the lower swar aare the fret positions.\nThe ﬁnal three notes were pulled, sothe audio and fret\nswar aare different.\nour own notation. InNorth Indian classical music, notes\nare described bysevenswar as.The yare kno wn asShadja\n(Sa), Rishab (Re), Gandhar (Ga), Madhyam (Ma), Pan-\ncham (Pa), Dhaivat (Dha), and Nishad (Ni). These are\nequi valent tothe Western solfe gescale. Weproduce sheet\nmusic (Figure 3)sho wing sitar musicians the audible note\nplayed and which fret wasused.\n4CONCLUSION\nWehavedeveloped asystem which produces sheet mu-\nsic for sitar musicians. Although the system works well\nfor simple sitar melodies, itcurrently does not detect glis-\nsandi and bends. Inaddition, since we detect note bound-\naries byexamining the fret data, we cannot detect multiple\nnotes pluck edonthesame string. Future work inthis area\nisplanned, inaddition toexperimenting with other pitch\ndetection algorithms (such asthe YIN method). Once we\ncan reliably detect typical sitar ﬂourishes (glissandi and\nnote bends), we must invent notation todisplay such ex-\npressi vemarks.\n5REFERENCES\n[1] P.Boers ma. Accurate short-term analysis ofthe fun-\ndamental frequenc yand the harmonics-to-noise ratio\nofasampled sound. InProc. ofthe Institute ofPho-\nnetic Sciences, Amster dam ,volume 17, pages 97–110,\n1993.\n[2] A.Kapur ,A.J.Lazier ,P.Davidson, R.S.Wilson, and\nP.R.Cook. The electronic sitar controller .InNIME ,\npages 7–12, 2004.\n[3] A. P.Klapuri. Automatic music transcription aswe\nkno w ittoday .Journal of New Music Resear ch,\n33(3):269–282, 2004.\n[4] A.Loscos, Y.Wang, and W.J.J.Boo. Lowlevelde-\nscriptors for automatic violin transcription. InProc. of\nISMIR 2006 ,pages 164–167, October 2006.\n[5] J.Yin, T.Sim, Y.Wang, and A.Sheno y.Music tran-\nscription using aninstrument model. Proc. ICASSP\n’05 ,3,March 2005."
    },
    {
        "title": "VISA: The Voice Integration/Segregation Algorithm.",
        "author": [
            "Ioannis Karydis",
            "Alexandros Nanopoulos",
            "Apostolos N. Papadopoulos",
            "Emilios Cambouropoulos"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416552",
        "url": "https://doi.org/10.5281/zenodo.1416552",
        "ee": "https://zenodo.org/records/1416552/files/KarydisNPC07.pdf",
        "abstract": "Listeners are capable to perceive multiple voices in music. Adopting a perceptual view of musical ‘voice’ that corresponds to the notion of auditory stream, a computational model is developed that splits musical scores (symbolic musical data) into different voices. A single ‘voice’ may consist of more than one synchronous notes that are perceived as belonging to the same auditory stream; in this sense, the proposed algorithm, may separate a given musical work into fewer voices than the maximum number of notes in the greatest chord. This is paramount, among other, for developing MIR systems that enable pattern recognition and extraction within musically pertinent ‘voices’ (e.g. melodic lines). The algorithm is tested against a small dataset that acts as groundtruth.",
        "zenodo_id": 1416552,
        "dblp_key": "conf/ismir/KarydisNPC07",
        "keywords": [
            "perceptual view",
            "computational model",
            "musical scores",
            "voices",
            "auditory stream",
            "MIR systems",
            "pattern recognition",
            "melodic lines",
            "algorithm",
            "groundtruth"
        ],
        "content": "VISA : THE VOICE INTEGRATION/SEGREGATION ALGORITHM \nIoannis  \nKarydis Alexandros \nNanopoulos Apostolos N. \nPapadopoulos Emilios  \nCambouropoulos \nDepartment of Informatics \nAristotle University of Thessaloniki \n{karydis, ananopou, papadopo} @csd.auth.gr  Dept. of Music Studies \nAristotle Univ. of Thessaloniki \nemilios@mus.auth.gr  \nABSTRACT \nListeners are capable to perceive multiple voices in \nmusic. Adopting a perceptual view of musical ‘voice’ \nthat corresponds to the notion of auditory stream, a \ncomputational model is developed that splits musical \nscores (symbolic musical data) into different voices. A \nsingle ‘voice’ may consist of more than one \nsynchronous notes that are perceived as belonging to \nthe same auditory stream; in this sense, the proposed \nalgorithm, may separate a given musical work into \nfewer voices than the maximum number of notes in the \ngreatest chord. This is paramount, among other, for \ndeveloping MIR systems that enable pattern recognition \nand extraction within musically pertinent ‘voices’ (e.g. \nmelodic lines). The algorithm is tested against a small \ndataset that acts as groundtruth. \n1. INTRODUCTION \nRecently, there have been a number of attempts [3, 5, 9, \n10, 11, 12, 13] for the computational modelling of the \nsegregation of polyphonic music into separate voices. \nMuch of this research is influenced by empirical studies \nin music perception [1, 6, 7] as well as by musicological \nconcepts such as melody, counterpoint, voice-leading \nand so on.   \nIt appears that the term ‘voice’ has different \nmeanings for different research fields (traditional \nmusicology, music cognition and computational \nmusicology). A detailed discussion is presented in [1]. \nA single musical example is given in Fig. 1 that \npresents three different meanings of the term voice. \n \nFigure 1 How many voices in each example?  \nStandard understanding of the term voice refers to a \nmono phonic sequence of successive non-overlapping \nmusical tones; a single voice is thought not to contain \nmulti-tone sonorities. However, if ‘voice’ is seen in the light of auditory streaming, then, it’s clear that the \nstandard meaning is not sufficient. It's possible that a \nsingle monophonic sequence may be perceived as more \nthan one voice/stream (e.g., pseudopolyphony or \nimplied polyphony) or that a passage containing \nconcurrent notes may be perceived as a single \nperceptual entity (e.g., homophonic passages in Fig.1c).  \nThe perceptual view of voice adopted in this study, \nallows for multi-tone simultaneities in a single ‘voice’, \nwhile bearing the most significant difference of the \nproposed model with existing ones. In Fig. 1, all \nexisting algorithms  (see exception regarding Kilian and \nHoos’s algorithm in the next section), that are based on \npurely monophonic definitions of voice, would find two \nvoices in the second example (Fig. 1b) and three voices \nin the third example (Fig. 1c). It is clear that such \nvoices are not independent voices. In terms of harmonic \nvoices, all examples can be understood as comprising of \nthree voices (triadic harmony). In terms of perceptual \nvoices/streams, each example is perceived as a single \nauditory stream (harmonic accompaniment); it makes \nmusical sense to consider the notes in each example as \na single coherent whole, as a unified harmonic \nsequence. The proposed algorithm determines a single \n‘voice’/stream in all three examples. \nIn this paper, initially, a number of recent voice \nseparation algorithms are briefly described and their \nmain differences to the current proposal are \nhighlighted. Then, the fundamental auditory streaming \nprinciples, forming the basis of the proposed model, are \npresented. The description of the proposed algorithm \nfollows, concluded by evaluation of the algorithm and \nresults on ten different musical works. \n2. RELATED WORK \nVoice separation algorithms such as [3, 5, 10, 11, \n12, 13] assume that ‘voice’ is a monophonic sequence \nof successive non-overlapping musical tones. The \nunderlying perceptual principles that organise tones in \nvoices are the principles of temporal and pitch \nproximity (cf. Huron’s [7] Temporal Continuity and \nPitch Proximity principles). In essence, these models \nattempt to determine a minimal number of monophonic \nlines/voices such that each line consists of successions \nof tones that are maximally proximal in the temporal \nand pitch dimensions.  \n© 2007 Austrian Computer Society (OCG). \n   \n \n Kilian and Hoos’s [9] model is pioneering in the \nsense that multi-note sonorities within single voices are \nallowed. The pragmatic goal of the algorithm is the \nderivation of reasonable score notation - not \nperceptually meaningful voices (see [9], p.39). The \nresults are not necessarily perceptually valid (e.g., a 4-\npart homophonic piece may be ‘forced’ to split into two \nmusical staves that do not correspond to perceptually \npertinent streams). The algorithm does not discover \nautomatically the number of independent musical \n‘voices’ in a given excerpt; if the user has not defined \nthe maximum number of voices, the algorithm \nautomatically sets the maximum number equal to the \nmaximum number of co-sounding notes – in this case \nthe algorithm becomes similar to all other algorithms \nmentioned above (see discussion in [8]). \n3. PERCEPTUAL PRINCIPLES FOR VOICE \nSEPARATION \nBregman [1] offers an in depth exploration of processes \nrelating to perceptual integration/segregation of \nsimultaneous auditory components. Coordinated and \nsynchronously evolving in time sounds tend to be \nperceived as components of a single auditory event.  \nConcurrent tones that start, evolve and finish together \ntend to be merged perceptually. The proposed principle \n(below) relates to Huron’s Onset Synchrony Principle \n[7] but it differs in a number of ways as discussed by \nCambouropoulos [2]. \nSynchronous Note Principle:  Notes with synchronous \nonsets and same inter-onset intervals IOIs (durations) \ntend to be merged into a single sonority. \nThe horizontal integration of musical elements \n(such as notes or chords) relies primarily on two \nfundamental principles: Temporal Continuity  and Pitch \nProximity  [7].  \nIt is suggested, that a voice separation algorithm \nshould start by identifying synchronous notes that tend \nto be merged into single sonorities and then use the \nhorizontal streaming principles to break them down \ninto separate streams (most algorithms ignore the \nvertical component). This is an optimisation process \nwherein various perceptual factors compete for the \nproduction of a ‘simple’ interpretation of the music in \nterms of a minimal number of streams. \n4. VISA : THE VOICE \nINTEGRATION/SEGREGATION ALGORITHM  \nThis section describes the proposed voice separation \nalgorithm VISA . \n4.1. Merging Notes into Single Sonorities  \nDuring vertical integration, according to the \nsynchronous note principle, we have to determine when \nto merge concurrent notes and thus require a merging \ncriterion. Given a set of concurrent notes S, the algorithm \nexamines the frequency of appearing concurrency in a \ncertain musical excerpt (window) around them. If \ninside the window most co-sounding notes have \ndifferent onsets/offsets, then it is most likely that we \nhave independent monophonic voices so occasional \nsynchronous notes should not be merged. Thus, by \nhaving a user-defined threshold T that signifies \nfrequency, if the ratio of concurrency is more than T, \nwe merge the notes of S as a single sonority. \n4.2. The Algorithm \nThe Voice Integration/Segregation Algorithm (VISA) \nreceives as input the musical piece in the form of a list \nL of notes that are sorted according to their onset times, \na window size w, and the threshold T. The output is a \nset of lists V (initially empty). After termination, each \nlist contains the notes of each detected voice, sorted by \nonset time. Notice that VISA does not demand a-priori \nknowledge of the number of voices. The proposed \nalgorithm is illustrated in Fig. 2. \n \nFigure 2 The VISA algorithm. \nIn VISA, a sweep line, starting from the beginning \nof L, proceeds in a step-wise fashion to the next onset \ntime in L. The set of notes having onsets equal to the \nposition of the sweep line is denoted as sweep line set \n(SLS ). Next, every SLS  is divided into clusters by \npartitioning the notes in the SLS  into a set of clusters C. \nThe ClusterVertically  procedure, detects contextual \ninformation, accepting, thus, w and T as parameters. If, \nbased on context, we decide to merge concurrent notes, \neach cluster contains all notes with the same IOI. \nOtherwise, if merging is not decided, each cluster \ncontains a single note. \nGiven the set of clusters, C, a bipartite graph is \nformed in order to assign them to voices, where one set \nof vertices corresponds to the currently detected voices \nand the other set corresponds to the clusters in C. \nBetween every pair of vertices in the graph, we draw an \nedge to which we assign a cost. Having determined the \ncost on every edge, we can solve the assignment \nproblem by finding the matching  with the lowest cost in \nthe bipartite graph. Two cases are possible: (i) If | V| < \n|C|, then we match voices to clusters. This is done by   \n \n assigning to each of the currently detected voices a \ncluster, in a way that the total cost is minimised. The \nremaining clusters that have not been assigned to a \nvoice constitute new voices that are added to V. This is \nhandled inside procedure MatchVoicesToClusters . (ii) \nConversely, if | V| ³ |C|, we match clusters to voices, i.e., \neach cluster is assigned to one of the currently detected \nvoices, in a way that the total cost is minimised. \nNevertheless, a matching may not be feasible, in which \ncase new voices, enabling a matching, are created.  \nFinally, we introduce two extra constraints to the \nproblem of a matching; (a) voice crossing should be \navoided and (b) the top voice should be minimally \nfragmented [12]. Section 4.3 presents more details for \nthe inclusion of constraints in the matching procedure. \n4.3. The Matching Process \nFor convenience, we convert the minimisation problem \nto an equivalent maximisation one. For this reason, the \nassignment of the cost w(eij) between a voice vi and a \ncluster cj is converted to max{ ekl} – w(eij), where \nmax{ ekl} is the maximum edge cost determined for the \nspecific instance of the matching problem (and this cost \nis due to the edge connecting voice vk and cluster cl) .  \n(a) pair-wise costsv1\nv2\nv3v1\nv2\nv3c1\nc2\nc3\nc4\nc5c1\nc2\nc3\nc4\nc5v19191\nv29115\nv30105c1c2c3c4c5\n1\n0\n8\n(b) best matching (c) best crossing-free matching  \nFigure 3 Maximum matching examples  \nTraditional bipartite matching algorithms do not \npreserve the order of the matching. In our case, order \npreservation is important (voice crossing), formulating \na new problem that can not be directly tackled by \nbipartite matching algorithms. Figure 3 illustrates three \nvoices, five clusters and the pair-wise cost for their \nassignment. A maximum weighted matching (Fig. 3b), \nwith a total cost of 23 does not necessarily avoid voice \ncrossing, while a crossing-free maximum weighted \nmatching with cost of 22 is depicted in Fig. 3(c). The \nproposed matching can handle larger number of \nvoices/clusters, and is based on [4].  \nThe matching process is depicted in Fig. 4(a), where \neach cell of the matrix M represents the total matching \ncost. The matrix is filled according to the recurrence \nequation (see [8]) of the dynamic programming. \n000000\n099999\n0910101414\n0010101522      c1   c2   c3  c4  c5\nv1\nv2\nv3\n(a) matching path      c1     c2     c3    c4    c5\n      v1     __     __    v2    v3\n(b) final assigment \nFigure 4  The matching process The best matching cost itself does not provide for the \nassignment of voices to clusters. To determine the \nmatching path (Fig. 4a) we perform a trace-back process \nstarting at the cell which contains the best matching \nvalue.  In the trace-back process we never choose a \nvertical cell, since no gaps are allowed to be placed on \nthe cluster sequence, meaning that all voices must be \nmatched. The final assignment is given in Fig. 4(b). \nAccording to the previous discussion, the running \ntime of the algorithm is O( n*m) (n>=2, m>=2) where n \nis the number of voices and m the number of clusters. \nEvidently, we need O( n*m) time to calculate all \nelements of the matrix M, and O( n+m) time to \nreconstruct the matching path. \n5. EXPERIMENTS AND RESULTS \nThe proposed algorithm has been tested on ten pieces \nwith clearly defined streams/voices which are used as \ngroundtruth. The first six pieces include four fugues \nand two inventions by J.S.Bach; these polyphonic works \nconsist of independent monophonic voices. Two \nmazurkas and a waltz by F.Chopin consist of a melody \n(upper staff) and accompanying harmony (lower staff). \nFinally, the “Harmony Club Waltz” by S.Joplin has two \nparallel homophonic streams (chordal ‘voices’) that \ncorrespond to the two piano staves. See excerpts in Figs \n5, 6, 7. \nIn this pilot study, our aim is to examine if a single \nalgorithm can be applied to two very different types of \nmusic (i.e. pure polyphonic music and music containing \nclear homophonic textures). All the parameters of the \nalgorithm are the same for all ten pieces, while the \nnumber of streams/voices is determined automatically. \nIt should be noted that for the pieces by Chopin and \nJoplin all other voice separation algorithms would \ndetermine automatically at least four different voices \n(up to eight voices) that do not have perceptual validity \n(and musicologically are problematic). \n \nFigure 5  Four independent streams/voices are present in this \nexcerpt from the Fugue No.1 in C major, WTCI, BWV846 by \nJ.S.Bach. The algorithm performs voice separation correctly \nexcept for the last five notes of the upper voice which are \nassigned to the 2nd voice rather than the first voice, as these \nare closer by a semitone to the last note of the second voice. \n \nFigure 6 In the opening of the Mazurka, Op.7, No.5 by \nF.Chopin, the algorithm detects correctly one voice (low \noctaves) and, then, switches automatically to two voices \n(melody and accompaniment).    \n \n \n \nFigure 7  Two independent chordal streams/voices are \ncorrectly determined by the algorithm in this excerpt from the \n“Harmony Club Waltz” by S.Joplin; the only mistake is \nindicated by the circled note which is placed ‘erroneously’ in \nthe upper stream (because of pitch proximity).  \nThe evaluation metrics used is the precision of the \nobtained result. For the previously described musical \ndataset, Table 1 shows the results. The effectiveness of \nthe proposed methodology is evident by the high \nprecision rates achieved for all ten pieces.  \n \nMusical Work Precision \nJ.S.Bach, Fugue No.1 in C major, BWV846  92,38% \nJ.S.Bach, Fugue No.14 in F# major, BWV859 95,56% \nJ.S.Bach, Fugue No.11 in F major, BWV 856 87,31% \nJ.S.Bach, Fugue No.7 in E major, BWV 852 97,52% \nJ.S.Bach, Invention No.1 in C Major, BWV 772 99.34% \nJ.S.Bach, Invention No.13 in A Min, BWV 784  96.45% \nF. Chopin, Mazurka, Op.7, No.5 100% \nF. Chopin, Mazurka in A Minor, Op. 67, No.4 88.8% \nF. Chopin, Waltz in B Minor, Op. 69, No. 2 90.31% \nS. Joplin, “Harmony Club Waltz” 98.12% \nTable 1  Results in terms of precision for the dataset.   \nThe results were examined in detail (qualitative \nanalysis). Most wrong results were given in cases where \nthe number of voices changes and erroneous \nconnections are introduced primarily due to pitch \nproximity (e.g., see last upper five notes in Fig. 5). \nKilian and Hoos [9] address this same problem \nclaiming that, in essence, it is unsolvable at the note \nlevel. A second kind of problem involves voice \ncrossing. Since voice crossing is disallowed, notes at \npoints where voices cross (in the Bach fugues) are \nassigned to wrong voices. A third type of mistake \nrelates to the breaking of vertically merged notes into \nsub-sonorities and allocating these to different voices; \nin this case the breaking point in the sonority may be \nmisplaced (e.g., circled note in Fig. 7). \n6. CONCLUSIONS \nIn this paper, the notions of voice and auditory stream \nhave been examined. It is suggested that, if ‘voice’ is \nunderstood as a musicological parallel to the concept of \nauditory stream, then multi-note sonorities should be \nallowed within individual ‘voices’.  It is proposed that a \nfirst step in voice separation is identifying synchronous \nnote sonorities and, then, breaking these into sub-\nsonorities incorporated in horizontal streams or \n‘voices’. \n The proposed voice separation algorithm, VISA , \nincorporates the two principles of temporal and pitch \nproximity, and additionally, the Synchronous Note Principle, performing in the general case where both \npolyphonic and homophonic elements are mixed \ntogether. \n7. REFERENCES \n[1] Bregman, A (1990) Auditory Scene Analysis: The \nPerceptual Organisation of Sound.  The MIT Press, \nCambridge (Ma). \n[2] Cambouropoulos, E. (2006) ‘Voice’ Separation: \ntheoretical, perceptual and computational perspectives. \nIn Proceedings of the 9th International Conference in \nMusic Perception and Cognition (ICMPC2006) , 22-23 \nAugust, Bologna, Italy. \n[3]  Cambouropoulos, E. (2000) From MIDI to Traditional \nMusical Notation. In Proceedings of the AAAI \nWorkshop on Artificial Intelligence and Music , July 3 - \nAug. 3, Austin, Texas. \n[4] Cormen, T., Leiserson, C.E., Rivest, R.L. and  Stein, C \n(2001).  Introduction to Algorithms , The MIT Press, \nCambridge (Ma). \n[5] Chew, E. and Wu, X. (2004) Separating voices in \npolyphonic music: A contig mapping approach. In \nComputer Music Modeling and Retrieval: Second \nInternational Symposium  (CMMR 2004), pp. 1-20. \n[6] Deutsch, D. (1999) Grouping Mechanisms in Music. In \nD. Deutsch (ed.), The Psychology of Music  (revised \nversion).  Academic Press, San Diego. \n[7] Huron, D. (2001) Tone and Voice: A Derivation of the \nRules of Voice-Leading from Perceptual Principles. \nMusic Perception , 19(1):1-64. \n[8] Karydis, I., Nanopoulos, A., Papadopoulos, A., \nCambouropoulos, E. and Manolopoulos Y. (2007) \nHorizontal and Vertical Integration/Segregation in \nAuditory Streaming: A Voice Separation Algorithm for \nSymbolic Musical Data. In proceedings of the confernce \nSound and Music Computing  (SMC07), Lefkada. \n[9] Kilian j. and Hoos H. (2002) Voice Separation: A Local \nOptimisation Approach. In Proceedings of the  Third \nInternational Conference on Music Information \nRetrieval  (ISMIR 2002), pp.39-46. \n[10] Kirlin, P.B. and Utgoff, P.E. (2005) VoiSe: Learning to \nSegregate Voices in Explicit and Implicit Polyphony. In \nProceedings of the Sixth International Conference on \nMusic Information Retrieval  (ISMIR 2005), Queen \nMary, Univ. of London (pp. 552-557).  \n[11] Madsen, S. T. and Widmer, G. (2006) Separating \nVoices in MIDI. In Proceedings of the 9th International \nConference in Music Perception and Cognition \n(ICMPC2006) , 22-26 August 2006, Bologna, Italy. \n[12] Temperley, D. (2001) The Cognition of Basic Musical \nStructures . The MIT Press, Cambridge (Ma). \n[13] Szeto, W.M. and Wong, M.H. (2003) A Stream \nSegregation Algorithm for Polyphonic Music Databases. \nIn Proceedings of the Seventh International Database \nEngineering and Applications Symposium  (IDEAS’03)."
    },
    {
        "title": "A Simple Algorithm for Automatic Generation of Polyphonic Piano Fingerings.",
        "author": [
            "Alia Al Kasimi",
            "Eric Nichols",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415880",
        "url": "https://doi.org/10.5281/zenodo.1415880",
        "ee": "https://zenodo.org/records/1415880/files/KasimiNR07.pdf",
        "abstract": "We present a novel method for assigning fingers to notes in a polyphonic piano score. Such a mapping (called a “fingering”) is of great use to performers. To accommodate performers’ unique hand sha our method relies on a simple, user function. We use dynamic programming to search the space of all possible fingerings for the optimal fingering under this cost function. Despite the simplicity of the algorithm we achieve reasonable and useful results.",
        "zenodo_id": 1415880,
        "dblp_key": "conf/ismir/KasimiNR07",
        "keywords": [
            "Piano Fingering",
            "Polyphonic Score",
            "Dynamic Programming",
            "Automatic Fingering",
            "Cost Function",
            "Performance Optimization",
            "Algorithm Design",
            "Music Performance",
            "User Customization",
            "Musical Instruments"
        ],
        "content": "A SIMPLE ALGORITHM F \nGENERATION \nAlia Al Kasimi \nDept. of Computer Science \nIndiana Univ. \nalia.alkasimi@gmail.com  \nABSTRACT \nWe present a novel method for assigning fingers to notes \nin a polyphonic piano score. Such a mapping (called  a \n“fingering”) is of great use to performers. To \naccommodate performers’ unique hand sha \nour method relies on a simple, user \nfunction. We use dynamic programming to search the \nspace of all possible fingerings for the optimal fi ngering \nunder this cost function. Despite the simplicity of  the \nalgorithm we achieve re asonable and useful results. \n1.  INTRODUCTION \nSeveral different algorithms have been proposed in \nrecent decades for automatic generation of musical \ninstrument fingerings [1,2,3,4,5]. We propose to build on \nthis previous work in two ways. First, in keeping w ith \nthe nature of most piano music, our algorithm handl es \npolyphonic  input – the lack of a polyphonic algorithm in \nthe literature is lamented in [2 ]. Second, we formulate \nthe solution in an elegant manner that facilitates rapi d \nimplementation and computation. \n2.  AUTOMATIC FINGERING \n2.1.  Representation of Solution Space \nBeginning with a symbolic representation of a music al \nscore, we generate a trellis graph 1 with one layer for \neach time point where a note begins or ends \neach layer a set of nodes representing all possible \nfingerings for the notes sounding at that time poin t \nFigure 1. Trellis Graph of Solution Space \n                                                           \n1 A trellis graph is a multilayer directed acyclic gr aph \nwhere nodes are only connected between adjacent lay ers. \n © 2007 Austrian Computer Society (OCG). A SIMPLE ALGORITHM F OR AUTOMATIC \nGENERATION OF POLYPHONIC PIANO FINGERINGS \nEric Nichols Christopher Raphael \nDept. Of Computer Science \nIndiana Univ. \nepnichol@indiana.edu \n School of Informatics \nIndiana Univ. \ncraphael@indiana.edu \nWe present a novel method for assigning fingers to notes \nin a polyphonic piano score. Such a mapping (called  a \n“fingering”) is of great use to performers. To \naccommodate performers’ unique hand sha pes and sizes, \nour method relies on a simple, user -adjustable cost \nfunction. We use dynamic programming to search the \nspace of all possible fingerings for the optimal fi ngering \nunder this cost function. Despite the simplicity of  the \nasonable and useful results.  \nINTRODUCTION  \nSeveral different algorithms have been proposed in \nrecent decades for automatic generation of musical \nWe propose to build on \nthis previous work in two ways. First, in keeping w ith \nthe nature of most piano music, our algorithm handl es \nthe lack of a polyphonic algorithm in \n]. Second, we formulate \nsolution in an elegant manner that facilitates rapi d \nAUTOMATIC FINGERING ALGORITHM \nBeginning with a symbolic representation of a music al \nwith one layer for \ntime point where a note begins or ends , and for \na set of nodes representing all possible \nfingerings for the notes sounding at that time poin t . \n \n. Trellis Graph of Solution Space  \nA trellis graph is a multilayer directed acyclic gr aph \nwhere nodes are only connected between adjacent lay ers.  \n The numbers correspond to fingers, where ‘1’ repres ents \nthe thumb and ‘5’ is the little finger \npossible fingerings as follows: \n1.  Within each node, fingers are assigned in order \nfrom lowest pitch to highest pitch \n2.  The same finger must  \npersists (no finger substitution is allowed). \n3.  Each finger may only depress one note. \nEach path from ‘start’ to the ‘end’ is a possible f ingering \nsolution. A weight (cost) is assigned \nbetween nodes in the graph , and \nused to compute the optimal fin \nPrevious monophonic fingering \nused a cost function based on \nbetween two notes. We call this the \nextend it to the polyphonic case. Additionally, we \nintroduce the vertical cost  quantifying the \nfingers involved in playing a particular chord \nof transitioning between two nodes is the sum of the \nhorizontal and vertical costs. \n2.2.  Vertical Cost \nThe vertical cost depends on the combination of pitches \nthat are present at a given point in time. \nalways rewrite polyphonic music as a collection of \nchords by using the tie symbo l ( \nFigure 2.  Excerpt from Aufschwung ( \nop.12) by Schumann \nFigure 3.  Vertical analysis of the same excerpt \nTo compute the vertical cost of a \ndecompose the fingering in the node \nof notes. For example, in the F Major triad \nassociated fingering 1-3- 5, we consider the two pairs \nA (1-3) and A-C (3-5). Each pair \nwhich two fingers are involved and the distance bet ween \nthe two notes being played , yielding the node cost \ncv(p,d) gives the cost for playing two notes separated by  \ndistance d with a particular pair \nivnCost )( \nOR AUTOMATIC \nFINGERINGS \nChristopher Raphael  \nSchool of Informatics  \nIndiana Univ.  \ncraphael@indiana.edu  \nThe numbers correspond to fingers, where ‘1’ repres ents \nlittle finger . We constrain the \npossible fingerings as follows:  \nWithin each node, fingers are assigned in order \nfrom lowest pitch to highest pitch  (no crossover). \n be used as long as a note \n(no finger substitution is allowed).  \nEach finger may only depress one note.  \nEach path from ‘start’ to the ‘end’ is a possible f ingering \nis assigned to each connection \n, and  the Viterbi algorithm is \noptimal fin gering sequence. \nfingering approaches have \nused a cost function based on the stretch in the hand \nbetween two notes. We call this the horizontal cost  and \nextend it to the polyphonic case. Additionally, we \nquantifying the  spread of the \nfingers involved in playing a particular chord . The cost \ntwo nodes is the sum of the \ndepends on the combination of pitches \npoint in time.  Note that we can \nalways rewrite polyphonic music as a collection of \nl ( Figures 2 and 3):  \n \nExcerpt from Aufschwung ( Phantasiestuck  \n \nVertical analysis of the same excerpt  \nTo compute the vertical cost of a node, we first \nthe fingering in the node  into adjacent pairs \nof notes. For example, in the F Major triad F-A-C with \n5, we consider the two pairs F-\nach pair ’s cost is determined by \nwhich two fingers are involved and the distance bet ween \n, yielding the node cost : \n \n(1) \n) gives the cost for playing two notes separated by  \nwith a particular pair p of fingers. \n∑\n==pairs \nkkvdPc#\n1),(   \n \n 2.3.  Horizontal Cost \nThe horizontal cost between two nodes is defined to  be \nthe average transition cost taking into account all  pairs of \nnotes consisting of one note from each chord: \n ∑\n∈∈→ =→\nb anjnih b ah djicNnnCost \n,),(1)(   (2) \nSpecifically, in Figure 4, the horizontal cost c h(p,d) is \nthe sum of the costs for transitioning from C to F, C to \nA, E to F, E to A, G to F, and G to A, divided by 6. \n \nFigure 4.  Horizontal cost example  \nOne might expect that c h and cv are equivalent – after all, \nplaying an ascending third (e.g. C up to E) with fingers 1 \nand 3 is just as easy as playing C and E simultaneously. \nHowever, there are two situations where this is not  true. \nFirst, hand motion is allowed between the notes in \nhorizontal cost. Second, if the direction of motion  is \nreversed, a crossover  occurs and difficulty increases.  \nIn our implementation we ignore the first of these \nproblems, and set c h=c v. In the case of a crossover \nsituation, however, c h is defined by a separate function. \nCrossovers involving the thumb are assigned lower c ost. \n2.4.  User Specification of Cost Functions \nIn order to generate fingerings tailored to a speci fic user, \nwe ask the user a series of questions about the dif ficulty \nof playing various intervals with particular pairs of \nfingers. The responses are used to define cost func tions \ncv and c h for each pair of fingers. Each cost function  has \nthe same general shape as the example in Figure 5, \nwhich represents all possible costs of stretches in volving \nfingers 2 and 5. In this example, the user has spec ified \nthe following information: \n• The smallest interval that is easily playable by \nfingers 2 and 5 is 4 half steps (smaller intervals result in \nuncomfortable compression of the hand). \n• The comfort zone lies between 4 and 7 half steps. \n• After eleven half steps the stretch is nearly \nimpossible. \n  \n \nFigure 5.  Cost function c v for finger pair 2,5 \n3.  RESULTS \nWe tested our system on several different music exc erpts \nand considering the simplicity of the approach we a re \npleased that this algorithm generates quite plausib le \npolyphonic piano fingerings. Additional improvement s \nwould include allowing finger substitution during a  \nsustained note and modelling the difference between  \nwhite and black keys. We also would like to extend the \nsystem to determine which hand (left or right) shou ld \nplay each note. See Figure 6 for sample output. \n4.  REFERENCES \n[1]  Hart, M., Bosch, R., & Tsai, E. (2000). “Finding \nOptimal Piano Fingerings.” The UMAP Journal, \n21 (2), 167-177. \n[2]  Lin, C. And Liu, D. “An Intelligent Virtual Piano \nTutor ”. Proceedings of VRCIA 2006 . 353-356. Hong \nKong, June 14-17, 2006. \n[3]  Parncutt, R., Sloboda, J. A., Clarke, E. F., Raekal lio, \nM., & Desain, P. (1997). “An Ergonomic Model of \nKeyboard Fingering for Melodic Fragments”. Music \nPerception, 14 (4) , 341-382. \n[4]  Radicioni et. al. “A segmentation-based prototype t o \ncompute string instruments fingering”. Proceedings \nof the Conference of Interdisciplinary Musicology \n(CIM04).  Graz, Austria, April 15-18, 2004. \n[5]  Sayegh, S. (1989). “Fingering for String Instrument s \nwith the Optimal Path Paradigm”. Computer Music \nJournal.  13 (3), 76-84. \n \n \nFigure 6.  System output: Excerpt from Ellmenreich’s Spinning  Song \nCost of Stretch Between Fingers 2 and 5 \n0246810 12 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \nDistance in Half-Steps Cost t"
    },
    {
        "title": "Search &amp; Select Intuitively Retrieving Music from Large Collections.",
        "author": [
            "Peter Knees"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416456",
        "url": "https://doi.org/10.5281/zenodo.1416456",
        "ee": "https://zenodo.org/records/1416456/files/Knees07.pdf",
        "abstract": "A retrieval system for large-scale collections that allows users to search for music using natural language queries and relevance feedback is presented. In contrast to existing music search engines that are either restricted to manually annotated meta-data or based on a query-by-example variant, the presented approach describes audio pieces via a traditional term vector model and allows therefore to retrieve relevant music pieces by issuing simple free-form text queries. Term vector descriptors for music pieces are derived by applying Web-based and audio-based similarity measures. Additionally, as the user selects music pieces that he/she likes, the subsequent results are adapted to accommodate to the user’s preferences. Real-world performance of the system is indicated by a small user study. 1 MOTIVATION AND CONTEXT The recent achievements of the MIR community have led to many innovative and possibly unconventional approaches to support users in finding desired music. Frequently, content-based analysis of the audio files or collaborative recommendations to point users to music they might like (e.g. [2]) are applied. Some approaches also incorporate information from different sources to build interactive interfaces (e.g. [6]). However, to retrieve music from large databases, many approaches rely on query-by-example methods where the query must consist of a piece of information that has a representation similar to the records in the database, e.g. in query-by-humming/singing systems. Since users are accustomed to text-based search engines, which have become the “natural” way to find and access all other types of content like images, videos, and text, query-by-example music search systems often lack broad acceptance. On the other hand, systems that offer textual queries, e.g. catalog search engines of commercial music re-sellers, permit only filtering based on attributes like artist, album, track name, year, or subjective labels like genre or style. To address some of these limitations, in [1], a system is presented that relies on a semantic ontology containing relations between meta-data and automatically extracted c⃝2007 Austrian Computer Society (OCG). acoustic properties and that can be queried via natural language phrases. In the retrieval process, textual queries must then be mapped to the semantic concepts. With this system, semantic queries like “something fast from...” or “something new from...” can be processed. However, as with the traditional systems, the cultural context of the indexed music pieces is ignored. For real-world applicability of a music search engine, it must in fact behave like a Web search engine like Google or Yahoo! and allow arbitrary queries like rock with great riffs or even melodic metal with opera singer as front woman. Our first steps into this direction can be found in [3]. By using Web-based features to describe music pieces in a collection, each piece can be represented by a term vector. Furthermore, audio-based similarity is incorporated to describe also those pieces for which no information can be found on the Web. Thus, we combine information about the context with information about the content. For retrieval, queries are sent to Google and the resulting Web pages are used to construct a query vector which can be compared to the term vectors of the pieces in the collection. In [4], we have modified this approach to include relevance feedback and to use a local Web page index for query vector construction instead of Google. In this work, the applicability of the proposed methods is demonstrated. The user can simply initiate the search for desired music by typing some descriptive terms. From the returned items, those after the user’s fancy are selected and transferred into a list of “harvested music pieces” (analogous to e.g. a shopping cart in an on-line shop). Based on the chosen music pieces, the consecutively presented results are modified such that they tend to contain more pieces similar to the ones in the “harvest list”. The user can continue searching by selecting (or ignoring) more results or by issuing the next query. 2 TECHNICAL FUNDAMENTALS In this section, we briefly review the technical basis of the presented application. Details can be found in [3, 4]. We derive track specific information from the Web by combining the results of three queries issued to Google:",
        "zenodo_id": 1416456,
        "dblp_key": "conf/ismir/Knees07",
        "keywords": [
            "natural language queries",
            "relevance feedback",
            "traditional term vector model",
            "audio pieces",
            "Web-based and audio-based similarity measures",
            "user preferences",
            "real-world performance",
            "small user study",
            "semantic ontology",
            "query-by-example methods"
        ],
        "content": "SEARCH & SELECT – INTUITIVELY RETRIEVING\nMUSIC FROM LARGE COLLECTIONS\nPeter Knees\nDepartment of Computational Perception\nJohannes Kepler University Linz, Austria\nABSTRACT\nA retrieval system for large-scale collections that allows\nusers to search for music using natural language queries\nand relevance feedback is presented. In contrast to exist-\ning music search engines that are either restricted to man-\nually annotated meta-data or based on a query-by-example\nvariant, the presented approach describes audio pieces via\na traditional term vector model and allows therefore to re-\ntrieve relevant music pieces by issuing simple free-form\ntext queries. Term vector descriptors for music pieces\nare derived by applying Web-based and audio-based sim-\nilarity measures. Additionally, as the user selects music\npieces that he/she likes, the subsequent results are adapte d\nto accommodate to the user’s preferences. Real-world per-\nformance of the system is indicated by a small user study.\n1 MOTIV ATION AND CONTEXT\nThe recent achievements of the MIR community have led\nto many innovative and possibly unconventional approach-\nes to support users in ﬁnding desired music. Frequently,\ncontent-based analysis of the audio ﬁles or collaborative\nrecommendations to point users to music they might like\n(e.g. [2]) are applied. Some approaches also incorporate\ninformation from different sources to build interactive in -\nterfaces (e.g. [6]).\nHowever, to retrieve music from large databases, many\napproaches rely on query-by-example methods where the\nquery must consist of a piece of information that has a rep-\nresentation similar to the records in the database, e.g. in\nquery-by-humming/singing systems. Since users are ac-\ncustomed to text-based search engines, which have be-\ncome the “natural” way to ﬁnd and access all other types\nof content like images, videos, and text, query-by-example\nmusic search systems often lack broad acceptance. On the\nother hand, systems that offer textual queries, e.g. cata-\nlog search engines of commercial music re-sellers, permit\nonly ﬁltering based on attributes like artist, album, track\nname, year, or subjective labels like genre or style.\nTo address some of these limitations, in [1], a system\nis presented that relies on a semantic ontology containing\nrelations between meta-data and automatically extracted\nc/circlecopyrt2007 Austrian Computer Society (OCG).acoustic properties and that can be queried via natural lan-\nguage phrases. In the retrieval process, textual queries\nmust then be mapped to the semantic concepts. With this\nsystem, semantic queries like “something fast from... ” or\n“something new from... ” can be processed. However, as\nwith the traditional systems, the cultural context of the in-\ndexed music pieces is ignored.\nFor real-world applicability of a music search engine, it\nmust in fact behave like a Web search engine like Google\nor Yahoo! and allow arbitrary queries like rock with great\nriffsor even melodic metal with opera singer as front wo-\nman. Our ﬁrst steps into this direction can be found in [3].\nBy using Web-based features to describe music pieces in a\ncollection, each piece can be represented by a term vector.\nFurthermore, audio-based similarity is incorporated to de -\nscribe also those pieces for which no information can be\nfound on the Web. Thus, we combine information about\nthecontext with information about the content . For re-\ntrieval, queries are sent to Google and the resulting Web\npages are used to construct a query vector which can be\ncompared to the term vectors of the pieces in the collec-\ntion. In [4], we have modiﬁed this approach to include\nrelevance feedback and to use a local Web page index for\nquery vector construction instead of Google.\nIn this work, the applicability of the proposed methods\nis demonstrated. The user can simply initiate the search\nfor desired music by typing some descriptive terms. From\nthe returned items, those after the user’s fancy are selecte d\nand transferred into a list of “harvested music pieces” (ana l-\nogous to e.g. a shopping cart in an on-line shop). Based\non the chosen music pieces, the consecutively presented\nresults are modiﬁed such that they tend to contain more\npieces similar to the ones in the “harvest list”. The user\ncan continue searching by selecting (or ignoring) more re-\nsults or by issuing the next query.\n2 TECHNICAL FUNDAMENTALS\nIn this section, we brieﬂy review the technical basis of the\npresented application. Details can be found in [3, 4].\nWe derive track speciﬁc information from the Web by\ncombining the results of three queries issued to Google:\n1. “artist ” music\n2. “artist ” “album ” music review\n3. “artist ” “title” music review -lyricsFor each query, at most 100 of the top-ranked Web\npages are retrieved and joined into a single set. All re-\ntrieved pages are cleaned from HTML tags and stop words\nin six languages. From each track’s set, a modiﬁed tf×idf\nrepresentation is calculated.\nTo complement context-based features with informa-\ntion on the content of the music, Single Gaussian MFCC\ndistribution models are calculated for each track [5]. Sinc e\nthe Kullback-Leibler divergence, which is applied for sim-\nilarity computation of the models, has some undesirable\nconsequences, a rank-based correction called Proximity\nVeriﬁcation is used for post-processing [7].\nThe usage of track features from two distinct sources\ncan be used (a)to reduce the dimensionality of the term\nvector space (via a modiﬁed χ2test), (b)to emphasize\nterms that occur frequently among similar sounding pieces,\nand – most important – (c)to describe music pieces with\nno (or few) associated information present on the Web.\nThe last two points are achieved by performing a Gaus-\nsian weighting over the 10 acoustically nearest neighbors’\nterm vectors.\nGiven a query, a vector space representation is con-\nstructed of the 20 most relevant Web pages from the set\nof retrieved pages, i.e. the pages used to construct the\ntrack-speciﬁc term vectors. Using the obtained query vec-\ntor, distances to all tracks in the collection are calculate d\nand ranked. The top results are then returned to the user.\nSelection of tracks marks them as relevant and results in\na re-weighting of the query vector toward the relevant and\naway from the non-relevant pieces. This is accomplished\nby incorporating Rocchio’s relevance feedback method [8].\n3 USER STUDY\nA small user study with 11 participants was conducted\nto get an impression of the retrieval application’s perfor-\nmance. To this end, each participant was asked to submit\n5 queries of choice to the system. Thus, in total, 55 differ-\nent queries have been issued by the participants. For each\nquery, 100 results were presented in groups of 20; selec-\ntion of tracks inﬂuenced the next 20 results. From the set\nof issued queries, 6 basic types of queries could be identi-\nﬁed. Since searching for lyrics is currently not supported,\nqueries addressing lyrics are not included.\n4 RESULTS AND CONCLUSIONS\nTable 1 shows the different categories as well as the num-\nber of queries belonging to these categories. Note that\na query can be assigned to multiple categories (e.g. vi-\nenna electro dj orrammstein music with strong keyboard ).\nWorst results can be observed for queries that are aimed\nto retrieve a speciﬁc track. Although the user may select\ntracks other than the one speciﬁed, naturally the number is\nvery low. Furthermore, it can be seen that users are most\nsatisﬁed with results for genre queries (e.g. eurodance )\nand geographically related queries (e.g. new orleans ).category # queries avg. relevant\ngenre 28 33.25\nartist 12 28.50\ninstrumentation 7 24.71\ntrack 6 2.50\ngeographical 5 33.00\nmovie related 4 16.00\nother 3 23.33\ntotal 55 29.75\nTable 1 . Identiﬁed query categories, the number of\nqueries belonging to these categories, and the average\nnumber of relevant music pieces (out of 100).\n5 ACKNOWLEDGMENTS\nThis research is supported by the Austrian Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung (FWF project\nnumber L112-N04) and the Vienna Science and Technol-\nogy Fund (WWTF project CIO10 “Interfaces to Music”).\n6 REFERENCES\n[1] S. Baumann, A. Kl ¨uter, and M. Norlien. Using nat-\nural language input and audio analysis for a human-\noriented MIR system. In Proc. 2nd WEDELMUSIC ,\nDarmstadt, Germany, 2002.\n[2] O. Celma, M. Ramirez, and P. Herrera. Foaﬁng the\nmusic: A music recommendation system based on\nRSS feeds and user preferences. In Proc. 6th ISMIR ,\nLondon, UK, 2005.\n[3] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A Mu-\nsic Search Engine Built upon Audio-based and Web-\nbased Similarity Measures. In Proc. 30th ACM SIGIR ,\nAmsterdam, the Netherlands, 2007.\n[4] P. Knees and G. Widmer. Searching for Music Using\nNatural Language Queries and Relevance Feedback.\nInProc. 5th Int’l Workshop on Adaptive Multimedia\nRetrieval , Paris, France, 2007.\n[5] M. Mandel and D. Ellis. Song-Level Features and\nSupport Vector Machines for Music Classiﬁcation. In\nProc. 6th ISMIR , London, UK, 2005.\n[6] E. Pampalk and M. Goto. Musicrainbow: A new user\ninterface to discover artists using audio-based similar-\nity and web-based labeling. In Proc. 7th ISMIR , Vic-\ntoria, Canada, 2006.\n[7] T. Pohle, P. Knees, M. Schedl, and G. Widmer. Au-\ntomatically Adapting the Structure of Audio Similar-\nity Spaces. In Proc. 1st Workshop on Learning the Se-\nmantics of Audio Signals , Athens, Greece, 2006.\n[8] J. J. Rocchio. Relevance feedback in information re-\ntrieval. In G. Salton, editor, The SMART Retrieval Sys-\ntem – Experiments in Automatic Document Process-\ning. Prentice Hall, 1971."
    },
    {
        "title": "Towards Musicdiff: A Foundation for Improved Optical Music Recognition Using Multiple Recognizers.",
        "author": [
            "Ian Knopke",
            "Donald Byrd"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416522",
        "url": "https://doi.org/10.5281/zenodo.1416522",
        "ee": "https://zenodo.org/records/1416522/files/KnopkeB07.pdf",
        "abstract": "This paper presents work towards a “musicdiff” program for comparing files representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and others strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basic methodology requires several stages: documents must be scanned and submitted to several OMR programs, programs whose strengths and weaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimentary error correction. We also describe a visualization tool for comparing multiple versions on a measure-by-measure basis. 1 INTRODUCTION This paper describes work on “musicdiff” program for music notation. There are many potential applications for such a program, just as there are for the text-file comparison programs that are ubiquitous these days. However, the one we are most interested in here is for use in an “engine” for a multiple-recognizer Optical Music Recognition (MROMR) system of the type described by Byrd and Schindele (2006). In fact, their work was part of a feasibility study for our MeTAMuSE project. A large part of the musicdiff problem is alignment, where the musical documents to be aligned can be expected to be relatively similar. In particular, they should have exactly the same structure, except in terms of details. So, if sections A and B in one version appear in reverse order in the other, ideally, the program would detect that; but if it doesn’t and instead says a section A before B was deleted and a new section (in fact, A) was inserted after it, it is not a serious problem. The first well-known UNIX diff program dates back to the seventies (Hunt and McIlroy, 1976), and many variations have been created since. Some do line-level comparisons, some do character-level, and some, e.g., Microsoft c⃝2007 Austrian Computer Society (OCG). Word, do both. The rough analogue of lines in this context is probably measures. But a measure can contain a lot of information. For direct use by people, something with finer granularity is highly desirable; for use in a MROMR engine, it is absolutely essential. For MROMR, we need to compare music-notation files generated by different programs. As of this writing, MusicXML is far and away the best choice because of its widespread support: we will soon say more about this. While we are most interested in a fully-automatic musicdiff as a component of an MROMR engine, we believe it also has great promise for interactive use by musicians. We shall say more about this application at the end of the paper. 2 MUSICXML AS AN OMR COMPARISON FORMAT Clearly a diff program requires its input files to be in formats it can compare. Until the recent widespread adoption of MusicXML for programs handling music notation, in practical terms, such formats did not exist. Now, with an intermediate conversion step, it is possible to write MusicXML from just about any OMR program, and to read it for editing or error-checking with most notation programs. XML is rapidly becoming one of the most prominent formats for data interchange, especially in the context of the Internet. An XML document can specify a format it uses; document formats are defined by specifications like the Document Type Definition (DTD). A DTD specifies the available element types, the relationships between them, and the attributes and ranges of values for elements and attributes. MusicXML is an XML document format, defined by its DTD. It takes its inspiration primarily from the MuseData (Selfridge-Field, 3 4) and Humdrum music encodings (Huron, 1997). MusicXML seeks to create a common interchange format between programs that use symbolic music data. The adoption of MusicXML by many popular notation programs has accelerated its usage significantly, to the point where it is showing signs of becoming a de facto standard. The nested-element layout of XML files is best represented as a tree data structure. Tree structures and tree \\ \\ \\ \\ \\ l KKK ö \\ \\ \\  \\ \\ KKK ö \\ l \\ \\ \\ \\ \\ Figure 1. Correct and Incorrect Encoding comparisons are one of the most-studied problems in computer science, so it would seem on the surface that creating a “diff” program for MusicXML documents would be easy, especially since there has been a fair amount of research specifically on comparing XML (Cobena et al., 2001). However, this assumption is incorrect. In all, three types of comparison difficulties can be identified:",
        "zenodo_id": 1416522,
        "dblp_key": "conf/ismir/KnopkeB07",
        "keywords": [
            "musicdiff",
            "music notation",
            "multiple recognizers",
            "Optical Music Recognition (OMR)",
            "engine for MROMR",
            "alignment",
            "MusicXML",
            "XML",
            "diff program",
            "MusicXML format"
        ],
        "content": "TOWARDSMUSICDIFF:AFOUNDATIONFORIMPROVEDOPTICAL\nMUSICRECOGNITIONUSINGMULTIPLERECOGNIZERS\nIanKnopke\nMusic Informatics Program\nSchool of Informatics\nIndiana University\niknopke@indiana.eduDonaldByrd\nSchool of Informatics &\nJacobs School of Music\nIndiana University\ndonbyrd@indiana.edu\nABSTRACT\nThis paper presents work towards a “musicdiff” program\nfor comparing ﬁles representing different versions of the\nsame piece, primarily in the context of comparing versions\nproduced by different optical music recognition (OMR)\nprograms. Previous work by the current authors and oth-\ners strongly suggests that using multiple recognizers will\nmake it possible to improve OMR accuracy substantially.\nThe basic methodology requires several stages: documents\nmust be scanned and submitted to several OMR programs,\nprograms whose strengths and weaknesses have previously\nbeen evaluated in detail. We discuss techniques we have\nimplemented for normalization, alignment and rudimen-\ntary error correction. We also describe a visualization too l\nfor comparing multiple versions on a measure-by-measure\nbasis.\n1 INTRODUCTION\nThis paper describes work on “musicdiff” program for\nmusic notation. There are many potential applications for\nsuch a program, just as there are for the text-ﬁle compar-\nison programs that are ubiquitous these days. However,\nthe one we are most interested in here is for use in an\n“engine” for a multiple-recognizer Optical Music Recog-\nnition (MROMR) system of the type described by Byrd\nand Schindele (2006). In fact, their work was part of a\nfeasibility study for our MeTAMuSE project.\nA large part of the musicdiff problem is alignment,\nwhere the musical documents to be aligned can be ex-\npected to be relatively similar. In particular, they should\nhave exactly the same structure, except in terms of details.\nSo, if sections A and B in one version appear in reverse\norder in the other, ideally, the program would detect that;\nbut if it doesn’t and instead says a section A before B was\ndeleted and a new section (in fact, A) was inserted after it,\nit is not a serious problem.\nThe ﬁrst well-known UNIX diff program dates back to\nthe seventies (Hunt and McIlroy, 1976), and many varia-\ntions have been created since. Some do line-level compar-\nisons, some do character-level, and some, e.g., Microsoft\nc/circlecopyrt2007 Austrian Computer Society (OCG).Word, do both. The rough analogue of lines in this con-\ntext is probably measures. But a measure can contain a lot\nof information. For direct use by people, something with\nﬁner granularity is highly desirable; for use in a MROMR\nengine, it is absolutely essential.\nFor MROMR, we need to compare music-notation ﬁles\ngenerated by different programs. As of this writing, Mu-\nsicXML is far and away the best choice because of its\nwidespread support: we will soon say more about this.\nWhile we are most interested in a fully-automatic mu-\nsicdiff as a component of an MROMR engine, we believe\nit also has great promise for interactive use by musicians.\nWe shall say more about this application at the end of the\npaper.\n2 MUSICXMLASANOMRCOMPARISON\nFORMAT\nClearly a diff program requires its input ﬁles to be in for-\nmats it can compare. Until the recent widespread adoption\nof MusicXML for programs handling music notation, in\npractical terms, such formats did not exist. Now, with an\nintermediate conversion step, it is possible to write Mu-\nsicXML from just about any OMR program, and to read it\nfor editing or error-checking with most notation programs.\nXML is rapidly becoming one of the most prominent\nformats for data interchange, especially in the context of\nthe Internet. An XML document can specify a format\nit uses; document formats are deﬁned by speciﬁcations\nlike the Document Type Deﬁnition (DTD). A DTD speci-\nﬁes the available element types, the relationships between\nthem, and the attributes and ranges of values for elements\nand attributes.\nMusicXML is an XML document format, deﬁned by\nits DTD. It takes its inspiration primarily from the Muse-\nData (Selfridge-Field, 3 4) and Humdrum music encod-\nings (Huron, 1997). MusicXML seeks to create a common\ninterchange format between programs that use symbolic\nmusic data. The adoption of MusicXML by many popular\nnotation programs has accelerated its usage signiﬁcantly,\nto the point where it is showing signs of becoming a de\nfacto standard.\nThe nested-element layout of XML ﬁles is best rep-\nresented as a tree data structure. Tree structures and tree/noteheads.s2/noteheads.s2 /noteheads.s2 /noteheads.s2 /noteheads.s2 /clefs.G/accidentals.2/accidentals.2/accidentals.2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2\n/accidentals.0/noteheads.s2 /accidentals.0/noteheads.s2/accidentals.2/accidentals.2/accidentals.2/timesig.C44 /noteheads.s2/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2 /noteheads.s2\nFigure1 . Correct and Incorrect Encoding\ncomparisons are one of the most-studied problems in com-\nputer science, so it would seem on the surface that cre-\nating a “diff” program for MusicXML documents would\nbe easy, especially since there has been a fair amount of\nresearch speciﬁcally on comparing XML (Cobena et al.,\n2001). However, this assumption is incorrect. In all, three\ntypes of comparison difﬁculties can be identiﬁed:\n1. Stylistic differences in MusicXML coding\n2. Errors as a product of the OMR procedure\n3. Errors in interpreting the MusicXML standard\nMusicXML is fairly well-deﬁned, but does allow con-\nsiderable latitude in the use of optional elements and at-\ntributes. These differences in encoding “style” make di-\nrect comparison of XML trees unworkable. For instance,\nMusicXML has MIDI elements, but they may or may not\nbe included in a ﬁle, regardless of the notational informa-\ntion. In effect, overlaid on every MusicXML ﬁle is a sec-\nond structure that in many situations does not map espe-\ncially well onto the underlying XML. Another problem is\nthat many XML tools, including the standard XSLT tools,\nare based around a stateless data processing model, which\nmay be difﬁcult or impossible to use for many MusicXML\ntasks, such as those involving its extremely general “for-\nward” and “backward” elements, or items such as slurs\nthat stretch between branches.\nA more subtle set of errors occurs in some MusicXML\nﬁles from misinterpretations of the MusicXML DTD. One\nof the most alarming examples occurred in the case of one\nprogram writing pitches as they are notated, instead of\nhow they sound as MusicXML requires. This would re-\nsult in measures where the accidental of a note is indicated\non the ﬁrst occurrence of a pitch, but is incorrect through-\nout the remaineder of the measure. Figure 1 shows both\nthe correct and incorrect encodings. While this problem\nhas been ﬁxed in the most recent version of this program,\nother interpretational errors remain. An ad-hoc survey of\nMusicXML documents produced by other programs, in-\ncluding many open source programs, revealed a host of\nother interpretational issues. In defense of MusicXML, it\nshould be noted that its authors support an active mailing\nlist and are working to resolve many such problems within\nthe music notation community as a whole, and there have\nbeen many improvements even within the life of this proj-\nect. However, anyone seeking to use MusicXML as an in-\nterchange format at the time of this writing should resign\nthemselves to a certain amount of experimenting. As an\nindication of the extent of the problem, counts of certain\nelements from the feasibility study document collection\nare given in Table 1; note the many discrepancies.Table1 . MusicXML element counts from the feasibility\nstudy documents\nElement PhotoScoreSharpEyeSmartScore\naccidental 394 452 471\nalter 260 1400 1432\nbackup 16 6 32\nbarline 28 30 21\nbeam 6809 6937 6485\nduration 5530 5588 5396\ndynamics 85 165 121\nfermata 0 5 18\nﬁfths 153 30 36\nforward 1 13 24\nkey 153 30 36\nmeasure 869 868 895\nmidi-instrument 20 0 21\npitch 5016 5158 4888\nslur 1105 1366 846\nstem 5016 5106 4843\ntie 132 136 226\ntuplet 48 8 2\nOMR OMR\nAlignment\nCombine using rules\nFinal check stageOMR\nMusicXML conversionScan in score\nFigure2 . Scanning and document correction procedure\n3 METHODOLOGY\nOur methodology is based on the assumption that different\nOMR programs have different strengths and weaknesses,\nand that we can determine what they are and take advan-\ntage of that knowledge to improve recognition rates. The\nbasic procedure is described in Figure 2. The process can\nbe described as follows: a score is scanned in at a suit-\nable resolution, normally 300 dpi, and then converted to\nMusicXML using several different OMR programs. The\nresults are normalized and aligned at the measure level.\nAll versions of each measure are then compared, and rules\nfor combining them so as to minimize errors are applied,\nbased on the situation. Finally, an optional check stage\nmay be used; this could involve human checking or an au-\ntomated heuristic of some sort.\nAt the moment, the procedure of scanning scores is be-\ning done by hand. However, we are experimenting with\ncode for mouse gesture control that, in combination with\nsome other scripting, aims to automate the entire process.\nThe core of the process is “combining using rules”, the\nerror correction stage. This requires an “error map” ofthe strengths and weaknesses of each program; such maps\nwere manually created, albeit in a rudimentary form, in\nthe original feasibility study for this project (Byrd and\nSchindele, 2006). Unfortunately, ours is a “moving tar-\nget”: new versions of programs appear regularly, and dif-\nferent combinations of programs may be available to dif-\nferent users of the ﬁnal system. What is required is a sys-\ntem forautomatically creating such an error map. Our\napproach to this is to use a set of “regression” test ﬁles\nfor which we have both scans and ground truth ﬁles. A\ncomparison of what the output should be and what it is for\nthese ﬁles can then be used to map out likely errors.\nFor these purposes, two test collections have been cre-\nated. The ﬁrst is the original collection of pieces from the\nfeasibility study by Byrd and Schindele (2006). Most of\nthese are normal selections from the classical canon, and\nas such have many articulations, editorial marks, rehearsa l\nnumbers, and other markings that occur in common prac-\ntice notation. However, the presence of these additional\nsymbols makes the job of the OMR programs more difﬁ-\ncult because of the increased opportunities for confusion\nbetween symbols. Consequently, for developmental pur-\nposes, we were interested in creating additional test ma-\nterial with very little other than notes, barlines, clefs, a nd\nkey signatures. As an initial step, a second data set was\ncreated from a collection of unaccompanied recorder mu-\nsic (Van Eyck, 1986) consisting of 62 brief pieces. Three\ndifferent OMR programs were used for these tests: Photo-\nScore, SharpEye, and SmartScore. These are occasionally\ndesignated in the text as ps,se, andssrespectively.\n3.1 Normalization\nNormalization is required because MusicXML supports\nmultiple ways to encode the same content. Our normal-\nization stage consists primarily of converting all duratio ns\nto a common timebase. We also make default values ex-\nplicit, and we put simultaneous MusicXML elements like\nnotes in a chord into a standard order.\n3.2 Alignment\nMany of the OMR errors we have seen are missing bar-\nlines, or misinterpreting them for other vertical structur es\nlike note stems or vice-versa. The result is missing or ex-\ntra measures, a serious problem because proper alignment\nof measures is essential before any kind of comparison\ncan be undertaken.\nOur ﬁrst attempt to solve this problem computed a num-\nber summarizing the content of each measure, then used\nthese numbers to ﬁnd the proper alignment of measures.\nWhile partially successful, we found cases where each\nprogram made slightly different mistakes in a series of\nmeasures, producing a string of slightly different scores;\nas a result, the algorithm tends to get lost and produce im-\nproper alignments.\nOur current alignment system uses edit distance with\na dynamic programming algorithm (Durbin et al., 1998;(i,j,k)(i,j−1,k)(i,j−1,k−1)(i,j,k−1)(i−1,j−1,k) (i−1,j,k)(i−1,j,k−1)(i−1,j−1,k−1)\nFigure3 . Three-dimensional global scoring “cube”\nGusﬁeld, 1997) that is heavily weighted to preserving mea-\nsure alignment. This is similar to many other systems used\nfor computing music similarity (Smith et al., 1998), except\nthat we align three (or perhaps more) note streams at the\nsame time. Instead of the normal two dimensions, here\nthe alignment for both the local and global matrices oc-\ncurs in a multi-dimensional space, with the output of each\nprogram assigned to a different axis.\nEach value in the local score matrix is computed as\nfollows:\np(i,j,k) = [avg(pi+pj+pk)−min(pi+pj+pk)](1)\nd(i,j,k) = [avg(di+dj+dk)−min(di+dj+dk)](2)\nl(i,j,k) =q×p(i,j,k) + (1−q)×d(i,j,k); (3)\nwhere pis pitch and dis duration. qis an adjustment\nfactor that controls the weighting between the pitch and\nduration components: a value of 0 pays attention to pitch\nalone, 1 to duration alone.\nA two-dimensional dynamic programming algorithm\nconsiders the insertion, deletion, and substitution costs be-\ntween any pair of notes in the two melodies. Each step can\nbe thought of as involving four points at the corners of a\nsquare, producing an answer in the lower right from scores\ncalculated for the other three. In the three-dimensional\ncase, the square becomes a cube, requiring the calcula-\ntion of seven scores instead of three, as shown in Figure\n3. Additionally, we assign a high cost to substitutions\nbetween a barline and a note, which essentially prevents\nnote/barline mismatches. For each token, this provides a\ncollection of eight values; the minimum is taken and as-\nsigned to the global score matrix.\nThis algorithm gives signiﬁcantly better results in our\ntests so far. In practice, we have found that higher qvalues\ntend to produce better results. This appears to be because\nmistakes in pitch (besides missing accidentals) are less\ncommon than duration errors with OMR; furthermore, in\nour experience, pitch errors are almost always the resultof missing notes entirely, which also produces duration\nerrors.\nOne problem with the above algorithm is that each ad-\nditional stream to align doubles the amount of work needed,\nso it does not scale well in this respect. However, it should\nstill perform acceptably with, say, ﬁve versions, which is\nprobably near the practical limit anyway.\n3.3 ErrorCorrection\nOnce proper measure alignment has been achieved, cor-\nrective rules must be applied on a case by case basis to\nany errors. For this purpose, we deﬁne an error as any\nsituation where the the programs do not all agree.\nErrors can be corrected in one of several ways. The\nprogram can simply decide to disregard the output of some\nof the programs in that circumstance. In situations where\nall programs disagree the context becomes more impor-\ntant, and actual corrective code must be applied.\nThere is much work to do in this area, and this system\nis still quite rudimentary. Nevertheless, our initial expe r-\niments show that even a small set of corrections and se-\nlection procedures can greatly improve the quality of the\noutput.\n3.4 DocumentVisualizationandNon-OMRApplica-\ntions\nWe have created a tool that can convert multiple aligned\nMusicXML fragments to traditional notation for easy vi-\nsual inspection. The output format is images embedded\nin a standard HTML page stored on a web server. Our\ntool also provides MIDI playback and offers access to the\nMusicXML used to produce the notation.\nTurning to the question of using music interactively\nrather than as part of an MROMR engine, the work on an\n“Electronic Variorum Edition” of Don Quixote described\nby Kochumman et al. (2004) might serve as a model. They\nhave written a standalone multisource editor (designed for\nscholars) and a web-based “virtual edition” viewer (for\nreaders). In music, as far as we know, nothing similar has\nbeen done with symbolic comparison, but quite a bit via\nsuperimposing one (semi-transparent) image on a similar\none and letting the user ﬁnd any differences. For exam-\nple, the Online Chopin Variorium Edition OCVE (2007)\nis relying solely on image superimposition so far, but their\npilot-project ﬁnal report shows (and discussion with prin-\ncipals of the project supports this) that they appreciate th e\nadvantages of symbolic comparison. Unfortunately, prop-\nerly comparing music as complex as Chopin’s is well be-\nyond what we expect to accomplish in the near future.\n4 CONCLUSIONS\nAt present, we have a prototype system that uses multiple\nOMR recognizers to produce a composite document that\nis superior to the output of any one of the programs. While\nit has not been tested much thus far, we believe it has the\npotential to lead to considerably better OMR systems. Aswe were completing this paper, we learned of a new ver-\nsion of PhotoScore (“Ultimate 5”) that apparently utilizes\ntwo recognizers. This both supports our intuitions about\nMROMR and gives us an opportunity to evaluate the con-\ncept in a context more limited than what we envision.\n5 ACKNOWLEDGEMENTS\nWe would like to acknowledge the support of Don Waters\nand Suzanne Lodato the Andrew W. Mellon Foundation.\nWithout their support this work would have been difﬁcult,\nif not impossible. We would also like to thank Geraint\nWiggins, Tim Crawford, and Chris Raphael for their help,\nconversation, and the occasional brain transplant. Addi-\ntional thanks to Bill Guerin for technical support.\nReferences\nByrd, D. and Schindele, M. (2006). Prospects for improv-\ning optical music recognition with multiple recogniz-\ners. InProceedingsoftheInternationalConferenceon\nMusicInformationRetrieval , pages 41–6.\nCobena, G., Abiteboul, S., and Marian, A. (2001). Detect-\ning changes in XML documents. In BDA .\nDurbin, R., Eddy, S., Krogh, A., and Mitchison, G.\n(1998).BiologicalSequenceAnalysis: Probabilistic\nModelsofProteinsandNucleicAcids . Cambridge Uni-\nversity Press.\nGusﬁeld, D. (1997). AlgorithmsonStrings,Trees,and\nSequences . Cambridge University Press.\nHunt, J. W. and McIlroy, M. (1976). An algorithm for dif-\nferential ﬁle comparison. Technical Report 41, Com-\nputing Science Technical Report, Bell Laboratories.\nHuron, D. (1997). HumdrumandKern: Selectivefea-\ntureencoding , pages 375–401. MIT Press, Cambridge,\nMass.\nKochumman, R., Monroy, C., Deng, J., Furuta, R., and\nUrbina, E. (2004). Tools for a new generation of schol-\narly edition uniﬁed by a TEI-based interchange format.\nInProceedingsofthe2004JointACM/IEEEConfer-\nenceonDigitalLibraries , pages 368–9.\nOCVE (2007). Online Chopin Variorum Edition. http:\n//www.ocve.org.uk/ .\nSelfridge-Field, E. (1993–4). The MuseData universe: A\nsystem of musical information. ComputinginMusicol-\nogy, (9):9–30.\nSmith, L., McNab, R. J., and Witten, I. H. (1998).\nSequence-based melodic comparison: A dynamic-\nprogramming approach. ComputinginMusicology ,\n(11):101–18.\nVan Eyck (1986). Der Fluyten Lust hof, Volume I. New\nVellekoop Edition."
    },
    {
        "title": "Towards Integration of MIR and Folk Song Research.",
        "author": [
            "Peter van Kranenburg",
            "Jörg Garbers",
            "Anja Volk",
            "Frans Wiering",
            "Louis P. Grijp",
            "Remco C. Veltkamp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414754",
        "url": "https://doi.org/10.5281/zenodo.1414754",
        "ee": "https://zenodo.org/records/1414754/files/KranenburgGVWGV07.pdf",
        "abstract": "Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other. Computational methods can support the study of the contents of these collections. Music Information Retrieval (MIR) research provides such methods. Yet a fruitful cooperation of both disciplines is difficult to achieve. We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR. 1 INTRODUCTION The goal of the WITCHCRAFT project (“What Is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes”) is to develop a content-based retrieval system for folk song melodies stored as audio and notation. This system will give access to the collection of folksong recordings and transcriptions of the Meertens Institute (a research institute for Dutch language and culture in Amsterdam). Its purposes are on the one hand to support Folk Song Research (FSR) 1 in classifying and identifying variants of folk songs and on the other hand to allow the general public to search for melodic content in the database of the Meertens Institute. In the current paper we focus on the former purpose. ‘Folk songs’ were sung by common people during work or social activities. One of their most important characteristics is that they are part of oral culture. The melodies and the texts are learned by imitation and participation rather than from books. In the course of this oral transmission, changes occur to the melodies. The resulting set of variants of a song form a so called ‘tune family’. Although attention has been paid to folk songs in the Music Information Retrieval (MIR) community, 2 very few studies focus on the particularities of orally transmitted melodies. In most cases folk songs were simply used because they were available as a test collection. As folk song melody research belongs to the domain of ethnomusicology, serious attempts to build software for process1 The equivalent of the German ‘Volksliedkunde’. 2 In this paper ‘MIR’ is taken in a very broad sense: not only specific retrieval research, but also other research that has computational processing of music as its subject. c⃝2007 Austrian Computer Society (OCG). ing folk song melodies should model concepts and methods that were developed in ethnomusicology. But this is not yet standard practice. Major impediments for fruitful collaboration are the unfamiliarity of researchers in both fields with each other’s methods and traditions, and the non-formalised nature of FSR concepts and theories. Therefore we need to find an approach to bridge this gap. In this paper we first give overviews of relevant work that has been done in both disciplines, and after that we describe an approach that may lead to better cooperation. 2 FOLK SONGS IN MIR Only a limited number of MIR applications and studies are specifically aimed at searching folk song collections. Some online search engines allow the user to search in a large collection of folk song melodies. The Danish Folklore Archives [3] and the Digital Archive of Finnish Folk Tunes [6] are primarily meant for folk songs, while engines like Themefinder [19], MELDEX [12] and Musipedia [13] have a more general scope. Only the Danish search engine posesses a query method that is motivated by FSR. One can search for a sequence of accented notes, which are assumed to be rather stable across variants of a melody. Folk song melodies have been used as data in a considerable number of MIR studies. In some cases folk songs were chosen because of their availability and not because of an interest in folk music as such. This applies to all eight papers in the complete ISMIR proceedings from 2001–2006 that employ the Essen folk song collection [16]. In none of these papers the implications of the choice of this data set is discussed. In most cases it is simply stated that this collection is used, or a pragmatic reason is provided, e.g., the need for a large music database, or the need for a collection of monophonic songs. The results of the more general questions addressed, such as meter classification, benchmark establishing or segmentation, have not been interpreted concerning their potential to contribute to folk song research. In broader MIR circles some more studies have been done that particularly focus on folk songs. The work of Zolt´an Juh´asz [10] is highly relevant in this respect. He selects his algorithms for their ability to answer questions about the data (mainly Hungarian folk song melodies) instead of employing the data to answer questions about his algorithms. By clustering contour representations of the melodies in various ways, his studies reveal differences between oral traditions in various countries. Another relevant publication is an article by David Huron in which he proposes to visualize geographic differences in music by showing densities on a map [9]. As an example, using the Essen collection, Huron visualizes the geographic density of certain types of cadence notes, showing that Western European songs are nearly three times as likely to have most of their phrases end on a note other than the tonic compared to Eastern European songs. Of the mentioned papers only those of Juh´asz explicitly state an interest in folk songs as part of oral traditions. 3 SOME PAST AND CURRENT APPROACHES TO MELODY IN FOLK SONG RESEARCH During the last century, the availability of collected folk song tunes has generated a considerable amount of musicological research. One of the primary concerns is how to deal with the specific type of variation caused by the process of oral transmission. Therefore we will first discuss oral transmission. Then classification and identification of melodies in the context of FSR will be discussed.",
        "zenodo_id": 1414754,
        "dblp_key": "conf/ismir/KranenburgGVWGV07",
        "keywords": [
            "Folk song research",
            "Computational methods",
            "Music Information Retrieval",
            "Ethnomusicology",
            "Oral culture",
            "Melodic content",
            "Folk song melodies",
            "Content-based retrieval",
            "Ethnomusicology",
            "Oral traditions"
        ],
        "content": "TOWARDS INTEGRATION OF MIR AND FOLK SONG RESEARCH\nPeter van Kranenburg, J ¨org Garbers, Anja Volk, Frans Wiering, Louis Grijp∗, Remco C. Veltkamp\nUtrecht University and∗Meertens Institute, Amsterdam\npetervk@cs.uu.nl\nABSTRACT\nFolk song research (FSR) often deals with large collec-\ntions of tunes that have various types of relations to each\nother. Computational methods can support the study of\nthe contents of these collections. Music Information Re-\ntrieval (MIR) research provides such methods. Yet a fruit-\nful cooperation of both disciplines is difﬁcult to achieve.\nWe present a role-model to structure this cooperation in\nwhich tasks and responsibilities are distributed among the\nroles of MIR, Computational Musicology (CM) and FSR.\n1 INTRODUCTION\nThe goal of the WITCHCRAFT project (“What Is Topi-\ncal in Cultural Heritage: Content-based Retrieval Among\nFolksong Tunes”) is to develop a content-based retrieval\nsystem for folk song melodies stored as audio and no-\ntation. This system will give access to the collection of\nfolksong recordings and transcriptions of the Meertens In-\nstitute (a research institute for Dutch language and culture\nin Amsterdam). Its purposes are on the one hand to sup-\nport Folk Song Research (FSR)1in classifying and iden-\ntifying variants of folk songs and on the other hand to al-\nlow the general public to search for melodic content in the\ndatabase of the Meertens Institute. In the current paper we\nfocus on the former purpose.\n‘Folk songs’ were sung by common people during\nwork or social activities. One of their most important\ncharacteristics is that they are part of oral culture. The\nmelodies and the texts are learned by imitation and partic-\nipation rather than from books. In the course of this oral\ntransmission, changes occur to the melodies. The result-\ning set of variants of a song form a so called ‘tune family’.\nAlthough attention has been paid to folk songs in the\nMusic Information Retrieval (MIR) community,2very\nfew studies focus on the particularities of orally transmit-\nted melodies. In most cases folk songs were simply used\nbecause they were available as a test collection. As folk\nsong melody research belongs to the domain of ethnomu-\nsicology, serious attempts to build software for process-\n1The equivalent of the German ‘V olksliedkunde’.\n2In this paper ‘MIR’ is taken in a very broad sense: not only spe-\nciﬁc retrieval research, but also other research that has computational\nprocessing of music as its subject.\nc/circlecopyrt2007 Austrian Computer Society (OCG).ing folk song melodies should model concepts and meth-\nods that were developed in ethnomusicology. But this is\nnot yet standard practice. Major impediments for fruit-\nful collaboration are the unfamiliarity of researchers in\nboth ﬁelds with each other’s methods and traditions, and\nthe non-formalised nature of FSR concepts and theories.\nTherefore we need to ﬁnd an approach to bridge this gap.\nIn this paper we ﬁrst give overviews of relevant work that\nhas been done in both disciplines, and after that we de-\nscribe an approach that may lead to better cooperation.\n2 FOLK SONGS IN MIR\nOnly a limited number of MIR applications and studies\nare speciﬁcally aimed at searching folk song collections.\nSome online search engines allow the user to search in a\nlarge collection of folk song melodies. The Danish Folk-\nlore Archives [3] and the Digital Archive of Finnish Folk\nTunes [6] are primarily meant for folk songs, while en-\ngines like Themeﬁnder [19], MELDEX [12] and Musi-\npedia [13] have a more general scope. Only the Danish\nsearch engine posesses a query method that is motivated\nby FSR. One can search for a sequence of accented notes,\nwhich are assumed to be rather stable across variants of a\nmelody.\nFolk song melodies have been used as data in a con-\nsiderable number of MIR studies. In some cases folk\nsongs were chosen because of their availability and not\nbecause of an interest in folk music as such. This applies\nto all eight papers in the complete ISMIR proceedings\nfrom 2001–2006 that employ the Essen folk song collec-\ntion [16]. In none of these papers the implications of the\nchoice of this data set is discussed. In most cases it is sim-\nply stated that this collection is used, or a pragmatic rea-\nson is provided, e.g., the need for a large music database,\nor the need for a collection of monophonic songs. The\nresults of the more general questions addressed, such as\nmeter classiﬁcation, benchmark establishing or segmenta-\ntion, have not been interpreted concerning their potential\nto contribute to folk song research.\nIn broader MIR circles some more studies have been\ndone that particularly focus on folk songs. The work of\nZolt´an Juh ´asz [10] is highly relevant in this respect. He\nselects his algorithms for their ability to answer questions\nabout the data (mainly Hungarian folk song melodies) in-\nstead of employing the data to answer questions about his\nalgorithms. By clustering contour representations of the\nmelodies in various ways, his studies reveal differencesbetween oral traditions in various countries.\nAnother relevant publication is an article by David\nHuron in which he proposes to visualize geographic dif-\nferences in music by showing densities on a map [9]. As\nan example, using the Essen collection, Huron visualizes\nthe geographic density of certain types of cadence notes,\nshowing that Western European songs are nearly three\ntimes as likely to have most of their phrases end on a note\nother than the tonic compared to Eastern European songs.\nOf the mentioned papers only those of Juh ´asz explicitly\nstate an interest in folk songs as part of oral traditions.\n3 SOME PAST AND CURRENT APPROACHES TO\nMELODY IN FOLK SONG RESEARCH\nDuring the last century, the availability of collected folk\nsong tunes has generated a considerable amount of musi-\ncological research. One of the primary concerns is how to\ndeal with the speciﬁc type of variation caused by the pro-\ncess of oral transmission. Therefore we will ﬁrst discuss\noral transmission. Then classiﬁcation and identiﬁcation of\nmelodies in the context of FSR will be discussed.\n3.1 Oral Tradition\nThe transmission of songs in an oral tradition is deter-\nmined by the capabilities of human perception, memory,\nperformance and creativity. Participants in the tradition\nhave representations of songs in their memories. The only\nway in which others have access to a song, is to listen to\na performance. Research into music cognition [15] shows\nthat the representation of a song in human memory is not\n‘literal’, but – – in the words of Bertrand Bronson – – “a ﬂuid\nidea of a song” [2]. During performance the actual ap-\npearance of the song is recreated. In the process of ‘trans-\nlation’ from the memory representation to audible words\nand melody, considerable variation may occur. As long as\nthe the process of performing a song from memory is not\nsufﬁciently understood, we have mainly to focus on the\nrecorded songs instances in order to understand this kind\nof variation.\nA comprehensive inventory of types of variation in\nGerman folk songs is made by Walter Wiora [21], who\nsummarizes the issue as: “Alles an der Beschaffenheit\neiner Melodie ist ver ¨anderlich”.3He divides the types\nof change in seven categories: 1. changes in contour,\n2. changes in tonality, 3. changes in rhythm, 4. inserting\nand deleting of parts, 5. changes of form, 6. changes in ex-\npression, 7. demolition of the melody. For each category\nhe gives many examples.\n3.2 Classiﬁcation\nA ‘classiﬁcation system’ is used to group melodies with\ncertain characteristics together. Examples are a common\nnumber of lines, a common number of syllables or a com-\nmon cadence note sequence. Overviews of classiﬁcation\n3Everything in a melody can change.systems are provided by [4] and [1]. In the following we\ngive examples for features used in different classiﬁcation\nsystems.\nWithin FSR, there is not one universally applicable\nclassiﬁcation system in existence. Most systems were de-\nveloped for speciﬁc corpora. One of the ﬁrst was devel-\noped by Ilmari Krohn. In his system the cadence notes\n(ending notes of the lines) are most important [5]. B ´ela\nBart´ok and Zolt ´an Kod ´aly adapted his system for Hun-\ngarian folk songs. In their publications songs were repre-\nsented by: 1. the number of lines, 2. the sequence of ca-\ndence notes, 3. the number of syllables in each line, and\n4. the ambitus [17, p. xxxiv]. In later work, Bart ´ok used\nanother system in which he divided Hungarian songs into\nthree classes, namely old style, new style and mixed style\nmelodies [17, p. xlii]. Subdivisions were made according\nto rhythmic characteristics and the number of lines. Ob-\nviously, this way of ordering the material is speciﬁcally\naimed at the corpus of Hungarian songs. As Bruno Nettl\npoints out, Bart ´ok’s particular choice of features for clas-\nsiﬁcation could only be made by someone already familiar\nwith the corpus for which the system was developed [14,\np. 124]. This applies to folk song classiﬁcation systems in\ngeneral [1, p. 33].\nIn the British-American folk song tradition, Bertrand\nBronson found the following features to be important:\n1. ﬁnal cadence, 2. mid cadence, 3. ﬁrst accented note,\n4. ﬁrst phrase cadence, 5. ﬁrst accented note of second\nphrase, 6. penultimate stress of second phrase, etc. [2].\nA classiﬁcation system based upon these features can be\nexpected to group songs in the same tune family together.\nIn [18] the German Archive of Folk Song (Deutsches\nV olksliedarchiv) uses an ordering based on the system of\nKrohn. The ﬁrst criterion is the number of lines. Within\nthe resulting groups the songs are ordered according to\ntheir cadence note sequences.\n3.3 Identiﬁcation\n‘Identiﬁcation’ of a song is related to the process of oral\ntransmission. If two song instances are derived from the\nsame common ‘ancestor’, they are considered to be the\nsame song [14, p. 114].4The identity of a song is a com-\nplex and abstract concept. It is not obvious what consti-\ntutes the ‘substance’ of a song that is shared among all\nhistorically derived variants. As a consequence, histori-\ncally linked variants may in a classiﬁcation system end\nup in entirely different classes. The possibility of inter-\nference between tune families complicates the issue even\nfurther. Because the concept of identity goes beyond in-\ndividual features of song instances, it is very difﬁcult to\ndevelop models that explain tune families.\nHowever, identiﬁcation of melodies is necessary to ad-\ndress a number of research questions, such as: Where do\nthe individual songs originate from? What were the most\npopular melodies in a certain time or at a certain place?\n4This causes an ambiguity in the term ‘song’, with which an individ-\nual performance can be meant, but also the tune family as a whole.Which inﬂuences from abroad can be traced? How did\nthe melodies develop over time?\nAt the Meertens Institute, the concept of ‘melo-\ndienorm’ (melody norm)5is used to group ‘genetically’\nrelated melodies. Because the contents of folk song col-\nlections are highly fragmentary, it is impossible to recon-\nstruct the complete history of melodies and to ﬁnd all vari-\nants that are derived from a common ‘ancestor’ melody.\nWhat is feasible is to ﬁnd related groups of melodies\nwithin the collection, based on melodic and textual sim-\nilarity and available metadata, and to try to link them to\nmelody norms in a second stage. For this a retrieval sys-\ntem is an important tool.\n4 COOPERATION AND INTEGRATION\nDespite some good examples presented in the previous\nsections, currently a profound mutual inﬂuence of MIR\nand FSR appears barely to exist. This seems to be true too\nfor the relation between Musicology and MIR in general.\nAlthough the subject is the same (music), there seems to\nbe a gap in the ways of understanding it. In our opinion\nboth disciplines suffer from this lack of mutual inﬂuence.\nCharacterizing the gap in an extreme way, we have\n1. folk song researchers who lack a fundamental under-\nstanding of the possibilities and limitations of compu-\ntational approaches, and 2. MIR researchers who do not\nhave a professional musical knowledge framework, which\ncauses a limited view on music and the way music func-\ntions in culture.\nThis limited view on music prevents MIR often from\nbeing really relevant to FSR (or to musicology in gen-\neral), as for instance, the problematic notion of ground\ntruth demonstrates. Sometimes it seems like MIR has\na stock of so called ‘experts’ from which truths can be\ndrawn. Once provided by the expert, MIR does not go be-\nyond this ground truth, thus making it a hermetic bound-\nary of MIR and musicology. The fundamental question is\nwhat we really want to achieve. Do we develop algorithms\nmerely to reproduce a given ‘ground truth’, or do we eval-\nuate the theories that are behind that ‘ground truth’? The\nsecond option will obviously lead to a better understand-\ning of music, which in turn will lead to better approaches\nfor music retrieval.\nBefore any useful software can be developed for\nfolk song research – – which is a core activity within the\nWITCHCRAFT project – – implementable models of FSR\nconcepts are needed. As Willard McCarty states in a more\ngeneral discussion about the relation between Computer\nScience and the Humanities [11, Ch.1], the process of\nmodeling itself is more important than the resulting mod-\nels, because it is in this process that knowledge is gen-\nerated about the concepts to be modeled. Therefore, the\nway a model fails is more interesting than the way a model\nsucceeds, because there lies an opportunity to improve un-\nderstanding. In our case, the most important concept to\nmodel is the melody norm.\n5Comparable to ‘tune family’ and ‘Melodietyp’.\nMIR FSR1 2\n3 4\nCMFigure 1 . Three-role model for integration: Music In-\nformation Retrieval (MIR), Computational Musicology\n(CM), and Folk Song Research (FSR).\nAlthough the modeling is more important than the\nmodels, for making the current state of knowledge avail-\nable for application, these models are needed. This leads\nto two kinds of activities. First, the process of model-\ning and second, the implementation and deployment of\nthe state-of-the-art models. These activities will alternate\nin an iterative process.\nWe now present a possible way to overcome the ob-\nserved ‘gap’ with the help of the three- rolemodel that is\nshown in Figure 1. In addition to the roles of MIR and\nFSR researchers, a ‘man in the middle’ role is needed.\nWe call this role ‘Computational Musicology’ (CM). It\ndoes not necessarily imply the need for an extra person\nin research teams. In exceptional cases one person might\ncombine all three roles, but it would be more common for\nresearchers either to combine both the MIR and CM or\n(probably less commonly) the FSR and CM roles.\nCM. In general the task for the CM-role is to connect\nthe two disciplines. For the activity of modeling the task\nis to ‘deconstruct’ the FSR-concepts in order to derive im-\nplementable models (arrows 1 and 2). After the ﬁrst it-\neration these models can be improved by providing FSR\nthe implemented models and letting FSR examine the way\nin which the previous models fail (arrows 3 and 4). An-\nother, more practical, task for CM is to provide FSR with\nready-to-use software frameworks and toolboxes, allow-\ning them to combine input, processing and output methods\nin various ways [7]. These toolboxes could consist of ba-\nsic melodic transformations, feature extractors, segmenta-\ntion algorithms, distance measures, clustering algorithms,\nclassiﬁcation methods, visualization tools, etc. that are rel-\nevant for evaluating musicological concepts.\nFSR. If folk song researchers put effort in getting a\ngeneral understanding of the possibilities and limitations\nof computational methods, they will realize that these\nmethods will not replace currently used methods, but pro-\nvide additional ways to explore the data and to evaluate\nthe usefulness of their concepts. Since ambiguous or in-\ntuitive concepts are difﬁcult to implement, the coopera-\ntion with Computer Science offers the opportunity to gain\nmore clear understanding of the underlying concepts. So,\nthe task for FSR is to be as precise as possible in deﬁning\nconcepts. Another, more practical effort that may be ex-\npected from folk song researchers, is to take some time to\nlearn how to use provided systems.\nMIR. MIR can provide numerous useful software com-ponents and user interface components in which FSR con-\ncepts are implemented in an efﬁcient and effective way.\nThese components have to be packed into toolboxes or li-\nbraries by CM, thereby hiding implementation details that\nhave no meaning in the musical domain and making com-\nponents compatible with each other. In practice probably\nall MIR researchers play the role of CM to a certain ex-\ntent. If they were not interested in music, they would not\nhave been involved in MIR. The discipline of MIR can\ngain much from pursuing the CM-role more ambitiously.\n5 CONCLUDING REMARKS\nFacing the tasks described in the previous section, what\ncould CM learn from the current situations of MIR and\nFSR as described in sections 2 and 3? The ultimate aim\nof FSR to identify melodies seems currently too ambi-\ntious to perform automatically (see section 3.3), since no\nproper implementable model of melody norm is available.\nTherefore software should support identiﬁcation by ﬁnd-\ning related melodies, leaving the decision whether to as-\nsign these melodies to a speciﬁc tune family to the inves-\ntigator. So, for CM, on the short term, classiﬁcation tasks\noffer more opportunities than identiﬁcation tasks.\nFrom the classiﬁcation approaches in section 3.2 we\ncan obtain a number of relevant features, such as cadence\nand accent note patterns, number of lines, and rhythmic\ncharacteristics. However, it will not be sufﬁcient to just\nimplement the models of e.g. Bart ´ok or Bronson, since\ntheir feature sets were not created with the power of com-\nputational methods in mind, and they were often created\nfor speciﬁc corpora. The possibilities Computer Science\noffers and the currently available computational power en-\nable new kinds of models. Therefore, entirely different\nfeatures might be used, such as contours, repeating pat-\nterns, features from music cognition, features that reﬂect\nperformance of untrained singers, and so on. Several of\nthese features have already been used (section 2) or are\ncurrently being explored [20, 8]. These new methods have\nto be developed in cooperation with musicologists who\nare able to provide the musical insights for modeling the\nfeatures, and for improving failing models, thus escaping\nthe problems of ground truths that were discussed in sec-\ntion 4. We envision an iterative process of modeling and\nimplementing that will result in an increasing understand-\ning of the concepts of folk song research, in particular the\nidentity of a tune family. This knowledge is highly valu-\nable for both folk song research and music information re-\ntrieval, and might also be of interest for other disciplines,\nmusic cognition in particular.\nAcknowledgements. This work was supported by the Netherlands\nOrganization for Scientiﬁc Research within the WITCHCRAFT project\nNWO 640-003-501, which is part of the CATCH-program.\n6 REFERENCES\n[1] Bohlman, P. V ., The Study of Folk Music in the Modern\nWorld , Bloomington, 1988.[2] Bronson, B. H., “Some Observations About Melodic\nVariation in British-American Folk Tunes”, Journal of\nthe American Musicological Society 3 (1950), 120–\n134.\n[3]Danish Folklore Archives – – 10,000 melodies , Retr.\n1 March, 2007, from http://www.dafos.dk/\nspmEnglish .\n[4] Elschekov ´a, A., “Methods of Classiﬁcation of Folk-\nTunes”, Journal of the International Folk Music Coun-\ncilXVIII (1966), 56–76.\n[5] Erd ´ely, S., “Classiﬁcation of Hungarian Folksongs”,\nThe Folklore and Folk Music Archivist V (1962), 3, 1f.\n[6]Finnish Folk Tunes , Retr. 1 March, 2007,\nfrom http://www.jyu.fi/musica/sks/\ncollection.html .\n[7] Garbers, J., Integration von Bedien- und Program-\nmiersprachen am Beispiel von OpenMusic, Humdrum\nund Rubato , [Diss.], Berlin, 2004.\n[8] Garbers, J., et al. , “On pitch and chord stability in\nfolk song variation retrieval”, [To appear in the Pro-\nceedings of the First International Conference of the\nSociety for Mathematics and Computation in Music],\n2007.\n[9] Huron, D., “Mapping European Folksong: Geograph-\nical Localization of Musical Features”, Computing in\nMusicology 12 (1999–2000), 169–183.\n[10] Juh ´asz, Z., “A Systematic Comparison of Differ-\nent European Folk Music Traditions Using Self-\nOrganizing Maps”, Journal of New Music Research\n35 (2006), nr. 2, 95–112.\n[11] McCarty, W. Humanities Computing , Basingstoke,\n2005.\n[12] [Without title] , Retr. 1 March, 2007 from http://\nwww.nzdl.org/musiclib .\n[13] Musipedia: The Open Music Encyclopedia , Retr.\n1 March, 2007, from http://www.musipedia.\norg.\n[14] Nettl, B., The Study of Ethnomusicology , 2nd ed, Ur-\nbana, 2005.\n[15] Peretz, I. and R. J. Zatore, “Brain Organization for\nMusic Processing”, Annual Review of Psychology 56\n(2005), 89–114.\n[16] Schaffrath, H., The Essen Folksong Collection , D.\nHuron (editor), Stanford CA, 1995.\n[17] Suchoff, B., “Editor’s Preface”, in: B. Bart ´ok,The\nHungarian Folk Song , Albany, 1981.\n[18] Suppan, W. and W. Stief (eds.), Melodietypen des\nDeutschen Volksgesanges , Tutzing, 1976.\n[19] Themeﬁnder , Retr. 1 March, 2007, from http://\nwww.themefinder.org .\n[20] V olk, A., et al. , “Comparing Computational Ap-\nproaches to Rhythmic and Melodic Similarity in Folk\nSong Research”, [To appear in the Proceedings of\nthe First International Conference of the Society for\nMathematics and Computation in Music], 2007.\n[21] Wiora, W., “Systematik der musikalischen Erschei-\nnungen des Umsingens”, Jahrbuch f ¨ur Volkslied-\nforschung 7 (1941), 128–195."
    },
    {
        "title": "Automated Synchronization of Scanned Sheet Music with Audio Recordings.",
        "author": [
            "Frank Kurth",
            "Meinard Müller",
            "Christian Fremerey",
            "Yoon-ha Chang",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416918",
        "url": "https://doi.org/10.5281/zenodo.1416918",
        "ee": "https://zenodo.org/records/1416918/files/KurthMFCC07.pdf",
        "abstract": "In this paper, we present a procedure for automatically synchronizing scanned sheet music with a corresponding CD audio recording, where suitable regions (given in pixels) of the scanned digital images are linked to time positions of the audio file. In a first step, we extract note parameters and 2D position information from the scanned images using standard software for optical music recognition (OMR). We then use a chroma-based synchronization algorithm to align the note parameters to the given audio recording. Our experiments show that even though the output of current OMR software is often erroneous, the music parameters extracted from the digital images still suffice to derive a reasonable alignment with the audio data stream. The resulting link structure can be used to highlight the current position in the scanned score or to automatically turn pages during playback of an audio recording. Such functionalities have been realized as plug-in for the SyncPlayer, which is a free prototypical software framework for bringing together various MIR techniques and applications. 1 INTRODUCTION Modern digital music libraries contain large amounts of textual, visual, and audio data as well as a variety of associated data representations. In particular for Western classical music, two prominent examples of widely-used and digitally available types of music representations are scanned sheet music (available as digital images) and audio recordings (e. g., in CD or MP3 format). These two representations complement each other describing music on different semantic levels. On the one hand, sheet music, which in our context denotes a printed form of musical score notation, is used to visually describe a piece of music in a compact and human readable from. Sheet music not only allows a musician to create a performance but it also reveals structural, harmonic, or melodic aspects of the music that may not be obvious from mere listening. On the other hand, an audio recording encodes the soundwave of an acoustic realization, which allows the listener to play-back a specific interpretation. c⃝2007 Austrian Computer Society (OCG). Given various representations of musically relevant information, e. g., as encoded by sheet music or as given by a specific audio recording, the identification of semantically related events is of great relevance for music retrieval and browsing applications [1, 4, 5]. Here, we will discuss the problem of scan-audio synchronization, which refers to the problem of linking regions (given as pixel coordinates) within the scanned images of given sheet music to semantically corresponding physical time positions within an audio recording. Such linking structures can be used to highlight the current position in the scanned score during playback of the recording, thus enhancing the listening experience as well as providing the user with tools for intuitive and multimodal music exploration. The importance of such a functionality, which is illustrated by Figure 4, has been emphasized in the literature, see, e. g., [4]. In this paper, we present a procedure for scan-audio synchronization, which is, to the best of the author’s knowledge, the first algorithm for performing this task in a fully automated fashion. The general idea is to transform both the scanned images as well as the corresponding audio recording into chroma-based mid-level representations, which can then be time-aligned via dynamic time warping. The details of our synchronization approach will be explained in Section 2. As one important subtask, we extract musical note events as well as the corresponding 2D pixel regions from the scanned sheet music using standard software for optical music recognition (OMR). In Section 3, we will discuss this OMR step and the problems arising from OMR recognition errors. The mid-level representations, as our experiments show, are robust enough to be successfully used for the scan-audio synchronization task even when corrupted by erroneous note events or by missing signatures. In Section 4, as a further contribution of this paper, we present a novel visualization plug-in for the SyncPlayer framework [6]. This interface synchronously displays the score position within the scanned images along with the audio playback. In the SyncPlayer context, the scan-audio synchronization constitutes a key component for a comprehensive tool facilitating multimodal navigation and retrieval in complex and inhomogeneous music collections. We conclude this paper with Section 5. Further references to related work as well as prospects on future work are given in the respective sections. 2 SYNCHRONIZATION PROCEDURE In this section, we describe our approach to scan-audio synchronization and summarize the background on the underlying techniques from MIR and audio signal processing. As discussed in the introduction, the input of the scanaudio synchronization algorithm consists of a scanned version of a musical score and a corresponding audio recording of the same piece of music. As an example, Figure 1 shows the first few measures of Beethoven’s Piano Sonata No. 23, Op. 57 (“Appassionata”) as well as the waveform of a recording by Barenboim of the same measures. In the first step of our synchronization algorithm, we transform the scanned score as well as the audio recording into a common mid-level representation, which allows for comparing and relating music data in various realizations and formats. To be more specific, we use chroma-based features, where the chroma correspond to the twelve traditional pitch classes of the equaltempered scale [2]. In Western music notation, the chroma correspond to the set {C, C♯, D, . . . , B} consisting of the twelve pitch spelling attributes. The audio recording is transformed into a sequence of normalized 12-dimensional chroma vectors, where each vector expresses the signal’s local energy distribution among the 12 pitch classes. Such a chroma representation can be obtained from a spectrogram by suitably pooling Fourier coefficients; for details, we refer to [2]. Figure 1 shows the resulting audio chromagram for our Beethoven example. Chroma-based audio features absorb variations in parameters such as dynamics, timbre, and articulation and closely correlate to the short-time harmonic content of the underlying audio signal. The transformation of the scanned score representation into a chroma representation consists of several steps. First, the score data such as note events, the key signature, the time signature, and other musical symbols are extracted from the scanned images using standard software for optical music recognition (OMR). Note that in addition to the musical score data, the OMR process provides us with pixel coordinates of the extracted data, allowing us to exactly localize all musical symbols within the scanned images. The technical details and the OMRinvolved problems of this step will be discussed separately in Section 3. Then, in the second step, a sequence of chroma features is synthesized from the OMR result. This is done in a straightforward fashion by using the sequence of the extracted note events, which are encoded by parameters for pitch as well as musical onset time and duration. Physical onset times and durations are calculated assuming a constant tempo. Note that in our case the particular choice of tempo is not crucial because differences in tempo will be compensated in the subsequent synchronization step. We choose a standard value of 120 bpm. A sequence of chroma features is then synthesized by sliding across the time axis with a temporal window while adding energy to the chroma bands that correspond to pitches that Scanned Score OMR and conversion Chroma",
        "zenodo_id": 1416918,
        "dblp_key": "conf/ismir/KurthMFCC07",
        "keywords": [
            "chroma-based synchronization algorithm",
            "note parameters",
            "2D position information",
            "standard software for optical music recognition (OMR)",
            "audio recording",
            "scanned sheet music",
            "audio playback",
            "mid-level representations",
            "dynamic time warping",
            "visual music exploration"
        ],
        "content": "AUTOMATED SYNCHRONIZATION OF SCANNED SHEET MUSIC WITH\nAUDIO RECORDINGS\nFrank Kurth, Meinard M ¨uller, Christian Fremerey, Yoon-ha Chang, and Michael Claus en\nBonn University\nDepartment of Computer Science III\nABSTRACT\nIn this paper, we present a procedure for automatically\nsynchronizing scanned sheet music with a corresponding\nCD audio recording, where suitable regions (given in pix-\nels) of the scanned digital images are linked to time po-\nsitions of the audio ﬁle. In a ﬁrst step, we extract note\nparameters and 2D position information from the scanned\nimages using standard software for optical music recogni-\ntion (OMR). We then use a chroma-based synchronization\nalgorithm to align the note parameters to the given audio\nrecording. Our experiments show that even though the\noutput of current OMR software is often erroneous, the\nmusic parameters extracted from the digital images still\nsufﬁce to derive a reasonable alignment with the audio\ndata stream. The resulting link structure can be used to\nhighlight the current position in the scanned score or to au-\ntomatically turn pages during playback of an audio record-\ning. Such functionalities have been realized as plug-in\nfor the SyncPlayer, which is a free prototypical software\nframework for bringing together various MIR techniques\nand applications.\n1 INTRODUCTION\nModern digital music libraries contain large amounts of\ntextual, visual, and audio data as well as a variety of as-\nsociated data representations. In particular for Western\nclassical music, two prominent examples of widely-used\nand digitally available types of music representations are\nscanned sheet music (available as digital images) and au-\ndio recordings (e. g., in CD or MP3 format). These two\nrepresentations complement each other describing music\non different semantic levels. On the one hand, sheet mu-\nsic, which in our context denotes a printed form of musical\nscore notation, is used to visually describe a piece of mu-\nsic in a compact and human readable from. Sheet music\nnot only allows a musician to create a performance but\nit also reveals structural, harmonic, or melodic aspects of\nthe music that may not be obvious from mere listening.\nOn the other hand, an audio recording encodes the sound-\nwave of an acoustic realization, which allows the listener\nto play-back a speciﬁc interpretation.\nc/circlecopyrt2007 Austrian Computer Society (OCG).Given various representations of musically relevant in-\nformation, e. g., as encoded by sheet music or as given by\na speciﬁc audio recording, the identiﬁcation of semanti-\ncally related events is of great relevance for music retriev al\nand browsing applications [1, 4, 5]. Here, we will discuss\nthe problem of scan-audio synchronization , which refers\nto the problem of linking regions (given as pixel coordi-\nnates) within the scanned images of given sheet music to\nsemantically corresponding physical time positions withi n\nan audio recording. Such linking structures can be used to\nhighlight the current position in the scanned score during\nplayback of the recording, thus enhancing the listening ex-\nperience as well as providing the user with tools for intu-\nitive and multimodal music exploration. The importance\nof such a functionality, which is illustrated by Figure 4,\nhas been emphasized in the literature, see, e. g., [4].\nIn this paper, we present a procedure for scan-audio\nsynchronization, which is, to the best of the author’s\nknowledge, the ﬁrst algorithm for performing this task in\na fully automated fashion. The general idea is to trans-\nform both the scanned images as well as the corresponding\naudio recording into chroma-based mid-level representa-\ntions, which can then be time-aligned via dynamic time\nwarping. The details of our synchronization approach will\nbe explained in Section 2.\nAs one important subtask, we extract musical note\nevents as well as the corresponding 2D pixel regions from\nthe scanned sheet music using standard software for opti-\ncal music recognition (OMR). In Section 3, we will dis-\ncuss this OMR step and the problems arising from OMR\nrecognition errors. The mid-level representations, as our\nexperiments show, are robust enough to be successfully\nused for the scan-audio synchronization task even when\ncorrupted by erroneous note events or by missing signa-\ntures.\nIn Section 4, as a further contribution of this paper, we\npresent a novel visualization plug-in for the SyncPlayer\nframework [6]. This interface synchronously displays the\nscore position within the scanned images along with the\naudio playback. In the SyncPlayer context, the scan-audio\nsynchronization constitutes a key component for a com-\nprehensive tool facilitating multimodal navigation and re -\ntrieval in complex and inhomogeneous music collections.\nWe conclude this paper with Section 5. Further refer-\nences to related work as well as prospects on future work\nare given in the respective sections.2 SYNCHRONIZATION PROCEDURE\nIn this section, we describe our approach to scan-audio\nsynchronization and summarize the background on the\nunderlying techniques from MIR and audio signal pro-\ncessing.\nAs discussed in the introduction, the input of the scan-\naudio synchronization algorithm consists of a scanned\nversion of a musical score and a corresponding audio\nrecording of the same piece of music. As an example,\nFigure 1 shows the ﬁrst few measures of Beethoven’s Pi-\nano Sonata No. 23, Op. 57 (“Appassionata”) as well as\nthe waveform of a recording by Barenboim of the same\nmeasures. In the ﬁrst step of our synchronization algo-\nrithm, we transform the scanned score as well as the au-\ndio recording into a common mid-level representation,\nwhich allows for comparing and relating music data in\nvarious realizations and formats. To be more speciﬁc,\nwe use chroma-based features, where the chroma corre-\nspond to the twelve traditional pitch classes of the equal-\ntempered scale [2]. In Western music notation, the chroma\ncorrespond to the set {C,C♯,D,... ,B}consisting of the\ntwelve pitch spelling attributes.\nThe audio recording is transformed into a sequence of\nnormalized 12-dimensional chroma vectors, where each\nvector expresses the signal’s local energy distribution\namong the 12 pitch classes. Such a chroma representation\ncan be obtained from a spectrogram by suitably pooling\nFourier coefﬁcients; for details, we refer to [2]. Figure 1\nshows the resulting audio chromagram for our Beethoven\nexample. Chroma-based audio features absorb variations\nin parameters such as dynamics, timbre, and articulation\nand closely correlate to the short-time harmonic content\nof the underlying audio signal.\nThe transformation of the scanned score representa-\ntion into a chroma representation consists of several steps .\nFirst, the score data such as note events, the key signa-\nture, the time signature, and other musical symbols are\nextracted from the scanned images using standard soft-\nware for optical music recognition (OMR). Note that in\naddition to the musical score data, the OMR process pro-\nvides us with pixel coordinates of the extracted data, al-\nlowing us to exactly localize all musical symbols within\nthe scanned images. The technical details and the OMR-\ninvolved problems of this step will be discussed separately\nin Section 3. Then, in the second step, a sequence of\nchroma features is synthesized from the OMR result. This\nis done in a straightforward fashion by using the sequence\nof the extracted note events, which are encoded by pa-\nrameters for pitch as well as musical onset time and du-\nration. Physical onset times and durations are calculated\nassuming a constant tempo. Note that in our case the par-\nticular choice of tempo is not crucial because differences\nin tempo will be compensated in the subsequent synchro-\nnization step. We choose a standard value of 120 bpm. A\nsequence of chroma features is then synthesized by sliding\nacross the time axis with a temporal window while adding\nenergy to the chroma bands that correspond to pitches thatScanned\nScore\nOMR and\nconversion\nChroma\n  \n0 2 4 6 8 10 12C C#D D#E F F#G G#A A#B \nsynchroni-\nzation\nChroma\n  \n0 1 2 3 4 5 6 7 8 9 10C C#D D#E F F#G G#A A#B \nconversion\nWave-\nform\n0 1 2 3 4 5 6 7 8 9 10−0.100.1\ntime [s]ampl.\nFigure 1 . Illustration of the scan-audio synchronization\nby means of the ﬁrst few measures of Beethoven’s Piano\nSonata No. 23, Op. 57 (“Appassionata”). The ﬁgure shows\na scanned musical score and the resulting score chroma-\ngram (upper part) as well as the waveform of a recording\nby Barenboim of the same measures and the resulting au-\ndio chromagram (lower part). The synchronization result\nbetween the two chroma representations is indicated by\nred bidirectional arrows.\nare active during the current temporal window. The result-\ning chroma vectors are then normalized. A similar strat-\negy for synthesizing chromagrams from symbolic data has\nbeen suggested in [5]. Figure 1 shows the resulting score\nchromagram for the Beethoven example.\nHaving transformed both the audio recording as well as\nthe scanned score into sequences of chroma vectors, one\ncan use standard algorithms based on dynamic time warp-\ning (DTW) to time-align the two sequences. For details,\nwe refer to [5, 7] and the references therein. Here, the\nmain idea is to build up a cross-similarity matrix by com-\nputing the pairwise distance between each score chroma\nvector and each audio chroma vector. In our implementa-\ntion, we simply use the inner vector product for the com-\nparison. An optimum-cost alignment path is determined\nfrom this matrix via dynamic programming. To speed up\nthis computationally expensive procedure, we use an efﬁ-\ncient multiscale version of DTW. The details of this pro-\ncedure can be found in [7]. The overall synchronization\nprocedure is illustrated by Figure 1.\nTo ﬁnally link spatial positions within the audio record-\ning to regions within the scanned images, we combine the\nsynchronization and OMR-results as follows: From the\npixel coordinates obtained in the OMR step, we derive a\ncorrespondence between the extracted OMR note events\nand the spatial regions within the image data displaying\nthese note events. The spatial regions are encoded by aScan\nOMR\nResult\nFigure 2 . Comparison of a scanned score and the corresponding result s acquired by OMR (excerpt of Beethoven’s Piano\nSonata No. 30, Op. 109). Errors in the OMR result are highligh ted by shaded boxes. Triangles at the end of a bar indicate\nthat the sum of the durations of the notes does not match the ex pected duration of the measure. To correct this, SharpEye\nhas “deactivated” several notes which is indicated by greye d-out note heads. Although in this example, the OMR result\nshows several recognition errors, there is still enough cor respondence to allow for a measure-wise synchronization wi th a\ncorresponding audio recording.\npage number and suitable 2D coordinates. Combining the\nspatial information and the above synchronization result,\nwe have all linking information needed to track and high-\nlight note events in the scanned score during the playback\nof the audio recording. Such a functionality will be dis-\ncussed in more detail in the subsequent sections and is\nillustrated by Figure 4.\n3 OPTICAL MUSIC RECOGNITION\nThe ﬁrst step in converting scanned sheet music to chroma\nfeatures is the recognition of symbols from the given im-\nage data and the interpretation of their musical semantics.\nWe use the SharpEye 2.68 software to batch process all\nscanned pages corresponding to a particular piece of mu-\nsic. The input scanned image ﬁles are given in the TIFF\nformat using 1 bit color (black and white) and a resolution\nof 600 dpi. The output is a SharpEye proprietary ASCII\nﬁle with extension *.mro that contains the recognition\nresults and the pixel positions of the recognized symbols\nwithin the image data.\nThe quality of OMR results in general strongly de-\npends on the quality of the input image data and the com-\nplexity of the underlying scores. In the context of this pa-\nper, we consider high quality scans of piano music. Gener-\nally, we obtain reasonably accurate recognition results fo r\nthis class of music. Despite a good overall quality in the\nOMR results, some problems may still occur because inscore notation, a recognition error of a single symbol can\nhave a high impact on the global semantics. For example,\na missing accidental at the beginning of a staff corrupts\nthe pitch of all corresponding notes in that staff. Mistakes\nin note durations, e.g. the recognition of an eighth as a\nquarter, can lead to inconsistencies in measure lengths.\nMore severe errors result from misinterpretation of score\nsemantics, e.g., the interpretation of a grand-staff as two\nindividual staffs.\nFor highlighting regions in the score images one has to\nchoose a suitable spatial and temporal granularity. Even\nthough the scan-audio synchronization is reasonable on a\nnote level resolution for most of the time, we choose a\ngranularity where the displayed region on the score corre-\nsponds to one musical measure. This leads to stable and\naccurately synchronized highlightings even in the case of\ntypical local OMR errors. Furthermore, highlighting en-\ntire measures also takes into account that a musician typ-\nically follows entire note groups or more general musical\nstructures rather than individual notes.\nFor testing and evaluating we access data provided\nby the Bavarian State Library (BSB), Munich, Germany,\nwhich is currently digitizing audio recordings and sheet\nmusic in the context of the PROBADO digital library\nproject [8]. The underlying music collection consists of\nclassical and romantic piano sonatas as well as a collec-\ntion of German 19th centuries piano songs amounting to\napproximately 6.000 images of scanned music and about\n1.200 pieces of music in total. For each scanned piece  \n0 1 2 3 4 5 6C C#D D#E F F#G G#A A#B   \n0 0.5 1 1.5 2 2.5 3 3.5C C#D D#E F F#G G#A A#B \nFigure 3 . Comparison of two chromagrams. The lower\nchromagram is derived from an audio recording of a ca-\ndenza in D♭-Major, played on a piano. The upper chroma-\ngram is synthesized from a symbolic score representation\nthat has missing accidentals. In particular, there are only\nthree ﬂats instead of ﬁve. The optimum alignment still\ndelivers an accurate synchronization on the basis of the\nunaffected pitches.\nof music, three different audio recordings are currently\ndigitized at BSB. We have tested our algorithm for scan-\naudio synchronization on a selection of piano music by\nBeethoven, Mozart and Schubert comprising more than\n160pages of sheet music.\nThe OMR works well in most cases but often suf-\nfers from minor glitches like missing accidentals or mis-\ntakes in note durations. However, because the detection\nof bar lines is quite stable, inconsistencies and confu-\nsions due to mistakes in note durations last no longer than\nthe measure of their occurrence. As an example, Fig-\nure 2 shows a comparison of a scanned score and the\ncorresponding results acquired by OMR for an excerpt of\nBeethoven’s Piano Sonata No. 30, Op. 109. Local errors\nin the OMR result are highlighted by shaded boxes. Our\nexperiments show that for a global measure-wise synchro-\nnization, these local errors have a negligible inﬂuence as\nthey are mainly absorbed by the coarseness of our chroma\nfeatures.\nThe possible impact of missing accidentals is illus-\ntrated in Figure 3, where two chromagrams are compared.\nThe lower chromagram is derived from an audio recording\nof a cadenza in D♭-Major played on a piano. The upper\nchromagram is synthesized from a symbolic score repre-\nsentation that suffers from two missing accidentals (three\ninstead of ﬁve ﬂats). Despite of these severe recognition\nerrors, the unaffected pitches still overrule the effect of the\ncorrupted pitches facilitating an accurate overall synchr o-\nnization of the two chromagrams.\nIn case of the previously discussed OMR errors of\nhigher semantic impact, such as the confusion of a two-\nstaff system with two one-staff systems, regional synchro-\nnization errors may occur. However, due to our DTW-\nbased synchronization approach, where the global scan-audio alignment is calculated, those errors affect the syn-\nchronization only locally in a neighborhood around the\ncorresponding regions.\nWe conclude this section by noting that a considerable\namount of OMR errors can be corrected in a postprocess-\ning step by using suitable heuristics. As an example, the\naccidentals detected at the beginning of each staff may be\nchecked for correctness by considering the global key or\nheuristics on detected accidentials in previous and subse-\nquent staffs. As another approach, OMR may be improved\nby combining different OMR strategies as suggested by\nByrd et al. [3].\n4 A SYNCPLAYER PLUG-IN FOR AUTOMATIC\nSCORE-TRACKING\nIn this section, we present a prototypical implementation\nof a graphical interface for automatic score tracking. The\ninterface has been implemented as a visualization plug-\nin of the previously proposed SyncPlayer framework [6],\nwhich is brieﬂy described in Section 4.1. Section 4.2 de-\nscribes the novel plug-in for automatic score tracking.\n4.1 The SyncPlayer Framework\nThe SyncPlayer is a client-server based software frame-\nwork that integrates various MIR applications such as\nmultimodal presentation of audio data, music synchro-\nnization, and content-based retrieval [6]. The user oper-\nates the client component , which in its basic mode acts\nlike a standard software audio player for recordings in\nthe*.mp3 and*.wav ﬁle formats. Figure 4 shows the\nSyncPlayer client with the main window located at the top\nleft. A remote computer system runs the server compo-\nnent, which supplies the client with content-related data\nassociated to the currently selected audio recording. Ex-\namples of such content-related data are lyrics of the vocal\ntracks, the musical score, or information on the musical\nform. Synchronously to acoustic audio playback, such\ndata is then presented to the user by means of several\nspecialized visualization plug-ins , which are available at\nthe client side. In the next section, we describe how the\nSyncPlayer is extended by a novel plug-in for visualizing\nscanned sheet music where highlighting of page regions is\nprovided to enable score tracking during audio playback.\nIn Figure 4, three instances of this novel plug-in are de-\npicted along with the audio player. For more details on\nthe SyncPlayer and existing plug-ins, we refer to our web\npage [9].\n4.2 A Plug-in for Automatic Score Tracking\nFor our SyncPlayer-based user scenario we assume that\nafter suitable digitization and scan-audio synchronizati on\nas discussed in the previous sections, the scanned sheet\nmusic as well as the information linking the scans to the\naudio recordings are available at the SyncPlayer server. A\ntypical scenario of a user operating the SyncPlayer client\nthen involves the following steps:Figure 4 . SyncPlayer client and three instances of the SheetMusic pl ug-in for displaying scanned sheet music. The bar in\nthe scanned score that corresponds to the current playback p osition is highlighted by a red box. The instances demonstra te\nthree different zoom levels: detailed view (lower left), fu ll page view (lower right) and thumbnail view (upper right).\n1. The user selects and opens an audio recording in the\nSyncPlayer Client.\n2. A connection to the SyncPlayer Server is estab-\nlished allowing the server to identify the selected\naudio recording within the audio database. If avail-\nable, the score image data and the bar highlight-\ning information on the audio recording are retrieved\nfrom its annotation database.\n3. The server notiﬁes the client about the availability\nof synchronized score image data for the selected\nrecording. If available, the client retrieves and dis-\nplays the available data in the SheetMusic plug-in.\nIn its basic mode, the SheetMusic plug-in displays one\npage of scanned sheet music as an image (Figure 4, lower\nleft and lower right plug-in windows). The plug-in offers\ncontrols for zooming and scrolling within the boundaries\nof the page. During the playback of a recording in the\nSyncPlayer, the bar that is currently played is highlighted\nin the score by a red translucent box. If the current bar\nis located on a different page or lies outside the visible\ndisplay area, the plug-in automatically loads and displaysthe corresponding page and adjusts the scroll settings to\nensure that the bar is visible.\nThe plug-in implements several zoom level strategies,\neach one offering a different tradeoff between detail and\noverview. The detailed view shows only a local part of\nthe current page surrounding the highlighted bar. The\nscrolling functionality is restricted to the page boundari es\n(Figure 4, lower left window). The full page view ﬁxes the\nzoom settings to always displaying one full page to give\na better overview on where the current playback position\nis located on the current page (Figure 4, lower right win-\ndow). An overview on the entire set of pages belonging\nto a piece of music is given in the thumbnail view . This\nview displays several pages at a time, highlighting the cur-\nrently active page by a red frame (Figure 4 upper right).\nAdditionally, the page numbers are displayed.\nThe SheetMusic plug-in not only has visualization ca-\npabilities, but also offers the user several ways to navigat e\nwithin the score and audio representations. By clicking\non a target bar, the user may request the plug-in to set the\nplayback of the audio recording to the starting time of that\nparticular measure. When the user selects a page in the\nthumbnail view, the player jumps to the playback positioncorresponding to the start of the page.\nIn the process of developing and testing the SheetMu-\nsic plug-in, we experimented with other styles of high-\nlighting suitable score regions. A sliding window, which\nmoves across the score in a note-by-note fashion, is prob-\nlematic in the current version because small (e.g., note-\nlevel) inaccuracies in the synchronization are likely to\nconfuse the user. Furthermore, as noted before, musicians\nusually tend to seize entire note groups and musical struc-\ntures rather than following individual notes. As another\nvisualization strategy, besides highlighting the current ly\nactive measure, several subsequent measures could be vi-\nsualized to allow a certain look-ahead. In future work,\nseveral other visualization strategies have to be imple-\nmented and evaluated on the basis of comprehensive user\nstudies.\n5 CONCLUSIONS\nIn this paper, we presented an automated procedure for\nscan-audio synchronization, which consists of aligning\nscanned sheet music with corresponding CD audio record-\nings. In the alignment, spatial regions of the sheet music\nare linked to musically corresponding temporal regions\nwithin the audio recording. In our approach, we used\nchroma-based features as a mid-level representation for\nencoding both the audio recording and the scanned sheet\nmusic, which allowed us to perform the synchronization\nstep using standard DTW techniques. The use of this\nmid-level representation allows us to exploit both the ro-\nbustness of chroma-based features to compensate for local\nOMR errors and the robustness of the global DTW to com-\npensate more global staff-level OMR errors. The evalua-\ntion of the proposed methods is carried out on a real-world\ncorpus of digitized sheet music and audio recordings cur-\nrently digitized at the Bavarian State Library.\nFor presenting the results of a scan-audio synchro-\nnization to a user, we implemented a SheetMusic plug-in\nfor the existing SyncPlayer framework. The plug-in of-\nfers several different modes for displaying scanned score\nimages along with audio playback and allows to syn-\nchronously highlight (and hence track) the current play-\nback position in the scanned score and to automatically\nturn pages.\nAs a next step, a more formal and exhaustive evaluation\nof the system should be conducted, especially regarding\nthe impact of different classes and rates of OMR errors on\nthe synchronization results. However, because the focus\nof this paper lies on the overall concept and interaction\nof components, such an evaluation is outside this paper’s\nscope. A formal evaluation is planned to be a topic in our\nfuture work.\nAcknowledgement\nThis work was supported in part by Deutsche Forschungs-\ngemeinschaft (DFG) under grant 554975 (1) OldenburgBIB48 OLof 01-02. We thank the project partners at BSB\nfor providing the digitized material used within this re-\nsearch project.\n6 REFERENCES\n[1] V. A RIFI, M. C LAUSEN , F. K URTH ,AND\nM. M ¨ULLER ,Synchronization of music data in\nscore-, MIDI- and PCM-format , Computing in\nMusicology, 13 (2004).\n[2] M. A. B ARTSCH AND G. H. W AKEFIELD ,Au-\ndio thumbnailing of popular music using chroma-\nbased representations , IEEE Trans. on Multimedia, 7\n(2005), pp. 96–104.\n[3] D. B YRD AND M. S CHINDELE ,Prospects for improv-\ning OMR with multiple recognizers , in Proc. ISMIR,\nVictoria, Canada, 2006, pp. 41–46.\n[4] J. W. D UNN , D. B YRD, M. N OTESS , J. R ILEY ,AND\nR. S CHERLE ,Variations2: Retrieving and using mu-\nsic in an academic setting , Special Issue, Commun.\nACM, 49 (2006), pp. 53–48.\n[5] N. H U, R. D ANNENBERG ,AND G. T ZANETAKIS ,\nPolyphonic audio matching and alignment for music\nretrieval , in Proc. IEEE WASPAA, New Paltz, NY ,\nOctober 2003.\n[6] F. K URTH , M. M ¨ULLER , D. D AMM , C. F REMEREY ,\nA. R IBBROCK ,AND M. C LAUSEN ,SyncPlayer—an\nadvanced system for content-based audio access , in\nProc. ISMIR, London, GB, 2005.\n[7] M. M ¨ULLER , H. M ATTES ,AND F. K URTH ,An efﬁ-\ncient multiscale approach to audio synchronization , in\nProc. ISMIR, Victoria, Canada, 2006, pp. 192–197.\n[8] T. S TEENWEG AND U. S TEFFENS ,Probado – non-\ntextual digital libraries put into practice , ERCIM\nNews, Special Theme: European Digital Library\n(2006), pp. 47–48.\n[9] S YNCPLAYER ,An advanced system for mult-\nmodal music access . Website, January 2007.\nhttp://www-mmdb.iai.uni-bonn.de/\nprojects/syncplayer ."
    },
    {
        "title": "Vivo Visualizing Harmonic Progressions and Voice-Leading in PWGL.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418189",
        "url": "https://doi.org/10.5281/zenodo.1418189",
        "ee": "https://zenodo.org/records/1418189/files/KuuskankareL07.pdf",
        "abstract": "This paper describes a novel tool called VIVO (VIsual VOice-leading) that allows to visually define harmonic progressions and voice-leading rules. VIVO comprises of a compiler and a collection of specialized visualization devices. VIVO takes advantage of several music related applications collected under the umbrella of PWGL (PWGL is a free cross-platform visual programming language for music and sound related applications). Our music notation application–Expressive Notation Package or ENP–is used here to build the user-interface used to visually define harmony and voice-leading rules. These visualizations are converted to textual rules by the VIVO compiler. Finally, our rule-based compositional system, PWGLConstraints, is used generate the final musical output using these rules. 1 BACKGROUND The musical problems that are interesting in terms of musical constraints programming are typically very demanding. Here, harmony, melody, voice-leading, and counterpoint provide challenges not only in an aesthetic sense but also in terms of a formal definition. Defining textually rules that solve a certain compositional or music analytical problem is a time consuming task and requires a lot of both musical and programming expertise. VIVO allows to visually define rudimentary rules of counterpoint. The approach presented here resembles the traditional teaching situation where a teacher or a textbook gives out examples of correct use of harmonic progressions or voice-leading. Using music notation as a framework for constructing the VIVO user-interface provides several advantages: (1) it is easy to write the rules, i.e., the user sees the exact musical context the rule is applied to; (2) it is possible to verify the correctness of the data by ’listening’ to the rules; (3) it is straightforward to edit, add and remove data; and (4) the user can easily edit the rule set and make subsets of it. Our environment for computer assisted composition, PWGL ([3]; http://www.siba.fi/pwgl/), is used to implement VIVO. PWGL offers a unique combination of graphical and textual programming tools. The two of c⃝2007 Austrian Computer Society (OCG). which are of primary importance in terms of VIVO are PWGLConstraints [4] and ENP [2]. VIVO uses an ENP score as a user-interface component. By entering musical material into the score makes it possible to define given aspects of harmony and voiceleading. The visual representation is then fed to the VIVO compiler which in turn generates textual rules that are suitable for PWGLConstraints. The final output generated by PWGLConstraints can then be shown in common music notation using ENP. There are also other rule-based systems that have been used to solve musical constraint satisfaction problems, e.g., Situation, Arno, OMClouds, Strasheela (see [1] for more information) and the choral harmonizer system by Kemal Ebcioglu. Furthermore, integrating visual tools in constraints programming has been studied, for example, in [5]. However, using music notation to define rules of counterpoint, in the way the VIVO does, is unique and cannot be found elsewhere. This paper gives a brief overview of some of the currently available VIVO tools. 2 THE VIVO TOOLS",
        "zenodo_id": 1418189,
        "dblp_key": "conf/ismir/KuuskankareL07",
        "keywords": [
            "VIVO",
            "VIsual VOice-leading",
            "compiler",
            "visualization devices",
            "PWGL",
            "music notation application",
            "Expressive Notation Package",
            "ENP",
            "rule-based compositional system",
            "PWGLConstraints"
        ],
        "content": "VIVO -VISUALIZING HARMONICPROGRESSIONS AND\nVOICE-LEADINGINPWGL\nMika Kuuskankare\nSibeliusAcademy\nCMTMikael Laurson\nSibeliusAcademy\nCMT\nABSTRACT\nThis paper describes a novel tool called VIVO (VIsual\nVOice-leading) that allows to visually deﬁne harmonic\nprogressionsandvoice-leadingrules. VIVOcomprisesof\nacompilerandacollectionofspecializedvisualizationde -\nvices. VIVO takes advantageof severalmusic related ap-\nplicationscollectedundertheumbrellaofPWGL(PWGL\nis a free cross-platformvisual programminglanguagefor\nmusic and sound related applications). Our music nota-\ntion application–ExpressiveNotation Package or ENP–is\nusedheretobuildtheuser-interfaceusedtovisuallydeﬁne\nharmonyandvoice-leadingrules. Thesevisualizationsare\nconvertedto textual rules by the VIVO compiler. Finally,\nour rule-based compositional system, PWGLConstraints,\nisusedgeneratetheﬁnalmusicaloutputusingtheserules.\n1 BACKGROUND\nThemusicalproblemsthat areinterestingin termsof mu-\nsicalconstraintsprogrammingaretypicallyverydemand-\ning. Here, harmony, melody, voice-leading, and counter-\npointprovidechallengesnotonlyinanaestheticsensebut\nalso in terms of a formal deﬁnition. Deﬁning textually\nrules that solve a certain compositional or music analyti-\ncalproblemisatimeconsumingtaskandrequiresalotof\nbothmusicalandprogrammingexpertise.\nVIVO allows to visually deﬁne rudimentary rules of\ncounterpoint. The approachpresented here resembles the\ntraditionalteachingsituationwhereateacheroratextboo k\ngives out examples of correct use of harmonic progres-\nsions or voice-leading. Using music notation as a frame-\nwork for constructing the VIVO user-interface provides\nseveral advantages: (1) it is easy to write the rules, i.e.,\nthe user sees the exact musical contextthe rule is applied\nto;(2)itispossibletoverifythecorrectnessofthedataby\n’listening’totherules;(3)itisstraightforwardtoedit, add\nand remove data; and (4) the user can easily edit the rule\nset andmakesubsetsofit.\nOur environment for computer assisted composition,\nPWGL([3]; http://www.siba.fi/pwgl/ ),isused\nto implement VIVO. PWGL offers a unique combination\nof graphical and textual programming tools. The two of\nc/circlecopyrt2007AustrianComputerSociety(OCG).which are of primary importance in terms of VIVO are\nPWGLConstraints[4]andENP[2].\nVIVO uses an ENP score as a user-interface compo-\nnent. By entering musical material into the score makes\nit possible to deﬁne given aspects of harmony and voice-\nleading. ThevisualrepresentationisthenfedtotheVIVO\ncompilerwhichinturngeneratestextualrulesthataresuit -\nableforPWGLConstraints. Theﬁnaloutputgeneratedby\nPWGLConstraints can then be shown in common music\nnotationusingENP.\nThereare also otherrule-basedsystemsthat havebeen\nused to solve musical constraint satisfaction problems,\ne.g., Situation, Arno, OMClouds, Strasheela (see [1] for\nmore information) and the choral harmonizer system by\nKemal Ebcioglu. Furthermore, integrating visual tools in\nconstraints programming has been studied, for example,\nin [5]. However, using music notation to deﬁne rules of\ncounterpoint, in the way the VIVO does, is unique and\ncannotbefoundelsewhere.\nThis paper gives a brief overview of some of the cur-\nrentlyavailableVIVOtools.\n2 THEVIVO TOOLS\n2.1 CommonHarmonicProgressions\nFigure 1 gives an example of a simpliﬁed harmonic pro-\ngression deﬁned with the help of VIVO. There are two\ncases,markedas 1and2intheexample. Theideahere\nis to give examples–written in music notation–of accept-\nable harmonic progressions. The setting or the inversion\nofthechordsis notimportant.\nFigure1. A simpliﬁedharmonicprogressiondatabase.\nThe relationships between two adjacent chords in\nVIVO are relative it is possible that harmonic sequences,\nother than the ones explicitly indicated in the database,\ncan be formed. For example, using the database given in\nFigure 1, it is possible to form among others the follow-\ning sequence: I-V/V,V/V-V,V-V/II. This scheme actually\nallows VIVO to modulate from one harmonic region to\nanother.2.2 Common Voice-LeadingCases\n2.2.1 SuspendedChords\nTherearecertaingraphicaldevicesthatcanbeusedtode-\nﬁne aspects of the voice-leading in more detail. Figure 2\nshowshowpreparingandresolvingasuspendedharmony\n(e.g.,I4−3) is deﬁned using VIVO. The horizontal lines\nconnecting two adjacent notes are used to constrain the\nmovement of a given member of the harmony. In Figure\n2,1deﬁneshowthesuspendedharmonyispreparedand\n2deﬁneshowit isresolved.\nFigure 2. A VIVO database deﬁning a suspended har-\nmonyanditsresolution.\nThisdatabasewouldproduceanuninterruptedchainof\nchords cycling the pattern prepared-suspended-resolved .\nFigure3givesanexampleofascore,generatedbyVIVO,\nusingtheabovedatabase.\nFigure 3. A four-voiced chorale fragment containing a\nchainofsuspensions.\n2.2.2 Neapolitanchord\nAscanbeseeninFigure3,thesuspensiondatabasegiven\nabove does not assume any particular inversion of the\nchords (or setting for that matter). However, in some\ncases–as in case of the Neapolitan Sixth Chord ( N6)–the\ninversion of the chord is of primary importance.Figure 4\ngivesa newvisualizationdevice(shownasathickdashed\nline in the left hand staff) that is used to indicate the root\nnoteofagivenchord.\nFigure 4 . The Neapolitan chord and its idiosyncratic\nvoice-leadingandenharmoniccontentdeﬁnedinVIVO.\n3 DISCUSSION\nVIVO is still more or less in the conceptual level. More\nwork is needed to create a good set of visualization de-vicesthataredescriptiveyetstraightforward.\nThere are many open questions dealing with such\nfundamental voice-leading cases as parallel-, hidden-,\nsimilar-, oblique- or contrary-motion; voice-crossing or\n-overlapping; open- and close-positions; or cadences. It\nshould also be investigated how to visualize key depen-\ndent issues such as the leading tone or harmonic regions,\netc.\nOne of the most interesting applications in terms of\nMIR would be to let VIVO learn harmonic progressions\nfromtherepertoire,e.g.,toanalyzeexistingscorestopro -\nducetheVIVOdatabase.\nSome of the issues enumerated above would probably\nbe already possible to realize. Using and existing score\nto create a VIVO database from, for example, a Bach\nChorale wouldn’trequire much extra work. Furthermore,\nopen-andclose-positionscouldverywell beenteredwith\nthe help of VIVO except that the VIVO rule compiler\nshouldbeinstructedtocompiletheseassocalledheuristic\nrules.\nIt is perhapsequallyevidentthat not all of the features\nmentioned above are suitable to be represented visually.\nIt is, however,possibleto augmentVIVO tocovera great\ndealofthem.\n4 ACKNOWLEDGMENTS\nTheworkofMikaelLaursonandMikaKuuskankarehave\nbeen supported by the Academy of Finland (SA 105557\nandSA 114116).\n5 REFERENCES\n[1] Anders, T., C. Anagnostopoulou, and M. Alcorn,\n“Strasheela: Design and Usage of a Music Compo-\nsition Environment Based on the Oz Programming\nModel”,Multiparadigm Programming in Mozart/OZ:\nSecond International Conference, MOZ 2004 (Roy,\nP.V., ed.),vol.LNCS 3389,Springer-Verlag,2005.\n[2] Kuuskankare, M. and M. Laurson, “Expressive No-\ntation Package”, Computer Music Journal , vol. 30,\nno.4,pp.67–79,2006.\n[3] Laurson, M. and M. Kuuskankare, “PWGL: A Novel\nVisual Language based on Common Lisp, CLOS and\nOpenGL”, Proceedings of International Computer\nMusic Conference , (Gothenburg, Sweden), pp. 142–\n145,2002.\n[4] Laurson, M. and M. Kuuskankare, “Extensible Con-\nstraint Syntax Through Score Accessors”, Journ´ees\nd’InformatiqueMusicale ,(Paris,France),2005.\n[5] Schulte, C., “Oz Explorer: A visual constraint pro-\ngrammingtool.”, ProceedingsoftheFourteenthInter-\nnationalConferenceonLogicProgramming ,TheMIT\nPress, 1997."
    },
    {
        "title": "Metadata Infrastructure for Sound Recordings.",
        "author": [
            "Catherine Lai",
            "Ichiro Fujinaga",
            "David Descheneau",
            "Michael Frishkopf",
            "Jenn Riley",
            "Joseph Hafner",
            "Brian McMillan"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415224",
        "url": "https://doi.org/10.5281/zenodo.1415224",
        "ee": "https://zenodo.org/records/1415224/files/LaiFDFRHM07.pdf",
        "abstract": "This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings.",
        "zenodo_id": 1415224,
        "dblp_key": "conf/ismir/LaiFDFRHM07",
        "keywords": [
            "heterogeneous",
            "distributed",
            "metadata",
            "repositories",
            "sound recording",
            "collections",
            "real-time",
            "querying",
            "harmonizing",
            "diverse metadata models"
        ],
        "content": "METADATA INFRASTRUCTURE FOR SOUND RECORDINGSCatherine Lai, Ichiro Fujinaga David Descheneau, Michael Frishkopf Jenn Riley Joseph Hafner, Brian McMillan Music Technology McGill University {lai, ich}@music.mcgill.ca Department of Music University of Alberta {dpd, michaelf}@ualberta.ca Digital Library Program Indiana University  jenlrile@indiana.edu McGill University Libraries McGill University {joseph.hafner, brian.mcmillan}@mcgill.ca ABSTRACT This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings. 1. INTRODUCTION Librarians and information technologists have been collaborating to formulate strategies for building digitized collections. The development of digital collections and technology applications has revolutionized libraries, offering them new opportunities to disseminate organized information, i.e., metadata, about their collections and special library holdings, such as sound recordings. As an expanding number of digital sound recording collections emerge, the task of searching at individual repository sites becomes impractical for end-users. For example, users have to learn multiple interfaces located at different sites where each interface is designed with different features, functionalities, and interaction metaphors. Users must also manually combine search results and move data between applications. The metadata infrastructure for sound recordings provides a common framework that allows different metadata systems of sound recordings to be shared and reused, thus facilitating the interoperability among heterogeneous repositories of digital sound recordings.  2. RELATED WORK New kinds of metadata infrastructures have been discussed in the literature. Tennant proposed a new bibliographic metadata infrastructure, which is interoperable between different library-based metadata standards and protocols [4].  Godby et al. have introduced new paradigms of metadata translation service for the purpose of increasing accessibility with improvements in searching [1]. In music, the Sheet Music Consortium has built a Web portal (at UCLA) where seven collections of sheet music, in distributed locations, can be accessed [3]. Other metadata aggregation experiments using OAI-PMH for digital libraries such as the NSDL have also been reported [2]. 3. SYSTEM FUNCTION The core function of the metadata infrastructure for sound recordings, the distributed metadata service (DMS), supports the following processes. First, queries are submitted from a search portal to the DMS where the queries are distributed synchronously to partner repository interfaces. Result sets from the repositories are then obtained and translated into RDF (Resource Description Framework) statements, which preserve source metadata elements and relationships. Finally, the RDF statement sets from partner repositories are aggregated and returned to the search portal for display. 4. IMPLEMENTATION 4.1. Partner Metadata Repository Three universities hosting heterogeneous digital repositories of sound recordings participate in the prototype of cross-repository interoperability: FolkwaysAlive! at the University of Alberta (a collection of world music, including commercial and field recordings), Variations2 at Indiana University (a collection of selected recordings and scores for instructional use at the IU Jacobs School of Music), and the digital archive of Handel LPs at McGill University. The metadata source repositories are located at partner institutions, with the exception of the IU Variations2 metadata repository (MR), which was not available as a public Web service at the time of the prototype’s design. To make the data available for searching in aggregations, the Variations2 MR was duplicated at the UA site where a data store and remote connection to it were created in order to proceed with the prototype. The partner repositories implement a variety of connection methods and search APIs. For FolkwaysAlive! repository, a connection to an RDF server is made. In the Handel repository, an HTTP connection with the custom XML query API is made. And in Variations2, a MySQL database query connection is made. The participating repositories, moreover, implement a wide variety of metadata schemas. The deployed metadata schemas come in various syntaxes with different metadata structure, © 2007 Austrian Computer Society (OCG).     reflecting a diversity of approaches to managing digital libraries of sound recordings. 4.2. Internal Services The DMS internal services perform real-time querying of existing MR services, in the context of the federation. First, all requests are directed to a URL rewriter, which intercepts and passes the requests to the Request Handler. The Request Handler inspects the query requests and forwards them to the Request Aggregator. The Request Aggregator then synchronously passes the query string to all partner repository interfaces, specified in a crosswalk configuration file. The crosswalk configuration is read at runtime with each search request. The RDF Translator then transforms metadata result sets from the repository interfaces (in custom XML) into RDF statements and converts metadata statement predicates in the RDF to their common (i.e., Dublin Core) equivalents, according to the custom metadata-to-DC equivalences specified in the configuration file, which maps one common element to many repository-specific elements. Finally, the repository-specific and common metadata is combined for all repositories and returned as search results to the Request Handler.  4.3. User Interface A search portal UI has been designed and provides a simple search box for entering the words or phrase. The set of metadata returned is the DC equivalent of the various types of metadata deployed at the different digital repositories. The results page shows brief records of the DC metadata and provides hyperlinks to the full metadata records residing at the host digital repositories. 5. CHALLENGES The absence of uniformity in the repository APIs and metadata implementation raised issues in areas of search and retrieval, system performance, and presentation.  First, inconsistent implementation of search and asset retrieval APIs at the partner repositories complicates consolidation of metadata. Different treatments of Boolean operators, wildcard operators, and query matching algorithm between the APIs affect the results of federated search operations. Second, custom configuration of performance metrics at the individual repositories, which is often implemented and optimized for local use, influences the overall performance of the Web portal. As an example, the McGill Handel asset retrieval API provides one metadata record at a time. In order to retrieve the full metadata necessary for translation to DC and subsequent transformation for correct display in the search portal, a separate HTTP request must be made for each desired asset. This requirement becomes a major contributor to network overhead and is inherently inefficient and problematic for the implementation of the metadata infrastructure. Third, different understanding of the common metadata elements when writing the configuration files can favor or exclude repositories. Since the implementation of the metadata crosswalk is optional per-element, unless a visible trace of the query is provided, recall of result sets may suffer in not knowing whether a repository is empty or a metadata mapping was excluded for a particular query string. Fourth, related to the various interpretations for common metadata elements, mapping of custom metadata to different common elements is possible (e.g., mapping of performer or lyricist to Dublin Core’s creator or contributor). This ambiguity degrades the quality of presentation. Consistent and precise mappings of metadata elements from local to common schemas can enhance the presentation of records by logical sorting of metadata result sets, such as grouping by creator.  6. FUTURE ENHANCEMENT For future iterations, enhancements will be made to provide better functionality and improved performance. These include development of a Web-based portal for partners to modify their own subsections of the central crosswalk configuration file; enforcement of consistent and precise mappings from local to common schemas; incorporation of parallel/asynchronous access to partner MRs; presentation of logical groupings of metadata result sets; providing visible trace of the query; and investigation of other search protocols and Web service tools such as Search/Retrieval via URL (SRU) and Common Query Language (CQL) for federated searching and returning metadata.  7. ACKNOWLEDGEMENT This research is, in part, funded by the ITST Research Grants of the SSHRC, Canada. 8. REFERENCES [1] Godby, C., Smith, D., and Childress, E. “Two paths to interoperable metadata,” Proceedings of the Dublin Core Conference, Seattle, Washington, October 2, 2003. [2] Lagoze, C., Krafft, D., Cornwell, T., Dushay, N., Eckstrom, D., and Saylor, J. “Metadata aggregation and ‘automated digital libraries’: A retrospective on the NSDL experience,” Proceedings of the Joint Conference on Digital Libraries, Chapel Hill, North Carolina, 2006. [3] Sheet Music Consortium. http://digital.library.ucla.edu/sheetmusic [4] Tennant, R. “A bibliographic metadata infrastructure for the 21st century,” Library Hi Tech, 22, 2 (2004), 175–81."
    },
    {
        "title": "Using 3D Visualizations to Explore and Discover Music.",
        "author": [
            "Paul Lamere",
            "Douglas Eck"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415022",
        "url": "https://doi.org/10.5281/zenodo.1415022",
        "ee": "https://zenodo.org/records/1415022/files/LamereE07.pdf",
        "abstract": "This paper presents Search Inside the Music an application for exploring and discovering new music. Search Inside the Music uses a music similarity model and 3D visualizations to provide a user with new tools for exploring and interacting with a music collection. With Search Inside the Music, a music listener can find new music, generate interesting playlists, and interact with their music collection. 1 INTRODUCTION Tools that help listeners find new music, re-find forgotten music, and create coherent playlists become increasingly important as music collections grow in size. At the same time, it is important to remember that the primary purpose of music is entertainment. As such, activities surrounding music such as music discovery and playlist generation should be engaging and entertaining activities. In this paper we describe the Search Inside the Music application. SITM uses interactive 3D visualizations of a music similarity space to allow a music listener to explore their music collection, to receive recommendations for new music, to generate interesting and coherent playlists, and to interact with the album artwork of a music collection. The resulting user interface is arguably more engaging and enjoyable to use than currently available commercial music interfaces. 2 MUSIC SIMILARITY The visualizations in the Search Inside the Music system rely on a music similarity model. The system is decoupled from the similarity model such that any music similarity model that can quickly determine the distance between any two songs in a music collection can be used. We’ve successfully used models based on the Marysyas system [3] as well as models developed here at Sun Microsystems [4]. Experience has shown that similarity models based upon maximum-margin classifiers such a support vector machines generate poor visualizations since these classic⃝2007 Austrian Computer Society (OCG). Figure 1. 3D View of the Music Space fiers tend to result in a number of tight clusters that are not well-suited for exploration and visualization. 3 VISUALIZATIONS Having a model of the musical similarity space allows us to represent a music collection in new and interesting ways. Figure 1 shows a 3-dimensional visualization of the music space. In this visualization songs are represented by spheres floating in space. Spheres that are close together in this space represent songs that are musically similar. The current playlist is shown by a connected sequence of floating album covers. Larger spheres represent songs that are favored by the listener. The music listener can interact with this visualization to explore the music space. Clicking on a sphere will cause the represented song to play. Clicking on the ‘More like this’ button will add more songs to the visualization that sound similar to the current song. The listener can generate interesting and varied playlists by selecting two end point songs and having the browser generate a path through the music collection that best connects the songs, minimizing jarring song transitions. Interacting in this way with the music collection can provide interesting insights into its contents and composition and provides an arguably more enjoyable interaction than is typically found in a more traditional music browser that relies primarily on lists of song titles and artist names. SITM uses a multi-dimensional scaling technique to project the high-dimensional music space into three diFigure 2. The Album Cloud Figure 3. The Album Grid mensions similar to the approaches used in [2] with optimizations described in [1].",
        "zenodo_id": 1415022,
        "dblp_key": "conf/ismir/LamereE07",
        "keywords": [
            "music similarity model",
            "3D visualizations",
            "music collection exploration",
            "new music discovery",
            "playlist generation",
            "engaging and entertaining",
            "music listener",
            "interactive interface",
            "music space visualization",
            "music similarity space"
        ],
        "content": "USING 3D VISUALIZATIONS TO EXPLORE AND DISCOVER MUSIC\nPaul Lamere\nSun Labs\nSun Microsystems\nBurlington, MA, USA\nPaul.Lamere@sun.comDouglas Eck\nSun Labs\nSun Microsystems\nBurlington, MA, USA\nDouglas.Eck@umontreal.ca\nABSTRACT\nThis paper presents Search Inside the Music an application\nfor exploring and discovering new music. Search Inside\nthe Music uses a music similarity model and 3D visual-\nizations to provide a user with new tools for exploring and\ninteracting with a music collection. With Search Inside\nthe Music, a music listener can ﬁnd new music, generate\ninteresting playlists, and interact with their music collec-\ntion.\n1 INTRODUCTION\nTools that help listeners ﬁnd new music, re-ﬁnd forgotten\nmusic, and create coherent playlists become increasingly\nimportant as music collections grow in size. At the same\ntime, it is important to remember that the primary purpose\nof music is entertainment. As such, activities surround-\ning music such as music discovery and playlist generation\nshould be engaging and entertaining activities.\nIn this paper we describe the Search Inside the Mu-\nsicapplication. SITM uses interactive 3D visualizations\nof a music similarity space to allow a music listener to\nexplore their music collection, to receive recommenda-\ntions for new music, to generate interesting and coher-\nent playlists, and to interact with the album artwork of a\nmusic collection. The resulting user interface is arguably\nmore engaging and enjoyable to use than currently avail-\nable commercial music interfaces.\n2 MUSIC SIMILARITY\nThe visualizations in the Search Inside the Music system\nrely on a music similarity model. The system is decoupled\nfrom the similarity model such that any music similarity\nmodel that can quickly determine the distance between\nany two songs in a music collection can be used. We’ve\nsuccessfully used models based on the Marysyas system\n[3] as well as models developed here at Sun Microsystems\n[4]. Experience has shown that similarity models based\nupon maximum-margin classiﬁers such a support vector\nmachines generate poor visualizations since these classi-\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nFigure 1 . 3D View of the Music Space\nﬁers tend to result in a number of tight clusters that are not\nwell-suited for exploration and visualization.\n3 VISUALIZATIONS\nHaving a model of the musical similarity space allows\nus to represent a music collection in new and interest-\ning ways. Figure 1 shows a 3-dimensional visualization\nof the music space. In this visualization songs are repre-\nsented by spheres ﬂoating in space. Spheres that are close\ntogether in this space represent songs that are musically\nsimilar. The current playlist is shown by a connected se-\nquence of ﬂoating album covers. Larger spheres represent\nsongs that are favored by the listener. The music listener\ncan interact with this visualization to explore the music\nspace. Clicking on a sphere will cause the represented\nsong to play. Clicking on the ‘More like this’ button will\nadd more songs to the visualization that sound similar to\nthe current song. The listener can generate interesting and\nvaried playlists by selecting two end point songs and hav-\ning the browser generate a path through the music collec-\ntion that best connects the songs, minimizing jarring song\ntransitions. Interacting in this way with the music collec-\ntion can provide interesting insights into its contents and\ncomposition and provides an arguably more enjoyable in-\nteraction than is typically found in a more traditional mu-\nsic browser that relies primarily on lists of song titles and\nartist names.\nSITM uses a multi-dimensional scaling technique to\nproject the high-dimensional music space into three di-Figure 2 . The Album Cloud\nFigure 3 . The Album Grid\nmensions similar to the approaches used in [2] with opti-\nmizations described in [1].\n3.1 Album Cloud\nAlbum artwork has always been an important part of mu-\nsic collecting. However, with the shift to digital music,\nopportunities to interact with the album artwork have di-\nminished. In SITM, we hope to bring back some of the\nopportunities to enjoy the artwork associated with a record\nalbum. Figure 2 shows the SITM album cloud. When a\nsong is playing, SITM displays the artwork for the cur-\nrent album, surrounded by the artwork for albums that\nhave similar sounding songs. With the album cloud, a\nmusic listener can quickly see a set of albums containing\nsimilar music, enhancing the opportunities for serendip-\nitous music discovery. SITM uses a real-world physics\nmodel to manage the motion of the album artwork, yield-\ning smooth, realistic album artwork transitions.\n3.2 Album Grid\nSITM provides tools for exploring a music collection. One\nsuch tool is the album grid shown in Figure 3. The album\ngrid shows the artwork for all of the albums in a music col-\nlection. The album artwork is arranged in a grid such that\nmusically similar albums appear adjacent to each other,\nwhile musically dissimilar albums appear far apart. At a\nglance, a music listener can view their entire music col-\nFigure 4 . The Album Spiral\nlection or focus in on particular neighborhoods within the\ncollection. A listener can click on any album in the grid\nto audition songs on the album. As with the album cloud,\nthe album artwork in the album grid is represented using\na real-world physics model, allowing for smooth, realistic\nmotion of the album artwork. The album grid is not con-\nstrained to be a planar grid. It can be shaped into rings,\nboxes, ellipses and various other shapes. Figure 4 shows\nthe album grid in the form of a spiral. The album artwork\nreshapes itself into the various forms following smooth\ntrajectories generated by the physics model. These ani-\nmations present a highly engaging experience for a music\nlistener interacting with their music collection.\n4 CONCLUSION\nIn this paper we describe the visualizations used in the\nSearch Inside the Music project. SITM uses 3D visualiza-\ntions to allow a music listener to explore and interact with\ntheir music collection, to help them to generate interesting\nplaylists and to discover new music. The user interface\nprovides a dynamic environment that is engaging and fun\nto use, helping to make the task of music exploration and\ndiscovery be more entertaining.\nAcknowledgments\nAlbum artwork images used courtesy of Magnatune.com\n5 REFERENCES\n[1] Matthew Chalmers. A linear iteration time layout al-\ngorithm for visualising high-dimensional data. Visual-\nization ’96 Proceedings , 1996.\n[2] John C. Platt. Fast embedding of sparse music similar-\nity graphs.\n[3] G. Tzanetakis and P. Cook. Marsyas: A framework for\naudio analysis, 2000.\n[4] Kris West and Paul Lamere. A model-based approach\nto constructing music similarity functions. EURASIP\nJournal on Advances in Signal Processing , 2007."
    },
    {
        "title": "Enabling Access to Sound Archives Through Integration, Enrichment and Retrieval: The EASAIER Project.",
        "author": [
            "Christian Landone",
            "Joseph Harrop",
            "Josh Reiss"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415584",
        "url": "https://doi.org/10.5281/zenodo.1415584",
        "ee": "https://zenodo.org/records/1415584/files/LandoneHR07.pdf",
        "abstract": "Many  digital  sound  archives  suffer  from  problems concerning on-line access: sound materials are often held separately from other related media, they are not easily browsed and little opportunity to search the actual audio content of the material is provided. The  EASAIER  project  aims  to  alleviate  these problems, offering a number of solutions to support sound archive  managers  and  users.  EASAIER  will  enable enhanced access to sound archives, providing multiple",
        "zenodo_id": 1415584,
        "dblp_key": "conf/ismir/LandoneHR07",
        "keywords": [
            "digital",
            "sound",
            "archives",
            "problems",
            "on-line",
            "access",
            "sound",
            "materials",
            "related",
            "media"
        ],
        "content": "Enabl ing Access t o Sound  Archives  through  Integra tion,  Enrich ment and \nRetrieval: the EAS AIER Pr oject\nChristian Landone Joseph Harrop Josh Reiss\nCentre fo r Digital  Music\nQueen Ma ry, U niversity  of \nLondon\nMile  End Road, Lo ndon \nE14NS\nUnited  King domRoyal Scottish  Academy of \nMusic and Dr ama\n100 Renf rew St,  Glas gow \nG23DB\nUnited  King domCentre fo r Digital  Music\nQueen Ma ry, U niversity  of \nLondon\nMile  End Road, Lo ndon \nE14NS\nUnited  King dom\nABSTRACT\nMany  digital sound archive s suffer from  problems \nconcerning  on-line access:  sound materials are often held \nseparatel y from other related  media, they are not easily \nbrow sed and little oppo rtunity to search  the actual audio \nconte nt of th e material is  provided.\nThe EASAIER  project aims to alleviate  these \nproblems, offering a number of solutions to support  sound \narchive  managers  and users.  EASAIER  will enable \nenhanced  access  to sound archi ves, providing  multiple \nmethods of retrieval,  integration  with other  media \narchive s, content e nrich ment and enha nced  access  tools.\n1.INT RODUCT ION\nA significant  problem  with prior  research  into audio \nprocessi ng, sound archive  access  and semantics is that it \nhas often not taken into accoun t the wealth of user needs \nstudies in this area.  Previo us work within the digital \nlibrary  community has identified strong  demand for \nspecif ic tools.  The EASAIER  (Enablin g Access  to Sound \nArchives through  Integ ration,  Enrich ment and Retrieval) \nproject will address  alrea dy identified user needs  for \nsound archi ves, as well as recent  work [1-3]  which \nprovide  a systematic study of what end users want from \nmusic retrieval  syste ms and the types  of queries  that they \nmake. Extensive research  by the JISC [4-5] also ident ified \nkey features to enrich sou nd and m usic archi ves.  \nMost audio  archi ves are also very limited in terms of \naccess  and interaction : if on-line listening  is available,  it \nis usually in a restricted  means constrained  not by the \nmaterial in the corpus,  but by the imposed functional ity of \nthe interface.  EASAIER  will remove these barrie rs, thus \nallowing the user to choose  their means of access and \npresentatio n.\nCollating the findings from these  studies, we have \nestabli shed th e following user needs require ments:\n•Choice  of interfaces: a simple web interface  may \nsatisfy the needs  of a student  but not those of a \nmusicologis t, therefore  different vers ions of the\nsystem's “front end”  are required .\n•Search  for media fitting a wide variety  of meta data : \nIncluding, where possible,  semantically  meaningful \ndescriptors  that are  automatically  extracted  during the\narchi val stage,  and the means to search  across  multiple \nand sometimes ambiguous selectio ns of m etadata.•Relevant  near  neighbour query- by-exa mple \nsearches:  A ranked list of related  audio  assets which \nmay be similar in musical quali ties (timbre, structure, \netc.)  is desirable.  This  is to be distinguished from \nthose syste ms that allow  the user to enter an audio \nsample a nd find if it exists in a database.  \n•Enriched access  and interac tion tools:  A set of tools \nthat simplify the task of marking points of interests  on \nthe audio  waveform should be included  in the front \nend. Instru ments that allow real-ti me manipulation of \naudio during playback,  such as time scaling and sound \nsource separation  can greatly  aid the analysi s pro cess.  \n2.SYS TEM ARCHI TECTURE\nThe deploy ed EASAIER  syste m exhibits a standard \nclient/server  architecture  (Figure 1).\nAll audio  assets are stored in a repositor y in binary \nform at; each  file is available  both in its original \nuncompressed format and as a high quality compressed \nversion us ed for strea ming audio to of f-site cl ients. \nIn most cases  the consorti um will not deploy  the \nstorage  compone nt as EASAIER  will seam lessly \nintegrate  with existing repositor y. \nAn archive  administration application  allow s the \nconte nt managers  to enter  assets in the archi ve and to \nprocess those that were already  stored  prior  to the \ndeplo yment of the EASAIER  system. The ingestion \nprocess extracts  musically relevant  features,  prompting \nthe administrator  to enter  manual anno tation and to edit \nautomatically  extracted  data  that is deemed unreliable  or \nambiguous by  the sys tem's internal me trics.   \nAll metadata  generate d during  the ingestion process \nis stored  in a separate  repository  as RDF triple s; this \nunit is connected  to SPARQL endpoin t that provides  a \nsuitable query  interface  in orde r to access the k nowledge \nheld by the sys tem's internal ontology.\nQuery  and access  services provide  connection s to the \nexternal world, allowing on-site and off-site end users to \naccess  the repository,  perform  searches  and retrieve \naudio asset s.\nThe end user will be able to access  the content of the \nEASAIER  archive  by means of an application   that can \nretrieve  an audio asse t and its associated  meta-data using \na variety  of non-mutually exclusive query \nmethodologies, su ch as:\n•Queries  based  on general  tags: i.e. find material  by \nauthor/t itle, genre  and year.© 2007 Austria n Computer Society (OCG ).•Musical  parame ters-based  queries:  i.e. find songs by \nkey, orchestration, tem po range.\n•Similarity-based  queries:  i.e. once  a musical audio \nasset has been  retrieved,  find other  assets that exhibit \nsome degree of similarity in terms of macroscopic \nstructure, timbre and har monic profile [6].\nThe EASAIER  syste m offers  two types  of user \ninterfaces  to the end user: a basic  web client with \nfunctionalities limited to retrieval  and playb ack and an \nadvanced  client that provides  extended  functionality.\nThe  advanced  interface  (Figure 2) includes \nvisualisation tools that allow the user to explore  all time-\nsynchronous  metadata  contained  in the repositor y, add \ntheir own annotatio ns and place  markers  for playback and \nlooping purposes.  \nA separate  panel  in the interface  enables  the access  to \nreal time sound processing  algorit hms; these tools  include \na sound restoration  and equali satio n unit, a time/pitch \nscale modifier and a sound source  separation processor.   \nFigure  1. EASAIER Syste m Architecture\n3.CONCLU SIONS\nThe EASAIER  project aims to create  a state-of-the-art \naccess  syste m for sound archi ves, incorpo rating  multiple, \nintegrated  retrieval  syste ms, and enriched  access  tools \nwhich allow  manipulation of the resources.  As of this \nwriting, the EASAIER  project has developed  protot ypes \nof most require d functionality, including the music \nretrieval  syste m and demonstrators  of integrated \ncompone nts, such as the metadata  extractor  and client \nprotot ype The potential  user community for sound archive s using EASAIER  is large  and wide rangi ng. It is \nthus hoped  that we will be able to deploy  the system  on \na large  scale,  benef iting sound archi ves in cultural \nheritage  institutions which until now, have been  unable \nto provide  advanced  access syste ms to the ir users.\nFigure  2. EASAIER Ad vanced  Client\n4.ACKNOWLE DGMEN TS\nThis work has been  partially  supported  by the European \nCommunity 6th Fram ework Prog ramme project \nEASAIER ( IST-033902):  www.easaier.or g.\n5.REFERENC ES\n[1] S. Downie  & S. J. Cunn ingham,  \"Toward  a theor y of \nmusic  information  retrieval  queries:  System  design \nimplications, \" 3rd Internatio nal Conference  on Music \nInformation  Retrieval, Paris,  France , 2002.\n[2] D. Bainbri dge, S. J. Cunningham,  & S. Downie,  \"How \npeople  describe  their  music  information  needs:  A grounded \ntheor y analysis  of musi c queries.,\"  4th Internati onal \nConference  on Music  Information  Retrieval,  Baltimore, \nMarylan d, 2003.\n[3] S. J. Cunni ngham,  N. Reeves , & M. Britland,  \"An \nethno graph ic study of music informatio n seeking:  implicatio ns \nfor the design  of a music  digital  library, \" 3rd ACM/IEEE-CS \njoint conferenc e on Digital  libraries ( JCDL), Houston,  Texas. \n[4] M. Asensio, \"JISC User Requirement  Study for a Mov ing \nPictures  & Sound Portal,\" The Joint  Informatio n Systems \nCommitte e, Final  Report  Nove mber  2003.\n[5] \"British  Library/J ISC Online  Audio  Usability  Evaluati on \nWorkshop, \" Joint  Informatio n Systems  Committe e (JISC), \nLondon, UK 11  October  2004.\n[6] M. Levy  & M. Sandler,  \"Application  of Segm entatio n & \nThumbnailin g to Music Browsin g & Searching,\"  AES 120 th \nConve ntion, Paris,  France, 2006.Binary Stor age RDF Storage ( Music O ntology)Query and access services\nSPARQL Endpoint\nOther Annotation s\nMusical Analysis\nIndexationMetadataAudio Files\nArchive administrator applicationData IngestionAdvan ced Client\nFront End\nBack EndWeb  Client"
    },
    {
        "title": "A Digital Collection of Brazilian Lundus.",
        "author": [
            "Rosana S. G. Lanzelotte",
            "Adriana O. Ballesté",
            "Martha Ulhoa"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415634",
        "url": "https://doi.org/10.5281/zenodo.1415634",
        "ee": "https://zenodo.org/records/1415634/files/LanzelotteBU07.pdf",
        "abstract": "Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAIPMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection.",
        "zenodo_id": 1415634,
        "dblp_key": "conf/ismir/LanzelotteBU07",
        "keywords": [
            "Lundu",
            "Brazilian",
            "popular",
            "musical",
            "form",
            "19th",
            "century",
            "musicologist",
            "Mozart",
            "Araújo"
        ],
        "content": "A DIGITAL COLLECTION OF BRAZILIAN LUNDUSRosana S. G. Lanzelotte Adriana O. Ballesté Martha Ulhoa Unirio – Universidade Federal do Estado do Rio de Janeiro Graduate Music Department Rio de Janeiro - Brazil rosana@unirio.br LNCC – Laboratório Nacional de Computação Científica Rio de Janeiro - Brazil adri@lncc.br Unirio – Universidade Federal do Estado do Rio de Janeiro Graduate Music Department Rio de Janeiro - Brazil mulhoa@unirio.br ABSTRACT Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAI-PMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection.  1. INTRODUCTION During the 19th century, European and African music styles met in Brazil, producing adaptations of European dances such as the waltz, the polka, the schottisch, as well as new genres, such as the suggestive and piquant lundu and the sentimental modinha.  In the surviving scores, lundus are characterized by Africanisms in the lyrics and syncopated rhythm, common elements found in many Brazilian popular forms. Mozart de Araújo (1904 – 1988), one of the most prominent Brazilian musicologists, devoted himself to studying modinhas e lundus [1]. He collected 45 printed and 3 manuscript lundus, which nowadays belong to a private collection, where public access is not allowed. This is the main motivation for the present work. A digital collection has been designed to store the digitized score images, midi files and metadata about the lundus. Dspace [4] has been chosen to implement the repository, as it is free, compliant with the Open Archives Initiative [8], and provides open access to the stored resources. The interoperability with other repositories is assured by OAI-PMH [9] - Protocol for Metadata Harvesting. This is the first on-line collection of such rare lundu score images and midi files. Moreover, it will provide musicologists and musicians with relevant bibliographic information about this repertoire. The implementation of this collection is part of a major project held at UNIRIO – Universidade Federal do Estado do Rio de Janeiro – that aims to make available a comprehensive digital library of Brazilian music [6]. It also takes part in a series of initiatives to preserve Brazilian cultural heritage [2] [5]. 2. IMPLEMENTATION ISSUES  2.1. Dspace functional model  Dspace functional model reflects the structure of an institution. It is hierarchically organized in communities and sub-communities, corresponding to departments or laboratories. Sub-communities contain collections, which are groupings of related items – described by metadata – to which correspond one or more digital resources.  Dspace provides for an authorization procedure, through which groups of users may be granted permission to add/remove and read/write actions on communities, sub-communities, collections, items and resources. A special user – the administrator - is entitled to perform all such actions. The administrator of a collection is also responsible for approving the submission of items. 2.2. Dspace at UNIRIO  Figure 1 illustrates the Dspace community at UNIRIO. It contains the sub-community CEMA – Center for the Memory of the Arts – which contains the collection LUNDUS.  \nFigure 1: Dspace community at UNIRIO © 2007 Austrian Computer Society (OCG).     \n  Figure 2 displays the Dspace screen with metadata describing the lundu “Yáyá, você quer morrer”, as well as the three digital resources associated to it: the score image (yaya_partitura.pdf), the incipit image (yaya_incipit.jpg) and the midi file (yaya_midi.mid).  \n Figure 2: Sample lundu: metadata and digital resources  To improve Dspace user-friendliness, a number of tools have been developed, for example to enable batch submissions of items.  2.3. OAI – PMH: Protocol for Metadata Harvesting  Dspace implements the Protocol for Metadata Harvesting (OAI-PMH) proposed in the scope of OAI to provide interoperability among repositories. Harvesting refers to gathering together metadata from a number of distributed data repositories into a combined data store.  Interoperability in the scope of OAI-PMH stands on the basic element set proposed by the Dublin Core Initiative [3]. Designed to provide a “core” set of descriptive metadata, the fifteen basic elements carry information on title, creator, subject, description, contributor, date, and so on. Consequently, they do not capture typical musical metadata, such as key, measure, instrumentation, etc. The creation of new elements to accommodate application specific metadata is not forbidden, but would interfere with compatibility and interoperability. One could, of course, use repetitively the element description, but this would not contribute to interoperability either. 3. CONCLUDING REMARKS This work has described the implementation of a digital collection of lundus, a typical popular musical form at the 19th century in Brazil. 48 lundu scores collected by Mozart de Araujo have been digitized and stored in a Dspace repository, as well as incipits and midi files.  The main contribution of this work consists in providing accessibility to a rare collection of lundus. Dspace implements the OAI-PMH protocol, and, thus, interoperability with other repositories is assured. The digital collection is worldwide accessible through the link www.unirio.br/ppgm/cema. It is the first digital collection of Brazilian musical scores compliant with the Open Archives Initiative. A drawback of OAI-PMH as currently implemented is that it implies the Dublin Core basic element set as a minimum standard for interoperability. Consequently, the protocol does not cope with application specific metadata, such as measure, key and instrumentation. The element description could store such data, but this would incur a loss of semantics and interoperability.  Dublin Core does not provide for assigning roles to agents, concerned by the elements creator and contributor. In the scope of OAI-PMH, the creator field stores the provenance of the resource, i.e., the institution in which repository the resource is kept. All intellectual agents are mentioned as contributors. Thus, it is not possible to distinguish between the composer and the author of the lyrics. To manage such difficulties, in future work MODS – Metadata Object Description Schema – [7] will be used. MODS, which has proposed by the Library of Congress, is a predefined XML schema for cataloguing all kinds of resources, both digital or non-digital. The connection between MODS and Dspace will be also investigated.  4. ACKNOWLEDGMENTS The authors thank Jupter Martins de Abreu Jr. for the intellectual contribution and Rodrigo De Santis and Tiago Ferreira for helping with the implementation. 5. REFERENCES [1] ARAÚJO, M. A Modinha e o Lundu no Século XVIII. São Paulo: Ed. Ricordi, 1963. [2] CASTRO, B.M. et al. BDB-MUS: a project for the preservation of Brazilian musical heritage. Proceedings of ISMIR 2006. [3] DCMI - Dublin Core Metadata Initiative. <http://www.dublincore.org/>  [4] DSPACE – DSpace Institutional Digital Repository System. <http://www.dspace.org/>.  [5] LANZELOTTE, R. et al. The Portinari Project - Science and Art Team Up Together to Help Cultural Projects. ICHIM 1993, England. [6] LANZELOTTE, R. ULHOA, M. BALLESTÉ, A. Sistemas de Informações Musicais. Revista Opus n.10, 2004, issn 1517-7017. http://www.anppom.com.br/opus/opus10/b_rosana.pdf [7] Metadata Object Description Schema (MODS) <http://www.loc.gov/standards/mods/> [8] OAI – Open Archives Initiative. <http://www.openarchives.org/> [9] OAI-PMH - The Open Archives Initiative Protocol for Metadata Harvesting http://www.openarchives.org/OAI/openarchivesprotocol.html"
    },
    {
        "title": "MIR in Matlab (II): A Toolbox for Musical Feature Extraction from Audio.",
        "author": [
            "Olivier Lartillot",
            "Petri Toiviainen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417145",
        "url": "https://doi.org/10.5281/zenodo.1417145",
        "ee": "https://zenodo.org/records/1417145/files/LartillotT07.pdf",
        "abstract": "We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio files. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio file, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio files, batches of files, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture. 1 MOTIVATION AND APPROACH MIRtoolbox is a Matlab toolbox dedicated to the extraction of musically-related features from audio recordings. It has been designed in particular with the objective of enabling the computation of a large range of features from databases of audio files, that can be subjected to statistical analyses. Few softwares have been proposed in this area. One particularity of our own approach relies in the use of the Matlab computing environment, which offers good visualisation capabilities and gives access to a large variety of other toolboxes. In particular, the MIRtoolbox makes use of functions available in public-domain toolboxes such as the Auditory Toolbox [6], NetLab [5] and SOMtoolbox [10]. Other toolboxes, such as the Statistics toolbox or the Neural Network toolbox from MathWorks, can be directly used for further analyses of the features extracted c⃝2007 Austrian Computer Society (OCG). by MIRtoolbox without having to export the data from one software to another. Such computational framework, because of its general objectives, could be useful to the research community in Music Information Retrieval (MIR), but also for educational purposes. For that reason, particular attention has been paid concerning the ease of use of the toolbox. In particular, complex analytic processes can be designed using a very simple syntax, whose expressive power comes from the use of an object-oriented paradigm. The different musical features extracted from the audio files are highly interdependent: in particular, as can be seen in figure 1, some features are based on the same initial computations. In order to improve the computational efficiency, it is important to avoid redundant computations of these common components. Each of these intermediary components, and the final musical features, are therefore considered as building blocks that can been freely articulated one with each other. Besides, in keeping with the objective of optimal ease of use of the toolbox, each building block has been conceived in a way that it can adapt to the type of input data. For instance, the computation of the MFCCs can be based on the waveform of the initial audio signal, or on the intermediary representations such as spectrum, or mel-scale spectrum (see Fig. 1). Similarly, autocorrelation is computed for different range of delays depending on the type of input data (audio waveform, envelope, spectrum). This decomposition of all feature extraction algorithms into a common set of building blocks has the advantage of offering a synthetic overview of the different approaches studied in this domain of research. 2 FEATURE EXTRACTION",
        "zenodo_id": 1417145,
        "dblp_key": "conf/ismir/LartillotT07",
        "keywords": [
            "MATLAB",
            "audio files",
            "musical features",
            "timbre",
            "tonality",
            "rhythm",
            "form",
            "Statistical analysis",
            "segmentation",
            "clustering"
        ],
        "content": "MIR IN MATLAB (II):\nA TOOLBOX FOR MUSICAL FEATURE EXTRACTION FROM AUDIO\nOlivier Lartillot, Petri Toiviainen\nUniversity of Jyv ¨askyl ¨a\nPL 35(M), 40014, Finland\nABSTRACT\nWe present the MIRtoolbox , an integrated set of functions\nwritten in Matlab, dedicated to the extraction of musical\nfeatures from audio ﬁles. The design is based on a mod-\nular framework: the different algorithms are decomposed\ninto stages, formalized using a minimal set of elementary\nmechanisms, and integrating different variants proposed\nby alternative approaches – including new strategies we\nhave developed –, that users can select and parametrize.\nThis paper offers an overview of the set of features, re-\nlated, among others, to timbre, tonality, rhythm or form,\nthat can be extracted with the MIRtoolbox . One particular\nanalysis is provided as an example. The toolbox also in-\ncludes functions for statistical analysis, segmentation and\nclustering. Particular attention has been paid to the design\nof a syntax that offers both simplicity of use and transpar-\nent adaptiveness to a multiplicity of possible input types.\nEach feature extraction method can accept as argument\nan audio ﬁle, or any preliminary result from intermediary\nstages of the chain of operations. Also the same syntax\ncan be used for analyses of single audio ﬁles, batches of\nﬁles, series of audio segments, multi-channel signals, etc.\nFor that purpose, the data and methods of the toolbox are\norganised in an object-oriented architecture.\n1 MOTIVATION AND APPROACH\nMIRtoolbox is aMatlab toolbox dedicated to the extrac-\ntion of musically-related features from audio recordings.\nIt has been designed in particular with the objective of en-\nabling the computation of a large range of features from\ndatabases of audio ﬁles, that can be subjected to statistical\nanalyses.\nFew softwares have been proposed in this area. One\nparticularity of our own approach relies in the use of the\nMatlab computing environment, which offers good visu-\nalisation capabilities and gives access to a large variety of\nother toolboxes. In particular, the MIRtoolbox makes use\nof functions available in public-domain toolboxes such\nas the Auditory Toolbox [6],NetLab [5] and SOMtoolbox\n[10]. Other toolboxes, such as the Statistics toolbox or\ntheNeural Network toolbox from MathWorks, can be di-\nrectly used for further analyses of the features extracted\nc\r2007 Austrian Computer Society (OCG).byMIRtoolbox without having to export the data from one\nsoftware to another.\nSuch computational framework, because of its general\nobjectives, could be useful to the research community in\nMusic Information Retrieval (MIR), but also for educa-\ntional purposes. For that reason, particular attention has\nbeen paid concerning the ease of use of the toolbox. In\nparticular, complex analytic processes can be designed us-\ning a very simple syntax, whose expressive power comes\nfrom the use of an object-oriented paradigm.\nThe different musical features extracted from the au-\ndio ﬁles are highly interdependent: in particular, as can be\nseen in ﬁgure 1, some features are based on the same ini-\ntial computations. In order to improve the computational\nefﬁciency, it is important to avoid redundant computations\nof these common components. Each of these intermediary\ncomponents, and the ﬁnal musical features, are therefore\nconsidered as building blocks that can been freely artic-\nulated one with each other. Besides, in keeping with the\nobjective of optimal ease of use of the toolbox, each build-\ning block has been conceived in a way that it can adapt to\nthe type of input data. For instance, the computation of\nthe MFCCs can be based on the waveform of the initial\naudio signal, or on the intermediary representations such\nas spectrum, or mel-scale spectrum (see Fig. 1). Similarly,\nautocorrelation is computed for different range of delays\ndepending on the type of input data (audio waveform, en-\nvelope, spectrum). This decomposition of all feature ex-\ntraction algorithms into a common set of building blocks\nhas the advantage of offering a synthetic overview of the\ndifferent approaches studied in this domain of research.\n2 FEATURE EXTRACTION\n2.1 Feature overview\nFigure 1 shows an overview of the main features imple-\nmented in the toolbox. All the different processes start\nfrom the audio signal (on the left) and form a chain of\noperations proceeding to right. Each musical feature is\nrelated to one of the musical dimensions traditionally de-\nﬁned in music theory. Boldface characters highlight fea-\ntures related to pitch and tonality. Bold italics indicate\nfeatures related to rhythm. Simple italics highlight a large\nset of features that can be associated to timbre and dynam-\nics. Among them, all the operators in grey italics can beAudio signal\nwaveformZero-crossing rate\nRMS energy\nEnvelopeLow Energy Rate\nAttack SlopeAttack Time\nEnvelope Autocorrelation TempoOnsets\nKey strength\nKey SOMKey\nMode\nPitchSpectrum\nTonal Centroid\nPulse clarity Spectral flux SpectrumFilterbankCentroid, Kurtosis, Spread, Skewness\nFlatness, Roll-off,  Entropy\nMFCC\nFluctuationBrightness, Roughness, Inharmonicity, Irregularity\nFlux\nMel-scale spectrum\nCepstrumChromagram\nAutocorrelationFigure 1 .Overview of the musical features that can be extracted with MIRtoolbox .\nin fact applied to many different representations.1\nMore elaborate tools have also been implemented that\ncan carry out higher-level analyses and transformations.\nIn particular, audio ﬁles can be automatically segmented\ninto a series of homogeneous sections, through the estima-\ntion of temporal discontinuities along diverse alternative\nfeatures such as timbre in particular [2, 3].\n2.2 Example: Rhythm analysis\nThe estimation of rhythmic pulsation, for instance, can be\nbased on the modelling of auditory perception of sound\nand music [8] (circled in Figure 1), as described in ﬁg-\nure 2. The audio signal is ﬁrst decomposed into auditory\nchannels using a bank of ﬁlters. Diverse types of ﬁlter-\nbanks have been implemented, and the number of chan-\nnels can be speciﬁed, as can be seen in the syntax shown\nbelow at code line (1). The envelope of each channel is\nthen extracted (2). As pulsation is generally related to\nincrease of energy only, the envelopes are differentiated,\nhalf-wave rectiﬁed, before being ﬁnally summed together\nagain (3). This gives a precise description of the variation\nof energy produced by each note event from the different\nauditory channels.\nAfter this onset detection, the periodicity is estimated\nthrough autocorrelation (5)2. However, if the tempo varies\nthroughout the piece, an autocorrelation of the whole se-\nquence will not show clear periodicities. In such cases it is\nbetter to compute the autocorrelation for a frame decom-\nposition (4)3. This yields a periodogram that highlights\nthe different periodicities, as shown in ﬁgure 2. In order to\nfocus on the periodicities that are more perceptible, the pe-\nriodogram is ﬁltered using a resonance curve [7] (5), after\nwhich the best tempos are estimated through peak picking\n(6), and the results are converted into beats per minute (7).\n1For more details about the feature extraction algorithms, see [3].\n2For the sake of clarity, several options in the following functions\nhave been omitted.\n3This shows an example of MIRtoolbox function that can adapt to\nthe type of input (audio waveform, envelope, etc.). Here, the frame size\nis 3 seconds and the hop factor .1.Due to the difﬁculty of choosing among the possible mul-\ntiples of the tempo, several candidates (three for instance)\nmay be selected for each frame, and a histogram of all the\ncandidates for all the frames, called periodicity histogram,\ncan be drawn (8).\nfb = mirfilterbank(a,20) (1)\ne = mirenvelope(fb,\n’Diff’,’Halfwave’,’Center’) (2)\ns = mirsum(e,’Center’) (3)\nfr = mirframe(s,3,.1) (4)\nac = mirautocor(fr,’Resonance’) (5)\np = mirpeaks(ac,’Total’,1) (6)\nt = mirtempo(p) (7)\nh = mirhisto(t) (8)\nThe whole process can be executed in one single line by\ncalling directly the mirtempo function with the audio input\nas argument:\nmirtempo(a,’Frame’) (9)\n2.3 Data analysis\nThe toolbox includes diverse tools for data analysis, such\nas a peak extractor, and functions that compute histograms,\nentropy, zero-crossing rates, irregularity or various statis-\ntical descriptors (centroid, spread, skewness, kurtosis, ﬂat-\nness) on data of various types, such as spectrum, envelope\nor histogram.\nThemirpeaks functions can accept any data returned\nby any other function of the MIRtoolbox and can adapt to\nthe different kinds of data of any number of dimensions.\nIn the graphical representation of the results, the peaks\nare automatically located on the corresponding curves (for\n1D data) or bit-map images (for 2D data). We have de-\nsigned a new strategy of peak selection, based on a notionAudio\nwaveformFilter\nbankAuto\ncorrelationFilter PeaksResonancecurve\nEnvelopes\nextraction\nDiff\n HWR\n +\nChannelsTempoPeriodo\ngramOnsets\nTime Time \nDelays Figure 2 .Successive steps for the estimation of tempo illustrated with the analysis of an audio excerpt. In the peri-\nodogram, high autocorrelation values are represented by bright nuances.\nofcontrast , discarding peaks that are not sufﬁciently con-\ntrastive (based on a certain threshold) with the neighbour-\ning peaks. This adaptive ﬁltering strategy hence adapts\nto the local particularities of the curves. Its articulation\nwith other more conventional thresholding strategies leads\nto an efﬁcient peak picking module that can be applied\nthroughout the MIRtoolbox .\nSupervised classiﬁcation of musical samples can also\nbe performed, using techniques such as K-Nearest Neigh-\nbours or Gaussian Mixture Models. One possible applica-\ntion is the classiﬁcation of audio recordings into musical\ngenres. The results of feature extraction processes can be\nstored as text ﬁles of various format, such as the ARFF\nformat that can be exported in the Weka machine learning\nenvironment [11].\n3 DESIGN OF THE TOOLBOX\n3.1 Data encapsulation\nAll the data returned by the functions in the toolbox are\nencapsulated into typed objects. The default display method\nassociated to all these objects is a graphical display of the\ncorresponding curves. In this way, when the display of\nthe values of a given analysis is requested, what is printed\nis not a listing of long vectors or matrices, but rather a\ncorrectly formatted graphical representation.\nThe actual data matrices associated to those data can be\nobtained by calling a method called mirgetdata , which\nconstructs the simplest possible data structure associated\nto the data.\n3.2 Adaptive syntax\nAs explained previously, the diverse functions of the tool-\nbox can accept alternative input. The name of a particular\naudio ﬁle (either in wav or au format) can be directly spec-\niﬁed as input:\nmirspectrum(’myfile’) (10)\nAlternatively, the audio ﬁle can be ﬁrst loaded using the\nmiraudio function, which can perform diverse opera-\ntions such as resampling, automated trimming of the si-\nlence at the beginning and/or at the end of the sequence,extraction of a given subsequence, centering, normaliza-\ntion with respect to RMS energy, etc.\na = miraudio(’myfile’,’Sampling’,11025,\n’Trim’,’Extract’,2,3,\n’Center’,’Normal’) (11)\nmirspectrum(a) (12)\nBatch analyses of audio ﬁles can be carried out by sim-\nply replacing the name of the audio ﬁle by the keyword\n’Folder’ .\nmirspectrum(’Folder’) (13)\nAny feature extraction can be based on the result of a\nprevious computation. For instance, the autocorrelation of\na spectrum curve can be computed as follows:\ns = mirspectrum(a) (14)\nas = mirautocor(s) (15)\nAnd product of curves can be performed easily:\nmirautocor(a) *mirautocor(s) (16)\nIn this particular example, the waveform autocorrelation\nmirautocor(a) is automatically converted to frequency\ndomain in order to be combined with the spectrum auto-\ncorrelation mirautocor(s) .\n3.3 Memory optimization\nThe ﬂexibility of the syntax requires a complex data repre-\nsentation that can handle alternative conﬁgurations (frame\nand/or channels decompositions, segmentation, batch anal-\nysis) [3]. This data structure could in theory become very\nextensive in terms of memory usage, especially if entire\nfolders of audio ﬁles are loaded into the memory in one\ngo. To overcome this, we have designed new methods\nallowing a better management of memory without dete-\nrioration of the syntactical simplicity and power. Audio\nﬁles are loaded one after the other in the memory, and if\nnecessary, long audio ﬁles are also divided into a series\nof successive blocks of frames that are loaded one after\nthe other. We plan to further optimise the computational\nefﬁciency of the toolbox by proposing the possibility of\ndistributing the computational loads among a network of\ncomputers, with the help of the Distributed Computing\nToolbox andEngine made available by Matlab.3.4 Software Development Kit\nThe different feature extraction algorithms will be pro-\ngressively reﬁned and new features will be added in fu-\nture versions of the MIRtoolbox . Users are encouraged to\nwrite their own functions, using the building blocks of-\nfered by the current version. A set of meta-functions have\nbeen designed that enable the writing of additional algo-\nrithms using very simple function templates. As the meta-\nfunctions take care of all the complex management of the\ndata structure and methods, the development of new algo-\nrithms can concentrate simply on the purely mathematical\nand DSP considerations. This may result in a computa-\ntional environment where large-scale MIR systems could\nbe developed, articulated one with each other, and com-\npared.\n4 RELATED WORKS\nFew softwares have been proposed in this area. Marsyas\n[9], is a framework for prototyping and experimentation\nwith computer audition applications. The architecture is\nbased on dataﬂow programming, where computation is\nexpressed as a network of processing nodes and compo-\nnents. Users can build their own dataﬂow network us-\ning a scripting language at run-time. The advantage of an\nobject-oriented paradigm, as used in the MIRtoolbox, is\nthe possibility of a drastic reduction of the syntax com-\nplexity4. Rather than a ready-to-use set of applications,\nMarsyas is a framework for building applications, pro-\nvided with a dozen of features restricted to low-level di-\nmensions. Batch of ﬁles can be processed as long as the\nsampling rate remains constant, as Marsyas does not, con-\ntrary to the MIRtoolbox, perform automatic sampling con-\nversion (except between 44100Hz and 22050Hz).\njAudio [4] is another framework for feature extraction\nwritten in Java. One particularity of this approach is the\nuse of a GUI for the feature selection, and the design of a\nmechanism for automated dependency handling to prevent\nduplicate calculations.\nMIRtoolbox includes most of the features (or variant of\nthem) available in the aforementioned frameworks, plus a\ndiverse collection of other lower and higher-level features\nrelated to musical dimensions such as timbre, key, rhythm,\netc. The simplicity and power of its syntax enables easy\ncombinations of the various operators and feature extrac-\ntors. Another speciﬁc advantage of MIRtoolbox concerns\nits extended visualisation capabilities, beneﬁting from the\nrichness of Matlab computing environment.\n5 A VAILABILITY OF THE MIRTOOLBOX\nFollowing our ﬁrst Matlab toolbox, called MIDItoolbox\n[1], dedicated to the analysis of symbolic representations\n4For instance, the patching example proposed in Marsyas User Man-\nual, which required 20 lines in Marsyas Scripting Language, can be im-\nplemented in four lines using the MIRtoolbox syntax.of music, the MIRtoolbox is offered for free to the research\ncommunity.5\n6 ACKNOWLEDGMENTS\nThis work has been supported by the European Commis-\nsion (NEST project “Tuning the Brain for Music”, code\n028570). The development of the toolbox has beneﬁt-\nted from productive collaborations with the other partners\nof the project, in particular Tuomas Eerola, Jose Fornari,\nMarco Fabiani, and students of our department.\n7 REFERENCES\n[1] Eerola, T. and P. Toiviainen. ”MIR in Matlab: The\nMidi Toolbox”, Proceedings of 5th International Con-\nference on Music Information Retrieval , 22–27, 2004.\n[2] Foote, J. and M. Cooper. ”Media Segmentation using\nSelf-Similarity Decomposition”, Proceedings of SPIE\nStorage and Retrieval for Multimedia Databases ,\n5021, 167–175, 2003.\n[3] Lartillot, O. and P. Toiviainen. ”A Matlab Toolbox for\nMusical Feature Extraction from Audio”, Proceedings\nof the International Conference on Digital Audio Ef-\nfects, 2007.\n[4] McEnnis, D., C. McKay, I. Fujinaga, and P. Depalle.\n2005. ”jAudio: A feature extraction library”, Proceed-\nings of the International Symposium on Music Infor-\nmation Retrieval , 600–3, 2005.\n[5] Nabney, I. ”NETLAB: Algorithms for pattern recog-\nnition”, Springer Advances In Pattern Recognition Se-\nries, 2002.\n[6] Slaney, M. Auditory Toolbox Version 2 , Technical Re-\nport. Interval Research Corporation, 1998-010, 1998.\n[7] Toiviainen, P. and J.S. Snyder. ”Tapping to Bach:\nResonance-based modeling of pulse”, Music Percep-\ntion, 21-1, 43–80, 2003.\n[8] Tzanetakis, G. and P. Cook. ”Multifeature Audio Seg-\nmentation for Browsing and Annotation”, Proceed-\nings of the 1999 IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , 1999.\n[9] Tzanetakis, G. and P. Cook. ”MARSYAS: A Frame-\nwork for Audio Analysis”, Organized Sound , 4-3,\n2000.\n[10] Vesanto, J. ”Self-Organizing Map in Matlab: the SOM\nToolbox”, Proceedings of the Matlab DSP Confer-\nence, 35–40, 1999.\n[11] Witten, I. H. and E. Frank. Data Mining: Practical\nmachine learning tools and techniques , 2nd Edition,\nMorgan Kaufmann, San Francisco, 2005.\n5It can be downloaded from the following URL: http://users.\njyu.fi/ ˜lartillo/mirtoolbox"
    },
    {
        "title": "TagATune: A Game for Music and Sound Annotation.",
        "author": [
            "Edith L. M. Law",
            "Luis von Ahn",
            "Roger B. Dannenberg",
            "Mike Crawford"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415568",
        "url": "https://doi.org/10.5281/zenodo.1415568",
        "ee": "https://zenodo.org/records/1415568/files/LawADC07.pdf",
        "abstract": "Annotations of audio files can be used to search and index music and sound databases, provide data for system evaluation, and generate training data for machine learning. Unfortunately, the cost of obtaining a comprehensive set of annotations manually is high. One way to lower the cost of labeling is to create games with a purpose that people will voluntarily play, producing useful metadata as a by-product. TagATune is an audio-based online game that aims to extract descriptions of sounds and music from human players. This paper presents the rationale, design and preliminary results from a pilot study using a prototype of TagATune to label a subset of the FreeSound database. 1 INTRODUCTION Human computation, the idea of channeling the collective human presence over networks to solve difficult AI problems, has had great success. One of the first realizations of this idea is an online game called ESP [10], later adopted as the Google image labeler, where two players collaborate to label images on the internet. The result is one of the largest existing databases of images with labeled content. More than half of the nation now has access to the internet, and 42% of those with access play games online [6]. The so-called games with a purpose take advantage of this burgeoning human interest in online games to solve important technological problems. This paper presents the rationale and iterative design of a new game called TagATune, which elicits from human players descriptions of sounds and music (collectively referred to as tunes). 2 RATIONALE TagATune is a human computation tool that is capable of gathering perceptually meaningful descriptions for audio data that are agreed upon by multiple players. Such data are useful for several purposes described below. Large audio databases have become invaluable resources for listeners, sound designers and composers. Current audio retrieval systems are primarily text-based, relying on accurate and comprehensive annotation of the data. However, keywords that describe a particular audio file are often subjective, based on one person’s opinion [11]. c⃝2007 Austrian Computer Society (OCG). TagATune has the potential to produce better labels at lower cost because the labor is essentially free and the validity of each label is confirmed by many players. Researchers have long been interested in what makes a tune warm or cold, scary or pleasant, bright or dull, happy or sad and to what extent. For example, ongoing studies attempt to relate the perceptions [2, 7] of sounds to their acoustical and physical properties. A better understanding of auditory perception can yield important insights for retrieval tasks and auditory-enhanced interfaces. Part of the labeled data collected by TagATune is based on shared perception of sounds and are useful towards this kind of psychoacoustic or phenomenological research. Another application is creating CAPTCHAs for the visually impaired. As a line of defense against bots, many websites now require humans to pass a visual CAPTCHA [9], which usually involves reading distorted characters against cluttered backgrounds. Common imageand textbased CAPTCHAs are inaccessible to the visually impaired. In addition, audio CAPTCHAs is a much needed alternative to visual CAPTCHAs, which have been successfully broken by various algorithms [4, 5]. Audio CAPTCHAs based on recorded speech exist, but as automatic speech recognition systems improve, labeled audio may offer more effective alternatives. TagATune differs from two recently developed music annotation games, MajorMiner [3] and The Listen Game [8], in several ways. First, in TagATune, players are immersed in an audio environment that is not limited to music, but also a variety of sound clips, which are essential for the production of usable sound CAPTCHAs. Second, instead of playing offline against a database [3] or simultaneously against multiple players [8], players of TagATune are paired with a partner with whom they must collaborate to label a given tune. This enhanced level of rapport shown to be critical in the popularity of the ESP game. Finally, in addition to tags, TagATune collects comparative information about sounds and music, which allows for more discriminative audio search. 3 TAGATUNE: A PROTOTYPE A prototype of TagATune is built in order to experiment with the game mechanics and to evaluate the usability and reception of the game. TagATune prototype is a simplified version of the game which uses sound clips only, lacking a blind-accessible interface, safeguards against cheating and inappropriate content, and other text matching and verification capability such as spell checking and synonym matching. In addition, the prototype is a simulated two-player game. In it, players have the illusion that they are playing with human partners, whereas in reality, they are playing with a simulated player, or a bot, whose moves and timing come from recorded sequences of previous gameplay. The use of a bot is an indispensable component of the game, since it ensures that every player is paired when there is an odd number of them. The audio used in the TagATune prototype are sound clips provided by the FreeSound Project. Within two months of its release, the project had “attracted over 2,300 members with over 1,650 sound contributions totaling more than 300 minutes of sound” (freesound.iua.upf.edu) and tens of thousands of downloads. Clips can be field recordings or synthesized audio containing a wide range of possible sounds, including music, rhythm, effects, ambience noise and speech. The collection of 23,084 sounds in the database vary between 0 and 4,522s in duration. For the prototype, 100 sound clips that are approximately 10 seconds long were randomly chosen. Limiting the amount of data makes it feasible to evaluate manually the quality and universality of the descriptions independent players submitted for each sound.",
        "zenodo_id": 1415568,
        "dblp_key": "conf/ismir/LawADC07",
        "keywords": [
            "human computation",
            "audio-based online game",
            "collective human presence",
            "perceptually meaningful descriptions",
            "audio retrieval systems",
            "audio CAPTCHAs",
            "audio databases",
            "audio-based annotation games",
            "audio-based CAPTCHAs",
            "audio-based interfaces"
        ],
        "content": "TAGATUNE: A GAME FOR MUSIC AND SOUND ANNOTATION\nEdith L. M. Law, Luis von Ahn, Roger B. Dannenberg, Mike Crawford\nCarnegie Mellon University\nSchool of Computer Science\nABSTRACT\nAnnotations of audio ﬁles can be used to search and in-\ndex music and sound databases, provide data for system\nevaluation, and generate training data for machine learn-\ning. Unfortunately, the cost of obtaining a comprehensive\nset of annotations manually is high. One way to lower the\ncost of labeling is to create games with a purpose that peo-\nple will voluntarily play, producing useful metadata as a\nby-product. TagATune is an audio-based online game that\naims to extract descriptions of sounds and music from hu-\nman players. This paper presents the rationale, design and\npreliminary results from a pilot study using a prototype of\nTagATune to label a subset of the FreeSound database.\n1 INTRODUCTION\nHuman computation, the idea of channeling the collective\nhuman presence over networks to solve difﬁcult AI prob-\nlems, has had great success. One of the ﬁrst realizations of\nthis idea is an online game called ESP [10], later adopted\nas the Google image labeler, where two players collabo-\nrate to label images on the internet. The result is one of the\nlargest existing databases of images with labeled content.\nMore than half of the nation now has access to the in-\nternet, and 42% of those with access play games online\n[6]. The so-called games with a purpose take advantage\nof this burgeoning human interest in online games to solve\nimportant technological problems. This paper presents\nthe rationale and iterative design of a new game called\nTagATune , which elicits from human players descriptions\nof sounds and music (collectively referred to as tunes).\n2 RATIONALE\nTagATune is a human computation tool that is capable of\ngathering perceptually meaningful descriptions for audio\ndata that are agreed upon by multiple players. Such data\nare useful for several purposes described below.\nLarge audio databases have become invaluable resources\nfor listeners, sound designers and composers. Current\naudio retrieval systems are primarily text-based, relying\non accurate and comprehensive annotation of the data.\nHowever, keywords that describe a particular audio ﬁle\nare often subjective, based on one person’s opinion [11].\nc/circlecopyrt2007 Austrian Computer Society (OCG).TagATune has the potential to produce better labels at lower\ncost because the labor is essentially free and the validity\nof each label is conﬁrmed by many players.\nResearchers have long been interested in what makes a\ntune warm orcold,scary orpleasant ,bright ordull,happy\norsadand to what extent. For example, ongoing studies\nattempt to relate the perceptions [2, 7] of sounds to their\nacoustical and physical properties. A better understand-\ning of auditory perception can yield important insights for\nretrieval tasks and auditory-enhanced interfaces. Part of\nthe labeled data collected by TagATune is based on shared\nperception of sounds and are useful towards this kind of\npsychoacoustic or phenomenological research.\nAnother application is creating CAPTCHAs for the vi-\nsually impaired. As a line of defense against bots, many\nwebsites now require humans to pass a visual CAPTCHA\n[9], which usually involves reading distorted characters\nagainst cluttered backgrounds. Common image- and text-\nbased CAPTCHAs are inaccessible to the visually impaired.\nIn addition, audio CAPTCHAs is a much needed alterna-\ntive to visual CAPTCHAs, which have been successfully\nbroken by various algorithms [4, 5]. Audio CAPTCHAs\nbased on recorded speech exist, but as automatic speech\nrecognition systems improve, labeled audio may offer more\neffective alternatives.\nTagATune differs from two recently developed music\nannotation games, MajorMiner [3] and The Listen Game\n[8], in several ways. First, in TagATune, players are im-\nmersed in an audio environment that is not limited to mu-\nsic, but also a variety of sound clips, which are essential\nfor the production of usable sound CAPTCHAs. Second,\ninstead of playing ofﬂine against a database [3] or simulta-\nneously against multiple players [8], players of TagATune\nare paired with a partner with whom they must collabo-\nrate to label a given tune. This enhanced level of rapport\nshown to be critical in the popularity of the ESP game.\nFinally, in addition to tags, TagATune collects compara-\ntive information about sounds and music, which allows for\nmore discriminative audio search.\n3 TAGATUNE: A PROTOTYPE\nA prototype of TagATune is built in order to experiment\nwith the game mechanics and to evaluate the usability and\nreception of the game. TagATune prototype is a simpliﬁed\nversion of the game which uses sound clips only, lack-\ning a blind-accessible interface, safeguards against cheat-\ning and inappropriate content, and other text matchingand veriﬁcation capability such as spell checking and syn-\nonym matching. In addition, the prototype is a simulated\ntwo-player game. In it, players have the illusion that they\nare playing with human partners, whereas in reality, they\nare playing with a simulated player, or a bot, whose moves\nand timing come from recorded sequences of previous game-\nplay. The use of a bot is an indispensable component of\nthe game, since it ensures that every player is paired when\nthere is an odd number of them.\nThe audio used in the TagATune prototype are sound\nclips provided by the FreeSound Project. Within two months\nof its release, the project had “attracted over 2,300 mem-\nbers with over 1,650 sound contributions totaling more\nthan 300 minutes of sound” (freesound.iua.upf.edu) and\ntens of thousands of downloads. Clips can be ﬁeld record-\nings or synthesized audio containing a wide range of pos-\nsible sounds, including music, rhythm, effects, ambience\nnoise and speech. The collection of 23,084 sounds in the\ndatabase vary between 0 and 4,522s in duration. For the\nprototype, 100 sound clips that are approximately 10 sec-\nonds long were randomly chosen. Limiting the amount\nof data makes it feasible to evaluate manually the quality\nand universality of the descriptions independent players\nsubmitted for each sound.\n3.1 Design\nOne way to assure that the description of a sound is mean-\ningful is to have two independent players agree on a par-\nticular description, a mechanism that has proven to work\nwell labeling images in the ESP Game [10]. Similar to\nESP, players of TagATune are not asked to describe the\nsound, but told to guess what their partners are thinking.\nTagATune is played by partners, who are paired randomly\nand anonymously from a pool of available players.\nThe partners are given three minutes to come up with\nagreed descriptions for as many sounds as possible. In\neach round, a sound is randomly selected from the database\nand presented to the partners. Controls are available for\nstopping or replaying the sound.\nA main difference between TagATune and ESP is that\nwhat people hear in a sound is often more subjective, am-\nbiguous, and imaginative than what they see in an im-\nage. The same sound can be perceived to come from\nvery different objects. For example, a “hissing” sound can\ncome from a cat, a tea boiler or a snake [1]. While natu-\nral sounds can be described in terms of their (imagined)\nsource, music is more abstract, and even the descriptive\nterms for music are ambiguous. In order to elicit from\nplayers speciﬁc and well-deﬁned annotation of the audio,\nTagATune displays a category word which speciﬁes what\nkind of description the game is seeking. The eight cate-\ngory words used in the prototype are Object ,Place ,Ac-\ntion,Color ,Mood ,Movie Genre ,Opposite (describe what\nthe sound is not), and Freebie (unrestricted description).\nPlayers enter guesses on what they think their partners\nhear. The players’ guesses so far are displayed to remind\nthem of their previous guesses, allowing them to strategi-\ncally plan their next move. Players can also choose to passon a difﬁcult sound. Both partners must pass before the\nnext sound is presented. Finally, the partners are not al-\nlowed any communication with each other, although they\nare notiﬁed of their partners’ activities.\nA description becomes an ofﬁcial tag that can be used\nfor search when it is agreed upon by enough people, the\nthreshold of which will depend on game statistics.\n3.2 Lessons Learned\nThe effectiveness of TagATune to collect meaningful de-\nscriptions of sounds rides on two factors — that the game\nis fun enough to attract players on a regular basis, and\nthat individuals agree, to a certain degree, on the descrip-\ntions of a given sound, even in an open-ended category\nsuch as color and movie genre. The prototype enables us\nto evaluate the extent to which the current game mechan-\nics achieve these two goals, and what design changes are\nnecessary to ﬁll the gap.\nOver the course of 5 days, 54 people signed up to play\nthe prototype game. The system recorded sequences of\ngameplay, including every description and passed trial,\nand their associated timestamps. At the end of the evalu-\nation period, players were asked for feedback concerning\ntheir impression of the prototype.\n3.2.1 TagATune as a Game\nFun is a vague concept that is difﬁcult to characterize,\nsince many different elements in a game come together\nto create the speciﬁc experience. The design of this game\nfocuses on three of these elements, namely its ability to\ncreate for each player a sense of competence, a pleasant\nand interesting sensory experience, and the opportunity to\nconnect with their partners. Table 1 shows the average\nrating (on a ﬁve point scale) to questions related to the\nenjoyability of the game.\nDid you ﬁnd this game enjoyable? 3.3\nDid you like playing with your partner? 3.5\nAre you likely to play this game again? 3.5\nTable 1 . How enjoyable is TagATune? Average rating on\nthe scale of 1 to 5, provided by 11 players who ﬁlled in\nthe survey at the end of the game. In this scale, 1=Not at\nall, 3=Somewhat, 5=Very.\nHaving a sense of competence at the game is an impor-\ntant source of motivation for players to revisit the game.\nAfter having people play the prototype game, it was im-\nmediately apparent that ﬁnding speciﬁc descriptions for a\ngiven sound is a difﬁcult task. The main problem is that\nthe randomly chosen category word is not always appro-\npriate for a given sound. In order to address this problem,\nthe ﬁnal game should have a mechanism for ﬁnding suit-\nable category words for each sound. Solutions include\nallowing free form entry of descriptions without a cate-\ngory constraint, allowing one of the two partners to se-\nlect the category word most appropriate for a given sound,and eliminating sound clip / word category pairs for which\nplayers often pass.\nTagATune must be able to provide a consistently inter-\nesting sensory experience for its players. In particular, it\nwas evident that the uneven quality of the sounds used in\nthe pilot study signiﬁcantly hindered the experience of the\ngame. One solution is to have partners rate the sound. For\nexample, pairs of players can get extra points by giving the\nsound a similar rating. This rating information can also be\nuseful to sound designers and music producers, who may\nbe interested in ﬁrst-impression judgements by the pub-\nlic. Unpleasant sounds can still be labeled as long as their\noccasional presentation does not degrade with the overall\nuser experience. The same rating system is applicable to\nmusic.\n3.2.2 TagATune as a Data Collection Tool\nThe most basic premise of any audio annotation games\nis that people generally perceive similar things in sounds\nand music. An encouraging observation from a pen-and-\npaper trial of the game is that there exist striking similar-\nities among different people’s descriptions of an arbitrary\naudio ﬁle. For example, when asked to describe the mood\nof a ﬁreworks sound clip, more than one person put down\nwords such as “exciting” and “happy.” Likewise, for a\nmellow guitar solo, common colors such as “green” and\n“yellow” were used.\nTable 2 shows some of the tags that were collected by\nthe prototype. Most notably, in contrast to the description\ngiven by the author of the sound (ﬁrst column), the de-\nscriptions by the players are usually simpler (single words\nor short phrases instead of sentences), perceptually more\nmeaningful (high-level concepts conveyed by the sound\ninstead of detailed description of how the sound is made),\nand more amenable to use as keywords in audio search.\nSound description Category Tags\nRecorded sound of my\nmouth with re-verb andplace forest, wood, jun-\ngle\necho freebie frog, cricket\nobject insect\nﬁeld recording of the\nmuehlkreisbahn — a smallaction driving, braking\nregional train which con-\nnects the northern part ofobject car, truck, motor-\ncycle\nupper austria with the town\nof linzplace on the bus, restau-\nrant, ship, train\nstation, factory\nThis drone is similar to\n“Dronetail 04” but a bit\nmore bright.mood alarming, freaky,\nscary, dark, creepy\nrubbed glass, granular syn-\nthesisobject paper\nTable 2 . Descriptions of four different sounds: author\nversus players\nA second observation is that the descriptions under dif-ferent categories can help to reinforce each other. For ex-\nample, the sounds of “frog,” “cricket,” and “insect” also\nremind the players of “forest,” “wood,” and “jungle.” Like-\nwise, for the sound that seems to be produced by motor\nvehicles, the places associated with that sound tend to be\nwhere cars can be heard.\nFinally, the pilot study showed that some category words\nare meaningless for certain sounds. 36% of the time, the\nplayers opted to pass instead of providing a description for\na sound.\n4 TAGATUNE: FINAL DESIGN\nThe design of the ﬁnal game beneﬁted enormously from\nthe lessons learned from this prototype. The ﬁnal TagATune\ndesign consists of two different rounds - the annotation\nround and the comparison round. The annotation round\nis identical to the prototype rounds, except that one of the\ntwo players gets to choose the category word for the tune.\nIn addition, each category word is attached with a score in-\ndicating its difﬁculty to describe with respect to the tune.\nThis gives players the ﬂexibility to choose an easier an-\nnotation task for fewer points, or a harder task for more\npoints but a higher chance of never arriving at an agree-\nment with their partners.\nFigure 1 . Preliminary TagATune Interface\nThe comparison round presents a tune, and asks the\nplayers to compare it to one or two other tunes of the same\ntype based on a particular question. There are three kinds\nof questions. Preference questions ask “which of the two\ntunes do you prefer?” Similarity questions ask “which of\nthe two tunes is more similar to this third tune?” Per-\nception questions ask “which of the two tunes is more X”\nwhere X is a description previously provided by the play-\ners for one of the tunes. The data collected in the compari-\nson round is more ﬁne-grained because the pairwise com-\nparisons can be converted into a ranking, which describes\nnot only how the audio is perceived by the players, but to\nwhat extent .\nPreference questions extract from each player a set of\npairwise preferences for each tune. From the pairwisepreferences, a complete ranking can be calculated for in-\ndividual players as well as for pools of players. This in-\nformation, together with the data gathered from the sim-\nilarity questions, will allow the game to selectively pick\naudio that players enjoy, thus motivating them to return\nto play the game. In addition, TagATune will provide the\nadditional functionality for uploading one’s own sounds\nand music into the game to be annotated by other play-\ners. Emerging artists can therefore use our game to test\nout the potential popularity of their creations. Finally, this\ndata can be used to improve music recommendation and\nautomatic playlist generation.\nPerception questions are constrained similarity ques-\ntions which ask players to compare the extent to which\ndifferent sounds and music can be described by a partic-\nular tag. For example, instead of asking ifa tune is ex-\nciting, the question permits us to ask how exciting it is,\nin comparison to other tunes. Audio search by similarity\ncan beneﬁt enormously from the availability of this type\nof similarity data.\n5 CONCLUSION\nIn this paper, we have introduced TagATune, an audio-\nbased game with a purpose for the annotation of sounds\nand music. TagATune has the potential to improve audio\nsearch, CAPTCHA accessibility, and the understanding of\nauditory perception.\nThe results from the prototype experiments are encour-\naging. Many of the labels of the sounds collected by the\ngame are more descriptive and perceptually meaningful\nthan the descriptions given by the authors of the sounds. 8\nout of 11 of the participants who ﬁlled in the survey ﬁnd\nthe game (somewhat, quite, or very) enjoyable, and 10 out\nof 11 said that they are (somewhat, quite or very likely) to\nplay the game again.\nThe fact that TagATune is a labeler for audio data rather\nthan images raises a unique set of research questions. One\nof the most obvious differences is that sounds and music\ntake time to listen to, whereas in ESP, players can almost\ninstantaneously judge the content of an image. To mini-\nmize the risk of longer audio ﬁles being skipped by play-\ners, it may be advantageous to break them up into smaller\nsegments, and serve those segments instead in the game.\nOn the other hand, arbitrarily truncating a tune may render\nit incomprehensible. In the future, we expect to perform\nautomatic segmentation or perhaps even make segmenta-\ntion a task for a new game.\nAs an effective data collection tool, TagATune must\nﬁrst fulﬁll the requirement of being attractive to its play-\ners. The success of TagATune rides on the ability of the\nsystem to control the enjoyability and difﬁculty of the game\nby pre-determining for any given tune, whether it is en-\njoyable to listen to and which category words are most\nappropriate. While TagATune is built to elicit human in-\ntelligence to solve the audio segmentation and classiﬁca-\ntion problem, ironically, the game itself must also tackle\nsome of the same problems in order to be attractive to itsplayers.\nThe full-scale release of the game will be launched\nalong with the GWAP portal (www.gwap.com). The ulti-\nmate goal of TagATune is a reusable system that can help\ngenerate millions of labels for sounds and music in pro-\nprietary and online databases.\n6 ACKNOWLEDGMENTS\nWe are especially grateful to Bram de Jong (FreeSound)\nfor encouragement and providing access to the data nec-\nessary for this project. Luis von Ahn is partially supported\nby a MacArthur Foundation Fellowship.\n7 REFERENCES\n[1] P. Cano and M. Koppenberger. Automatic sound anno-\ntation. IEEE Workshop on Machine Learning for Sig-\nnal Processing , pages 391–400, 2004.\n[2] B. Giordano. An annotated bibliography. soundob-\nject.org, 2001.\n[3] M. Mandel and D. Ellis. A web-based game for col-\nlecting music metadata. In 8th International Confer-\nence on Music Information Retrieval (ISMIR) , 2007.\n[4] G. Mori and J. Malik. Recognizing objects in ad-\nversarial clutter: Breaking a visual captcha. In Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , volume 1, pages 134–141, 2003.\n[5] G. Moy, N. Jones, C. Harkless, and R. Potter. Distor-\ntion estimation techniques in solving visual captchas.\nInConference on Computer Vision and Pattern Recog-\nnition (CVPR) , volume 2, pages 23–28, 2004.\n[6] NTIA. A nation online: How americans are expand-\ning their use of the internet. U.S. Department of Com-\nmerce. www.ntia.doc.gov, 2002.\n[7] D. Rocchesso, R. Bresin, and M. Fernstrom. Sounding\nobjects. IEEE Multimedia , 10(2):42–52, 2003.\n[8] D. Turnbull, R. Liu, L. Barrington, and G. Lanckriet.\nA game-based approach for collecting semantic anno-\ntations of music. In 8th International Conference on\nMusic Information Retrieval (ISMIR) , 2007.\n[9] L. von Ahn, M. Blum, and J. Langford. Telling hu-\nmans and computers apart automatically: How lazy\ncryptographers do ai. Communications of the ACM ,\n47:57–60, feb 2004.\n[10] L. von Ahn and L. Dabbish. Labeling images with a\ncomputer game. In ACM Conference on Human Fac-\ntors in Computing Systems (CHI) , pages 319–326,\n2004.\n[11] E. Wold, T. Blum, and D. Keislar. Content-based clas-\nsiﬁcation, search and retrieval of audio. IEEE Multi-\nmedia , 3:27–36, 1996."
    },
    {
        "title": "Audio Fingerprint Identification by Approximate String Matching.",
        "author": [
            "Jérôme Lebossé",
            "Luc Brun"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417927",
        "url": "https://doi.org/10.5281/zenodo.1417927",
        "ee": "https://zenodo.org/records/1417927/files/LebosseB07.pdf",
        "abstract": "An audio fingerprint is a small digest of an audio file which allows to identify it among a database of candidates. This paper first presents a fingerprint extraction algorithm. The identification task is performed by a new identification scheme which combines string matching algorithms and q-grams filtration. 1 ROBUST FINGERPRINT EXTRACTION METHOD Given an input signal sample eventually corrupted, its fingerprint allows to quickly retrieve the original file among a database of known audio files when it exists. Since an input audio sample should be identified, audio fingerprints should be composed of elementary keys (called subfingerprints) computed continuously along the signal. Fingerprints construction schemes are usually based on a sequence of overlapping windows on which sub-fingerprint values are computed [3]. This method called enframing insures a fixed detection rate of sub-fingerprints but is sensitive to cropping or shifting operations (Section 3). On the other hand, onset methods [1] are less sensitive to cuts alterations. However, these methods don’t insure any minimal detection rate of frames per second which may allow to bound the time required for the identification of an audio file. Our idea is to combine the respective advantages of enframing and onsets methods by selecting a small time interval (Iemax) with maximal energy within a larger one (Io) (Figure 1). Then, this signal segmentation process allows to quickly synchronize two fingerprints despite eventual shift operations. We then define the sub-fingerprint values as the lengths (in ms) between two consecutive Iemax. More details about our fingerprint extraction algorithm may be found in [4]. 2 FINGERPRINT RECOGNITION Several recognition techniques [3] may be used to compare an input fingerprint to a database. Within our framec⃝2007 Austrian Computer Society (OCG). Figure 1. Audio Segmentation Method work, string matching methods provide a natural framework to account for shifted or wrongly detected Iemax intervals between two fingerprints. Highly similar sequences between two strings are identified by computing an edit distance. Using a finite alphabet, the edit distance is often scored using a simple metric where a match is awarded a positive score while a mismatch is awarded a negative score. However within the fingerprint recognition framework, two co-derivative fingerprints should share long sequences of common symbols with few mismatches. We thus designed a scoring function which increases non linearly when a sequence of matches is encountered. This scoring function is defined as: S(i, j) =        αS(i −1, j −1) + β If si = sj 1 γ max   0, S(i, j −1), S(i −1, j)  −β otherwise (1) The constants α, β, γ are determined experimentally (Section 3) and satisfy 1 < γ < α in order to decrease the score when mismatches are encountered at a lower step than it increases during a sequence of matching symbols. A comparison of an input fingerprint with all the database fingerprints may be performed using string matching",
        "zenodo_id": 1417927,
        "dblp_key": "conf/ismir/LebosseB07",
        "keywords": [
            "audio fingerprint",
            "digest",
            "identification",
            "string matching algorithms",
            "q-grams filtration",
            "robust fingerprint extraction",
            "enframing",
            "onset methods",
            "signal segmentation",
            "edit distance"
        ],
        "content": "AUDIO FINGERPRINTIDENTIFICATION BYAPPROXIMATESTRING\nMATCHING\nJerome Lebosse\nFrance TelecomR&D\n42 rue des coutures\n14000 Caen,FRANCE\njerome.lebosse@orange-ftgroup.comLuc Brun\nGREYCUMR6072\n6 boulevarddu MarchalJuin\n14050 Caen,FRANCE\nluc.brun@greyc.ensicaen.fr\nABSTRACT\nAnaudioﬁngerprintisasmalldigestofanaudioﬁlewhich\nallows to identify it among a database of candidates. This\npaperﬁrstpresentsaﬁngerprintextractionalgorithm. The\nidentiﬁcation task is performed by a new identiﬁcation\nscheme which combines string matching algorithms and\nq-grams ﬁltration.\n1 ROBUST FINGERPRINT EXTRACTION\nMETHOD\nGivenaninputsignalsampleeventuallycorrupted,itsﬁn-\ngerprint allows to quickly retrieve the original ﬁle among\na database of known audio ﬁles when it exists. Since\nan input audio sample should be identiﬁed, audio ﬁnger-\nprintsshouldbecomposedofelementarykeys(called sub-\nﬁngerprints ) computed continuously along thesignal.\nFingerprintsconstructionschemesareusuallybasedon\nasequenceofoverlappingwindowsonwhichsub-ﬁngerprint\nvalues are computed [3]. This method called enframing\ninsuresaﬁxeddetectionrateofsub-ﬁngerprintsbutissen-\nsitive to cropping or shifting operations (Section 3). On\ntheotherhand,onsetmethods[1]arelesssensitivetocuts\nalterations. However,thesemethodsdon’tinsureanymin-\nimaldetectionrateofframespersecondwhichmayallow\nto bound the time required for the identiﬁcation of an au-\ndio ﬁle.\nOurideaistocombinetherespectiveadvantagesofen-\nframing and onsets methods by selecting a small time in-\nterval ( Iemax) with maximal energy within a larger one\n(Io)(Figure1). Then,thissignalsegmentationprocessal-\nlowstoquicklysynchronizetwoﬁngerprintsdespiteeven-\ntual shift operations. We then deﬁne the sub-ﬁngerprint\nvalues as the lengths (in ms) between two consecutive\nIemax. More details about our ﬁngerprint extraction al-\ngorithm may befound in[4].\n2 FINGERPRINT RECOGNITION\nSeveral recognition techniques [3] may be used to com-\npare an input ﬁngerprint to a database. Within our frame-\nc/circlecopyrt2007 AustrianComputer Society (OCG).\nFigure 1. Audio Segmentation Method\nwork, string matching methods provide a natural frame-\nworktoaccountforshiftedorwronglydetected Iemaxin-\ntervalsbetweentwoﬁngerprints. Highlysimilarsequences\nbetween two strings are identiﬁed by computing an edit\ndistance. Usingaﬁnitealphabet,theeditdistanceisoften\nscored using a simple metric where a match is awarded\na positive score while a mismatch is awarded a negative\nscore. However within the ﬁngerprint recognition frame-\nwork,twoco-derivative ﬁngerprintsshouldsharelongse-\nquences of common symbols with few mismatches. We\nthus designed a scoring function which increases non lin-\nearly when a sequence of matches is encountered. This\nscoringfunction is deﬁned as:\nS(i,j) =\n\nαS(i−1,j−1) +β Ifsi=sj\n1\nγmax\n0,\nS(i,j−1),\nS(i−1,j)\n−βotherwise\n(1)\nTheconstants α,β,γaredeterminedexperimentally(Sec-\ntion 3) and satisfy 1< γ < α in order to decrease the\nscore when mismatches are encountered at a lower step\nthan itincreases during a sequence of matching symbols.\nA comparison of an input ﬁngerprint with all the data-\nbaseﬁngerprintsmaybeperformedusingstringmatching\nmethods based on our scoring function (equation 1). This\ncomparison is usually speed up using a q-grams ﬁltering\napproach [2]. The basic idea of our ﬁltering method con-\nsists to weight each common q-gram between two strings\nbya scoredeﬁned byequation 1.\nMore formally, let us denote by QD,Ia common q-\ngram between the input ﬁngerprint Iand a database’s ﬁn-gerprint D. Given a maximal string’s length mthe score\nofQD,Iisdeﬁned as:\nscore(QD,I) =p\ni=1q\nj=1S(I[Ii, Ii+m], D[Dj, Dj+m])(2)\nwhere S(I[Ii,Ii+m],D[Dj,Dj+m])denotesourscor-\ningfunctioncomputedonthetwosub-stringsoflength m\nstarting at indexes IiandDj. Given equation 2, let us de-\nnote by QD,I⊂Dthe set of common q-grams between\nDandI. The score of a database’s ﬁngerprint is thus de-\nﬁnedasthesumofscoresofitscommon q-gramswiththe\ninput:\nscore(I, D) =\nQD,I⊂Dscore(QD,I) (3)\nExperiments (Section 3) show that the co-derivative ﬁn-\ngerprint, when it exists within the database, is always the\none getting the higher score. We can thus retrieve a co-\nderivative ﬁngerprint when it exists within the database.\nHowever, an identiﬁcation method should also be able to\ndecide if an input ﬁngerprint has no co-derivative within\nthe database.\nSince our ﬁltering process always ranks ﬁrst the co-\nderivative ﬁngerprint when it exists, our decision rule has\nonly to conﬁrm the existence within the database of the\nﬁngerprint with higest score. The fact that our ﬁltering\nprocess does not provide a valid decision rule is mainly\ndue to the small value of mconsidered for each common\nq-gram in equation 3. We thus designed a decision cri-\nterion based on the score between the input ﬁngerprint\nand the database ﬁngerprint ranked ﬁrst by our ﬁltering\nprocessprocess. Bothﬁngerprintsarecomparedonlarger\nsub-strings of length M > m and at the two positions\n(imax,jmax)suchthat I[imax,imax+q] =D[jmax,jmax+\nq])andS(I[imax,imax+m] =D[jmax,jmax+m]))is\nmaximal amount all the scores computed between Iand\nDusing equation 2. Our input ﬁngerprint’s score is thus\ndeﬁned as:\nscore(I) =score(I[imax, imax+M], D[jmax, jmax+M])(4)\n3 RECOGNITION EXPERIMENTS\nOurdatabasecontainstheﬁngerprintsof350songsofap-\nproximately 4 minutes each. In our experiments, a sam-\nple of 5 seconds of each database’s audio ﬁle has been\nextracted randomly and compressed at 128kbps. The ﬁn-\ngerprintsofthesesampleshavethenbeencomparedtothe\nones of thedatabase.\nTheminimumsizeofthe q-gramsforourﬁltrationpro-\ncedure has been ﬁxed to 5in these experiments. The val-\nues of α,γandβ(equation 1) have been respectively set\nto:α= 1.5;γ= 1.1andβ= 20.\nLet us ﬁrst consider the results obtained using equa-\ntion3. TheﬁrsttheecolumnsofTable2untitled Filtering\nscores shows min, max and mean scores according to\nequation 3 of the three database ﬁngerprints with highest\nrank. In all our experiments, the database ﬁngerprint cor-\nresponding to the input has always been ranked ﬁrst. TheFiltering scores Final Scores\nScores 1st 2nd 3rd 1st 2nd3rd\nMin 14878 0 0100000 00\nMean 11.105240.72 143.74 3.1015163\nMax 4.10620.10310.1035.10172800590\nFigure 2. Filteringand Final scores\nscore of the second ﬁngerprint corresponds to the score\nthat would be obtained if the ﬁngerprint of the input au-\ndio ﬁle was removed from the database. As shown by\nthe cells (Min,1st) and (Max, 2nd), one input ﬁngerprint\nnot present within the database may obtain a higher score\nthan another input ﬁngerprint whose co-derivative belong\ntothedatabase. Thisnegativepropertyofequation3does\nnot allow to decide if a ﬁngerprint is present within the\ndatabase.\nThelastthreecolumnsofTable2showthescorescom-\nputed according to equation 4 of the three database’s ﬁn-\ngerprint selected in the previous experiment. As shown\nby the cells (Min,1st) and (Max, 2nd) of the last three\ncolumns, the highest score (according to equation 4) of\naﬁngerprintnotpresentwithinthedatabaseismuchlower\nthanthelowestscoreobtainedbyaninputﬁngerprintwhose\nco-derivative is present within the database. In these ex-\nperiments,anythresholdbetweenthesetwovaluesallows\ntodecide ifaﬁngerprint ispresent withinthedatabase.\n4 CONCLUSIONS\nWehavepresentedinthispaperanaudioﬁngerprintiden-\ntiﬁcation method. A ﬁrst ﬁltering based on qgrams and\nstring matching algorithms allows to retrieve the closest\ndatabase ﬁngerprint from the input. A ﬁnal score based\nonanewscoringfunctionisattachedtotheﬁngerprintre-\nturned by the database ﬁltering. This ﬁnal score allows\nto check if an input ﬁngerprint has a co-derivative within\nthe database. In future work we plan to improve the in-\ndexation scheme of our database according to the use of\nq-gramsperformed within our ﬁlteringstep.\n5 REFERENCES\n[1] S. Hainsworth and M. Macleod, “Onset de-\ntection in musical audio signals,” in Proc. of\nICMC, 2003.\n[2] P. Jokinen and E. Ukkonen, “Two algo-\nrithms for approximate string matching in sta-\ntic texts,” in Proc. of MFCS , pp. 240-248,\n1991.\n[3] J.HaitsmaandT.Kalker, “Ahighlyrobustau-\ndio ﬁngerprinting system,” in Proc. of ISMIR ,\npp. 144-148, 2002.\n[4] J.Leboss ´eandL.BrunandJ.C.Pailles, “Aro-\nbustaudioﬁngerprintextractionalgorithm,” in\nProc.ofSPPRA ,Innsbruck(Austria),February\n2007."
    },
    {
        "title": "Preliminary Analyses of Information Features Provided by Users for Identifying Music.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie",
            "M. Cameron Jones"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415860",
        "url": "https://doi.org/10.5281/zenodo.1415860",
        "ee": "https://zenodo.org/records/1415860/files/LeeDJ07.pdf",
        "abstract": "This paper presents preliminary findings based on the analyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information features are compared with the results from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the “aboutness” of the underlying musical work. The presence of erroneous information in the queries is also discussed.",
        "zenodo_id": 1415860,
        "dblp_key": "conf/ismir/LeeDJ07",
        "keywords": [
            "user-provided information features",
            "Google Answers",
            "music queries",
            "information features",
            "comparing with previous studies",
            "new feature types",
            "understanding of information",
            "erroneous information",
            "preliminary findings",
            "identification of music works"
        ],
        "content": "PRELIMINARY ANALYSES OF INFORMATION FEATURES \nPROVIDED BY USERS FOR IDENTIFYING MUSIC\nJin Ha Lee J. Stephen Downie M. Cameron Jones \nUniversity of Illinois at Urbana-Champaign \nGraduate School of Library and Information Science \nABSTRACT \nThis paper presents preliminary findings based on the \nanalyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information featur es are compared with the \nresults from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the \n“aboutness” of the underlying musical work. The presence of erroneous information in the queries is also discussed. \n1. INTRODUCTION \nThe lack of empirical data regarding the various aspects \nof real-life music queries is an obstacle to truly understanding users’ music search behaviors in real-life situations. This lack of understanding has negative implications for designing and evaluating music information retrieval (MIR) systems. This is especially so because the common assump tions of MIR researchers \nregarding the nature of musi c queries have been found to \nbe remarkably different from th e real-world situation [5]. \nIn the past few years, a limited but increasing number of user studies have been conducted in an attempt to address this issue. In particular, previous studies of music information queries (e.g.,[1],[3],[8]) have identified a variety of different types of information features provided by users in queries presented to search intermediaries, and also provided some quantitative data (i.e., frequency counts of need types and feature types in the analyzed queries).  \nNotwithstanding these studies, there is still a lack of \nformal understanding regarding the nature and use of the information features present in queries. More specifically, a comprehensive understanding of the associations between the types of needs expressed by users and the information features they provide to help satisfy their needs does not yet exist. In the previous studies, the proportions of queries containing various features were reported at a very broad level, over all different types of queries expressing different needs. The usage statistics for each feature in each type of music query may be significantly different from the \noverall usage statistics as suggested in studies of other non-textual information searching behaviors [4], but we do not have enough empirical data about this at the present.  \nThis paper presents an overview of the method and \nselect results of a large-scale and ongoing series of analyses designed to provide the MIR community with a stronger empirical and theoretical foundation for the comprehension of real-world MIR queries. In this paper, we will limit our scope by reporting on the empirical data and interesting findings with regard to one specific type of MIR query we have labeled the “Identification” type (defined in section 2.1). \n2. DATA COLLECTION AND ANALYSIS \n2.1. Query Data Collection \nTague-Sutcliffe’s [11] definitions of query and search \nstatement  are adopted in this paper. Query  is defined as \n“the verbalized statement of a user’s need” and the search statement  is defined as “a single string, \nexpressed in the language of the system, which triggers a search of the database”. Music query is thus defined as the natural language statements in which users express their needs for music objects or information about those objects (i.e., metadata).   \nQueries are an essential component of IR processes \nand also an excellent sour ce for collecting information \nto identify the kinds of info rmation features that are \nrelevant for various user tasks [3]. The particular website selected as the source of real-life query data is the Google Answers website\n1, an online reference \nservice provided by G oogle. Upon receiving the \napproval from the University of Illinois’ Institutional Review Board (IRB), all the queries posted under the music category (2208) were collected in April 2005.  \nThere are a variety of music information needs \nexpressed in the queries we analyzed in the Google Answers data set. These in clude needs for identifying \nand/or locating recordings, obtaining lyrics, background information, or recommendations, etc. Of these, the most common type of queries was the ones expressing need for “identification”. An “identification query” is one in which users seek help in identifying a particular  \npiece of music (typically some thing that they have heard \nbefore), or the name(s) of particular  artist(s) that they \nhave encountered with regard to some particular \n                                                          \n \n1 http://answers.google.com/answers/ © 2007 Austrian Computer Society (OCG).   \n \n   musical work (sometimes referred to as a “known-item \nsearch” in Library Science [9]) . In this study we refer to \nthis class of query as “Identification” queries. \nAmong our data set of 2208 queries, 575 queries \nwere for identifying music and 110 were for identifying artist(s). After filtering out the queries posted by the same inquirer multiple times, 566 unique identification queries were identified and analyzed. \n2.2. Data Encoding \nCont\nent analysis is the principal method used for \nanalyzing our query data. Content analysis is defined as a “systematic, replicable technique for investigating the manifest and the latent content of any artifact of communication to understand themes and orientations” [7]. This method enables researchers to systematically collect and organize unstructured information into a standardized format that allo ws one to make inferences \nabout the characteristics of recorded material [7]. Query data were manually encoded. After identifying the needs expressed in each query, all the instances of various information features present in the query text were marked up with tags that identify the type of feature. Such tagging allows for the easy extraction and examination of all instances  of the features. Figure 1 \nshows an example of a marked-up query record.  \nFigure 1.  Example of a marked-up query record  \n \nFor this encoding process, a standardized set of \ncategories of features was needed as the basis for developing the tag set. The first step was to converge all the categories of needs and features from the previous \nMIR user studies (i.e.,[1],[3],[8]), and establish an initial set of features (categories) as the pre-coding scheme. The main reason for starting with the categories from previous studies rather than developing them from scratch was to maintain some comparability of the features with the ones used in the previous studies. These features were regarded as tentative and subject to revision based on the further analysis of queries. By an \niterative coding process, the features are, and continue \nto be,  refined to a sufficient level so that they are \nmutually exclusive, unambiguous, and comprehensive when taken together for expressing the information provided in the music information queries. 3. DISCUSSION \nTable 1 compares the informa tion features identified in \nthis study with the ones from the previous study of Google Answers music queries by Bainbridge et al. [1]. The primary difference between this study and [1] is that this study focused only on the information features used in Identification type queries, where Bainbridge et al. collected features from mu sic queries of all types. \nTable 1.  Comparison of the information features \nidentified in Bainbridge et al. [1] and this study  \nFeatures used in queries % of queries \ncontaining the \nfeature \n[1]  \nN=50\n2 This \nstudy \nN=566\nLYRIC  28.9 60.6\nDATE 31.9 59.2\nGENRE 32.7 35.5\nARTIST NAME *55.0 19.3\nORCHESTRATION 13.5 **16.8\nABOUT [LYRIC STORY]*** 2.6 15.4\nWHERE HEARD/PLACE HEARD 24.1 14.7\nAFFECT/MOOD  2.4 14.0\nWORK TITLE 35.6 13.6\nAUDIO/VIDEO EXAMPLE 4.4 10.8\nSIMILAR (work/artist) 4.6 9.2\nTEMPO  2.4 7.6\nNATIONALITY (of music/artist) 12.5 4.2\nLANGUAGE 2.0 3.7\nCOLLECTION TITLE 12.2 2.7\nLABEL 5.4 0.1\nLINK (to bibliographic info) 2.9 -\n* PEFORMER (47.8%) + COMPOSER (7.2%) ** MUSICAL INSTRUMENT (12.8%) + VOCAL (4.0%) *** LYRIC STORY  was used in Bainbridge et al. [1]  \nFrom this table, we can observe a notable increase in \nthe use of LYRIC , DATE , A/V EXAMPLE  and \nAFFECT/MOOD  information in Identification queries. \nA similar decrease in ARTIST NAME , WORK TITLE , \nNATIONALITY , and COLLECTION TITLE  is also \npresent. Note that ARTIST NAME  and WORK TITLE  are \nNOT zero even though these are Identification queries. Many Identification queries contain just one of the two features, or contain guesses (many of which are wrong) about what the title or artist might be (see section 3.4). \n3.1. Prevalence of Date/Time Information \nInformation features related to the time dimension (i.e., \ndate information) were prevalent throughout the analyzed queries. The specifi city of this information \nranges from vague (e.g., “old”, “recent”) through a specific decade, date range, to a particular year, and sometimes even to a specific date and time. 199 out of 566 queries (35.2%) mentioned the date when the   \n \n   sought music was released and 156 queries (27.6%) \nmentioned the date when the user heard the sought music. The association between date and genre information was also observed. In 8 queries, date was also used to represent a par ticular music style (e.g., has \na “70’s feel to it”). All together, 335 out of 566 (59%) of all analyzed queries contained some kind of information on the time dimension.  \nSo why is the date information so frequently used in \nidentifying a particular mu sical work? One argument \ncan be made regarding the experience  of music. \nExperiencing music does seem to capture that particular moment in the users’ life and later bring back the memories and feelings “at the time” to the users [2]. Most of these users who want to identify certain music have experienced the sought music in some point of time and space, and the time information seems to work as a major retrieving clue for recalling that memory.  \n3.2. Other Notable Features \n3.2.1.  SIMILAR Feature  \nThe SIMILAR  feature was used to mark up references to \nknown artists or works used to describe attributes of the sought music. Music similarity has been typically associated with playlist generation or music recommendation, but our analysis shows that it is also used for seeking a particular music object. In the analyzed queries, 46 queri es (8.1%) contained \nreferences to similar artist(s) whereas only 7 queries (1.2%) referenced similar musical work(s).\n  \n3.2.2.  AFFECT/MOOD Feature \nThe AFFECT/MOOD  feature was used in 79 queries for \nidentifying music (14.0%), a considerably larger \nproportion of queries than 2.4% reported over all types of queries in [1]. Some examples of the moods that our study has found include: “lovely” (3), “mellow” (3), “fun(ny)” (3), “funky” (3), “dreamy” (2), “hypnotic” (2), “happy” (2), “sad” (2), “quirky” (2). This finding suggests that mood information is an important “hook” in the minds of the searchers when they cannot remember the names of the artist or title of work.  \n3.2.3.  ABOUT Feature  and LYRIC STORY Feature  \nIn the initial coding process for this study, the ABOUT  \nfeature was used to mark up general descriptions where the user tried to explain wh at the “subject” of the music \nwas “about”. Most of these descriptions turned out to be “lyric stories” which was one of the features included in [1]. However, some users were describing what the related music video was “about” or what the album cover looked like. Others were describing an overall \n“theme” of the music rather than the “lyric story”, for \nexample, “Christmas”, “Ha lloween”, “love”, “war”, \n“jealousy”, and so on.  3.3. New Information Features \nTable 2 presents the informa tion features that were not \nlisted in the previous study by Bainbridge et al. [1]. Among these features, the most common one was the MEDIA  feature (44.0%) used for part of text where the \nuser describes the media from which he/she heard the sought music. Various expressions of certainty or uncertainty as to the accur acy of the user-provided \ninformation were used in conjunction with other features (30.7%). Other common features include: the LYRIC DESCRIPTION  feature (30.0%) referring to part \nof text where the user provides additional explanation of the lyric fragments he/she is providing such as the type (e.g., refrain, chorus), part (e .g., at the beginning, end of \nthe song), and frequency of occurrence (e.g., repeated X \ntimes), and ASSOCIATED USE  (30.0%) for the part \ndescribing the use of music in other work(s) including movies, commercials, video games, and so on.   \nTable 2.  New information features identified in queries \nfor identifying music  \nFeatures used in queries % of \nqueries \ncontaining \nthe feature\nMEDIA (e.g., radio, TV)  44.0 \nEXPRESSION OF (UN)CERTAINTY  30.7 \nLYRIC DESCRIPTION  30.0 \nASSOCIATED USE (e.g ., movie, ad)  30.0 \nGENDER OF ARTIST 20.5 \nMUSICAL STYLE DESCRIPTION  19.8 \nRELATED WORK TITLE 15.9 \nPRIOR SEARCH INFORMATION 13.4 \nSCENE (where music was used) 13.3 \nRELATED EVENT 4.2 \nREGION  2.6 \nVERSION INFORMATION 1.9 \nMELODY DESCRIPTION 0.7 \n3.4. Providing Incorrect Information \nIn music queries, consistently misheard lyrics, \nmisinterpretations of genre, misunderstood artist names, and inexact released dates are rather common. However, it is interesting that, notwithstanding these errors, users, at least in some cases, are still able to obtain correct identifications of their sought-after music [10]. In our Google Answers data set, we can point to a number of examples where the user was still able to find the correct music although the user’s information was incorrect like the ones that follow: \nQuery (#501789): \nI only caught some of the lyrics. <other> \nI only heard it once </other> . \n<uncertainty> I am not sure that the \nwording is absolutely correct \n</uncertainty> . IT was a <gender> female \n</gender>  singer, very <style>plain \n</style> , <affect> beautiful </affect> ,   \n \n   <tempo>slow</tempo> .  <lyric>\"there will \nbe no black flag on my door\" </lyric>   \n<lyric>\"I am in love\" </lyric>  <lyric>\"I \nwill go down with vengence\" </lyric>  \nAnswer: \n...I'm pretty certain that the song you \nheard was Dido's \"White Flag\". A snippet of the lyrics are: \n\"I will go down with this ship And I \nwon't put my hands up and surrender There will be no white flag above my door I'm in love and always will be\" \n...Search Strategy (on Google):  \"be no black flag\" \ndido \"white flag\" lyrics... \nQuery (#342762): \nLooking for a song. They [sic] lyrics go \n<lyric>”My Mamma done told me, When I \nwas in knee-socks” </lyric>  It’s a \n<genre>jazzy</genre>  number. \nAnswer: \n…Well, you were close. The lyrics refer \nto “knee pants” not “knee socks” and the genre is blues rather than jazz…. \n \nFor human intermediaries, the incorrect information \npresent seems to be easily overcome. However, within an automated system, such errors can be catastrophic. This is one reason why we are noting expressions of UNCERTAINTY  via our <uncertainty> tag to see if \nconsistent clues can be derived to signal to an automated system that they should deal with the contained information in some special way (perhaps using some type of re-weighting approach) to improve search robustness.  \n4. CONCLUSION AND FUTURE RESEARCH \nOur preliminary results show that the Identification type \nqueries display different feature distributions than those found in previous, more general studies. For future research, we will continue to explore the differences in search behaviors for the queries expressing other needs. We will also compare these features to the ones used in queries for other non-textual cultural objects as certain similarities have come to our attention, especially with regard to image searching. A number of interpretive attributes including ab stract concept (e.g., ABOUT ), \nexternal relation (e.g., SIMILARITY ) and reactive \nattribute (e.g., UNCERTAINTY ) observed here were also \nfound in Jörgensen’s study [6] of image descriptions.  \n5. ACKNOWLEDGMENTS \nThis work supported by The Mellon Foundation, and \nthe National Science Foundation (NSF IIS-0327371). 6. REFERENCES \n[1] Bainbridge, D., Cunningham, S. J., & Downie, \nJ. S. “How people describe their music information needs: A grounded theory analysis of music queries”, Proceedings of the 4th \nInternational Conference on Music Information Retrieval (ISMIR),  Baltimore, \nUSA, 2003. \n[2] Cunningham, S. J., Bainbridge, D., & Falconer, \nA. “More of an art than a science: Supporting the creation of playlists and mixes”, Proceedings of the 7th International Conference on Music Information Retrieval,  \nBarcelona, Spain, 2004. \n[3] Downie, J. S., & Cunningham, S. J. “Toward a \ntheory of music information retrieval queries: System design implications”, Proceedings of \nthe 3rd International Conference on Music Information Retrieval, Paris, France, 2002. \n[4] Fidel, R. “The image retrieval task: \nImplications for the design and evaluation of image databases”, The New Review of  \nHypermedia and Multimedia , 3, 181-199, 1997. \n[5] Futrelle, J. & Downie, J. S. “Interdisciplinary \ncommunities and research issues in music information retrieval”, Proceedings of the 3rd \nInternational Conference on Music Information Retrieval, Paris, France, 2002. \n[6] Jörgensen, C. “Attributes of images in \ndescribing tasks”, Information Processing and \nManagement, 34 (2/3), 161-174, 1998. \n[7] Krippendorff, K. Content analysis: An \nintroduction to its methodology  (2nd ed.). \nThousand Oaks, CA: Sage, 2004. \n[8] Lee, J. H., Downie, J. S., & Cunningham, S. J. \n“Challenges in cro ss-cultural/multilingual \nmusic information seeking”, Proceedings of \nthe 6th International Conference on Music Information Retrieval,  London, UK, 2005. \n[9] Lee, J. H., Renear, A., & Smith, L. C. \n“Known-item searching: Variations on a concept”, Proceedings of the 69th ASIS&T \nAnnual Meeting . Austin, USA, 2006. \n[10] Lee, J. H., & Renear, A. “How incorrect \ninformation delivers corr ect search results: A \npragmatic analysis of queries”, Proceedings of \nthe 70th ASIS&T Annual Meeting.  Milwaukee, \nUSA, 2007.  \n[11] Tague-Sutcliffe, J. “The pragmatics of \ninformation retrieval experimentation, revisited”,  Information Processing and \nManagement, 28 (4), 467-490, 1992."
    },
    {
        "title": "A Unified System for Chord Transcription and Key Extraction Using Hidden Markov Models.",
        "author": [
            "Kyogu Lee",
            "Malcolm Slaney"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415208",
        "url": "https://doi.org/10.5281/zenodo.1415208",
        "ee": "https://zenodo.org/records/1415208/files/LeeS07.pdf",
        "abstract": "A new approach to acoustic chord transcription and key extraction is presented. As in an isolated word recognizer in automatic speech recognition systems, we treat a musical key as a word and build a separate hidden Markov model for each key in 24 major/minor keys. In order to acquire a large set of labeled training data for supervised training, we first perform harmonic analysis on symbolic data to extract the key information and the chord labels with precise segment boundaries. In parallel, we synthesize audio from the same symbolic data whose harmonic progression are in perfect alignment with the automatically generated annotations. We then estimate the model parameters directly from the labeled training data, and build 24 key-specific HMMs. The experimental results show that the proposed model not only successfully estimates the key, but also yields higher chord recognition accuracy than a universal, key-independent model. 1 INTRODUCTION A musical key and a chord are among important attributes of Western tonal music. A key defines a referential point or a tonal center upon which other musical phenomena such as melody, harmony, and cadence are arranged. Particularly, a key and succession of chords over time, or chord progression based on the key forms the core of harmony in a piece of music. Hence analyzing the overall harmonic structure of a musical piece often starts with labeling every chord at every beat or measure based on the key. Finding the key and labeling the chords automatically from audio are of great use for those who want to do harmonic analysis of music. Once the harmonic content of a piece is known, a sequence of chords can be used for further higher-level structural analysis where themes, phrases or forms can be defined. Chord sequences and the timing of chord boundaries are also a compact and robust mid-level representation of musical signals, and have many potential applications such as music identification, music segmentation, music c⃝2007 Austrian Computer Society (OCG). similarity finding, audio summarization, and mood classification. Chord sequences have been successfully used as a front end to the audio cover song identification system [1]. For these reasons and others, automatic chord recognition has recently attracted a number of researchers in the Music Information Retrieval field. Some systems use a simple pattern matching algorithm [2, 3, 4] while others use more sophisticated machine learning techniques such as hidden Markov models or Support Vector Machines [5, 6, 7, 8, 9]. Hidden Markov models (HMMs) are very successful for speech recognition, whose high performance is largely attributed to gigantic databases with labels accumulated over decades. Such a huge database not only helps estimate the model parameters appropriately, but also enables researchers to build richer models, resulting in better performance. However, there are very few such database available for music. Furthermore, the acoustical variance in music is far greater than that in speech in terms of its frequency range, timbre due to different instrumentations, dynamics, and/or duration, and thus even more data is needed to build generalized models. It is very difficult to obtain a large set of training data for music, however. First of all, it is very hard for researchers to acquire a large collection of music. Secondly, hand-labeling the chord boundaries in a number of recordings is not only an extremely time consuming and tedious task but also is subject to errors made by humans. In order to automate the extremely laborious task of obtaining labeled training data for supervised learning algorithm, we use symbolic data such as MIDI files to generate chord names and precise time boundaries, as well as to create audio. Audio and chord-boundary information generated from the same symbolic files are in perfect alignment, and we can use them to directly estimate the model parameters. In doing so, we build 24 key-specific models, one for each for 24 major/minor keys. In this paper, we present a new approach to key estimation and chord recognition at the same time by borrowing the idea from isolated word recognizers. In an isolated word recognizer using HMMs, as described in [10], V number of word models are first built, and when an unknown word is given, the observation sequence is obtained via a proper feature analysis. Then model likelihoods are computed for all V models, and word recognition is done by selecting a model whose likelihood is highest. Likewise, once we trained 24 key-specific HMMs, key estimation is accomplished by selecting the key model with the highest likelihood given the observation sequence of input audio; i.e., key = argmax k Pr(O, Q|λk), (1) where key is an estimated key, O is an observation sequence, Q is a state path, and λk is a key model for key k. Once the key model is selected from Equation 1, we can obtain the chord sequence by taking the optimal state path QOP T = Q1Q2 · · · QT returned by the Viterbi decoder. The overall system for key estimation and chord recognition is shown in Figure 1. Input: = optimal state path = chord sequence Pr(O, Q|λ1) key = argmaxk Pr(O, Q|λk) Pr(O, Q|λK) Pr(O, Q|λ2) λ1 λ2 λK O = O1O2 · · · OT Q = Q1Q2 · · · QT ... Figure 1. System for key estimation and chord recognition. There are several advantages to this approach. First, a great number of symbolic files are freely available. Second, we do not need to manually annotate key names or chord boundaries with chord names to obtain training data. Third, sufficient training data enables us to build a model for each key, which not only results in increased performance for chord recognition but also provides key information. This paper is organized as follows. A review of related work is presented in Section 2; in Section 3, we explain the method of obtaining the labeled training data, and describe the procedure of building our models and the feature set we used to represent the state observation in the models; in Section 4, we present empirical results with discussions, and draw conclusions followed by directions for future work in Section 5. 2 RELATED WORK Several systems have been previously described for chord recognition from the raw audio waveform. Sheh and Ellis proposed a statistical learning method for chord segmentation and recognition using the chroma features [5]. They used the hidden Markov models (HMMs) trained by the Expectation-Maximization(EM) algorithm, and treated the chord labels as hidden values within the EM framework. In training the models, they used only the chord sequence as an input to the models, and applied the forward-backward algorithm to estimate the model parameters. The frame accuracy they obtained was about 76% for segmentation and about 22% for recognition, respectively. The poor performance for recognition may be due to insufficient training data compared with a large set of classes (147 chord types trained on 20 songs). It is also possible that the flat-start initialization of training data yields incorrect chord boundaries resulting in poor parameter estimates. Bello and Pickens also used the chroma features and HMMs with the EM algorithm to find the crude transition probability matrix for each input [6]. What was novel in their approach was that they incorporated musical knowledge into the models by defining a state transition matrix based on the key distance in a circle of fifths, and avoided random initialization of a mean vector and a covariance matrix of observation distribution. In addition, in training the model’s parameter, they selectively updated the parameters of interest on the assumption that a chord template or distribution is almost universal regardless of the type of music, thus disallowing adjustment of distribution parameters. The accuracy thus obtained was about 75% using beat-synchronous segmentation with a smaller set of chord types (24 major/minor triads only). In particular, they argued that the accuracy increased by as much as 32% when the adjustment of the observation distribution parameters is prohibited. The present paper expands our previous work on chord recognition [8, 9], where we used symbolic data to obtain a large amount of labeled training data from which model parameters can be directly estimated without using an EM algorithm where model initialization is critical. In this paper, however, we first propose a unified model to accomplish simultaneously two closely related musical tasks – key estimation and chord transcription – by building key-specific HMMs. Furthermore, we use a feature vector called Tonal Centroid instead of chroma vector, which has been the feature set of choice in chord recognition systems. To authors’ knowledge, this work is the first to use a tonal centroid vector for key extraction as well as for chord transcription. 3 SYSTEM Our chord transcription system starts off by performing harmonic analysis on symbolic data to obtain label files with chord names and precise time boundaries. In parallel, we synthesize the audio files with the same symbolic files using a sample-based synthesizer. We then extract appropriate feature vectors from audio which are in perfect sync with the labels, and use them to train our models.",
        "zenodo_id": 1415208,
        "dblp_key": "conf/ismir/LeeS07",
        "keywords": [
            "musical key",
            "chord",
            "harmonic analysis",
            "symbolic data",
            "audio synthesis",
            "Tonal Centroid",
            "key-specific HMMs",
            "chord transcription",
            "feature vector",
            "system"
        ],
        "content": "AUNIFIEDSYSTEM FORCHORD TRANSCRIPTION AND KEY\nEXTRACTIONUSING HIDDENMARKOV MODELS\nKyoguLee\nCenter forComputerResearch inMusicand Acoustics\nStanford University\nkglee@ccrma.stanford.eduMalcolmSlaney\nYahoo! Research\nSunnyvale,CA94089\nmalcolm@ieee.org\nABSTRACT\nA new approach to acoustic chord transcription and key\nextractionispresented. Asin an isolatedwordrecognizer\nin automatic speech recognition systems, we treat a mu-\nsical key as a word and build a separate hidden Markov\nmodel for each key in 24 major/minor keys. In order to\nacquire a large set of labeled training data for supervised\ntraining, we ﬁrst perform harmonic analysis on symbolic\ndata to extract the key information and the chord labels\nwith precise segment boundaries. In parallel, we synthe-\nsize audio from the same symbolic data whose harmonic\nprogression are in perfect alignment with the automati-\ncally generated annotations. We then estimate the model\nparameters directly from the labeled training data, and\nbuild 24 key-speciﬁc HMMs. The experimental results\nshow that the proposed model not only successfully es-\ntimates the key, but also yields higher chord recognition\naccuracythana universal,key-independentmodel.\n1 INTRODUCTION\nAmusicalkeyandachordareamongimportantattributes\nof Western tonal music. A key deﬁnes a referential point\nor a tonal center upon which other musical phenomena\nsuch as melody,harmony,and cadenceare arranged. Par-\nticularly, a key and succession of chords over time, or\nchordprogressionbasedonthekeyformsthecoreofhar-\nmony in a piece of music. Hence analyzing the overall\nharmonicstructureofa musicalpieceoftenstartswith la-\nbeling every chord at every beat or measure based on the\nkey.\nFinding the key and labeling the chords automatically\nfromaudio are of great use for those who want to do har-\nmonic analysis of music. Once the harmoniccontent of a\npiece is known,a sequenceof chordscan be used forfur-\ntherhigher-levelstructuralanalysiswherethemes,phras es\norformscanbedeﬁned.\nChord sequences and the timing of chord boundaries\nare also a compact and robust mid-level representation\nof musical signals, and have many potential applications\nsuch as music identiﬁcation, music segmentation, music\nc/circlecopyrt2007AustrianComputerSociety(OCG).similarityﬁnding,audiosummarization,andmoodclassi-\nﬁcation. Chordsequenceshave beensuccessfully used as\na front end to the audio cover song identiﬁcation system\n[1]. For these reasons and others, automatic chord recog-\nnition has recently attracted a number of researchers in\nthe Music InformationRetrieval ﬁeld. Some systems use\na simplepatternmatchingalgorithm[2, 3,4]whileothers\nuse more sophisticated machine learning techniquessuch\nas hidden Markov models or Support Vector Machines\n[5, 6, 7, 8, 9].\nHidden Markov models (HMMs) are very successful\nforspeechrecognition,whosehighperformanceislargely\nattributed to gigantic databases with labels accumulated\nover decades. Such a huge database not only helps es-\ntimate the model parameters appropriately, but also en-\nablesresearcherstobuildrichermodels,resultinginbett er\nperformance. However, there are very few such database\navailable for music. Furthermore, the acoustical variance\nin music is far greater than that in speech in terms of its\nfrequencyrange,timbreduetodifferentinstrumentations ,\ndynamics, and/or duration, and thus even more data is\nneededtobuildgeneralizedmodels.\nIt is very difﬁcult to obtain a large set of training data\nfor music, however. First of all, it is very hard for re-\nsearcherstoacquirealargecollectionofmusic. Secondly,\nhand-labelingthechordboundariesinanumberofrecord-\ningsisnotonlyanextremelytimeconsumingandtedious\ntask butalsois subjecttoerrorsmadebyhumans.\nIn order to automate the extremely laborious task of\nobtaininglabeledtrainingdata for supervisedlearningal -\ngorithm,we use symbolicdata such as MIDIﬁles to gen-\nerate chord names and precise time boundaries, as well\nas to create audio. Audio and chord-boundary informa-\ntiongeneratedfromthe samesymbolicﬁlesare inperfect\nalignment, and we can use them to directly estimate the\nmodel parameters. In doing so, we build 24 key-speciﬁc\nmodels,oneforeachfor24major/minorkeys.\nIn this paper, we present a new approach to key esti-\nmationandchordrecognitionatthesametimebyborrow-\ning the idea from isolated word recognizers. In an iso-\nlated word recognizerusing HMMs, as describedin [10],\nVnumberofwordmodelsareﬁrstbuilt,andwhenanun-\nknownwordisgiven,theobservationsequenceisobtained\nvia a proper feature analysis. Then model likelihoodsare\ncomputedfor all Vmodels,andword recognitionis doneby selecting a model whose likelihood is highest. Like-\nwise,oncewetrained24key-speciﬁcHMMs,keyestima-\ntion is accomplished by selecting the key model with the\nhighestlikelihoodgiventheobservationsequenceofinput\naudio;i.e.,\nkey= argmax\nkPr(O, Q|λk), (1)\nwhere keyis an estimatedkey, Ois an observationse-\nquence, Qis a state path, and λkis a key model for key\nk.\nOnce the key model is selected from Equation 1, we\ncan obtain the chordsequenceby taking the optimal state\npathQOPT=Q1Q2· · ·QTreturned by the Viterbi de-\ncoder. The overall system for key estimation and chord\nrecognitionisshownin Figure1.\nInput:\n= optimal state path\n= chord sequencePr(O, Q |λ1)\nkey= argmaxkPr(O, Q |λk)\nPr(O, Q |λK)Pr(O, Q |λ2)λ1\nλ2\nλKO=O1O2· · ·OTQ=Q1Q2· · ·QT\n...\nFigure 1. System for key estimation and chord recogni-\ntion.\nThere are several advantages to this approach. First, a\ngreat number of symbolic ﬁles are freely available. Sec-\nond, we do not need to manually annotate key names or\nchordboundarieswithchordnamestoobtaintrainingdata.\nThird, sufﬁcient training data enables us to build a model\nfor each key, which not only results in increased perfor-\nmance for chord recognition but also provides key infor-\nmation.\nThispaperisorganizedasfollows. Areviewofrelated\nwork is presented in Section 2; in Section 3, we explain\nthe methodof obtainingthe labeled trainingdata, andde-\nscribe the procedure of building our models and the fea-\nture set we used to represent the state observation in the\nmodels; in Section 4, we present empirical results with\ndiscussions, and draw conclusionsfollowed by directions\nforfutureworkinSection5.\n2 RELATED WORK\nSeveralsystemshavebeenpreviouslydescribedforchord\nrecognitionfromthe rawaudiowaveform. ShehandEllis\nproposedastatisticallearningmethodforchordsegmenta-\ntion and recognition using the chroma features [5]. They\nused the hidden Markov models (HMMs) trained by the\nExpectation-Maximization(EM)algorithm,andtreatedthe\nchord labels as hidden values within the EM framework.\nIntrainingthemodels,theyusedonlythechordsequence\nasaninputtothemodels,andappliedtheforward-backward\nalgorithm to estimate the model parameters. The frameaccuracy they obtained was about 76% for segmentation\nand about 22% for recognition, respectively. The poor\nperformance for recognition may be due to insufﬁcient\ntraining data compared with a large set of classes (147\nchord types trained on 20 songs). It is also possible that\nthe ﬂat-start initialization of trainingdata yields incor rect\nchordboundariesresultingin poorparameterestimates.\nBello and Pickens also used the chroma features and\nHMMswiththeEMalgorithmtoﬁndthecrudetransition\nprobability matrix for each input [6]. What was novel in\ntheir approachwas that theyincorporatedmusical knowl-\nedge into the models by deﬁning a state transition matrix\nbasedonthekeydistanceina circleofﬁfths,andavoided\nrandom initialization of a mean vector and a covariance\nmatrix of observation distribution. In addition, in train-\ning the model’s parameter, they selectively updated the\nparametersofinterestontheassumptionthatachordtem-\nplate or distribution is almost universal regardless of the\ntypeofmusic,thusdisallowingadjustmentofdistribution\nparameters. The accuracy thus obtained was about 75%\nusing beat-synchronous segmentation with a smaller set\nof chord types (24 major/minor triads only). In particu-\nlar,theyarguedthattheaccuracyincreasedbyasmuchas\n32% when the adjustment of the observation distribution\nparametersisprohibited.\nThepresentpaperexpandsourpreviousworkonchord\nrecognition [8, 9], where we used symbolic data to ob-\ntain a large amount of labeled training data from which\nmodelparameterscanbedirectlyestimatedwithoutusing\nan EM algorithmwhere modelinitialization is critical. In\nthispaper,however,weﬁrstproposeauniﬁedmodeltoac-\ncomplishsimultaneouslytwocloselyrelatedmusicaltasks\n– key estimation and chord transcription – by building\nkey-speciﬁc HMMs. Furthermore, we use a feature vec-\ntorcalled TonalCentroid insteadofchromavector,which\nhasbeenthefeaturesetofchoiceinchordrecognitionsys-\ntems. To authors’ knowledge, this work is the ﬁrst to use\na tonal centroid vector for key extraction as well as for\nchordtranscription.\n3 SYSTEM\nOur chord transcription system starts off by performing\nharmonic analysis on symbolic data to obtain label ﬁles\nwithchordnamesandprecisetimeboundaries. Inparallel,\nwesynthesizetheaudioﬁleswiththesamesymbolicﬁles\nusing a sample-basedsynthesizer. We then extract appro-\npriatefeaturevectorsfromaudiowhichareinperfectsync\nwith thelabels,andusethemtotrainourmodels.\n3.1 ObtainingLabeled Training Data\nInordertotrainasupervisedmodel,weneedalargenum-\nber of audio ﬁles with corresponding label ﬁles which\nmust contain chord names and boundaries. To automate\nthis laborious process, we use symbolic data to generate\nlabel ﬁles aswell asto createtime-alignedaudioﬁles. To\nthisend,weﬁrstconvertasymbolicﬁletoaformatwhichcanbeusedasaninputtoachord-analysistool. Chordan-\nalyzer then performsharmonicanalysis and outputsa ﬁle\nwith root information and note names from which com-\npletechordinformation( i.e.,rootanditssonority–major\nor minor) is extracted. Sequence of chords are used as\npseudo ground-truth or labels when training the HMMs\nalongwithproperfeaturevectors.\nWe used symbolic ﬁles in MIDI (Musical Instrument\nDigital Interface)format. Forharmonicanalysis,we used\nthe Melisma Music Analyzer developed by Sleator and\nTemperley [11]. The Melisma Music Analyzer takes a\npiece of music represented by an event list, and extracts\nmusical information from it such as meter, phrase struc-\nture,harmony,pitch-spelling,andkey. Bycombininghar-\nmony and key information extracted by the analysis pro-\ngram, we can generate label ﬁles with sequence of chord\nnamesandaccurateboundaries.\nThe symbolic harmonic-analysis program was tested\nonacorpusofexcerptsandthe48fuguesubjectsfromthe\nWell-TemperedClavier , andtheharmonyanalysisandthe\nkey extraction yielded an accuracy of 83.7% and 87.4%,\nrespectively[12].\nWe then synthesize the audio ﬁles using Timidity++.\nTimidity++ is a free software synthesizer, and converts\nMIDI ﬁles into audio ﬁles in a WAVE format.1It uses a\nsample-based synthesis technique to create harmonically\nrich audio as in real recordings. We used Fluid3 sound\nfontforsynthesis. Therawaudioisdownsampledto11025\nHz,and12-dimensionalchromafeaturesareﬁrstextracted\nfromaudio,andwereprojectedontoa6-dimensionalspace\ntogeneratetonalcentroidfeatures,whicharedescribedin\nmoredetailinSection3.2. Weusedtheframesizeof8192\nsamplesandthe hopsize of 2048samples, corresponding\nto743msand186ms,respectively.\nTheMIDIﬁlesweusedforharmonicanalysisandsyn-\nthesiswereacquiredfrom http://www.mididb.com ,\nwhichwereallrockmusic. ThenumberofMIDIﬁleswas\n1,046, which correspond to 1,070,752 feature frames or\n55.25 hours of audio. Figure 2 shows the distribution of\n24keysfrom1,046ﬁles.\n3.2 FeatureVector\nHarteet. alproposeda6-dimensionalfeaturevectorcalled\nTonalCentroid ,andusedit to detectharmonicchangesin\nmusical audio [13]. It is based on the Harmonic Network\norTonnetz, which is a planar representationof pitch rela-\ntions where pitch classes having close harmonicrelations\nsuch as ﬁfths, major/minor thirds have smaller Euclidean\ndistancesontheplane.\nTheHarmonicNetworkisatheoreticallyinﬁniteplane,\nbut is wrapped to create a 3-D Hypertorus assuming en-\nharmonic and octave equivalence, and therefore there are\njust 12 chromatic pitch classes. If we reference C as a\npitchclass0, thenwe have12distinct pointsonthe circle\nof ﬁfths from 0-7-2-9- · · ·-10-5, and it wraps back to 0 or\nC. If we travel on the circle of minor thirds, however,we\n1http://timidity.sourceforge.net/CC#DEbEFF#GAbABbBCmC#mDmEbmEmFmF#mGmAbmAmBbmBm020406080100120140160180Key distribution of training data\n  \nFigure2. Keydistributionfrom1,046MIDIﬁles.\nFigure 3. Visualizing the 6-D Tonal Space as three cir-\ncles: ﬁfths, minor thirds, and major thirds from left to\nright. Numbers on the circles correspondto pitch classes\nandrepresentnearestneighborsineachcircle. TonalCen-\ntroidforA majortriad(pitchclass 9,1,and4)is shownat\npointA(adaptedfromHarte et. al[13]).\ncomebacktoareferentialpointonlyafterthreestepsasin\n0-3-6-9-0. Thecircleofmajorthirdsisdeﬁnedinasimilar\nway. This is visualized in Figure 3. As shown in Figure\n3, thesix dimensionsareviewedasthreecoordinatepairs\n(x1, y1),(x2, y2),and(x3, y3).\nUsing the aforementioned representation, a collection\nof pitches like chords is described as a single point in\nthe 6-D space. Harte et. alobtained a 6-D tonal cen-\ntroid vector by projecting a 12-bin tuned chroma vector\nonto the three circles in the equal tempered Tonnetz de-\nscribed above. By calculating the Euclidean distance be-\ntween successive analysis frames of tonal centroid vec-\ntors, they successfully detect harmonic changes such as\nchordboundariesfrommusicalaudio.\nWhilea12-dimensionalchromavectorhasbeenwidely\nusedinmostchordrecognitionsystems,itwasshownthat\nthe tonal centroid feature yielded far less errors in [14].\nThe hypothesiswas that the tonal centroid vector is more\nefﬁcient and more robust because it has only 6 dimen-\nsions, and it puts emphasis on the interval relations such\nas ﬁfths, major/minor thirds, which are key intervals that\ncomprisemost ofmusicalchordsinWestern tonalmusic.3.3 Key-SpeciﬁcHidden MarkovModel\nAhiddenMarkovmodel[10]isanextensionofadiscrete\nMarkovmodel,inwhichthestatesare hiddeninthesense\nthat an underlying stochastic process is not directly ob-\nservable, but can only be observedthroughanother set of\nstochasticprocesses.\nWerecognizechordsusing24-stateHMMs. Eachstate\nrepresentsasinglechord,andtheobservationdistributio n\nismodeledbyasingleGaussianwithdiagonalcovariance\nmatrix. Statetransitionsobeytheﬁrst-orderMarkovprop-\nerty;i.e., the future is independent of the past given the\npresent state. In addition, we use an ergodic model since\nwe allow every possible transition from chord to chord,\nandyetthetransitionprobabilitiesarelearned.\nInourmodel,wehavedeﬁnedtwochordtypes–major\nand minor – for each of 12 chromatic pitch classes, and\nthus we have 24 classes in total. We grouped triads and\nseventhchordswith thesamerootintothesamecategory.\nFor instance, we treated E minor triad and E minor sev-\nenth chord as just E minor chord without differentiating\nthetriadandtheseventh.\nWith the labeled training data obtained from the sym-\nbolicﬁles, we ﬁrst trainourmodelsto estimatethe model\nparameters for each key model. Once the model param-\neters are learned, we extract the feature vectors from the\nreal recordings, and estimate the key by computing the\nlikelihood of each key model given the observation se-\nquenceusingtheViterbialgorithm,whichalsoreturnsthe\noptimal state path, i.e., chord sequence, in a maximum\nlikelihoodsense.\nFigure 4 shows the transition probability matrix of a\nC major key model and the mean and covariance vectors\nof a G major chord in the same model. Transition prob-\nability matrix is strongly diagonal since the frame rate is\nusuallyfasterthantheratethechordchanges. However,it\nisalsoshownthatthetransitionstodominantorsubdomi-\nnantchordsarerelativelyfrequentasisexpectedin West-\nern tonal music. This is indicated by darker off-diagonal\nlines5 or7semitonesapartfromthemaindiagonalline.\nFigure 5 displays the transition probabilities from G\nmajor chord in C major and in C minor key models. The\nobviousdifferenceintransitionprobabilitiesshowninFi g-\nure5, in spite of the fact that they areboth fromthe same\nG major chord, supports our hypothesis that key-speciﬁc\nmodelshelpmakeacorrectdecisionespeciallywhenthere\nis great confusioncaused by a observationvector. For in-\nstance, F major and F minor triad share two chord notes\n– F and C – in common, and thus the observation may\nlook verysimilar. Given no priorabout the key, when the\nprevious chord was G major chord, it will be difﬁcult to\nmake a conﬁdent decision about which chord will follow\nwhen the observationvectorlooksambiguous. Assuming\nit’s in a C major key, however, the system will recognize\nit as F major chord with much more conﬁdence since the\ntransitionprobabilityfromGmajortoFmajorchordisfar\ngreaterthan thatfromG majorto F minorchordin Cma-\njor key (indicated by a dashed circle on the left). For the\nsame reason, it will be identiﬁed as F minor chord withCC#DEbEFF#GAbABbBCmC#mDmEbmEmFmF#mGmAbmAmBbmBm00.010.020.030.040.050.060.070.080.090.1Transition probabilities from G major chord in C major and C minor key models\n  \nC major key\nC minor key\nFigure 5. Transition probabilitiesfrom G major chord in\nCmajorkey(solid)andinCminorkeymodel(dash-dot).\nNote that the probability to itself (G major chord) is very\nhighinbothcases.\nconﬁdence in a C minor key model (right dashed circle).\nWe believe such key-speciﬁc chord progression charac-\nteristics will help decrease confusion seen in observation\nvectors,resultinginincreasedperformance.\n4 EXPERIMENTAL RESULTS AND ANALYSIS\n4.1 Evaluation\nWe tested our models’ performance on the two whole al-\nbums of Beatles (CD1: Please Please Me , CD2:Beatles\nFor Sale), which were also used as test data set in [3, 6].\nEach album contains 14 tracks, and ground-truth annota-\ntions were provided by Harte and Sandler at the Digital\nMusic CenteratUniversityofLondonin QueenMary.2\nIn computing scores, we only counted exact matches\nascorrectrecognition. Wetoleratedtheerrorsatthechord\nboundaries by having a time margin of one frame, which\ncorrespondsapproximatelyto 0.19 second. This assump-\ntion is fair since the segment boundaries were generated\nbyhuman,whichcannotberazorsharp.\nToexaminethe validityofthekey-speciﬁcmodels,we\nalsobuiltthekey-independent,universalmodeltrainedon\nall 1,046ﬁles,andcomparedtheperformance.\n4.2 ResultsandDiscussion\nTable 1 shows the frame-rate accuracy in percentage for\neachmodel.\nModel Key-independent Key-speciﬁc Increase\nCD1 61.026 63.978 4.837\nCD2 84.482 84.746 0.312\nTotal 72.754 74.362 2.210\nTable 1. Chordrecognitionresults(%accuracy)\n2http://www.elec.qmul.ac.uk/digitalmusic/24x24 transition probability matrix of C major key model\nCC#DEbEFF#GAbABbBCmC#mDmEbmEmFmF#mGmAbmAmBbmBmC\nC#\nD\nEb\nE\nF\nF#\nG\nAb\nA\nBb\nB\nCm\nC#m\nDm\nEbm\nEm\nFm\nF#m\nGm\nAbm\nAm\nBbm\nBm\nx1 y1 x2 y2 x3 y3−0.0100.010.020.030.040.050.060.070.080.09Mean and covariance vectors of G major chord in C major key model\nFigure 4. (a) 24 ×24transitionprobabilitymatrix of a C major key model. For v iewing purpose,logarithmwas taken of\nthe original matrix. Axes are labeled in the order of major an d minor chords. (b) Mean and covariance vectors of a G\nmajorchordinthe samemodel.\nThe results shown in Table 1 prove our hypothesis on\nkey-speciﬁc models although they don’t make a signiﬁ-\ncant improvementover a key-independentmodel. Partic-\nularly, the increase in performance is greater with CD1\nfor which overall performance is much lower than that\nfor CD2. A possible explanation for this is using key-\nspeciﬁc modelsis of greater help when more ambiguities\narepresentinobservationvectors.\nOur results compare favorably with other state-of-the-\nartsystemsproposedbyHarteandSandler[3]orbyBello\nand Pickens [6]. Using the same test data set, Harte and\nSandlerobtained53.9%and70.8%offrame-rateaccuracy\nfor CD1 and CD2, respectively. They deﬁned 48 differ-\nenttriadsincludingdiminishedandaugmentedtriads,and\nusedapatternmatchingalgorithmforchordidentiﬁcation,\nfollowed by median ﬁltering for smoothing. Using the\nHMMs with 24 states for just major/minor chords, Bello\nand Pickens’ system yielded the performance of 68.55%\nand 81.54% for CD1 and CD2, respectively. However,\nthey went through a pre-processing stage of beat detec-\ntion to perform a tactus-based analysis. Without a beat-\nsynchronousanalysis,theiraccuracydropsdownto58.96%\nand 74.78% for each CD, which is lower than our results\nwhichare63.98%and84.75%.\nAs to key estimation, 22 out of 26 tracks were cor-\nrectly identiﬁed, corresponding to 84.62% of accuracy.\nTwotracksweredisregardedinkeyestimationbecauseof\ntheir ambiguities. Table 2 shows a confusion matrix for\nthekeyestimationtask.\nWe can observe from the confusion matrix in Table 2\nthat all mis-recognizedkeys are in ﬁfth relationswith the\noriginal keys (G-C and D-A). This is probably because\nsuchkeysnotonlysharemanychordsincommonbutalso\nthechordprogressionpatternissimilar toeachother.KeyCDEGAB♭Accuracy(%)\nC(4)30010075.00\nD(5)02003040.00\nE(8)008000100.00\nG(4)10030075.00\nA(4)000040100.00\nB♭(1)000001100.00\nTable 2. Confusion matrix for key estimation task. Cor-\nrectestimationisin boldface.\n5 CONCLUSION\nInthispaper,wepresentedauniﬁedsystemforautomatic\nchordtranscriptionandkeyextractionfromtherawaudio.\nThe main contribution of this work is the demonstration\nthatbothmusicaltaskscanbeaccomplishedwithoutusing\nany other feature vectors or algorithms by building key-\nspeciﬁc models.\nUsing symbolicmusic ﬁles such as MIDIwas a keyto\navoidingtheextremelylaboriousprocessofmanualanno-\ntation. In order to achieve this goal, we ﬁrst performed\nharmonic analysis on the symbolic data, which contains\nnoise-free pitch and time information, to generate label\nﬁles with chord names and precise time boundaries. In\nparallel, by using a sample-based synthesizer, we could\ncreateaudioﬁleswhichhaveharmonicallyrichspectraas\nin real acoustic recordings. The label ﬁles andaudiogen-\nerated from the same symbolic ﬁles are in perfect align-\nment,andwere usedto trainourmodels.\nAs feature vectors, we used a 6-dimensional feature\ncalled Tonal Centroid, which was provedto outperforma\nconventionalchromafeatureinpreviousworkbythesame\nauthors.\nEach state in HMMs was modeled by a multivariate,\nsingle Gaussian completely represented by its mean vec-\ntor and covariancematrix. We have deﬁned 24 classes orchord types in our models, which include for each pitch\nclass major and minor chords. We treated seventh chords\nas theircorrespondingroottriads, and disregardeddimin-\nishedandaugmentedchordssincetheyveryrarelyappear\ninWestern tonalmusic,especiallyin rockmusic.\nBasedonthecloserelationshipbetweenkeyandchord\ninWestern tonalmusic,wehavebuilt24key-speciﬁc\nHMMs, one for each key. We then applied the same ap-\nproachasusedinisolatedwordrecognitionsystems. That\nis, giventheobservationsequence,wecomputedthelike-\nlihood of each key model to estimate the key using the\nViterbidecoder,whichalsoreturnedtheoptimalstatepath\nthatisidenticaltothe frame-ratechordsequence.\nExperiments showed an increase in chord recognition\naccuracy with the key-speciﬁc model compared with the\nkey-independent model trained on all data regardless of\nkeys.\nIn this paper, we trained our models only on rock mu-\nsic,andthetestdatawasofthesamekind. Itwasshownin\n[14]that thegenrealso hasagreat impactinmodel’sper-\nformance. We thereforeplanto buildgenre-speciﬁcmod-\nelsandcombinethemwithkey-speciﬁcmodelstodevelop\namodelforeachgenreandkey,whichmayworkforgenre\nclassiﬁcation aswell asforchordrecognitionandkey ex-\ntraction. A smoothing technique may be accompanied in\nsuchmodelsduetoa datasparsityproblem.\nInaddition,weconsiderhigher-orderHMMsinthefu-\nture because chord progressions based on Western tonal\nmusictheoryrevealsuchhigher-ordercharacteristics. Th ere-\nfore, knowing two or more preceding chords will help\nmake a decision with more conﬁdence. We also plan to\nbuildrichermodelsusingGaussianmixturemodelsorSup-\nportVectorMachinesinordertobetterrepresenttheemis-\nsion probabilities as we increase the size of training data\nevenmore.\n6 REFERENCES\n[1] K. Lee, “Identifying cover songs from audio us-\ning harmonic representation,” in extended abstract\nsubmitted to Music Information Retrieval eXchange\ntask,BC, Canada,2006.\n[2] T. Fujishima, “Realtime chord recognition of musi-\ncal sound: A system using Common Lisp Music,”\ninProceedingsoftheInternationalComputerMusic\nConference . Beijing: InternationalComputer Mu-\nsic Association,1999.\n[3] C. A. Harte and M. B. Sandler, “Automatic chord\nidentiﬁcation using a quantised chromagram,” in\nProceedings of the Audio Engineering Society .\nSpain: AudioEngineeringSociety,2005.\n[4] K. Lee, “Automatic chord recognition using en-\nhancedpitchclassproﬁle,”in ProceedingsoftheIn-\nternational Computer Music Conference , New Or-\nleans,USA,2006.[5] A. Sheh and D. P. Ellis, “Chord segmentation and\nrecognition using EM-trained hidden Markov mod-\nels,” inProceedingsof the InternationalConference\non Music Information Retrieval , Baltimore, MD,\n2003.\n[6] J. P. Bello and J. Pickens, “A robust mid-level rep-\nresentation for harmonic content in music signals,”\ninProceedings of the International Conference on\nMusic InformationRetrieval ,London,UK,2005.\n[7] J. Morman and L. Rabiner, “A system for the au-\ntomatic segmentationand classiﬁcation of chord se-\nquences,” in Proceedingsof Audio and Music Com-\nputingforMultimediaWorkshop ,Santa Barbar,CA,\n2006.\n[8] K.LeeandM.Slaney,“Automaticchordrecognition\nusing an HMM with supervised learning,” in Pro-\nceedings of the International Conference on Music\nInformationRetrieval ,Victoria,Canada,2006.\n[9] ——, “Automatic chord recognition from audio us-\ning a supervised HMM trained with audio-from-\nsymbolic data,” in Proceedings of Audio and Music\nComputingforMultimediaWorkshop ,Santa Barbar,\nCA, 2006.\n[10] L. R. Rabiner, “A tutorial on hidden Markov mod-\nels and selected applicationsin speech recognition,”\nProceedingsoftheIEEE ,vol.77,no.2,pp.257–286,\n1989.\n[11] D. Sleator and D. Temperley, “The Melisma Mu-\nsic Analyzer,” http://www.link.cs.cmu.edu/music-\nanalysis/,2001.\n[12] D. Temperley, The cognitionof basic musical struc-\ntures. TheMITPress,2001.\n[13] C. A. Harte, M.B. Sandler,andM. Gasser,“Detect-\ning harmonicchangein musical audio,”in Proceed-\nings of Audio and Music Computingfor Multimedia\nWorkshop ,SantaBarbara,CA, 2006.\n[14] K. Lee, “A system for automatic chord recogni-\ntionfromaudiousinggenre-speciﬁchiddenMarkov\nmodels,” in International Workshop on Adaptive\nMultimedia Retrieval , Paris, France, 2007, accepted\nforpublication."
    },
    {
        "title": "Globe of Music Music Library Visualization Using Geosom.",
        "author": [
            "Stefan Leitich",
            "Martin Topf"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416930",
        "url": "https://doi.org/10.5281/zenodo.1416930",
        "ee": "https://zenodo.org/records/1416930/files/LeitichT07.pdf",
        "abstract": "Music collections are commonly represented as plain textual lists of artist, title, album etc. for each contained music track. The large volume of personal music libraries makes them difficult to browse and access for users. In respect to possible information visualization techniques, no established convenient user interfaces exist. By using a spherical self-organizing map algorithm on low level audio features and processing the resulting map data, a Geographic Information System is used to visualize a music collection. This results in an aspiring music library visualization, which can be handled intuitively by the user and even provides new possibilities for accessing a music collection in the digital domain. 1 INTRODUCTION The advent of digital media libraries has definitely passed. Nearly every media consumer is in possession of a digital library of media documents nowadays. This of course includes private music collections. In previous days, one browsed through her own music collection by picking CDs from shelves, rearranging stacks and heaps, while searching for the desired track. This was on the one hand an annoying task, but on the other hand somehow part of the listening experience. After having handled a CD a few hundred times, while searching for another one, the artwork as visual experience becomes strongly connected to the tracks the CD contains. In the digital domain the dominant interface for browsing a personal media library was, and partly still is, a search task in a long list of plain textual information. Those lists contain known metadata of a song like artist, title, album, etc. In commercially available media playback applications, the textual playlist is still the most prominent kind of library representation that the user has to deal with. One exception is Apple iTunes with its “cover flow view”. In this visualization, the user has the possibility of browsing through the tracks of her media library, while the artwork is presented in a neatly animated horizontal lineup, with the selected song’s artwork in the center. c⃝2007 Austrian Computer Society (OCG). The coinciding popularity of Geographic Information Systems (GIS), for example Google Earth or NASA WorldWind 1 , means that users are familiar with the interactions possible in such a system. One can easily navigate by rotating, tilting and zooming to access the desired view. Our approach, Globe of Music, makes use of a spherical self-organizing map (SOM) algorithm, to arrange the items of a digital music library on a sphere in context of a GIS. 2 RELATED WORK An early publication in the research domain of Music Information Retrieval (MIR) by Tzanetakis, tackled not only feature extraction aspects for audio and related classification methods, but also possible visualization techniques in this domain [9]. He proposed these techniques as interaction methods to enhance audio editors supported by automatic feature extraction, but also browsers for audio collections to visualize timbre or genre information of audio in a 2D and 3D manner. Similar in structure to this approach is the work by Pampalk [7], who developed a psychoacoustically motivated approach for feature extraction and combined it with a visualization technique using a landscape metaphor the so-called Islands of Music. This approach was further developed by Pampalk himself, using aligned SOMs to support multiple, user-adjustable views of a music collection [5], still using the landscape metaphor. Knees et al. [2] interpreted the height profile of the landscape metaphor to create a 3D representation of the Islands of Music. Additionally, they introduced an anisotropic auralization in this approach, by rendering a 5.1 surround sound model and augmenting the environment by related images and terms retrieved from the web in this user interface. Contrasting visualization techniques are presented in the work by Torrens et al. [8]. Here, 2D visualizations in the form of a disc, a rectangle, and a tree-map are presented, using criterias like genre, artist, year of creation, and a definable quantitative criterion such as playcount. Goto proposes in [1] an user interface Musicream combining the query-by-example and browsing paradigm. 1 http://worldwind.arc.nasa.gov Music tracks are visualized as music-discs and described by feature vectors. The so-called “streaming function” creates a steady flow of music-discs over the screen. If the user picks one of these discs and approaches another disc with it, similar music-discs, in terms of feature vector distance, will stick together and form a playlist. Another approach is the joined work by Goto and Pampalk [6] called MusicRainbow. This approach exploits a combination of signal based information (spectral similarity and fluctuation patterns) and information from the web. Visually, this depicts a “circular rainbow” with similar artists placed next to each other using a one-dimensional circular SOM. High-level terms (e.g.“rock”) on the inside and on the outside music-related terms (e.g. instruments, style attributes, etc.) are used for labeling. Colored rings encode different styles of music, and are sorted by the size of the styles part within the collection. A turning knob is used as an input device to browse the music collection. Our approach is motivated by the landscape metaphor, but augments this with a spherical representation using a spherical SOM Globe of Music. The spherical SOM intrinsically abolishes the border effect of a SOM, and the sphere or globe serves as the user interface metaphor of intuitive perception. A more detailed description is given in the next section. 3 APPROACH Our approach is composed of three steps: (1) extracting signal-based features from audio, (2) using the GeoSOM algorithm [11] to arrange music tracks according to feature vectors, and finally (3) transforming this information into a GIS renderable format, which depicts the user interface.",
        "zenodo_id": 1416930,
        "dblp_key": "conf/ismir/LeitichT07",
        "keywords": [
            "Digital media libraries",
            "Personal music collections",
            "Browsing and access",
            "User interfaces",
            "Spherical self-organizing map",
            "Geographic Information System",
            "Visualization techniques",
            "Audio features",
            "Music library visualization",
            "Intuitive user handling"
        ],
        "content": "GLOBE OF MUSIC - MUSIC LIBRARY VISUALIZATION USING\nGEOSOM\nStefan Leitich, Martin Topf\nUniversity of Vienna\nDepartment of Distributed and Multimedia Systems\nLiebiggasse 4/3-4, 1010 Vienna, AT\nABSTRACT\nMusic collections are commonly represented as plain tex-\ntual lists of artist, title, album etc. for each contained mu-\nsic track. The large volume of personal music libraries\nmakes them difﬁcult to browse and access for users. In\nrespect to possible information visualization techniques,\nno established convenient user interfaces exist. By using\na spherical self-organizing map algorithm on low level au-\ndio features and processing the resulting map data, a Ge-\nographic Information System is used to visualize a music\ncollection. This results in an aspiring music library vi-\nsualization, which can be handled intuitively by the user\nand even provides new possibilities for accessing a music\ncollection in the digital domain.\n1 INTRODUCTION\nThe advent of digital media libraries has deﬁnitely passed.\nNearly every media consumer is in possession of a digi-\ntal library of media documents nowadays. This of course\nincludes private music collections. In previous days, one\nbrowsed through her own music collection by picking CDs\nfrom shelves, rearranging stacks and heaps, while search-\ning for the desired track. This was on the one hand an\nannoying task, but on the other hand somehow part of the\nlistening experience. After having handled a CD a few\nhundred times, while searching for another one, the art-\nwork as visual experience becomes strongly connected to\nthe tracks the CD contains.\nIn the digital domain the dominant interface for brows-\ning a personal media library was, and partly still is, a\nsearch task in a long list of plain textual information.\nThose lists contain known metadata of a song like artist, ti-\ntle, album, etc. In commercially available media playback\napplications, the textual playlist is still the most promi-\nnent kind of library representation that the user has to deal\nwith. One exception is Apple iTunes with its “cover ﬂow\nview”. In this visualization, the user has the possibility\nof browsing through the tracks of her media library, while\nthe artwork is presented in a neatly animated horizontal\nlineup, with the selected song’s artwork in the center.\nc\r2007 Austrian Computer Society (OCG).The coinciding popularity of Geographic Information\nSystems (GIS), for example Google Earth or NASA\nWorldWind1, means that users are familiar with the in-\nteractions possible in such a system. One can easily navi-\ngate by rotating, tilting and zooming to access the desired\nview.\nOur approach, Globe of Music , makes use of a spheri-\ncal self-organizing map (SOM) algorithm, to arrange the\nitems of a digital music library on a sphere in context of a\nGIS.\n2 RELATED WORK\nAn early publication in the research domain of Music In-\nformation Retrieval (MIR) by Tzanetakis, tackled not only\nfeature extraction aspects for audio and related classiﬁca-\ntion methods, but also possible visualization techniques\nin this domain [9]. He proposed these techniques as in-\nteraction methods to enhance audio editors supported by\nautomatic feature extraction, but also browsers for audio\ncollections to visualize timbre or genre information of au-\ndio in a 2D and 3D manner.\nSimilar in structure to this approach is the work by\nPampalk [7], who developed a psychoacoustically moti-\nvated approach for feature extraction and combined it with\na visualization technique using a landscape metaphor -\nthe so-called Islands of Music . This approach was fur-\nther developed by Pampalk himself, using aligned SOMs\nto support multiple, user-adjustable views of a music col-\nlection [5], still using the landscape metaphor. Knees et\nal. [2] interpreted the height proﬁle of the landscape meta-\nphor to create a 3D representation of the Islands of Music .\nAdditionally, they introduced an anisotropic auralization\nin this approach, by rendering a 5.1 surround sound model\nand augmenting the environment by related images and\nterms retrieved from the web in this user interface.\nContrasting visualization techniques are presented in\nthe work by Torrens et al. [8]. Here, 2D visualizations in\nthe form of a disc, a rectangle, and a tree-map are pre-\nsented, using criterias like genre, artist, year of creation,\nand a deﬁnable quantitative criterion such as playcount.\nGoto proposes in [1] an user interface - Musicream -\ncombining the query-by-example and browsing paradigm.\n1http://worldwind.arc.nasa.govMusic tracks are visualized as music-discs and described\nby feature vectors. The so-called “streaming function”\ncreates a steady ﬂow of music-discs over the screen. If\nthe user picks one of these discs and approaches another\ndisc with it, similar music-discs, in terms of feature vector\ndistance, will stick together and form a playlist. Another\napproach is the joined work by Goto and Pampalk [6]\ncalled MusicRainbow . This approach exploits a combi-\nnation of signal based information (spectral similarity and\nﬂuctuation patterns) and information from the web. Visu-\nally, this depicts a “circular rainbow” with similar artists\nplaced next to each other using a one-dimensional circular\nSOM. High-level terms (e.g.“rock”) on the inside and on\nthe outside music-related terms (e.g. instruments, style at-\ntributes, etc.) are used for labeling. Colored rings encode\ndifferent styles of music, and are sorted by the size of the\nstyles part within the collection. A turning knob is used as\nan input device to browse the music collection.\nOur approach is motivated by the landscape metaphor,\nbut augments this with a spherical representation using a\nspherical SOM - Globe of Music . The spherical SOM in-\ntrinsically abolishes the border effect of a SOM, and the\nsphere or globe serves as the user interface metaphor of\nintuitive perception. A more detailed description is given\nin the next section.\n3 APPROACH\nOur approach is composed of three steps: (1) extracting\nsignal-based features from audio, (2) using the GeoSOM\nalgorithm [11] to arrange music tracks according to fea-\nture vectors, and ﬁnally (3) transforming this information\ninto a GIS renderable format, which depicts the user inter-\nface.\n3.1 Feature Extraction\nThe features used to describe the music pieces, are the\nStatistical Spectrum Descriptor (SSD) evaluated by Lidy\nin [4]. According to Lidy they showed reasonable perfor-\nmance for classiﬁcation by similarity tasks in relation to\nfeature vector dimension.\nThe SSD is composed of seven statistical moments\n(mean, median, variance, skewness, kurtosis, min- and\nmax-value) for the 24 critical bands. The descriptor cap-\ntures the frequency characteristics in terms of statistical\ninformation about the audio signal, resulting in a feature\nvector of 168 elements.\nThe collection used for initial user experiments (cf.\nSection 4) is the training part of the ISMIR Genre Classiﬁ-\ncation Collection of the ISMIR 2004 contest, provided by\nMagnatune2. The collection used, consists of 728 tracks\nof different styles: classical (319), electronic (115), jazz/-\nblues (26), metal/punk (45), rock/pop (101), world (122).\n2http://www.magnatune.com\nFigure 1 . Icosahedron with tessellation frequency 1 to\n4 [11].\n3.2 GeoSOM\nNeural networks, especially the self-organizing map pro-\nposed by Kohonen [3], are popular tools for information\norganization in visualization techniques used in MIR. In\nour approach we apply a spherical SOM, namely the Geo-\nSOM proposed by Wu et al. in [11, 10], which provides,\nthrough a 2D data structure, a space- and time-efﬁcient\napproach for neighborhood searching. For a detailed com-\nparison of different spherical SOM implementations, refer\nto [11].\nThe GeoSOM is suited for the spherical representation\nthrough the use of a tesselated icosahedron (see Fig. 1) in-\nstead of a rectangular or hexagonal lattice of a basic SOM.\nFurthermore it reduces data distortion due to the removal\nof boundaries.\nA short overview of this algorithm and its data structure\nis given in the following part of this paper. For a detailed\ndescription we refer to [11].\nAn icosahedron-based geodesic dome is, among the\nﬁve platonic polyhedra, the most similar one to a sphere.\nWith the exception of the original 12 vertices (5 neigh-\nbors), all vertices have 6 immediate neighbors. Depending\non the desired number of neurons N(N=f2\u000310 + 2 ),\nthe icosahedron can be tessellated to increase its dome’s\nfrequencyf.\nCutting the dome open and tilting in a way to retrieve\nan orthogonally aligned 2D grid, results in Fig. 2. The\npole’s (A, C) vertices are repeated 4 times (e.g. A, A’,\n... , A””) and vertices along the cut edges are duplicated\nonce (e.g. E, E’). Be aware that there are also connections\nalong one diagonal between two points (see enlargement\nof point E in Fig. 2). This must be considered when calcu-\nlating the distance between two points in the 2D grid, and\nwhen searching and updating the neighborhood.\nFor storing the 2D matrix we used two one-\ndimensional arrays, called uarray andvarray as pro-\nposed in Wu et. al [11]. Vertices of a certain ucoordinate\nare stored in a varray (ordered according to their vco-\nordinates) and those varrays themselves are contained in\ntheuarray. The duplicated points can be found by start-\ning from the marked points in Fig. 2 and proceeding in the\nindicated directions. Boundaries starting at points P and\nQ are duplicated as P’ and Q’ at the other end of the data\nstructure.\nThe training process of the GeoSOM is similar to the\ntraining of a SOM, considering the different data structure\nregarding the tesselated icosahedron. After the trainingFigure 2 . The 2D data structure for icosahedron-based\ngeodesic domes [11].\nFigure 3 . Total view of Globe of Music .\nprocess the GeoSOM data structure is mapped onto the\nsphere. The data structure is traversed in diagonals from\nthe lower left to the upper right and coordinates are calcu-\nlated.\n3.3 Geographic Information System\nWe made use of the NASA World Wind (Ver. 1.4) GIS, an\nopen source project developed at NASA Ames Research\nCenter. It allows users to explore the earth via satellite im-\nagery (e.g. LandSat dataset), elevation data, and meteoro-\nlogic data, also to retrieve views of other planets. World\nWind offers the possibility of extending its functional-\nity via a plugin mechanism and the deﬁnition of custom\n“worlds”. For an extensive description of NASA World\nWind we recommend the project’s website3.\nIn order to deﬁne a custom globe like the Globe of Mu-\nsic, an XML ﬁle deﬁning the most basic attributes of a\nworld - for example name, equatorial radius and server\n3http://worldwind.arc.nasa.govconnection information, if the world contains remotely\nstored terrain data - has to be created\nFig. 3 shows a screenshot of the Globe of Music visu-\nalization in World Wind.\nInformation displayed on a globe in World Wind is ar-\nranged in layers above the basic terrain data. Layers make\nit possible to place, icons on certain longitude/latitude co-\nordinates with a deﬁned height above surface, including\nname, texture, dimension, and a link, to be followed when\nclicking the related icon. These are the most important\nattributes used in our implementation, the possibilities for\nvisualizing information on a layer are manifold.\nIn a layer deﬁnition the data deﬁned in boundaries and\nplacename ﬁles can also be imported by referencing the\nboundary and placename data, and deﬁning visualization\nparameters such as the range of viewers altitude for visi-\nbility, distance above surface, font, etc.\nThe layer we have created contains a background im-\nage and icons for every music track placed on the Globe\nof Music . Icons are textured with the artwork and linked\nto the corresponding music ﬁle (see Fig. 3). Pointing to an\nicon with the mouse cursor displays the track’s artist and\ntitle information.\nCoordinates of the icons are determined by the neuron\n(sector) of the GeoSOM the music track belongs to. If\nthere is more than one track placed on a neuron, tracks are\nstringed in this sector next to each other on the globe.\nBoundaries (e.g. a country’s border) and placenames\n(e.g. a country’s name) are represented in World Wind via\nbinary ﬁle formats. With the Globe of Music , these for-\nmats are used to (1) outline sectors on the globe related\nto a GeoSOM neuron, and (2) to deﬁne placenames, in\nour case artist name and track title, for the coordinates at\nwhich icons are placed. This allows a visualization of the\nsectors that the globe is divided into, and the usage of the\nbuilt-in place ﬁnder tool to query for and locate individual\ntracks by text search.\n4 INITIAL USER EXPERIMENTS\nIn the setting of the initial user experiments, users were\npresented the Globe of Music visualization and were given\na description and mission in the form of: (1) This visual-\nization depicts a music collection. (2) Music tracks are\narranged on the globe, represented by icons of related art-\nwork. (3) Minor interaction explanations were given if re-\nquired (rotate, zoom, tilt, click for playback). (4) The mu-\nsic collection contains music tracks presumably unknown\nto the user. (5) Browse through and explore this collection\nand try to determine its composition.\nTest-users were allowed to browse for approximately\n15-20 min., followed by a survey about their experience.\nThe composition of the group of test-users was in terms of\ngender 2 to 1 (male to female). The group’s size was 12\npeople, with an average age of 27.25 ( \u001b\u00195:88).\nA survey was conducted on the basis of a questionnaire\nand a semi-structured interview, by asking questions about\nthe composition of the music collection involved, organi-zation of the music title’s placement, experiences of orien-\ntation in the visualization, and letting the test-users com-\nment freely.\nThe test candidates were asked to guess the size of the\nmusic collection presented, and to enumerate the varieties\nof music genres, as well as the size of a genre’s collection\npart found while browsing. The intention was to ﬁgure out\nhow well an unknown music collection can be explored.\nThe genres contained in the collection could be clearly de-\ntermined, 10 out of 12 persons found all of them. Classical\nmusic was correctly determined as the dominant genre of\nthe collection.\nUsers were asked about their impression of the orga-\nnization of songs on the globe. They were not given any\ninformation a priori, neither did the visualization include\nany information about organization of the placement. The\nonly way to determine the organization was by listening to\nthe music tracks. Again, 10 out of 12 persons were con-\nvinced there is a systematical approach in the placement of\ntracks. Common agreement was present about the neigh-\nborhood placement of similarly perceived songs, but spec-\nulations varied from longitudinal or latitudinal mapped\nparameters for placement.\nThe determination of orientation in the Globe of Music\nwas done by asking the test-users to locate a music title\nthey recalled whilst browsing. This task was performed\nvery well and proved the importance of visual association\nbetween music listened to and its artwork. The greater\npart of the test group stated it was easy to ﬁnd a certain\ntrack by remembering the related artwork.\nIn common comments, users reported a problem with\nnavigation, because of the globe’s axis. This circumvents\nthe free rotation around all axes and makes navigation\ndifﬁcult near the poles. Overall, users were very inter-\nested in using such a visualization and asked for common\nplayback application features such as managing playlists,\nadding new songs to the collection, and information about\nthe actual music title playing.\n5 CONCLUSION\nIn this paper we presented a novel approach for visualiz-\ning music collections using a GIS metaphor. It was con-\nducted by feeding a spherical SOM (GeoSOM) with sig-\nnal based feature vectors for music track description, and\nvisualizing the resulting spatial distribution on a globe in\nNASA World Wind. Inital user experiments with our pro-\ntotype showed promising results in terms of high user ac-\nceptance. This visualization provides the user with an in-\ntuitive interface which is easy to handle and fun to ex-\nplore. The ideas and feature requests collected during ex-\nperiments encourage further development.\n6 ACKNOLEDGEMENTS\nWe would like to thank the students Robert Neuner, An-\ndreas Seidler, and Michael M ¨uller supporting our workwith their investigations regarding data formats and ex-\ntending NASA World Wind.\n7 REFERENCES\n[1] M. Goto and T. Goto. Musicream: New music play-\nback interface for streaming, sticking, sorting, and re-\ncalling musical pieces. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval ,\npages 404–411, London, UK, 11-15 September 2005.\n[2] P. Knees, M. Schedl, T. Pohle, and G. Widmer. An\ninnovative three-dimensional user interface for explor-\ning music collections enriched. In MULTIMEDIA ’06:\nProceedings of the 14th annual ACM international\nconference on Multimedia , pages 17–24, New York,\nUSA, 2006. ACM Press.\n[3] T. Kohonen, editor. Self-organizing maps . Springer-\nVerlag New York, Inc., Secaucus, NJ, USA, 1997.\n[4] T. Lidy and A. Rauber. Evaluation of feature extrac-\ntors and psycho-acoustic transformations for music\ngenre classiﬁcation. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval ,\npages 34–41, London, UK, 11-15 September 2005.\n[5] E. Pampalk, S. Dixon, and G. Widmer. Exploring mu-\nsic collections by browsing different views. In Pro-\nceedings of the 4th International Conference on Music\nInformation Retrieval (ISMIR’03), Washington, D.C.,\nUSA, October 2003.\n[6] E. Pampalk and M. Goto. Musicrainbow: A new user\ninterface to discover artists using audio-based similar-\nity and web-based labeling. In 7th International Con-\nference on Music Information Retrieval , pages 367–\n370, Victoria, BC, Canada, 8-12 October 2006.\n[7] E. Pampalk, A. Rauber, and D. Merkl. Content-based\norganization and visualization of music archives. In\nMULTIMEDIA ’02: Proceedings of the tenth ACM\ninternational conference on Multimedia , pages 570–\n579, New York, USA, 2002. ACM Press.\n[8] M. Torrens, P. Hertzog, and J.L. Arcos. Visualizing\nand exploring personal music libraries. In 5th Inter-\nnational Conference on Music Information Retrieval ,\nBarcelona, Spain, 10-14 October 2004.\n[9] G. Tzanetakis. Manipulation, analysis and retrieval\nsystems for audio signals. Technical Report TR-651-\n02, Princeton Computer Science, 2002.\n[10] Y . Wu and M. Takatsuka. The geodesic self-organizing\nmap and its error analysis. In ACSC ’05: Proceedings\nof the Twenty-eighth Australasian conference on Com-\nputer Science , pages 343–351, Darlinghurst, Aus-\ntralia, 2005. Australian Computer Society, Inc.\n[11] Y . Wu and. Takatsuka. Spherical self-organizing map\nusing efﬁcient indexed geodesic data structure. Neural\nNetworks , 19(6-7):900–910, July-August 2006."
    },
    {
        "title": "Towards a Human-Friendly Melody Characterization by Automatically Induced Rules.",
        "author": [
            "Pedro J. Ponce de León",
            "David Rizo",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414902",
        "url": "https://doi.org/10.5281/zenodo.1414902",
        "ee": "https://zenodo.org/records/1414902/files/LeonRQ07.pdf",
        "abstract": "There is an increasing interest in music information retrieval for reference, motive, or thumbnail extraction from a piece in order to have a compact and representative representation of the information to be retrieved. One of the main references for music is its melody. In a practical environment of symbolic format collections the information can be found in standard MIDI file format, structured as a number of tracks, usually one of them containing the melodic line, while the others contain the accompaniment. The goal of this work is to analyse how statistical rules can be used to characterize a melody in such a way that one can understand the solution of an automatic system for selecting the track containing the melody in such files. 1 INTRODUCTION Digital music scores can be found in digital collections in a number of ways. One of the most popular formats is the standard MIDI file, where the information is structured in such a way that the melody part is stored in one or more tracks, often separately from the rest of the musical content. This is frequently the case of modern popular music. The literature about melody voice identification in the symbolic domain is quite poor. Ghias et al. [2] built a system to process MIDI files extracting a sort of melodic line using simple heuristics. Tang et al. [7] present a work where the aim is to propose candidate melody tracks, given a MIDI file. They take decisions based on single features derived from informal assumptions about what a melody track should be. Other works in this line have been recently published at the light of recent developments [6] like that of Madsen and Widmer [4] that uses information theory measures like entropy to approach the problem. The large-term goal of this work is to pose the general question What is a melody? under a statistical approach. The answer would be given as sets of rules that are both human-readable and automatically learnt from score corpora. This could be of interest for musicologists or helpful in applications such as melody matching, motif extraction, melody extraction (ringtones), etc. In this paper the first stages of this work are presented. c⃝2007 Austrian Computer Society (OCG). 2 METHODOLOGY The steps performed to obtain sets of rules that characterize melody tracks can be outlined as follows: Feature extraction Describe a MIDI track by a set of low-level statistical descriptors. Decision tree learning Learn a set of decision trees from a tagged corpus of MIDI-tracks. Rule extraction Extract rules from decision tree branches. Rule simplification Prune rule antecedents. Rule selection Select best rules by ranking. The details about the first two steps can be found in [6]. The low-level statistical descriptors utilized to describe musical content are based on several categories of features that assess melodic and rhythmic properties of a music sequence, such as pitch, note interval, and note duration distribution descriptors, as well as track related properties such as polyphony rate, duration, etc. Most distribution descriptors are also present in normalized form with respect to the value range for that descriptor in the whole song. This allows to know the context in which a particular descriptor value is computed. The sets of rules that characterize melody tracks are initially extracted from an ensemble of decision trees that has shown good performance at this task [6]. The rules are simplified by pruning non useful antecedents and then, for each rule set, rules are ranked according to different metrics. Note that there is no feature selection stage, as the Random Forest [1] method used to learn decision tree ensembles performs its own feature selection process. Expressing a concept by rules has the advantage of being a human-readable description of the characterization process. The simplification and ranking steps focus on obtaining a small, manageable rule system. A side goal has been to test whether a small number of selected rules can perform comparably to an ensemble of decision trees, typically containing hundreds or thousands of nodes.",
        "zenodo_id": 1414902,
        "dblp_key": "conf/ismir/LeonRQ07",
        "keywords": [
            "digital music scores",
            "MIDI file format",
            "melody part",
            "melodic line",
            "information retrieval",
            "statistical rules",
            "automatically learnt",
            "decision tree learning",
            "rule extraction",
            "rule simplification"
        ],
        "content": "TOWARDS A HUMAN-FRIENDLY MELODY CHARACTERIZATION BY\nAUTOMATICALLY INDUCED RULES\nPedro J. Ponce de Le ´on, David Rizo, Jos ´e M. I ˜nesta\nUniversidad de Alicante, Spain\nDepartamento de Lenguajes y Sistemas Inform ´aticos\n{pierre,drizo,inesta }@dlsi.ua.es\nABSTRACT\nThere is an increasing interest in music information re-\ntrieval for reference, motive, or thumbnail extraction fro m\na piece in order to have a compact and representative rep-\nresentation of the information to be retrieved. One of the\nmain references for music is its melody. In a practical\nenvironment of symbolic format collections the informa-\ntion can be found in standard MIDI ﬁle format, structured\nas a number of tracks, usually one of them containing the\nmelodic line, while the others contain the accompaniment.\nThe goal of this work is to analyse how statistical rules can\nbe used to characterize a melody in such a way that one\ncan understand the solution of an automatic system for se-\nlecting the track containing the melody in such ﬁles.\n1 INTRODUCTION\nDigital music scores can be found in digital collections in\na number of ways. One of the most popular formats is the\nstandard MIDI ﬁle, where the information is structured in\nsuch a way that the melody part is stored in one or more\ntracks, often separately from the rest of the musical con-\ntent. This is frequently the case of modern popular music.\nThe literature about melody voice identiﬁcation in the\nsymbolic domain is quite poor. Ghias et al. [2] built a\nsystem to process MIDI ﬁles extracting a sort of melodic\nline using simple heuristics. Tang et al. [7] present a work\nwhere the aim is to propose candidate melody tracks, given\na MIDI ﬁle. They take decisions based on single features\nderived from informal assumptions about what a melody\ntrack should be. Other works in this line have been re-\ncently published at the light of recent developments [6]\nlike that of Madsen and Widmer [4] that uses information\ntheory measures like entropy to approach the problem.\nThe large-term goal of this work is to pose the general\nquestion What is a melody? under a statistical approach.\nThe answer would be given as sets of rules that are both\nhuman-readable and automatically learnt from score cor-\npora. This could be of interest for musicologists or helpful\nin applications such as melody matching, motif extraction,\nmelody extraction (ringtones), etc. In this paper the ﬁrst\nstages of this work are presented.\nc/circlecopyrt2007 Austrian Computer Society (OCG).2 METHODOLOGY\nThe steps performed to obtain sets of rules that character-\nize melody tracks can be outlined as follows:\nFeature extraction Describe a MIDI track by a set of\nlow-level statistical descriptors.\nDecision tree learning Learn a set of decision trees from\na tagged corpus of MIDI-tracks.\nRule extraction Extract rules from decision tree branches.\nRule simpliﬁcation Prune rule antecedents.\nRule selection Select best rules by ranking.\nThe details about the ﬁrst two steps can be found in [6].\nThe low-level statistical descriptors utilized to describ e\nmusical content are based on several categories of features\nthat assess melodic and rhythmic properties of a music\nsequence, such as pitch, note interval, and note duration\ndistribution descriptors, as well as track related propert ies\nsuch as polyphony rate, duration, etc. Most distribution\ndescriptors are also present in normalized form with re-\nspect to the value range for that descriptor in the whole\nsong. This allows to know the context in which a particu-\nlar descriptor value is computed.\nThe sets of rules that characterize melody tracks are\ninitially extracted from an ensemble of decision trees that\nhas shown good performance at this task [6]. The rules\nare simpliﬁed by pruning non useful antecedents and then,\nfor each rule set, rules are ranked according to different\nmetrics.\nNote that there is no feature selection stage, as the Ran-\ndom Forest [1] method used to learn decision tree ensem-\nbles performs its own feature selection process.\nExpressing a concept by rules has the advantage of be-\ning a human-readable description of the characterization\nprocess. The simpliﬁcation and ranking steps focus on\nobtaining a small, manageable rule system. A side goal\nhas been to test whether a small number of selected rules\ncan perform comparably to an ensemble of decision trees,\ntypically containing hundreds or thousands of nodes.\n2.1 Rule extraction from decision trees\nIn a previous work [6], Random Forest classiﬁers were\nused to learn an ensemble of decision trees capable of dis-\ncriminating melody tracks in a MIDI ﬁle.The datasets used to learn the trees in a Random For-\nest are collections of MIDI ﬁles where tracks are labeled\nwith a boolean tag indicating whether the track contains a\nmelody line or not. Thus, the leaves of the decision trees\nare also tagged with a boolean value, so there are positive\nand negative leaves. For each tree, a rule set is extracted.\nRules are extracted following positive branches (leading\nto a positive leaf). Negative branches are ignored. From\neach positive branch a rule is obtained:\nif(X1∧X2∧...∧Xn)then TrackIsMelody\nwhere Xiare the tests found in each tree node traversed\nfollowing a positive branch. As all rules have the same\nconsequent, we will drop it from herein.\n2.1.1 Track characterization by rule set ensemble\nA rule set is built as the disjunction of all the rules ex-\ntracted from the same tree. No particular rule ordering is\nimposed on a rule set. When such a rule set is applied to a\nsample, ﬁring at least one rule sufﬁces for that sample to\nbe tagged as a melody track. From a statistical viewpoint,\neach rule in the set stands for a certain type of melody.\nAn ensemble of Krule sets is obtained from the learnt\ndecision trees. As each rule set outputs a decision, the\nwhole ensemble is applied to a sample calculating the ratio\nbetween positive decisions and K. This value is taken as\nthe probability for a track to be a melody track.\n2.2 Rule set simpliﬁcation\nThe number of conditions in the antecedent part of a rule\ncan be too large to be easily read. Moreover, complex\nrules are often very speciﬁc, overﬁtting the training set.\nA rule can be generalized by dropping some of the condi-\ntions from its antecedent. This process is explained below.\nOn the other hand, the decision trees learnt by the Ran-\ndom Forest classiﬁer from large training sets are usually\nbig, leading also to huge rule sets1, so it is desirable to re-\nmove rules that do not characterize a lot of samples. Fur-\nthermore, a given sample in a dataset could ﬁre more than\none rule, thus making the presence of some rules in the set\nnot necessary. The method used here to reduce the size of\na rule set consists of ranking rules according to a measure\nbased on how many samples from a validation dataset ﬁre\nthem. The more samples (MIDI tracks) ﬁre a rule, the bet-\nter the rule. After the ranking is made, the best Nrules\nare selected from each rule set in order to classify new\nsamples, discarding the rest.\n2.3 Antecedent pruning\nThe method used here is to perform a test for consequent’s\nindependence from each condition Xifor each rule [5]. A\nχ2test with a 95% conﬁdence interval is performed con-\nsidering condition relevance as independent from other\nconditions in the same rule. This makes the test to be very\nconservative, dropping only conditions that do not satisfy\n1In our experiments, trees with more than 500 leaves (and thousa nds\nof nodes) have been obtained.the test’s hypothesis for all validation samples. A valida-\ntion dataset different from the initial training set is used to\ntest single rule conditions.\n2.4 Rule selection\nOnce rules are simpliﬁed using the method from the previ-\nous section, a rule ranking is established in order to select\nthe best rules in each rule set. The procedure used in this\nwork to rank the rules in a rule set Rusing a validation\ndataset Dis as follows:\n1. Sort rules in Rdecreasingly, according to the num-\nber of samples in the training set ﬁring each rule.\n2. For each rule riinR:2\n(a)ri\nscore=Score (ri,D)\n(b)D=D−ri\n⊕−ri\n/circleminus\n3. Sort Raccording to rscore , in decreasing order.\n4. Select the ﬁrst Nrules of Rand discard the rest.\nThere are several choices for the scoring function. In\nthis work, two functions have been tested:\n•Score 1(ri,D) =ri\n⊕\n•Score 2(ri,D) =ri\n⊕/(ri\n⊕+ri\n/circleminus)\nAdditionally, a variant of the ranking procedure that\ndoes not remove samples from D(step 2b) has been tested\nalong with the Score 1function. We denote this variant as\nScore 0.\nThe same validation dataset is used for all rule sets in\nthe rule system derived from the decision trees.\n3 EXPERIMENTS AND RESULTS\nFor the experiments presented here, rule systems have been\nderived from an ensemble of decision trees built using a\nRandom Forest classiﬁer with F= 5 features and K=\n10trees. Therefore rule systems consist of ten rule sets.\nThere is one rule system per training corpus.\n3.1 Datasets\nFour corpora have been used: SMALL ,LARGE ,RWC-\nGandRWC-P . The corpora SMALL andLARGE are de-\nscribed in [6]3. They contain songs of classical, jazz and\npopular music downloaded from a number of freely acces-\nsible Internet sites. These two corpora have been used to\ntrain and validate the system. The RWC corpora, used to\ntest the system, are MIDI ﬁles from the well-known RWC\nPopular (RWC-P) andGenre (RWC-G) Music Databases\n[3]. Tracks in these corpora (detailed in table 1) have been\nmanually tagged as melody or non-melody tracks.\n2ri\n⊕is the number of melody tracks in Dthat ﬁre rule riandri\n/circleminusis\nthe number of non-melody tracks in Dthat ﬁre rule ri.\n3Where SMALL is named as ALL200 andLARGE is the union of the\nCLA,JAZandKAR corpora in that paper.Corpus Tracks Melodies Songs Validation set\nSMALL 2775 554 600 LARGE\nLARGE 15168 2337 2513 SMALL\nRWC-G 311 44 48 –\nRWC-P 801 74 75 –\nTable 1 . Structure of the corpora\n3.2 Antecedent pruning results\nThe results of the antecedent pruning process are summa-\nrized in table 2.\nRule system SMALL LARGE\nUniq. cond. 779 (816) 1878 (2297)\n% pruning uniq. 4.5% 18.2%\nCond. 4370 (4692) 22481 (25581)\nAvg. cond./rule 8.3 (9) 10.8 (12.3)\n% pruning 6.9% 12.1%\nTable 2 . Rule antecedent pruning results. Numbers in\nparentheses indicate number of conditions before pruning.\nAs the ﬁgures in the table are not that impressive, recall\nfrom section 2.3 that the testing procedure is very con-\nservative, removing only conditions that are not relevant\nfor the whole validation dataset. Another fact that con-\ntribute to the small pruning percentage achieved is that,\nfor a given descriptor, there are a lot of similar but not\nequal conditions, as we are using two digit precision in\nﬂoating point numbers.\nFor the experiments that follow, the resulting pruned\nrule systems are used.\n3.3 Rule ranking results\nTable 3 summarizes rule coverage of the validation dataset\nwhen considering all rules in a rule set. Figures in the sec-\nond and third rows are average percentages of the valida-\ntion dataset covered by a rule set from the corresponding\nrule system. Rule coverage percentages are a rough esti-\nmation of a rule system accuracy. In this case, both rule\nsystems perform comparably on their respective valida-\ntion datasets, with more than 85% of the melody tracks\nﬁring at least one rule in a set. However, the coverage\nof non-melody tracks is somewhat high, especially for the\nLARGE rule system.\nRule system SMALL LARGE\nmelody tracks 86% 87%\nnon-melody tracks 17% 32%\nzero scoring 23% 67%\nTable 3 . Rule coverage summary.\nThe last row shows the percentage of rules in a rule\nset, in average, that scored zero, i.e. rules not ﬁred by any\nmelody track. Note that these percentages are the same for\nthe two scoring functions used. These rules are dropped\nfrom the rule sets, thus reducing their size in the LARGE\nrule system to one third. This means keeping about 70\nrules out of more than 200 for each rule set, in average.Figure 1 shows the average rule set coverage percent-\nages when selecting the Nbest rules from each rule set\nin a rule system. Note that when Nis small, the Score 1\nfunction gives a better coverage ratio for melodies. On\nthe other hand, the Score 2function gives better cover-\nage ratios for large N. In particular, the LARGE rule sys-\ntem achieves more than 75% melody track coverage in\naverage while maintaining the non-melody track coverage\nvery low (about 2%).\n 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 %\n 40  35  30  25  20  15  10  5  1Avg. rule set coverage\nN\n 0 % 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 %\n 40  35  30  25  20  15  10  5  1Avg. rule set coverage\nN\nFigure 1 .Average rule set coverage when the number of rules\nselected ( N) varies from 1 to 40, for both rule systems and score\nfunctions Score 1(top) and Score 2(bottom). Lines indicate\ncoverage for SMALL and (+) indicate coverage for LARGE . Plots\nover 20% are for melody tracks. Plots under 20% are for non-\nmelody tracks.\n3.4 Track selection procedure\nTwo experiments are presented. In the ﬁrst one, all tracks\nfrom test MIDI ﬁles are classiﬁed as melody or non-melody\ntrack. In the second one, a single track is selected from a\nMIDI ﬁle as its melody track. Therefore, given a ﬁle, all\nits non-empty tracks are classiﬁed and their probabilities\nof being a melody are obtained. Then the track with the\nhighest probability is selected as the melody track. If all\ntracks have zero probability, no melody track is selected.\nFor each experiment, several possibilities are explored\nand the rule system results are compared to those obtained\nby the Random Forest from which the rules are derived.3.5 Experiments\n3.5.1 Melody versus non-melody classiﬁcation\nThe RWC-G andRWC-P datasets have been used as test\nsets. The rule systems have three free parameters: the\nnumber of best rules Nto be selected from each rule set,\nthe minimum number θof rule sets that must agree to tag\na track as a melody, and the scoring function used to rank\nthe rules in the sets. Each combination of N,θand the\nscoring function results in a distinct rule system classiﬁe r.\nRecall that there are 10rule sets in a rule system. The\nrange of values used in these experiments is N∈1..20,\nθ∈1..10and three different scoring functions. A sum-\nmary of the best results obtained applying these classiﬁers\nto the RWC datasets is summarized in tables 4 and 5. The\nF-measure is used as the accuracy measure.\nSco Nθ%OK Prec Rec F\nSMALL 0 2 2 91.3 0.67 0.75 0.71\n(rules) 1 9 2 78.1 0.39 0.93 0.70\n2 17 4 89.4 0.60 0.80 0.68\nSMALL(RF) – – 6 90.4 0.65 0.68 0.67\nLARGE 0 17 3 87.5 0.54 0.75 0.63\n(rules) 1 14 3 88.7 0.57 0.84 0.68\n2 8 3 92.3 0.76 0.66 0.71\nLARGE(RF) – – 6 92.0 0.71 0.72 0.72\nTable 4 . Best results for the RWC-G dataset. (Sco = score;\nRF = Random Forest; reference results in italics)\nSco Nθ%OK Prec Rec F\nSMALL 0 1 3 98.1 0.89 0.91 0.90\n(rules) 1 5 6 98.1 0.90 0.89 0.90\n2 2 2 98.4 0.88 0.96 0.91\nSMALL(RF) – – 6 97.8 0.84 0.93 0.89\nLARGE 0 3 3 98.6 0.91 0.95 0.93\n(rules) 1 9 4 98.8 0.90 0.97 0.94\n2 5 2 98.8 0.90 0.97 0.94\nLARGE(RF) – – 6 98.6 0.90 0.97 0.94\nTable 5 . Best results for the RWC-P dataset.\n3.5.2 Per-song melody track selection\nNow, the goal is to know how many times the method se-\nlects as melody track a proper one among those in a ﬁle.\nFor each MIDI ﬁle, the classiﬁers output the track with\nthe highest probability of being a melody, except when all\nthese probabilities are zero, in which case the system says\nthat the ﬁle has no melody track.\nAn answer is considered as a success if:\n1. The ﬁle has at least one track tagged as melody and\nthe selected track is one of them.\n2. The ﬁle has no melody tracks and the answer is that\nthere is no melody track.\nTable 6 shows the best rule system results and those\nusing Random Forest classiﬁers. The results for the RWC-\nPdataset show us a hint that rule systems with a few tensof rules can perform comparably to ensemble of decision\ntrees with hundreds of leaves.\nRWC-G RWC-P\nSco N %OK Sco N %OK\nSMALL 1 3 70.5 0 2 97.3\nSMALL (RF) – – 75.0 – 6 94.7\nLARGE 1 5 68.2 0 6 97.3\nLARGE (RF) – – 72.9 – 6 96.0\nTable 6 . Best melody selection results for RWC-G (left)\nandRWC-P (right).\n4 CONCLUSIONS AND FUTURE WORK\nIn this work a method to obtain reduced rule systems from\npreviously learnt random forests that characterize melody\ntracks has been exposed. Such rule systems perform com-\nparably to the original decision tree ensembles.\nTo be able to give a more human-readable description a\nfuzzyﬁcation process is required on the sets of rules. Cur-\nrently we are working on this process.\nThe study of these rules can lead also to the description\nof other track categories, such as solo orchorus tracks.\n5 ACKNOWLEDGMENTS\nSupported by the projects: GV06/166 and CICyT TIN2006–\n14932–C02, partially supported by EU ERDF. The au-\nthors want to thank Andr ´es P´erez for his tagging help.\n6 REFERENCES\n[1] L. Breiman, “Random forests,” Machine Learning ,\nvol. 45, no. 1, pp. 5–32, October 2001.\n[2] A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith,\n“Query by humming: Musical information retrieval in\nan audio database,” in Proc. of 3rd ACM Int. Conf.\nMultimedia , 1995, pp. 231–236.\n[3] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“Rwc music database: Music genre database and mu-\nsical instrument sound database.” in ISMIR , 2003.\n[4] S. T. Madsen and G. Widmer, “Towards a computa-\ntional model of melody identiﬁcation in polyphonic\nmusic.” in IJCAI , 2007, pp. 459–464.\n[5] J. R. Quinlan, “Simplifying decision trees.” Int. J.\nHum.-Comput. Stud. ,51(2), pp. 497–510, 1999.\n[6] D. Rizo, P. J. Ponce de Le ´on, C. P ´erez-Sancho, A. Per-\ntusa, and J. M. I ˜nesta, “A pattern recognition approach\nfor melody track selection in midi ﬁles,” in Proc. of IS-\nMIR 2006 , Victoria, Canada, 2006, pp. 61–66.\n[7] M. Tang, C. L. Yip, and B. Kao, “Selection of melody\nlines for music databases.” in Proceedings of An-\nnual Int. Computer Software and Applications Conf.\nCOMPSAC , 2000, pp. 243–248."
    },
    {
        "title": "Automatic Instrument Recognition in a Polyphonic Mixture Using Sparse Representations.",
        "author": [
            "Pierre Leveau",
            "David Sodoyer",
            "Laurent Daudet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414984",
        "url": "https://doi.org/10.5281/zenodo.1414984",
        "ee": "https://zenodo.org/records/1414984/files/LeveauSD07.pdf",
        "abstract": "In this paper, we introduce a method to address automatic instrument recognition in polyphonic music. It is based on the decomposition of the music signal with instrumentspecific harmonic atoms, yielding an approximate object representation of the signal. A post-processing is then applied to exhibit ensemble saliences that give clues about the number of instruments and their labels. The whole algorithm is then applied on artificial mixes of solo performances. The identification of the number of instrument reaches 73 % on 10-s segments and the fully blind problem of identification of the ensemble label without prior knowledge on the number of instruments is 17 %. 1 INTRODUCTION Orchestration is a critical information for the automatic indexing of music. It gives an important clue about the music genres, and is often necessary for the query of sound samples for electronic music composing. Automatic Instrument Recognition has raised some interest over the latest years (see [1] for an overview). Early studies have addressed the recognition of isolated music notes, then of solos phrases. For these two contexts, machines now reach the performance of expert musicians. However, mono-instrument music is only a small part of the overall recorded music, that involves natural or artificial mixes of instruments. To deal with multi-instrument music, several strategies have been adopted. Template-based approaches have first been proposed [2, 3]. Other approaches adapt “bag-offrames” approaches to polyphony [4]. Other techniques consist in estimating jointly the instrument sources activated in a probabilistic framework [5], at a heavy computational cost. A recent work [6] presents a representation showing the instrument presence probabilities in the timepitch plane without note detection. Ensemble classes can also be modeled using standard feature-based representations in addition with a hierarchical taxonomy [7], when the number of instrument combinations is tractable. c⃝2007 Austrian Computer Society (OCG). In this paper a recent development in the decomposition of music signals is studied for the recognition of music instrument in ensemble music. It relies on principles coming from the sparse approximations domain. To get a useful sparse representation of a signal, two aspects have to be investigated: the building of a signal model (dictionary design), and, given a dictionary, the choice of an algorithm and its optimization towards a faster or better approximation. Techniques from the sparse approximation domain have already been used for automatic music transcription in an unsupervised way [8]: the building of the dictionary was done in an data-driven way, prohibiting the signal analysis in view of prior knowledge of the sources. The introduction of prior knowledge about the sources in dictionaries has been presented in [9]: this knowledge is put in the amplitudes of the note partials. In section 2, the decomposition algorithm is briefly described, then a post-processing is introduced to take a decision on the orchestration. The experiments of ensemble recognition are detailed in section 3. 2 ALGORITHM",
        "zenodo_id": 1414984,
        "dblp_key": "conf/ismir/LeveauSD07",
        "keywords": [
            "Automatic Instrument Recognition",
            "Polyphonic music",
            "Instrument-specific harmonic atoms",
            "Approximate object representation",
            "Post-processing",
            "Ensemble saliences",
            "Number of instruments",
            "Ensemble label identification",
            "Multi-instrument music",
            "Sparse approximations domain"
        ],
        "content": "AUTOMATIC INSTRUMENT RECOGNITION IN A POLYPHONIC\nMIXTURE USING SPARSE REPRESENTATIONS\nPierre Leveau1,2\n1GET-ENST (T ´el´ecom Paris)\n46, rue Dareau\n75014 ParisDavid Sodoyer2, Laurent Daudet2\n2University Pierre et Marie Curie\nInstitut Jean Le Rond D’Alembert, LAM team\n11, rue de Lourmel\n75015 Paris\nABSTRACT\nIn this paper, we introduce a method to address automatic\ninstrument recognition in polyphonic music. It is based\non the decomposition of the music signal with instrument-\nspeciﬁc harmonic atoms, yielding an approximate object\nrepresentation of the signal. A post-processing is then ap-\nplied to exhibit ensemble saliences that give clues about\nthe number of instruments and their labels. The whole al-\ngorithm is then applied on artiﬁcial mixes of solo perfor-\nmances. The identiﬁcation of the number of instrument\nreaches 73 % on 10-s segments and the fully blind prob-\nlem of identiﬁcation of the ensemble label without prior\nknowledge on the number of instruments is 17 %.\n1 INTRODUCTION\nOrchestration is a critical information for the automatic in-\ndexing of music. It gives an important clue about the mu-\nsic genres, and is often necessary for the query of sound\nsamples for electronic music composing.\nAutomatic Instrument Recognition has raised some in-\nterest over the latest years (see [1] for an overview). Early\nstudies have addressed the recognition of isolated music\nnotes, then of solos phrases. For these two contexts, ma-\nchines now reach the performance of expert musicians.\nHowever, mono-instrument music is only a small part of\nthe overall recorded music, that involves natural or artiﬁ-\ncial mixes of instruments.\nTo deal with multi-instrument music, several strategies\nhave been adopted. Template-based approaches have ﬁrst\nbeen proposed [2, 3]. Other approaches adapt “bag-of-\nframes” approaches to polyphony [4]. Other techniques\nconsist in estimating jointly the instrument sources acti-\nvated in a probabilistic framework [5], at a heavy compu-\ntational cost. A recent work [6] presents a representation\nshowing the instrument presence probabilities in the time-\npitch plane without note detection. Ensemble classes can\nalso be modeled using standard feature-based representa-\ntions in addition with a hierarchical taxonomy [7], when\nthe number of instrument combinations is tractable.\nc/circlecopyrt2007 Austrian Computer Society (OCG).In this paper a recent development in the decomposi-\ntion of music signals is studied for the recognition of mu-\nsic instrument in ensemble music. It relies on principles\ncoming from the sparse approximations domain. To get a\nuseful sparse representation of a signal, two aspects have\nto be investigated: the building of a signal model ( dictio-\nnary design ), and, given a dictionary, the choice of an al-\ngorithm and its optimization towards a faster or better ap-\nproximation. Techniques from the sparse approximation\ndomain have already been used for automatic music tran-\nscription in an unsupervised way [8]: the building of the\ndictionary was done in an data-driven way, prohibiting the\nsignal analysis in view of prior knowledge of the sources.\nThe introduction of prior knowledge about the sources in\ndictionaries has been presented in [9]: this knowledge is\nput in the amplitudes of the note partials. In section 2,\nthe decomposition algorithm is brieﬂy described, then a\npost-processing is introduced to take a decision on the or-\nchestration. The experiments of ensemble recognition are\ndetailed in section 3.\n2 ALGORITHM\n2.1 Decomposition algorithm\nThe signal model and decomposition algorithm have been\nintroduced in [9]. For space constraints, only the main\nfeatures of the algorithm are highlighted here.\n2.1.1 Signal Model\nThe signal is decomposed as a linear combination of short\npieces of signal h, called harmonic atoms :\nx(t) =NX\nn=1αnhsn,un,f0n,An,Φn(t). (1)\nThe set of all the atoms available to decompose the sig-\nnal is called a dictionary .\nThe parameters of these atoms are the scale sn, the time\nlocalization un, the fundamental frequency f0n, the partial\namplitudes An={am,n}m=1:Mand the partial phases\nΦn={φm,n}m=1:M. An atom his itself deﬁned as alinear combination of partial atoms:\nhs,u,f 0,A,Φ(t) =MX\nm=1amejφmgs,u,m.f 0(t) (2)\nwhere the amplitudes of the Mpartials are constrained toPM\nm=1a2\nm= 1 and the signal gcorresponding to each\npartial is given by a Gabor atom:\ngs,u,f=wµt−u\ns¶\ne2jπft(3)\nwithwa time and frequency localized window.\nIn our study, each Avector is linked to an instrument\nandia pitch p(integer Midi Code), and is learned from\ndatabases of isolated instrument notes or solo performan-\nces, as shown in section 2.1.3.\n2.1.2 Decomposition algorithm\nThe Matching Pursuit algorithm [10] is then performed to\ndecompose the signal with this dictionary. In a nutshell,\nit consists in selecting the atom the most correlated with\nthe signal, to subtract it from the signal, and to iterate on\nthe residual. After several iterations, a decomposition of\ntype (1) is obtained. The set of the selected atoms with\ntheir respective weights is called a Book. With the param-\neters mentioned in Section 3, the runtime takes about 10\ntimes real-time on a 3 GHz monoprocessor, with a Matlab\nimplementation.\n2.1.3 Learning\nThe atoms are ﬁrst learned on a set of 3 different databases\nof isolated notes [11, 12, 13], annotated in pitch pand in-\nstrument i. For a given time frame of size s, the technique\nconsists in selecting the harmonic comb that best matches\nthe signal, and then in picking the amplitudes of the par-\ntials on this comb. For each instrument and note, the am-\nplitude vectors sets are then quantized using a K-means\nalgorithm.\nTo build dictionaries that are closer to realistic play-\ning conditions, solo performances are then analyzed. In\nthis case, the notes are not localized nor annotated. The\nMatching Pursuit algorithm with the aforementioned dic-\ntionary is thus used because of its capability to automat-\nically adapt to the music notes. The dictionary that we\nuse for this task is built only with the atoms of the corre-\nsponding known instrument, whose construction has been\ndescribed in the previous paragraph. However, the al-\ngorithm is modiﬁed before the subtraction step: the har-\nmonic comb whose fundamental frequency corresponds to\nthe selected atom is selected to perform the partial ampli-\ntudes picking. Moreover, to prevent the selection of har-\nmonic atoms in the residual, the atom is not subtracted:\nthe signal is set to 0 at the extracted atom localization. A\nquantization step is then performed, as for isolated notes\ndictionaries.2.2 Pitch-and-instrument salience\nAn atom extraction can be seen as a “pitch-and-instrument”\nsalience extractor, since it correlates both a spectral en-\nvelope and a harmonic comb with the signal. Given an\nextracted atom at fundamental frequency f0, scale sand\nlocalization u, we deﬁne the f0-and-instrument salience\nfor instrument i as1:\nSi= max\nA∈Ci,p{|/angbracketleftx, hs,u,f 0,A,Φn/angbracketright|} (4)\nIf an instrument ienveloppe cannot play the pitch p, i.e.\nCi,p=∅, its salience is set to 0. Although not required for\nthe decomposition, all instrument saliences for every se-\nlected atom are kept for the scoring step: they are needed\nfor the ensemble saliences evaluations.\n2.3 From Instrument Salience to Ensemble Salience\nThe scoring algorithm processes the output of the decom-\npositions to have an indication of which instruments are\nplaying. Here, a frame-based scoring is developed: for a\ngiven time frame, the score of a given ensemble class de-\npends on which atoms have been extracted and on their\nf0-and-instrument salience.\n44.5 55.5 66.5 77.5 88.5 94050607080 (a)\nTime [s] Pitch [midi]Bo\nCl\nCo\nFl\nOb\nVa\nVl\nTime [s]Ensemble label(b)\n44.5 55.5 66.5 77.5 88.5 9Bo&Fl&Va&VaBo&Fl&Va&VlBo&Fl&VlBo&Fl&Vl&VlBo&ObBo&Ob&ObBo&Ob&Ob&ObBo&Ob&Ob&VaBo&Ob&Ob&VlBo&Ob&VaBo&Ob&Va&VaBo&Ob&Va&VlBo&Ob&VlBo&Ob&Vl&VlBo&VaBo&Va&Va\nFigure 1 . Bassoon (Bo) and Oboe (Ob) duo (synthetic\nmix): (a) Book representation in the Time-Pitch plane:\natoms are represented by rectangles, whose width is the\natom scale and height is their amplitudes, (b) Ensemble\nSaliences for a subset of ensemble labels (high saliences\nare darker).\nGiven the decomposition of a music signal, there can\nbe several atoms per time frame since the music is in gen-\neral polyphonic. The ﬁrst step to perform is to select\nwhich atoms are present for each time frame, the timeline\nbeing sampled at the greatest common divider between the\n∆ucorresponding to each scale. Then, the contribution of\neach atom aon a given time sample is equal to the value\n1Note that the inner product is not depending on the values of Φiff0\nis high enough since the partials atoms can be considered as orthogonal:\n|/angbracketleftx, hs,u,f 0,A,Φn/angbracketright|2=PM\nm=1|/angbracketleftx, gs,u,m.f 0/angbracketright|2at instant uof the weighting window starting at uamul-\ntiplied by the atom weight. Hence, given a time frame u\nand an ensemble label e, its ensemble salience is the fol-\nlowing2:\nSe(u) =max Ce∈CeP\na∈CeSia(u)w(u−ua\nsa)\nNβ\ne(5)\nwhere Ceis the set of all the instrument salience combina-\ntions whose time support overlap with u. For example, if\ntwo atoms are present at time u, the salience of ensemble\nCo&Fl (Cello and Flute) is the maximum between the sum\nof the Flsalience for the ﬁrst atom and the Cosalience for\nthe second one, and sum of the Cosalience for the ﬁrst\natom and the Flsalience for the second one, divided by\n2β. An example of book output and corresponding en-\nsemble saliences is displayed on Figure 1.\nTheβparameter is a sparsity parameter: it balances\nthe weight between the sum of all atom saliences and the\nnumber of instrument taken to explain the resulting sig-\nnal. It can be optimized with respect to the number of\ninstrument detection.\n2.4 Voting\nDecisions taken on single time frames does not provide\nuseful information as such. However, one can be inter-\nested on decisions taken on the whole music signal, or a\nsegment of it. To get a global decision from local ones,\nvoting techniques must be employed. The technique used\nin this study is derived from a probabilistic framework.\nOther techniques, like majority-vote, have been tried but\nthey yield to weaker results. First, the ensemble saliences\nare mapped to ensemble Pseudo Log-Likelihoods (PLL),\nthen a segment PLL for each ensemble label is computed\nby adding the PLL of each time frames. The mapping\nof a ensemble salience Se(u)to PLL Le(u)is achieved\nwith the following formula: Le(u) = (Se(u))γ, where γ\nweighs the inﬂuence of salience amplitudes over the over-\nall score in the segment. Like βin previous Section, the γ\ncoefﬁcient has to be optimized on a development set. The\ndecision over the all segment is obtained by summing all\nthe PLL. It corresponds to an hypothesis of statistical in-\ndependence between each time frame. This hypothesis is\nclearly erroneous in music signals (the orchestration does\nnot change at every short time frame), but is commonly\ntaken for fusion of local likelihood.\n3 EXPERIMENTS\n3.1 Parameters\nThe parameters used for the decomposition are s= 46ms,\n∆ = 23 ms.f0is sampled logarithmically with a step of\n2Using the L2normqP\na∈Ce(Sia(u)w(u−ua\nsa))2instead of the\nL1normP\na∈CeSia(u)w(u−ua\nsa)would be more consistent with the\noptimality criterion of the decomposition, however it leads to weaker\nresults in the studied applications\n0.10.20.30.40.50.60.70.80.90.20.30.40.50.60.70.80.9\nβPercentage of good Ni detections\n  \nper observation\n2 seconds\n10 seconds\nrandom guessFigure 2 . Accuracy of Number of Instrument Detection\nas a function of βfor decision on single times frames, 2\nseconds segments and 10 seconds segments.\n1/10tone. The decompositions are performed until the\nSignal-To-Residual ratio reaches 20 dB.\nThe development set and the test set are composed of\nartiﬁcial mixes of solo phrases extracted from commer-\ncial CDs, from sources different from the one used for\natom learning. The mixes are done by summing the mono-\ninstrument signals of instruments bassoon (Bo), cello (Co),\nclarinet (Cl), ﬂute (Fl), oboe (Ob), viola (Va) and violin\n(Vl) after an energy normalization. For each set, 100 10s\nsamples have been made, 25 for each ensemble cardinal.\n3.2 Optimization of parameters\nThe parameters βandγhave to be tuned to maximize\nthe accuracy of the estimation of the number of instru-\nments, which is required to estimated the good instrument\nlabel. Optimizing these parameters for instrument label\naccuracy would overﬁt the algorithm for the solo recog-\nnition, that is the easiest problem. In our experiments on\nthe development set, the best γparameter has shown to be\nindependant on the decision window: the value γ= 0.8\ngives the best results.\n3.3 Ensemble recognition\nFor these values, the instrument recognition rates for deci-\nsions on 10 s segments are depicted on Figure 3. It shows\nthat the problem of ﬁnding an instrument among the mix\nis correctly addressed when the number of instrument is\nknown (from 70 % to 100 %, depending on the ensemble\ntype), and a less accurately when it is not known (from 54\n% to 84 %). However, as the required number of instru-\nments increases, the method fails at correctly identifying\nthem alltogether. Dealing with ensembles of more than\nthree instruments needs more reﬁned techniques both at\ndecomposition step and post-processing or more prior in-\nformation, since the problem is signiﬁcantly more difﬁcult\n(results for random draw is at less than 1 %).Ens. solo   Ens. duo    Ens. trio   Ens. quartet00.10.20.30.40.50.60.70.80.91\n  1 instrument\n2 instruments\n3 instruments\n4 instrumentsFigure 3 . Ensemble recognition results for each subset\n(solos, duos, trios, quartets). For each ensemble, the three\ngroups of bars depict respectively the results of a random\ndraw, the results of our algorithm with no knowledge on\nthe number of instruments playing, and the results know-\ning the number of instruments playing.\n4 CONCLUSION\nIn this paper, we have developed a novel approach to the\nhighly complex problem of identifying the instruments\nplaying in ensemble music. The approach consists in get-\nting a knowledge-assisted mid-level representation of the\nsignal, then in performing a post-processing using ensem-\nble saliences based on individual instrument saliences de-\nrived from the representation. The results are encouraging\nfor the estimation of the number of instrument, but weak\nfor the ensemble classiﬁcation, which is a much more dif-\nﬁcult problem without prior information on ensemble la-\nbels occurences.\nFuture work will be dedicated to the improvement of\nthe decomposition step by ﬁrst reﬁning atom parameters,\nin order to better ﬁt the underlying signal structures, and\nthen by grouping atoms into molecules to catch temporal\ndependencies. The joint estimation of atom combinations\nwill also be investigated using more elaborated sparse de-\ncomposition algorithms. Finally, the post-processing will\nbe improved by using melodic line tracking techniques, to\ndisambiguate highly polyphonic mixes.\n5 ACKNOWLEDGEMENTS\nL. Daudet is partially supported by French ANR under\ncontract ANR-06- JCJC-0027-01- DESAM.\n6 REFERENCES\n[1] P. Herrera, A. Klapuri, and M. Davy. Signal Process-\ning Methods for Music Transcription , chapter 6 - Au-\ntomatic Classiﬁcation of Pitched Musical Instrument\nSounds. Springer, 2006.[2] K. Kashino and H. Murase. A sound source identiﬁ-\ncation system for ensemble music based on template\nadaptation and music stream extraction. Speech Com-\nmunication , 27:337–349, March 1999.\n[3] T. Kinoshita, S. Sakai, and H. Tanaka. Musical sound\nsource identiﬁcation based on frequency component\nadaptation. In Proc. IJCAI Worshop on Computational\nAuditory Scene Analysis , pages 18–24, 1999.\n[4] J. Eggink and G. J. Brown. Instrument recognition in\naccompanied sonatas and concertos. In Proc. of IEEE\nInt. Conf. on Audio, Speech and Signal Processing\n(ICASSP) , 2004.\n[5] E. Vincent. Musical source separation using time-\nfrequency source priors. IEEE Trans. on Audio,\nSpeech and Language Processing , 14(1):91–98, Jan-\nuary 2006.\n[6] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and\nG. Okuno. Instrogram: A new musical instrument\nrecognition technique without using onset detection\nnor f0 estimation. In Proc. of IEEE Int. Conf. on\nAudio, Speech and Signal Processing (ICASSP) , vol-\nume 5, pages 229–232, 2006.\n[7] S. Essid, G. Richard, and B. David. Instrument recog-\nnition in polyphonic music based on automatic tax-\nonomies. IEEE Trans. on Audio, Speech and Lan-\nguage Processing , 14(1):68–80, January 2006.\n[8] P. Smaragdis and J.C. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In\nProc. of IEEE Int. Workshop on Applications of Signal\nProcessing to Audio and Acoustics (WASPAA) , pages\n177–180, 2003.\n[9] P. Leveau, E. Vincent, G. Richard, and L. Daudet.\nMid-level sparse representations for timbre identiﬁca-\ntion: design of an instrument-speciﬁc harmonic dic-\ntionary. In 1st Workshop on Learning the Semantics of\nAudio Signals , dec 2006.\n[10] S. Mallat and Z. Zhang. Matching pursuits with time-\nfrequency dictionaries. IEEE Trans. on Signal Pro-\ncessing , 41(12):3397–3415, December 1993.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Musical Instrument Sound Database. Dis-\ntributed online at http://staff.aist.go.jp/m.goto/RWC-\nMDB/.\n[12] Iowa database, http://theremin.music.uiowa.edu/mis.html.\n[13] Studio online database,\nhttp://forumnet.ircam.fr/402.html?l=1."
    },
    {
        "title": "A Semantic Space for Music Derived from Social Tags.",
        "author": [
            "Mark Levy",
            "Mark B. Sandler"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415628",
        "url": "https://doi.org/10.5281/zenodo.1415628",
        "ee": "https://zenodo.org/records/1415628/files/LevyS07.pdf",
        "abstract": "In this paper we investigate social tags as a novel highvolume source of semantic metadata for music, using techniques from the fields of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags define a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivatedtwo-dimensional subspace representing musical emotion. 1 INTRODUCTION Social tags are of interest as a potential high-volumesource of descriptive metadata for music. Such metadata can either be used directly to drive search applications, as already happens to some extent in the commercial domain, or as a source of groundtruth to train audio content-based classification and search engines. In the academic literature, comparable text metadata for music has previously been found by mining web-pages such as blogs and music reviews [2, 19, 9]. Although some interesting preliminary results have been reported, two significant problems are associated with this approach. Firstly, text retrieved from the web is often noisy, i.e. it unavoidably contains a great deal of irrelevant content. Secondly, for computational reasons, and because the noise problem becomes insuperable, text has to be mined on a per-artist rather than per-track basis: as a result it offers only low-quality groundtruth for learning the characteristics of audio content. Social tags as applied to individual tracks appear to offer a solution to both of these issues. At the time of writing there is no previous relevant academic literature on social tags for music. The tags discussed here were all applied to individual tracks. They were aggregated from the last.fm 1 and MyStrands 2 web services during January and February 2007. 1 http://ws.audioscrobbler.com 2 https://www.musicstrands.com c⃝2007 Austrian Computer Society (OCG). Music in general has no semantics, in the strict sense of representing or being ‘about’ something, and, perhaps as a result, tags for music are often discursive. Of some 45,000 distinct tags in our dataset, over a third of consist of three or more words, while over 10% contain 5 or more words: these are frequently complete phrases. In our experiments we therefore treat tags as regular text, tokenizing them with a standard stop-list (to remove common words such as ‘it’, ‘and’, ‘the’, etc.). We then create a conventional document-term matrix, tabulating the number of occurrences of each word in tags applied to each track. We do not use a stemmer, because of the idiosyncratic vocabulary of social tagging and the large number of words used as proper nouns (particularly artist names). Working with words rather than tags nonetheless goes some way towards capturing the common meaning of alternate forms such as ‘female vocalist’, ‘female vocals’, ‘good female vocals’, ‘sexy female vocals’, ‘lovely female vocals’, etc. The language of tags for music is ad hoc and often highly informal, as shown by the following few tags selected at random: ‘all my hope is gone’, ‘oregon trips’, ‘my favourite muse songs’, ‘french-canadian’,‘Tool Mix’, ‘comp1’, ‘ragga rhythm’, ‘Dave Brubeck Quartet’, ‘american wedding’, ‘fora do mundo’, ‘space trucking’, ‘right in two’, ‘desert island songs songs which keep me alive or otherwise enraged’, ‘heard on 96wave’, ‘put on mikey cds’. We might indeed question whether the collaborative tagging model can be applied successfully to music: beyond standard metadata like artist or title, which are already likely to be known in any real application, it may be far from obvious which tags are appropriate for any particular track. This study provides evidence, however, that despite the vagaries of individual tags, patterns of co-occurrences of words in tags can reveal terms or combinations of terms which are significantly grounded in the music they describe (rather than expressing arbitrary personal reactions) and generalisable across tracks. In particular we show that tags define a vector space with highly attractive properties for music retrieval, and which appears to have genuine semantics. Table 1. Top terms describing Portishead Tags Web-mined text trip-hop cynical electronic produced portishead smooth female vocalists dark downtempo particular alternative loud mellow amazing chillout vocal sad unique 90s simple 2 THE SEMANTIC SPACE OF TAGS",
        "zenodo_id": 1415628,
        "dblp_key": "conf/ismir/LevyS07",
        "keywords": [
            "social tags",
            "semantic metadata",
            "music",
            "information retrieval",
            "multivariate data analysis",
            "browse-by-mood",
            "psychologically-motivated",
            "Correspondence Analysis",
            "track level",
            "artist"
        ],
        "content": "ASEMANTICSPACE FOR MUSICDERIVEDFROM SOCIAL TAGS\nMark Levy\nCentreforDigitalMusic\nQueen Mary,UniversityofLondon\nMileEndRoad, LondonE1 4NS\nmark.levy@elec.qmul.ac.ukMark Sandler\nCentre forDigitalMusic\nQueen Mary,UniversityofLondon\nMileEnd Road, LondonE14NS\nmark.sandler@elec.qmul.ac.uk\nABSTRACT\nIn this paper we investigate social tags as a novel high-\nvolumesourceofsemanticmetadataformusic,usingtech-\nniques from the ﬁelds of information retrieval and multi-\nvariatedataanalysis. Weshowthat,despitetheadhocand\ninformallanguageoftagging,tagsdeﬁnealow-dimensional\nsemanticspacethatisextremelywell-behavedatthetrack\nlevel, in particular being highly organised by artist and\nmusical genre. We introduce the use of Correspondence\nAnalysistovisualisethissemanticspace,andshowhowit\ncanbeappliedto createabrowse-by-moodinterfacefora\npsychologically-motivatedtwo-dimensionalsubspacerep -\nresentingmusicalemotion.\n1 INTRODUCTION\nSocialtagsareofinterestasapotentialhigh-volumesourc e\nof descriptive metadata for music. Such metadata can ei-\nther be used directly to drive search applications, as al-\nready happensto some extent in the commercial domain,\nor as a source of groundtruthto train audiocontent-based\nclassiﬁcation and search engines. In the academic litera-\nture, comparable text metadata for music has previously\nbeen found by mining web-pages such as blogs and mu-\nsic reviews[2, 19, 9]. Althoughsome interestingprelimi-\nnary results have been reported, two signiﬁcant problems\nare associated with this approach. Firstly, text retrieved\nfrom the web is often noisy, i.e. it unavoidably contains\na great deal of irrelevant content. Secondly,for computa-\ntional reasons, and because the noise problem becomes\ninsuperable, text has to be mined on a per-artist rather\nthan per-track basis: as a result it offers only low-quality\ngroundtruth for learning the characteristics of audio con-\ntent. Social tags as applied to individual tracks appear to\noffer a solution to both of these issues. At the time of\nwriting there is no previous relevant academic literature\nonsocialtagsformusic.\nThe tags discussed here were all applied to individual\ntracks. Theywereaggregatedfromthelast.fm1andMyS-\ntrands2webservicesduringJanuaryandFebruary2007.\n1http://ws.audioscrobbler.com\n2https://www.musicstrands.com\nc/circlecopyrt2007AustrianComputerSociety(OCG).Music in general has no semantics, in the strict sense\nof representing or being ‘about’ something, and, perhaps\nas a result, tags for music are often discursive. Of some\n45,000 distinct tags in our dataset, over a third of con-\nsist of three or more words, while over 10% contain 5 or\nmorewords: thesearefrequentlycompletephrases. Inour\nexperiments we therefore treat tags as regular text, tok-\nenizingthemwithastandardstop-list(toremovecommon\nwordssuchas‘it’,‘and’,‘the’,etc.). Wethencreateacon-\nventionaldocument-termmatrix,tabulatingthenumberof\noccurrencesofeachwordintagsappliedtoeachtrack. We\ndonotusea stemmer,becauseoftheidiosyncraticvocab-\nularyofsocialtaggingandthelargenumberofwordsused\nas propernouns(particularlyartist names). Workingwith\nwordsratherthantagsnonethelessgoessomewaytowards\ncapturingthecommonmeaningofalternateformssuchas\n‘female vocalist’, ‘female vocals’, ‘good female vocals’,\n‘sexyfemalevocals’,‘lovelyfemalevocals’,etc.\nThe language of tags for music is ad hoc and often\nhighly informal, as shown by the following few tags se-\nlected at random: ‘all my hope is gone’, ‘oregon trips’,\n‘myfavouritemusesongs’,‘french-canadian’,‘ToolMix’,\n‘comp1’,‘raggarhythm’,‘DaveBrubeckQuartet’,‘amer-\nican wedding’, ‘fora do mundo’, ‘space trucking’, ‘right\nin two’, ‘desert island songs - songs which keep me alive\nor otherwise enraged’, ‘heard on 96wave’, ‘put on mikey\ncds’. We might indeed question whether the collabora-\ntive tagging model can be applied successfully to music:\nbeyondstandardmetadatalike artist ortitle, whichareal-\nreadylikelytobeknowninanyrealapplication,itmaybe\nfar from obvious which tags are appropriate for any par-\nticulartrack.\nThisstudyprovidesevidence,however,thatdespitethe\nvagaries of individual tags, patterns of co-occurrencesof\nwords in tags can reveal terms or combinations of terms\nwhich are signiﬁcantly grounded in the music they de-\nscribe(ratherthanexpressingarbitrarypersonalreactio ns)\nandgeneralisableacrosstracks. Inparticularweshowthat\ntagsdeﬁneavectorspacewithhighlyattractiveproperties\nformusicretrieval,andwhichappearstohavegenuinese-\nmantics.Table 1. ToptermsdescribingPortishead\nTags Web-mined text\ntrip-hop cynical\nelectronic produced\nportishead smooth\nfemalevocalists dark\ndowntempo particular\nalternative loud\nmellow amazing\nchillout vocal\nsad unique\n90s simple\n2 THESEMANTIC SPACEOFTAGS\n2.1 Tagsvsweb-mined text\nTheonlycomparablesourceofhigh-volumemetadatafor\nmusic explored to date is web-mined text. This is typ-\nically retrieved by searching for pages that appear to be\nrelevanttoaparticularartist,andthenattemptingtoreta in\nonlytermsthat relateto theirmusic[2,19]. Theresulting\ntext is inherently noisy on two levels. Firstly, the pages\nretrieved by any automated system are not guaranteed to\nbe relevant (in particular when an artist’s name has other\nmeanings), and come from a variety of kinds of source,\neach with its own characteristic vocabulary. Secondly, in\ngeneral only a small unknownpart of the content of each\npage will refer directly to music of interest. One conse-\nquence of the inevitable inclusion of irrelevant terms is\nthat the vocabulary size explodes. A typical web crawl\nreported in [9] found over 200,000 terms for a set of 200\nwell-knownartists. Incontrast,wefoundlessthan13,500\ndistinct tags for tracks by the same set of artists. Such a\ncomparison is necessarily informal, because of the difﬁ-\nculty of comparingthe sizes of the input data sources(50\nweb pages vs tags from the order of 100 different users\nfor each artist). More importantly, however, web-mining\nappears to be impractical as a source of metadata at the\ntracklevel,astheproblemsofnoisemultiplystill further .\nThe vocabulary of tags is different from web-mined\ntextnotonlyin size, butalso in character,as illustratedi n\nTable1,whichcomparesthetenmostwidelyappliedtags\nfor the group Portishead with the top web-mined adjec-\ntivesgivenin[18]. Weobservethat,incontrasttothetags,\nas many as half of the web-mined adjectives (‘cynical’,\n‘produced’,‘particular’,‘amazing’,‘unique’)areveryu n-\nlikelytobegroundedinthemusicofthisparticulargroup.\n2.2 Catalogueorganisation\nA natural question to ask of any new representation for\ncollections of music is to what extent it respects a tradi-\ntional recording catalogue organisation, in which tracks\nare groupedby artist and genre. A gooddeal of work has\nbeen devoted to addressing this issue in relation to low-\nlevelaudiofeatures,withtheproblemcast,perhapssome-whatunhelpfully,asapairofclassiﬁcationtasks(see[14]\nfor a recent review). The conclusion, after several years\nof research, is that currentlow-level feature sets lead to a\nrepresentationthat is only weaklystructuredby artist and\ngenre[1,12].\nWhile individual genre tags attached to tracks are not\nreliable in general, we can reasonably ask whether the\nsemantic space deﬁned by co-occurrences of terms does\ncapture concepts of artist and genre, and, if so, what di-\nmensionalityisrequiredtorepresenttheseconceptseffec -\ntivelyina vectorspace.\n2.3 Retrievalexperiments\nThe results reported here are based on 236,974 tags col-\nlected for 5,722 tracks drawn from all of the mainstream\ngenres. The total vocabulary size, after tokenizing with\na standard stop-list, was 24,160 distinct words, of which\n13,312wereappliedto2ormoretracks,and3,992to10or\nmore. Thechoiceoftrackswasseededwithasetofartists\nbalancedacrossthemainstreammusicalgenres,andanin-\nﬂuential list of words associated with musical expression\nfrom [8], expanded with synonyms from their WordNet\nsynsets [6]. The scale of the dataset was chosen to give\nreasonable coverage across tracks and terms without be-\ncomingcomputationallyintractable.\nTo investigatethe organisationof the tag space, and to\ngive a quantitative comparison with web-mined text for\nretrieval applications, we replicated the experimental se t-\nup used in [9], in which similarities were calculated for\na set of 224 well-known artists split equally over 14 gen-\nres. From our dataset, we found tags for 1196 tracks by\n223 of the 224 artists, with between 4 and 12 tracks for\neach artist. We measured retrieval performance over this\ndataset,usingeachtrackinturnasa query.\nWe created a document-term matrix X={xij}with\nsimpletf-idfvalues\nxij=tfijlogN\ndfj(1)\nwhere tf ijis the number of times that term jappears in\nthetagsfortrack i,dfjisthenumberoftrackswhosetags\ncontain term j, andNis the total number of tracks. We\nthen used cosinedistance to comparethe term vectorsfor\neachtrack.\nWe compared three different approaches when calcu-\nlating the term frequenciestf ij: weighting them to reﬂect\nthenumberofuserswhohadappliedthetermtothetrack\nin question; ignoring the number of users; and using the\nqtag3part-of-speech (POS) tagger to restrict the terms\nconsideredto adjectivesonly. The weightsin the ﬁrst ap-\nproach are based on unexplained ‘counts’ published by\nlast.fm, and should therefore be considered ad hoc: we\nuse them only to get an idea of the potential value of in-\ncludingsuchinformation.\nWe extended our experiment by using Latent Seman-\nticAnalysis(LSA)[5]toreducethedimensionalityofthe\n3http://www.english.bham.ac.uk/staff/omason/software /qtag.htmlfeature vector for each track: we calculated the rank- k\nSingularValueDecompositionof thedocument-termma-\ntrix˜Xk=UkSkVT\nkfor a range of ranks, and based our\nsimilarities on the reduced vectors UkSk. The SVD was\ncalculatedoverourfull dataset.\n2.4 Results\nWeshowper-wordmeanAveragePrecision(AP),overthe\nsets of artist and genre labels, in Fig. 2. The AP for a\nparticularqueryiscalculatedas\nAP =/summationtextN\nr=1P(r)rel(r)\nR(2)\nwhere P(r)istheprecisionatrank r,rel(r)is1ifthedoc-\nument at rank r is relevant and 0 otherwise, Ris the total\nnumberof relevantdocuments,and Nis the total number\nofdocumentsinthecollection. APthereforemeasuresthe\naverage precision over the ranks at which each relevant\ntrack is retrieved. The per-word mean AP for a particu-\nlar genre or artist label is the mean AP over all queries\nlabelled with that term. Besides being a standard IR per-\nformance metric, mean AP rewards the retrieval of rele-\nvant tracks ahead of irrelevant ones, and is consequently\nan extremely good indicator of how our vector space is\norganised.\nVectors based on term frequencies using all terms ap-\npliedtothesetracksclearlyperformbetterthanthosebase d\nonadjectivesonly. Thebeneﬁtoftakinguserweightsinto\naccountissomewhatlessclear,improvinggenreprecision\nat all ranks, but having a negligible effect on artist pre-\ncision above rank 60. Using the weights has the effect\nof emphasizing the ‘majority view’ for the relevance of\na particular term to any given track, and a possible inter-\npretation of the results is that genre precision improves\nartiﬁciallyasminorityopinionsare discounted.\nUsingthefulltermvectors,thegenreprecisionreaches\n80%, and the artist precision 61%. For historical rea-\nsons, [9] gives genre performanceas a Leave One Out 1-\nnearest neighbour classiﬁcation rate (effectively showin g\nprecision at rank 2) of 87%. Using our full term vectors,\nthe LOO genre classiﬁcation rate was 95%. The rate us-\ning the nearest track by a differentartist to the query was\n83%. Using LSA at ranks 30 and above consistently im-\nproves the genre precision, and with the weighted counts\nthemaximumisover82%atrank20. LSAimprovesartist\nprecisionatranks200andabove,withamaximumof63%\nat rank300.\nThere is, of course, no ‘right answer’ for the precision\nthat we would hope for when doing retrieval in a vector\nspace for tracks, because songs by other artists or from\ndifferent genres can quite reasonably be considered very\nsimilar to any given query. On the other hand, organisa-\ntionbyartistandgenreiswellunderstoodbymusiclovers,\nandthelackofsuchorganisationinlow-levelfeaturerep-\nresentations appears to be a major barrier to their accep-\ntanceinpracticalapplications. Ourviewisthatareduced-\ndimensionsemanticspacedeﬁnedbytagsmaybeanideal1011021031040.20.30.40.50.60.70.80.9\nLSA rankmean Average PrecisionRetrieval performance by genre\n  \nall terms\nuser weighted\nadjectives\n1011021031040.20.30.40.50.60.70.8\nLSA rankmean Average PrecisionRetrieval performance by artist\n  \nall terms\nuser weighted\nadjectives\nFigure1. Retrievalperformanceoftagtermvectors\nrepresentation, capturing rich descriptions for each trac k\nbased on a very large vocabulary, but also respecting tra-\nditionalcatalogueorganisationwithhighprecision.\n3 THESUB-SPACEOFMUSICAL EMOTION\n3.1 The dimensional representationofemotion\nA signiﬁcant psychological literature has investigated a\nso-called ‘dimensional’ approach to the representation of\nemotion in general [15, 11, 13, 16], and with particular\nregard to emotional responses to music [8, 17]. A focus\nof these studieshas been to map relevant terms onto low-\ndimensional spaces with named axes, intended to corre-\nspondto internalhumanrepresentationsof emotion. This\nhas led to a widely-accepted emotional space with 2 pri-\nmary signiﬁcant dimensions, most frequently referred toasvalence(frompleasanttounpleasant)and arousal(from\nmildtointense). Afurthertwosecondarydimensionshave\nbeenidentiﬁedinsomestudies,butaregenerallyregarded\naslesssigniﬁcant[4].\nSocial tags provide a unique source of high-volume,\nnon-invasivedatafromwhichtostudyemotionalresponses\nto music. We investigate elsewhere the extent to which\ntagging conformsto, or departs from, established models\nforemotioninmusic[10],notinginparticularthatthevo-\ncabulary for mood and emotion arising organically from\nthe user community in tags differs signiﬁcantly from that\ncommonlyusedincontrolledpsychologicalexperiments.\nJustaswecancomparetracksbyco-occurrenceofterms,\nwe can compare terms by co-occurrence over tracks: we\nsimply use the columns of the document-term matrix to\ncreate track vectorsrepresentingterms. ApplyingLSA to\nthe matrix as in Section 2.3, the dimensionally-reduced\nterm vectors are given by VS. We selected all the words\nin our dataset that were applied to at least 50 tracks, and\nwhichappearto relateto moodormusical expression,re-\nsulting in a list of 57 emotion words. We then trained a\nSelf-OrganisingMaponthetrackvectorsforthesewords,\nusing LSA at rank 40, and mapped each word onto its\nbest-matchingunitinthetrainedSOM.Theresultingcon-\nﬁguration of terms is shown in Table 2, and gives an im-\npression of the organisation of emotion words in our se-\nmantic space. This shows some relationship to the tradi-\ntionalarousal-valenceaxes,withvalenceincreasingbroa dly\nfromleftto rightandarousalfromtoptobottom.\n3.2 CorrespondenceAnalysisforvisualisation\nCorrespondenceAnalysis(CA)isawell-establishedtech-\nnique of dimension reduction used primarily for visualis-\ning multivariatecategoricaldata [3, 7]. It hastwo proper-\ntiesthatmakeit extremelyattractiveforourpurposes:\n1. itenablesthevisualisationoftwosetsofcross-tabulat ed\nvariables(inour case tracksandsemantic terms) in\nthe samelow-dimensionalspace;\n2. Euclidean distances in the visualisation represent\ndistributional( χ2) distancesinthedata.\nCAisageneralisedformofPrincipalComponentAnal-\nysis suitable for application to an MbyNtable of co-\noccurrence data F, where Fhas been normalised to have\ntotal sum 1. CA ﬁnds a low-dimensional projection of F\nwhich optimally preserves χ2-distances between row and\ncolumnproﬁles\nfc|r=i=/parenleftbiggfi1\nfi, ...,fiN\nfi/parenrightbigg\nfr|c=j=/parenleftbiggf1j\nfj, ...,fMj\nfj/parenrightbigg\nwhere fi, fjare the row and column sums respectively,\ni.e.fi=/summationtextN\nj=1fijandfj=/summationtextM\ni=1fij.\nTheχ2-metric between row proﬁles is a weighted Eu-\nclideandistancewheretheweightforeachcolumnisgivenby1\nfj; the metric between column proﬁles is weighted\nsimilarly by1\nfi. The χ2-metric has the desirable prop-\nerty that distances between columns (tag words) do not\nchange if columns (tracks) with identical proﬁles (nor-\nmalisedtermvectors)areamalgamated,andviceversa.\nWe computea generalisedSVD of F\n˜F=U∆V′(3)\nwhere∆isa diagonalmatrix,and UandVsatisfy\nU′(Fr)−1U=V′(Fc)−1V=I (4)\nwhereFrandFcare diagonal matrices of the row and\ncolumnsumsrespectively. Co-ordinates Sofrow proﬁles\nontoaxes Uarethengivenby\nfc|r=US (5)\nwhere\nS=∆V′(Fc)−1(6)\nCo-ordinates Tof columnproﬁlesonto axes Vare given\nsimilarlyby\nfr|c=VT (7)\nwhere\nT=∆U′(Fr)−1(8)\nRow and column proﬁles can then be plotted in the same\nd-dimensional space, taking only the ﬁrst dco-ordinates\nofSandT. Althoughitisnotmeaningfulingeneraltoin-\nterpret row-column distances in this visualisation, it doe s\nshow the relative distances of a single row (track) to all\nthecolumns(emotionwords),andviceversa.\nThis suggests a natural application of CA with d= 2\nto create a browse-by-mood interface to a collection of\ntracks, using a normalised portion of our document-term\nmatrix,withrowproﬁlesrepresentingtracksandcolumns\nrestricted to mood terms. The resulting plot of tracks and\nterms shows mood words in a meaningful relationship,\nwhile tracks in any particular region of the space should\nbewell describedbynearbywords.\n3.3 Evaluation\nWe testedthisapproachonasmalllist of14moodwords,\nconsisting of the subset of terms from the classic list of\nmusical emotions given in [8] which were applied to at\nleast50tracksinourdataset,andthesubsetof3176tracks\ntagged with at least one of these words. In Figure 2 we\nshow the resulting positions of the terms and tracks. We\nevaluate the organisation of the plot by calculating the\nmean AP foreach moodword,wherewe considera track\nto be relevantto its closest moodword inthe plot if it has\nbeentaggedwithit.\nTocomplywiththeallowableinterpretationofdistances\ninCA,wetakethemeanAPforeachtermonlyovertracks\nwhichareclosertoitintheCA spacethantheyaretoany\nother term (so each track in the dataset gets considered\nexactly once). The results are given in Table 3, showingTable 2. Emotiontermsmappedontoa SOM\nsoft chill relax sweet summer\nmellow chillout happy\nlove romantic relaxing smooth downtempo fun\ndreamy\nbeautiful melancholic soothing melodic feelgood upbeat\ncalm catchy\nslow sleep pretty lovely uplifting fast funky\nsad bittersweet nice\nquiet\nmelancholy night singalong heavy cool\nemotional\nmoody haunting intense energetic sexy\ndepressing clean sex\ndark experimental ethereal silent angry psychedelic party\natmospheric intensity\n−1−0.5 00.5 11.5 22.5 3−2−1.5−1−0.500.51\ncalm\ncheerfuldark\nenergeticgloomy\nintense\njoyouslove\nnicesad\nsunny\ntendertragic\nwhimsicalcalm\ncheerfuldark\nenergeticgloomy\nintense\njoyouslove\nnicesad\nsunny\ntendertragic\nwhimsical\nFigure2. CA jointplotofmoodwordsandtracksTable 3. MeanAveragePrecisionformoodwords\nMood mean AP\ncalm 0.998\ncheerful 1.000\ndark 0.947\nenergetic 0.925\ngloomy 0.987\nintense 0.924\njoyous 1.000\nlove 1.000\nnice 0.939\nsad 0.965\nsunny 0.942\ntender 1.000\ntragic 1.000\nwhimsical 0.919\nthat the plot partitions the space almost perfectly by this\nmeasure, although it is important to note that precision is\nmeasuredhereagainstwordsfoundintagsthemselves,not\na veriﬁableexternalsourceofinformation.\n4 CONCLUSIONS\nDespite the ad hoc and informal usage typical of social\ntagging, tags are highly effective in capturing music sim-\nilarity. Althoughthey are often discursive, tags for music\nappear to capture sensible attributes grounded in individ-\nual tracks, deﬁning a well-behaved similarity space with\nan effective dimensionality of around 102. Given these\nencouraging results as to the usefulness of tags as mu-\nsic metadata, and the low dimensionality of an effective\nfeature space for music similarity, future work includes\nthe use of tags as groundtruthfor joint feature-annotation\nmodelsformusic.\n5 ACKNOWLEDGMENTS\nThe authors would like to thank Gunter Kreutz for illu-\nminating discussions about psychological approaches to\nrepresentingmusicalemotion.\nThis research was supported by EPSRC grants\nGR/S84750/01(Hierarchical Segmentationand Semantic\nMarkup of Musical Signals) and EP/E017614/1 (Online\nMusicRecognitionAndSearching).\n6 REFERENCES\n[1] J.-J.Aucouturier. Tenexperimentsonthemodellingof\npolyphonic timbre . PhD thesis, University of Paris 6,\n2006.\n[2] S. Baumann and O. Hummel. Using cultural meta-\ndataforartistrecommendations.In Proc.Wedelmusic ,\n2003.[3] J.-P. Benz´ ecri. Histoire et pr´ ehistoire de l’analyse des\ndonn´ ees. Cahiers de l’Analyse des Donn ´ees, 2:9–40,\n1977.\n[4] G.L.Collier.Beyondvalenceandactivityintheemo-\ntional connotations of music. Psychology of Music ,\n35(1):110–131,2007.\n[5] S. Deerwester, S. Dumais, G. W. Furnas, T. K. Lan-\ndauer, and R. Harshman. Indexing by latent semantic\nanalysis. Journal of the Society for Information Sci-\nence,1990.\n[6] C. Fellbaum, editor. WordNet: An Electronic Lexical\nDatabase .MITPress, 1998.\n[7] M. J. Greenacre. Theory and applications of corre-\nspondenceanalysis .AcademicPress, 1984.\n[8] K.Hevner.Experimentalstudiesoftheelementsofex-\npression in music. American Journal of Psychology ,\n48:246–68,1936.\n[9] P. Knees, E. Pampalk, and G. Widmer. Artist classiﬁ-\ncationwith web-baseddata.In Proc.ISMIR ,2004.\n[10] G.KreutzandM.Levy.Emotionannotationsforpop-\nularmusicininternetcommunities.Inpreparation.\n[11] C.E.Osgood,G.J.Succi,andP.H.Tannenbaum. The\nmeasurementof meaning .Universityof IllinoisPress,\n1957.\n[12] E.Pampalk. Computationalmodelsofmusicsimilarity\nand their application to music information retrieval .\nPhDthesis, ViennaUniversityofTechnology,2006.\n[13] J. Russell. A circumplex model of affect. Journal of\nPersonality and Social Psychology , 39(6):1161–78,\n1980.\n[14] N. Scaringella, G. Zoia, and D. Mlynek. Automatic\ngenre classiﬁcation of music content: a survey. IEEE\nSignalProcessingMagazine ,23(2):133–141,2006.\n[15] H.Schlosberg.Threedimensionsofemotion. Psycho-\nlogicalReview ,61(2):81–8,March1954.\n[16] P. Shaver, J. Schwartz, D. Kirson, and C. O’Connor.\nEmotion knowledge: further exploration of a proto-\ntypeapproach. JournalofPersonalityandSocialPsy-\nchology,52(6):1061–86,1987.\n[17] L.Wedin.Dimensionanalysisofemotionalexpression\nin music. Swedish Journal of Musicology , 51:119–\n140,1969.\n[18] B.Whitman.Semanticrankreductionofmusicaudio.\nInProc.IEEEWASPAA ,2003.\n[19] B. Whitman. Learning the Meaning of Music . PhD\nthesis,MIT,2005."
    },
    {
        "title": "Alternative Digitization Approach for Stereo Phonograph Records Using Optical Audio Reconstruction.",
        "author": [
            "Beinan Li",
            "Simon de Leon",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415746",
        "url": "https://doi.org/10.5281/zenodo.1415746",
        "ee": "https://zenodo.org/records/1415746/files/LiLF07.pdf",
        "abstract": "This paper presents the first Optical Audio Reconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preservation. This contactless and imaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. By using a white-light interferometry optical profiler, we are able to extract stereo audio information encoded in the 3D profile of the phonograph record grooves.",
        "zenodo_id": 1415746,
        "dblp_key": "conf/ismir/LiLF07",
        "keywords": [
            "Optical Audio Reconstruction",
            "stereo phonograph records",
            "precision metrology",
            "digital image processing",
            "groove contour data",
            "digital audio",
            "contactless approach",
            "imaging-based method",
            "potential to restore",
            "33rpm long-playing stereo records"
        ],
        "content": "ALTERNATIVE DIGITIZATION APPROACH FOR \nSTEREO PHONOGRAPH RECORDS USING OPTICAL \nAUDIO RECONSTRUCTION \nBeinan Li, Simon de Leon, Ichiro Fujinaga \nMusic Technology Area, Schulich School of Music, McGill University, Montreal, Quebec \n{beinan.li, simon.deleon}@mail.mcgill.ca ich@music.mcgill.ca \nABSTRACT \nThis paper presents the first Optical Audio \nReconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preserva tion. This contactless and \nimaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. \nBy using a white-light interferometry optical profiler, \nwe are able to extract ster eo audio information encoded \nin the 3D profile of the phonograph record grooves.  \n1. INTRODUCTION \nDigital archiving for phonograph recordings is already \non the agenda of cultural heritage preservation work around the world. This is especially noted in the US National Recording Preservation Act of 2000. However, traditional phonograph systems are unable to restore broken records. As a result, the US Library of Congress has offered sponsorship for OAR research [1].  \nBy utilizing precision metrology and digital image \nprocessing, OAR has been applied to wax cylinders [1] and 78rpm (revolutions per minute) records [2] [3] and has successfully extracted satisfactory audio using 2D information. Encouraged by these results, the McGill Image to Audio Conversion (MItAC) project is the first to successfully apply OAR with 3D metrology to 33rpm stereo phonograph records. Unlike mechanical systems, OAR is the only possible method for extracting audio information from damaged and broken records, which will make more historical records available for the fields such as Music Information Retrieval (MIR). \n2. BASICS OF MECHANICAL PHONOGRAPH \nAUDIO \nA mechanical sound recording system converts the \nacoustic energy of the sound into the groove undulations along a spiral trajectory on wax cylinders or vinyl discs. The playback sy stem then uses a playback \nstylus to trace and convert the groove undulations into \naudio while rotating the record. Two types of groove modulation are available for \nmono records:\n vertical and lateral. Both methods are \nused in combination in stereo LPs. The sum of both modulations of the stylus in the groove provides the left channel signal, while the difference provides the right channel signal.  \nIf a disc is broken, turntable-based playback systems \nare not able to follow the groove modulations. OAR systems, on the other hand, have the potential of extracting its audio information by scanning the broken pieces and stitching togeth er their groove data. \n3. THE EXISTING OAR APPROACHES \nOptical reproduction of sound with a laser beam has \nbeen successfully used in obtaining audio from phonograph records [4]. These laser-based systems, however, are unable to handle broken records. \nBased on confocal microscopy, Fadeyev et al. \ndevised two OAR systems for monophonic recordings: one for 78rpm records [1], and the other for an Edison Blue Amberol cylinder [2]. \nThe system of Stotzer et al. uses a camera to rapidly \nphotograph 78rpm records for preservation purposes. The resulting film is digitally scanned and converted in to 2D image, which is then analyzed to obtain the audio \ninformation contained in the lateral undulations [3]. \nNone of the above methods are directed towards the \nreconstruction of 33rpm stereo phonograph records. \n4. OAR FOR STEREO PHONOGRAPH \nRECORDS \n4.1. White-light Interferomet ry Optical Profiler \nOptical interferometry has been widely used in \napplications requiring accurate measurements of distances, displacements, and vibrations [5]. We are \nusing a Wyko NT8000 series white-light interferometry profiler.\n With the ability to adjust its focus vertically, a \nwhite-light interferometer can achieve a vertical resolution of better than 1nm. At the 10X magnification (a field of view (FOV) of 0.644mm x 0.483mm), which is used in the experiment of this paper, the lateral resolution is 1 μm.   \nAccording to the mech anism of the stereo \nphonograph recording, only the spatial positions of both \ntop edges and the bottom of the groove are needed in order to extract the stereo  audio data.   \n \n4.2. Stitching the Pieces of the Results \nA disc area containing audio was divided into an array \nof overlapped regions of th e FOV size, each of which \nwas scanned individually. The results were then stitched together to produce a composite  of the entire area. Such \na stitching can be performed either in the image domain after the scan or in the audio domain after the audio extractions for all the regions. Each region is called a stitching frame (SF). In this paper, we used a simple audio-domain stitching that takes the mean amplitude in the overlapping region of both consecutive SFs. \n4.3. Converting Positional Data to Audio \nBy tracing a selected groove in the resulting images and \nmeasuring at each spatial samp le increment, the radial \ndistances relative to the di sc center and the vertical \npositions of the groove bottom and the top edges are obtained. The lateral and vertical stylus velocities were then derived from the positional information. Finally, the stereo signal was extr acted by combining the \nvertical and lateral data as described in Section 2. \n5. RESULTS \nWe used an audio system test LP [6] and measured a \nstereo signal with a silent left channel and a 1kHz sine wave in the right channel. The signal simplicity gives us a fairly straightforward way to validate the stereo result. The target signal shown in Figure 5.1 is measured from approximately three periods of the right-channel sine wave. The extracted lateral and vertical undulations of one of the grooves are shown in Figure 5.2. The phasing in (a) and (b) demonstrates that the resulting left and right channel signals ar e in accordance with the \nprinciple described in Section 2. \n6. DISCUSSIONS \nAs an optimized setting of our profiler, with a 10X \nmagnification, a vertical scan speed of 15 μm/s, 20% \ninter-SF overlapping, the required time for scanning one side of an entire 33rpm stereo phonograph record area containing an audio signal is about 10 days. The required storage space for a s canned disc image is about \n198GB (without compression). \n7. CONCLUSION AND FUTURE WORK \nOAR methods are the only way of restoring broken \nphonograph records. In this work, OAR is applied to the reconstruction of 33rpm stereo phonograph records for the first time in the world. In the future, more extensive experiments will be conducte d on various 33rpm stereo \nrecords. Additionally, image restoration methods will be explored to improve the reconstructed audio quality. \n \nFigure 5.1.  The 3D contour view of the segments of four \ngrooves from a stereo signal with a 1kHz sine wave in the right channel and a silent left channel. The result is composed of three stitching frames, each of which is of the FOV size. \n \n  \n(a)                                      (b) \nFigure 5.2.  The stylus velocity data extracted from the right-\nchannel stereo groove in Figure 5.1. (a) The lateral velocity of the stylus; (b) The vertical velocity of the stylus.\n  \n8. ACKNOWLEDGEMENT \nWe would like to thank the Canada Foundation for Innovation \nand the Daniel Langlois F oundation for financial support.  \n9. REFERENCES \n[1] Fadeyev, V., and C. Haber. “Reconstruction of \nmechanically recorded sound by image processing”, LBNL Report 51983 . 2003. \n[2] Fadeyev, V., C. Haber, C. Maul, J. McBride, \nand M. Golden. “Reconstruction of recorded sound from an edison cylinder using three-dimensional non-contact optical surface metrology”, LBNL Report 54927 . 2004. \n[3] Stotzer, S., O. Johnsen, F. Bapst, C. Sudan, and \nR. Ingold. “Phonographic sound extraction using image and signal processing”, Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing  4:289–92, Montreal, Canada, 2004. \n[4] Iwai, T., T. Asakura, T. Ifubuke, and T. \nKawashima. “Reproduction of sound from old wax phonograph cylinders using the laser-beam reflection method”, Applied Optics  25 (5): \n597−604, 1986. \n[5] Hariharan, P. Basics of interferometry . Elsevier \nAcademic Press, Boston, MA, 1992. \n[6] Shure Brothers, Inc. “’An audio obstacle \ncourse’ the Shure trackability test record”, \nTTR-101, Evanston, IL, 196 7."
    },
    {
        "title": "Improving Genre Classification by Combination of Audio and Symbolic Descriptors Using a Transcription Systems.",
        "author": [
            "Thomas Lidy",
            "Andreas Rauber",
            "Antonio Pertusa",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416344",
        "url": "https://doi.org/10.5281/zenodo.1416344",
        "ee": "https://zenodo.org/records/1416344/files/LidyRPQ07.pdf",
        "abstract": "Recent research in music genre classification hints at a glass ceiling being reached using timbral audio features. To overcome this, the combination of multiple different feature sets bearing diverse characteristics is needed. We propose a new approach to extend the scope of the features: We transcribe audio data into a symbolic form using a transcription system, extract symbolic descriptors from that representation and combine them with audio features. With this method, we are able to surpass the glass ceiling and to further improve music genre classification, as shown in the experiments through three reference music databases and comparison to previously published performance results. 1 INTRODUCTION Audio genre classification is an important task for retrieval and organization of music databases. Traditionally the research domain of genre classification is divided into the audio and symbolic music analysis and retrieval domains. The goal of this work is to combine approaches from both directions that have proved their reliability in their respective domains. To assign a genre to a song, audio classifiers use features extracted from digital audio signals, and symbolic classifiers use features extracted from scores. These features are complementary; a score can provide very valuable information, but audio features (e.g., the timbral information) are also very important for genre classification. To extract symbolic descriptors from an audio signal it is necessary to first employ a transcription system in order to detect the notes stored in the signal. Transcription systems have been investigated previously but a wellperforming solution for polyphonic music and a multitude of genres has not yet been found. Though these systems might not be in a final state for solving the transcription problem, our hypothesis is that they are able to augment the performance of an audio genre classifier. In this work, a new transcription system is used to get a symbolic representation from an audio signal. c⃝2007 Austrian Computer Society (OCG). Figure 1. General framework of the system The overall scheme of our proposed genre classification system is shown in Figure 1. It processes an audio file in two ways to predict its genre. While in the first branch, the audio feature extraction methods described in Section",
        "zenodo_id": 1416344,
        "dblp_key": "conf/ismir/LidyRPQ07",
        "keywords": [
            "audio genre classification",
            "glass ceiling",
            "timbral audio features",
            "symbolic music analysis",
            "transcription system",
            "symbolic descriptors",
            "reference music databases",
            "symbolic representation",
            "polyphonic music",
            "genre classification"
        ],
        "content": "IMPROVING GENRE CLASSIFICATION BY COMBINATION OF AUDIO\nAND SYMBOLIC DESCRIPTORS USING A TRANSCRIPTION SYSTEM\nThomas Lidy, Andreas Rauber\nVienna University of Technology, Austria\nDepartment of Software Technology\nand Interactive SystemsAntonio Pertusa, Jos ´e Manuel I ˜nesta\nUniversity of Alicante, Spain\nDepartamento de Lenguajes y\nSistemas Inform ´aticos\nABSTRACT\nRecent research in music genre classiﬁcation hints at a\nglass ceiling being reached using timbral audio features.\nTo overcome this, the combination of multiple different\nfeature sets bearing diverse characteristics is needed. We\npropose a new approach to extend the scope of the fea-\ntures: We transcribe audio data into a symbolic form using\na transcription system, extract symbolic descriptors from\nthat representation and combine them with audio features.\nWith this method, we are able to surpass the glass ceil-\ning and to further improve music genre classiﬁcation, as\nshown in the experiments through three reference music\ndatabases and comparison to previously published perfor-\nmance results.\n1 INTRODUCTION\nAudio genre classiﬁcation is an important task for retrieval\nand organization of music databases. Traditionally the re-\nsearch domain of genre classiﬁcation is divided into the\naudio and symbolic music analysis and retrieval domains.\nThe goal of this work is to combine approaches from both\ndirections that have proved their reliability in their respec-\ntive domains. To assign a genre to a song, audio classiﬁers\nuse features extracted from digital audio signals, and sym-\nbolic classiﬁers use features extracted from scores. These\nfeatures are complementary; a score can provide very\nvaluable information, but audio features (e.g., the timbral\ninformation) are also very important for genre classiﬁca-\ntion.\nTo extract symbolic descriptors from an audio signal\nit is necessary to ﬁrst employ a transcription system in\norder to detect the notes stored in the signal. Transcrip-\ntion systems have been investigated previously but a well-\nperforming solution for polyphonic music and a multitude\nof genres has not yet been found. Though these systems\nmight not be in a ﬁnal state for solving the transcription\nproblem, our hypothesis is that they are able to augment\nthe performance of an audio genre classiﬁer. In this work,\na new transcription system is used to get a symbolic rep-\nresentation from an audio signal.\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nFigure 1 . General framework of the system\nThe overall scheme of our proposed genre classiﬁca-\ntion system is shown in Figure 1. It processes an audio ﬁle\nin two ways to predict its genre. While in the ﬁrst branch,\nthe audio feature extraction methods described in Section\n3.1 are applied directly to the audio signal data, there is\nan intermediate step in the second branch. A polyphonic\ntranscription system, described in Section 3.2.1, converts\nthe audio information into a form of symbolic notation.\nThen, the symbolic feature extractor (c.f. Section 3.2.2) is\napplied on the resulting representation, providing a set of\nsymbolic descriptors as output. The audio and symbolic\nfeatures extracted from the music serve as combined in-\nput to a classiﬁer (c.f. Section 3.3). Section 4 provides a\ndetailed evaluation of the approach and Section 5 draws\nconclusions and outlines future work.\n2 RELATED WORK\nAucouturier and Pachet report about a glass ceiling be-\ning reached using timbre features for music classiﬁca-\ntion [1]. In our work on combining feature sets from\nboth the audio and the symbolic MIR domains we aim\nat breaking through this glass ceiling and bringing fur-\nther improvements to music genre classiﬁcation. To our\nknowledge there are is previous work combining audio\nand symbolic approaches for music classiﬁcation. McKay\net al. suggested this possibility in 2004 [12], but they also\npointed out that the transcription techniques were not re-\nliable enough to extract high-level features from them.\nHowever, there are many related works on audio genre\nclassiﬁcation. Li and Tzanetakis [9] did experiments on\nvarious combinations of FFT, MFCC, Beat and Pitch fea-\ntures using Support Vector Machines (SVM, MPSVM)\nand Linear Discriminant Analysis (LDA). Mandel and\nEllis [11] compared MFCC-based features extracted atthe song-level with extraction at the artist-level, investi-\ngated different distance measures for classiﬁcation, and\ncompared results from SVM and k-NN, where SVM per-\nformed better in all results. Pampalk et al. [14] combined\ndifferent feature sets based on Fluctuation Patterns and\nMFCC-based Spectral Similarity in a set of experiments.\nOne of the four databases used overlaps with one of the\nthree we use. Bergstra et al. [2] described the approach\nthey used in the MIREX 2005 evaluation. They employed\na combination of 6 different feature sets and applied Ad-\naBoost for ensemble classiﬁcation.\nAbout symbolic genre classiﬁcation, there are previous\nstudies like [12] that extract features from scores, using a\nlearning scheme to classify genres, reporting good results.\nThe symbolic features used in our study are based on those\ndescribed in [16], which were used for symbolic music\nclassiﬁcation. One of the main components of our work is\na polyphonic transcription system. This it is not a solved\ntask and a very active topic in MIR research; some of the\nmain previous approaches were reviewed in [7].\nThis study is related to [10], as our goal is to improve\nprevious music genre classiﬁcation results by extension of\nthe feature space through the novel approach of including\nfeatures extracted from symbolic transcription.\n3 SYSTEM DESCRIPTION\n3.1 Audio Feature Extraction\n3.1.1 Rhythm Patterns\nThe feature extraction process for a Rhythm Pattern [17,\n10] is composed of two stages. First, the speciﬁc loudness\nsensation on 24 critical frequency bands is computed, by\nusing a Short Time FFT, grouping the resulting frequency\nbands to the Bark scale, applying spreading functions to\naccount for masking effects and successive transforma-\ntion into the Decibel, Phon and Sone scales. This results\nin a psycho-acoustically modiﬁed Sonogram representa-\ntion that reﬂects human loudness sensation. In the second\nstep, a discrete Fourier transform is applied to this Sono-\ngram, resulting in a (time-invariant) spectrum of loudness\namplitude modulation per modulation frequency for each\nindividual critical band. After additional weighting and\nsmoothing steps, a Rhythm Pattern exhibits magnitude of\nmodulation for 60 modulation frequencies (between 0.17\nand 10 Hz) on 24 bands, and has thus 1440 dimensions.\n3.1.2 Rhythm Histograms\nA Rhythm Histogram (RH) aggregates the modulation\namplitude values of the individual critical bands computed\nin a Rhythm Pattern and is thus a lower-dimensional de-\nscriptor for general rhythmic characteristics in a piece of\naudio [10]. A modulation amplitude spectrum for criti-\ncal bands according to the Bark scale is calculated, as for\nRhythm Patterns. Subsequently, the magnitudes of each\nmodulation frequency bin of all critical bands are summedup to a histogram, exhibiting the magnitude of modulation\nfor 60 modulation frequencies between 0.17 and 10 Hz.\n3.1.3 Statistical Spectrum Descriptors\nIn the ﬁrst part of the algorithm for computation of a Sta-\ntistical Spectrum Descriptor (SSD) the speciﬁc loudness\nsensation is computed on 24 Bark-scale bands, equally as\nfor a Rhythm Pattern. Subsequently, the mean, median,\nvariance, skewness, kurtosis, min- and max-value are cal-\nculated for each individual critical band. These features\ncomputed for the 24 bands constitute a Statistical Spec-\ntrum Descriptor. SSDs are able to capture additional tim-\nbral information compared to Rhythm Patterns, yet at a\nmuch lower dimension of the feature space (168 dim.), as\nshown in the evaluation in [10].\n3.1.4 Onset Features\nAn onset detection algorithm described in [15] has been\nused to complement audio features. The onset detector\nanalyzes each audio frame labeling it as an onset frame or\nas a not-onset frame. As a result of the onset detection,\n5 onset interval features have been extracted: minimum,\nmaximum, mean, median and standard deviation of the\ndistance in frames between two consecutive onsets. The\nrelative number of onsets are also obtained, dividing the\nnumber of onset frames by the total number of frames of\na song. As this onset detector is based on energy varia-\ntions, the strength of the onset, which corresponds with\nthe value of the onset detection function o(t), can pro-\nvide information about the timbre; usually, an o(t)value\nis high when the attack is shorter or more percussive (e.g.,\na piano), and low values are usually produced by softer\nattacks (e.g., a violin). The minimum, maximum, mean,\nmedian and standard deviation of the o(t)values of the\ndetected onsets were also added to the onset feature set,\nwhich ﬁnally consists of 11 features.\n3.2 Symbolic Feature Extraction\n3.2.1 Transcription System\nTo complement the audio features with symbolic features\nwe developed a new polyphonic transcription system to\nextract the notes. This system converts the audio signal\ninto a MIDI ﬁle that will later be analyzed to extract the\nsymbolic descriptors. It does not consider rhythm, only\npitches and note durations are extracted. Therefore, the\ntranscription system converts a mono audio ﬁle sampled\nat 22 kHz into a sequence of notes. First, performs a Short\nTime Fourier Transform (STFT) using a Hanning window\nwith 2048 samples and 50% overlap. With these parame-\nters, the temporal resolution is 46 ms. Zero padding has\nbeen used, multiplying the original size of the window\nby 8 and adding zeroes to complete it before the STFT\nis computed. This technique does not increase resolution,\nbut the estimated amplitudes and frequencies of the new\nspectral bins are usually more accurate than applying in-\nterpolation.Then, the onset detection stage described in [15] is per-\nformed, classifying each time frame tias onset or not-\nonset. The system searches for notes between two con-\nsecutive onsets, analyzing only one frame between two\nonsets to detect each chord. To minimize the note attack\nproblems in fundamental frequency ( f0) estimation, the\nframe chosen to detect the active notes is to+ 1, being\ntothe frame where an onset was detected. Therefore, the\nspectral peak amplitudes 46 ms after an onset provide the\ninformation to detect the actual chord.\nFor each frame, we use a peak detection and estimation\ntechnique proposed by Rodet called Sinusoidal Likeness\nMeasure (SLM) [19]. This technique can be used to ex-\ntract spectral peaks corresponding to sinusoidal partials,\nand this way residual components can be removed. SLM\nneeds two parameters: the bandwith W, that has been set\nasW= 50 Hz and a threshold µ= 0.1. If the SLM value\nvΩ< µ, the peak will be removed. After this process, an\narray of sinusoidal peaks for each chord is obtained.\nGiven these spectral peaks, we have to estimate the\npitches of the notes. First, the f0candidates are chosen\ndepending on their amplitudes and their frequencies. If\na spectral peak amplitude is lower than a given threshold\n(experimentally, 0.05 reported good results), the peak is\ndiscarded as f0candidate, because in most instruments\nusually the ﬁrst harmonic has a high amplitude. There are\ntwo more restrictions for a peak to be a f0candidate: only\nf0candidates within the range [50Hz-1200Hz] are consid-\nered, and the absolute difference in Hz between the candi-\ndate and the pitch of its closest note in the well-tempered\nscale must be less than fdHz. Experimentally, setting this\nvalue to fd= 3 Hz yielded good results. This is a ﬁxed\nvalue independent of f0because this way many high fre-\nquency peaks that generate false positives are removed.\nOnce a subset of f0candidates is obtained, a ﬁxed\nspectral pattern is applied to determine whether the can-\ndidate is a note or not. The spectral pattern used in this\nwork is a vector in which each position represents a har-\nmonic value relative to the f0value. Therefore, the ﬁrst\nposition of the vector represents f0amplitude and will al-\nways be 1, the second position contains the relative am-\nplitude of the second partial respect to the ﬁrst, one and\nso on. The spectral pattern spused in this work contains\nthe amplitude values of the ﬁrst 8 harmonics, and has been\nset to sp= [1,0.5,0.4,0.3,0.2,0.1,0.05,0.01], which is\nsimilar to the one proposed by Klapuri in [6]. As differ-\nent instruments have different spectra, this general pattern\nis more adequate for some instruments, such as a piano,\nand less realistic for others, like a violin. This pattern was\nselected from many combinations tested.\nAn algorithm is applied over all the f0candidates to\ndetermine whether a candidate is a note or not. First, the\nharmonics hthat are a multiple of each f0candidate are\nsearched. A harmonic hbelonging to f0is found when the\nclosest spectral peak to f0his within the range [−fh, fh],\nbeing fh:\nfh=hf0/radicalbig\n1 +β(h2−1) (1)\nwith β= 0.0004 . There is a restriction for a candidateto be a note; a minimum number of its harmonics must\nbe found. This number was empirically set to half of the\nnumber of harmonics in the spectral pattern. If a candidate\nis considered as a note, then the values of the harmonic\namplitudes in the spectral pattern (relative to the f0ampli-\ntude) are subtracted from the corresponding spectral peak\namplitudes. If the result of a peak subtraction is lower\nthan zero, then the peak is removed completely from the\nspectral peaks. The loudness lnof a note is the sum of its\nexpected harmonic amplitudes.\nAfter this stage, a vector of note candidates is obtained\nat each time frame. Notes with a low absolute or relative\nloudness are removed. Firstly, the notes with a loudness\nln< γ are eliminated. Experimentally, a value γ= 5\nreported good results. Secondly, the maximum note loud-\nnessLn= max lnat the target frame is computed, and\nthe notes with ln< ηL nare also discarded. After exper-\niments, η= 0.1was chosen. Finally, the frequency and\nloudness of the notes are converted to MIDI notes.\n3.2.2 Symbolic Features\nA set of 37 symbolic descriptors was extracted from the\ntranscribed notes. This set is based on the features de-\nscribed in [16], that yielded good results for monophonic\nclassical/jazz classiﬁcation, and on the symbolic features\ndescribed in [18], used for melody track selection in MIDI\nﬁles. The number of notes, number of signiﬁcant si-\nlences, and the number of non-signiﬁcant silences were\ncomputed. Note pitches, durations, Inter Onset Intervals\n(IOI) and non-diatonic notes were also analyzed, reporting\nfor each one their highest and lowest values, their average,\nrelative average, standard deviation, and normality. The\ntotal number of IOI was also taken into account, as the\nnumber of distinct pitch intervals, the count of the most\nrepeated pitch interval, and the sum of all note durations,\ncompleting the symbolic feature set.\n3.3 Classiﬁcation\nThere are several alternatives of how to design a music\nclassiﬁcation system. The option we chose is to concate-\nnate different feature sets and provide the combined set to\na standard classiﬁer that receives an extended set of fea-\nture attributes on which it bases its classiﬁcation decision\n(c.f. Figure 1). For our experiments we chose linear Sup-\nport Vector Machines. We used the SMO implementation\nof the Weka machine learning software [21] with pairwise\nclassiﬁcation and the default Weka parameters (complex-\nity parameter C= 1.0). We investigated the performance\nof the feature sets individually in advance and then de-\ncided which feature sets to combine. In Section 4 we ex-\namine which feature sets achieve the best performance in\ncombination. Other possibilities include the use of classi-\nﬁer ensembles, which is planned for future work.4 EVALUATION\nOur goal was to achieve improvements of music genre\nclassiﬁcation by our novel approach of combining feature\nsets from the symbolic and audio music information re-\ntrieval domains. In order to demonstrate the achievements\nwe made, we compare our results to the performance of\nthe audio features only, previously reported in [10], using\nthe same databases and the same evaluation method.\n4.1 Data Sets\nThe three data sets that we used are well-known and avail-\nable within the MIR community and are used also by\nother researchers as reference music collections for exper-\niments. For an overview of the data see Table 1. One of\nthe data sets (‘GTZAN’) was compiled by George Tzane-\ntakis [20] and consists of 1000 audio pieces equally dis-\ntributed over 10 popular music genres.\nThe other two music collections were distributed dur-\ning the ISMIR 2004 Audio Description Contest [3] and\nare still available from the ISMIR 2004 web site. The\n‘ISMIRrhythm’ data set was used in the ISMIR 2004\nRhythm classiﬁcation contest. The collection consists of\n698 excerpts of 8 genres from Latin American and ball-\nroom dance music. The ‘ISMIRgenre’ collection was\navailable for training and development in the ISMIR 2004\nGenre Classiﬁcation contest and contains 1458 songs\nfrom Magna tune.com organized unequally into 6 genres.\n4.2 Evaluation Method\nFor evaluation we adhere to the method we used in the\npreceding study [10]. To compare the results with other\nperformance numbers reported in literature on the same\ndatabases, we use (stratiﬁed) 10-fold cross validation. As\ndescribed in Section 3.3, we use Support Vector Machines\nfor classiﬁcation. We report macro-averaged Precision\n(PM) and Recall ( RM),F1-Measure and Accuracy ( A),\nas deﬁned in [10]. This way we are able to compare the\nresults of this study directly to the performance reported\nin [10], and we can use the best results of the previous\nstudy as a baseline for the current work.\n4.3 Performance of Individual Feature Sets\nIn the ﬁrst set of experiments, we performed an evalua-\ntion of the ability of the individual feature sets described\nin Section 3 to discriminate the genres of the data sets.\nThis gives an overview of the potential of each feature\nset and its expected contribution to music genre classiﬁ-\ncation. The performance of three of the four audio feature\nsets has been already evaluated in [10], but the experiment\nhas nevertheless been repeated, to (1) approve the results,\n(2) show the baseline of the individual feature sets and (3)\nprovide a comparison of the individual performance of all\n5 feature sets used in this work.\nTable 2 shows Precision, Recall, F1-Measure and Ac-\ncuracy for the 5 feature sets, as well as their dimensional-Table 1 . Data sets used for evaluation\ndata set cl. ﬁles ﬁle duration total duration\nGTZAN 10 1000 30 seconds 05:20\nISMIRrhythm 8 698 30 seconds 05:39\nISMIRgenre 61458 full songs 18:14\nity. The features extracted by the Onset detector seem to\nperform rather poorly, but considering the low dimension-\nality of the set (compared to the others), the performance\nis nonetheless respectable. In particular, if we consider a\n“dumb classiﬁer” attributing all pieces to the class with the\nhighest probability (i.e. the largest class), the lower base-\nline would be 10 % Accuracy for the GTZAN data set,\n15.9 % for the ISMIRrhythm data set and 43.9 % for the\nISMIRgenre data set. Hence, the Onset features exceed\nthis performance substantially, making them valuable de-\nscriptors.\nThe most interesting set of descriptors are the symbolic\nones derived from the transcribed data as described in Sec-\ntion 3.2. Their Accuracy surpassed that of the Rhythm\nHistogram features, which are computed directly from au-\ndio, on the ISMIRgenre data set and they also achieved\nremarkable performance on both other data sets.\nIf we compare the results of the RH, SSD and RP fea-\ntures to those reported in [10], we notice small deviations,\nwhich are probably due to (1) minor (bug) corrections in\nthe code of the feature extractor and (2) changes made in\nnewer versions of the Weka classiﬁer.\n4.4 Feature Set Combinations\nThere are potentially many feature combination possibili-\nties. In our experiments we combined the Onset and Sym-\nbolic features with the best-performing audio feature set\nand combinations of the previous evaluation (see [10]).\nThe baseline is taken from the maximum values in each\ncolumn of Table 5 in [10].\nTable 3 shows the results of our approach of combin-\ning both audio and symbolic features. Adding Symbolic\nfeatures to the SSD features improves the results by sev-\neral percent. Together with Onset features, the Accuracy\nof SSD features on the ISMIRrhythm data set is increased\nby 10 percentage points. On the ISMIRgenre data set this\nfeature combination achieves the best result, with 81.4 %\nAccuracy. Together with RH features, Accuracy reaches\n76.8 % on the GTZAN set. The combination of all 5\nfeature sets achieves a remarkable 90.4 % on the ISMIR-\nrhythm collection. Compared to the baseline of 2005, im-\nprovements were made consistently for all performance\nmeasures on all databases.\n4.5 Comparison to other works\n4.5.1 GTZAN data set\nLi and Tzanetakis performed an extensive study on indi-\nvidual results and combinations of 4 different feature sets\n(FFT, MFCC, Beat and Pitch features) and three differ-\nent classiﬁers [9]. The best result (on 10-fold cross val-Table 2 . Evaluation of individual feature sets. Dimensionality of feature set, macro-averaged Precision ( PM), macro-\naveraged Recall ( RM),F1-Measure and Accuracy ( A) in %.\nGTZAN ISMIRrhythm ISMIRgenre\nFeature Set dim. PMRMF1 APMRMF1 APMRMF1 A\nOnset 11 34.4 34.9 34.1 34.9 44.8 44.4 40.3 48.4 26.9 33.9 29.7 58.0\nSymbolic 37 41.2 41.3 40.8 41.3 49.6 47.9 46.7 51.1 40.0 43.0 39.7 66.0\nRH 60 43.5 44.0 42.8 44.0 84.7 81.9 82.8 82.7 47.5 40.8 39.3 64.4\nSSD 168 72.6 72.6 72.5 72.6 58.0 57.6 57.6 59.6 75.7 68.7 71.4 78.6\nRP 1440 64.2 64.4 64.1 64.4 87.1 86.1 86.5 86.5 67.0 65.7 66.2 75.9\nTable 3 . Evaluation of feature set combinations. Best results boldfaced.\nGTZAN ISMIRrhythm ISMIRgenre\nFeature Sets dim. PMRMF1 APMRMF1 APMRMF1 A\nOnset+Symb. 48 50.4 50.5 50.2 50.5 60.1 59.9 59.7 61.6 40.7 44.6 41.7 68.0\nSSD+Onset 179 74.6 74.5 74.4 74.5 65.9 65.1 65.3 67.6 76.8 70.8 73.2 79.6\nSSD+Symb. 205 76.0 75.7 75.8 75.7 62.1 62.0 62.0 63.6 76.5 71.2 73.3 81.0\nSSD+Onset+Symb. 216 76.4 76.1 76.2 76.1 67.8 67.6 67.6 69.5 77.9 72.2 74.5 81.4\nRH+SSD+Onset+Symb. 276 76.9 76.8 76.8 76.8 87.3 86.8 86.9 87.1 76.8 71.6 73.7 80.5\nRP+SSD+Onset+Symb. 1656 74.3 74.3 74.2 74.3 90.1 89.4 89.7 89.8 72.8 71.7 72.2 80.6\nRP+RH+SSD+Onset+Symb. 1716 74.0 74.0 73.9 74.0 91.0 90.0 90.4 90.4 73.0 71.9 72.4 80.9\nBest result 2005 [10] 74.8 74.9 74.8 74.9 85.0 83.4 84.2 84.2 76.9 72.0 73.3 80.3\nidation) using pairwise SVM was 69.1 % Accuracy, us-\ning LDA 71.1 %. Li et al. [8] reported an Accuracy of\n74.9 % in a 10-fold cross validation of DWCH features\non the GTZAN data set using SVMs with pairwise clas-\nsiﬁcation and 78.5 % using one-versus-the-rest. With our\ncurrent approach we achieved 76.8 % and surpassed the\nperformance on pairwise classiﬁcation.\nBergstra et al. describe the approach they used in the\nMIREX 2005 evaluation in [2]. They used a combination\nof 6 different feature sets and applied AdaBoost for en-\nsemble classiﬁcation. The authors mention 83 % achieved\n“in trials” on the GTZAN database, but they do not report\nabout the experiment setup (e.g. number of folds).\n4.5.2 ISMIRrhythm data set\nIn [5] Flexer et al. proposed a combination scheme based\non posterior classiﬁer probabilities for different feature\nsets. They demonstrated their approach by combining a\nspectral similarity measure and a tempo feature in a k-\nNN (k=10) 10-fold cross validation on the ISMIRrhythm\ndata set, achieving a major improvement over linear com-\nbination of distance matrices. Their maximum reported\nAccuracy value was 66.9 %.\nWe compared the approach in [10] to Dixon et al.\nachieving 96 % Accuracy incorporating a-priori tempo in-\nformation about the genres and 85.7 % without [4]. With\nthe current proposed approach we achieve 90.4 % without\nusing any external information.\n4.5.3 ISMIRgenre data set\nThe authors of [14] performed experiments on combina-\ntion of different feature sets and used a data set that corre-sponds to the training set of the ISMIR 2004 genre contest\nand thus to 50 % of our database. However, they used a\nspeciﬁc splitting of the data, involving an artist ﬁlter. Al-\nthough recommended by recent studies, we did not apply\nan artist ﬁlter in our experiments, because we would not\nbe able to compare the results to previous studies. More-\nover, their experiments were evaluated using a nearest-\nneighbor classiﬁer and leave-one-out cross validation, an-\nother reason why they cannot be compared to ours. Nev-\nertheless, they achieved an improvement on genre classi-\nﬁcation by determining speciﬁc weights for the individual\nfeature sets, with a maximum Accuracy of 81 % without\nusing the artist ﬁlter. In [13] an extended set of experi-\nments with other features and similarity measures is re-\nported on an equal database and test setup, however, no\nhigher results are reported than the previous one.\n5 CONCLUSIONS AND FUTURE WORK\nWith our approach of combining audio with symbolic fea-\ntures derived through the use of a transcription system\nwe achieved improvements on three reference benchmark\ndata sets, consistently for all four performance measures\nreported. Although improvements on classiﬁcation are not\nof substantial magnitude, it seems that the “glass ceiling”\ndescribed in [1] can be surpassed by combining features\nthat describe diverse characteristics of music.\nFuture work includes investigation of the feature space,\nespecially of the high-dimensional Rhythm Patterns fea-\nture set. First approaches to reduce the dimensional-\nity have been undertaken by using Principal Component\nAnalysis, but a more sophisticated approach of feature se-\nlection will be investigated.There is still room for improvement of the onset detec-\ntor (e.g. including tempo information) and the transcrip-\ntion system, and with improvements, the performance of\nthe symbolic descriptors is expected to increase as well.\nAdditional symbolic features can be included in future.\nWe also plan to test different classiﬁers and to employ\nclassiﬁer ensembles. Alternative approaches can be envis-\naged, such as the individual classiﬁcation of the audio and\nsymbolic feature sets combining the decision of both bran-\nches using a classiﬁer ensemble (e.g. decision by majority\nvote), or the usage of different classiﬁers which receive\nthe same input, either individual or combined feature sets.\nIn conclusion, many improvements can be still done\nto increase the performance of this combined audio music\nclassiﬁcation approach that has yielded remarkable results\nin these ﬁrst experiments.\n6 ACKNOWLEDGMENTS\nThis work is supported by the Spanish PROSEMUS\nproject with code TIN2006-14932-C02 and the EU FP6\nNoE MUSCLE, contract 507752.\n7 REFERENCES\n[1] J.-J. Aucouturier and F. Pachet. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences , 1(1), 2004.\n[2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. Kegl. Aggregate features and AdaBoost for music\nclassiﬁcation. Machine Learning , 65(2-3):473–484,\n2006.\n[3] P. Cano, E. G ´omez, F. Gouyon, P. Herrera, M. Kop-\npenberger, B. Ong, X. Serra, S. Streich, and N. Wack.\nISMIR 2004 audio description contest. Technical Re-\nport MTG-TR-2006-02, MTG, Pompeu Fabra Univer-\nsity, April 6 2006.\n[4] S. Dixon, F. Gouyon, and G. Widmer. Towards char-\nacterisation of music via rhythmic patterns. In Proc.\nISMIR , pages 509–516, Barcelona, Spain, 2004.\n[5] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer. Prob-\nabilistic combination of features for music classiﬁca-\ntion. In Proc. ISMIR , Victoria, Canada, October 8-12\n2006.\n[6] A. Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proc. IS-\nMIR, pages 216–221, Victoria, Canada, 2006.\n[7] A. Klapuri and M. Davy. Signal Processing Methods\nfor Music Transcription . Springer-Verlag, New York,\n2006.\n[8] T. Li, M. Ogihara, and Q. Li. A comparative study\non content-based music genre classiﬁcation. In Pro-\nceedings of the International ACM Conference on Re-\nsearch and Development in Information Retrieval (SI-\nGIR) , pages 282 – 289, Toronto, Canada, 2003.[9] T. Li and G. Tzanetakis. Factors in automatic musical\ngenre classiﬁcation of audio signals. In IEEE Work-\nshop on Applications of Signal Processing to Audio\nand Acoustics , pages 143–146, New Paltz, NY , USA,\nOctober 19-22 2003.\n[10] T. Lidy and A. Rauber. Evaluation of feature extractors\nand psycho-acoustic transformations for music genre\nclassiﬁcation. In Proc. ISMIR , pages 34–41, London,\nUK, September 11-15 2005.\n[11] M.I. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In Proc.\nISMIR , London, UK, September 11-15 2005.\n[12] C. McKay and I. Fujinaga. Automatic genre classiﬁ-\ncation using large high-level musical feature sets. In\nProc. ISMIR , pages 525–530, Barcelona, Spain, Octo-\nber 10-14 2004.\n[13] E. Pampalk. Computational Models of Music Simi-\nlarity and their Application to Music Information Re-\ntrieval . PhD thesis, Vienna University of Technology,\nAustria, March 2006.\n[14] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof audio-based music similarity and genre classiﬁca-\ntion. In Proc. ISMIR , pages 628–633, London, UK,\nSeptember 11-15 2005.\n[15] A. Pertusa, A. Klapuri, and J.M. I ˜nesta. Recogni-\ntion of note onsets in digital music using semitone\nbands. In Proc. 10th Iberoamerican Congress on Pat-\ntern Recognition (CIARP) , LNCS, pages 869–879,\n2005.\n[16] P. J. Ponce de Le ´on and J. M. I ˜nesta. A pattern recog-\nnition approach for music style identiﬁcation using\nshallow statistical descriptors. IEEE Trans. on Systems\nMan and Cybernetics C , 37(2):248–257, 2007.\n[17] A. Rauber, E. Pampalk, and D. Merkl. The SOM-\nenhanced JukeBox: Organization and visualization of\nmusic collections based on perceptual models. Journal\nof New Music Research , 32(2):193–210, June 2003.\n[18] D. Rizo, P.J. Ponce de Le ´on, C. P ´erez-Sancho, A. Per-\ntusa, and J.M. I ˜nesta. A pattern recognition approach\nfor melody track selection in midi ﬁles. In Proc. IS-\nMIR, pages 61–66, Victoria, Canada, 2006.\n[19] X. Rodet. Musical sound signals analysis/synthesis:\nSinusoidal+residual and elementary waveform mod-\nels.Applied Signal Processing , 4:131–141, 1997.\n[20] G. Tzanetakis. Manipulation, Analysis and Retrieval\nSystems for Audio Signals . PhD thesis, Computer Sci-\nence Department, Princeton University, 2002.\n[21] I.H. Witten and E. Frank. Data Mining: Practical ma-\nchine learning tools and techniques . Morgan Kauf-\nmann, San Francisco, 2nd edition, 2005."
    },
    {
        "title": "A Query by Humming System that Learns from Experience.",
        "author": [
            "David Little 0001",
            "David Raffensperger",
            "Bryan Pardo"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416642",
        "url": "https://doi.org/10.5281/zenodo.1416642",
        "ee": "https://zenodo.org/records/1416642/files/LittleRP07.pdf",
        "abstract": "Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries.  This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user.",
        "zenodo_id": 1416642,
        "dblp_key": "conf/ismir/LittleRP07",
        "keywords": [
            "Query-by-Humming",
            "sung or hummed query",
            "search for related musical themes",
            "robust QBH system",
            "adapt to different singers",
            "deployed systems",
            "user provided feedback",
            "improve performance",
            "straight-forward genetic algorithm",
            "significant improvement"
        ],
        "content": "A QUERY BY HUMMING SYSTEM THAT LEARNS FROM EXPERIENCEDavid Little, David Raffensperger, Bryan Pardo EECS Department Northwestern University Evanston, IL 60201 d-little,d-raffensperger,pardo@ northwestern.edu ABSTRACT Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries.  This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user. 1. INTRODUCTION Deployed search engines used to find music documents, such as amazon.com, rely on metadata about the song title and performer name as their indexing mechanism. Often, a person is able to sing a portion of the piece, but cannot specify the title, composer or performer. Query by humming (QBH) systems [1] solve this mismatch between database keys and user knowledge by matching a sung query to musical themes in a database, returning the most similar themes. One of the main difficulties in building an effective QBH system is dealing with the variation between sung queries and the melodies used as database search keys. Singers may go out of tune, sing at a different tempo than expected, or in a different key [1, 7]. Further, singers differ in their error profiles. One may have poor pitch, while another has poor rhythm.  Since it is not possible to predict all individual singer profiles before deployment, a robust QBH system should be able to adapt to different singers after deployment. Current QBH systems do not have this capability. While there has been significant prior work that addresses (or is applicable to) singer error modelling [7, 9, 11] for QBH, researchers have not focused on fully automated, ongoing QBH optimization after deployment. Thus, these approaches are unsuited for this task, requiring either hundreds of example queries to customize to an individual [7, 9], or training examples where the internal structure of each query is aligned by the trainer to the structure of the target [11].  \n Figure 1. System diagram We are developing a QBH system (Figure 1) that personalizes a singer model based on user feedback, learning the model on-line, after deployment without intervention from the system developers and after only a few example queries. The user sings a query (step 1 in the figure). The system returns a list of songs from the database, ranked by similarity (step 2). The user listens to the songs returned and selects the desired one (step 3). The more a person uses and corrects the system, the better the system performs. Our system employs user feedback to build a database of paired queries and correct targets (step 4). These pairings are used to optimize the parameters of our note segmentation and note interval similarity parameters for specific users (step 5) or groups of users.   In this paper, we focus on how we automatically optimize backend QBH system performance, given a small set of example queries. We refer the reader to [10] for a description of the user interface and user interaction.  2. QUERY REPRESENTATION In a typical QBH system, a query is first transcribed into a time-frequency representation where the fundamental frequency and amplitude of the audio is estimated at very short fixed intervals (on the order of 10 milliseconds). We call this sequence of fixed-frame estimates of fundamental frequency a melodic contour representation. Figure 2 shows the melodic contour of a sung query as a dotted line. © 2007 Austrian Computer Society (OCG).     \n Figure 2. Example note intervals as <PI, ,LIR> pairs. We segment the melodic contour into notes and then use a note interval representation. The pitch of each note is the median value in its segment. Each note interval is represented by the pitch interval (PI) between adjacent note segments (encoded as un-quantized musical half-steps) and the log of the ratio between the length of a note segment and the length of the following segment (LIR) [8]. Figure 2 shows several note intervals as PI, LIR pairs.   This representation has several advantages over a melodic contour: it is both transposition and tempo invariant. It is also compact, only encoding salient points of change (note transitions), rather than every 10 millisecond frame. This results in a speed-up of two orders of magnitude when matching queries to targets. Work in [1] has shown that the precision and recall of search using quantized note intervals is slightly worse than when using melodic contour. We use unquantized note intervals. Use of unquantized PI and LIR values makes the representation insensitive to issues caused by a singer inadvertently singing in an unexpected tuning (A4 ≠ 440), or slowly changing tuning and tempo over the course of a query. This improves search performance relative to quantized note intervals. A previous study showed that use of unquantized note intervals significantly improved search performance compared to quantized note intervals [5]. 3. NOTE SEGMENTATION Our system first transcribes the query as a sequence of 10 millisecond frames. Each frame is a three element vector containing values for pitch, amplitude and harmonicity (relative strength of harmonic components to non harmonic components) [13]. We assume significant changes in these three features occur at note boundaries. Thus, we wish to determine what constitutes significant change. For example, a singer may use vibrato at times and not at other times. Thus, the amount of local pitch variation that constitutes a meaningful note boundary in one query may be insufficient to qualify as a note boundary in another query by the same singer. We wish to take local variance into account when determining whether or not a note boundary has occurred. Note segmentation is related to the problem of visual edge detection [3]. Accounting for local variation has been helped edge detection in cases where portions of the image may be blurry and other portions are sharp [3]. The Mahalanobis distance [6] differs from the Euclidean distance in that it normalizes distances over a covariance matrix M. Using the Mahalanobis lets one measure distance between frames relative to local variation. In a region of large variance, a sudden change will mean less than in a relatively stable region. A previous study showed that our use of the Mahalanobis over Euclidean distance significantly improved search performance [5] We find the distance between adjacent frames in the sequence using the Mahalanobis distance measure, shown in Equation 1. Given a frame fi, we assume a new note has begun wherever the distance between two adjacent frames fi and fi+1, exceeds a threshold, T.  \n! (fi\"fi+1)M\"1(fi\"fi+1# ) >T$new note (1) The matrix M is a covariance matrix, calculated from the variance within a rectangular window around the frame fi.  Our note segmenter has four tuneable parameters: the segmentation threshold (T), and the weights (w) for each of the three features (pitch, harmonicity and amplitude). We address tuning of these four parameters in Section 6. Once we have estimated note segment boundaries, we build note intervals from these note segments.  4. MODELING SINGER ERROR Once a query is encoded as a sequence of note intervals, we compare it to the melodies in our database. Each database melody is scored for similarity to the query using a dynamic-programming approach to performing string alignment [9]. Rather than use a fixed match reward, the match reward is based on a similarity function s for note intervals. Ideally we would like interval ai to be similar to interval bj if ai likely to be sung when a singer intended to sing bj. That is, likely errors should be considered similar to the correct interval, and unlikely errors should be less similar. Such a function lets a string-alignment algorithm correctly match error-prone singing to the correct target, as long as the singer is relatively consistent with the kinds of errors produced.   In previous work  [9], we had participants listen to note intervals and attempt reproduce the intervals by singing. This study showed that the most common errors were octave displacements of one or two octaves. The next most common errors were half-step and whole step errors around the expected note interval, or  around peaks offset by an octave.  This supports the observations of Shepard [12], who proposes a pitch chroma representation where octaves are relatively close to each other in the chroma space. This suggests that singing errors can be effectively modelled by a set of Gaussian distributions centered on the expected pitch interval and on intervals offset by one or more octaves. The normal function, N(a,µ,σ) returns the value for a given by a Gaussian function, centered on µ, with a standard deviation σ. Equation 4 shows our note-interval similarity function, based on the normal function.     \n||(,)(,,)(,12,) nirrrrppppinsxywNyxwNyxi!\"!=#=++$  (4) Let x and y be two note intervals.  Here, xp and yp are the pitch intervals of x and y respectively, and xr and yr are the rhythmic ratios (LIRs) of x and y. The values wp and wr are the weights assigned to pitch and rhythm. The sum of wp and wr is 1.  The pitch similarity is modeled using 2n+1  Gaussians, each centered at one or more octaves above or below the expected pitch interval. The height of each Gaussian is determined by an octave decay parameter λ, in the range from than 1 to 0. This similarity function provides us with five parameters to tune: the pitch and rhythm weight (wp and wr), the sensitivity to distances for pitch and rhythm (σp and σr), and the octave decay (λ). Figure 3 shows two octaves of the positive portion of the pitch dimension of this function, given two example parameter settings. \n Figure 3. The pitch dimension of the similarity function in Equation 5. 5. SYSTEM TRAINING We train the system by tuning the parameters of our note segmenter (Equations 1 and 2) and note similarity reward function (Equation 5). We measure improvement using the mean reciprocal rank (MRR) of a set of n queries. We define the rank of a query as the order of the correct song in the search results. MRR emphasizes the importance of placing correct target songs near the top of the list while still rewarding improved rankings lower down on the returned list of songs [1]. Values for MRR range from 1 to 0, with higher numbers indicating better performance. A MRR of 0.25 indicates the correct answer was, on average, in the top four songs returned by the search engine.  We use a simple genetic algorithm [14] to tune system parameters.  Each individual in the population is one set of parameter values for Equations 1, 2 and 5. The fitness function is the MRR of the parameter settings over a set of queries. The genetic algorithm represents each parameter as a binary fraction of 7 bits, scaled to a range of 0 to 1. We allow crossover to occur between (not within) parameters.  During each generation, the fitness of an individual is found based on the MRR of the correct targets for a set of queries. Parameter settings (individuals) with high MRR values are given higher probability of reproduction (fitness proportional reproduction).  6. EMPIRICAL EVALUATION Our empirical evaluation sought to evaluate the extent to which the system was able to improve search  performance in response to training, both in the case of individualized training to a particular singer and also general training over a larger set of singers. Our query set was drawn from the QBSH corpus [4] used during the 2006 MIREX comparison of query-by-humming systems [2]. We used 10 singers, each singing the same 15 songs from this dataset. Our target database was composed of the 15 targets corresponding to these queries plus 986 distracter melodies drawn from a selection of Beatles songs, folk songs and classical music, resulting in a database of 1001 melodies. Chance performance, on a database of this size would result in an MRR ≈ 0.005, given a uniform distribution.  For the genetic algorithm, we chose a population size of 60. Initial tests showed learning on this task typically ceases by the 30th generation, thus results shown here report values from training runs of 40 generations.  In practice, we would like to utilize user-specific training only when it improves performance relative to an un-personalized system. One simple option is to only use user-specific parameters if the user-specific performance (MRRu) is superior to the performance using parameters learned on a general set of queries by multiple users (MRRg). To test this idea, we first trained the system on all queries from nine of ten singers. We then tested on all the queries from the missing singer. Cross validation across singers was performed, thus the experiment was repeated ten times, testing with the queries from a different singer each time. To speed learning, training was done using a random sample of 250 target songs from the database. For each trial, the set of parameters with the best training performance was evaluated by finding the MRR of the testing queries, searching over all 1001 melodies in the database. This gave us parameters for each singer that were learned on the queries by the other nine singers. These are the general parameter settings for a singer.  The mean MRR testing performance of the general parameters was 0.235 (Std. Dev.=0.063).We then performed a user-specific version of training. We used 3-fold cross validation across 15 queries for each of the same ten singers used for training the general parameters: we optimized parameters on the selected ten queries and tested on the remaining five. This provided us with 30 total trails for the specific parameters: three trials for each of the ten singers. The mean MRR testing performance for the specific parameters was: 0.228 (Std Dev. = 0.14). For each trial we compared MRRs (the training performance of the learned user-specific parameters) to MRRg (the training performance of the general parameters learned from the other nine singers). If MRRs > MRRg + ε  on the training set, we used the user-specific parameters. Else, we used the general    parameters. For this experiment, ε was an error margin set to 0.04.    Once the parameters (general or user-specific) were selected, we tested them on the testing set for that trial. We called this a combined trial. The combined trials had an average MRR of 0.289 (Std. Dev. = 0.086). A t-test indicated the improvement of the combined results over the general and the specific parameter settings is statistically significant (p ≤ 0.024).  On 50% of the combined trials the specific parameters were used and improved performance compared to general parameters. On 13% of the trials, specific parameters were used, but had worse testing performance than the general parameters. On the remaining 36% of trials, the general parameters were used. Figure 4 shows the average MRR performance for untrained, general and combined parameters.  \n Figure 4. Average search performance using untrained, general and combined (both User-Specific and General) parameters. 7. CONCLUSIONS We have described a QBH system that automatically customizes parameters to individuals or groups after deployment. Our results show that by correctly combing parameters trained to specific users, and a set trained over a general population, these combined parameters significantly improve mean search performance, resulting in a mean MRR of 0.289 on a database of 1001 melodies. This roughly corresponds to consistently placing the correct target in the top four results. This compares to an MRR of 0.0151 (Std. Dev. = 0.0018) prior to training.  Our results also show unquantized note intervals and note segmentation that takes into account local pitch variation significantly improve performance.   In future work we will explore how performance varies with respect to the number of training examples and improve the information that can be used for training while maintaining user-specificity. We will also explore more sophisticated criteria to determine when user-specific training should be used. 8. REFERENCES [1]  R. Dannenberg, W. Birmingham, B. Pardo, N. Hu, C. Meek and G. Tzanetakis, \"A Comparative Evaluation of Search Techniques for Query-by-Humming Using the MUSART Testbed\", Journal of the American Society for Information Science and Technology (2007), pp. in press. [2]  J. S. Downie, K. West, A. Ehmann and E. Vincent, \"The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview\", 6th International Conference on Music Information Retrieval, September 11-15, London, UK, 2005. [3]  J. H. Elder and S. W. Zucker, \"Local scale control for edge detection and blur estimation\", Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20 (1998), pp. 699-716. [4]  Jyh-Shing and R. Jang, \"QBSH: A corups for Designing QBSH (Query by Singing/Humming) Systems\", 2006. [5]  D. Little, D. Raffensperger and B. Pardo, \"Online Training of a Music Search Engine\", Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007 [6]  P. C. Mahalanobis, \"On the generalised distance in statistics,  \" Proceedings of the National Institute of Science of India 12 (1936), pp. 49-55. [7]  C. Meek and W. Birmingham, \"A Comprehensive Trainable Error model for sung music queries\", Journal of Artificial Intelligence Research, 22 (2004), pp. 57-91. [8]  B. Pardo and W. Birmingham, \"Encoding Timing Information for Music Query Matching\", International Conference on Music Information Retrieval, Paris, France, 2002. [9]  B. Pardo, W. P. Birmingham and J. Shifrin, \"Name that Tune: A Pilot Study in Finding a Melody from a Sung Query\", Journal of the American Society for Information Science and Technology, 55 (2004), pp. 283-300. [10]  B. Pardo and D. Shamma, \"Teaching a Music Search Engine through Play\", CHI 2007, Computer/Human Interaction San Jose, California, 2007. [11]  C. Parker, A. Fern and P. Tadepalli, \"Gradient boosting for sequence alignment\", The Twenty-First National Conference on Artificial Intelligence, Boston, MA, 2006. [12]  R. N. Shepard, \"Geometrical Approximations to the structure of musical pitch\", Psychological Review, 89 (1982), pp. 305-309. [13]  G. Tzanetakis and F. Cook, \"A framework for audio analysis based on classification and temporal segmentation\", EUROMICRO Conference, Milan, 1999, pp. 61-67. [14]  A. Wright, \"Genetic algorithms for real parameter optimization\", The First workshop on the Foundations of Genetic Algorithms and Classier Systems, Bloomington, Indiana, 1990."
    },
    {
        "title": "A Web-Based Game for Collecting Music Metadata.",
        "author": [
            "Michael I. Mandel",
            "Daniel P. W. Ellis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414768",
        "url": "https://doi.org/10.5281/zenodo.1414768",
        "ee": "https://zenodo.org/records/1414768/files/MandelE07.pdf",
        "abstract": "We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and specific. Analysis of preliminary data shows that we are able to collect objective and specific descriptions of clips and that players tend to agree with one another. 1 MOTIVATION AND GAME PLAY The easiest way for people to find music is by describing it with words. Whether hearing about a new band from a friend, browsing a large catalog, or locating a specific song, verbal descriptions, although imperfect, generally suffice. While there are notable community efforts to verbally describe large corpora of music, only an automatic music description system can adequately label brand new, obscure, or unknown music. To train such a system, however, requires human generated descriptions. Thus in this project, we endeavor to collect ground truth about specific, objective aspects of music by asking humans to describe short musical excerpts, which we call clips, in the context of a web-based game 1 . Such a game entertains people while simultaneously collecting useful data. Not only is the data collected interesting, but the game itself makes novel contributions to the field of “human computation.” Here is an example of a how a player experiences the game. First she requests a new clip to be tagged. This clip could be one that other players have seen before or one that is brand new, she does not know which she will receive. She listens to the clip and describes it with a few words: harp, female, and sad. The word harp already has been used by exactly one other player, so it scores her one point. In addition, the player who first used it scores two points. The word female has already been used by at least two players, so it scores our player zero points. The word sad has not been used by anyone before, so it scores 1 The game is available to play at: http://game.majorminer.com c⃝2007 Austrian Computer Society (OCG). no points immediately, but has the potential to score two points should another player subsequently use it. The player then goes to her game summary. The summary shows both clips that she has recently seen and those that she has recently scored on, e.g. if another user has agreed with one of her tags. It also reveals the artist, album, and track names of each clip and allows the user to see another user’s tags for each clip. The next time she logs in, the system informs her that three of her descriptions have been used by other players in the interim, scoring her six points while she was gone. A number of authors have explored the link between music and text, especially Whitman [4]. More recently, [2] has applied ideas from the image retrieval literature to associate text with music. In the ESP Game [3] pairs of players describe the same image and score points when they agree. This game popularized the idea of allowing free form responses that only score points when verified. 2 DESIGN CONSIDERATIONS We designed the game with many goals in mind. Our main goal, which shaped the design of the scoring rules, was to encourage users to describe the music thoroughly, to be original, yet relevant. Our second goal, which informed the method for picking clips to show, was for the game to be fun for both new and veteran users. We also wanted to avoid cheating, collusion, or other manipulations of the scoring system or, worse, the data collected. While games like the ESP game pair a player with a single partner, ours in a sense teams a player with all of the other players who have ever seen a particular clip. It is possible that a pair of players could vary widely in skill level or familiarity with the clip under consideration, frustrating both players. The non-paired format allows the most creative or expert players to cooperate with each other asynchronously. It also allows the systematic",
        "zenodo_id": 1414768,
        "dblp_key": "conf/ismir/MandelE07",
        "keywords": [
            "web-based game",
            "collecting descriptions",
            "fun",
            "easy",
            "objective",
            "participants",
            "score points",
            "rules",
            "thorough",
            "original"
        ],
        "content": "A WEB-BASED GAME FOR COLLECTING MUSIC METADATA\nMichael I Mandel\nColumbia University\nLabROSA, Dept. Electrical Engineering\nmim@ee.columbia.eduDaniel P W Ellis\nColumbia University\nLabROSA, Dept. Electrical Engineering\ndpwe@ee.columbia.edu\nABSTRACT\nWe have designed a web-based game to make collecting\ndescriptions of musical excerpts fun, easy, useful, and ob-\njective. Participants describe 10 second clips of songs and\nscore points when their descriptions match those of other\nparticipants. The rules were designed to encourage users\nto be thorough and the clip length was chosen to make\njudgments more objective and speciﬁc. Analysis of pre-\nliminary data shows that we are able to collect objective\nand speciﬁc descriptions of clips and that players tend to\nagree with one another.\n1 MOTIVATION AND GAME PLAY\nThe easiest way for people to ﬁnd music is by describing\nit with words. Whether hearing about a new band from\na friend, browsing a large catalog, or locating a speciﬁc\nsong, verbal descriptions, although imperfect, generally\nsufﬁce. While there are notable community efforts to ver-\nbally describe large corpora of music, only an automatic\nmusic description system can adequately label brand new,\nobscure, or unknown music. To train such a system, how-\never, requires human generated descriptions.\nThus in this project, we endeavor to collect ground\ntruth about speciﬁc, objective aspects of music by ask-\ning humans to describe short musical excerpts, which we\ncall clips, in the context of a web-based game1. Such\na game entertains people while simultaneously collecting\nuseful data. Not only is the data collected interesting, but\nthe game itself makes novel contributions to the ﬁeld of\n“human computation.”\nHere is an example of a how a player experiences the\ngame. First she requests a new clip to be tagged. This\nclip could be one that other players have seen before or\none that is brand new, she does not know which she will\nreceive. She listens to the clip and describes it with a few\nwords: harp,female , and sad. The word harp already\nhas been used by exactly one other player, so it scores her\none point. In addition, the player who ﬁrst used it scores\ntwo points. The word female has already been used by at\nleast two players, so it scores our player zero points. The\nword sadhas not been used by anyone before, so it scores\n1The game is available to play at: http://game.majorminer.com\nc/circlecopyrt2007 Austrian Computer Society (OCG).no points immediately, but has the potential to score two\npoints should another player subsequently use it.\nThe player then goes to her game summary. The sum-\nmary shows both clips that she has recently seen and those\nthat she has recently scored on, e.g. if another user has\nagreed with one of her tags. It also reveals the artist, al-\nbum, and track names of each clip and allows the user\nto see another user’s tags for each clip. The next time\nshe logs in, the system informs her that three of her de-\nscriptions have been used by other players in the interim,\nscoring her six points while she was gone.\nA number of authors have explored the link between\nmusic and text, especially Whitman [4]. More recently,\n[2] has applied ideas from the image retrieval literature to\nassociate text with music. In the ESP Game [3] pairs of\nplayers describe the same image and score points when\nthey agree. This game popularized the idea of allowing\nfree form responses that only score points when veriﬁed.\n2 DESIGN CONSIDERATIONS\nWe designed the game with many goals in mind. Our main\ngoal, which shaped the design of the scoring rules, was to\nencourage users to describe the music thoroughly, to be\noriginal, yet relevant. Our second goal, which informed\nthe method for picking clips to show, was for the game to\nbe fun for both new and veteran users. We also wanted\nto avoid cheating, collusion, or other manipulations of the\nscoring system or, worse, the data collected.\nWhile games like the ESP game pair a player with a\nsingle partner, ours in a sense teams a player with all of\nthe other players who have ever seen a particular clip.\nIt is possible that a pair of players could vary widely in\nskill level or familiarity with the clip under considera-\ntion, frustrating both players. The non-paired format al-\nlows the most creative or expert players to cooperate with\neach other asynchronously. It also allows the systematic\nintroduction of new clips, avoiding a “cold start.” These\nbeneﬁts come at the price of vulnerability to asynchronous\nversions of the attacks that afﬂict paired games.\nThe design of the game’s scoring rules reﬂects our ﬁrst\ngoal, to encourage users to thoroughly describe clips. To\nfoster relevance, users only score points when other users\nagree with them. To encourage originality, users are given\nmore points for being the ﬁrst to use a particular descrip-\ntion on a given clip and are given no points for a tag thatLabel Veriﬁed Label Veriﬁed\ndrums 793 vocals 120\nguitar 720 jazz 120\nmale 615 voice 119\nrock 571 vocal 118\nsynth 429 hip hop 118\nelectronic 414 slow 112\npop 375 80s 94\nbass 363 beat 89\nfemale 311 fast 84\ndance 297 drum machine 83\ntechno 224 british 68\nelectronica 155 country 65\npiano 153 soft 58\nrap 140 instrumental 55\nsynthesizer 136 house 53\nTable 1 . The 30 most popular tags and the number of clips\non which each was veriﬁed by two players.\ntwo players have already agreed upon. Currently, the ﬁrst\nplayer to use a particular tag on a clip scores two points\nwhen it is veriﬁed by a second player, who scores one\npoint. By carefully choosing when clips are shown to\nplayers, we can adjust the difﬁculty of scoring and use the\ntension created by the scoring rules to inspire originality\nwithout inducing frustration.\nWhen a player requests a new clip, we have the free-\ndom to return whatever clip we like and we adjust the\nchoice based on the players experience level. We either\ndraw a clip from the pool of clips that have been seen by\nother players, which are the only clips on which immedi-\nate scoring is possible, or a brand new clip, introducing it\ninto that pool. For players who have not seen many clips\nyet, we draw clips from this pool to facilitate immediate\nscoring. For players who have seen a fraction, γ, of this\npool, we usually draw clips that have already been seen,\nbut with probability γdraw a brand new clip. While new\nclips do not allow immediate scoring, they do offer the\nopportunity to be the ﬁrst to use many tags, thus scoring\nmore points when others agree later.\nOnce a player has labeled a clip, he has the opportu-\nnity to see the name of the song and the performer along\nwith the labels that another player has used to describe\nthat same clip. We choose to reveal the labels of the ﬁrst\nplayer to describe a given clip, these labels will remain the\nsame no matter how many subsequent players see the clip.\nBecause of the clip choice described above, the labels re-\nvealed are likely to be those of an experienced user and\ncan then serve as exemplars for new players.\n3 DATA COLLECTED\nThe type of music present in the database affects the labels\nthat are collected and our music comes from four sources.\nThe ﬁrst, and biggest source, contained electronic music,\ndrum and bass, post-punk, brit pop, and indie rock. The\nsecond contained indie rock and hip hop. The third con-\ntained pop, country, and contemporary rock. And the last\ncontained jazz.\nAt the time of this paper’s writing, the site had been livefor 3 months, in which 361 users had registered. A total of\n2183 clips had been labeled selected at random from 2547\ntracks, each with an average of 25.0 clips. Each clip had\non average been seen by 6.03 users, and described with\n27.56 tags, 4.48 of which had been veriﬁed. See Table 1\nfor some of the most frequently used descriptions.\nCertain patterns are observable in the collected descrip-\ntions. As can be seen in Table 1, the most popular tags de-\nscribe genre, instrumentation, and the gender of the singer,\nif there are vocals. People do use descriptive words, like\nsoft, loud, quiet, fast, slow, and repetitive, but less fre-\nquently. Emotional words are less popular, perhaps be-\ncause they are difﬁcult to verbalize in a way that others\nwill likely agree with. And there are hardly any words\ndescribing rhythm, except for an occasional beat. Lyrics\nalso prove to be useful tags, and a corpus of music labeled\nwith lyrics might facilitate lyric transcription.\nPlayers also use the names of artists they recognize.\nFor example, cure has been veriﬁed 12 times, bowie 8\ntimes, and radiohead 6 times. Of the clips veriﬁed as\nbowie , however, three were performed by Gavin Friday,\nSuede, and Pulp. This label most likely indicates songs\nthat sound like David Bowie’s music regardless of the ac-\ntual performer. Artists used in this way could be the an-\nchors in an “anchor space” where music is described by\nits similarity to that of well known artists [1].\n4 FUTURE WORK\nThere is much that we would like to do with this data in\nthe future: train models to automatically describe music,\nanalyze the similarities between clips, between users, and\nbetween words, investigate ways to combine audio-based\nand word-based music similarity to help improve both, use\nautomatic descriptions as features for further manipula-\ntion, investigate an anchor space built from the data col-\nlected here, use descriptions of clips to help determine the\nstructure of songs, and so forth.\n4.1 Acknowledgments\nThanks to Marios Athineos, Graham Poliner, Neeraj Kumar, and Jo-\nhanna Devaney. This work was supported by the Fu Foundation School\nof Engineering and Applied Science via a Presidential Fellowship, and\nby the Columbia Academic Quality Fund, and by the National Science\nFoundation (NSF) under Grant No. IIS-0238301. Any opinions, ﬁnd-\nings and conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reﬂect the views of the NSF.\n5 REFERENCES\n[1] Adam Berenzweig, Daniel PW Ellis, and Steve Lawrence. Anchor\nspace for classiﬁcation and similarity measurement of music. In Proc\nIntl Conf on Multimedia and Expo (ICME) , 2003.\n[2] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. Modeling\nmusic and words using a multi-class naive bayes approach. In Proc\nIntl Symp Music Information Retrieval , October 2006.\n[3] Luis von Ahn and Laura Dabbish. Labeling images with a computer\ngame. In Proc SIGCHI conference on Human factors in computing\nsystems , pages 319 – 326, 2004.\n[4] Brian Whitman and Daniel PW Ellis. Automatic record reviews. In\nProc Intl Symp Music Information Retrieval , 2004."
    },
    {
        "title": "Visualizing Music: Tonal Progressions and Distributions.",
        "author": [
            "Arpi Mardirossian",
            "Elaine Chew"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417773",
        "url": "https://doi.org/10.5281/zenodo.1417773",
        "ee": "https://zenodo.org/records/1417773/files/MardirossianC07.pdf",
        "abstract": "This paper presents a music visualization tool that shows the tonal progression in, and tonal distribution of, a piece of music on Lerdahl’s two-dimensional tonal pitch space. The method segments a piece into uniform time slices, and determines the most likely key in each slice. It then generates the visualization by dynamically showing the sequence of keys as translucent, growing discs on the twodimensional plane. The frequency of a key is indicated by the size of its colored disc. Each color and position corresponds to a key, and related keys are shown in proximity with related colors. The visual result effectively presents the changing distribution of the keys employed. The proposed visualization is an improvement over more basic charting methods, such as histograms, and it maintains standards of information design in the form of added dimensionality, color, and animation. We show that the visualization is invariant under music transformations that preserve the piece’s identity. We conclude by illustrating how this method may be used to visually distinguish between tonal progression and distribution patterns in western classical versus Armenian folk music. 1 INTRODUCTION Music visualization literature can be broadly grouped into two categories: visualization of individual pieces of music (our focus), and of collections of pieces. It can be said that the first form of music visualization created for individual pieces was music notation itself. An experienced musician can often look at the score of a piece and “see” what the music sounds like. Music notation cannot be used readily as a mainstream form of visualization because it can take years of training to learn to decipher the subtleties of the encoded information. Our goal is to create a more intuitive visualization that reveals important features of the music that may not be readily audible to the inexperienced ear. The challenge with developing such a visualization is that music is complex, consisting of multiple inter-related features. A successful visualization must strike a balance between simplicity and comprehensiveness. We aim to create imagery c⃝2007 Austrian Computer Society (OCG). that is both intuitive and informative. In this paper, we propose a visual interface based on Lerdahl’s Tonal Pitch Space [12], which portrays all major and minor keys on a two-dimensional (2D) plane. The distributions of keys are indicated as growing colored discs, where the colors correspond to the keys detected, and the size of the discs to the key frequency. Figure 1 shows the visual interface. The user selects the piece and the granularity of analysis through the graphical user interface. Figure 1. Snapshot of Visualization Interface In our previous work ([14, 15]), we investigated how key distributions could be successfully used to assess similarity between pieces, demonstrating that key distributions, although a summarization of the musical content, can serve as good representations of pieces. The current visualization method is an extension and improvement of the key distribution approach, expanding and adding richness to the simple histogram representation through an increase in dimensionality, addition of color, and animation. According to Tufte [21], an acknowledged expert in information design and visual literacy, increasing the number of dimensions of a visualization sharpens the information resolution. In the histogram, the keys were shown on a one-dimensional line, while in the new visual interface, the keys (all major and minor keys) are shown on a 2D plane, thus capturing the network of inter-relations amongst keys. The frequency of the keys (the third dimension) is shown in the size of the discs. Furthermore, the progression of disc growth shows the range of movement of keys within the piece over time. Hence, we have essentially four dimensions of information captured in a dynamic 2D interface. Tufte states that representations that progress, such as the proposed animated visualization, can be referred to as ‘small multiple designs’ and “answer directly [the question of ‘compared to what?’] by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives.” The proposed visual interface incorporates these ideas of small multiple design by taking a sequence of keys and showing the evolution frame-byframe. This dynamic visualization allows one to see the sequential progression of keys, an important component in communicating with music. Third, Tufte states the fundamental uses of color in information design as being: to label (color as noun), to measure (color as quantity), to imitate reality (color as representation), and to enliven or decorate (color as beauty). In our visualization, color labels by distinguishing between keys, measures by displaying the amount of time spent in each key, imitates reality by showing the relationship between keys, and decorates since the same visualization in black and white would not be nearly as visually pleasing. The remainder of the paper is organized as follows: Section 2 presents related work in music visualization, Section 3 describes our visualization system, followed by validation of the visualization through translation analysis in Section 4, and demonstration of the visualization system in Section 5. Section 6 presents our conclusions. 2 RELATED WORK This section reviews a selection of the many music visualization systems developed so as to put the work presented here in perspective. As noted above, visualizations can be broadly categorized into visualizations of collections and individual pieces. Since our work does not consider collections, this review will be limited to visualizations of individual pieces. These systems may be further subcategorized as follows: representations of direct versus interpreted data, and static versus dynamic presentations. Direct data refers to data that is extracted directly from the music (such as pitch and onset time), while interpreted data refers to information that must be determined from extracted data (for example, tempo and key). Let us consider visualizations of direct data. The most basic visualizations in this category are 2D waveforms and spectrograms which usually show time on the x-axis, and have primary values of interest on the y-axis. Additional mappings of these primary values are often shown using color or greyscale ranges. Misra, Wang, and Cook [17] present such visualizations (real time) with some added features and dimensionality. Another example of direct data visualization is Malinowski’s “Music Animation Machine” [13], which dynamically shows notes in a simplified piano roll representation. We now turn our attention to systems that visualize interpreted data. We first review static systems. One approach to music visualization is to create self-similarity maps. In the work developed by Cooper & Foote [8], the acoustic similarity between all instants of an audio recording is calculated and displayed on a 2D grid. Similar or repeating elements are visually distinct, allowing identification of structural and rhythmic characteristics. Another self-similarity visualization by Wattenberg et. al. [19] displays musical form as a sequence of translucent arches. Each arch connects two repeated, identical passages of a composition. By using repeated passages as landmarks, the maps reveal deep structures in musical compositions. An early work by Cohn [7] established mappings of music onto the harmonic network (also known as the tonnetz). We now transition to visualizations of interpreted data that are also dynamic. Related to the harmonic network visualization is Toiviainen & Krumhansl’s [20] visualization of listeners’ continuous ratings of tonal contexts on a toroid representation of keys (shown in 2D). Their work measures and models real-time responses using selforganizing maps. Chouvel [5] showed tonal analyses of a number of pieces on a hexagonal version of the tonnetz. Gomez & Bonada [10] developed a tool to visualize the tonal content of polyphonic audio signals. This tool includes different views that may be used for the analysis of tonal content of a music piece through visualization of chord and key estimation, and tonal similarity assessment. Sapp [18] developed a multi-timescale visualization technique for displaying the output from key-finding algorithms. In his visualization, the horizontal axis represents time in the score, while the vertical axis represents the duration of an analysis window used to select music for the key-finding algorithm. Each analysis window result is colored according to the determined key. The following works also maintain history information. Langer & Goebl [11] introduced a method for displaying tempo and loudness variations of expressive music performance. In this visualization, a dot moves through a 2D space representing tempo (x-axis) and loudness (yaxis), leaving behind a trace of the recent trajectory that may be interpreted as the performance path. Chew & Franc¸ois [4] developed a visual environment in which tonal information from musical performances are mapped, in real time, to a three-dimensional representation of tonal space. Their MuSA.RT analysis and visualization system also portrays musical memory as a trajectory that touches on the recently visited tonal regions. Our current approach can be considered a 2D counterpart of this work, with the difference that it shows not only the keys as they unfold, it also portrays the cumulative key information as dynamically varying spatial distributions of colored discs. 3 SYSTEM DESCRIPTION This section describes the components of our music visualization method, which displays the progression of the tonal content of a music piece. We begin by slicing a piece of music into segments of uniform time length, and determining the key for each segment using a key-finding algorithm. We then map the sequence of keys onto a 2D space that contains points representing all possible keys.",
        "zenodo_id": 1417773,
        "dblp_key": "conf/ismir/MardirossianC07",
        "keywords": [
            "Music visualization tool",
            "Lerdahls two-dimensional tonal pitch space",
            "Uniform time slices",
            "Most likely key determination",
            "Dynamic visualization",
            "Translucent growing discs",
            "Frequency of a key",
            "Color and position",
            "Music transformations",
            "Western classical versus Armenian folk music"
        ],
        "content": "VISUALIZING MUSIC: TONAL PROGRESSIONS AND DISTRIBUTIONS\nArpi Mardirossian and Elaine Chew\nUniversity of Southern California Viterbi School of Engineering\nEpstein Department of Industrial and Systems Engineering\nIntegrated Media Systems Center\nLos Angeles, CA 90089, USA\nABSTRACT\nThis paper presents a music visualization tool that shows\nthe tonal progression in, and tonal distribution of, a piece\nof music on Lerdahl’s two-dimensional tonal pitch space.\nThe method segments a piece into uniform time slices, and\ndetermines the most likely key in each slice. It then gen-\nerates the visualization by dynamically showing the se-\nquence of keys as translucent, growing discs on the two-\ndimensional plane. The frequency of a key is indicated\nby the size of its colored disc. Each color and position\ncorresponds to a key, and related keys are shown in prox-\nimity with related colors. The visual result effectively\npresents the changing distribution of the keys employed.\nThe proposed visualization is an improvement over more\nbasic charting methods, such as histograms, and it main-\ntains standards of information design in the form of added\ndimensionality, color, and animation. We show that the\nvisualization is invariant under music transformations that\npreserve the piece’s identity. We conclude by illustrating\nhow this method may be used to visually distinguish be-\ntween tonal progression and distribution patterns in west-\nern classical versus Armenian folk music.\n1 INTRODUCTION\nMusic visualization literature can be broadly grouped into\ntwo categories: visualization of individual pieces of music\n(our focus), and of collections of pieces. It can be said that\nthe ﬁrst form of music visualization created for individual\npieces was music notation itself. An experienced musician\ncan often look at the score of a piece and “see” what the\nmusic sounds like. Music notation cannot be used readily\nas a mainstream form of visualization because it can take\nyears of training to learn to decipher the subtleties of the\nencoded information.\nOur goal is to create a more intuitive visualization that\nreveals important features of the music that may not be\nreadily audible to the inexperienced ear. The challenge\nwith developing such a visualization is that music is com-\nplex, consisting of multiple inter-related features. A suc-\ncessful visualization must strike a balance between sim-\nplicity and comprehensiveness. We aim to create imagery\nc/circlecopyrt2007 Austrian Computer Society (OCG).that is both intuitive and informative.\nIn this paper, we propose a visual interface based on\nLerdahl’s Tonal Pitch Space [12], which portrays all major\nand minor keys on a two-dimensional (2D) plane. The dis-\ntributions of keys are indicated as growing colored discs,\nwhere the colors correspond to the keys detected, and the\nsize of the discs to the key frequency. Figure 1 shows the\nvisual interface. The user selects the piece and the granu-\nlarity of analysis through the graphical user interface.\nFigure 1 . Snapshot of Visualization Interface\nIn our previous work ([14, 15]), we investigated how\nkey distributions could be successfully used to assess sim-\nilarity between pieces, demonstrating that key distribu-\ntions, although a summarization of the musical content,\ncan serve as good representations of pieces. The current\nvisualization method is an extension and improvement of\nthe key distribution approach, expanding and adding rich-\nness to the simple histogram representation through an in-\ncrease in dimensionality, addition of color, and animation.\nAccording to Tufte [21], an acknowledged expert in in-\nformation design and visual literacy, increasing the num-\nber of dimensions of a visualization sharpens the infor-\nmation resolution. In the histogram, the keys were shown\non a one-dimensional line, while in the new visual inter-\nface, the keys (all major and minor keys) are shown on\na 2D plane, thus capturing the network of inter-relations\namongst keys. The frequency of the keys (the third di-\nmension) is shown in the size of the discs. Furthermore,\nthe progression of disc growth shows the range of move-\nment of keys within the piece over time. Hence, we have\nessentially four dimensions of information captured in a\ndynamic 2D interface.Tufte states that representations that progress, such as\nthe proposed animated visualization, can be referred to as\n‘small multiple designs’ and “answer directly [the ques-\ntion of ‘compared to what?’] by visually enforcing com-\nparisons of changes, of the differences among objects, of\nthe scope of alternatives.” The proposed visual interface\nincorporates these ideas of small multiple design by taking\na sequence of keys and showing the evolution frame-by-\nframe. This dynamic visualization allows one to see the\nsequential progression of keys, an important component\nin communicating with music.\nThird, Tufte states the fundamental uses of color in in-\nformation design as being: to label (color as noun), to\nmeasure (color as quantity), to imitate reality (color as\nrepresentation), and to enliven or decorate (color as beauty).\nIn our visualization, color labels by distinguishing between\nkeys, measures by displaying the amount of time spent in\neach key, imitates reality by showing the relationship be-\ntween keys, and decorates since the same visualization in\nblack and white would not be nearly as visually pleasing.\nThe remainder of the paper is organized as follows:\nSection 2 presents related work in music visualization,\nSection 3 describes our visualization system, followed by\nvalidation of the visualization through translation analysis\nin Section 4, and demonstration of the visualization sys-\ntem in Section 5. Section 6 presents our conclusions.\n2 RELATED WORK\nThis section reviews a selection of the many music visual-\nization systems developed so as to put the work presented\nhere in perspective. As noted above, visualizations can\nbe broadly categorized into visualizations of collections\nand individual pieces. Since our work does not consider\ncollections, this review will be limited to visualizations\nof individual pieces. These systems may be further sub-\ncategorized as follows: representations of direct versus\ninterpreted data, and static versus dynamic presentations.\nDirect data refers to data that is extracted directly from the\nmusic (such as pitch and onset time), while interpreted\ndata refers to information that must be determined from\nextracted data (for example, tempo and key).\nLet us consider visualizations of direct data. The most\nbasic visualizations in this category are 2D waveforms and\nspectrograms which usually show time on the x-axis, and\nhave primary values of interest on the y-axis. Additional\nmappings of these primary values are often shown using\ncolor or greyscale ranges. Misra, Wang, and Cook [17]\npresent such visualizations (real time) with some added\nfeatures and dimensionality. Another example of direct\ndata visualization is Malinowski’s “Music Animation Ma-\nchine” [13], which dynamically shows notes in a simpli-\nﬁed piano roll representation.\nWe now turn our attention to systems that visualize in-\nterpreted data. We ﬁrst review static systems. One ap-\nproach to music visualization is to create self-similarity\nmaps. In the work developed by Cooper & Foote [8], the\nacoustic similarity between all instants of an audio record-ing is calculated and displayed on a 2D grid. Similar or\nrepeating elements are visually distinct, allowing identiﬁ-\ncation of structural and rhythmic characteristics. Another\nself-similarity visualization by Wattenberg et. al. [19] dis-\nplays musical form as a sequence of translucent arches.\nEach arch connects two repeated, identical passages of a\ncomposition. By using repeated passages as landmarks,\nthe maps reveal deep structures in musical compositions.\nAn early work by Cohn [7] established mappings of\nmusic onto the harmonic network (also known as the ton-\nnetz). We now transition to visualizations of interpreted\ndata that are also dynamic. Related to the harmonic net-\nwork visualization is Toiviainen & Krumhansl’s [20] visu-\nalization of listeners’ continuous ratings of tonal contexts\non a toroid representation of keys (shown in 2D). Their\nwork measures and models real-time responses using self-\norganizing maps. Chouvel [5] showed tonal analyses of a\nnumber of pieces on a hexagonal version of the tonnetz .\nGomez & Bonada [10] developed a tool to visualize the\ntonal content of polyphonic audio signals. This tool in-\ncludes different views that may be used for the analysis\nof tonal content of a music piece through visualization\nof chord and key estimation, and tonal similarity assess-\nment. Sapp [18] developed a multi-timescale visualization\ntechnique for displaying the output from key-ﬁnding algo-\nrithms. In his visualization, the horizontal axis represents\ntime in the score, while the vertical axis represents the\nduration of an analysis window used to select music for\nthe key-ﬁnding algorithm. Each analysis window result is\ncolored according to the determined key.\nThe following works also maintain history informa-\ntion. Langer & Goebl [11] introduced a method for dis-\nplaying tempo and loudness variations of expressive mu-\nsic performance. In this visualization, a dot moves through\na 2D space representing tempo (x-axis) and loudness (y-\naxis), leaving behind a trace of the recent trajectory that\nmay be interpreted as the performance path. Chew &\nFranc ¸ois [4] developed a visual environment in which tonal\ninformation from musical performances are mapped, in\nreal time, to a three-dimensional representation of tonal\nspace. Their MuSA.RT analysis and visualization system\nalso portrays musical memory as a trajectory that touches\non the recently visited tonal regions. Our current approach\ncan be considered a 2D counterpart of this work, with the\ndifference that it shows not only the keys as they unfold,\nit also portrays the cumulative key information as dynam-\nically varying spatial distributions of colored discs.\n3 SYSTEM DESCRIPTION\nThis section describes the components of our music vi-\nsualization method, which displays the progression of the\ntonal content of a music piece. We begin by slicing a piece\nof music into segments of uniform time length, and deter-\nmining the key for each segment using a key-ﬁnding algo-\nrithm. We then map the sequence of keys onto a 2D space\nthat contains points representing all possible keys.3.1 Segmentation\nWe begin by segmenting each piece into a given number of\nsegments, m, of uniform length. The value mis selected\nby the user by moving the slider on the left-hand-side of\nthe display panel. It controls the level of detail, and de-\ngree of stability, of the visualizations. As mincreases, so\ndoes the level of granularity of the information displayed.\nNote that there are alternate methods of segmentation, in-\ncluding natural and sliding window methods, that may be\nconsidered in future work.\n3.2 Key Determination\nOnce a piece is segmented, the key of each segment must\nbe determined. While any key-ﬁnding algorithm may be\ninvoked to identify the keys (see [16] for references to\nkey-ﬁnding algorithms), we utilize the Spiral Array Cen-\nter of Effect Generator (CEG) algorithm [2, 1]. The Spi-\nral Array is a geometric model for tonality that represents\ntonal elements, such as pitch classes and keys, using a set\nof nested helixes. The collection of pitches of a piece are\nmapped to their corresponding positions in the pitch class\nspiral using a pitch spelling algorithm [3]. An aggregate\nposition of these positions is obtained by weighting each\npitch class representation by its proportional duration in\nthe segment. The key is then determined through a near-\nest neighbor search for the nearest key presentation on the\nmajor and minor key helixes. This key ﬁnding algorithm\ncan be used for both MIDI and audio input [16, 1, 6]. Even\nthough we illustrate the visualization using MIDI data, the\nmethod extends to audio music visualization as well.\n3.3 Tonal Pitch Space\nIn music theory, pitch spaces model relationships between\npitches based on the degree of relatedness among them,\nwith closely related pitches placed near one another, and\nless closely related pitches placed farther apart. Models of\npitch space may be in the form of graphs, groups, lattices,\nor geometrical ﬁgures such as helixes. For our visualiza-\ntion method, we use Lerdahl’s 2D representation of major\nand minor keys in his Tonal Pitch Space [12].\nRefer to Table 1 for a depiction of Lerdahl’s key space;\nmajor keys are notated in capital letters while minor keys\nare not. In this arrangement of keys, the circle of ﬁfths\nis placed on the horizontal axis while relative and parallel\nmajor/minor relationships alternate along the vertical axis.\nRecall that the circle of ﬁfths depicts relationships among\nthe 12 pitch classes comprising the scale. The relative\nminor of a particular major key (or the relative major of\na minor key) is the key which has the same key signature\nbut a different tonic. The parallel minor of a particular\nmajor key (or the parallel major of a minor key) is the\nminor key with the same tonic. The tonic is the ﬁrst note\nof a musical scale. Note that the Tonal Pitch Space may\nbe extended inﬁnitely as we cycle through all keys.d/sharpg/sharpc/sharpf/sharp b e a\nF/sharpB E A D G C\nf/sharp b e a d g c\nA D G C F B /flatE/flat\na d g c f b /flate/flat\nC F B /flatE/flatA/flatD/flatG/flat\nc f b /flate/flata/flatd/flatg/flat\nTable 1 . Tonal Pitch Space\n3.4 Color Selection\nEvery possible key is assigned a different color for visual-\nization. The circle of ﬁfths and the color wheel are merged\nto determine the color assignments. Figure 2 depicts the\ncircle of ﬁfths with each key assigned to a color from the\ncolor wheel. Keys on the outer ring represent major keys\nwhile keys on the inner ring represent minor keys. The\nmain idea of this color assignment is to have keys that are\nconsidered to be close one to another be assigned colors\nthat are also related. For example, C Major and A Minor\nare assigned a dark and light green respectively.\nFigure 2 . Color Assignments for Major and Minor Keys\n3.5 Animation\nThis section outlines the way the animated visualization\nlooks and progresses. The background of the visualization\ncontains points that represent the keys in the Tonal Pitch\nSpace. Each point is a different color according to the\ncoloring scheme outlined above. The visualization is syn-\nchronized with the music. As a music piece progresses,\nthe disc over the key of the present segment grows by one\nunit, indicating the key of that segment, and the cumula-\ntive information of the key distribution. Each time a key\nis re-visited, the disc over that point grows. At the end of\nthe piece, the visualization displays a 2D version of the\ndistribution of keys for the piece, with the size of discs\nrepresenting the frequency of the keys.\n3.6 User Interface\nThe visualization method outlined above has been imple-\nmented in an intuitive user interface to promote ease-of-use and to encourage the process of exploration and dis-\ncovery. Refer to Figure 1 for a snapshot of the interface.\nThe user can select to view the visualization synchronized\nwith the music, or without music replay, and a set delay\nbetween each frame. The user may also select the piece\nto visualize by clicking on the desired piece in the menu.\nThe last parameter controlled by the user is the segmenta-\ntion size, selected by moving the slider, the value of which\nranges from 5 to 60. The user may obtain any key name\nby placing the mouse over a point on the grid of keys.\n3.7 Example\nConsider the ﬁrst variation of Beethoven’s 32 Variations\nin C Minor (WoO80). Refer to Figure 3 for a frame-by-\nframe illustration of the visualization of this piece. The\nsegmentation parameter, m, was chosen to be 8, the num-\nber of bars in the piece. The sequence of identiﬁed keys\nfor the slices is as follows: C Minor, F Major, C Minor, C\nMajor, C Minor, C Minor, F Minor, C Minor. Each frame\nshows the up-to-date analysis of each slice. In each frame,\nthe disc corresponding to the key of the current segment\ngrows in size. For example, we know from the visualiza-\ntion that the piece begins and ends in the key of the piece\n(C Minor) because, in both the ﬁrst and last frame, the disc\ncorresponding to the C Minor point grows in size. Addi-\ntionally, recall that the Tonal Pitch Space has each key\nrepeated such that the window on the grid dictates which\nkeys will be shown multiple times. In this particular ex-\nample, there are no repeats because of the relatively small\nsize of each frame. In contrast, there are many repeated\nkeys (and key distribution patterns) in Figure 1.\nFigure 3 . Frame-by-Frame Visualization of Beethoven’s\nWoO80 First Variation\n4 VALIDATION\nThis section presents a formal validation of our visualiza-\ntion method. If a music visualization method aims to go\nbeyond being simply aesthetically pleasing, and strives to\ntransform music into a visual medium, then it must share\ncertain important characteristics with the music. We test\nwhether our proposed visualization method is in fact a\ngood mapping of music onto a visual space by consid-\nering its invariance under the transformations outlined byDorrell in [9], namely, pitch and octave translation, time\nand amplitude scaling, and time translation. These are the\ntypes of changes in music that do not inﬂuence human per-\nception in the recognition of a piece. For this analysis we\nconsider the theme of Mozart’s Ah, Vous Dirai-je, Maman\n(K265). The piece is segmented into 9 slices for the visu-\nalizations; Figure 4 shows the last visualization frame.\nFigure 4 . Last Frame of Visualization of Mozart’s K265\nTheme - Original Piece and Alterations\n4.1 Pitch Translation Invariance\nPitch Translation transposes a piece into a different key.\nTransposition does not alter the musical quality of a piece\nin any signiﬁcant way. In fact, we do not normally con-\nsider a piece transposed into a different key as being a\ndifferent piece. The patterns revealed by our visualiza-\ntion method remain intact, and are simply shifted over to\nthe area of the new key. Consider again the example of\nMozart’s K265 theme which is originally in the key of C\nMajor. We transposed it to the key of F Major. Refer to\nFigures 4(a) and 4(b) for the last frame of the visualiza-\ntion of the original and transposed piece respectively. In\ncontrast, a visualization method that uses only color and\nnot spatial position to label a key would result in less sim-\nilarity between the original and transposed pieces.\n4.2 Octave Translation Invariance\nOctave Translation refers to the transposition of a piece\ninto a different octave. It does not alter the quality of the\nmusic either, and could be considered a special type of\npitch transposition. Refer to Figure 4(c) for the last frame\nof the visualization of the example piece transposed down\none octave. Notice that since the points representing the\nkeys on the Tonal Pitch Space do not distinguish between\noctaves, the visualization is identical to the original. Oc-\ntave translation bears different similarities to the originalthan other transpositions. This is reﬂected in the visual-\nization, where octave translation has no effect while other\ntranspositions are indicated by a spatial translation.\n4.3 Time Scaling Invariance\nTime Scaling refers to the changing of the tempo. If a\npiece is played faster or slower, we recognize it as be-\ning the same piece. This is translated into the visualiza-\ntion in Figure 4(d), which shows a time-scaled version of\nMozart’s K265. We sped up the original piece by dou-\nbling its tempo. Since each piece is segmented into an\nequal number of segments, time-scaling has no effect on\nthe visualization. For both the original and fast version,\neach segment has the exact same content.\n4.4 Amplitude Scaling Invariance\nAmplitude Scaling refers to changing the volume of a piece.\nThis simply states that turning the volume up or down\ndoes not change the music. This could however have an\neffect on certain computation methods. Because our visu-\nalization method is based on tonal features, the amplitude\nhas no effect.\n4.5 Time Translation Invariance\nTime Translation refers to the time at which a piece is\nplayed. This is perhaps the most obvious of all the sym-\nmetries. A piece is exactly the same if it is played now, in\nﬁve minutes, or in a year. Our visualization will also look\nthe same for the same piece no matter when it is invoked.\n5 DEMONSTRATIONS\nWe now demonstrate the functionality of our visualization\nmethod with several examples. The ability to see the high\nlevel tonal progression of a piece over time, and its us-\nage of different tonalities, could provide insight into the\ndeep structures and nature of individual pieces, as well as\ndifferent genres of music.\n5.1 Classical Music\nClassical and popular western music have a common struc-\nture that we have come to expect. In general, classical\npieces begin in the key of the piece, then travel through\nthe terrain of various other keys, and ultimately return to\nthe original key at the end of the piece. These pieces can\nbe thought of as having a center ‘star’ around which the\npiece revolves even though there is variation in how far\na piece will stray from this center, and how often it will\nreturn to visit it through the course of the piece.\nAs an example, reconsider the visualization of the ﬁrst\nvariation of Beethoven’s Variations in C Minor (WoO80)\nshown in Figure 3. Notice, in the ﬁrst frame, that the piece\nbegins in C Minor (the key of the piece). The key then\ntravels to F Major, revisits C Minor, travels to C Major,revisits C Minor again, travels to F Minor, and ﬁnally re-\nturns to C Minor in the last frame.\n5.2 Armenian Music\nIn contrast to the general visual sequence and patterns laid\nout by classical music, Armenian traditional music gener-\nates a different pattern. Instead of having a center of in-\nterest, the visualization tool reveals a sequential pattern of\nkey progression that does not return to the original key.\nTypically, a piece begins in and stays in one key for a pe-\nriod of time, and then moves to a neighboring key. The\npiece typically does not end in the key in which it began.\nWe provide two examples of this tonal behavior.\nFigure 5 . Frame-by-Frame Visualization of Armenian\nfolk song ‘Amber Goran’\nConsider the Armenian folk song entitled ‘Amber Goran’\n(‘Lost Clouds’). Refer to Figure 5 for a frame-by-frame\nview of the visualization of this piece with m= 8. Notice\nhow the piece begins in F Major and remains there from\nframes 1 through 4, and then travels to F Minor, and stays\nthere for the remainder frames.\nFigure 6 . Frame-by-Frame Visualization of Armenian\ndance song ‘Mbarerk’\nNow consider a traditional Armenian dance song enti-\ntled ‘Mbarerk’ (‘Dances’). The visualization of this piece\n(m= 8) is shown in Figure 6. The piece stays in B Minor\nfor frames 1 to 5, then travels to D Major for frame 6, and\nends by traveling to G Major for frames 7 and 8.6 CONCLUSION\nMusic has movement that is intertwined with the compo-\nnent of time. The visualization method we have developed\nstrives to exhibit this same characteristic. Our method seg-\nments a piece of music into a given number of slices, de-\ntermines the keys for each slice, and dynamically displays\nthe keys on a 2D space. It also uses color to identify keys,\nthus adding further richness to the visualization.\nThe advantage of the proposed method is that it not\nonly indicates the tonality of each segment as it occurs, it\nalso shows the cumulative distribution of tonalities used\nthus far. The spatial arrangement and the animated pro-\ngression capture contextual change in a piece as well as\nthe relative contextual change. We have shown how this\nmethod maintains standards of information design, and is\na successful translation of music onto a visual space. We\nhave also given examples of how the visualizations may\nbe used for the comparison of pieces, for example, of dif-\nfering genres.\n7 ACKNOWLEDGEMENTS\nThis material is based upon work supported by a Uni-\nversity of Southern California (USC) Digital Dissertation\nFellowship, and by the National Science Foundation (NSF)\nunder grant No. 0347988. Any opinions, ﬁndings, and\nconclusions or recommendations expressed in this mate-\nrial are those of the authors, and do not necessarily reﬂect\nthe views of the USC Graduate School or NSF.\n8 REFERENCES\n[1] Chew, E. “Modeling Tonality: Applications to Music\nCognition”, Proceedings of the 23rd Annual Meeting\nof the Cognitive Science Society , 2001.\n[2] Chew, E. Towards a Mathematical Model of Tonality .\nPh.D. thesis, Massachusetts Institute of Technology,\n2000.\n[3] Chew, E. and Chen, Y .-C. “Real-Time Pitch Spelling\nUsing the Spiral Array”, Computer Music Journal ,\n2005.\n[4] Chew, E. and Franc ¸ois, A.R.J. “Interactive Multi-\nScale Visualizations of Tonal Evolution in MuSA.RT\nOpus 2”, Newton Lee (ed.): Special Issue on Music\nVisualization and Education, ACM Computers in En-\ntertainment , 2005.\n[5] Chouvel, J.-M. “Repr ´esentation harmonique hexago-\nnal toro ¨ıde”, Musim ´ediane−revue audiovisuelle et\nmultim ´edia d’analyse musicale , [Web site] December\n2005, URL: www.musimediane.com\n[6] Chuan, C.-H. and Chew, E. “Fuzzy Reasoning in Pitch\nClass Determination for Polyphonic Audio Key Find-\ning”, Proceedings of the 6th International Conference\nfor Music Information Retrieval , 2005.[7] Cohn, R. “Neo-Riemannian Operations, Parsimo-\nnious Trichords, and Their ‘Tonnetz’ Representa-\ntions”, Journal of Music Theory , 1997.\n[8] Cooper, M. and Foote, J. “Visualizing Musical Struc-\nture and Rhythm via Self-Similarity”, Proceedings\nof the International Conference on Computer Music ,\n2001.\n[9] Dorrell, P. What is Music? Solving a Scientiﬁc Mys-\ntery. Phillip Dorrell, 2005.\n[10] Gomez, E. and Bonada, J. “Tonality Visualization\nof Polyphonic Audio”, Proceedings of International\nComputer Music Conference , 2005.\n[11] Langer, J. and Goebl, W. “Visualizing Expressive Per-\nformance in Tempo-Loudness Space”, Computer Mu-\nsic Journal , 2003.\n[12] Lerdahl, F. Tonal Pitch Space . Oxford University\nPress, 2001.\n[13] “Music Animation Machine”, [Web site] 2007, URL:\nwww.musanim.com\n[14] Mardirossian, A. and Chew, E. “Key Distributions\nas Musical Fingerprints for Similarity Assessment”,\nProceedings of the 1st IEEE International Workshop\non Multimedia Information Processing and Retrieval ,\n2005.\n[15] Mardirossian, A. and Chew, E. “Music Summarization\nVia Key Distributions: Analyses of Similarity Assess-\nment Across Variations”, Proceedings of the 7th Inter-\nnational Conference on Music Information Retrieval ,\n2006.\n[16] “1st Annual Music Information Retrieval Eval-\nuation eXchange”, [Web site] 2006, URL:\nwww.music-ir.org/mirex2005\n[17] Misra, A., Wang, G. and Cook, P. R. “sndtools: Real-\nTime Audio DSP and 3D Visualization”, Proceed-\nings of the International Computer Music Conference ,\n2005.\n[18] Sapp, C. “Harmonic Visualizations of Tonal Mu-\nsic”, Proceedings of the International Computer Mu-\nsic Conference , 2001.\n[19] “The Shape of Song”, [Web site] 2007, URL:\nwww.turbulence.org/Works/song\n[20] Toiviainen, P. and Krumhansl, C.L. “Measuring and\nModeling Real-Time Responses to Music: The Dy-\nnamics of Tonality Induction”, Perception , 2003.\n[21] Tufte, E. Envisioning Information . Graphics Press,\nCheshire, CT, 1990."
    },
    {
        "title": "Automatic Derivation of Musical Structure: A Tool for Research on Schenkerian Analysis.",
        "author": [
            "Alan Marsden"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415814",
        "url": "https://doi.org/10.5281/zenodo.1415814",
        "ee": "https://zenodo.org/records/1415814/files/Marsden07.pdf",
        "abstract": "This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable ‘matrix’ of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate ‘goodness’ metrics to be incorporated into the software and tested on actual music.",
        "zenodo_id": 1415814,
        "dblp_key": "conf/ismir/Marsden07",
        "keywords": [
            "software",
            "automatic derivation",
            "musical structures",
            "MIR tasks",
            "information about musical structure",
            "solution space",
            "valid structural analysis",
            "dynamic-programming algorithm",
            "candidate goodness metrics",
            "actual music"
        ],
        "content": "AUTOMATIC DERIVATION OF MUSICAL STRUCTURE: \nA TOOL FOR RESEARCH ON SCHENKERIAN ANALYSIS \nAlan Marsden \nLancaster Institute for the Contemporary Arts \nLancaster University, UK \nABSTRACT \nThis paper describes software to facilitate researc h on \nthe automatic derivation of hierarchical (Schenkeri an) \nmusical structures from a musical surface. Many MIR  \ntasks require information about musical structure, or \nwould perform better if such information were avail able. \nAutomatic derivation of musical structure faces two  sig- \nnificant obstacles. Firstly, the solution space of possible \nstructural analyses of a piece is very large. Secon dly, \npieces can have more than one valid structural anal ysis, \nand there is little firm agreement among music theo rists \nabout how to distinguish a good analysis. To circum vent \nthe first of these obstacles, software has been dev eloped \nwhich derives a tractable ‘matrix’ of possibilities  from a \nmusical surface (i.e., MIDI-like note-time informat ion). \nThe matrix is somewhat like the intermediate result s of a \ndynamic-programming algorithm, and in a similar way  it \nis possible to extract a particular structural anal ysis from \nthe matrix by following the appropriate path from t he \ntop level to the surface. It therefore provides a t ool to \nfacilitate research on the second obstacle by allow ing \ncandidate ‘goodness’ metrics to be incorporated int o the \nsoftware and tested on actual music. \n1.  THE SIGNIFICANCE OF STRUCTURAL \nINFORMATION \nMany tasks in Music Information Retrieval (MIR) re-\nquire information about musical structure, or would  per- \nform better if such information were available. A p rime \nexample is the retrieval of segments which are musi cally \nsimilar. There can be no doubt that, in Classical m usic, a \ntheme and its variations are similar in some sense,  yet \nthe details of both the sound and the actual sequen ces of \nnotes can be very different. The melody might be he av- \nily ornamented or simplified, and sometimes a com- \npletely different melody is used within broadly the  same \nharmonic sequence. Exactly the same applies in the case \nof jazz improvisation on an existing piece, most re adily \nseen in ‘jazz standards’. The similarity in these c ases is \nnot in surface features but in the underlying music al \nstructure. While descriptions of structure may be r ela- \ntively arcane, requiring knowledge of complex music  theory, its perception appears to be commonplace: n aïve \nlisteners are aware of the similarity between a the me and \nits variation or an original tune and its rendition  by a \njazz ensemble. \nSoftware to derive a structural analysis automatica lly \nwould therefore be a very useful tool in MIR. Softw are \nto derive elements of musical structure, such as me tre, \nharmony or grouping, does exist, but not to give a de- \nscription of the harmonic-melodic pattern of notes.  Sig- \nnificant obstacles exist to developing such softwar e, \nsome music-theoretic and some technical, which will  be \ndiscussed in the following two sections. Thereafter  some \nrecently implemented software which goes part-way \ntowards automatic derivation of musical structure, and \nfacilitates systematic research on Schenkerian redu ction, \nis described.1 \n2.  STRUCTURE IN MUSIC THEORY \nBy ‘musical structure’ I mean a description of the pat- \nterns of notes which occur in a piece of music, suf fi- \nciently accurate to allow the reconstruction of eno ugh of \nthe actual sequences of notes in the piece to be re cog- \nnised by most listeners familiar with the original piece. \nFurthermore, it must contain information about the con- \nfigurations of notes which is not immediately prese nt in \nthe sequences of notes themselves, and pieces which  \nhave different sequences of notes but similar confi gura- \ntions should sound more similar than pieces with eq ually \ndifferent sequences of notes but different configur ations. \nFor Western tonal music, including Classical music in \nthe period c.1650 to c.1900, plus significant quant ities \nof later music, most film music, popular music and jazz, \na number of frameworks for the description of music al \nstructure have been proposed in music theory. (Fram e- \nworks proposed for atonal music, some early music a nd \nsome non-Western music are not widely accepted.) Th e \nmost widely influential framework in music theory i s \nundoubtedly that proposed by the Austrian theorist \nHeinrich Schenker [9]. A more systematic theory, ve ry \ndifferent in form but using many of the same ideas,  has \nbeen proposed by Lerdahl & Jackendoff [6], provokin g \nsignificant interest among computer scientists.  \n                                                           \n1 A fuller description of this project can be found at \nhttp://www.lancs.ac.uk/staff/marsdena/research/sche nker  © 2007 Austrian Computer Society (OCG).   \n \n /uniF063\n/uniF063\n/uniF063\n/uniF063\n/uniF063\n/uniF063/uniF026/uniF062/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077\n/uniF026/uniF062/uniF077/uniF077/uniF077/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA\n/uniF026/uniF062/uniF077/uniF077/uniF077/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA\n/uniF026/uniF062/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA\n/uniF026/uniF062/uniF0FA/uniF0CF /uniF0CF/uniF0CF/uniF0FA/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0FA/uniF0FA\n/uniF026/uniF062/uniF0FA/uniF0CF /uniF0CF/uniF0CF/uniF0FA/uniF0CF /uniF0CF /uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0FA/uniF0FAWhile the theory of Lerdahl & Jackendoff has the ad - \nvantage of systematic description, it does not, in my \nview, give a sufficiently detailed description of a  musi- \ncal structure. It describes a structure of melody p lus \nharmonic support rather than a full contrapuntal st ruc- \nture. I therefore choose to base a structural descr iption \non Schenkerian theory. (These issues are discussed more \nfully in [7], where a computational structural repr esenta- \ntion based on Schenkerian theory is described.) Fur - \nthermore, Schenkerian theory has the advantage of h av- \ning a large quantity of published analyses which ca n take \nthe place of a ‘ground truth’ in the testing of MIR  soft- \nware. \nSchenkerian theory describes musical structure in \nterms of hierarchical levels (‘foreground’, ‘middle -\nground’ and ‘background’ in Schenkerian terms), and  \nanalyses are expressed in ‘graphs’ which demonstrat e \nhow a piece of music is constructed by the progress ive \nelaboration of a simple fundamental structure. This  is \nillustrated in Figure 1, which shows an analysis of  the \nfirst two bars of Mozart’s Rondo K.494. (A proper \nSchenkerian analysis would conflate several of thes e \nlevels, leaving detail for the reader to infer, and  the nota- \ntion would use noteheads without slurs for higher l evels. \nFigure 1 is intended to be easier to read but to gi ve the \nsame information.) Slurs here are not performance d irec- \ntions but show aspects of the analysis. The slur be tween \nA4 and F4 crotchets at the start of the fifth stave , for \nexample, indicates that these two notes join togeth er to \nform a single chord at the next higher level. \nThere has been previous study of the possibility of  \nimplementing Schenkerian analysis by computer. \nKassler [3-5] demonstrated that systematisation of \nSchenkerian theory was possible and proceeded as fa r as \na system able to derive an analysis from a middlegr ound. \nExtension of this to derive an analysis from a musi cal \nsurface has not yet been reported, I suspect in par t be- \ncause of the problem of the size of the solution sp ace, \ndiscussed below. More recently Mavromatis & Brown \n[8] have demonstrated the mathematical possibility of \nimplementing Schenkerian theory as a context-free \ngrammar, but personal communication from Mavromatis  \nindicates that this too has foundered on the proble m of \nthe size of the solution space. Gilbert & Conklin [ 1] get \nround this by using a probabilistic grammar to deri ve \nmelodic reductions. Other computer-based work invol v- \ning aspects of Schenkerian theory has not attempted  to \ngenerate analyses from actual pieces, (e.g., [10]).  Hama- \nnaka, Hirata & Tojo [2] have implemented a system t o \nmake reductions according to the theory of Lerdahl & \nJackendoff, using their theory of preference rules.  How- \never, human intervention is required to adjust para me- \nters to direct reduction towards an acceptable resu lt. \nThis paper presents the first computer software sys - \ntem which derives quasi-Schenkerian analyses of unc on- \nstrained polyphonic pieces of music purely on the b asis \nof pitch and time information . However, as is made clear \nbelow, there is still considerable work to be done before \nanalyses can be practically derived from full piece s, and before any confidence can be placed in the actual a naly- \nses derived. In the first case, the time taken to d erive \nanalyses is currently too great, but it is the seco nd issue \nwhich is the real area for research. As mentioned a bove, \nmusic theory does not yet supply unequivocal criter ia to \nguide the process of analysis towards a good soluti on \nwhich reflects the structure heard. (Analysts typic ally \nrely on their own hearing and musical judgement.) B y \ncreating a system which generates sets of analyses,  em- \npirical research to determine appropriate criteria is now \npossible. \n3.  SIZE OF THE SOLUTION SPACE \nIn [7], I demonstrate that a Schenkerian analysis o f a \npiece can be represented as a directed acyclic grap h \nwhich tends towards resembling a binary tree. Each note \nof the surface of a piece is a terminal node of thi s graph. \nThe ‘roots’ are the notes of the highest level redu ction. \nSimultaneous voices tend to be analysed in parallel  bi- \nnary trees, but interactions between voices are rep re- \nsented by links between trees, causing the analysis  to \nbecome properly a directed graph instead of simply a \ncollection of trees. If we temporarily disregard th e con- \nstraints which make a graph valid in Schenkerian te rms, \nthe number of possible analyses of a piece is at le ast as \nmany as the number of binary trees possible with n ter- \nminal nodes, where n is the maximum number of notes \nin any voice in the piece. This is the ‘Catalan num ber’ \nCn: (2 n)!/( n+1)! n!. Thus we can expect the solution \nspace for Schenkerian analyses of a piece to grow f acto- \nrially with the size of that piece.  \nIt is very likely that the constraints of Schenkeri an \ntheory impose a sufficiently powerful restriction t o ren- \nder this solution space tractable (otherwise how wo uld Figure 1 . ‘Schenkerian’ analysis of Mozart K.494   \n \n Schenkerian analyses ever be made?), but in the pre sent \nstate of knowledge we cannot express these constrai nts \nwith sufficient rigour and confidence to allow the design \nof a tractable Schenkerian-analysis system. The aim  of \nthe research project reported in this paper is to i mple- \nment a practical tool which facilitates the systema tic \nstudy of Schenkerian analysis so that these constra ints \ncan be discovered and tested. The ultimate objectiv e is \nto use the results of this research to implement au tomatic \nstructure-deriving software which completes its tas k \nwith sufficient efficiency and accuracy for MIR tas ks \nsuch as segmentation and the discovery of pattern a nd \nsimilarity. \n4.  SOFTWARE DESIGN \nThe approach taken in this project is similar to ‘d ynamic \nprogramming’: a matrix of local, partial solutions is de- \nrived such that a complete solution can be construc ted \nby taking a particular path through the matrix, joi ning \npartial solutions to make a complete solution. The sur- \nface of a piece is first divided into a sequence of  ‘seg- \nments’ such that notes only begin or end at the beg in- \nnings or ends of segments. Notes which span several  \nsegments are divided into a sequence of notes conne cted \nby ties. A segment thus consists of a set of notes (which \nmight be tied to other notes in preceding or follow ing \nsegments) occupying a certain span of time.  \nThe essential analysis procedure is to take all pai rs of \nconsecutive segments and derive from them all possi ble \nreductions of that sequence of segments. ‘Possible re- \nductions’ means reductions which follow the princip les \nof Schenkerian analysis—passing notes, neighbours \nnotes, etc.—and which are harmonically and tonally \nmutually consistent. There are some constraints als o on \nthe local context: in some cases certain notes must  occur \nin a preceding or following segment (e.g., passing notes \nmust have a certain note in a following segment to pass \nto).  \nThe result of this step is a set of new segments, a ll \noccupying a span of time which is the sum of the sp ans \nof the two ‘child’ segments. The number of segments  in \nthis set can be large, but it is limited because se gments \nare only distinguished by the notes they contain (p lus \ndetails of contextual requirements, but these are a lso \nlimited), and there is only a finite (and relativel y small) \nset of possible notes. The procedure is then applie d re- \ncursively to all resulting segments until the top l evel, \nwhere segments cover the entire span of the piece. The \nresult is a triangular matrix of sets of segments w hich \nconstitutes a conflation of all possible reductions  of the \npiece. To derive a complete reduction, one need onl y \nselect one top-level segment and then recursively s elect \npairs of children until the bottom level is reached . \nThe analysis procedure can be explained further by \nreference to Figure 2, which reflects the analysis of the \nend of the example shown in Figures 1 & 3. Cells a1  to \na3 reflect the last three segments of the example. The \nsegments of cell b1 are derived by finding all poss ible ways of combining the segments of a1 and a2 so that  \nconsecutive notes form permissible progressions who se \nharmonic and tonal constraints are consistent. The tied \nnotes C5 in a1 and a2 can only combine with each ot her, \nbut the G5 and E5 can combine in three different wa ys, \nresulting in G5, E5 or both G5 and E5 at the level \nabove. Thus cell b1 contains three segments, all of  them \ncontaining the note C5 while E5 and G5 appear in tw o \neach. Cell b2 is derived from combining a2 and a3, and \ncontains the segment which is the only permissible way \nof combining these notes into a single chord. Cell c1 is \nderived from combining both a1 with b2 and b1 with a3. \nThere are several possible ways of combining these \nsegments, but they all result in just three segment s, all of \nwhich contain F5 while C5 and A4 are contained in t wo \neach. \nThe basic size of the matrix (the number of sets of  \nsegments) is obviously related to the square of the  length \nof the music analysed, so the space requirement of the \nreduction algorithm can be expected to be of order \nO( n2). However, the number of pairs of spans to be con-\nsidered, when deriving the new segments for a new \nlonger span, increases at each higher level of the matrix, \nand the time requirement is of order O( n3). The real con- \nstraint on tractability, however, is the number of seg- \nments in each set. The upper limit on this number i s 2 \nraised to the power of the total number of differen t notes \nwhich might make up a segment. This is the number o f \ndifferent notes in the music analysed, which is not  (nec- \nessarily) related to the length of the music, and f urther- \nmore is limited by the number of different notes po ssible \nin any piece of music, which is fixed by the instru - \nment(s) on which it is to be played. Thus this does  not, \nin principle, increase the order of complexity of t he al- \ngorithm. However, the number of possible segments i s \nextremely large. For example, an eighteenth-century  \npiano has 61 notes, and so there are approximately \n2.3*10 18  possible combinations of different notes which \ncould appear in segments. Many of these are harmoni - \ncally impossible and/or impossible to play, but the  num- \nber of harmonically possible and playable segments is \nstill extremely large. The time taken by the analys is pro- \ncedure is related to the square of the average numb er of \nsegments for each span, so a truly tractable analys is pro- c1 : F5    or  F5    or  F5 \n C5 A4 C5 \n   A4 \n b2 : F5 \n _C5 \n A4 \nb1 : G5    or  E5    or  G5 \n C5 C5 E5 \n   C5  \na1 : G5 \n C5_ a2 : E5 \n _C5 a3 : F5 \n A4 \nFigure 2 . Extract from analytical matrix, covering the \nlast three segments of the example in Figure 3.   \n \n cedure depends on keeping this number small. Curren tly, \nderiving the matrix of reductions for a fragment wi th just \n15 segments takes about 5 minutes. An important top ic \nof research will therefore be mechanisms to keep th e \nnumber of different segments derived at each step t o a \nminimum without preventing the derivation of desira ble \ncomplete analyses. \n5.  AN EXAMPLE \nFigure 3 shows a result of applying the software to  the \nmusic example in Figure 1 (Mozart’s Rondo, K.494).1 \nAs indicated above, the software generates a matrix  con- \ntaining a set of possible analyses. The software in cludes \na number of mechanisms for assigning a score to eac h \nsegment (e.g., the minimum total number of notes in  this \nsegment and all its descendents), and a mechanism f or \npruning the matrix so that only segments with the b est \nscore are retained.  \nOne possible way of assigning a score to a segment is \nto count the minimum number of elaborations require d \nto derive this segment from the surface. (The scori ng is a \nlittle more sophisticated than a simple count, in t hat \nrepetitions count for less than neighbour notes, fo r ex- \nample.) When this is applied to the matrix of segme nts \narising from analysis of the first two bars of Moza rt’s \nrondo, and when only the best-scoring segments are re- \ntained, only four possible complete analyses remain . \nThese share the same segments at every point except  the \nfirst segment of the second-highest level where var ious \ncombinations of G5, F5, Bb4 and F4 are possible. Fi g- \nure 3 shows the resulting analysis when the segment  with \nall of these notes is chosen.  \nThe analyses of Figures 1 and 3 do not match, so in  \nthat sense the software has failed to derive the co rrect \nanalysis of this music. On the other hand, there ar e only \ntwo fundamental errors in the analysis of Figure. F irstly, \nthe reduction of the last two chords of the fourth stave \nproduces a bad rhythm in the third stave (the softw are \ncurrently does not take rhythm into account at all) . Sec- \nondly, the reduction of the first two chords in the  third \nstave produces a bad chord in the second stave (the  \nsoftware currently does not take account of inversi ons of \nchords, of harmonic sequence, or of the expectation  to \nstart on the tonic). This result can therefore be d escribed \nas promising. \nFuture research will incorporate broader considera-\ntions of rhythm and structural norms into scoring \nmechanisms. These mechanisms will be tested by com-\nparing the resulting generated analyses with actual  \nanalyses by Schenker and his pupils. Successful sco ring \nsystems will form the basis of mechanisms for pruni ng \nbad analyses from early in the derivation process, in the \nhope of arriving at a reliable structure-derivation  system \nwhich is sufficiently reliable to form the basis of  MIR \nsystems. \n                                                           \n1Demonstration software, including this example, may  be viewed at  \nhttp://www.lancs.ac.uk/staff/marsdena/schenker  6.  REFERENCES \n[1]  Gilbert, E., & Conklin, D., “A Probabilistic \nContext-Free Grammar for Melodic Reduction”, \nInternational Workshop on Artificial Intelligence \nand Music, IJCAI-07, Hyderabad, India, 2007. \n[2]  Hamanaka, M., Hirata, K. & Tojo, S., “ATTA: \nAutomatic Time-Span Tree Analyzer Based on \nExtended GTTM”, in Proceedings of the Sixth \nInternational Conference on Music Information \nRetrieval, ISMIR 2005, 358–365. \n[3]  Kassler M., Proving Musical Theorems I: The \nMiddleground of Hienrich Schenker’s Theory of \nTonality (Tech. Rep. No. 103). Sydney, Australia: \nUniversity of Sydney, School of Physics, Basser \nDepartment of Computer Science, 1975. \n[4]  Kassler M., “Explication of the middleground of \nSchenker’s theory of tonality”, Miscellanea \nMusicologica: Adelaide Studies in Musicology, v.9, \n72–81, 1977. \n[5]  Kassler M., “APL applied in music theory”, APL \nQuote Quad, v.18, 209–214, 1988. \n[6]  Lerdahl, F. & Jackendoff R., A Generative Theory \nof Tonal Music, MIT Press, 1983. \n[7]  Marsden, A., “Generative Structural Representation \nof Tonal Music”, Journal of New Music Research, \nv.34, 409–428, 2005. \n[8]  Mavromatis, P., & Brown, M., “Parsing Context-\nFree Grammars for Music: A Computational Model \nof Schenkerian Analysis”, Proceedings of the 8th \nInternational Conference on Music Perception and \nCognition, Evanston, USA, 2004, 414–415. \n[9]  Schenker, H. Der frei Satz, Vienna: Universal \nEdition, 1935. Published in English as Free \nComposition , translated and edited by E. Oster, \nLongman, 1979. \n[10]  Smoliar, S.W. “A Computer Aid for Schenkerian \nAnalysis”, Computer Music Journal , v.4, 41–59. /uniF063\n/uniF063\n/uniF063\n/uniF063\n/uniF063/uniF026/uniF062/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077/uniF077\n/uniF026/uniF062/uniF077/uniF077/uniF077/uniF077/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0FA/uniF0AA/uniF0FA/uniF0FA/uniF0AA/uniF0AA\n/uniF026/uniF062/uniF077/uniF077/uniF077/uniF077/uniF0CF/uniF0CF/uniF0FA/uniF0AA/uniF0FA/uniF0FA/uniF0AA/uniF0AA\n/uniF026/uniF062/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0FA/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0FA/uniF0FA\n/uniF026/uniF062/uniF0FA/uniF0CF /uniF0CF/uniF0CF/uniF0FA/uniF0CF /uniF0CF /uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0FA/uniF0FA\nFigure 3 . Automatic analysis of Mozart K.494"
    },
    {
        "title": "Polyphonic Instrument Recognition Using Spectral Clustering.",
        "author": [
            "Luis Gustavo Martins",
            "Juan José Burred",
            "George Tzanetakis",
            "Mathieu Lagrange"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415074",
        "url": "https://doi.org/10.5281/zenodo.1415074",
        "ee": "https://zenodo.org/records/1415074/files/MartinsBTL07.pdf",
        "abstract": "The identification of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classification of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classification of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes. 1 INTRODUCTION The increasing quantity of music titles available in digital format added to the huge amount of personal music storage capacity available today has resulted in a growing demand for more efficient and automatic means of indexing, searching and retrieving music content. The computer identification of the instruments playing in a music signal can assist the automatic labeling and retrieval of music. Several studies have been made on the recognition of musical instruments on isolated notes or in melodies played by a single instrument. A comprehensive review of those techniques can be found in [1]. However, the recognition of musical instruments in multi-instrumental, polyphonic music is much more complex and presents additional challenges. The main challenge stands from the fact that tones from performing instruments can overlap in time and frequency. Therefore, most of the isolated note recognition techniques that have been proposed in the literature are inappropriate for polyphonic music signals. Some of the proposed techniques for the instrument c⃝2007 Austrian Computer Society (OCG). note 1 note n ... Sound Source Formation note 1 / inst 1 note n / inst i ... Timbre Models Matching Matching Peak Picking Sinusoidal Analysis ... ... ... Figure 1. System diagram block. recognition on polyphonic signals consider the entire audio mixture, avoiding any prior source separation [2, 3]. Other approaches are based on the separation of the playing sources, requiring the prior knowledge or estimation of the pitches of the different notes [4, 5]. However, robustly extracting the fundamental frequencies in such multiple pitch scenarios is difficult. In this paper, we propose a framework for timbre classification of polyphonic, multi-instrumental music signals using automatically separated sound sources. Figure 1 presents a block-diagram of the complete system. It starts by taking a single-channel audio signal and uses a sinusoidal analysis front-end for estimating the most prominent spectral peaks over time. The detected spectral peaks are then grouped into clusters according to cues inspired from Computational Auditory Scene Analysis (i.e. frequency, amplitude and harmonic proximity) and formulated as a graph partitioning problem. The normalized cut, a technique from the Computer Vision field, is then used as a global criterion for segmenting graphs. Contrary to other approaches [6, 7], this source separation technique does not require any prior knowledge or pitch estimation. As demonstrated in previous works by the authors [8, 9] and later in section 4, the resulting clusters capture reasonably well the underlying sound sources and events (i.e. notes, in the case of music signals) present in the audio mixture. After the sound source separation stage, each identified cluster is matched to a collection of six timbre models namely piano, oboe, clarinet, trumpet, violin and alto sax. These models are a compact description of the spectral envelope and its evolution in time, and were previously trained using isolated note audio recordings. The design of the models, as well as their application to isolated note classification, were described in [10]. The outline of the paper is as follows. In section 2 we describe the sound source separation technique, which starts from a sinusoidal representation of the signal followed by the application of the normalized cut for source separation. In section 3 we briefly describe the training of the timbre models and focus on the matching procedure used to classify the separated clusters. We then evaluate the system performance in section 4 and close with some final conclusions. 2 SOUND SOURCE SEPARATION Computational Auditory Scene Analysis (CASA) systems aim at identifying perceived sound sources (e.g. notes in the case of music recordings) and grouping them into auditory streams using psycho-acoustical cues [11]. However, as remarked in [6] the precedence rules and the relevance of each of those cues with respect to a given practical task is hard to assess. Our goal is to use a flexible framework where these perceptual cues can be expressed in terms of similarity between time-frequency components. The separation task is then carried out by clustering components which are close in the similarity space (see Figure 2). Once identified, those clusters will be matched to timbre models in order to perform the instrument identification task.",
        "zenodo_id": 1415074,
        "dblp_key": "conf/ismir/MartinsBTL07",
        "keywords": [
            "sound source separation",
            "timbre classification",
            "polyphonic music signals",
            "graph partitioning problem",
            "normalized cut",
            "computational auditory scene analysis",
            "sinusoidal analysis front-end",
            "six musical instruments",
            "timbre models",
            "isolated note audio recordings"
        ],
        "content": "POLYPHONIC INSTRUMENT RECOGNITION\nUSING\nSPECTRAL CLUSTERING\nLuis Gustavo Martins\nTelecommunications and Multimedia Unit\nINESC Porto\nPorto, Portugal\nlmartins@inescporto.ptJuan Jos ´e Burred\nCommunication Systems Group,\nTechnical University of Berlin,\nBerlin, Germany\nburred@nue.tu-berlin.deGeorge Tzanetakis, Mathieu Lagrange\nComputer Science Department,\nUniversity of Victoria\nVictoria, BC, Canada\n[gtzan, lagrange]@uvic.ca\nABSTRACT\nThe identiﬁcation of the instruments playing in a poly-\nphonic music signal is an important and unsolved prob-lem in Music Information Retrieval. In this paper, we pro-pose a framework for the sound source separation and tim-bre classiﬁcation of polyphonic, multi-instrumental musicsignals. The sound source separation method is inspiredby ideas from Computational Auditory Scene Analysisand formulated as a graph partitioning problem. It uti-lizes a sinusoidal analysis front-end and makes use of thenormalized cut, applied as a global criterion for segment-ing graphs. Timbre models for six musical instruments areused for the classiﬁcation of the resulting sound sources.The proposed framework is evaluated on a dataset con-sisting of mixtures of a variable number of simultaneouspitches and instruments, up to a maximum of four concur-rent notes.\n1 INTRODUCTION\nThe increasing quantity of music titles available in dig-\nital format added to the huge amount of personal musicstorage capacity available today has resulted in a growingdemand for more efﬁcient and automatic means of index-ing, searching and retrieving music content. The computeridentiﬁcation of the instruments playing in a music signalcan assist the automatic labeling and retrieval of music.\nSeveral studies have been made on the recognition\nof musical instruments on isolated notes or in melodiesplayed by a single instrument. A comprehensive review\nof those techniques can be found in [1]. However, therecognition of musical instruments in multi-instrumental,\npolyphonic music is much more complex and presents ad-ditional challenges. The main challenge stands from thefact that tones from performing instruments can overlapin time and frequency. Therefore, most of the isolated\nnote recognition techniques that have been proposed inthe literature are inappropriate for polyphonic music sig-nals. Some of the proposed techniques for the instrument\nc/circlecopyrt2007 Austrian Computer Society (OCG).note 1\nnote n\n...Sound\nSource\nFormationnote 1 / inst 1\nnote n / inst i\n...Timbre\nModels\nMatching\nMatchingPeak\nPickingSinusoidal\nAnalysis... ......\nFigure 1 . System diagram block.\nrecognition on polyphonic signals consider the entire au-\ndio mixture, avoiding any prior source separation [2, 3].Other approaches are based on the separation of the play-ing sources, requiring the prior knowledge or estimationof the pitches of the different notes [4, 5]. However,robustly extracting the fundamental frequencies in suchmultiple pitch scenarios is difﬁcult.\nIn this paper, we propose a framework for timbre clas-\nsiﬁcation of polyphonic, multi-instrumental music signalsusing automatically separated sound sources. Figure 1presents a block-diagram of the complete system. It startsby taking a single-channel audio signal and uses a sinu-soidal analysis front-end for estimating the most promi-nent spectral peaks over time. The detected spectral peaksare then grouped into clusters according to cues inspiredfrom Computational Auditory Scene Analysis (i.e. fre-quency, amplitude and harmonic proximity) and formu-lated as a graph partitioning problem. The normalized cut,a technique from the Computer Vision ﬁeld, is then usedas a global criterion for segmenting graphs. Contrary toother approaches [6, 7], this source separation techniquedoes not require any prior knowledge or pitch estimation.\nAs demonstrated in previous works by the authors [8,\n9] and later in section 4, the resulting clusters capture rea-\nsonably well the underlying sound sources and events (i.e.notes, in the case of music signals) present in the audio\nmixture. After the sound source separation stage, each\nidentiﬁed cluster is matched to a collection of six timbremodels namely piano, oboe, clarinet, trumpet, violin and\nalto sax. These models are a compact description of the\nspectral envelope and its evolution in time, and were pre-\nviously trained using isolated note audio recordings. Thedesign of the models, as well as their application to iso-\nlated note classiﬁcation, were described in [10].\nThe outline of the paper is as follows. In section 2\nwe describe the sound source separation technique, which\nstarts from a sinusoidal representation of the signal fol-\nlowed by the application of the normalized cut for source\nseparation. In section 3 we brieﬂy describe the training ofthe timbre models and focus on the matching procedure\nused to classify the separated clusters. We then evaluate\nthe system performance in section 4 and close with someﬁnal conclusions.\n2 SOUND SOURCE SEPARATION\nComputational Auditory Scene Analysis (CASA) systems\naim at identifying perceived sound sources (e.g. notes inthe case of music recordings) and grouping them into au-\nditory streams using psycho-acoustical cues [11]. How-\never, as remarked in [6] the precedence rules and the rel-evance of each of those cues with respect to a given prac-\ntical task is hard to assess. Our goal is to use a ﬂexible\nframework where these perceptual cues can be expressed\nin terms of similarity between time-frequency components.The separation task is then carried out by clustering com-\nponents which are close in the similarity space (see Fig-ure 2). Once identiﬁed, those clusters will be matched totimbre models in order to perform the instrument identiﬁ-cation task.\n2.1 Sinusoidal Modeling\nMost CASA approaches consider auditory ﬁlterbanks and/\nor correlograms as their front-end [12]. In these ap-proaches the number of time-frequency components is rel-atively small. However closely-spaced components withinthe same critical band are hard to separate. Other ap-\nproaches [6, 13] consider the Fourier Spectrum as their\nfront-end. In these approaches, in order to obtain sufﬁ-cient frequency resolution a large number of componentsis required. Components within the same frequency re-gion can be pre–clustered together according to a stabil-ity criterion computed using statistics over the considered\nregion. However, this approach has the drawback of in-\ntroducing another clustering step, and opens the issue ofchoosing the right descriptors for those pre-clusters. Al-ternatively, a sinusoidal front-end is helpful to providemeaningful and precise information about the auditory\nscene while considering only a limited number of compo-nents, and is the representation we consider in this work.\nSinusoidal modeling aims to represent a sound signal\nas a sum of sinusoids characterized by amplitudes, fre-\nquencies, and phases. A common approach is to segment\nthe signal into successive frames of small duration so thatthe stationarity assumption is met. For each frame, the\nlocal maxima of the power spectrum are identiﬁed and a\nbounded set of sinusoidal components is estimated select-ing the peaks with the highest amplitudes.Spectral Peaks\nSinusoidal\nAnalysisCluster SelectionSimilarity\nComputation\nNormalized Cut\nSinusoidal\nSynthesis\nFigure 2 . Block-Diagram of the Sound Source Separation\nalgorithm.\nThe discrete signal xk(n)at frame index kis then mod-\neled as follows:\nxk(n)=Lk/summationdisplay\nl=1alkcos/parenleftbigg2π\nFsflk·n+φlk/parenrightbigg\n(1)\nwhere Fsis the sampling frequency and φlkis the\nphase at the beginning of the frame of the l-th com-\nponent of Lksine waves. The flandalare the fre-\nquency and the amplitude of the l-th sine wave, respec-\ntively, both of which are considered as constant within theframe. For each frame k, a set of sinusoidal parameters\nS\nk={p1k,···,pLkk}is estimated. The system parame-\nters of this Short-Term Sinusoidal (STS) model Skare the\nLktriplets plk={flk,alk,φlk}, often called peaks .\n2.2 Spectral Clustering\nIn order to simultaneously optimize partial tracking and\nsource formation, we construct a graph over the entire du-ration of the sound mixture. Unlike approaches based on\nlocal information [14], we utilize the global normalizedcut criterion to partition the graph (spectral clustering).\nThis criterion has been successfully used for image andvideo segmentation [15]. In our perspective, each parti-tion is a set of peaks that are grouped together such that\nthe similarity within the partition is minimized and the dis-similarity between different partitions is maximized. Byappropriately deﬁning the similarity between peaks a va-riety of perceptual grouping cues can be used.\nThe edge weight connecting two peaks p\nlkandpl/primek/prime(k\nis the frame index and lis the peak index) depends on the\nproximity of frequency, amplitude and harmonicity:\nW(plk,pl/primek/prime)= Wf(plk,pl/primek/prime)·Wa(plk,pl/primek/prime)\n·Wh(plk,pl/primek/prime) (2)\nwhere Wxare typically radial basis functions of distance\namong the two peaks in the xaxis. For more details see\n[8, 9].\nMost existing approaches that apply the Ncut algorithm\nto audio [16] consider the clustering of components over\none analysis frame only. However, the time integration\n(i.e. partial tracking) is as important as the frequency one0\n50\n100\n150\n200 05001000150020002500\n00.050.10.150.20.25\nFrequency (Hz)Time (frames)AmplitudeCluster 0\nCluster 1\nFigure 3 . Resulting sound source formation clusters for\ntwo notes played by a piano and an oboe (E4 and B4, re-spectively).\n(i.e. source formation) and should be carried out at the\nsame time. We therefore consider the sinusoidal compo-nents extracted within the entire mixture as proposed in[8]. We considered a maximum of 20 sinusoids per framewhich are 46 ms long, using a hop size of 11 ms.\nFigure 3 depicts the result of the sound source separa-\ntion using the normalized cut for a single-channel audiosignal with mixture of two notes (E4 and B4\n1, same on-\nset, played by a piano and an oboe, respectively). Each dotcorresponds to a peak in the time-frequency space and thedifferent coloring reﬂects the cluster to which it belongs(i.e. its source).\n3 TIMBRE IDENTIFICATION\n3.1 Timbre Models\nOnce each single-note cluster of sinusoidal parameters has\nbeen extracted, it is classiﬁed into an instrument from apredeﬁned set of six: piano ( p), oboe ( o), clarinet ( c),\ntrumpet ( t), violin ( v) and alto sax ( s). The method models\neach instrument as a set of time-frequency templates, onefor each instrument. The template describes the typical\nevolution in time of the spectral envelope of a note. The\nspectral envelope is an appropriate representation to gen-\nerate features to analyze sounds described by sinusoidalmodeling, since it matches the salient peaks of the spec-\ntrum, i.e., the amplitudes a\nlkof the partials.\nThe training process consists of arranging the training\ndataset as a time-frequency matrix X(g,k)of size G×K,\nwhere gis the frequency bin index and kis the frame in-\ndex, and performing spectral basis decomposition upon itusing Principal Component Analysis (PCA). This yields afactorization of the form X=BC , where the columns of\ntheG×Gmatrix Bare a set of spectral basis sorted in de-\ncreasing order of contribution to the total variance, and C\n1Throughout this paper we use the convention A4 = 440Hz.0\n0.2\n0.4\n0.6\n0.8\n1 2000 4000 60008000 10000−80−60−40−200\nFrequency (Hz)Time (normalized)Amplitude (dB)\n(a) Piano\n0.2\n0.4\n0.6\n0.8\n1 2000 4000 6000 8000 10000−80−60−40−200\nFrequency (Hz)Time (normalized)Amplitude (dB)\n(b) Oboe\nFigure 4 . Examples of prototype envelopes for a range of\none octave.\nis theG×Kmatrix of projected coefﬁcients. By keeping\na reduced set of R<G basis, we obtain both a reduction\nof the data needed for a reasonable approximation and,more importantly for our purpose, a representation basedonly on the most essential spectral shapes.\nHaving as goal a pitch-independent classiﬁcation, the\ntime-frequency templates should be representative for awide range of notes. In the training process, notes fromseveral pitches must be considered to obtain a singlemodel. The training samples are subjected to sinusoidalmodeling, and arranged in the data matrix Xby linearly\ninterpolating the amplitude values to a regular frequencygrid deﬁned at the locations of the Gbins. This is im-\nportant for appropriately describing formants, which aremostly independent of the fundamental frequency.\nThe projected coefﬁcients of each instrument in the R-\ndimensional PCA space are summarized as a prototypecurve by interpolating the trajectories corresponding to\nthe individual training samples at common time points and\npoint-wise averaging them. When projecting back into thetime-frequency domain by a truncated inverse PCA, each\nP\ni-point prototype curve will correspond to a G×Pipro-\ntotype envelope Mi(g,k)for instrument i. We consider\nthe same number of time frames P=Pifor all instrument\nmodels. Figure 4 shows the obtained prototype envelopes\nfor the fourth octave of a piano and of an oboe.\nDepending on the application, it can be more con-\nvenient to perform further processing on the reduced-\ndimensional PCA space or back in the time-frequency\ndomain. When classifying individual notes, a distance0\n0.2\n0.4\n0.6\n0.8\n1 500100015002000−4−3−2−10\nFrequency (Hz)Time (normalized)Magnitude (dB)\nFigure 5 . Weak matching of an alto sax cluster and a\nportion of the piano prototype envelope.\nmeasure between unknown trajectories and the prototype\ncurves in PCA space has proven successful [10]. In thecurrent source separation application, the clusters to bematched to the models can contain regions of unresolvedoverlapping partials or outliers, which can introduce im-portant interpolation errors when adapted to the G-bin fre-\nquency grid needed for projection onto the bases. Thismakes working in the time-frequency domain more con-\nvenient in the present case.\n3.2 Timbre Matching\nEach one of the clusters obtained by the sound source sep-\naration step is matched against each one of the prototypeenvelopes. Let us denote a particular cluster of Kframes\nrepresented as an ordered set of amplitude and frequencyvectors A=(a\n1,..., aK),F=(f1,..., fK)of possibly\ndiffering lengths L1,...,L K.\nWe need to evaluate the prototype envelope of model\niat the frequency support of the input cluster j. This\noperation is denoted by ˜Mij=Mi(Fj). To that end,\nthe time scales of both input and model are ﬁrst normal-ized. Then, the model frames closest to each one of theinput frames in the normalized time scale are selected.Finally, each new amplitude value ˜m\nij\nlkis linearly inter-\npolated from the neighboring amplitude values of the se-lected model frame.\nWe then deﬁne the distance between a cluster jand an\ninterpolated prototype envelope ias\nd(A\nj,˜Mij)=1\nKjKj/summationdisplay\nk=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtLj\nk/summationdisplay\nl=1(aj\nlk−˜mij\nlk)2 (3)\ni.e., the average of the Euclidean distances between frames\nof the input clusters and interpolated prototype envelope\nat the normalized time scale. The model ˜Mijminimiz-\ning this distance is chosen as the predicted instrument forTrue instruments classiﬁed\np o c t v s as\n100 0 0 0 0 0 p\n0 100 8 8 0 0 o\n0 0 67 0 33 0 c\n0 0 0 92 0 8 t\n0 0 0 0 58 8 v\n0 0 25 0 8 83 s\nTable 1 . Confusion matrix for single–note instrument\nidentiﬁcation. We considered 6 different instruments fromthe RWC database: piano ( p), oboe ( o), clarinet ( c), trum-\npet ( t), violin ( v), alto sax ( s).\nclassiﬁcation. Figure 5 shows an attempt to match a clus-\nter extracted from an alto sax note and the correspondingsection of the piano prototype envelope. As it is clearlyvisible, this weak match results in a high distance value.\n4 EXPERIMENTS\nThe current framework implementation does still not fully\ntake into consideration timing information and continuityissues, such as note onsets and durations. Given so, wewill limit the evaluation procedure to the separation andclassiﬁcation of concurrent notes sharing the same onset\nand played from different instruments.\nThe evaluation dataset was artiﬁcially created mixing\naudio samples of isolated notes of piano, oboe, clarinet,trumpet, violin and alto sax, all from the RWC MusicDatabase [17]. The training dataset used to derive the tim-bre models for each instrument (see Section 3) is com-posed of audio samples of isolated notes, also from theRWC Music Database. However, in order to get mean-ingful timbre recognition results, we used independent in-stances of each instrument for the evaluation dataset andfor the training dataset. Ground-truth data was also cre-ated for each mixture and includes information about thenotes played and the corresponding instrument. Given thatthe timbre models used in this work showed good resultsfor a range of about two octaves [10], we constrained thenotes used for evaluation to the range C4 to B4. Further-more, for simplicity’s sake, we have only considered noteswith a ﬁxed intensity in this evaluation.\n4.1 Timbre identiﬁcation for single note signals\nWe started by evaluating the performance of the timbre\nmatching block (as discussed in Section 3.2) for the case\nof isolated notes coming from each of the six instruments\nmodeled. This provides a base-ground with which will be\npossible to compare the ability of the framework to clas-sify notes separated from mixtures. For the case of iso-\nlated notes, the sound source separation block reduces its\naction to just performing sinusoidal analysis, since thereare no other sources to be separated. This basically only\nresults in the loss of the non-harmonic residual, which\nalthough not irrelevant to timbre identiﬁcation, has been\ndemonstrated to have a small impact in the classiﬁcation2-note 3-note 4-note total\nRCL PRC F1 RCL PRC F1 RCL PRC F1 RCL PRC F1\np 83 100 91 22 100 36 00 0 23 100 38\no 133 75 96 100 46 63 67 40 50 86 50 63\nc 33 100 50 33 100 50 40 86 55 36 93 52\nt 89 100 94 58 100 74 58 64 61 67 85 75\nv 67 67 67 83 45 59 83 36 50 80 43 56\ns 100 43 60 78 60 63 60 75 67 67 62 64\ntotal 75 79 77 56 64 59 46 56 50 56 64 60\nTable 2 . Recall and precision values for instrument presence detection in multiple-note mixtures.\nperformance [18]. Table 1 presents the confusion matrix\nfor the instrument classiﬁcation for a dataset of 72 isolatednotes, ranging from C4 to B4, from each one of the sixconsidered instruments. The system presents an overall\nclassiﬁcation accuracy of 83.3%, being violin and clarinetthe instruments posing the biggest difﬁculties.\n4.2 Instrument presence detection in mixtures of notes\nWe then evaluated the ability of the system to separate and\nclassify the notes from audio ﬁles with up to 4 simultane-ously sounding instruments. A combination of 54 differ-ent instruments and mixtures of 2-, 3- and 4-notes wascreated (i.e. 18 audio ﬁles for each case).\nThe ﬁrst and simplest evaluation we performed was to\ntest the system ability to detect the presence of an instru-\nment in a mixture of up to 4 notes. In this case it was justa matter of matching each one of the six timbre modelswith all the separated clusters and counting the true and\nfalse positives for each instrument. A true positive (TP) is\nhere deﬁned as the number of separated clusters correctlymatched to an instrument playing in the original mixture(such information is available in the dataset ground-truth).\nAfalse positive (FP) can be deﬁned as the number of clus-\nters classiﬁed as an instrument not present in the originalaudio mixture. Given these two values, it is then possibleto deﬁne three performance measures for each instrument-Recall (RCL) ,Precision (PRC) and F-Measure (F1) :\nRCL =TP\nCOUNTPRC =TP\nTP+FP(4)\nF1=2×RCL ×PRC\nRCL+PRC(5)\nwhere COUNT is the total number of instances of an in-\nstrument over the entire dataset (i.e. the total number of\nnotes it plays). As shown in Table 2, the system was ableto correctly detect 56% of the occurrences of instruments\nin mixtures of up to 4 notes, with a precision of 64%. Pi-\nano appears as the most difﬁcult timbre to identify, specif-ically for the case of 4-note mixtures, where from the ex-\nisting 15 notes playing in the dataset, none was correctly\ndetected as coming from that instrument. As anticipated,the system performance degrades with the increase of thenumber of concurrent notes. Nevertheless, it was still pos-sible to retrieve 46% of the present instruments in 4-notemixtures, with a precision of 56%.4.3 Note separation and timbre identiﬁcation in mix-\ntures of notes\nAlthough informative, the previous evaluation has a caveat\n– it does not allow to precisely verify if a separated andclassiﬁed cluster does in fact correspond to a note playedwith the same instrument in the original audio mixture.In order to fully assess the separation and classiﬁcationperformance of the framework, we tried to make a corre-spondence between each separated cluster and the notes\nplayed in the mix (available in the ground-truth).\nA possible way to obtain such a correspondence is by\nestimating the pitch of each one of the detected clusters,using a simple technique. For each cluster we calculatedthe histogram of peak frequencies. Since the audio record-ings of the instruments used in this evaluation are fromnotes with steady pitch over time (i.e. no vibrato, glissan-dos or other articulations), the peaks on the histogram pro-vide a good indication of the frequencies of the strongestpartials. Having the set of the strongest partial frequen-cies, we then performed another histogram of the differ-ences among all partials and selected the highest mode asthe best F0 candidate for that cluster.\nGiven these pitch correspondences, it is now possible\nto check the signiﬁcance of each separated cluster as agood note candidate, as hypothesized in Section 1. For theentire dataset, which includes a total of 162 notes from allthe 2-, 3- and 4-note audio mixtures, the system was ableto correctly establish a pitch correspondence for 55% ofthe cases (67%, 57% and 49% for the 2-, 3- and 4-notemixtures, respectively). These results can not however betaken as an accurate evaluation of the sound source sepa-ration performance, as they are inﬂuenced by the accuracyof the pitch estimation technique.\nThe results in Table 3 show the correct classiﬁcation\nrate for all modeled instruments and multiple-note scenar-\nios, excluding the clusters whose correspondence was not\npossible to establish. This allows decoupling the sourceseparation/pitch estimation performance from the timbre\nidentiﬁcation accuracy. Table 3 shows a correct identi-\nﬁcation rate of 47% of the separated notes overall, di-minishing sharply its accuracy with the increase of con-\ncurrent notes in the signal. This shows the difﬁculties\nposed by the overlap of spectral components from differ-\nent notes/instruments into a single detected cluster.Instrument Detection Rate\n2-note 3-note 4-note overall\np 67 67 0 55\no 100 86 60 81\nc 33 29 19 26\nt 75 33 22 43\nv 67 100 50 75\ns 75 36 42 44\ntotal 65 50 33 47\nTable 3 . Instrument classiﬁcation performance for 2-, 3-\nand 4-note mixtures.\n5 DISCUSSION\nWe proposed a framework for the sound source separation\nand timbre classiﬁcation of single-channel polyphonic mu-sic played by a mixture of instruments. Although using aconstrained scenario, the experiments show the potentialof the system to achieve sound source separation and iden-tiﬁcation of music instruments using timbre models. Weplan on extending this framework for the analysis of con-tinuous music by taking into consideration prior time seg-mentation of the music notes, based on their onsets anddurations. This will allow us to deal with more realis-tic scenarios and to compare the proposed approach withother state-of-the-art systems.\nFurthermore, the proposed framework is versatile and\nﬂexible enough to include new features at a later stage\nthat may allow overcoming some of its current limita-tions. The use of timbre models as a-priori informationat the sound source separation stage will be an interestingtopic of future research. The extraction of new and moredescriptors directly from the estimated cluster parameters(e.g. pitch, timbre features, timing information, etc.) willallow the development of innovative applications for theautomatic analysis and sophisticated processing of real-world polyphonic music signals.\n6 ACKNOWLEDGMENTS\nPart of this research was performed at the Analy-\nsis/Synthesis team, IRCAM, Paris. The research work\nleading to this paper has been partially supported by the\nEuropean Commission under the IST research network ofexcellence VISNET II of the 6th Framework Programme.\n7 REFERENCES\n[1] P . Herrera, G. Peeters, and S. G. Dubnov, “Auto-\nmatic classiﬁcation of musical instrument sounds,”\nJournal of New Music Research , vol. 32, no. 1, pp.\n3–22, 2003.\n[2] S. Essid, G. Richard, and B. David, “Instrument\nrecognition in polyphonic music,” in Proc. ICASSP ,\nPhiladelphia, USA, 2005.\n[3] A. Livshin and X. Rodet, “Musical instrument iden-\ntiﬁcation in continuous recordings,” in Int. Conf. on\nDigital Audio Effects (DAFx) , Naples, Italy, 2004.[4] K. Kashino and H. Murase, “A sound source identiﬁ-\ncation system for ensemble music based on templateadaptation and music stream extraction,” Speech\nCommunication , , no. 27, 1999.\n[5] B. Kostek, “Musical instrument classiﬁcation and\nduet analysis employing music information retrieval\ntechniques,” Proceedings of the IEEE , vol. 92, no. 4,\npp. 712–729, 2004.\n[6] E. Vincent, “Musical source separation using time-\nfrequency priors,” IEEE Trans. on Audio, Speech\nand Language Processing , vol. 14(1), 2006.\n[7] J. Eggink and G. J. Brown, “A missing feature\napproach to instrument identiﬁcation in polyphonicmusic,” in Proc. ICASSP , 2003.\n[8] M. Lagrange and G. Tzanetakis, “Sound source\ntracking and formation using normalized cuts,” inProc. ICASSP , Honolulu, USA, 2007.\n[9] M. Lagrange, L.G. Martins, J. Murdoch, and\nG. Tzanetakis, “Normalized cuts for singing voiceseparation and melody extraction,” submitted to the\nIEEE Trans. on Acoustics, Speech, and Signal Pro-\ncessing (Special Issue on MIR) , 2007.\n[10] J. J. Burred, A. Robel, and X. Rodet, “An accu-\nrate timbre model for musical instruments and itsapplication to classiﬁcation,” in W orkshop on Learn-\ning the Semantics of Audio Signals , Athens, Greece,\n2006.\n[11] A.S. Bregman, Auditory Scene Analysis: The Per-\nceptual Organization of Sound , MIT Press, 1990.\n[12] D. Wang and G. J. Brown, Eds., Computational Au-\nditory Scene Analysis: Principles, Algorithms andApplications , Wiley, 2006.\n[13] S.H. Srinivasan and M. Kankanhalli, “Harmonic-\nity and dynamics based audio separation,” in Proc.\nICASSP , 2003, vol. 5, pp. v–640 – v–643.\n[14] R.J. McAulay and T.F. Quatieri, “Speech anal-\nysis/synthesis based on sinusoidal representation,”\nIEEE Trans. on Acoustics, Speech, and Signal Pro-\ncessing , vol. 34(4), pp. 744–754, 1986.\n[15] J. Shi and J. Malik, “Normalized cuts and image\nsegmentation,” IEEE Trans. on Pattern Analysis and\nMachine Intelligence , vol. 22(8), pp. 888–905, 2000.\n[16] S.H. Srinivasan, “Auditory blobs,” in Proc. ICASSP ,\n2004, vol. 4, pp. iv–313 – iv–316.\n[17] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“RWC music database: Music genre database and\nmusical instrument sound database,” in Int. Conf. on\nMusic Information Retrieval (ISMIR) , 2003.\n[18] A. Livshin and X. Rodet, “The importance of the\nnon-harmonic residual,” in AES 120th Convention\n,\nParis, France, 2006."
    },
    {
        "title": "Discovering Chord Idioms Through Beatles and Real Book Songs.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon",
            "Christopher Harte",
            "Michael A. Casey",
            "Benjamin Fields"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415564",
        "url": "https://doi.org/10.5281/zenodo.1415564",
        "ee": "https://zenodo.org/records/1415564/files/MauchDHCF07.pdf",
        "abstract": "Modern collections of symbolic and audio music content provide unprecedented possibilities for musicological research, but traditional qualitative evaluation methods cannot realistically cope with such amounts of data. We are interested in harmonic analysis and propose key-independent chord idioms derived from a bottom-up analysis of musical data as a new subject of musicological interest. In order to motivate future research on audio chord idioms and on probabilistic models of harmony we perform a quantitative study of chord progressions in two popular music collections. In particular, we extract common subsequences of chord classes from symbolic data, independent of key and context, and order them by frequency of occurrence, thus enabling us to identify chord idioms. We make musicological observations on selected chord idioms from the collections. 1 INTRODUCTION Traditional musicology consists of qualitative studies using small data sets, so that it is not possible to ascertain whether the conclusions drawn from the study are representative of a broader corpus of music. Harmonic analysis is no exception to that rule, and there is plenty of literature on harmony in Western music and Jazz (e.g. [3]). One of the disadvantages of this approach is that the choice of data and its interpretation are subjective. Music Information Retrieval methods provide us with increasingly powerful tools that can be applied to strip some of such subjectivity from the analyses by quantitatively evaluating features over large collections of music. Only in recent years has considerable effort been put into the automatic analysis of chord changes in audio and symbolic representations. Most of the efforts in audio chord analysis are concerned with the actual extraction of chords (e.g. [5], [2] and [8]) – which is an interesting and very difficult task – but do not address musicological questions. In the symbolic domain too, efforts have mainly been directed towards chord transcription. Examples include MIDI-driven harmony retrieval such as the Melisma Harmony Program [10] or harmonic labelling with Bayesian c⃝2007 Austrian Computer Society (OCG). Model Selection [9]. On the other hand, Pachet [7] considers hand-annotated chord labels and infers a notion of surprise in chord sequences and a set of chord substitution rules. 2 THE STUDY We follow Pachet by using manually extracted chord labels for our analyses. We deliberately avoid any analysis of tonality or other high-level features because we believe that they are coded implicitly in chord sequences of sufficient length. That is, the chord sequences themselves represent the evolution of harmony over time. Our aim is to create an inventory (or: dictionary) of chord sequences together with an analysis of their statistical frequencies in order to discover chord idioms, prominent chord sequences in a particular style, genre or historical period. This inventory will also function as a basis for further probabilistic research into harmonic structures. We examine two collections of manually labelled chord symbols in text form. One is Harte’s Beatles Chord Database [6] consisting of all the chords of all 180 songs featured on original Beatles studio albums. The database includes start and end times of chords in songs relative to the original Beatles’ recordings. The other collection is a transcription of the chords of 244 Jazz standards from the Real Book [12].",
        "zenodo_id": 1415564,
        "dblp_key": "conf/ismir/MauchDHCF07",
        "keywords": [
            "Harmonic analysis",
            "Musicological research",
            "Qualitative evaluation methods",
            "Key-independent chord idioms",
            "Chord progressions",
            "Music collections",
            "Probabilistic models of harmony",
            "Automatic analysis of chord changes",
            "Chord substitution rules",
            "Inventory of chord sequences"
        ],
        "content": "DISCOVERING CHORD IDIOMS\nTHROUGH BEATLES AND REAL BOOK SONGS\nMatthias Mauch Simon Dixon Christopher Harte\nQueen Mary, University of London,\nCentre for Digital Music\nmatthias.mauch@elec.qmul.ac.ukMichael Casey Benjamin Fields\nGoldsmiths, University of London,\nComputing Department\nABSTRACT\nModern collections of symbolic and audio music content\nprovide unprecedented possibilities for musicological re-\nsearch, but traditional qualitative evaluation methods can-\nnot realistically cope with such amounts of data. We are\ninterested in harmonic analysis and propose key-independ-\nentchord idioms derived from a bottom-up analysis of\nmusical data as a new subject of musicological interest.\nIn order to motivate future research on audio chord id-\nioms and on probabilistic models of harmony we perform\na quantitative study of chord progressions in two popu-\nlar music collections. In particular, we extract common\nsubsequences of chord classes from symbolic data, inde-\npendent of key and context, and order them by frequency\nof occurrence, thus enabling us to identify chord idioms.\nWe make musicological observations on selected chord id-\nioms from the collections.\n1 INTRODUCTION\nTraditional musicology consists of qualitative studies us-\ning small data sets, so that it is not possible to ascertain\nwhether the conclusions drawn from the study are repre-\nsentative of a broader corpus of music. Harmonic analysis\nis no exception to that rule, and there is plenty of literature\non harmony in Western music and Jazz (e.g. [3]). One of\nthe disadvantages of this approach is that the choice of\ndata and its interpretation are subjective. Music Informa-\ntion Retrieval methods provide us with increasingly pow-\nerful tools that can be applied to strip some of such subjec-\ntivity from the analyses by quantitatively evaluating fea-\ntures over large collections of music. Only in recent years\nhas considerable effort been put into the automatic anal-\nysis of chord changes in audio and symbolic representa-\ntions. Most of the efforts in audio chord analysis are con-\ncerned with the actual extraction of chords (e.g. [5], [2]\nand [8]) – which is an interesting and very difﬁcult task –\nbut do not address musicological questions.\nIn the symbolic domain too, efforts have mainly been\ndirected towards chord transcription. Examples include\nMIDI-driven harmony retrieval such as the Melisma Har-\nmony Program [10] or harmonic labelling with Bayesian\nc\r2007 Austrian Computer Society (OCG).Model Selection [9]. On the other hand, Pachet [7] con-\nsiders hand-annotated chord labels and infers a notion of\nsurprise in chord sequences and a set of chord substitution\nrules.\n2 THE STUDY\nWe follow Pachet by using manually extracted chord la-\nbels for our analyses. We deliberately avoid any analysis\nof tonality or other high-level features because we believe\nthat they are coded implicitly in chord sequences of suf-\nﬁcient length. That is, the chord sequences themselves\nrepresent the evolution of harmony over time.\nOur aim is to create an inventory (or: dictionary) of\nchord sequences together with an analysis of their statisti-\ncal frequencies in order to discover chord idioms , promi-\nnent chord sequences in a particular style, genre or histor-\nical period. This inventory will also function as a basis for\nfurther probabilistic research into harmonic structures.\nWe examine two collections of manually labelled chord\nsymbols in text form. One is Harte’s Beatles Chord Data-\nbase [6] consisting of all the chords of all 180 songs fea-\ntured on original Beatles studio albums. The database in-\ncludes start and end times of chords in songs relative to\nthe original Beatles’ recordings. The other collection is a\ntranscription of the chords of 244 Jazz standards from the\nReal Book [12].\n2.1 Chord Classes\nAlthough the notations in the two collections are relatively\ncompatible, the ﬁrst step was to translate the chord la-\nbels into a standardised format. A chord label consists\nof a root note r2f0; : : : ; 11g, and a chord type repre-\nsented as a tuple (b; c), where b2f0; : : : ; 11gdescribes\nthe bass note in semitones above the chord root, and c2\nfC1; : : : ; C NCgrepresents the remaining chord structure.\nMore precisely, each Ciis the set of pitches in the chord,\nexpressed in semitones above the root. Thus a chord type\nidentiﬁes the structure of the chord, e.g. a minor triad in\nroot position (0;f0;3;7g), or the second inversion of a\ndominant 7th chord (7;f0;4;7;10g).\nAlthough studying the use of speciﬁc chord types is in\nitself interesting, for discovering general chord idioms itis more advantageous to create categories of chords that\nhave a similar harmonic function (see also section 3.3). In\norder to do so we establish sets of chord classes, which we\nthen use instead of the actual chord types.\none class C1 all chords (only root note dif-\nferences count)\nﬁve classesC1 major chords, C5 and C2\nC2 minor chords\nC3 half-diminished and dimin-\nished chords\nC4 augmented chords, incl. “7+”\netc.\nC5 chords with a suspended 4th\nfullC1f0;4;7g(major)\nC2f0;3;7g(minor)\nC3f0;4;7;10g(7th)\nC4f0;4;7;10;14g(9th)\n: : :\nCNCf. . .g\nTable 1 . Three sets of chord classes, representing three\ndifferent levels of generality.\nFrom the many possible sets of chord classes, we pro-\npose the musically plausible sets in table 1. For example,\nusing the ﬁve classes set the chord types in the sequence\nDm7 | G13 | CMaj7/E | A9 |\nfrom Afternoon in Paris would be translated to (0; C2),\n(0; C1),(4; C1)and (0; C1). In the following we will al-\nways refer to the ﬁve classes set, except where stated.\n2.2 Chord Changes and Sequences\nLets= (bi; ci)i=1:::nbe a sequence of nsuccessive chord\ntypes, and d= (d1; : : : ; d n\u00001); di2f0; : : : ; 11gthe root\ndifferences (modulo 12) between them. Then we call S=\n(s; d)alength nchord sequence . For instance, a simple\nchord change is represented by a length 2 chord sequence.\nWith the difference in semitones between DandGbeing\n5, the ﬁrst chord change in the above example is\ns= ((0 ; C2);(0; C1)); d 1= 5;\nor, in a more accessible way,\nminor+5\u0000! major :\nThis representation has the advantage of not containing\nthe actual chord roots, so it is invariant to transposition.\n2.3 Chord idioms\nNot all chord sequences of same length are equally im-\nportant. The expression chord idioms denotes chord se-\nquences that we deem special. For instance, we consider\nthe ii-V-I progression one central idiom in the Jazz litera-\nture. According to [1] an idiom in natural language is“an expression in the usage of a language that\nis peculiar to itself [. . . ] in having a mean-\ning that cannot be derived from the conjoined\nmeanings of its elements [...]”\n“a style or form of artistic expression that is\ncharacteristic of [. . . ] a period [. . . ] ”\nApproaching harmony in an analogous way, we take a\nbottom-up approach in order to investigate harmonic us-\nage, rather than a top-down approach based on some (un-\nknown) underlying rule model.\nOur conception of chord idioms guides the choice of\nthe length of the chord sequences. For instance, a length\n2 chord sequence (i.e. a chord change) is not appropri-\nate because it is too short to capture local tonality and is\nakin to a word in spoken language rather than to an idiom.\nOn the other hand, chord sequences of length greater than\n8 in small or medium size collections will become very\nrare, which prevents them from being “characteristic” of\na collection (as required in the deﬁnition) and makes a\nstatistical analysis nearly impossible. The algorithm itself\ndoes not require any particular sequence length, but for the\npresent article we (somewhat arbitrarily) chose a length\nof 4 chords in order to illustrate our methodology. Four\nis also a typical phrase length. In section 2.4 we intro-\nduce two measures for chord sequence frequency whose\ncombination might help us to recognise chord idioms au-\ntomatically.\n2.4 Estimator for Chord Sequence Probability\nIt is not straight-forward to ﬁnd a good estimator for the\nprobability of a chord sequence as the overall relative fre-\nquency is biased towards repetitive patterns. Instead, for\nevery chord sequence we estimate\n1. the probability piof a chord sequence Sioccurring\nat least once in a song.\n2. the probability qithat a chord sequence in a song is\nSi, conditional on Sioccurring at least once in the\nsong.\nThese probabilities are based on the frequency at which\nthe chord sequences occur in a given music database. Note\nhowever, that – due to the fact that we consider overlap-\nping chord sequences – only piis robust to alternating\nchord changes, while qiwill count the chord sequence\nmaj+5\u0000! maj+7\u0000! maj+5\u0000! maj\nthree times in the following chord progression, not just\ntwice!\nD G |D G |D G |D G |\n3 RESULTS\n3.1 Chord Class Frequencies\nFigure 1 shows the distribution of chord classes for the\ntwo data sets. It is evident that major chords prevail inmaj min dim aug sus00.20.40.60.8\n  \nReal BookBeatlesFigure 1 . Frequencies for the ﬁve classes in the Real Book\nand the Beatles Chord Database\nboth collections. In the Beatles’ songs 76% of all chords\nare classiﬁed major and 20% minor, leaving only a tiny\nfraction to the other chord classes. The Jazz standards in\nthe Real Book are more balanced in that respect and fea-\nture only 54% of major chords leaving 33% to the minor\nchords and 6% to the diminished chords.\n3.2 The Most Commonly Used Chord Sequences\nAs argued in section 2.3 we will use length 4 sequences as\na starting point for the considerations.\n3.2.1 Beatles\nIn the Beatles songs the prevailing idiom is major chords\nalternating in ﬁfths (or fourths), appearing in 41% of the\nsongs (see table 2). Typical instances of that would be\nI-IV-I-IV , but also V-I-V-I. Maybe one could call this the\n“blues/rock back and forth” idiom. Interestingly, the six\nhighest ranking sequences contain only ﬁfth (+7 semi-\ntones) or fourth (+ 5 semitones) chord changes of major\nchords.\nThe ﬁrst chord sequence that features a minor chord\nranks at 15. The latter is the second most frequent se-\nquence of the Real Book collection, typically ii-V-I-IV .\nMoreover, that same chord sequence is also the highest\nranking one that does notrepeat a chord. All the others\ncontain chord changes that add up to 12 or 24 at some\npoint.\n3.2.2 Real Book\nAs was expected, the most common chord sequences in\nthe Real Book follow the circle of ﬁfths idiom (ascend-\ning in fourths/descending in ﬁfths). In fact, combinations\nof that kind of sequence seem to dominate all other se-\nquences. A huge difference to the Beatles can be spotted\neven at rank 1 of table 3. This chord sequence requires\none chord to have a non-diatonic note. So not only do the\nmost widely used Real Book sequences not repeat a chord,\nthey even force some kind of modulation.\n3.3 Searching for Known Chord Sequences\nSearching the chord sequences for known idioms can help\nto ﬁnd evidence for the songwriters’ inﬂuences. For in-\nstance, the opening sequence of the Beatles’ Yesterday :\nmaj+11!min+5!maj+5!minshift pi\n1maj+7! maj+5! maj+7! maj 0.41\n2maj+5! maj+7! maj+5! maj 0.40\n3maj+5! maj+5! maj+7! maj 0.31\n4maj+5! maj+7! maj+7! maj 0.24\n5maj+7! maj+7! maj+5! maj 0.23\n6maj+7! maj+5! maj+5! maj 0.23\n7maj+5! maj+2! maj+5! maj 0.22\n8maj+2! maj+5! maj+5! maj 0.21\n9maj+7! maj+10! maj+7! maj 0.19\n...\n15 min+5! maj+5! maj+5! maj 0.10\n...\n–maj+11! min+5! maj+5! min 0\n...\nTable 2 . Beatles: ranking of length 4 chord sequences\nwith empirical probability piof appearing in a song from\nthe Beatles Chord Database\nis unique within the Beatles Database, but occurs in 17\nsongs in the Real Book (rank 29) including the Charlie\nParker standard Conﬁrmation andLike Someone in Love\nby Johnny Burke.\nHowever, when classifying the chords according to the\none class set, i.e.\n\u000f+11!\u000f+5!\u000f+5!\u000f\nwe can ﬁnd more instances (with differing chord types but\nidentical root differences) in 12 other songs from the Bea-\ntles Database, including the early Do You Want to Know\na Secret andThe Long and Winding Road on their ﬁnal\nalbum.\n4 FUTURE WORK\nIn order to achieve more generality and to be able to com-\npare different harmony styles we will extend this work to\nlarger and more diverse manually transcribed collections.\nWhere no chord transcription is available we are planning\nto take advantage of the increasingly reliable methods of\nchord inference from MIDI data as mentioned above.\nJust as for melodies, the meaningful processing of chord\nsequences depends on their rhythmic structure. We have\nretrieved relative chord lengths from the Beatles Database’s\nonset and offset times as well as from the beat-aligned no-\ntation in the Real Book, which allows us to exploit har-\nmonic rhythm in the future.\nThe probabilities piandqicomplement each other and\nsuggest a two-step generative model for chord sequences\nin the sense of Temperley’s [11] generative model for melo-\ndies. Our estimates can be used as parameters in such\nmodels, and the information on harmonic rhythm could\nserve as a prior for advanced duration modelling.shift pi\n1maj+5! min+5! maj+5! maj 0.28\n2min+5! maj+5! maj+5! maj 0.25\n3maj+0! min+5! maj+5! maj 0.23\n4min+5! maj+5! min+5! maj 0.20\n...\n9min+5! maj+5! maj+9! min 0.16\n...\n14 maj+5! maj+5! maj+5! maj 0.11\n15 maj+9! min+5! min+5! maj 0.11\n16 dim+5! maj+5! min+5! maj 0.11\n...\n19 min+5! maj+7! min+5! maj 0.10\n...\n29 maj+11! min+5! maj+5! min 0.08\n...\nTable 3 . Real Book: ranking of length 4 chord sequences\nwith empirical probability piof appearing in a song from\nthe Real Book\nAnother source of improvement will be the annotation\nof section boundaries, marking the singular chord sequen-\nces at the beginning and – most importantly – the end-\ning of sections. Besides providing an alignment for chord\nidioms, this step could also aid other segmentation algo-\nrithms.\nHowever, the problem of ﬁnding special chord idioms\nremains, as pure chord sequence frequencies do not ex-\nplain the phenomenon very well. In linguistics, word fre-\nquencies have been used to ﬁnd collocations, i.e. word\npairs that appear in conjuction more often than could be\nexpected from their respective frequencies [4]. We expect\nto ﬁnd more meaningful chord sequences by using piand\nqiin conjunction with those methods. Also, functional\nchord distances can be calculated from chord sequences\nin the style of Pachet’s substitution rules [7].\nFindings in the symbolic domain instantly pose the ques-\ntion if it possible to design equivalent algorithms in the\naudio domain. The prerequisite to idiom discovery and\nanalysis in this domain is the classiﬁcation of individual\nchords or chord classes. We are developing a technique\nusing support vector machines trained with synthesised\naudio to classify segmented audio by chord type. In pre-\nliminary tests, correctness of labels is signiﬁcantly better\nthan random assignment, with frame-by-frame accuracy\nof approximately 40%. There is great potential in this\nmethod of chord labelling, though in its current state there\nis room for much improvement. Once accurate chord iden-\ntiﬁcation is achieved it is possible to examine idioms in\ndigital audio in much the same way as this paper shows in\nthe symbolic domain.5 CONCLUSIONS\nIn this paper we have presented our ﬁrst approach to har-\nmonic analysis from manually labelled chord symbols aim-\ning at the discovery of chord idioms. We have categorised\nchords into musically motivated classes with different lev-\nels of generalisation, along with a key-independent deﬁni-\ntion of chord sequences. Chord sequences were retrieved\nfrom two collections comprising over 400 songs and or-\ndered according to a proposed frequency measure. Ex-\namples show how the method can help to understand the\nuse of chords in the collections and illustrate the musico-\nlogical relevance of chord idioms. In the future we will\nexplore idioms in both the symbolic and audio domains.\n6 ACKNOWLEDGEMENTS\nThis work is part of the OMRAS2 project funded by EPSRC\ngrant EP/E017614/1.\n7 REFERENCES\n[1] Merriam-Webster Online Dictionary, April 2007.\n[2] J. P. Bello and J. Pickens. A Robust Mid-level Repre-\nsentation for Harmonic Content in Music Signals. In\nProc. ISMIR 2005, London, UK , 2005.\n[3] D. de la Motte. Harmonielehre . dtv/B ¨arenreiter, 1995.\n[4] T. Dunning. Accurate Methods for the Statistics of\nSurprise and Coincidence. Computational Linguistics ,\n19(1):61–74, 1994.\n[5] C. Harte and M. Sandler. Automatic Chord Identi-\nfcation using a Quantised Chromagram. In Proceed-\nings of 118th Convention . Audio Engineering Society,\n2005.\n[6] C. Harte, M. Sandler, S. A. Abdallah, and E. Gomez.\nSymbolic representation of musical chords: A pro-\nposed syntax for text annotations . In Proc. ISMIR\n2005, London, UK , 2005.\n[7] F. Pachet. Surprising Harmonies. In International\nJournal of Computing Anticipatory Systems , 1999.\n[8] J.-F. Paiement, D. Eck, and S. Bengio. A Probabilistic\nModel for Chord Progressions. In Proc. ISMIR 2005,\nLondon, UK , 2005.\n[9] C. Rhodes, D. Lewis, and D. M ¨ullensiefen. Bayesian\nModel Selection for Harmonic Labelling. May 2007.\nMCM Berlin.\n[10] D. Sleator and D. Temperley. The Melisma Music An-\nalyzer. web page: www.link.cs.cmu.edu, 2007.\n[11] D. Temperley. Music and Probability . The MIT Press,\nJanuary 2007.\n[12] various. The Real Book . Hal Leonard Corporation, 6th\nedition, September 2004."
    },
    {
        "title": "Sociology and Music Recommendation Systems.",
        "author": [
            "Daniel McEnnis",
            "Sally Jo Cunningham"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414854",
        "url": "https://doi.org/10.5281/zenodo.1414854",
        "ee": "https://zenodo.org/records/1414854/files/McEnnisC07.pdf",
        "abstract": "Music recommendation systems have centred on two different approaches: content based analysis and collaborative filtering. Little attention has been paid to the reasons why these techniques have been effective. Fortunately, the social sciences have asked these questions. One of the findings of this research is that social context is much more important than previously thought. This paper introduces this body of research from sociology and its relevance to music recommendation algorithms. 1 INTRODUCTION Traditionally, music has been recommended by trusted friends. Automating this process is a challenging problem since social factors play such a major role in determining what makes a good recommendation. Very little information about social factors has been utilized in the MIR music recommendation literature or why people choose to listen to the music that they do and in what contexts—correct guesses under idealized listening situations are sufficient information for MIR. Fortunately, the social science literature provides concrete research on these questions. 2 MUSIC RECOMMENDATION SYSTEMS There have been a number of music recommendation systems proposed. Generally, these have used either contentbased analysis or collaborative filtering for generating play lists. More recent systems have used user-supplied metadata and web-based data to augment content-based approaches. Logan [7] uses a purely content based approach, producing play lists either directly from similarity, or similarity to a set of songs. Pampalk et al. [10] also uses contentbased analysis but the play lists are altered by users with explicit ratings, as did Pauws [11]. Pandora.com is a commercial example of a content-based recommendation system. Two of Chen and Chen’s [3] recommendation algorithms utilize purely content-based approaches. One of Chen and Chen’s [3] algorithms utilizes a pure collaborative filtering approach. Crossen [4] utilizes user c⃝2007 Austrian Computer Society (OCG). recommendations to determine the music to play in a shared space, filtering over hand-picked genre classifications. Celma et al. [2] constructs networks of artists based on their FOAF (friend of a friend) profiles. Sandvold et al. [12] extended this with tagging and content based analysis. 3 EXISTING CONTEXT ANALYSIS WITHIN MIR Earlier work has explored the importance of context and culture in MIR. Lee et al. [6] examined the challenges of cross-language music queries. Uitdenbogerd and Schyndel provided an overview of social context, culture, and psychological foundations [14]. Moelants et al. [9] demonstrate the dangers of context-insensitive content-based analysis and the importance of context-aware tagging. 4 SOCIOLOGY Bennett [1] sums it up: ’Consumers take the structures of meaning the musical and extra-musical resources associated with particular genres of pop and combine them with meanings of their own to produce distinctive patterns of consumption and stylistic expression.’ This sociological approach approaches assume from the outset that musical preferences will differ from culture to culture and that even the meanings of the same music in different cultures will be different. This philosophy of the non-universality of both music and its meaning is inherent in all modern research in sociology.",
        "zenodo_id": 1414854,
        "dblp_key": "conf/ismir/McEnnisC07",
        "keywords": [
            "music recommendation systems",
            "content based analysis",
            "collaborative filtering",
            "social context",
            "music recommendation literature",
            "user-supplied metadata",
            "web-based data",
            "user recommendations",
            "social factors",
            "MIR music recommendation"
        ],
        "content": "SOCIOLOGY AND MUSIC RECOMMENDATION SYSTEMS\nDaniel McEnnis\nWaikato University\nComputer ScienceSally Jo Cunningham\nWaikato University\nComputer Science\nABSTRACT\nMusic recommendation systems have centred on two\ndifferent approaches: content based analysis and collabo-\nrative ﬁltering. Little attention has been paid to the rea-\nsons why these techniques have been effective. Fortu-\nnately, the social sciences have asked these questions. One\nof the ﬁndings of this research is that social context is\nmuch more important than previously thought. This pa-\nper introduces this body of research from sociology and\nits relevance to music recommendation algorithms.\n1 INTRODUCTION\nTraditionally, music has been recommended by trusted\nfriends. Automating this process is a challenging problem\nsince social factors play such a major role in determining\nwhat makes a good recommendation. Very little informa-\ntion about social factors has been utilized in the MIR mu-\nsic recommendation literature or why people choose to lis-\nten to the music that they do and in what contexts—correct\nguesses under idealized listening situations are sufﬁcient\ninformation for MIR.\nFortunately, the social science literature provides con-\ncrete research on these questions.\n2 MUSIC RECOMMENDATION SYSTEMS\nThere have been a number of music recommendation sys-\ntems proposed. Generally, these have used either content-\nbased analysis or collaborative ﬁltering for generating play\nlists. More recent systems have used user-supplied meta-\ndata and web-based data to augment content-based ap-\nproaches.\nLogan [7] uses a purely content based approach, pro-\nducing play lists either directly from similarity, or similar-\nity to a set of songs. Pampalk et al. [10] also uses content-\nbased analysis but the play lists are altered by users with\nexplicit ratings, as did Pauws [11]. Pandora.com is a com-\nmercial example of a content-based recommendation sys-\ntem. Two of Chen and Chen’s [3] recommendation algo-\nrithms utilize purely content-based approaches.\nOne of Chen and Chen’s [3] algorithms utilizes a pure\ncollaborative ﬁltering approach. Crossen [4] utilizes user\nc/circlecopyrt2007 Austrian Computer Society (OCG).recommendations to determine the music to play in a shared\nspace, ﬁltering over hand-picked genre classiﬁcations.\nCelma et al. [2] constructs networks of artists based on\ntheir FOAF (friend of a friend) proﬁles. Sandvold et al.\n[12] extended this with tagging and content based analy-\nsis.\n3 EXISTING CONTEXT ANALYSIS WITHIN MIR\nEarlier work has explored the importance of context and\nculture in MIR. Lee et al. [6] examined the challenges of\ncross-language music queries. Uitdenbogerd and Schyn-\ndel provided an overview of social context, culture, and\npsychological foundations [14]. Moelants et al. [9] demon-\nstrate the dangers of context-insensitive content-based anal-\nysis and the importance of context-aware tagging.\n4 SOCIOLOGY\nBennett [1] sums it up: ’Consumers take the structures of\nmeaning - the musical and extra-musical resources asso-\nciated with particular genres of pop - and combine them\nwith meanings of their own to produce distinctive patterns\nof consumption and stylistic expression.’\nThis sociological approach approaches assume from\nthe outset that musical preferences will differ from culture\nto culture and that even the meanings of the same music in\ndifferent cultures will be different. This philosophy of the\nnon-universality of both music and its meaning is inherent\nin all modern research in sociology.\n4.1 Locality\nMusic is interpreted in terms of how it expresses local\nissues and concerns, often quite removed from the cir-\ncumstances that inspired the music’s creation, dramati-\ncally changing the meaning across locations and especially\ncultures.\nFor example, compare ’Geordie’ hip hop culture in New-\ncastle, England to hip hop culture in Germany. Newcastle\nis a largely working class town that is homogeneous and\nhas class divisions. As a result, music, especially hip hop,\nis interpreted in terms of class struggle, not racial identity.\nIn Frankfurt, hip hop by ethnic German youth is a re-\nbellion against the assumption that they are not German\nbecause of their appearance. For this group, hip hop is\nan expression of assimilation and national identity, a radi-\ncally different meaning.A more poignant example of the inﬂuence of local cul-\nture on interpretation in music is the way Chopin has been\ncelebrated as a Polish icon over time. Each change in gov-\nernment has resulted in differing musical properties being\nassociated with his work. Interestingly, his lifetime pre-\ncedes all the interpretations, so there can not be changes\nin the composer that affected these changes in interpreta-\ntion [8].\nThis demonstrates that attempts to deﬁne universal mean-\nings for music are destined to fail, especially across cul-\ntural boundaries.\n4.2 Media Inﬂuences\n’Mediators of Taste’[5]—the individuals and entities with\ninﬂuence of trends within musical subcultures—provide\ninsights into how they exert this power.\nThere exist a number of small music publications that\nspecialize in ’discovering’ new subgroups. These pro-\ncesses do more than just report on new subgroups: they\nconstruct the group, giving it an identity [13]. This pro-\ncess initially takes ’outsider’ groups and creates an insider\nstatus in a new group. Later media on the group publicizes\nand sustain it. This pattern for socially constructing genre\nis demonstrated by the changes in social groups associated\nwith genres of dance music [13]\nMuch of this media is now available in electronic form\nfor analysis. By analyzing these media publications, key-\nwords and genres can be linked to music that had little or\nno previous social metadata.\n4.3 Changing Meanings\nThere are two distinct types of social groups: subculture\nand neo-tribes.\nA subculture is a social group that is signiﬁcantly dif-\nferent from others, and requires a signiﬁcant commitment\nto join the group. Members of a subculture share very sim-\nilar musical tastes with an insider language of niche mu-\nsic. An example of this kind of group is the Goth move-\nment [5].\nA neo-tribe refers to a loose group of people that share\na subset of musical tastes [1]. Membership tends to be\ndegrees of membership rather than discrete and involves\nlittle commitment to be a member\nBoth of these two terms describe very different ap-\nproaches to constructing communities. Software that is\nable to recognize these groups can better predict musical\ntastes by utilizing the homogeneity of these groups.\n5 ACKNOWLEDGEMENTS\nThe ﬁrst author would like to acknowledge the generous\nsupport of the Waikato Doctoral Scholarship.\n6 REFERENCES\n[1] Andy Bennet. Popular Music and Youth Culture .\nMacMillan Press Ltd, London, 2000.[2] Oscar Celma, Miquel Ram `ırez, and Perfecto Herrera.\nGetting music recommendations and ﬁltering news-\nfeeds from foaf descriptions. International Confer-\nence on Music Information Retrieval , 2005.\n[3] Hung-Chen Chen and Arbee L. P. Chen. A music rec-\nommendation system based on music data grouping\nand user interests. CIKM , 2001.\n[4] Andrew Crossen, Jay Budzik, and Kristian J. Ham-\nmond. Flytrap: Intelligent group music recommenda-\ntion. IUI, 2002.\n[5] Paul Hodkinson. Goth: Identity, Style and Subculture .\nBerg, Oxford, 2002.\n[6] Jin Ha Lee, J. Stephan Downie, and Sally Jo Cunning-\nham. Challenges in cross-cultural/multilingual music\ninformation seeking. International Conference on Mu-\nsic Information Retrieval , 2006.\n[7] Beth Logan. Music recommendation from song sets.\nInternational Conference on Music Information Re-\ntireval , 2004.\n[8] Zdzislaw Mach. Ethnicity, Identity, and Music: The\nMusical Construction of Place , chapter National an-\nthems: the case of Chopin as a national composer.\nBerg, Oxford, 1994.\n[9] Dirk Moelants, Olmo Cornelis, Marc Leman, Jos\nGansemans, Rita De Caluwe, Guy De Tr ´e, Tom\nMatth ´e, and Axel Hallez. Problems and opportunities\nof applying data- and audio-mining techniques to eth-\nnic music. International Conference on Music Infor-\nmation Retrieval , 2006.\n[10] Elias Pampalk and Martin Gasser. An implementation\nof a simple playlist generator based on audio simlarity\nmeasures and user feedback. International Conference\non Music Information Retrieval , 2006.\n[11] Steffen Pauws and Sander van de Wijdeven. User eval-\nuation of a new interactive playlist generation concept.\nInternational Conference on Music Information Re-\ntrieval , 2005.\n[12] Vegard Sandvold, Thomas Aussenac, ´Oscar Celma,\nand Perfecto Herrera. Good vibrations: Music discov-\nery through personal musical concepts. International\nConference on Music Information Retrieval , 2006.\n[13] Sarah Thorton. Club Cultures: Music, Media and Sub-\ncultural Capital . Weslyan University Press, Middle-\ntown, 1996.\n[14] Alexandra Uitdenbogerd and Ron Vvan Schyndel. A\nreview of factors affecting music recommender suc-\ncess. International Conference on Music Information\nRetrieval , 2002."
    },
    {
        "title": "jWebMiner: A Web-Based Feature Extractor.",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417679",
        "url": "https://doi.org/10.5281/zenodo.1417679",
        "ee": "https://zenodo.org/records/1417679/files/McKayF07.pdf",
        "abstract": "jWebMiner is a software package for extracting cultural features from the web. It is designed to be used for arbitrary types of MIR research, either as a stand-alone application or as part of the jMIR suite. It emphasizes extensibility, generality and an easy-to-use interface. At its most basic level, the software operates by using web services to extract hit counts from search engines. Functionality is available for calculating a variety of statistical features based on these counts, for variably weighting web sites or limiting searches only to particular sites, for excluding hits that do not contain particular filter terms, for defining synonym relationships between certain search strings, and for applying a number of additional search configurations.",
        "zenodo_id": 1417679,
        "dblp_key": "conf/ismir/McKayF07",
        "keywords": [
            "software package",
            "extracting cultural features",
            "web services",
            "statistical features",
            "generality",
            "easy-to-use interface",
            "MIR research",
            "jMIR suite",
            "extensibility",
            "filter terms"
        ],
        "content": "JWEBMINER: A WEB-BASED FEATURE EXTRACTOR \nCory McKay                             Ichiro Fujinaga \nMusic Technology Area and CIRMMT, Schulich School of Music, McGill  University \nMontreal, Quebec, Canada \ncory.mckay@mail.mcgill.ca, ich@music.mcgill.ca  \nABSTRACT \njWebMiner is a software package for extracting cult ural \nfeatures from the web. It is designed to be used fo r arbi- \ntrary types of MIR research, either as a stand-alon e ap- \nplication or as part of the jMIR suite. It emphasiz es ex- \ntensibility, generality and an easy-to-use interfac e. \nAt its most basic level, the software operates by u sing \nweb services to extract hit counts from search engi nes. \nFunctionality is available for calculating a variet y of \nstatistical features based on these counts, for var iably \nweighting web sites or limiting searches only to pa rticu- \nlar sites, for excluding hits that do not contain p articular \nfilter terms, for defining synonym relationships be tween \ncertain search strings, and for applying a number o f ad- \nditional search configurations. \n1.  JMIR AND GENERAL-PURPOSE MIR TOOLS \nMany MIR research areas are strongly dependent upon  \nthe central tasks of extracting features and applyi ng clas- \nsification algorithms to them. Examples include mus ic \nrecommendation, playlist generation, performer or c om- \nposer identification, genre classification, instrum ent \nidentification, and many others. The jMIR software suite \nhas been developed as an integrated set of tools fo r gen- \neral-purpose MIR research in these areas. jMIR is d e- \nsigned to provide an open framework that encourages  \ncollaborative research and sharing of algorithms. E ase of \nuse for researchers with varying technical backgrou nds \nis a central priority, and all components include w ell-\ndocumented GUIs. The jMIR components are all open \nsource and implemented in Java, with a plugin-based  \narchitecture that emphasizes extensibility. \nOne of the key goals of jMIR is the facilitation of  re- \nsearch that combines low-level features (based on b asic \nsignal processing and human physiology), high-level  \nfeatures (based on musical abstractions) and cultur al \nfeatures (based on sociocultural information outsid e the \nscope of musical content itself). Each of these fea ture \ntypes encompasses potentially significantly differe nt \ninformation, with the implication that combining th em \ncould improve the performance of many areas of MIR \nresearch. This supposition has been supported by ex - \nperimental gains in performance when low-level and \ncultural features have been combined in the past [8 ].  \njMIR therefore includes, among other components, a \nlow-level feature extractor for processing audio fi les \n(jAudio [4]), a high-level feature extractor for pr ocess- \ning MIDI files (jSymbolic [5]), and a web-based cul tural \nfeature extractor (jWebMiner, the subject of this p aper). 2.  CULTURAL FEATURES AND THE WEB \nThere is psychological and musicological reason to be- \nlieve that cultural factors beyond the content of m usic \nitself play an essential role in how humans interpr et and \norganize music. North and Hargreaves, for example, \nfound experimentally that the style of a piece can influ- \nence listeners’ liking for it more than the piece i tself [6], \nand Fabbri has argued that content-based technical and \nformal aspects of music represent only one of five ways \nin which musical genres can be characterized [2]. \nThe web offers a valuable source of information fro m \nwhich cultural features can be extracted. Data mine d \nfrom the web can also be useful in acquiring ground  \ntruth for use in training and evaluating MIR system s. \nA number of important MIR studies have been pub- \nlished experimentally investigating co-occurrence a naly- \nsis of web data (e.g., [1], [3], [7], [8], [9]). jW ebMiner, \nhowever, is the first out-of-the-box cultural featu re ex- \ntractor designed for general-purpose MIR research. \n3.  OVERVIEW OF JWEBMINER \njWebMiner is a software package for extracting cult ural \nfeatures from the web using web services for use in  a \nvariety of MIR research areas. At its most basic le vel, \njWebMiner operates by accessing search engines to a c- \nquire hit counts for various search strings. For ex ample, \ncalculations involving how often the names of diffe rent \nmusicians co-occur on the same web pages (compared to \nhow often they occur individually) can provide insi ghts \non the relative similarity of the musicians to each  other. \nSimilarly, the cross tabulation of song names and m usi- \ncal genres can be used to classify music by genre. Such \nbasic hit counts can result in noisy results, howev er, so it \nis necessary to include additional functionality. \njWebMiner begins by parsing either iTunes XML, \nACE XML, Weka ARFF or text files in order to acquir e \nstrings to use in searches. Users may also manually  enter \nsearch strings in the GUI (see Figure 1). The softw are \nthen accesses the web to either measure the co-\noccurrence of each value in one field with other va lues \nin the same field, or to measure the cross tabulati on of \nvalues in different fields. \nResearch has indicated (e.g., [3], [7]) that the be st \nchoice of statistical procedure for processing hit counts \ncan vary depending on the task at hand. For example , \none must consider not only the accuracy of an appro ach, \nbut also its search complexity, as web services typ ically \ninvolve daily limits on queries. jWebMiner therefor e \nallows users to choose between a variety of metrics  and \nscoring systems to base features upon. \n© 2007 Austrian Computer Society (OCG). \n   \n \n \nFigure 1.  jWebMiner’s GUI.  \nUsers can specify string synonyms so that hit count s \nwill be combined for linked synonyms. This could be  \nuseful, for example, in a genre classification task  where \nthe class names “R&B” and “RnB” are equivalent. \njWebMiner also allows user-definable filter strings . \nThe software can be set to ignore all web pages tha t do \nnot contain general filter terms such as “music,” f or ex- \nample, or application-specific terms such as “genre ” or \n“mood.” This can be useful in avoiding irrelevant a nd \nnoisy hit counts. For instance, a feature extractio n \nshould not count co-occurrences of “The Doors” with  \n“Metal” or “Rap” unless they refer to music rather than \nthe building industry or door knockers.  \nIt is also possible to set jWebMiner to limit searc hes \nto particular sites, such as the All Music Guide, P itch- \nfork, etc. in order to emphasize musically relevant  and \nreliable sites. jWebMiner also allows users to assi gn \nvarying weights to particular sites as well as to t he web \nas a whole when feature values are calculated.  \njWebMiner outputs feature values as ACM XML, \nWeka ARFF or delimited text files. Feature values m ay \nalso be accessed directly via the GUI. \n4.  JWEBMINER AND WEB SERVICES \njWebMiner utilizes web services to extract features  from \nthe web using, currently, either Yahoo! (via REST) or \nGoogle (via SOAP). The included Yahoo! license allo ws \n5000 queries per day per IP address. Users must pro vide \ntheir own Google SOAP API License, however, as \nGoogle ceased releasing new licenses in 2006.  \njWebMiner’s API includes an extensible plugin inter - \nface that facilitates the addition of further web s ervice \nresources in the future. A highly configurable sear ch \ndialog box is also available for use in debugging n ew \nimplementations and comparing specific results from  \ndifferent web services side by side. \n5.  CONCLUSIONS AND FUTURE RESEARCH \njWebMiner is a general-purpose tool for easily extr act- \ning cultural features from the web. The flexibility  of the \ninterface and the extensibility of the API encourag e ex- perimentation with various techniques. Extracted fe a- \ntures can be used directly or combined with other f eature \ntypes extracted with software such as jAudio and jS ym- \nbolic. Future research will focus on analyzing the actual \ncontent of web sites as well as utilizing additiona l web \nservice resources, such as Audioscrobbler and Amazo n. \njWebMiner and the other components of jMIR are \navailable at http://jmir.sourceforge.net. \n6.  ACKNOWLEDGEMENTS \nWe would like to thank Mark Zadel for making the de - \ntails of his research available. We would also like  to \nthank the SSHRC and the Centre for Interdisciplinar y \nResearch in Music Media and Technology for their ge n- \nerous financial support. \n7.  REFERENCES \n[1]  Ellis, D. P. W., B. Whitman, A. Berenzweig, and \nS. Lawrence. 2002. The quest for ground truth in \nmusical artist similarity. Proceedings of the \nInternational Conference on Music Information \nRetrieval.  170–7.  \n[2]  Fabbri, F. 1981. A theory of musical genres: Two \napplications. In Popular Music Perspectives,  D. \nHorn and P. Tagg, eds. Göteborg: IASPM. \n[3]  Geleijnse, G., and J. Korst. 2006. Web-based \nartist categorization. Proceedings of the \nInternational Conference on Music Information \nRetrieval.  266–71. \n[4]  McEnnis, D., C. McKay, and I. Fujinaga. 2006. \njAudio: Additions and improvements. \nProceedings of the International Conference on \nMusic Information Retrieval.  385–6. \n[5]  McKay, C., and I. Fujinaga. 2006. jSymbolic: A \nfeature extractor for MIDI files. Proceedings of \nthe International Computer Music Conference.  \n302–5. \n[6]  North, A. C., and D. J. Hargreaves. 1997. Liking \nfor musical styles. Music Scientae  1: 109–28. \n[7]  Schedl, M., T. Pohle, P. Knees, and G. Widmer. \n2006. Assigning and visualizing music genres by \nweb-based co-occurrence analysis. Proceedings \nof the International Conference on Music \nInformation Retrieval.  260–5. \n[8]  Whitman, B., and P. Smaragdis. 2002. \nCombining musical and cultural features for \nintelligent style detection. Proceedings of the \nInternational Conference on Music Information \nRetrieval. 47–52. \n[9]  Zadel, M., and I. Fujinaga. 2004. Web services \nfor music information retrieval. Proceedings of \nthe International Conference on Music \nInformation Retrieval. 478–83."
    },
    {
        "title": "Singer Identification in Polyphonic Music Using Vocal Separation and Pattern Recognition Methods.",
        "author": [
            "Annamaria Mesaros",
            "Tuomas Virtanen",
            "Anssi Klapuri"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417395",
        "url": "https://doi.org/10.5281/zenodo.1417395",
        "ee": "https://zenodo.org/records/1417395/files/MesarosVK07.pdf",
        "abstract": "This paper evaluates methods for singer identification in polyphonic music, based on pattern classification together with an algorithm for vocal separation. Classification strategies include the discriminant functions, Gaussian mixture model (GMM)-based maximum likelihood classifier and nearest neighbour classifiers using Kullback-Leibler divergence between the GMMs. A novel method of estimating the symmetric Kullback-Leibler distance between two GMMs is proposed. Two different approaches to singer identification were studied: one where the acoustic features were extracted directly from the polyphonic signal and one where the vocal line was first separated from the mixture using a predominant melody transcription system. The methods are evaluated using a database of songs where the level difference between the singing and the accompaniment varies. It was found that vocal line separation enables robust singer identification down to 0dB and -5dB singer-to-accompaniment ratios. 1 INTRODUCTION Singing voice is the main focus of attention in musical pieces with a vocal part; most people use the singers voice as the primary cue for identifying a song. Also, a natural classification of music, besides genre, is the artist name (often equivalent to singers name). A singer identification system would be useful for MIR (music information retrieval) systems in case of identifying singers for songs. The inherent difficulties lie in the nature of the problem: the voice is usually accompanied by other musical instruments and even though humans are extremely skilful in recognizing sounds in acoustic mixtures, interfering sounds usually make the automatic recognition very difficult. Two main approaches to singer identification have been studied: one where features are computed directly from the polyphonic signal and another using separation and analysis of the vocal source. Treating the polyphonic mix directly and extracting the features for classification relies on the assumption that the singing voice is sufficiently dominating in the feature values. As preprocessing, the authors of [9, 10] located the time segments where vocals c⃝2007 Austrian Computer Society (OCG). are present. After endpoint detection, in [10] the author used a fixed-length segment of 25 s to compute the features. Reported results were 82% on a number of 45 songs from 8 singers, using MFCCs as features and GMM models and maximum likelihood classification. The second approach is the separation of vocals from the polyphonic mixture. A statistical approach to vocals separation is presented in [5]. Another method to accomplish vocals separation is extracting the harmonic components of the predominant melody from the sound mixture and then resynthesizing the melody by using a sinusoidal model [1, 8]. In addition, the authors of [1] selected reliable frames of the obtained melody to get classification between the vocal and non-vocal frames. Reported results are 95% correct classification on a number of 40 songs from 10 singers, using 15 linear prediction mel cepstral coefficients and 64 components GMM maximum likelihood classification. The question that arises is which of the former methods is more robust to accompaniment influences, and to which degree. This paper gives an evaluation of different classification methods in polyphonic case and also separation of the vocal line. Mixtures with various relative levels of the singing and accompaniment were used in order to evaluate the robustness of the methods. 65 songs from 13 singers were mixed at levels starting with clean voice to 0dB and -5dB singing-to-accompaniment ratio (SAR). Classification strategies include linear and quadratic discriminant functions, GMM based maximum likelihood classifier and nearest neighbor classifiers using Kullback-Leibler divergence between GMMs of the song under analysis and the singers. The acoustic material was produced so that the accompaniment does not provide any information about the singer’s identity. This ensures that the evaluation is based on singer identification and not on the accompaniment. The paper is organised as follows. Section 2 gives general guidelines about the features and the classification",
        "zenodo_id": 1417395,
        "dblp_key": "conf/ismir/MesarosVK07",
        "keywords": [
            "pattern classification",
            "vocal separation",
            "polyphonic music",
            "singer identification",
            "acoustic features",
            "accompaniment",
            "symmetric Kullback-Leibler distance",
            "GMM-based maximum likelihood classifier",
            "Kullback-Leibler divergence",
            "linear and quadratic discriminant functions"
        ],
        "content": "SINGER IDENTIFICATION INPOLYPHONIC MUSICUSING VOCAL\nSEPARATION ANDPATTERNRECOGNITION METHODS\nAnnamariaMesaros,TuomasVirtanen,AnssiKlapuri\nTampere University of Technology\nInstitute of SignalProcessing\nABSTRACT\nThis paper evaluates methods for singer identiﬁcation in\npolyphonicmusic,basedonpatternclassiﬁcationtogether\nwithanalgorithmforvocalseparation. Classiﬁcationstra -\ntegies include the discriminant functions, Gaussian mix-\nture model (GMM)-based maximum likelihood classiﬁer\nand nearest neighbour classiﬁers using Kullback-Leibler\ndivergence between the GMMs. A novel method of esti-\nmating the symmetric Kullback-Leibler distance between\ntwoGMMsisproposed. Twodifferentapproachestosinger\nidentiﬁcation were studied: one where the acoustic fea-\ntures were extracted directly from the polyphonic signal\nand one where the vocal line was ﬁrst separated from the\nmixture using a predominant melody transcription sys-\ntem. Themethodsareevaluatedusingadatabaseofsongs\nwheretheleveldifferencebetweenthesingingandtheac-\ncompaniment varies. It was found that vocal line separa-\ntion enables robust singer identiﬁcation down to 0dB and\n-5dB singer-to-accompaniment ratios.\n1 INTRODUCTION\nSinging voice is the main focus of attention in musical\npieceswithavocalpart;mostpeopleusethesingersvoice\nas the primary cue for identifying a song. Also, a nat-\nural classiﬁcation of music, besides genre, is the artist\nname (often equivalent to singers name). A singer iden-\ntiﬁcation system would be useful for MIR (music infor-\nmationretrieval)systemsincaseofidentifyingsingersfo r\nsongs. The inherent difﬁculties lie in the nature of the\nproblem: the voice is usually accompanied by other mu-\nsical instruments and even though humans are extremely\nskilful in recognizing sounds in acoustic mixtures, inter-\nferingsoundsusuallymaketheautomaticrecognitionvery\ndifﬁcult.\nTwomainapproachestosingeridentiﬁcationhavebeen\nstudied: one where features are computed directly from\nthe polyphonic signal and another using separation and\nanalysis of the vocal source. Treating the polyphonic mix\ndirectly and extracting the features for classiﬁcation re-\nliesontheassumptionthatthesingingvoiceissufﬁciently\ndominating in the feature values. As preprocessing, the\nauthors of [9, 10] located the time segments where vocals\nc/circlecopyrt2007 AustrianComputer Society (OCG).are present. After endpoint detection, in [10] the author\nused a ﬁxed-length segment of 25 s to compute the fea-\ntures. Reportedresultswere82%onanumberof45songs\nfrom8singers,usingMFCCsasfeaturesandGMMmod-\nelsand maximum likelihood classiﬁcation.\nThe second approach is the separation of vocals from\nthe polyphonic mixture. A statistical approach to vocals\nseparation is presented in [5]. Another method to accom-\nplishvocals separation isextracting the harmonic compo-\nnents of the predominant melody from the sound mixture\nand then resynthesizing the melody by using a sinusoidal\nmodel [1, 8]. In addition, the authors of [1] selected re-\nliable frames of the obtained melody to get classiﬁcation\nbetweenthevocalandnon-vocalframes. Reportedresults\nare 95% correct classiﬁcation on a number of 40 songs\nfrom 10 singers, using 15 linear prediction mel cepstral\ncoefﬁcients and 64 components GMM maximum likeli-\nhood classiﬁcation.\nThequestionthatarisesiswhichoftheformermethods\nismorerobusttoaccompanimentinﬂuences,andtowhich\ndegree. This paper gives an evaluation of different classi-\nﬁcationmethodsinpolyphoniccaseandalsoseparationof\nthe vocal line. Mixtures withvarious relative levels of the\nsingingandaccompanimentwereusedinordertoevaluate\nthe robustness of the methods. 65 songs from 13 singers\nwere mixed at levels starting with clean voice to 0dB and\n-5dB singing-to-accompaniment ratio (SAR). Classiﬁca-\ntion strategies include linear and quadratic discriminant\nfunctions,GMMbasedmaximumlikelihoodclassiﬁerand\nnearest neighbor classiﬁers using Kullback-Leibler diver -\ngence between GMMs of the song under analysis and the\nsingers. The acoustic material was produced so that the\naccompaniment does not provide any information about\nthe singer’s identity. This ensures that the evaluation is\nbased on singer identiﬁcation and not on the accompani-\nment.\nThepaperisorganisedasfollows. Section2givesgen-\neral guidelines about the features and the classiﬁcation\nmethods, including a detailed description of the proposed\nKullback-Leibler divergence between GMMs. Section 3\nexplains the vocal separation algorithm, then in section\n4 the organization of the different classiﬁcation tasks is\ndescribed. The experimental results are presented in the\nsame section, then conclusions and future directions are\npointed out.2 FEATURES AND MODELS\nThe MFCCs (Mel-frequency cepstral coefﬁcients) have\nbeen the most successful acoustic features in speech and\nspeaker recognition systems. They have also been suc-\ncessfully used in artist identiﬁcation [4] and instrument\nidentiﬁcation. A bank of ﬁlters equally spaced in Mel-\nfrequency scale resamples the frequency axis. A discrete\ncosine transform (DCT) is applied to the mel-resolution\npower spectrum, and the lower coefﬁcients of the DCT\nare used to represent a rough shape of the spectrum. The\nfeatures used for classiﬁcation are vectors of 12 MFCCs,\ncomputed on 34 ms frames. The zeroth order coefﬁcient\nwasusedtodetectthevoicedframesandwasdiscardedin\nthe classiﬁcation. Delta-MFCCs arenot used.\n2.1 Linear and quadratic discriminant functions\nDiscriminant analysis is a simple technique for classify-\ning a set of observations into predeﬁned classes. Based\nontrainingdata,thetechniqueconstructsasetofdiscrim-\ninant functions\nLi=xTai+ci (1)\nwhereaiis a vector of discriminant coefﬁcients of class\ni,xis a feature vector and cis a constant. Given a new\nobservation, the discriminant functions are evaluated and\nthe observation is assigned to the class having the high-\nest value of the discriminant function. After individual\nframes classiﬁcation, the entire signal is assigned to the\nclass where the majority of the frames were assigned. By\nallowing cross terms, we obtain quadratic discriminant\nfunctionsoftheform xTAix+ci(Aibeingamatrix)that\ncan model more complex boundaries between classes.\n2.2 GMM-based maximum likelihood classiﬁer\nAGaussianmixturemodel(GMM)fortheprobabilityden-\nsity function (pdf) of xis deﬁned as a weighted sum of\nmultivariate normal distributions:\np(x) =N/summationdisplay\nn=1wnN(x;µn,Σn), (2)\nwhere wnis the weight of the n-th component, Nis the\nnumber of components and N(x;µn,Σn)is the pdf of\nthe multivariate normal distribution with mean vector µn\nand diagonal covariance matrix Σn. The weights wnare\nnonnegative and sum up to unity. The standard procedure\ntotrainaGMMistheexpectation-maximization(EM)al-\ngorithm, and the resulting parameters form an inherently\ndiscriminativemodelofthesingerclasses. Theclassiﬁca-\ntion principle in the maximum likelihood classiﬁcation is\ntoﬁndtheclass iwhichmaximizesthelikelihood Lofthe\nset of observations X={x1,x2,... ,xM}:\nL(X;λi) =M/productdisplay\nm=1pi(xm) (3)where λidenotes the i-th GMM and pi(xm)the value\nof its pdf for observation xm. The above criterion as-\nsumesthattheobservationprobabilitiesinsuccessivetim e\nframesare statisticallyindependent.\n2.3 Song-level nearest neighbour classiﬁer\nAsanalternativetocombiningframe-levelfeatures,song-\nlevelfeatures[4],wheretheclassiﬁcationisbasedonlon-\nger signal segments, have recently turned out to produce\ngood results in artist classiﬁcation. For example Mandel\nand Ellis [4] measured the similarity between two signals\nbythedistancebetweentheirframe-levelfeaturedistribu -\ntions.\nIn this paper we propose a similarity measure based\non symmetric Kullback-Leibler divergence to be used in\nnearest-neighbor classiﬁcation. We have a set of previ-\nously trained singer GMMs and the pdf of the observed\nfeatures of a song is modeled with a GMM. The song is\nassignedtosingerclasshavingthesmallestKLdivergence\nvalue.\nThesymmetricKullback-Leiblerdivergencebetweena\nsinger pdf p1(x)and a songpdf p2(x)isgiven by\nS(p1(x)||p2(x)) =D(p1(x)||p2(x))+D(p2(x)||p1(x)),\n(4)\nwhere theKullback-Leibler divergence Disgiven as\nD(p1(x)||p2(x)) =/integraldisplay\np1(x) logp1(x)\np2(x)dx,(5)\nwhere the integral denotes multiple integration over the\nwhole feature space. When p1(x)andp2(x)are modeled\nwith GMMs, the above integral can be solved only when\na single Gaussian is used [3]. Some methods exist for ap-\nproximating the divergence [3]. Monte-Carlo approxima-\ntion [4] for multiple Gaussians calculates the divergence\nbyusingasetofsamples x1,x2,... ,xM,drawnfromthe\ndistribution p1(x):\nD(p1(x)||p2(x))≈M/summationdisplay\nm=11\nMlogp1(xm)\np2(xm).(6)\nWhenthedimensionalityof xislarge,anaccurateapprox-\nimation requires a huge amount of samples and is there-\nforenot computationally practical.\nHere we use the observations X1=x1\n1,x1\n2,... ,x1\nM\nthat were used to train the distribution p1(x)as samples\nxm. They are the most representative samples of the dis-\ntribution, since the distribution was trained using them.\nWe observe that the resulting empirical Kullback-Leibler\ndivergence can bewrittenusing thelikelihoods (3) as\nDemp(p1(x)||p2(x)) =1\nMlogL(X1;λ1)\nL(X1;λ2).(7)\nSince the term L(X1;λ1)is ﬁxed for each model λ2, the\nempiricalKullback-Leibler divergence correspondstothe\nmaximum likelihood classiﬁcation [4].InthesymmetricempiricalKullback-Leiblerdivergence\nwe include the empirical Kullback-Leibler divergence\nDemp(p2(x)||p1(x))obtainedusingthesetofpoints X2=\nx2\n1,x2\n2,... ,x2\nNwhich are the observations used to train\nthedistribution p2(x). ThesymmetricempiricalKullback-\nLeibler divergence can then bewrittenas\nSemp(p1(x)||p2(x)) =1\nMNlogL(X1;λ1)L(X2;λ2)\nL(X1;λ2)L(X2;λ1))\n(8)\nTheabovemeasureisclosetothecross-likelihoodratio[2,\n7]withtheexceptionthatterms L(X1;λ1)andL(X2;λ2)\narein[2,7]replacedby L(X1;λ12)andL(X2;λ12),where\nthe model λ12istrained usingboth X1andX2.\n3 VOCALS SEPARATION\nFor the separation of vocals from the accompaniment, we\napply the melody transcription system [6] followed by si-\nnusoidal modeling resynthesis. Within each frame, the\nmelody transcriber estimates whether signiﬁcant melody\nlineispresent,andestimatestheMIDInotenumberofthe\nmelody line.\nIn the voice resynthesis, harmonic overtones are gen-\nerated at integer multiples of the estimated fundamental\nfrequency. Amplitudes and phases are estimated at every\n20msfromthepolyphonicsignalbycalculatingthecross-\ncorrelation between the signal and a complex exponential\nhavingtheovertonefrequency. Time-domainsignalisob-\ntained by interpolation of the parameters between succes-\nsive frames\n4 SIMULATION EXPERIMENTS\nThe database consists of 13 singers, containing both male\nandfemaleperfomerswithvaryinglevelsofsingingskills.\nFrom each singer, 4-6 melodies with length of 20-30 sec-\nonds were recorded with sampling rate of 44100 Hz and\n16 bit resolution. Each singer was given the same accom-\npaniment. This ensures that the accompaniment and the\nmixing procedures are not singer speciﬁc. All the clas-\nsiﬁcation experiments were performed using 4-fold cross\nvalidation so that the training set contains all the data of\nSAR[dB] -5051030\nLDF 2842556163\nQDF 4253576975\nGMM-A 3836536571\nGMM-KL-A 2651637378\nGMM-S 2528445057\nGMM-KL-S-1NN 2132325559\nGMM-KL-S-3NN 2642486173\nG-KL-A 1325364038\nG-Mah 2534485765\nTable1. Classiﬁersperformancesonpolyphonicmixtures\nat different SARsa singer except the one song that is tested. The reported\nresultsaretheaverage of the4experiments.\nWe used both artist-level and song-level GMMs, the\nlatter resembling the modeling in [4]. The number of\nGaussiansinallthemodelswas10. Theartist-levelGMM\nis trained with all the songs from the training set, the re-\nsulting model being associated with the singer identity.\nFor testing, the likelihood of the test song was calculated\nunder each of the 13 GMMs representing singers, and\nthe most likely singer was chosen. The song-level mod-\nelling constructs one GMM for each song, obtaining sev-\neral GMMs associated to each singer, then the test song\nis classiﬁed according to the singer of the song which is\nclosest to the one under analysis. The KL divergence dis-\ntance was used with nearest neighbor classiﬁcation, 1NN\nin artist-level GMM, 1NN and 3NN in song-level GMM.\nWealsotestedthesymmetricKLdivergencebetweenartist-\nlevel single Gaussians and the Mahalanobis distance [4].\nThe acronyms used for the described classiﬁers are the\nfollowing: LDF - linear discriminant functions; QDF -\nquadratic discriminant functions; GMM-A - artist-level\nGMMs, maximum likelihood classiﬁcation; GMM-KL-A\n- artist-level GMMs and KL divergence; GMM-S - song-\nlevel GMMs, maximum likelihood classiﬁcation; GMM-\nKL-S-1NN, GMM-KL-S-3NN - song level GMMs and\nKLdivergencewithoneandwiththreenearestneighbors;\nG-KL-A-artist-levelsingleGaussianandKLdivergence;\nG-Mah- artist-level Mahalanobis distance.\nEachclassiﬁcationexperimentwasrunforvariousSARs:\n-5dB, 0dB, 5dB, 10dB and 30dB, directly on the poly-\nphonic mixture and also on the separated vocal line from\neach type of SAR mixture. The same SAR data was used\nboth in training and testing. Also when separation was\nused, separation was applied alsoduring the training.\nIn the ﬁrst stage, the different classiﬁers were tested\nfor the various SARs and the average classiﬁcation rates\nare presented in Table 1. The linear discriminant function\nclassiﬁer is used to check the separability of the dataset;\nits classiﬁcation performance and the two best classiﬁers\naredepicted inFigure 1, left.\nWith separation, the classiﬁcation performance of the\ndiscussed classiﬁers shows visible improvement, as pre-\nsented in Table 2 and in Figure 1, right. The identiﬁca-\nSAR [dB] -5051030\nLDF 4446505946\nQDF 6361677767\nGMM-A 6775798084\nGMM-KL-A 6369827875\nGMM-S 5159717376\nGMM-KL-S-1NN 5061656567\nGMM-KL-S-3NN 5161596569\nG-KL-A 4651505148\nG-Mah 5351535148\nTable 2. Classiﬁers performances on vocals separated\nfrompolyphonic mixtures at different SARs−50510 302030405060708090100\nSinging to accompaniment ratio [dB]Correct [%]Performance of classifiers on polyphonic data\nLDF\nGMM−A\nGMM−KL−A\n−50510 302030405060708090100\nSinging to accompaniment ratio [dB]Correct [%]Performance of classifiers on separated vocals\nLDF\nGMM−A\nGMM−KL−A\nFigure 1. LDF baseline and the twobest classiﬁersfor polyphonic dat a (left)and separated vocals (right)\ntionaccuracyimprovesat0dBSARfrom36%to75%for\nGMM-A, and for GMM-KL-A it improves from 51% to\n69%. One effect of the separation procedure is that the\nnoisy sections of the melody, where no harmonic content\nisfound, are reduced tosilence.\nTheGMM-KL-Aclassiﬁerseemstobemorerobustfor\nthe nonseparated case, and it performs comparable with\nthe GMM-A classiﬁer in the separated cases. The song\nlevel modeling and 3NN KL distance classiﬁcation also\nshows robustness for the separated vocals case, but not as\nlarge improvement as the artist-level modeling. A simple\nexplanation of this is the small number of training sam-\nples,thistypeofmodelingbeingmoreappropriatetomu-\nsic classiﬁcation in large databases where an artist GMM\nhas a very large amount of data available fortraining.\n5 CONCLUSIONS\nIn this paper we tested methods for singer identiﬁcation\nin polyphonic music. Identiﬁcation on both polyphonic\nmusic and separated vocals was tested. The simulation\nresults show that singer identiﬁcation down to realistic\nSARs (0dB, -5dB) is possible. The vocals separation im-\nproves the identiﬁcation performance signiﬁcantly at low\nSARs. The proposed method for approximating the Kull-\nback-Leiblerdivergenceproducescomparableresultswith\nthe best reference methods on separated vocals. On poly-\nphonicdata,itenablesbetteraverageaccuracythantheex-\nisting approaches. The future work includes different sta-\ntistical models such as hidden Markov models and other\nclassiﬁcation methods suchas support vector machines.1\n6 REFERENCES\n[1] Fujihara, H., Kitahara, T., Goto, M., et. al. ”Singer\nIdentiﬁcation Based on Accompaniment Sound Re-\n1This work was supported by the Academy of Finland, project No.\n5213462 (Finnish centre of Excellence program 2006-2011). The au-\nthors wish to thank Matti Ryyn ¨anen for providing the algorithm for\nmelodytranscription.duction and Reliable Frame Selection”, Proc. of 6th\nISMIR, London, U.K.,2005.\n[2] Gish, H., Siu, M-H., Rohlicek, R. ”Segregation of\nspeakers for speech recognition and speaker identiﬁ-\ncation”,Proc. of ICASSP , Toronto, Canada, 1991\n[3] Hershey, J. and Olsen, P. ”Approximating the Kull-\nback Leibler Divergence Between Gaussian Mixture\nModels”, Proc. ofICASSP , Honolulu, USA, 2007\n[4] Mandel, M. and Ellis, D. ”Song-Level Features and\nSupport Vector Machines for Music Classiﬁcation”,\nProc of .6thISMIR , London, U.K.,2005.\n[5] Ozerov, A., Philippe, P., et. al. ”One Microphone\nSinging Voice Separation using Source-adapted Mod-\nels”,Proc.of2005IEEEWorkshoponApplicationsof\nSignalProc.toAudioandAcoustics ,NewYork,USA,\n2005\n[6] Ryyn ¨anen, M. and Klapuri, A. ”Transcription of the\nSinging Melody in Polyphonic Music”, Proc of .7th\nISMIR, Victoria,BC, Canada, 2006\n[7] Tsai, W-H, Wang, H-M. ”Speech utterance clustering\nbased on the maximization of within-cluster homo-\ngeneity of speaker voice characteristics”, Journal of\nthe Acoustical Society of America, no. 3, vol. 3 , 2006\n[8] Yipeng, L. and Wang, D. ”Singing Voice Separation\nfromMonauralRecordings”, Proc.of7thISMIR ,Vic-\ntoria, BC, Canada, 2006\n[9] Youngmoo, E.K.and Whitman, B. ”Singer Identiﬁca-\ntion in Popular Music using Warped Linear Predic-\ntion”,Proc. of 3rdISMIR , Paris, France, 2002\n[10] Zhang, T. ”System and Method for Automatic Singer\nIdentiﬁcation”, IEEE International Conference on\nMultimedia and Expo , Baltimore, MD,2003."
    },
    {
        "title": "A Methodology for the Segmentation and Identification of Music Works.",
        "author": [
            "Riccardo Miotto",
            "Nicola Orio"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415952",
        "url": "https://doi.org/10.5281/zenodo.1415952",
        "ee": "https://zenodo.org/records/1415952/files/MiottoO07.pdf",
        "abstract": "The identification of unknown recordings is a challenging problem that has several applications. In this paper, we focus on the identification of alternative releases of a given music work. To this end, a statistical model of the possible performances of a given score is built from the recording of a single performance. The methodology is based on the automatic segmentation of audio recordings, exploiting a technique that has been proposed for text segmentation. The segmentation is followed by the automatic extraction of a set of relevant audio features from each segment. Identification is then carried out using an application of hidden Markov models. The approach has been tested with a collection of orchestral music, showing good results in the identification of acoustic performances. 1 INTRODUCTION The automatic identification of music works has a number of applications, that range from digital right management, to automatic metadata extraction, and to music access and retrieval. Given the amount of music recordings that are continuously released, manual identification of music works is an unfeasible task. A common approach to music identification is to extract, directly from a recording in digital format, its audio fingerprint, which is a unique set of features that allows for the identification of digital copies even in presence of noise, distortion, and compression. It can be seen as a content-based signature that summarizes an audio recording. A comprehensive tutorial about audio fingerprinting techniques and applications can be found in [3]. Audio fingerprinting systems are normally designed to identify at the same time the music score, which is the symbolic notation of the music events, and the particular recording of a performance, which is an audio signal as captured by one or more microphones [15]. On the other hand, the identification of a music work may be carried out also without linking the process to a particular performance. There are some cases where this approach may be required. Music identification of broadcasted live performances may not benefit from the fingerprints of other performances, because most of the acoustic parameters may be different. c⃝2007 Austrian Computer Society (OCG). In the case of classical music, the same works may have hundreds of different recordings, and it is not feasible to collect all of them to create a different fingerprint for each recording. An alternative approach to music identification is audio watermarking. In this case, research on psychoacoustics is exploited to embed an arbitrary message, the watermark, in a digital recording without altering the human perception of the sound [2]. The message can provide metadata about the recording (such as title, author, performers), the copyright owner, and the user that purchases the digital item [6]. Similarly to fingerprints, audio watermarks should be robust to distortions, additional noise, A/D and D/A conversions, and compressions. On the other hand, watermarking techniques require that the message is embedded in the recording before its distribution, a situation that can be applied only on newly released material. This paper reports a novel methodology for automatic identification of music works from the recording of a performance, yet independently from the particular performance. Unknown music works are identified through a collection of indexed audio recordings, ideally stored in a music digital library. The approach can be considered a generalization of audio fingerprinting, because the relevant features used for identification are not linked to a particular performance of a music work. This work extends previous work on music identification based on audio to score matching [11], where performances were modeled starting from the corresponding music scores. Also in this case, identification is based on hidden Markov models (HMMs). The application scenario is the automatic labeling of performances of tonal Western music through a match with pre-labeled recordings that are already part of an incremental music collection. Audio to audio matching has been proposed in [9, 5] for classical music audio to audio matching and audio to audio alignment respectively, and in [7] for pop music. 2 HIGH LEVEL DESCRIPTION OF MUSIC PERFORMANCES The identification of music performances is based on a audio to audio matching process, which goal is to retrieve all the audio recordings from a database that represents the same musical content as the audio query. This is typically the case when the same piece of music is available in several interpretations and arrangements. The basic idea of the proposed approach is that, even if two different performances of the same music work may dramatically differ in terms of acoustic features, it is still possible to generalize the music content of a recording to model the acoustic features of other, alternative, performances of the same music work. A recording can thus be used to statistically model other recordings, providing that they are all performed from the same score. With the aim of creating a statistical model of the score directly from the analysis of a performance, the proposed methodology is based on a number of different steps. In a first step, segmentation extracts audio subsequences that have a coherent acoustic content. Audio segments are likely to be correlated to stable parts in a music score, where there is no change in the number of different voices in a polyphony. Coherent segments of audio are analyzed through a second step, called parameter extraction, which aims at computing a set of acoustic parameters that are general enough to match different performances of the same music work. In a final step described in Section 3, modeling, a HMM is automatically built from segmentation and parametrization to model music production as a stochastic process. At matching time, an unknown recording of a performance is preprocessed in order to extract the features modeled by the HMMs. All the models are ranked according to the probability of having generated the acoustic features of the unknown performance.",
        "zenodo_id": 1415952,
        "dblp_key": "conf/ismir/MiottoO07",
        "keywords": [
            "audio fingerprinting",
            "audio watermarking",
            "music identification",
            "audio to audio matching",
            "hidden Markov models",
            "score matching",
            "performance modeling",
            "music production",
            "stochastic process",
            "acoustic features"
        ],
        "content": "AMETHODOLOGYFORTHESEGMENTATIONAND\nIDENTIFICATIONOFMUSICWORKS\nRiccardoMiottoandNicolaOrio\nUniversityofPadova\nDepartmentofInformationEngineering\nABSTRACT\nTheidentiﬁcationofunknownrecordingsisachalleng-\ningproblemthathasseveralapplications. Inthispaper,\nwefocusontheidentiﬁcationofalternativereleasesofa\ngivenmusicwork.Tothisend,astatisticalmodelofthe\npossibleperformancesofagivenscoreisbuiltfromthe\nrecordingofasingleperformance. Themethodologyis\nbasedontheautomaticsegmentationofaudiorecordings,\nexploitingatechniquethathasbeenproposedfortextseg-\nmentation.Thesegmentationisfollowedbytheautomatic\nextractionofasetofrelevantaudiofeaturesfromeach\nsegment.Identiﬁcationisthencarriedoutusinganappli-\ncationofhiddenMarkovmodels.Theapproachhasbeen\ntestedwithacollectionoforchestralmusic,showinggood\nresultsintheidentiﬁcationofacousticperformances.\n1 INTRODUCTION\nTheautomaticidentiﬁcationofmusicworkshasanum-\nberofapplications,thatrangefromdigitalrightmanage-\nment,toautomaticmetadataextraction,andtomusicac-\ncessandretrieval.Giventheamountofmusicrecordings\nthatarecontinuouslyreleased,manualidentiﬁcationof\nmusicworksisanunfeasibletask.\nAcommonapproachtomusicidentiﬁcationistoex-\ntract,directlyfromarecordingindigitalformat,its audio\nﬁngerprint,whichisauniquesetoffeaturesthatallows\nfortheidentiﬁcationofdigitalcopieseveninpresenceof\nnoise,distortion,andcompression. Itcanbeseenasa\ncontent-basedsignaturethatsummarizesanaudiorecord-\ning.Acomprehensivetutorialaboutaudioﬁngerprinting\ntechniquesandapplicationscanbefoundin[3]. Audio\nﬁngerprintingsystemsarenormallydesignedtoidentifyat\nthesametimethemusicscore,whichisthesymbolicnota-\ntionofthemusicevents,andtheparticularrecordingofa\nperformance,whichisanaudiosignalascapturedbyone\normoremicrophones[15].Ontheotherhand,theidenti-\nﬁcationofamusicworkmaybecarriedoutalsowithout\nlinkingtheprocesstoaparticularperformance.Thereare\nsomecaseswherethisapproachmayberequired.Music\nidentiﬁcationofbroadcastedliveperformancesmaynot\nbeneﬁtfromtheﬁngerprintsofotherperformances,be-\ncausemostoftheacousticparametersmaybedifferent.\nc/circlecopyrt2007AustrianComputerSociety(OCG).Inthecaseofclassicalmusic,thesameworksmayhave\nhundredsofdifferentrecordings,anditisnotfeasibleto\ncollectallofthemtocreateadifferentﬁngerprintforeach\nrecording.\nAnalternativeapproachtomusicidentiﬁcationis au-\ndiowatermarking.Inthiscase,researchonpsychoacous-\nticsisexploitedtoembedanarbitrarymessage,thewa-\ntermark,inadigitalrecordingwithoutalteringthehu-\nmanperceptionofthesound[2]. Themessagecanpro-\nvidemetadataabouttherecording(suchastitle,author,\nperformers),thecopyrightowner,andtheuserthatpur-\nchasesthedigitalitem[6]. Similarlytoﬁngerprints,au-\ndiowatermarksshouldberobusttodistortions,additional\nnoise,A/DandD/Aconversions,andcompressions. On\ntheotherhand,watermarkingtechniquesrequirethatthe\nmessageisembeddedintherecordingbeforeitsdistribu-\ntion,asituationthatcanbeappliedonlyonnewlyreleased\nmaterial.\nThispaperreportsanovelmethodologyforautomatic\nidentiﬁcationofmusicworksfromtherecordingofaper-\nformance,yetindependentlyfromtheparticularperfor-\nmance. Unknownmusicworksareidentiﬁedthrougha\ncollectionofindexedaudiorecordings,ideallystoredin\namusicdigitallibrary. Theapproachcanbeconsidered\nageneralizationofaudioﬁngerprinting,becausetherele-\nvantfeaturesusedforidentiﬁcationarenotlinkedtoapar-\nticularperformanceofamusicwork.Thisworkextends\npreviousworkonmusicidentiﬁcationbasedonaudioto\nscorematching[11],whereperformancesweremodeled\nstartingfromthecorrespondingmusicscores. Alsoin\nthiscase,identiﬁcationisbasedonhiddenMarkovmod-\nels(HMMs). Theapplicationscenarioistheautomatic\nlabelingofperformancesoftonalWesternmusicthrough\namatchwithpre-labeledrecordingsthatarealreadypart\nofanincrementalmusiccollection.Audiotoaudiomatch-\ninghasbeenproposedin[9,5]forclassicalmusicaudioto\naudiomatchingandaudiotoaudioalignmentrespectively,\nandin[7]forpopmusic.\n2 HIGHLEVELDESCRIPTIONOFMUSIC\nPERFORMANCES\nTheidentiﬁcationofmusicperformancesisbasedona\naudiotoaudiomatchingprocess,whichgoalistoretrieve\nalltheaudiorecordingsfromadatabasethatrepresents\nthesamemusicalcontentastheaudioquery.Thisistyp-icallythecasewhenthesamepieceofmusicisavailable\ninseveralinterpretationsandarrangements.\nThebasicideaoftheproposedapproachisthat,evenif\ntwodifferentperformancesofthesamemusicworkmay\ndramaticallydifferintermsofacousticfeatures,itissti ll\npossibletogeneralizethemusiccontentofarecordingto\nmodeltheacousticfeaturesofother,alternative,perfor-\nmancesofthesamemusicwork.Arecordingcanthusbe\nusedtostatisticallymodelotherrecordings,providingth at\ntheyareallperformedfromthesamescore.\nWiththeaimofcreatingastatisticalmodelofthescore\ndirectlyfromtheanalysisofaperformance,theproposed\nmethodologyisbasedonanumberofdifferentsteps.Ina\nﬁrststep,segmentationextractsaudiosubsequencesthat\nhaveacoherentacousticcontent. Audiosegmentsare\nlikelytobecorrelatedtostablepartsinamusicscore,\nwherethereisnochangeinthenumberofdifferentvoices\ninapolyphony.Coherentsegmentsofaudioareanalyzed\nthroughasecondstep,called parameterextraction ,which\naimsatcomputingasetofacousticparametersthatare\ngeneralenoughtomatchdifferentperformancesofthe\nsamemusicwork.InaﬁnalstepdescribedinSection3,\nmodeling,aHMMisautomaticallybuiltfromsegmenta-\ntionandparametrizationtomodelmusicproductionasa\nstochasticprocess.Atmatchingtime,anunknownrecord-\ningofaperformanceispreprocessedinordertoextract\nthefeaturesmodeledbytheHMMs. Allthemodelsare\nrankedaccordingtotheprobabilityofhavinggenerated\ntheacousticfeaturesoftheunknownperformance.\n2.1 SegmentationoftheAudioSignal\nTheaudiorecordingofaperformanceisacontinuousﬂow\nofacousticfeatures,whichdependsonthecharacteristics\nofthemusicnotes–pitch,amplitude,andtimbre–that\nvarywithtimeaccordingtothemusicscoreandtothe\nchoicesofthemusicians.Inordertobestructured,theau-\ndioinformationhastoundergoa segmentationprocess.\nAccordingto[1],thewordsegmentationcanhavetwo\ndifferentmeanings: oneisrelatedtomusicologyandis\nnormallyusedinsymbolicmusicprocessing,whereasthe\notheronefollowsthesignalprocessingpointofviewand\nitisusedwhendealingwithacousticsignals.Thissecond\naspectofsegmentationistheoneaddressedinthispaper.\nInthiscase,theaimofsegmentationistodividea\nmusicalsignalintosubsequencesthatareboundedbythe\npresenceofmusicevents.Anevent,inthiscontext,occurs\nwheneverthecurrentpatternofamusicalpieceismodi-\nﬁed. Suchmodiﬁcationscanbeduetooneormorenew\nnotesbeingplayedorstopped.Thisapproachtosegmen-\ntationismotivatedbythecentralrolethatpitchplaysin\nmusiclanguage.Infactthesegmentationoftheacoustic\nﬂowcanbeconsideredtheprocessofhighlightingaudio\nexcerptswithastablepitch.\nTheﬁrststepoftheapproachisbasedonthecompu-\ntationofthesimilarityoftheaudioframes.Thisiscom-\nputedasthecosineoftheanglebetweenthefrequency\nrepresentationsoftwoaudioframes.Thus,given Xand\nFigure1.Exampleofsegmentationofamonophonicau-\ndiorecording,representedbyitsenergyenvelope\nYtheFouriertransformsoftwoframes:\nsim(X,Y) =X·Y\n|X| · |Y|(1)\nHighcorrelationisexpectedbetweenframeswherethe\nsamenotesareplaying,whileadropincorrelationbe-\ntweentwosubsequentframesisrelatedtoachangeinthe\nactivenotes.Similaritybetweendifferentpartsofanau-\ndiorecordingcanberepresentedwithasymmetricmatrix\nwherehighvaluesoftheelementscorrespondtohighsim-\nilarity.\nPuresimilarityvaluesbasedoncorrelationmaynotbe\ncompletelyreliableforasegmentationtask,asithasbeen\nshownfortextsegmentation,becausechangesinthelocal\ncorrelationcouldbemorerelevanttoitsabsolutevalue.\nForthisreason,segmentationhasbeencarriedoutaccord-\ningtothemethodologyproposedin[4]fortextsegmen-\ntation.Thebasicideaisthat,innon-parametricstatistic al\nanalysis,onecomparestherankofdatasetswhenqual-\nitativebehaviorissimilarbuttheabsolutequantitiesare\nunreliable. Thus,foreachcoupleofframes {X,Y }that\nrepresentsanelementofthesimilaritymatrix,thesimi-\nlarityvalueissubstitutedbyits rank,whichisdeﬁnedas\nthenumberofneighborselementswhichsimilarityisless\nthansim(X,Y).Thatis\nr(X,Y) =||{A,B}||:sim(A,B)< sim (X,Y)(2)\nwherematrixelements {A,B}representtheneighborel-\nementsofelement {X,Y }andtheoperator ||·||computes\nthenumberofelements.\nOncetherankiscomputedforeachcoupleofframes,\nhierarchicalclusteringonthesimilaritymatrixisexploi ted\ntosegmentasequenceoffeaturesincoherentpassages.\nTheclusteringstepcomputesthelocationofboundaries\nusingReynar’smaximizationalgorithm[14],amethodto\nﬁndthesegmentationthatmaximizestheinsidedensity\nofthesegments. Apreliminaryanalysisofthesegmen-\ntationstepallowedustosetathresholdfortheoptimal\nterminationofthehierarchicalclustering. Itisinterest -\ningtonotethatitispossibletotunetheterminationof\nhierarchicalclustering,inordertoobtaindifferentleve ls0  50 100 150 200 250100020003000Segment FFT Filtering\nFFT BinsFFT Values\n0 50 100 150 200 250100020003000Segment FFT Filtering\nFFT BinsFFT Values\nFigure2.Parametersextractionconsidering 70%(left)and 95%(right)oftheoverallenergy\nofclustergranularity,forinstanceatnoteleveloraccord -\ningtodifferentsourcesoraudioclasses.Figure1depicts\nthecomputedsegmentsovertheenergytrendofanaudio\nrecording\n2.2 FeatureExtractionfromSegments\nInordertoobtainageneralrepresentationofanacous-\nticperformance,eachsegmentneedstobedescribedby\nacompactsetoffeaturesthatareautomaticallyextracted.\nInlinewiththeapproachtosegmentation,alsoparameter\nextractionisbasedontheideathatpitchinformationis\nthemostrelevantforamusicidentiﬁcationtask.Because\npitchisrelatedtothepresenceofpeaksinthefrequency\nrepresentationofanaudioframe,theparameterextraction\nstepisbasedonthecomputationoflocalmaximainthe\nFouriertransformofeachsegment,averagedoverallthe\nframesinthesegment.\nThepositionsoflocalmaximaarelikelytoberelated\ntothepositionsalongthefrequencyaxisoffundamen-\ntalfrequencyandtheﬁrstharmonicsofthenotesthatare\nplayedineachframe.Inprincipleitcouldbeexpectedthat\nallthedifferentperformancesofagivenmusicworkwill\nhavesimilarspectra. Yetthisassumptiondoesnothold\nforrealperformances,becauseofdifferencesinperform-\ningstyles,timbre,roomacoustics,recordingequipment,\nandaudiopostprocessing. Amoregeneralassumption\nisthatalternativeperformanceswillhaveatleastsimilar\nlocalmaximainthefrequencyrepresentations,thatisthe\ndominantpitcheswillbeinclosepositions.\nWhencomparingthelocalmaximaofthefrequency\nrepresentation,ithastobeconsideredthatFourieranaly-\nsisisbiasedbythewindowingofasignal,whichdepends\nonthetypeandofthelengthofthewindow.Theseeffects\nareexpectedbothonthereferenceperformancesandon\ntheperformancetoberecognized.Moreover,smallvari-\nancesonthepeakspositionsarelikelytoappearbetween\ndifferentperformancesofthesamemusicwork,because\nofimprecisetuninganddifferentreferencefrequency.For\nthesereasons,thefeaturesarecomputedbyaveragingthe\nFFTvaluesofalltheframesinasegment,byselectingthe\npositionsofthelocalmaxima,andbyassociatingtoeachmaximumafrequencyintervalwiththesizeofaquarter\ntone.Figure2exempliﬁestheapproach:thelightlinesde-\npicttheaverageFFTofasegment,whilethedarkerrect-\nanglesshowtheselectedintervals.\nThenumberofintervalsiscomputedautomatically,by\nrequiringthatthesumoftheenergycomponentsthatfall\nwithintheselectedintervalsisaboveagiventhreshold.\nFigure2depictstwopossiblesetsofrelevantintervals,de -\npendingonthepercentageoftheoverallenergyrequired:\n70%ontheleftand 95%ontheright.Itcanbenotedthat\nasmallthresholdmayexcludesomeofthepeaks,which\narethusnotusedascontentdescriptors.\nFeatureextractionoftheperformancetoberecognized\niscarriedoutbycomputing,forthesetofintervalsofeach\nsegment,theamountofenergythatfallswithinthefre-\nquencyintervals.Thusfeatureextractionfortheunknown\nperformanceisdrivenbythefeatureextractionoftheper-\nformancesinthedatabase,thatistheapproachisbased\nontheexpecteddistributionoftheenergyalongthefre-\nquencyaxis.\n3 PERFORMANCEMODELINGAND\nIDENTIFICATION\nEachmusicworkismodeledbyahiddenMarkovmodel,\nwhichparametersarecomputedfromanindexedperfor-\nmance.HMMsarestochasticﬁnite-stateautomata,where\ntransitionsbetweenstatesareruledbyprobabilityfunc-\ntions[12]. Ateachtransition,thenewstateemitsaran-\ndomvectorwithagivenprobabilitydensityfunction. A\nHMM λ,madeofasetof NstatesQ={q1,... ,q N},is\ncompletelydeﬁnedby:aprobabilitydistributionforstate\ntransitions,thatistheprobabilitytogofromstate qito\nstateqj;aprobabilitydistributionforobservations,thatis\ntheprobabilitytoobservethefeatures rwheninstate qj.\nMusicworkscanbemodeledwithaHMMproviding\nthatstatesarelabeledwitheventsintheaudiorecord-\ning,transitionsmodelthetemporalevolutionoftheau-\ndiorecording,andobservationsarerelatedtotheaudio\nfeaturespreviouslyextractedthathelpdistinguishingdi f-\nferentevents. Themodelishiddenbecauseonlytheau-diofeaturescanbeobservedanditisMarkovianbecause\ntransitionsandobservationsareassumedtodependonly\nontheactualstate.\nThenumberofstatesinthemodelisproportionalto\nthenumberofsegmentsintheperformance. Inparticu-\nlar,experimentshavebeencarriedoutusingaﬁxednum-\nberof nstatesforeachsegment,wherestatescaneither\nperformaself-transitionoraforward-transitions.Asde-\nscribedin[13],ifallthestatesinagivensegmenthave\nthesameself-transitionprobability p,theprobabilityof\nhavingagivensegmentdurationisanegativebinomial.\nOncechosenthevalueof n,itispossibletocompute pon\nthebasisofthedurationofthesegments,inordertosta-\ntisticallymodeltheexpecteddurationoftheeventsofthe\nperformancetoberecognized.\nApreliminaryevaluationwithsyntheticperformances\nwheredurationshavebeenartiﬁcallymodiﬁed,showed\nthatthismodelingisrobusttolargetimingvariationsbe-\ntweentheperformancesusedtobuildthemodelsandthe\nperformancestoberecognized.Identiﬁcationratewasnot\nsubstantiallyaffectedevenwhentempowastwiceasfast\nortwiceasslow.\nFigure3depictsanexcerptofanHMM,representing\ntwostatesandtheirtransitionprobabilities. Eachstate\nintheHMMislabeledtoagivensegmentand,accord-\ninglywiththeparameterextractionstep,emitstheproba-\nbilitythatarelevantfractionoftheoverallenergyiscar-\nriedbythefrequencyintervalscomputedattheprevious\nstep. Themodelingofemissionprobabilitiesbuildupon\nanapproachtoscorefollowingandalignmentthathas\nbeenpresentedin[10]usingdynamictimewarpingand\nissimilartotheonepresentedin[11]usingHMMand,in\nfact,oneofthegoalsofthisworkwastocreateacom-\nmonframeworkwhereanunknownperformancecouldbe\nrecognizedfromeitheritsscoreoranalternativeperfor-\nmance.\n3.1 Identiﬁcation\nRecognition,oridentiﬁcation,isprobablythemostcom-\nmonapplicationofHMMs. Theidentiﬁcationproblem\nmaybestatedasfollows:\ngivenanunknownaudiorecording,described\nbyasequenceoffeatures R={r(1),· · ·,r(T)}\nandgivenasetofcompetingmodels λi:ﬁnd\nthemodel λthatmorelikelygenerated R\nThemostcommonapproachtoHMM-basedidentiﬁ-\ncation,istocomputetheprobabilitythat λigenerates R\nregardlessofthestatesequence.Thiscanbeexpressedby\nequation\nλ= arg max\niP(R|λi)(3)\nwheretheconditionalprobabilityiscomputedoverallthe\npossiblestatesequencesofamodel.Theprobabilitycan\nbecomputedefﬁcientlyusingthe forwardprobabilities .\nEvenifthisapproachisthecommonpractiseforspeech\nandgesturerecognition,itmaybearguedthatalsopathsthathavenorelationshipwiththeactualperformancegive\napositivecontributiontotheﬁnalprobability. Forin-\nstance,apossiblepath,whichcontributestotheoverall\ncomputationoftheforwardprobabilities,mayconsistin\ntheﬁrststateoftheHMMthatcontinuouslyperformsself-\ntransitions.Theseconsiderationsmotivatedthetestingo f\ntwoadditionalapproaches,withtheaimoftakingintoac-\ncountonlytheoptimalpaththatalignsthetwoperfor-\nmances. Preliminarytestsshowedthattheclassicalap-\nproachbasedontheforwardprobabilitiesoutperformsthe\notherapproachesthattakeintoaccounteithertheglobalor\nthelocaloptimalalignment.Theresultsarenotreported\ninthispaperandcanbefoundin[8].\n3.2 ComputationalComplexity\nItisknownintheliteraturethatthecomputationofthe\nforwardprobabilitiesrequires O(DTN2)time,where D\nisthenumberofcompetingmodels, Tisthedurationof\ntheaudiosequenceinanalysisframes,and Nistheaver-\nagenumberofstatesofthecompetingHMMs. Consid-\neringthat,asdescribedinSection3,eachstatemayper-\nformamaximumoftwotransitions,itcanbeshownthat\ncomplexitybecomes O(DTN). Inordertoincreaseef-\nﬁciency,thelengthoftheunknownsequenceshouldbe\nsmall,thatisthemethodshouldgivegoodresultsalso\nwithshortaudioexcerpts.\nAnimportantparameterforcomputationalcomplexity\nisthenumberofstates N. Aﬁrstapproachtoreduce\nNistocomputeacoarsesegmentations,whichcorre-\nspondstoasmallernumberofgroupofstates. Onthe\notherhand,acoarsesegmentationmaygivepoorresultsin\ntermsofemissionprobabilities,becauseasinglesegment\ncouldrepresentpartsoftheperformancewithalowinter-\nnalcoherence.Anotherapproachtoreducethecomputa-\ntionalcomplexityistouseasmallnumberofstates nfor\neachsegment,andmodelthedurationswithhighervalues\noftheself-transitionprobabilities p.Aspreviouslymen-\ntioned,inourexperimentswefoundthatsetting n= 4for\neachsegmentgaveagoodcompromise.\n4 EXPERIMENTALEVALUATION\nThemethodologyhasbeenevaluatedwithrealacoustic\ndatafromoriginalrecordingstakenfromthepersonalcol-\nlectionoftheauthors.TonalWesternmusicrepertoirehas\nbeenusedasatest-bedbecauseitiscommonpracticethat\nmusiciansinterpretamusicworkwithoutalteringpitch\ninformation,whichisthemainfeatureusedforidentiﬁ-\ncation. Theaudioperformancesusedtocreatethemod-\nelswere 206incipitsoforchestralworksofwellknown\ncomposersofBaroque,Classical,andRomanticperiods.\nAlltheincipitsusedforthemodelinghadaﬁxedlength\nof10seconds.Theaudioﬁleswereallpolyphonicrecord-\nings,withasamplingrateof44.1kHz,andtheyhavebeen\ndividedinframesof 2048samples,applyingahamming\nwindow,withanoverlapof 1024samples.Withthesepa-\nrameters,anewobservationiscomputedevery 23.2mil-Figure3.GraphicalrepresentationofHMMcorrespondingtotwogene ralsegments.\nFigure4.Rankdistributionsofcorrectmatches\nliseconds.\nTherecordingstoberecognizedwere 50differentper-\nformancesofasubsetofthemusicworksusedtobuildthe\nmodels.Alsointhiscasetheyweretheincipitofthemusic\nworks,withalengthof 8seconds.Thegoalwastohavea\nhighlikelihoodthateachunknownperformanceswasin-\ncludedinthecorrespondingperformanceinthedatabase,\neveninthecasewherethetwoperformanceshadadiffer-\nenttempo.Alltheotherparametersoftheaudioﬁleswere\nthesame.The 50audioexcerptshavebeenconsideredas\nunknownsequencestobeidentiﬁed,usingtheapproach\npresentedinSection3.1.Figure4showsthepercentages\natwhichthecorrectaudiorecordingwasrankedasthe\nmostsimilarone,andwhenitwasrankedwithintheﬁrst\ntwo,three,ﬁve,tenandtwentypositions. Asitcanbe\nseen,42outof 50queries( 84%)werecorrectlyidentiﬁed,\nwhile 45queries( 90%)returnedacorrectmatchamong\ntop3models.Moreover,only 3queries( 6%)returnedthe\ncorrectmatchaftertheﬁrst 10positionsandnoneafterthe\nﬁrst20positions.TheMeanReciprocalRank(MRR)for\nallthe50recordingswas 87.78.\n4.1 EffectsofLossyCompression\nInordertotesttherobustnessofthemethodology,weap-\npliedalossycompressionalgorithmtotheexperimental\nsetup. Inparticular,thecompressionhasbeenapplied\nonlytotheaudioexcerptstoberecognized,simulating\narealsituationwheretheelementsofthedatabaseareof\nhighqualitywhilethereisnocontrolaboutthequalityof\ntheexcerptssubmittedbytheusers.\nWecompressedtheperformancesusingMP3encod-\ning,atthreedifferentbitrates:32,64and128kbps.Inthiswaywecouldcomparedifferentlevelsofquality,from\ntwopoorbitratesuptoacommononeamongthedigi-\ntalmusicthattravelsthroughtheWeb,whichisusually\nconsideredofsatisfactoryquality.Theresultsareshown\ninTable1,whichreportsthepercentageofperformances\nthathavebeenrankedwithindifferentthresholdstogether\nwiththeMRR.Asitcanbeseen,theapproachisrobustto\nlossycompression,becausethethreebitratesgavealmost\nthesameresults,withadecreaseineffectivenessofabout\n2%comparedtotheresultswithoutcompression.\nCompression32kbps64kbps128kbps\n=1828282\n≤3888888\n≤5909090\n≤10929494\n≤2098100100\nMRR85.8685.9185.95\nTable 1. Identiﬁcation rates in presence of lossy-\ncompression, in terms of percentage of being ranked\nwithingiventhresholdsandofMeanReciprocalRank\n4.2 RobustnesstoAdditionalNoise\nInthelasttest,weveriﬁedtherobustnessofthealgorithm\nwithnoisyrecordings. Tothisend,weaddedacompo-\nnentofwhitenoisetotheaudioelementstoberecognized.\nAlsointhiscase,weﬁguredoutasituationinwhichthe\ndatabasecontainshighqualityelementswhereastheun-\nknownaudioexcerptscouldbedisturbedordamaged.A\npinknoisehasbeenaddedwithanenergyof −12,−18,\nand−24dBinrespecttothemaximumpeakofthesignal\nofthequery,whichwassetto 0dB.Clearly,highvaluesof\nnoisearenotrealistic,becauseatleastfor −12dB,some\npartsofthequerywerealmostinaudible. Ontheother\nhand,anoiseof −24dBseemedtobeagoodapproxima-\ntionofthetypicalnoiseofoldanalogtapes.\nTheresultsareshowninTable2,whichreportstheper-\ncentageofperformancesthathavebeenrankedwithindif-\nferentthresholdstogetherwiththeMRR.Theresultsshow\nthattherecognitionrateissensibletothepresenceofad-\nditionalnoise,evenifitisunlikelythatsuchpoorquality\nrecordingswillbeofinterestfortheenduser.Theresults\ncouldprobablybeimprovedbyapplyingsomenoisere-\nmovaltooltocorruptedrecordings. Yet,inordertobe\nmeaningful,thiscombinedapproachhastobeappliedtorealnoisyrecordings,whichwillbecollectedandadded\ntothetestcollectioninthefuture.\nNoiselevel-12dB-18dB-24dB\n=1325464\n≤3466674\n≤5527284\n≤10648488\n≤20749096\nMRR42.6262.9071.92\nTable2.Identiﬁcationresultsinpresenceofwhitenoise,\nin terms of percentage of being ranked within given\nthresholdsandofMeanReciprocalRank\n5 CONCLUSIONS\nAmethodologyforautomaticmusicidentiﬁcationbased\nonHMMshasbeenproposed.Themethodologyhasbeen\ntestedonacollectionofdigitalacousticperformances.\nExperimentalresultsshowedthat,atleastfortonalWest-\nernmusic,itispossibletoachieveagoodidentiﬁcation\nratethatwasabout 84%withtheoptimalconﬁgurationof\ntheparameters.\nTheseresultssuggestthattheapproachcanbesuccess-\nfullyexploitedforaretrievaltask,wheretheuserqueries\nthesystemthroughanacousticrecordingofamusicwork.\nTheautomaticidentiﬁcationofunknownrecordingscan\nbeexploitedasatoolforsupervisedmanuallabeling:the\nuserispresentedwitharankedlistofcandidatemusic\nworks,fromwhichhecanchoosethecorrectone.Inthis\nway,thetaskcanbecarriedoutalsobynonexpertusers,\nbecausetheywillbeabletodirectlycomparetherecord-\ningsoftheunknownandofthereferenceperformances\nthroughdirectlistening.Oncethattheunknownrecording\nhasbeencorrectlyrecognized,itcanbeindexedandjoint\ntothemusicaldigitallibrary,allowingustoincrementthe\ninformationstoredinsideit.\nFutureworkswillinvolvetheextensiontoothermusic\ngenres,inparticularpopandrockmusic,forwhichpre-\nliminaryresultsonasmallcollectionhavebeenalready\nobtained. Currentworksregardtherealizationofadis-\ntributedprototype,withtheaimsofincreasingthescala-\nbilityoftheapproach.\n6 REFERENCES\n[1] Aucouturier,J. SegmentationofMusicSignalsandAp-\nplicationstotheAnalysisofMusicalStructure .Mas-\nterThesis,King’sCollege,UniversityofLondon,UK,\n2001.\n[2] Boney, L., Tewﬁk, A., Hamdy, K.”Digitalwater-\nmarksforaudiosignals”, IEEEProceedingsMultime-\ndia,473–480,1996.[3]Cano,P.,Batlle,E.,Kalker,T.,Haitsma,J.”Areview\nofaudioﬁngerprinting”, JournalofVLSISignalPro-\ncessing,41,271–284,2005.\n[4]Choi, F. ”Advances in domain independent linear\ntextsegmentation”, ProceedingsoftheConferenceon\nNorthAmericanchapteroftheAssociationforCom-\nputationalLinguistics ,26–33,2000.\n[5]Dixon,S.,Widmer,G.”MATCH:amusicalignment\ntoolchest”,ProceedingsoftheInternationalConfer-\nenceofMusicInformationRetrieval ,492–497,2005.\n[6]Haitsma,J.,vanderVeen,M.,Kalker,T.,Bruekers,\nF.”Audiowatermarkingformonitoringandcopypro-\ntection”,ProceedingsoftheACMworkshopsonMul-\ntimedia,119–122,2000.\n[7]Hu,N.,Dannenberg,R.,Tzanetakis,G.”Polyphonic\naudiomatchingandalignmentformusicretrieval”,\nProceedingsoftheIEEEWorkshoponApplicationsof\nSignalProcessingtoAudioandAcoustics ,185–188,\n2003.\n[8]Miotto,R.,Orio,N.”Recognitionofmusicperfor-\nmancesthroughaudiomatching”, Proceedingsofthe\nItalianResearchConferenceonDigitalLibraryMan-\nagementSystems,inpress,2007.\n[9]M¨uller,M.,Kurth,F.,Clausen,M.”Audiomatch-\ningviachroma-basedstatisticalfeatures”, Proceed-\ningsoftheInternationalConferenceofMusicInfor-\nmationRetrieval,288–295,2005.\n[10] Orio, N., Schwarz, D.”Alignmentofmonophonic\nandpolyphonicmusictoascore”, Proceedingsofthe\nInternationalComputerMusicConference ,129–132,\n2001.\n[11] Orio,N.”Automaticrecognitionofaudiorecordings”,\nProceedingsoftheItalianResearchConferenceon\nDigitalLibraryManagementSystems ,15–20,2006.\n[12] Rabiner,L.,Juang,B. FundamentalsofSpeechRecog-\nnition.Prentice-Hall,EnglewoodCliffs,NJ,1993.\n[13] Raphael,C.”Automaticsegmentationofacousticmu-\nsical signals using hidden markov models”, IEEE\nTransactionsonPatternAnalysisandMachineIntelli-\ngence,21,360–370,1999.\n[14] Reynar,J.TopicSegmentation:AlgorithmsandAppli-\ncations.PhDThesis,ComputerandInformationSci-\nence,UniversityofPennsylvania,1998.\n[15] Suga,Y.,Kosugi,N.,Morimoto,M.”Real-timeback-\ngroundmusicmonitoringbasedoncontent-basedre-\ntrieval”,ProceedingsoftheACMInternationalCon-\nferenceonMultimedia ,120–127,2004."
    },
    {
        "title": "Robust Music Identification, Detection, and Analysis.",
        "author": [
            "Mehryar Mohri",
            "Pedro J. Moreno 0001",
            "Eugene Weinstein"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418043",
        "url": "https://doi.org/10.5281/zenodo.1418043",
        "ee": "https://zenodo.org/records/1418043/files/MohriMW07.pdf",
        "abstract": "In previous work, we presented a new approach to music identification based on finite-state transducers and Gaussian mixture models. Here, we expand this work and study the performance of our system in the presence of noise and distortions. We also evaluate a song detection method based on a universal background model in combination with a support vector machine classifier and provide some insight into why our transducer representation allows for accurate identification even when only a short song snippet is available. 1 INTRODUCTION Automatic detection and identification of music have been the subject of several recent studies both in research [5, 6, 1] and industry [16, 4]. Music identification consists of determining the identity of the song matching a partial recording supplied by the user. In addition to allowing the user to search for a song, it can be used by content distribution networks such as Google YouTube to identify copyrighted audio within their systems and for recording labels to monitor radio broadcasts to ensure correct accounting. Music identification is a challenging task because the partial recording supplied may be distorted due to noise or channel effects. Moreover, the test recording may be short and consist of just a few seconds of audio. Since the size of the database is limited another crucial task is that of music detection, that is that of determining if the recording supplied contains an in-set song. Previous work in music identification (see [2] for a recent survey) can be classified into hashing and non-hashing approaches. The hashing approach involves computing local fingerprints, that is feature values over a window, retrieving candidate songs matching the fingerprints from a database indexed by a hash table, and picking amongst the candidates using some accuracy metric. Haitsma et al. [5] used hand-crafted features of energy differences between Bark-scale cepstra. The fingerprints thus computed were looked up in a large hash table of fingerprints for all songs in the database. Ke et al. [6] used a similar approach, but selected the features automatically using boosting. Covell et al. [4] further improve on Ke and extend the technique c⃝2007 Austrian Computer Society (OCG). beyond music to broadcast news identification. Two main limitations of hashing approaches are the requirement to match a fingerprint exactly or almost exactly, and the need for a disambiguation step to reject false positive matches. In contrast, the use of Gaussian mixtures allows our system to tolerate variations in acoustic conditions more naturally. Our use of finite state transducers (FSTs) allows us to index music event sequences in an optimal and compact way and, as demonstrated in this work, is highly unlikely to yield a false positive match. Finally, this representation permits the modeling and analysis of song structure by locating similar sound sequences within a song or across multiple songs. An example of a non-hashing approach is the work of Batlle et al [1]. They proposed decoding MFCC features over the audio stream directly into a sequence of audio events, as in speech recognition. Both the decoding and the mapping of sound sequences to songs is driven by hidden Markov models (HMMs). However, the system looks only for atomic sound sequences of a particular length, presumably to control search complexity. Our own music identification system was first presented in Weinstein and Moreno [17]. Our approach is to automatically select an inventory of music sound events using clustering and train acoustic models for each such event. We then use finite-state transducers to represent music sequences and guide the search process efficiently. In contrast to previous work, ours allows recognition of an arbitrarily long song segment. In previous work, we reported the identification accuracy of our music processing system in ideal conditions. Here, we examine the problem of music detection and identification under adverse conditions, such as additive noise, time stretching and compression, and encoding at low bit rates. 2 MUSIC IDENTIFICATION",
        "zenodo_id": 1418043,
        "dblp_key": "conf/ismir/MohriMW07",
        "keywords": [
            "automatic detection",
            "music identification",
            "hashing approach",
            "non-hashing approach",
            "Gaussian mixtures",
            "finite state transducers",
            "music event sequences",
            "song detection",
            "hidden Markov models",
            "music processing system"
        ],
        "content": "ROBUST MUSIC IDENTIFICATION, DETECTION, AND ANALYSIS\nMehryar Mohri1,2, Pedro Moreno2, and Eugene Weinstein1,2\n1Courant Institute of Mathematical Sciences\n251 Mercer Street, New York, NY 10012.\n2Google Inc.\n76 Ninth Avenue, New York, NY 10011.\nABSTRACT\nIn previous work, we presented a new approach to music\nidentiﬁcation based on ﬁnite-state transducers and Gaus-\nsian mixture models. Here, we expand this work and study\nthe performance of our system in the presence of noise\nand distortions. We also evaluate a song detection method\nbased on a universal background model in combination\nwith a support vector machine classiﬁer and provide some\ninsight into why our transducer representation allows for\naccurate identiﬁcation even when only a short song snip-\npet is available.\n1 INTRODUCTION\nAutomatic detection and identiﬁcation of music have been\nthe subject of several recent studies both in research [5,\n6, 1] and industry [16, 4]. Music identiﬁcation consists\nof determining the identity of the song matching a partial\nrecording supplied by the user. In addition to allowing\nthe user to search for a song, it can be used by content\ndistribution networks such as Google YouTube to identify\ncopyrighted audio within their systems and for recording\nlabels to monitor radio broadcasts to ensure correct ac-\ncounting.\nMusic identiﬁcation is a challenging task because the\npartial recording supplied may be distorted due to noise\nor channel effects. Moreover, the test recording may be\nshort and consist of just a few seconds of audio. Since\nthe size of the database is limited another crucial task is\nthat of music detection , that is that of determining if the\nrecording supplied contains an in-set song.\nPrevious work in music identiﬁcation (see [2] for a re-\ncent survey) can be classiﬁed into hashing and non-hashing\napproaches. The hashing approach involves computing lo-\ncal ﬁngerprints, that is feature values over a window, re-\ntrieving candidate songs matching the ﬁngerprints from a\ndatabase indexed by a hash table, and picking amongst the\ncandidates using some accuracy metric. Haitsma et al. [5]\nused hand-crafted features of energy differences between\nBark-scale cepstra. The ﬁngerprints thus computed were\nlooked up in a large hash table of ﬁngerprints for all songs\nin the database. Ke et al. [6] used a similar approach, but\nselected the features automatically using boosting. Covell\net al. [4] further improve on Ke and extend the technique\nc/circlecopyrt2007 Austrian Computer Society (OCG).beyond music to broadcast news identiﬁcation.\nTwo main limitations of hashing approaches are the re-\nquirement to match a ﬁngerprint exactly or almost exactly,\nand the need for a disambiguation step to reject false pos-\nitive matches. In contrast, the use of Gaussian mixtures\nallows our system to tolerate variations in acoustic con-\nditions more naturally. Our use of ﬁnite state transducers\n(FSTs) allows us to index music event sequences in an op-\ntimal and compact way and, as demonstrated in this work,\nis highly unlikely to yield a false positive match. Finally,\nthis representation permits the modeling and analysis of\nsong structure by locating similar sound sequences within\na song or across multiple songs.\nAn example of a non-hashing approach is the work of\nBatlle et al [1]. They proposed decoding MFCC features\nover the audio stream directly into a sequence of audio\nevents, as in speech recognition. Both the decoding and\nthe mapping of sound sequences to songs is driven by hid-\nden Markov models (HMMs). However, the system looks\nonly for atomic sound sequences of a particular length,\npresumably to control search complexity.\nOur own music identiﬁcation system was ﬁrst presented\nin Weinstein and Moreno [17]. Our approach is to auto-\nmatically select an inventory of music sound events using\nclustering and train acoustic models for each such event.\nWe then use ﬁnite-state transducers to represent music se-\nquences and guide the search process efﬁciently. In con-\ntrast to previous work, ours allows recognition of an arbi-\ntrarily long song segment. In previous work, we reported\nthe identiﬁcation accuracy of our music processing system\nin ideal conditions. Here, we examine the problem of mu-\nsic detection and identiﬁcation under adverse conditions,\nsuch as additive noise, time stretching and compression,\nand encoding at low bit rates.\n2 MUSIC IDENTIFICATION\n2.1 Acoustic Modeling\nOur acoustic modeling approach consists of jointly learn-\ning an inventory of music phones and the sequence of\nphones best representing each song. We compute mel-\nfrequency cepstral coefﬁcient (MFCC) features for each\nsong. Cepstra have recently been shown to be effective in\nthe analysis of music [1, 15, 7]. We use 100ms windows\nover the feature stream, and keep the ﬁrst twelve coefﬁ-\ncients, the energy, and their ﬁrst and second derivatives to\nproduce a 39-dimensional feature vector. 200 300 400 500 600 700 800 900\n 0 2 4 6 8 10 12 14 16 18 20Edit Distance\nTraining Iteration1,024 phones512 phones256 phonesFigure 1 . Average edit distance per song vs. training iter-\nation.\nEach song is initially broken into pseudo-stationary seg-\nments. Single diagonal covariance Gaussian models are\nﬁtted to each window. We hypothesize segment bound-\naries where the KL divergence between adjacent windows\nis above an experimentally determined threshold. We then\napply divisive clustering to the song segments in which all\npoints are initially assigned to one cluster. At each clus-\ntering iteration, the centroid of each cluster is perturbed\nin two opposite directions of maximum variance to make\ntwo new clusters. Points are reassigned to the new cluster\nwith the higher likelihood [8]. In a second step we ap-\nplyk-means clustering. For each cluster, we train a single\ninitial diagonal covariance Gaussian model.\nThe standard EM algorithm for Gaussian mixture model\n(GMM) training cannot be used since there are no refer-\nence song transcriptions. Instead, we use an unsupervised\nlearning approach similar to that of [1] in which the statis-\ntics representing each music phone and the transcriptions\nare inferred simultaneously. We alternate between ﬁnding\nthe best transcription per song given the current model and\nreﬁning the GMMs given the current transcriptions.\nTo measure the convergence of our algorithm we use\nthe edit distance, here deﬁned as the minimal number of\ninsertions, substitutions, and deletions of music phones re-\nquired to transform one transcription into another. For a\nsong set Sletti(s)be the transcription of song sat iter-\nation iand ED (a, b)the edit distance of sequences aand\nb. At each iteration i, we compute the total edit distance\nCi=P\ns∈SED(ti(s), ti−1(s))as our convergence mea-\nsure. Figure 1 illustrates how this quantity changes during\ntraining for three phone inventory sizes, and shows that it\nconverges after around twenty iterations.\n2.2 Recognition Transducer\nOur music identiﬁcation system is based on weighted ﬁnite-\nstate transducers and Viterbi decoding as is common in\nspeech recognition [12]. The decoding is based on the\nacoustic model described in the previous section and a\ncompact transducer that maps music phone sequences to\ncorresponding song identiﬁers.\nGiven a ﬁnite set of songs S, the music identiﬁcation\ntask is to ﬁnd the songs in Sthat contain a query song\nsnippet x. Hence, the recognition transducer must map\nany sequence of music phones appearing in a song to the\ncorresponding song identiﬁers.\nMore formally, let ∆denote the set of music phones.\nThe song set S={x1, . . . , x m}is a set of sequences in∆∗. Afactor , orsubstring , of a sequence x∈∆∗is a se-\nquence of consecutive phones appearing in x. Thus, yis a\nfactor of xiff there exists u, v∈∆∗such that x=uyv.\nThe set of factors of xis denoted by Fact( x)and more\ngenerally the set of factors of all songs in Sis denoted by\nFact( S). A correct transcription of an in-set song snippet\nis thus an element of Fact( S). The recognition transducer\nTmust thus represent a mapping from transcription fac-\ntors to numerical song identiﬁers:\n[ [T] ] : Fact( S)→N\nx /mapsto→[ [T] ](x) =yx.(1)\nFigure 2 shows a transducer T0mapping each song to its\nidentiﬁer, when Sis reduced to three short songs. We can\nconstruct a factor transducer from T0simply by adding /epsilon1\ntransitions from the initial state to each state and by mak-\ning each state ﬁnal. However, in order for efﬁcient search\nto be possible, the transducer must further be deterministic\nand minimal. Determinizing the transducer constructed in\nthis fashion can result in an exponential size blowup. In\nour previous work [17], we gave a method for construct-\ning a compact recognition transducer Tusing weights to\nrepresent song identiﬁers with the help of weighted deter-\nminization and minimization algorithms [9, 11].\nWe have empirically veriﬁed the feasibility of this con-\nstruction. For 15,455songs, the total number of transi-\ntions of the transducer Tis about 53.0M, only about 2.1\ntimes that of the minimal deterministic transducer T0rep-\nresenting all songs. We present elsewhere a careful analy-\nsis of the size of the factor automaton of an automaton and\nprovide worst case bounds in terms of the size of the orig-\ninal automaton or transducer representing all songs [13].\nThese bounds suggest that our method can scale to a larger\nset of songs, e.g., several million songs.\n2.3 Improving Robustness\nIn the presence of noise or distortions, the recognized mu-\nsic phone sequence xmay be corrupted by decoding er-\nrors. However, the transducer Tassociating music phone\nsequences to song identiﬁers only accepts correct music\nphone sequences as inputs. To improve robustness, we\ncan compose a transducer TEwithTthat allows corrupted\ntranscriptions to also be accepted, resulting in the map-\nping[ [T◦TE] ](x). A particular corruption transducer TE\nis the edit distance transducer, which associates a cost to\neach edit operation [10]. In this case, the above composi-\ntion has the effect of allowing insertions, deletions, and\nsubstitutions to corrupt the input sequence xwhile pe-\nnalizing any path allowing such corruptions in the Viterbi\nbeam search algorithm. The costs may be determined an-\nalytically to reﬂect a desired set of penalties, or may be\nlearned to maximize identiﬁcation accuracy.\nRobustness can also be improved by including data re-\nﬂecting the expected noise and distortion conditions in the\nacoustic model training process. The resulting models are\nthen adapted to handle similar conditions in the test data.\n3 MUSIC DETECTION\nOur music detection approach relies on the use of a uni-\nversal background music phone model (UBM) model that01 mp_72:ε\n3 mp_736:ε\n6mp_736:ε2mp_240:ε\n4mp_736:ε\n7mp_28:ε10mp_2: Beatles--Let_It_Be\n5mp_240:ε mp_20:Madonna--Ray_Of_Light\n8mp_349:ε9mp_448:εmp_889:Van_Halen--Right_NowFigure 2 . Finite-state transducer T0mapping each song to its identiﬁer.\ngenerically represents all possible song sounds. This is\nsimilar to the techniques used in speaker identiﬁcation\n(e.g., [14]). The UBM is constructed by combining the\nGMMs of all the music phones. We apply a divisive clus-\ntering algorithm to yield a desired number of mixture com-\nponents.\nTo detect out-of-set songs, we compute the log-likeli-\nhood of the best path in a Viterbi search through the reg-\nular song identiﬁcation transducer and that given a trivial\ntransducer that allows only the UBM. When the likelihood\nratio of the two models is large, one can be expect the\nsong to be in the training set. However, a simple threshold\non the likelihood ratio is not a powerful enough classi-\nﬁer for accurate detection. Instead, we have been using a\ndiscriminative method for out-of-set detection. We con-\nstruct a three-dimensional feature vector [ Lr, Lb,(Lr−\nLb)] for each song snippet, where LrandLbare the log-\nlikelihoods of the best path and background acoustic mod-\nels, respectively. These serve as the features for a support\nvector machine (SVM) classiﬁer [3].\n4 EXPERIMENTS\nOur training data set consisted of 15,455songs. The aver-\nage song duration was 3.9minutes, for a total of over 1,000\nhours of training audio. The test data consisted of 1,762\nin-set and 1,856out-of-set 10-second snippets drawn from\n100in-set and 100out-of-set songs selected at random.\nThe ﬁrst and last 20seconds of each song were omitted\nfrom the test data since they were more likely to consist\nof primarily silence or very quiet audio. Our music phone\ninventory size was 1,024units, each model consisting of\n16mixture components. For the music detection exper-\niments, we also used a UBM with 16components. We\ntested the robustness of our system by applying the fol-\nlowing transformations to the audio snippets:\na. WNoise- x: additive white noise (using sox). Since\nwhite noise is a consistently broadband signal, this sim-\nulates harsh noise. xis the noise amplitude compared to\nsaturation (i.e., WNoise- 0.01is0.01of saturation).\nb. Speed- x: speed up or slow down by factor of x\n(usingsox). Radio stations frequently speed up or slow\ndown songs in order to produce more appealing sound [1].\nc. MP3- x: mp3 reencode at xkbps (using lame ). This\nsimulates compression or transmission at a lower bitrate.\nFor the detection experiments we used the LIBSVM\nimplementation with a radial basis function (RBF) kernel.\nThe accuracy was measured using 10-fold cross-validation\nand a grid search for the values of γin the RBF kernel and\nthe trade-off parameter Cof support vector machines [3].\nThe identiﬁcation and detection accuracy results are\npresented in Table 1. The identiﬁcation performance is\nalmost ﬂawless on clean data. The addition of white noise\ndegrades the accuracy when the mixing level of the noise\nis increased. This is to be expected as the higher mix-Table 1 . Identiﬁcation accuracy rates under various test\nconditions\nCondition Identiﬁcation Detection\nAccuracy Accuracy\nClean 99.4% 96 .9%\nWNoise-0.001 ( 44.0dB SNR) 98.5% 96 .8%\nWNoise-0.01 ( 24.8dB SNR) 85.5% 94 .5%\nWNoise-0.05 ( 10.4dB SNR) 39.0% 93 .2%\nWNoise-0.1 ( 5.9dB SNR) 11.1% 93 .5%\nSpeed-0.98 96.8% 96 .0%\nSpeed-1.02 98.4% 96 .4%\nSpeed-0.9 45.7% 85 .8%\nSpeed-1.1 43.2% 87 .7%\nMP3-64 98.1% 96 .6%\nMP3-32 95.5% 95 .3%\ning levels result in a low signal-to-noise ratio (SNR). The\ninclusion of noisy data in the acoustic model training pro-\ncess slightly improves identiﬁcation quality – for instance,\nin the WNoise-0.01 experiment, the accuracy improves\nfrom 85.5%to88.4%. Slight variations in playback speed\nare handled quite well by our system (high 90’s); however,\nmajor variations such as 0.9x and 1.1x cause the accuracy\nto degrade into the 40’s. MP3 recompression at low bi-\ntrates is handled well by our system.\nThe detection performance of our system is in the 90’s\nfor all conditions except the 10% speedup and slowdown.\nThis is most likely due to the spectral shift introduced by\nthe speed alteration technique. This shift results in a mis-\nmatch between the audio data and the acoustic models.\nWe believe that a time scaling method that maintains spec-\ntral characteristics would be handled better by our acous-\ntic models. We will test this assumption in future work.\n5 FACTOR UNIQUENESS ANALYSIS\nWe observed that our identiﬁcation system performs well\nwhen snippets of ﬁve seconds or longer are used. Indeed,\nthere is almost no improvement when the snippet length\nincreases from ten seconds to the full song. To further\nanalyze this, we examined the sharing of factors across\nsongs. Let two song transcriptions x1, x2∈Sshare a\ncommon factor f∈∆∗such that x1=ufv andx2=\nafc;u, v, a, c ∈∆∗. Then the sections in these two songs\ntranscribed by fare similar. Further, if a song x1has a\nrepeated factor f∈∆∗such that x1=ufvfw ;u, v, w ∈\n∆∗, then x1has two similar audio segments. If |f|is large,\nthen it is unlikely that the sharing of fis coincidental, and\nlikely represents a repeated structural element in the song.\nFigure 3 gives the number of non-unique factors over\na range of lengths. This illustrates that some sharing of\nlong elements is present, indicating similar music seg-\nments across songs. However, factor collisions decrease\nrapidly as the factor length increases. For example, we can 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000\n 0 20 40 60 80 100 120Non-unique Factors\nFactor LengthFigure 3 . Number of factors occurring in more than one\nsong in Sfor different factor lengths.\nsee that for factor length of 50, only 256out of the 24.4M\nexisting factors appear in more than one song. Consid-\nering that the average duration of a music phone in our\nexperiments is around 200ms, a factor length of 50corre-\nsponds to around ten seconds of audio. This validates our\ninitial estimate that ten seconds of music are sufﬁcient to\nuniquely map the audio to a song in our database. In fact,\neven with factor length of 25music phones, there are only\n962non-unique factors out of 23.9Mtotal factors. This\nexplains why even a ﬁve-second snippet is sufﬁcient for\naccurate identiﬁcation.\n6 CONCLUSION\nWe described a music identiﬁcation system based on Gaus-\nsian mixture models and weighted ﬁnite-state transducers\nand its performance in the presence of noise and other dis-\ntortions. Our approach allows us to leverage the robust-\nness of GMMs to maintain good accuracy in the presence\nof low to medium noise levels. In addition, the compact\nrepresentation of the mapping of music phones to songs\nallows for efﬁcient decoding, and thus high accuracy. We\nhave also implemented a music detection system using\nthe likelihoods of the decoder output as input to a sup-\nport vector machine classiﬁer and provided an empirical\nanalysis of factor uniqueness across songs, verifying that\nﬁve-second or longer song snippets are sufﬁcient for very\nlow factor collision and thus accurate identiﬁcation.\nAcknowledgements\nWe thank the members of the Google speech team, in particular Michiel\nBacchiani, Mike Cohen, Michael Riley, and Johan Schalkwyk, for their\nhelp, advice, and support. The work of Mehryar Mohri and Eugene We-\ninstein was partially supported by the New York State Ofﬁce of Sci-\nence Technology and Academic Research (NYSTAR). This project was\nalso sponsored in part by the Department of the Army Award Num-\nber W81XWH-04-1-0307. The U.S. Army Medical Research Acqui-\nsition Activity, 820 Chandler Street, Fort Detrick MD 21702-5014 is the\nawarding and administering acquisition ofﬁce. The content of this mate-\nrial does not necessarily reﬂect the position or the policy of the Govern-\nment and no ofﬁcial endorsement should be inferred.\n7 REFERENCES\n[1]E. Batlle, J. Masip, and E. Guaus. Automatic song iden-\ntiﬁcation in noisy broadcast audio. In IASTED Interna-\ntional Conference on Signal and Image Processing , Kauai,\nHawaii, 2002.[2]P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A review of\naudio ﬁngerprinting. Journal of VLSI Signal Processing\nSystems , 41:271–284, 2005.\n[3]C. Cortes and V . Vapnik. Support-vector networks. Ma-\nchine Learning , 20(3):273–297, 1995.\n[4]M. Covell and S. Baluja. Audio ﬁngerprinting: Combin-\ning computer vision & data stream processing. In Interna-\ntional Conference on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP) , Honolulu, Hawaii, 2007.\n[5]J. Haitsma, T. Kalker, and J. Oostveen. Robust audio hash-\ning for content identiﬁcation. In Content-Based Multime-\ndia Indexing (CBMI) , Brescia, Italy, September 2001.\n[6]Y . Ke, D. Hoiem, and R. Sukthankar. Computer vi-\nsion for music identiﬁcation. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 597–604, San Diego, June 2005.\n[7]Beth Logan and Ariel Salomon. A music similarity func-\ntion based on signal analysis. In IEEE International Con-\nference on Multimedia and Expo (ICME) , Tokyo, Japan,\nAugust 2001.\n[8]M.Bacchiani and M. Ostendorf. Joint lexicon, acoustic\nunit inventory and model design. Speech Communication ,\n29:99–114, November 1999.\n[9]M. Mohri. Finite-state transducers in language and speech\nprocessing. Computational Linguistics , 23(2):269–311,\n1997.\n[10] M. Mohri. Edit-distance of weighted automata: General\ndeﬁnitions and algorithms. International Journal of Foun-\ndations of Computer Science , 14(6):957–982, 2003.\n[11] M. Mohri. Statistical Natural Language Processing. In\nM. Lothaire, editor, Applied Combinatorics on Words .\nCambridge University Press, 2005.\n[12] M. Mohri, F. C. N. Pereira, and M. Riley. Weighted\nFinite-State Transducers in Speech Recognition. Com-\nputer Speech and Language , 16(1):69–88, 2002.\n[13] Mehryar Mohri, Pedro Moreno, and Eugene Weinstein.\nFactor automata of automata and applications. In 12th\nInternational Conference on Implementation and Applica-\ntion of Automata (CIAA) , Prague, Czech Republic, July\n2007.\n[14] A. Park and T.J. Hazen. ASR dependent techniques for\nspeaker identiﬁcation. In International Conference on\nSpoken Language Processing (ICSLP) , Denver, Colorado,\nUSA, September 2002.\n[15] D. Pye. Content-based methods for the management of\ndigital music. In ICASSP , pages 2437–2440, Istanbul,\nTurkey, June 2000.\n[16] A. L. Wang. An industrial-strength audio search algo-\nrithm. In International Conference on Music Information\nRetrieval (ISMIR) , Washington, DC, October 2003.\n[17] E. Weinstein and P. Moreno. Music identiﬁcation with\nweighted ﬁnite-state transducers. In International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , Honolulu, Hawaii, 2007."
    },
    {
        "title": "Drum Transcription in Polyphonic Music Using Non-Negative Matrix Factorisation.",
        "author": [
            "Arnaud Moreau",
            "Arthur Flexer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417455",
        "url": "https://doi.org/10.5281/zenodo.1417455",
        "ee": "https://zenodo.org/records/1417455/files/MoreauF07.pdf",
        "abstract": "We present a system that is based on the non-negative matrix factorisation (NMF) algorithm and is able to transcribe drum onset events in polyphonic music. The magnitude spectrogram representation of the input music is divided by the NMF algorithm into source spectra and corresponding time-varying gains. Each of these source components is classified as a drum instrument or non-drum sound and a peak-picking algorithm determines the onset times. 1 INTRODUCTION The transcription of percussive instruments in music signals is an important step to analyzing its rhythmical content, which is useful in genre classification or beat/meter detection. The detection of drum occurrences is a less difficult task than the transcription of harmonic instruments because percussive instruments in general stay constant in pitch throughout the recording. In the case of pure percussive music different approaches give reasonably accurate results [4], but if pitched instruments are also present in the signal, the task becomes very difficult because they disturb the transcription process. There are two different approaches to the problem that appear in literature: (i) Onset detection based systems [5] first search the input signal for potential drum onsets and then classify them. (ii) Separation based systems [6, 3] first use source separation",
        "zenodo_id": 1417455,
        "dblp_key": "conf/ismir/MoreauF07",
        "keywords": [
            "non-negative matrix factorization",
            "transcribe drum onset events",
            "polyphonic music",
            "magnitude spectrogram",
            "source spectra",
            "time-varying gains",
            "drum instrument",
            "non-drum sound",
            "peak-picking algorithm",
            "genre classification"
        ],
        "content": "DRUM TRANSCRIPTION IN POLYPHONIC MUSIC USING\nNON-NEGATIVE MATRIX FACTORISATION\nArnaud Moreau\nThe Austrian Research Institute\nfor Artiﬁcial Intelligence\nFreyung 6/6, A-1010 Vienna, Austria\na.moreau@gmx.netArthur Flexer\nInstitute of Medical Cybernetics\nand Artiﬁcial Intelligence\nCenter for Brain Research\nMedical University of Vienna, Austria\narthur.ﬂexer@meduniwien.ac.at\nABSTRACT\nWe present a system that is based on the non-negative\nmatrix factorisation (NMF) algorithm and is able to tran-\nscribe drum onset events in polyphonic music. The mag-\nnitude spectrogram representation of the input music is di-\nvided by the NMF algorithm into source spectra and corre-\nsponding time-varying gains. Each of these source com-\nponents is classiﬁed as a drum instrument or non-drum\nsound and a peak-picking algorithm determines the onset\ntimes.\n1 INTRODUCTION\nThe transcription of percussive instruments in music sig-\nnals is an important step to analyzing its rhythmical con-\ntent, which is useful in genre classiﬁcation or beat/meter\ndetection. The detection of drum occurrences is a less dif-\nﬁcult task than the transcription of harmonic instruments\nbecause percussive instruments in general stay constant in\npitch throughout the recording. In the case of pure percus-\nsive music different approaches give reasonably accurate\nresults [4], but if pitched instruments are also present in\nthe signal, the task becomes very difﬁcult because they\ndisturb the transcription process. There are two different\napproaches to the problem that appear in literature: (i) On-\nset detection based systems [5] ﬁrst search the input sig-\nnal for potential drum onsets and then classify them. (ii)\nSeparation based systems [6, 3] ﬁrst use source separation\nmethods like Independent Subspace Analysis, Prior Sub-\nspace Analysis or NMF to divide the input audio signal\ninto source signals and then use some sort of peak-picking\nalgorithm to ﬁnd relevant onsets. Our system can be seen\nas an extension of the work presented by Helen and Virta-\nnen [3] who classiﬁed the source signals obtained by the\nNMF algorithm into drum and non-drum signals in order\nThe authors would like to thank Helmut Sch ¨onleitner of the cul-\ntural center AKKU (http://www.akku-steyr.at) who provided the multi-\nchannel recordings that have been used to evaluate our algorithm. The\nAustrian Research Institute for Artiﬁcial Intelligence acknowledges sup-\nport from the ministries BMUKK and BMVIT.\nc\r2007 Austrian Computer Society (OCG).to extract the drum-only signal from the mixture via re-\nsynthesis. We, however, not only try to detect the drum\nsources but also classify them into single drum instru-\nments.\n2 METHOD\nThe magnitude spectrogram Xis computed using a han-\nning window of size 4096 samples (93 ms) and 75 % over-\nlap. It is therefore a matrix of size f\u0002t(resolution in\nthe frequency domain \u0002number of frames). One short-\ntime spectrum vector at frame tis modelled as a sum of c\ncomponents, each having a constant spectrum Sand time-\nvarying gain A(t). This can be written as X\u0019SA.S\nis a matrix of size f\u0002candAis a matrix of size c\u0002t.\nThe components are estimated using the NMF algorithm\nin [3].\nThe spectra and the gains obtained in the NMF decom-\nposition are used as input to the feature extraction pro-\ncess. All the features we considered are listed in Table\n1. The spectral features are computed from the source\nspectra S(f)and the temporal features are computed from\nthe time-varying gains A(t). Most of those features are\ncommonly used in pattern recognition (for details see [5]).\nNoise-likeness and percussiveness [6] measure the rough-\nness of the spectrum and the sharpness of attacks in the\ngain, respectively. Peak time and peak ﬂuctuation are the\nmedian and the interquartile range of the durations of the\npeaks ( A(t)\u00150:2 max A(t)) in the gain. Periodicity [2]\nmeasures the correlation of the time-shifted signal. In or-\nder to preserve the temporal information, not present in\nthe 10 MFCCs calculated on the source spectrum (which\ncorresponds to 1 frame), we add dynamic MFCCs and\n\u0001MFCCs which are calculated from the magnitude spec-\ntrogram ( S\u0001Apeak) of the most prominent peak in the\ntime-varying gain. Their means and standard deviations\nare used as features.\nFor classiﬁcation we use a simple one-nearest-neighbor\nalgorithm that works with the scale-invariant mahalanobis-\ndistance. To train our classiﬁer we use 22 polyphonic mu-\nsic excerpts from recordings of different music styles of\nvariable length (5 or 10 seconds) that are divided into 15–\n25 components by the NMF algorithm. These music ex-spectral features temporal features\nspectral centroid temporal centroid\nspectral kurtosis temporal kurtosis\nspectral skewness temporal skewness\nspectral rolloff crest factor\nspectral ﬂatness peak time\nspectral contrast peak ﬂuctuation\nnoise likeness percussiveness\nstandard deviation periodicity\n10 MFCCs\n20 dynamic MFCCs (mean+std)\n20 dynamic \u0001MFCCs (mean+std)\nTable 1 . Overview of all features considered in the classi-\nﬁcation process.\ncerpts have no overlap with the test data used for evalu-\nation. All of these components are hand-labelled by lis-\ntening to them after re-synthesis. The feature extraction\nis carried out on those components, resulting in 415 fea-\nture vectors, distributed as follows: 32 bass drum (BD), 56\nsnare drum (SD), 34 hihat (HH), 293 non drum (ND). The\ndistribution of the different classes is approximately the\nsame as in the test data. Initial experiments showed that\nthis method outperforms the training based on recordings\nof isolated drum sounds.\nAll time-varying gains of drum instruments are fed into\nthe peak picking algorithm. We used a slightly modiﬁed\nversion of the one presented in [1].\n3 EVALUATION AND DISCUSSION\nThe proposed system has been evaluated on a song of 1\nminute length, which has been divided into 5 second ex-\ncerpts. The data set contains a total of 260 onsets, which\nare distributed as follows: 89 BD, 55 SD, 116 HH. It is a\nrecording of contemporary jazz music played by drums,\nkeyboards and a bass guitar. The NMF algorithm was\ncarried out using 15 components ( c= 15 ). The tran-\nscribed onsets othave been compared to the reference on-\nsetsorusing the procedure proposed in [4]. Precision rate\nRp= (T\u0000fp)=T,recall rate Rr= (R\u0000tn)=Randin-\nstrument hit rate Rh= 1\u0000(fp+tn)=Rwhere fp : : : false\npositives, tn : : : true negatives, T : : : transcribed events\nandR : : : reference events, are computed. Results are\ngiven in Table 2. The obtained results only serve as illus-\ntration of the system’s capabilities since they have been\ncomputed using only one reference song. Whereas BD\nevents are transcribed very satisfactory, the recognition of\nSD and HH events is less than optimal. Paulus and Virta-\nnen [4] achieve an average Rhof96% with their method\ntranscribing drum-only music, so only our result on the\nBD transcription is acceptable considering the difference\nin difﬁculty.\nIt seems save to say that it is very difﬁcult, even for ad-\nvanced listeners, to separate the proposed classes properly.\nHowever, there seems to be room for improvements in ourBD SD HH mean\nRp%89:47 36:71 34:00 53:39\nRr%86:52 43:64 12:93 47:69\nRh%75:28\u000047:27\u000015:52 4:16\nTable 2 . Results of processing one song of 60 sec length.\nsystem: (i) More excerpts for training data of variable\nplaying styles and employing more powerful classiﬁers\n(e.g. SVMs) should deﬁnitely warrant an improvement.\n(ii) Misclassiﬁcation of a component has a big impact on\nthe overall accuracy since all the events it contains will\nbe misclassiﬁed. Classifying individual onsets instead of\ncomponents might reduce this impact. (iii) The NMF al-\ngorithm decomposes the magnitude spectrogram into con-\nstant source spectra that vary in gain over time, this is why\nit is so suitable for representing percussive instruments,\nbecause their spectrum doesn’t change over time. The\nproblem is that if pitched instruments are present in the\nmix, each note played is modelled as one component, re-\nsulting in a huge number of components. When too lit-\ntle components are chosen (as it is very often the case\nbecause of performance issues), pitched instruments are\nlikely to be mixed within the components. While anno-\ntating the training data we found out that HH events are\nmost likely to be affected by this phenomenon. HH events\nare the most frequent of the drum events in our evaluation\nsong (44.62 %). That is why component number estima-\ntion would surely provide a major improvement.\n4 REFERENCES\n[1] S. Dixon. Onset detection revisited. In Proc. of the\nDAFx , pages 133–137, Montreal, Quebec, Canada,\nSept. 18–20, 2006.\n[2] T. Heittola and A. Klapuri. Locating segments with\ndrums in music signals. In Proc. of the 3rd ISMIR ,\nFrance, October 2002.\n[3] M. Helen and T. Virtanen. Separation of drums from\npolyphonic music using non-negative matrix factor-\nization and support vector machine. In Proc. of the\n13th EUSIPCO , Antalya, Turkey, September 2005.\n[4] J. Paulus and T. Virtanen. Drum transcription with\nnon-negative spectrogram factorisation. In Proc. of the\n13th EUSIPCO , Antalya, Turkey, September 2005.\n[5] K. Tanghe, S. Degroeve, and B. De Baets. An algo-\nrithm for detecting and labeling drum events in poly-\nphonic music. In Proc. of the ﬁrst MIREX , London,\nUK, September 11-15 2005.\n[6] C. Uhle, C. Dittmar, and T. Sporer. Extraction of drum\ntracks from polyphonic music using independent sub-\nspace analysis. In Proc. of the 4th ICA , Nara, Japan,\nApril 2003."
    },
    {
        "title": "Evaluating a Chord-Labelling Algorithm.",
        "author": [
            "Daniel Müllensiefen",
            "David Lewis 0001",
            "Christophe Rhodes",
            "Geraint A. Wiggins"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417779",
        "url": "https://doi.org/10.5281/zenodo.1417779",
        "ee": "https://zenodo.org/records/1417779/files/MullensiefenLRW07.pdf",
        "abstract": "This paper outlines a method for evaluating a new chordlabelling algorithm using symbolic data as input. Excerpts from full-score transcriptions of 40 pop songs are used. The accuracy of the algorithm’s output is compared with that of chord labels from published song books, as assessed by experts in pop music theory. We are interested not only in the accuracy of the two sets of labels but also in the question of potential harmonic ambiguity as reflected the judges’ assessments. We focus, in this short paper, on outlining the general approach of this research project. 1 INTRODUCTION AND BACKGROUND Our motivation comes from the need to derive sequences of chord labels from transcriptions of pop songs for a current project 1 hosted at Goldsmiths College, one subtask of which is to provide a summary of the harmonic structure of a song in the form of a sequence of chord labels, of the sort used in lead sheet notation. Several algorithms have been proposed that assign chord labels to points in time, based on note events in a score-like data structure (see e.g. [6]), but none of these algorithms proved fully suited for our purpose. We require the algorithm not only to give the chord root and chord type, functional bass note and extensions for the note events in a time window but also to decide on the optimal width of the time window itself and, furthermore, deal with music where the structures of classical harmony may apply to only a limited extent. We have proposed [5] using Bayesian model selection to tackle segmentation into appropriate time windows and chord label assignment simultaneously. An initial evaluation using manually-generated ground truth showed an accuracy of around 75% for root and type of the chord at each beat of the test set. This preliminary evaluation raised some concerns and questions that motivated this paper; chief among these was the way in which the ambiguity of the task is not considered in ground-truth-based evaluation. 1 http://doc.gold.ac.uk/isms/m4s c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417779,
        "dblp_key": "conf/ismir/MullensiefenLRW07",
        "keywords": [
            "pop songs",
            "chordlabelling algorithm",
            "symbolic data",
            "full-score transcriptions",
            "pop music theory",
            "harmonic ambiguity",
            "Bayesian model selection",
            "ground truth",
            "accuracy",
            "chord root and type"
        ],
        "content": "EVALUATINGA CHORD-LABELLINGALGORITHM\nDaniel M ¨ullensiefen, DavidLewis, Christophe Rhodes, GeraintWigg ins\nDepartmentofComputing,Goldsmiths,UniversityofLondon ,London,SE14 6NW\n{d.mullensiefen,d.lewis,c.rhodes,g.wiggins }@gold.ac.uk\nABSTRACT\nThis paper outlines a method for evaluatinga new chord-\nlabellingalgorithmusingsymbolicdataasinput. Excerpts\nfrom full-score transcriptions of 40 pop songs are used.\nThe accuracy of the algorithm’s output is compared with\nthat of chord labels from published song books, as as-\nsessed by experts in pop music theory. We are interested\nnotonlyintheaccuracyofthetwosetsoflabelsbutalsoin\nthe question of potential harmonic ambiguity as reﬂected\nthe judges’assessments. We focus,in thisshort paper,on\noutliningthegeneralapproachofthisresearchproject.\n1 INTRODUCTION ANDBACKGROUND\nOur motivation comes from the need to derive sequences\nofchordlabelsfromtranscriptionsofpopsongsforacur-\nrent project1hosted at Goldsmiths College, one subtask\nof which is to provide a summary of the harmonic struc-\nture of a song in the form of a sequence of chord labels,\nofthe sort used in leadsheet notation. Severalalgorithms\nhave been proposed that assign chord labels to points in\ntime, based on note events in a score-like data structure\n(seee.g.[6]), but none of these algorithms proved fully\nsuited forour purpose. We requirethe algorithmnot only\ntogivethechordrootandchordtype,functionalbassnote\nand extensions for the note events in a time window but\nalso to decide on the optimal width of the time window\nitself and, furthermore, deal with music where the struc-\ntures of classical harmony may apply to only a limited\nextent.\nWe have proposed[5] using Bayesian model selection\ntotacklesegmentationintoappropriatetimewindowsand\nchord label assignment simultaneously. An initial evalu-\nation using manually-generated ground truth showed an\naccuracy of around 75% for root and type of the chord\nat each beat of the test set. This preliminary evaluation\nraised some concerns and questions that motivated this\npaper; chief among these was the way in which the am-\nbiguityofthetaskisnotconsideredinground-truth-based\nevaluation.\n1http://doc.gold.ac.uk/isms/m4s\nc/circlecopyrt2007AustrianComputerSociety(OCG).1.1 Inherentambiguityofthechord-labellingtask\nFrom the classical and pop music analysis literature [4,\n1, 3] there are several well-known cases where harmonic\nambiguityhas no simple solution andthe choiceof chord\nlabeldependsonwhattheanalystwishestoconvey.These\nsources of ambiguity include: the ambiguities in chord\nroot assignment; the incompatibility of a musical style\nwithclassicalharmony;andtherelativeautonomyofbass\nnote and harmony. They pose a challenge for the ap-\nproach of evaluating an algorithm against a ground truth\nthat strictly allows only for a single correct chord label\nassigned to a particular set of pitch classes. In many sit-\nuations there is no single correct answer, or there may be\nseveral acceptable ones. This is true not only for chord\nlabelling but also for other tasks in MIR, such as genre\nclassiﬁcation,songsegmentationorchorusﬁnding.\nWe try to answer two questions regarding the evalua-\ntionofambiguousdata. Firstly,towhat degreedoexperts\nagree(ordisagree)aboutagivenchord-labellingsolution ?\nIf there is high agreement among human experts that a\nchord label is wrong and they propose identical or simi-\nlar corrections, this may be taken to indicate that there is\ngenerally one correct solution to chord labelling. In this\ncase, the traditional approach of working with deﬁnitive,\ncontext-free ground-truth data can be justiﬁed. The sec-\nond question relates to the performance of our labelling:\ndoes the agreement between the labelling and the human\nexperts differ signiﬁcantly from the agreement between\ntheexperts? Or,inotherwords,arethecomputer-generated\nlabelssigniﬁcantlyworsethanthebaselineasgivenbythe\nexperts’responses?\n2 METHOD\n2.1 The chordlabelling algorithm\nOur chord-labelling algorithm consists of three modules\nthat determine chord-type and root, functional bass note,\nand chord extensions. The core module is the Bayesian\nchord-type and root model that also decides on the ap-\npropriate window size for labelling. The window size is\nthen fed as an input to the bass note and extensionsmod-\nules. The details of the novel Bayesian core module are\ndescribed in [5]; it consists of two essential parts. The\nﬁrst is a class of models for pitch-class contributions to\na window given a triadic chord (we currently consider\nthe chord types major, minor, sus4, sus9, augmented anddiminished), modeled using the Dirichlet distribution for\nproportions. Thesecondcomponentofthechordrootand\ntype labelling scheme decides what regions to treat as a\nuniﬁedwhole. Forthis,weuseBayesianModelSelection\n(seee.g.[2]),andcurrentlyconsiderallpossiblebeat-wise\nsubdivisionsof a bar. Fordeterminingthefunctionalbass\nnoteofatimewindowweusearulesystemthatgenerally\nfavours longer and more prominent pitch classes sound-\ninginthelowerregister. Asourcoremodelcurrentlyonly\ntakes a set of pitch classes and no voice-leadinginforma-\ntion into account,we restrict ourselvesto labellingexten -\nsions as notes that are not part of the model-derivedtriad\nandthathavea signiﬁcantdurationin thechord.\n2.2 The evaluationmethod\nWe obtaineddetailedfeedbackfromfourhighly-qualiﬁed\nexperts on chord labellings of excerpts of 40 pop songs\naslabelledfor songbooksandby ouralgorithm;each ex-\npert is an academic musician with substantial experience\nin score reading and pop harmony analysis. Their task\nwas to compare chord labels in a lead sheet-like repre-\nsentation with a score of the song (taken from MIDI, via\nSibelius). They were asked to indicate whether the chord\nlabellingwascorrect,foreachbeatofeachbaronthelead\nsheet,evaluatingseparatelythecorrectnessofchordsym-\nbol(chord-typeandroot),bassnote,andchordextensions.\nAudio realisations of the songs were provided to the\nexpertsonCD,andtenexcerpts(differentforeachexpert)\nwere providedwithout scores. In these cases, the experts\nwere instructed to perform their assessment of the chord\nlabellingsbyearonly.\n2.3 Coherenceofexperts’judgements\nOur evaluation can be divided into two steps that corre-\nspond to the two main questions discussed in 1.1 above.\nAs there are no time constraints or other sources of dis-\ntractioninthetask,andaswebelieveallfourexpertstobe\nsufﬁcientlyqualiﬁed,wetreatalloftheirresponsesasnec -\nessarilytruedataandnotasapproximatejudgementswith\na measurement error attached. As a performance mea-\nsure, we use the relative number of beats for which an\nexpert disagrees with the labelling on the leadsheet (the\nbeat error rate). We assess the coherence in three ways,\nanswering slightly different questions. In an initial para -\nmetric test we ask whether the experts made roughly the\nsamenumberofcorrectionstotheirscores,usingasavery\nsimple indicator the potential overlap between the ranges\nof±3standarddeviationsaroundthemeanbeaterrorrate\nofeachpairofjudges. Alackofoverlapindicatesthattwo\nexpertsassess theaccuracyofthe algorithmdifferently.\nRegardless of whether the overall level of agreement\nwith the chord labellings is comparable between experts,\nwe ask whether they agree on a rank level of which ex-\ncerpts are considered accurate and which are not. An\nitem-based non-parametric correlation, Spearman’s ρ, is\nused. We also compare the corrections that the experts\nprovide for instances of disagreement with the algorith-miclabelling. Wethereforeselectall instanceswheretwo\nexpertsprovideacorrectionatapointofdisagreementand\ncount the relative number of instances where the correc-\ntion is identical. The binary variable which reﬂects iden-\ntity ordifferenceinthecorrectionsoftwo judgesistested\nagainst a minimum threshold of agreement required with\na binomialtest.\n2.4 Performanceofchord-labellingalgorithm\nFor assessing the performance of our chord labelling it-\nself and the labels from song books, we use the median,\nrange and inter-quartile range of the beat error rates. As\nthereispotentialmutualdisagreementbetweenjudges,we\ntreat the feedback provided by each expert as an individ-\nual chord-labelling solution, and evaluate it just like the\nalgorithmic labelling, again using the percentiles for the\nlabelling from each expert with regard to the other three\nexperts,resultinginanerrorbaseline.\n3 CONCLUSION\nWith this paper we tackle two problems that frequently\narise when dealing with ground-truth data in MIR tasks.\nThe ﬁrst concerns evaluating the feasibility of working\nwith a single ground truth, i.e.a data set where for ev-\nery instance of multivariate musical data there is a pre-\ndeﬁned datum to be predicted for a correct answer to be\npredicted. The second problemrequires a method for sit-\nuationsinwhichthereisambiguitythatallowsforseveral\ncorrect answers for a given set of musical data, and thus\nthedatacantakemultipleequallyorvariablyvalidvalues.\n4 ACKNOWLEDGEMENTS\nTheauthorsaresupportedbyEPSRCgrantsEP/D038855\nand GR/S84750. MIDI transcriptionsused in this project\nwereprovidedbyGeerdesmidimusic2.\n5 REFERENCES\n[1] D. de la Motte. Harmonielehre . dtv / B¨ arenreiter,\n1976.\n[2] D. J. C. MacKay. Information Theory, Inference, and\nLearning Algorithms . Cambridge University Press,\n2003.\n[3] A. Moore. Rock: The primary Text . Open University,\n1993.\n[4] W. Piston. Harmony .VictorGollancz,1948.\n[5] C. Rhodes, D. Lewis, and D. M¨ ullensiefen. Bayesian\nModel Selection for Harmonic Labelling. In Mathe-\nmaticsandComputationinMusic , Berlin,2007.\n[6] D. Temperley. The Cognition of Basic Musical Struc-\ntures. MITPress, Cambridge,MA, 2001.\n2http://www.midimusic.de/"
    },
    {
        "title": "Transposition-Invariant Self-Similarity Matrices.",
        "author": [
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414814",
        "url": "https://doi.org/10.5281/zenodo.1414814",
        "ee": "https://zenodo.org/records/1414814/files/MullerC07.pdf",
        "abstract": "Self-similarity matrices have become an important tool for visualizing the repetitive structure of a music recording. Transforming an audio data stream into a feature sequence, one obtains a self-similarity matrix by pairwise comparing all features of the sequence with respect to a local cost measure. The basic idea is that similar audio segments are revealed as paths of low cost along diagonals in the resulting self-similarity matrix. It is often the case, in particular for classical music, that certain musical parts are repeated in another key. In this paper, we introduce the concept of a transposition-invariant self-similarity matrix, which reveals the repetitive structure even in the presence of key transpositions. Furthermore, we introduce an associated transposition index matrix displaying harmonic relations within the music recording. As an application, we sketch how our concept can be used for the task of audio structure analysis. 1 INTRODUCTION The general concept of self-similarity matrices, which has been introduced to the music context by Foote [3], reveals the repetitive structure of a time-dependent data streams. One first transforms a given audio recording into a sequence V := (v1, v2, . . . , vN) of feature vectors vn ∈F, 1 ≤n ≤N, where F denotes a suitable feature space (e. g., a space of spectral, MFCC, or chroma vectors). Then, based on a suitable local cost measure c : F × F →R, one forms an N-square selfsimilarity matrix S defined by S(n, m) := c(vn, vm), 1 ≤n, m ≤N, comparing all features in a pairwise fashion. The crucial observation is that a pair of similar segments in the audio recording is revealed as a path of low cost along diagonals in the resulting self-similarity matrix. As the running example of this paper, we consider the first movement of Beethoven’s piano sonata Op. 31, No. 2 (“Tempest”) in a recording by Barenboim. The rough musical form of this movement is given by A1A2BA3C, where A1 corresponds to the exposition (measures 0–90), A2 to the repetition of the exposition, B to the development (measures 93–142), A3 to the recapitulation (measures 143–217), and C to a short coda (measures 218– 228). The musical parts A1 and A2, which are mere repetitions in the score, are played by Barenboim in the same c⃝2007 Austrian Computer Society (OCG). fashion and correspond to the time intervals [0 : 124] and [130 : 251] (measured in seconds) of the recording, respectively. However, even though A3 semantically corresponds to A1, there are significant variations in structure and key. A musical analysis shows that A1 has the substructure A1 = R1S1T1U1, where R1 represents the first measure, S1 measures 2–7 (part of the first theme), T1 measures 8–40 (continuation of the first theme and the transfer to the second theme), and U1 measures 41–90 (second theme). Similarly, one has substructures A2 = R2S2T2U2 and A3 = R3X3S3T ′ 3U3. Here, the three Rand S-parts more or less coincide. Similarly, the three U-parts closely correspond to each other, however, with one difference: U3 is a modulated version of U1 transposed five semitones upwards (and later transposed seven semitones downwards). Furthermore, A3 contains an additional part X3 and part T ′ 3 significantly differs from its counterpart T1 in structure and key. A conventional self-similarity matrix as shown in Figure 1 (a) (with respect to chroma-based audio features as discussed in Section 2), reveals only parts of the musical structure. In particular, the path starting at coordinate (0, 130) and ending at (124, 251) indicates the similarity of the time intervals [0 : 124] (part A1) and [130 : 251] (part A2). Similarly, there are paths reflecting the similarity of the three Rand S-parts. However, repetitive segments that differ by some transposition are not reflected by the self-similarity matrix. In Section 2, we introduce the concept of transpositioninvariant self-similarity matrices that are invariant under all transpositions. In particular, we adopt an idea by Goto [4], which is based on the observation that the transpositions can be handled by cyclically shifting the chroma. Here, the chroma correspond to the twelve traditional pitch classes of the equal-tempered scale [1]. In Section 3, we sketch how the transposition-invariant selfsimilarity matrices can be used for automated audio structure analysis. In Section 4, we conclude this paper and give prospects on future work. Further references to related work are given in the respective sections. 2 TRANSPOSITION-INVARIANT SELF-SIMILARITY MATRIX The properties of a self-similarity matrix S depend on the kind of audio features extracted from the audio recording as well as on the local cost measure c. In the following, we use chroma-based audio features as described, 100 200 300 400 500 50 100 150 200 250 300 350 400 450 500",
        "zenodo_id": 1414814,
        "dblp_key": "conf/ismir/MullerC07",
        "keywords": [
            "self-similarity matrices",
            "visualizing repetitive structure",
            "audio data stream",
            "feature sequence",
            "pairwise comparing",
            "local cost measure",
            "transposition-invariant",
            "transposition index matrix",
            "harmonic relations",
            "audio structure analysis"
        ],
        "content": "TRANSPOSITION-INV ARIANT SELF-SIMILARITY MATRICES\nMeinard M ¨uller and Michael Clausen\nBonn University\nDepartment of Computer Science III\nABSTRACT\nSelf-similarity matrices have become an important tool\nfor visualizing the repetitive structure of a music record-\ning. Transforming an audio data stream into a feature se-\nquence, one obtains a self-similarity matrix by pairwise\ncomparing all features of the sequence with respect to a\nlocal cost measure. The basic idea is that similar audio\nsegments are revealed as paths of low cost along diagonals\nin the resulting self-similarity matrix. It is often the cas e,\nin particular for classical music, that certain musical par ts\nare repeated in another key. In this paper, we introduce the\nconcept of a transposition-invariant self-similarity mat rix,\nwhich reveals the repetitive structure even in the presence\nof key transpositions. Furthermore, we introduce an as-\nsociated transposition index matrix displaying harmonic\nrelations within the music recording. As an application,\nwe sketch how our concept can be used for the task of\naudio structure analysis.\n1 INTRODUCTION\nThe general concept of self-similarity matrices, which\nhas been introduced to the music context by Foote [3],\nreveals the repetitive structure of a time-dependent data\nstreams. One ﬁrst transforms a given audio recording\ninto a sequence V:= (v1,v2,... ,v N)of feature vec-\ntorsvn∈ F ,1≤n≤N, where Fdenotes a suit-\nable feature space (e. g., a space of spectral, MFCC, or\nchroma vectors). Then, based on a suitable local cost\nmeasure c:F × F → R, one forms an N-square self-\nsimilarity matrix Sdeﬁned by S(n,m) := c(vn,vm),\n1≤n,m≤N, comparing all features in a pairwise fash-\nion. The crucial observation is that a pair of similar seg-\nments in the audio recording is revealed as a path of low\ncost along diagonals in the resulting self-similarity matr ix.\nAs the running example of this paper, we consider the\nﬁrst movement of Beethoven’s piano sonata Op. 31, No. 2\n(“Tempest”) in a recording by Barenboim. The rough\nmusical form of this movement is given by A1A2BA3C,\nwhere A1corresponds to the exposition (measures 0–90),\nA2to the repetition of the exposition, Bto the develop-\nment (measures 93–142), A3to the recapitulation (mea-\nsures 143–217), and Cto a short coda (measures 218–\n228). The musical parts A1andA2, which are mere repe-\ntitions in the score, are played by Barenboim in the same\nc/circlecopyrt2007 Austrian Computer Society (OCG).fashion and correspond to the time intervals [0 : 124]\nand[130 : 251] (measured in seconds) of the record-\ning, respectively. However, even though A3semanti-\ncally corresponds to A1, there are signiﬁcant variations\nin structure and key. A musical analysis shows that A1\nhas the substructure A1=R1S1T1U1, where R1repre-\nsents the ﬁrst measure, S1measures 2–7 (part of the ﬁrst\ntheme), T1measures 8–40 (continuation of the ﬁrst theme\nand the transfer to the second theme), and U1measures\n41–90 (second theme). Similarly, one has substructures\nA2=R2S2T2U2andA3=R3X3S3T′\n3U3. Here, the\nthreeR- and S-parts more or less coincide. Similarly,\nthe three U-parts closely correspond to each other, how-\never, with one difference: U3is a modulated version of U1\ntransposed ﬁve semitones upwards (and later transposed\nseven semitones downwards). Furthermore, A3contains\nan additional part X3and part T′\n3signiﬁcantly differs from\nits counterpart T1in structure and key.\nA conventional self-similarity matrix as shown in Fig-\nure 1 (a) (with respect to chroma-based audio features as\ndiscussed in Section 2), reveals only parts of the musi-\ncal structure. In particular, the path starting at coordina te\n(0,130) and ending at (124,251) indicates the similarity\nof the time intervals [0 : 124] (partA1) and[130 : 251]\n(partA2). Similarly, there are paths reﬂecting the similar-\nity of the three R- andS-parts. However, repetitive seg-\nments that differ by some transposition are not reﬂected\nby the self-similarity matrix.\nIn Section 2, we introduce the concept of transposition-\ninvariant self-similarity matrices that are invariant un-\nder all transpositions. In particular, we adopt an idea\nby Goto [4], which is based on the observation that the\ntranspositions can be handled by cyclically shifting the\nchroma. Here, the chroma correspond to the twelve tra-\nditional pitch classes of the equal-tempered scale [1]. In\nSection 3, we sketch how the transposition-invariant self-\nsimilarity matrices can be used for automated audio struc-\nture analysis. In Section 4, we conclude this paper and\ngive prospects on future work. Further references to re-\nlated work are given in the respective sections.\n2 TRANSPOSITION-INV ARIANT\nSELF-SIMILARITY MATRIX\nThe properties of a self-similarity matrix Sdepend on the\nkind of audio features extracted from the audio record-\ning as well as on the local cost measure c. In the fol-\nlowing, we use chroma-based audio features as described,100 200 300 400 50050100150200250300350400450500\n0.10.20.30.40.50.60.70.80.9\n100 200 300 400 50050100150200250300350400450500\n0.10.20.30.40.50.60.70.80.9\n100 200 300 400 5001\n2\n3\n4\n100 200 300 400 5001\n2\n3\n4\n5\n6(a)\n(c)(b)\n(d)\nFigure 1 . First movement of Beethoven’s piano sonata Op. 31, No. 2 (“T empest”) in a recording by Barenboim. (a)\nSelf-similarity matrix S. Low costs are indicated by dark colors (cost 0corresponds to black) and high costs by light\ncolors (cost 1corresponds to white). (b)Transposition-invariant self-similarity matrix σmin(S).(c)Groups of mutually\nsimilar audio segments obtained from S.(d)Groups of mutually similar audio segments obtained from σmin(S).\ne. g., in [1, 4, 6]. Assuming the equal-tempered scale, the\nchroma correspond to the set {C,C♯,D,... ,B}that con-\nsists of the twelve pitch spelling attributes as used in West -\nern music notation. Note that in the equal-tempered scale\ndifferent pitch spellings such C♯andD♭refer to the same\nchroma. We consider the feature space\nF:=/braceleftbig\nv∈R12/vextendsingle/vextendsingle/summationtext12\ni=1v(i)2= 1/bracerightbig\nof normalized 12-dimensional chroma vectors v=\n(v(1),v(2),... ,v (12)) , where v(1) corresponds to\nchroma C,v(2)to chroma C♯, and so on. Then, the\ngiven audio signal is decomposed into a sequence V=\n(v1,v2,... ,v N)of normalized chroma vectors vn∈ F ,\n1≤n≤N, which expresses the signal’s local energy\ndistribution among the 12 pitch classes. Such a chroma\nrepresentation can be obtained, e. g., from a spectrogram\nby suitably pooling Fourier coefﬁcients [1] or by using\nmultirate ﬁlter bank techniques [6]. Chroma-based audio\nfeatures absorb variations in parameters such as dynam-\nics, timbre, and articulation and closely correlate to the\nshort-time harmonic content of the underlying audio sig-\nnal. In the following, we use a feature sampling rate of\n1Hz, i. e., each vector corresponds to one second of the\noriginal audio signal.\nFurthermore, we use the local cost measure c:F ×\nF → Rdeﬁned by c(v,w) := 1 − /an}bracketle{tv,w/an}bracketri}htforv,w∈ F.\nSince vandware normalized, the inner product /an}bracketle{tv,w/an}bracketri}ht\ncoincides with the cosine of the angle between vandw.\nActually, in the following, we use an enhanced version of\nthe local cost measure by incorporating contextual infor-\nmation, see [5] for details. The resulting self-similarity\nmatrix will be denoted by Sand is shown in Figure 1 (a)\nfor our Beethoven example.\nTo account for transpositions, we revert to the obser-\nvation by Goto [4] that the twelve cyclic shifts of a 12-\ndimensional chroma vector naturally correspond to thetwelve possible transpositions. In contrast to previous ap -\nproaches, we incorporate all transpositions into a single\nself-similarity matrix. To this end, let σ:F → F denote\nthecyclic shift deﬁned by\nσ((v(1),v(2),... ,v (12))) := ( v(2),... ,v (12),v(1))\nforv:= (v(1),... ,v (12)) ∈ F . Then, for a given\naudio data stream with chroma-based feature sequence\nV:= (v1,v2,... ,v N), thei-transposed self-similarity\nmatrix σi(S)is deﬁned by\nσi(S)(n,m) :=c(vn,σi(vm)),\nfor1≤n,m ≤Nandi∈Z. Obviously, one has\nσ12(S) =S. Intuitively, σi(S)describes the similar-\nity relations between the original audio data stream and\nthe audio data streams transposed by isemitones upwards\n(modulo 12). Taking the minimum over the twelve dif-\nferent cylic shifts, we obtain the transposition-invariant\nself-similarity matrix σmin(S)deﬁned by\nσmin(S)(n,m) := min i∈[0:11]/parenleftBig\nσi(S)(n,m)/parenrightBig\n.\nFurthermore, we store the minimizing shift indices in an\nadditional N-square matrix I, which is referred to as\ntransposition index matrix :\nI(n,m) := argmini∈[0:11]/parenleftBig\nσi(S)(n,m)/parenrightBig\n.\nWe illustrate this concept by means of two exam-\nples. Figure 1 (b) shows the transposition-invariant self-\nsimilarity matrix of our Beethoven example. The most\nstriking difference to the conventional self-similarity m a-\ntrix shown in Figure 1 (a) are the two additional paths\nin the upper left part. (Due to the symmetry of Sand\nσmin(S), we only consider the part above the main diag-\nonal in the following discussion.) The ﬁrst of these paths50 100 15020406080100120140160180\n0.10.20.30.40.50.60.70.80.9\n50 100 15020406080100120140160180\n0.10.20.30.40.50.60.70.80.9\n20 40 60 80 100 120 140 160 1801\n2\n3\n20 40 60 80 100 120 140 160 1801\n2\n3(a) (b)\n(c) (d)\nFigure 2 . Zager & Evans, “In the year 2525”. (a)Self-similarity matrix S.(b)Groups of mutually similar audio segments\nobtained from S.(c)Transposition-invariant self-similarity matrix σmin(S).(d)Groups obtained from σmin(S).\n50 100 15020406080100120140160180\n01234567891011\n50 100 15020406080100120140160180\n50 100 15020406080100120140160180\n50 100 15020406080100120140160180(a) (b) (c) (d)\nFigure 3 . Zager & Evans, “In the year 2525”. (a)Color-coded representation of the transposition index mat rixI. The\nthree black-white images indicate the positions (black col or), where the minimizing index is (b)i= 0 corresponding to\nno shift, (c)i= 1corresponding to one semitone upwards, (d)i= 2corresponding to two semitones upwards.\nstarts at coordinate (67,452) and ends at (120,504) in-\ndicating the similarity of the time intervals [67 : 120]\n(partU1) and[452 : 504] (partU3). Similarly, the sec-\nond of these paths starts at coordinate (196,452) and ends\nat(247,504) indicating the similarity of the time intervals\n[196 : 247] (partU2) and [452 : 504] (partU3). Thus,\nthese paths reveal the modulated repetition of the second\ntheme in the recapitulation. We will continue our discus-\nsion of further additional path relations in Section 3.\nAs second example, we consider the song “In the\nyear 2525” by Zager & Evans, which has the musi-\ncal form AB0\n1B0\n2B0\n3B0\n4C0\n1B1\n5B1\n6C1\n2B2\n7EB2\n8F. The song\nstarts with a slow intro, which is represented by the A-\npart. The chorus of the song, which is represented by\ntheB-parts, is repeated 8times. In particular, B1\n5and\nB1\n6are transpositions by one semitone upwards and B2\n7\nandB2\n8are transpositions by two semitones upwards of\nthe ﬁrst four B-parts B0\n1toB0\n4. The respective transposi-\ntion indices have been indicated by the additional super-\nscripts. Similarly the two transitional C-parts are shifted\nversions from each other. Figure 2 (a) shows the conven-\ntional self-similarity matrix. The path relations reveal t he\nsimilarities of the four audio segments corresponding to\nthe ﬁrst four B-parts as well as the similarity between the\naudio segments corresponding to B1\n5andB1\n6and to B2\n7andB2\n8, respectively. However, the pairwise similarity re-\nlations between all eight B-parts only become visible in\nthe transposition-invariant self-similarity matrix show n in\nFigure 2 (b).\nThe transposition index can be read off from the trans-\nposition index matrix I, which is shown in Figure 3 (a) for\nthe song “In the year 2525” in a color-coded form. Note\nthat, opposed to the self-similarity matrices, Iis not sym-\nmetric along the main diagonal. Actually, a minimizing\nindex iat coordinate (n,m)induces a minimizing index\n12−iat coordinate (m,n). For the sake of a better vi-\nsualization, the three separate black-white images shown\nin Figure 3 (b)–(d) indicate by the black color all coordi-\nnates(n,m), where the minimizing index in the deﬁnition\nofσmin(S)(n,m)isi= 0,i= 1, andi= 2, respectively.\nWe ﬁrst discuss the case i= 0 as shown in Figure 3 (b).\nHere, the black color at coordinate (n,m)indicates that\nc(vn,σi(vm))assumes a minimal values for i= 0. In\nother words, the chroma vector vnis closer to vmthan to\nany other shifted version of vm. This constitutes a neces-\nsary condition for the short-term harmonic content of the\naudio signal at time position mto be close to the one at\ntime position n. (However, this condition is not sufﬁcient\nin the sense that the cost c(vn,vm)may still be high in\nabsolute terms.) Therefore, as expected, the minimizingindex is i= 0 at all positions, where the conventional\nself-similarity matrix reveals paths of low cost. Analo-\ngously, Figure 3 (c) reveals all coordinates (n,m), where\nthe short-term harmonic at time position mrelates by one\nsemitone upwards to the the short-term harmonic at time\nposition n. Thus, the black regions of Figure 3 (c) reveal\nthe upper semitone relation of B1\n5B1\n6to the ﬁrst four B-\nparts. In addition, they also reveal upper semitone relatio n\nbetween B2\n7B8\n6andB1\n5B1\n6. Figure 3 (d) has a similar in-\nterpretation. Finally, the regularly placed patches (shor t\npaths) in Figure 3 (c) and (d) reveal interesting substruc-\ntures of the B-parts. Indeed, each B-part itself consists\nof four subparts which are harmonically correlated: the\nsecond subparts is a shifted version of the ﬁrst one going\none semitone downwards. The third subpart is shifted a\nfurther semitone downwards, before the melody is going\nupwards again in the fourth subpart.\n3 AUDIO STRUCTURE ANALYSIS\nWe will now sketch, how transposition-invariant self-\nsimilarity matrices can be used for efﬁcient audio struc-\nture analysis . Here, the goal is to automatically extract the\nrepetitive structure or, more generally, the musical form\nof the underlying piece of music, see, e. g., [1, 2, 4, 6].\nIn our experiments, we used an implementation of the ap-\nproach described in [6], which computes groups of audio\nsegments within an audio ﬁle that are similar in harmonic\nprogression. This is achieved by running through the fol-\nlowing general steps:\n1. Extract chroma-based features from the audio sig-\nnal and compute the transposition-invariant self-\nsimilarity matrix σmin(S)as well as the transposi-\ntion index matrix I. By incorporating contextual\ninformation at various tempo levels into the cost\nmeasure, the structural properties of the matrix are\nenhanced, see [5].\n2. Extract off-diagonal paths from σmin(S)using a\ngreedy strategy. Each path encodes a pair of sim-\nilar segments. This step takes care of relative differ-\nences in the tempo progression between musically\nsimilar segments.\n3. Derive the global repetitive structure from the sim-\nilarity pairs by using suitable clustering techniques.\nIn particular, we employ a one-step transitivity clus-\ntering procedure, which balances out the inconsis-\ntencies introduced by inaccurate and incorrect path\nextractions, see [6].\nAs output, the algorithm delivers a list of groups with\neach group representing a set of mutually similar audio\nsegments. The ﬁnal result for the Beethoven example is\nshown in Figure 1 (d). Each of the six rows corresponds\nto a group of mutually similar audio segments, where each\nsegment is represented by a gray bar. For example, the\nﬁrst row reveals the similarity between the exposition A1and its repetition A2. The second row reveals the simi-\nlarity between the three U-parts corresponding to the sec-\nond theme, where U3is a transposed version of U1and\nU2. Note that this similarity group is not detected when\nusing the conventional self-similarity matrix S, cf. Fig-\nure 1 (c). Furthermore, the fourth row, which consists\nof8segments, reveals some interesting substructure: the\nslow introduction R1(ﬁrst measure) of the “Tempest” is\nrepeated several times throughout the piece in different\nkeys.\nSimilarly, Figure 2 (d) shows the ﬁnal result of the\nextracted global repetitive structure for the song “In the\nyear 2525”. The ﬁrst row encodes a group of eight mu-\ntually similar audio segments, which are exactly the eight\nB-parts. In contrast, when using the conventional self-\nsimilarity matrix S, this group is split up into three dif-\nferent groups as illustrated by Figure 2 (c). Furthermore,\nthe second and third row of Figure 2 (d) reveal some su-\nperstructure, which are not present in Figure 2 (c). For\nexample, the second row reveals the similarity between\nB0\n3B0\n4C0\n1andB1\n5B1\n6C1\n2.\n4 CONCLUSIONS\nIn this paper, we have introduced transposition-invariant\nself-similarity matrices, which reveal the repetitive au-\ndio structure even in the presence of key changes. Note\nthat previous approaches to structure analysis such as [4]\nachieve transposition invariance by computing similarity\ngroups for all twelve transpositions separately, which are\nthen suitably merged in a postprocessing step. In con-\ntrast to this, we incorporate all transpositions into a sing le\nself-similarity matrix, which then allows for performing a\nsingly joint path extraction and clustering step only. Our\nexperiments showed that such a joint procedure not only\nsigniﬁcantly increases the efﬁciency of the overall algo-\nrithm, but also stabilizes the clustering step for deriving\nthe similarity groups. An interesting yet open problem is\nto consider not only transpositions but also other types of\nmodulations such as changes from major to minor keys\nand vice versa.\n5 REFERENCES\n[1] M. A. B ARTSCH AND G. H. W AKEFIELD ,Audio thumb-\nnailing of popular music using chroma-based representa-\ntions , IEEE Trans. on Multimedia, 7 (2005), pp. 96–104.\n[2] R. D ANNENBERG AND N. H U,Pattern discovery tech-\nniques for music audio , in Proc. ISMIR, Paris, France, 2002.\n[3] J. F OOTE ,Visualizing music and audio using self-similarity ,\nin ACM Multimedia, 1999, pp. 77–80.\n[4] M. G OTO,A chorus-section detecting method for musical\naudio signals , in Proc. IEEE ICASSP, 2003, pp. 437–440.\n[5] M. M ¨ULLER AND F. K URTH ,Enhancing similarity matrices\nfor music audio analysis , in Proc. IEEE ICASSP, 2006.\n[6] ,Towards structural analysis of audio recordings in\nthe presence of musical variations , EURASIP Journal on\nAdvances in Signal Processing, (2007). Article ID 89686,\n18 pages."
    },
    {
        "title": "Automatic Transcription of Music Audio Through Continuous Parameter Tracking.",
        "author": [
            "Eric Nichols",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416162",
        "url": "https://doi.org/10.5281/zenodo.1416162",
        "ee": "https://zenodo.org/records/1416162/files/NicholsR07.pdf",
        "abstract": "We present a method for transcribing arbitrary pitched music into a piano-roll-like representation that also tracks the amplitudes of the notes over time. We develop a probabilistic model that gives the likelihood of a frame of audio data given a vector of amplitudes for the possible notes. Using an approximation of the log likelihood function, we develop an objective function that is quadratic in the timevarying amplitude variables, while also depending on the discrete piano-roll variables. We optimize this function using a variant of dynamic programming, by repeatedly growing and pruning our histories. We present results on a variety of different examples using several measures of performance including an edit-distance measure as well as a frame-by-frame measure. 1 INTRODUCTION Polyphonic audio transcription has received considerable attention in recent years and is holds promise in MIR for its potential for automatically creating symbolic music representations from audio. Research in this area has produced a wide variety of approaches [1], [3]-[11] with significant contributions, though the problem is deeply challenging and remains open. A recurring theme in this work is that of representing a music spectrogram as a superposition of fixed (but trainable) templates modeling various note aspects. Examples of such template-based approaches are non-negative matrix representations for frames over notes [1], [10] or fundamental frequencies [6], or note-based models involving time-extent as well [7]. Our approach shares some methodology with [11], although that work uses peak detection instead of templates. While these local model-fitting problems are difficult, perhaps even more challenging is the problem of assembling a global interpretation of the data, beyond the frame or note level. Such full transcription problems [3], [8], require the integration of more global musical knowledge. Our work is in this latter category. In addition to attempting polyphonic transcription from unknown sources, we simultaneously estimate the time-varying note amplitude parameters, hoping their knowledge will lead to more © 2007 Austrian Computer Society (OCG). discriminating models. This work has overlap with [4], though our approach is note-based, rather than harmonicbased. The amplitude envelopes themselves may be of primary interest for some applications. 2 A PROBABILISTIC MODEL We present here a probabilistic model describing the likelihood of a frequency spectrum given an assumed configuration of sounding pitches. We denote our sampled time signal as x(m) for m = 0, . . . M −1. Our entire analysis is based on the spectrogram of the time data which we define as st(k) =",
        "zenodo_id": 1416162,
        "dblp_key": "conf/ismir/NicholsR07",
        "keywords": [
            "polyphonic audio transcription",
            "MIR",
            "polyphonic transcription",
            "global interpretation",
            "template-based approaches",
            "non-negative matrix representations",
            "fundamental frequencies",
            "time-extent",
            "local model-fitting problems",
            "global musical knowledge"
        ],
        "content": "AUTOMA TIC TRANSCRIPTION OFMUSIC AUDIO THR OUGH\nCONTINUOUS PARAMETER TRA CKING\nEric Nichols\nDept. ofComputer Science\nIndiana Univ.\nepnichol@indiana.eduChristopher Raphael\nSchool ofInformatics\nIndiana Univ.\ncraphael@indiana.edu\nABSTRA CT\nWepresent amethod fortranscribing arbitrary pitched\nmusic intoapiano-roll-lik erepresentation thatalsotracks\ntheamplitudes ofthenotes overtime. Wedevelop aproba-\nbilistic model thatgivesthelikelihood ofaframe ofaudio\ndata givenavector ofamplitudes forthepossible notes.\nUsing anapproximation oftheloglikelihood function, we\ndevelop anobjecti vefunction thatisquadratic inthetime-\nvarying amplitude variables, while also depending onthe\ndiscrete piano-roll variables. Weoptimize thisfunction\nusing avariant ofdynamic programming, byrepeatedly\ngrowing andpruning ourhistories. Wepresent results on\navariety ofdifferent examples using severalmeasures of\nperformance including anedit-distance measure aswell as\naframe-by-frame measure.\n1INTR ODUCTION\nPolyphonic audio transcription hasrecei vedconsiderable\nattention inrecent years and isholds promise inMIR\nforitspotential forautomatically creating symbolic mu-\nsicrepresentations from audio. Research inthisarea has\nproduced awide variety ofapproaches [1],[3]-[11] with\nsigni\u0002cant contrib utions, though theproblem isdeeply\nchallenging andremains open. Arecurring theme inthis\nworkisthatofrepresenting amusic spectrogram asasu-\nperposition of\u0002xed(buttrainable) templates modeling\nvarious note aspects. Examples ofsuch template-based\napproaches arenon-ne gativematrix representations for\nframes overnotes [1],[10] orfundamental frequencies\n[6],ornote-based models involving time-e xtent aswell\n[7].Ourapproach shares some methodology with [11],al-\nthough thatworkuses peak detection instead oftemplates.\nWhile these local model-\u0002tting problems aredif\u0002cult,\nperhaps evenmore challenging istheproblem ofassem-\nbling aglobal interpretation ofthedata, beyond theframe\nornote level.Such fulltranscription problems [3],[8],\nrequire theintegration ofmore global musical knowledge.\nOur workisinthislatter category.Inaddition toat-\ntempting polyphonic transcription from unkno wnsources,\nwesimultaneously estimate thetime-v arying note ampli-\ntude parameters, hoping their knowledge willleadtomore\n©2007 Austrian Computer Society (OCG).discriminating models. This workhasoverlap with [4],\nthough ourapproach isnote-based, rather than harmonic-\nbased. The amplitude envelopes themselv esmay beof\nprimary interest forsome applications.\n2APROBABILISTIC MODEL\nWepresent here aprobabilistic model describing thelike-\nlihood ofafrequenc yspectrum givenanassumed con\u0002g-\nuration ofsounding pitches.\nWedenote oursampled time signal asx(m)form=\n0;:::M\u00001.Ourentire analysis isbased onthespectro-\ngram ofthetime data which wede\u0002ne as\nst(k)=\f\f\f\f\fN\u00001X\nn=0x(tL+n)w(n)e2\u0019ikn\nN\f\f\f\f\f2\nwhere Nistheframe length, Listhehop size,wis\nanN-point windo wfunction, andk=0;:::;N=2,t=\n0;:::;T\u00001,where T=1+b(M\u0000N)=Lc.\nWeprobabilistically model thetime slices ofthe\nspectrogram, st(\u0001),asfollows. Suppose that webegin\nwith anidealized template spectrum, q(i;\u0001),foreach pos-\nsible note indexedbyi=1;:::I.Here weassume that\nq(i;\u0001)isaprobability distrib ution sothatq(i;k)\u00150andP\nkq(i;k)=1.These note models caneither beesti-\nmated from actual data, asdiscussed inalater section, or\nsimply \u0002xedaccording toapredetermined model. Inthis\nlatter case wehaveused themixture ofdiscrete Gaussians\nmodel\nq(i;k)=HX\nh=1phN(k;\u0016(i;h);\u001b2(i;h)) (1)\nwhere N(k;\u0016;\u001b2)=P(k\u00001=2<X<k+1=2)for\nnormally distrib utedXwith mean\u0016(i;h)=h!0(i)and\nvariance \u001b2(i;h)=ah!0(i)+b.HereP\nhph=1and\n!0(i)isthefundamental frequenc yoftheithpitch.\nLet\u000bt(i)\u00150denote thecontrib ution ofpitchiduring\nframe t,sothat\u000bt(i)=0ifthenote isabsent and\nq\u000bt(k)=X\ni\u000bt(i)q(i;k) (2)\ndenotes theidealized template forframe t.Wemodel\nthedata likelihood atframe t,givenq\u000bt(\u0001),byassum-\ningthat thefst(k)gareindependent andthatst(k)\u0018Poisson (q\u000bt(k)).Forthisassumption tomakesense, we\nscale toapoint where thetruncation tointegralvalues pro-\nduces nosigni\u0002cant loss. Theloglikelihood canthen be\nwritten as\nlogP(s(t;\u0001)jq\u000bt(\u0001)) (3)\n=c+N=2X\nk=0s(t;k)logq\u000bt(k)\u0000q\u000bt(k) (4)\n=c+N=2X\nk=0st(k)log IX\ni=1\u000bt(i)q(i;k)!\n\u0000IX\ni=1\u000bt(i)q(i;k)\nwhere cisaconstant notdepending onq\u000bt.Thus wehave\naparametric probability model with parameters f\u000bt(i)g.\n3BUILDING THE LATTICE\nOur immediate goal istocreate acollection ofhypothe-\nsesforthenotes thatsound ineach frame. Forsimplicity ,\nwewillcallthese collections chords while ackno wledg-\ningthattheyarenotexactly thesame astheusual musical\nmeaning oftheword. Thecollections ofchord hypothe-\nses,indexedbytheframe t,canbethought ofasalattice.\nOuraudio recognition willbephrased asasearch forthe\nbest path through thislattice. Since ahypothesis thatis\nmissing from thislattice willneverbeconsidered during\nrecognition, wewish toerronthesideofinclusion byad-\nmitting anyhypothesis thatseems plausible.\nOursearch forcandidate hypotheses ismade easier by\nthetractable nature ofourloglikelihood function. Com-\nputing derivativesoftheloglikelihood with respect tothe\n\u000bparameters showsthatthenegativeoftheHessian ofthe\nloglikelihood isnonne gativede\u0002nite. Thus theloglike-\nlihood isconvexinthe\u000bparameters. The virtue here is\nthatthenumerical optimization oftheloglikelihood over\nthe\u000bparameters isrelati velyeasy toperform while we\nareassured of\u0002nding aglobal optimum duetoconvex-\nity.The only minor dif\u0002culty ishandling thepositi vity\nconstraints onthe\u000bparameters. Wehaveaccomplished\nthisusing thebarrier method, [2],which approximates the\nconstrained optimization byaseries ofunconstrained op-\ntimization problems.\nLet\u000b\u0003\nt(i)betheoptimal parameters found bymaxi-\nmizing theloglikelihood forframe t.Wecreate ourini-\ntialsetofchord hypotheses forframe tbytaking uptoR\nnotes whose contrib ution totheoverall spectrum exceeds\nathreshold.\nS(t)=fi2I:\u000b\u0003\nt(i)\u0015max(Tmin;\u000b\u0003\nt(i(R))g\nwhere \u000b\u0003\nt(i(R))istheRthhighest ofthef\u000b\u0003\nt(i)g(R=5\ninourexperiments). Our initial chord hypotheses for\nframe tarethen takentobethepower set,orsetofall\npossible 2jS(t)jsubsets, ofS(t).Wedenote thisinitial\ncollection ofhypotheses byL(t)=P(S(t)).Asmen-\ntioned before, wehope nottomiss possible hypothesesatthisstage sowetrytobeasliberal aspossible inthe\nconstruction ofourinitial lattice. Tothisend, weform an\nexpanded lattice including allchords detected innearby\nframes aswell.\n4ASIMPLE RECOGNITION SCHEME\nWewish tolabel each frame, t,with achord label,C(t)2\n\u0016L(t)such thatthedata support each chord label andthe\nhorizontal evolution ofthechords makesacertain mini-\nmalmusical sense. Tothisend, each sequence offrames\nC=(C0;:::CT\u00001)isscored asS(C)=D(C)+E(C)\nwhere thedata score, D(C),andthepenalty term,E(C),\nmeasure these twoaspects ofthehypothesis C.\nThedata score isde\u0002ned by\nD(C)=T\u00001X\nt=0logP(stjq\u000bCt)\nwhere\n\u000bCt(i)=\u001aP\nkst(k)=jCtji2Ct\n0 otherwise\nInother words, weuseatemplate created byequal con-\ntributions ofthehypothesized notes such thatthetotal en-\nergyexplained bythetemplate isequal tothetotal energy\ninthespectrum. Thepenalty term isgivenby\nE(C)=\u0000LenterT\u00001X\nt=1jfi2C(t):i62C(t\u00001)gj\nwhere Lentergivesthepenalty foreach entering note.\nWiththispenalty function itisasimple matter to\u0002nd\ntheglobally optimal path through thelattice using dy-\nnamic programming.\n5SIMUL TANEOUS RECOGNITION AND\nAMPLITUDE TRA CKING\nThe recognition scheme presented inSection 4suffers\nfrom some weaknesses. Forone, itisnotpossible to\nrecognize rearticulations (repeated pitches) since thedata\nterm isidentical forboth held andrearticulated hypothe-\nses,while thepenalty term willalwaysprefer thehypoth-\nesiswith fewernotes. Inaddition, themodel assumes that\ntheamplitudes (\u000b's)ofallofthenotes inachord are\nthesame. Givennoother information, thisseems asrea-\nsonable asanyassumption, butiscertain tobefarfrom\nreality .Weintroduce inthissection amethod forsimul-\ntaneous recognition ofthechord sequence andtracking of\ntheamplitude parameters. Themethod hasmuch thesame\n\u0003avorasRao-Blackwellized particle \u0002ltering, though we\nseek amost-lik elypath, rather than a\u0002ltered solution.\nOur essential approach performs dynamic program-\nming to\u0002ndthebestchord sequence, asbefore. However,\neach chord hypothesis isscored notbyanumber ,but\nbyafunction thatmeasures thegoodness ofthehypoth-\nesisasafunction oftheunkno wnamplitude parameters.This function isrepresented parametrically ,sothatwecan\nupdate theparameters asthehypothesis ages asinthe\nKalman \u0002lter.This method overcomes some weaknesses\noftheprevious approach byrepresenting thedata likeli-\nhood asafunction oftheamplitude parameters clearly\nadesirable traitforthedata model.\nAcomplete hypothesis forourdata isnowgivenas\nasequence ofchords, C=(C0;:::;CT\u00001),along with\ntheamplitude vectors, \u000b=(\u000b0;:::;\u000bT\u00001),andabi-\nnary breakpoint vectorb=(b0;:::;bT\u00001)withb0=0\nandbT\u00001=0.Werequire thechord tobeconstant be-\ntween breakpoints: Ct=Ct\u00001forbt6=0,though we\ndonotrequire thechord tochange after abreakpoint.\nInthiswaywerepresent something likearearticulation,\nthough wedonotdistinguish between therearticulating\nandsustaining chord members. Each amplitude vector de-\nscribes theamplitudes ofthenotes inthecurrent chord\nwith thenon-chord members constrained tohave0ampli-\ntude:\u000bt(i)=0fori62Ct.Inrepresenting thegood-\nness ofahypothesis wepenalize both hypotheses that\ndon'tagree with thedata andhypotheses thatarenotmu-\nsically plausible.\nInournewversion, thedata score depends onthe\nunkno wnamplitude parameters (the\u000btparameters of\nEqn. 3).Since thisloglikelihood function wasshown\ntobeconvexintheamplitude parameters inSection 3,it\nseems reasonable toapproximate itbyaquadratic func-\ntioninthef\u000b(t)gparameters. Weparameterize thisby\nlogP(stjq\u000bt)\u0019ht+1\n2(\u000bt\u0000mt)tQt(\u000bt\u0000mt)\nwhere (ht;mt;Qt)arefound bycompleting thesquare\nontheTaylor series approximation ofthedata loglikeli-\nhood expanded around ouroptimizing point, \u000b\u0003\nt.Wedon't\nknowabout thenon-local quality ofthequadratic approxi-\nmation, butthisapproximation isquite useful forthecom-\nputations thatfollow.Forinstance, ifwewanttoapprox-\nimate thedata loglikelihood foraspeci\u0002c chord, Ct,as\nafunction oftheamplitude parameters, weneed only re-\nstrict tozero the\u000bt(i)parameters where i62Ct.Wewill\ndenote thisrestriction by(hCt;mCt;QCt).HereQCtis\nobtained simply byzeroing outtherowsandcolumns of\nQtnotcontained inC(t),whilehC(t)andmC(t)areob-\ntained assimple linear functions oftheoriginal parameters\n(asintheGaussian conditional mean). Thedata score of\nahypotheses isthen\nD(C;\u000b)=T\u00001X\nt=0Dt(Ct;\u000bt)\ndef=T\u00001X\nt=01\n2(\u000bt\u0000mCt)tQCt(\u000bt\u0000mCt)\nWemodify ourprevious penalty term byadding ina\ncomponent thatfavors\u000btvectors thatvarysmoothly over\ntheduration ofachord, aswould beexpected. That is\nE(C;b;\u000b)=T\u00001X\nt=0Et(Ct\u00001;Ct;bt;\u000bt\u00001;\u000bt)def=\u0000X\nb(t)=1LenterjCt\u0000Ct\u00001j\n\u0000X\nb(t)=0rjj\u000bt\u0000\u000bt\u00001jj2\nforsome constant r>0.Note thatwehavenoreason-\nable priors ontheparameters \u000btnecessary foraBayesian\nformulation ofthisproblem; instead, weprefer theframe-\nworkofmaximum likelihood, with thepenalty term pro-\nviding smoothing. Ourgoal isnowtomaximize ourtotal\nhypothesis score, S(C;b;\u000b)=D(C;\u000b)+E(C;b;\u000b).\n5.1Performing theOptimization\nOurscore function ismore dif\u0002cult tooptimize than that\nofSection 4duetothecombination ofdiscrete (C;b)and\ncontinuous (\u000b)parameters. Still, ourbasic approach is\ntoperform dynamic programming justasbefore. Tothat\nendweusethevector notation at\n0=(a0;:::;at)forany\nvectora,andde\u0002ne\nSt(Ct\n0;bt\n0;\u000bt\n0)=tX\n\u001c=0D\u001c(C\u001c;\u000b\u001c)\n+E\u001c(C\u001c;C\u001c\u00001;b\u001c;\u000b\u001c\u00001;\u000b\u001c)\nwhich canbewritten more compactly with thenotation\nxt=(Ct;bt;\u000bt)as\nSt(xt\n0)=tX\n\u001c=0D\u001c(x\u001c)+E\u001c(x\u001c\u00001;x\u001c)\nOurdynamic programming recursion isthen\nS\u0003\nt(xt)=max\nxt\u00001S\u0003\nt\u00001(xt\u00001)+Dt(xt)+Et(xt\u00001;xt)(5)\nwhere S\u0003\nt(xt)isthescore oftheoptimal partial hypothesis\nending inxt.\nUnfortunately ,thedynamic recursion cannot becom-\nputed intheusual way,duetothecontinuous parameters\ninEqn. 5.However,wecanapproximate these calcula-\ntions. Ourapproach istogrowatreeofpartial hypotheses\nwhere anode inthetreeatdepththasassociated discrete\nvariables bt;Ctwhere Ct2L(t).After thetthiteration\nouralgorithm, theterminal nodes areallatdeptht,soeach\npath from theroot totheterminal node corresponds toa\npossible history ofthediscrete variables. Rather than scor-\ningeach terminal node with asingle number ,wescore the\nterminal nodes with afunction thatmeasures thegood-\nness ofahistory asafunction oftheunkno wncontinuous\nvariables atframe t.Wewillalternately growandprune\nthetreeinanefforttoapproximate thedynamic program-\nming recursion.\nSuppose thatCt\n0;bt\n0isapossible discrete history .The\nfunction describing thequality ofthishypothesis is\n~SCt\n0;bt\n0(\u000bt)=max\n\u000bt\u00001\n0S(Ct\n0;bt\n0;\u000bt\n0) (6)\n=hCt\n0;bt\n0(7)\n+(\u000bt\u0000mCt\n0;bt\n0)0QCt\n0;bt\n0(\u000bt\u0000mCt\n0;bt\n0)where themaximum isover\u000bt\u00001\n0such that\u000b\u001c(i)=0\nwheni62C\u001c(i)for\u001c=0;:::;t\u00001.This function canbe\ncomputed recursi vely,using theusual dynamic program-\nming idea, duetothequadratic dependence onthecontin-\nuous parameters.\nThus, each hypothesis concerning the\u0002rsttframes cor-\nresponds toaterminal node inourtreeatlevelt,corre-\nsponding tothediscrete history Ct\n0;bt\n0andtheparametri-\ncally represented score function ofEqns. 6,7.\n5.2Pruning theHypotheses\nOur treeconstruction will notbefeasible without some\npruning ofthehistories. Ourapproach topruning isbased\nonthefollowing observ ation. Suppose wehavetwodis-\ncrete histories, Ct\n0;bt\n0and\u0016Ct\n0;\u0016bt\n0where Ct=\u0016Ctand\nbt=\u0016bt.If~SCt\n0;bt\n0(\u000bt)>~S\u0016Ct\n0;\u0016bt\n0(\u000bt)foralllegal\u000bt\nthat is,\u000btwith non-ne gativecoordinates such that\n\u000bt(i)=0when i62Ctthen anycontinuation of\n\u0016Ct\n0;\u0016bt\n0willalwaysscore worse then thesame continuation\nofCt\n0;bt\n0.Thus, thehistory \u0016Ct\n0;\u0016bt\n0may bepruned with no\npossibility oflosing theoptimal history inourtreecon-\nstruction. Wecallsuch apruning adpcutof f.This idea\nextends tothecase inwhich acollection ofhypotheses\ndominate another hypothesis, though thedetermination\nofsuch acollection iscomputationally challenging.\nWeapproximate this idea byperforming dpcutof fs\nwhen adiscrete hypothesis seems unlik elytobeoptimal\nforanylegalvalue of\u000bt.Wedothisbyapproximating,\nforeach survi ving hypothesis, \u0016Ct\n0;\u0016bt\n0,thelegalpoint\u000b\u0003\nt\nwhere ~S\u0016Ct\n0;\u0016bt\n0ismaximum. If,forthispoint\n~SCt\n0;bt\n0(\u000b\u0003\nt)>~S\u0016Ct\n0;\u0016bt\n0(\u000b\u0003\nt)\nforanyofthesurvi vingCt\n0;bt\n0weconclude that, having\nfailed tobemaximal atitsmaximizing point\u000b\u0003\nt,~S\u0016Ct\n0;\u0016bt\n0isunlik elytobemaximal atanylegalpoint\u000bt.Thus, we\nprune \u0016Ct\n0;\u0016bt\n0.\nOurtreeconstruction then proceeds asfollows.After\neach iteration, t,wehaveacollection ofhistories, Ht,\neach with aquadratic score function asinEqns. 6,7. In\nthet+1iteration, weproduce theset\n~H(t+1)=f(Ct+1\n0;bt+1\n0):(Ct\n0;bt\n0)2H(t);\nCt+12L(t+1);\nbt+1=0ifCt+16=Ctg\nThus everysurvi ving history canbecontinued byanypos-\nsible frame hypothesis inL(t+1),while weonly allow\nthepossibility ofrearticulation bt+1=1foracontinued\nchord. Foreach newhistory in~H(t+1)wecompute the\noptimal score function ~SCt+1\n0;bt+1\n0(\u000bt+1).Onthese histo-\nrieswe\u0002rst prune according totheprocedure described\nabovefordynamic programming cutof fs.This phase ap-\nproximates theprocess ofpruning histories thatcould not\nbepart oftheeventual optimal history .Asecond prun-\ningphase further discards histories until weareleftwith\na\u0002xednumber ofpossibilities. Inthisphase thehistories\naresorted according toourapproximation ofthebestscoretheycould produce onthelegalspace. Wethen retain the\nMbesthistories anddenote thissetbyHt+1.\n6TRAINING THE MODEL\nSome ofourexperiments havebeen performed using the\nreasonable, butsome what arbitrary ,probability model for\ntheaudio spectrum giveninEqn. 1.Theadvantage ofsuch\nanapproach isthatitcanbeapplied toanymusic audio,\nwithout prior knowledge ofthemanyfactors leading to\nthepresentation ofanote spectrum. However,givensuch\nprior knowledge, weshould beable toimpro veourmodel\ntobetter capture reality .\nWeassume thatouraudio training data isaccompanied\nwith both asymbolic representation oftheaudio, asina\nMIDI \u0002le, aswell asascorematc h,giving acorrespon-\ndence between thesymbolic representation andtheaudio.\nWeassume only onemodel foreverypossible pitch, as\nwould beappropriate forpiano music.\nAsdiscussed insection 2,wemodel thefst(k)gasre-\nalizations ofindependent Poisson random variables with\nEst(k)=q\u000bt(k)=X\ni2Ct\u000bt(i)q(i;k)\nwhere, during thetraining phase, Ctisknown duetothe\nscore match. This assumption canbeviewed asaconse-\nquence oftheassumption thatst(k)=P\ni2CtZt(i;k)\nwhere thefZt(i;k)gareindependent Poisson random\nvariables withEZt(i;k)=\u000bt(i)q(i;k).Weviewthe\ntraining process asmaximum likelihood estimation ofthe\nf\u000bt(i)gandfq(i;k)gparameters. Though wemust esti-\nmate both ofthese together ,weareonly interested inthe\ntemplate spectra q(i;\u0001)where wecontinue toassume\nthatP\nkq(i;k)=1.These template spectra and\u000bt(i)pa-\nrameters areestimated using astraightforw ardapplication\noftheEMalgorithm inwhich theZt(i;k)areregarded as\nthehidden variables whilest(k)isobserv able.\n7EXPERIMENTS\nWeimplemented andtested thetwoalgorithms described\naboveonseveraldiversemusical examples:\nBach WTC I,Cminor fugue (performed onpiano)\nBeetho venDuet No.3forClarinet andBassoon\nBrahms Symphon y2,Mvt. 1\nCopland FanfarefortheCommon Man\nHaydn String Quartet No.62(Emperor)\nWechose minute-long excerpts from recordings ofeach\npiece andsampled downto8kHz mono audio. Theframe\nsizewas\u0002xedto512samples with ahopsizeof256sam-\nplesforallexperiments.\nEvaluation oftranscription results isitself anontri v-\nialtask. Here weadopt adual strate gyofproviding both\nquantitati veaccurac yresults andaqualitati vediscussion\nofthealgorithm output.Lenter Mean Correct Mean Expected Percent Mean Extra Edit Dist %\nLenter /rNotes/Frame Notes/Frame Correct Notes/Frame corinsdel\nBach simple 0.1 1.6/1.9/1.5 2.4/2.2/2.1 70/85/73 1.7/1.2/0.7 594141\nsimple EM 0.1 1.8/1.8/1.5 \u001f\u001f/\u001f\u001f/\u001f\u001f 78/83/71 1.0/0.7/0.5 643621\ncontinuous 0.1/100 1.4/1.5/1.4 \u001f\u001f/\u001f\u001f/\u001f\u001f 58/70/68 1.6/1.2/0.9 425814\nBeetho ven simple 0.5 1.4/1.4/1.2 2.0/1.8/1.8 70/79/64 1.1/0.7/0.5 514942\ncontinuous 0.2/50 0.1/0.3/0.1 \u001f\u001f/\u001f\u001f/\u001f\u001f 6/16/63 2.8/2.2/1.2 208068\nBrahms simple 0.4 1.8/2.5/1.3 5.6/3.5/2.1 33/73/62 1.3/0.8/0.6 188215\ncontinuous 0.4/200 0.8/1.4/0.9 \u001f\u001f/\u001f\u001f/\u001f\u001f 14/40/44 2.0/1.3/0.9 178313\nCopland simple 1.0 1.2/1.3/0.8 2.2/1.9/1.9 55/67/43 0.4/0.2/0.1 41595\nsimple EM 1.0 1.3/1.3/0.9 \u001f\u001f/\u001f\u001f/\u001f\u001f 58/70/49 0.6/0.4/0.3 366423\ncontinuous 0.7/200 1.1/1.3/1.0 \u001f\u001f/\u001f\u001f/\u001f\u001f 48/65/52 1.0/0.5/0.4 287238\nHaydn simple 0.5 2.0/2.2/1.6 3.5/2.9/2.5 55/77/65 1.1/0.7/0.5 396120\ncontinuous 0.3/100 1.7/1.9/1.3 \u001f\u001f/\u001f\u001f/\u001f\u001f 48/64/54 0.9/0.5/0.3 326823\nTable 1.Results: each entry iscomputed with three different scoring metrics: exact/pitch class/min. homon ym.\n7.1Quantitati veEvaluation\nToevaluate thealgorithms numerically ,we\u0002rst convert\nthecomputed optimal paths intoMIDI \u0002les. Forthesim-\nplerecognition scheme (Section 4)wetraversetheoptimal\npath andgenerate aMIDI `note on'message atanyframe\nwhere anewnote appears. A`note off'isgenerated at\nthe\u0002rstframe where thenote does notappear .Wegener -\nateMIDI forthecontinuous tracking algorithm (Section\n5)inasimilar manner ,butwith theadded condition thata\nnote must haveanamplitude greater than asmall positi ve\ncutof fbefore being considered `on'.\nComparing theMIDI \u0002les tothescore match, wecol-\nlected theaccurac yresults showninTable 1.\nParameter values Thevalues forLenterused ineach ex-\nperiment. risgivenforcontinuous tracking experiments.\nMean correctnotes perframe The average number of\ncorrect notes generated foreach audio frame. The three\nvalues ineach cellcorrespond tothree different evalua-\ntionmetrics (seebelow).\nMean expected notes perframe The total number of\nnotes inthetargetscore divided bythenumber offrames.\nPercent correctThe total number ofcorrect notes di-\nvided byexpected notes, ignoring extranotes.\nMean extra notes perframe These areincorrect notes;\ni.e.,anynotes generated bythealgorithm thatwere not\nlisted inthetargetscore foreach frame.\nEdit distance Wecompute aversion ofedit distance\nadapted topolyphonic music. Unlik etheother metrics\ninthetable, editdistance isunconcerned with note dura-\ntions andaudio frames. Thethree numbers ineach cellare\nthenumber ofcorrect matches, insertions, anddeletions,\nexpressed aspercentages ofthenumber oftargetnotes.\nNote thatduetothelargenumber ofinsertions anddele-\ntions, wereport these values separately forclarity instead\nofcombining allthree intoasingle score.\nForcells with multiple metrics computed, the\u0002rst entry\ncorresponds tothenatural (exact) metric: anote iscor-\nrectifitappears inthetargetscore; otherwise itislabeledas`extra'. The second metric reported isbased onpitch\nclass: foreach frame, theoutput chord andtargetchord\nareboth reduced toasetofpitch classes (i.e.octaveerrors\nareignored). The\u0002nal metric issimilar ,buteach chord is\nreduced toa`minimal homon ym' byignoring notes that\nareharmonics ofnotes lowerinpitch.\n7.2Parameter Selection andAudio Output\nBoth algorithms include asetofparameters thatcanbe\ntuned toimpro veperformance. Thevalues ofLenterand\nrshownabovewere selected bytrial anderror .Values of\nLenterbetween 0:1and1:5typically seemed best, asdid\nvalues ofrranging between 10and200.Other crucial\nparameters thatremained \u0002xedincluded:\nMaximum number ofnotes perframe Fixed to\u0002ve\n(simple algorithm) orfour (continuous).\nMaximum number ofhistories Fixedto150;thetreeof\nbestpaths waspruned tothissizeateach iteration.\nMinimum amplitude Notes with energyless than this\ncutof fwere ignored inthelattice generation phase. Set\nto0:01(simple) or0:05(continuous).\nThealgorithm togenerate MIDI \u0002les described abovehas\ntwomain weaknesses. First, incases where thesame note\nisrearticulated without aninterv ening rest, noadditional\nMIDI `note on'will begenerated. Repeated notes are\nthusbecombined intosingle long MIDI notes, eliminating\nrearticulations from theoutput. Second, theMIDI \u0002les do\nnottakeadvantage ofthecontinually varying amplitude\ndata captured bythecontinuous model.\nWegenerated audio output \u0002les1from thecontinuous\nalgorithm bysuperimposing sine waves.Foreach note in\neach frame ofaudio, wegenerated thefundamental fre-\nquenc ywith itscomputed amplitude andadded in\u0002ve\novertones with amplitudes decreasing as1=n.\n1Audio \u0002les in.wavformat are available online at\nhttp://xavier.informatics.indiana.edu/\ncraphael/ismir07/transcription/7.3Discussion\nBoth algorithms beginbygenerating ahypothesis lattice.\nFormost experiments, thecorrect hypothesis appears in\nabout 90% ofthelattice frames, giving usanupper bound\nontheaccurac ywecanexpect from later stages. Ourbest\nresult wastheBach excerpt, with 78% notes exactly cor-\nrect when using thesimple algorithm with trained note\ntemplates. Without EM training performance drops to\n70%. This excerpt isperhaps thesimplest interms oftim-\nbreforourmodel, because itonly involvesoneinstrument.\nEvenwhen trained, ourmodel assigns only oneprobabil-\nitydistrib ution toeach MIDI note. Inamore comple x\npiece such astheBrahms symphon y,thedifferent timbres\nofmultiple instruments may confound thepitch model.\nResults ofthepitch class andhomon ymmetrics also\npoint toproblems related toinstrument timbre: there is\ngenerally asigni\u0002cant jump inthepercent correct when\nweconsider thepitch class (octa veequivalence) metric.\nForexample, inthesimple algorithm result forBach, there\nisa15% increase inaccurac yifweignore octavemis-\ntakes.Octaveerrors arise from misinterpreting thefunda-\nmental frequenc y,which canhappen easily ifthemodel\nmisrepresents thetruedistrib ution ofenergyacross differ-\nentharmonics. IntheBach case, training themodel re-\nduces thedisparity between theexact andpitch class met-\nrics, suggesting atimbre problem before training.\nTable 1showsthatthere areoften asmanyextranotes\nascorrect notes. This may bethemost glaring weakness\noftheresults. However,thehomon ymmetric sheds some\nlight: there arefarfewerextranotes perframe when we\ndiscount notes thataremembers oftheovertone series of\ncorrect notes. Listening totheresults suggests another\nsource oferrors: some notes lasttoolong andoverlap suc-\ncessi venotes. Due toreverberation andresonance, notes\npersist longer than notated inamusical score. Determin-\ningthepercei vedcutof ftime ofanote, asopposed tothe\nacoustic decay time, isadif\u0002cult open question.\nSurprisingly ,thesimple algorithm alwaysscored bet-\nterthan thecontinuous tracking version. However,recall\nthatthecontinuous results arebased oncomparison ofthe\ngenerated MIDI \u0002lewith thetargetoutput. Indeed, the\naudio synthesized from thecontinuous algorithm sounded\nconsistently better than theMIDI output from thesimple\nalgorithm. Thelifelik equality oftheresults suggests that\ncontinually-track edamplitudes capture more performance\ndetails than simple note onset events. Theoutput recalls\nthedynamic range ofapiece, balance between instru-\nments, andcrescendos/decrescendos within asingle note.\nWhile there isnostandardized collection ofaudio data\nand associated evaluation metrics inthe\u0002eld ofpoly-\nphonic audio recognition, ourresults subjecti velyappear\nnottobeasgood assome other workcited intheintro-\nduction. However,ourapproach solvestheslightly differ-\nentproblem ofsimultaneous note andamplitude tracking.\nThecontinuous amplitude values output byouralgorithm\nmay beuseful inatranscription system thataims tocap-\nturedynamics aswell aspitches, or,similarly ,inasystem\nforanalysis ofexpressi vemusical performance.8REFERENCES\n[1]Abdallah S.,Plumble yM.,Polyphonic Transcrip-\ntionbyNon-Ne gativeSparse Coding ofPowerSpec-\ntra,Proceedings oftheFifthInternational Confer -\nence onMusic Information Retrie val, ISMIR 2004\npp.318-325, Barcelona, Spain, 2004.\n[2]BoydS.P.,Vandenber ghe L.2004. ConvexOp-\ntimization .Cambridge, UK: Cambridge University\nPress.\n[3]Cemgil A.T.,Kappen H.J.,Barber D.AGener -\nativeModel forMusic Transcription, IEEE Trans-\nactions onAudio, Speec handLangua geProcessing\n14(2), March 2006.\n[4]Dubois C.,and DavyM., Joint Detection and\nTracking ofTime-V arying Harmonic Components: a\nGeneral Bayesian Frame work,IEEE Transactions\nonAudio, Speec handLangua geProcessing toap-\npear.\n[5]Godsill S.,andDavyM.,Bayesian Harmonic Mod-\nelsforMusical Pitch Estimation and Analysis, \nProc.IEEE International Conf .onAcoustics Speec h,\nand Signal Processing (ICASSP2002), pp.1769-\n1772.\n[6]Goto M., AReal-T ime Music Scene-Description\nSystem: Predominant-F0 Estimation forDetecting\nMelody andBass Lines inReal-W orld Audio Sig-\nnals,ISCA Journal ,43(4):311-329,2004.\n[7]Kameoka H.,Nishimoto Y.,Sagayama S.,`Au-\ndioStream Segregation ofMulti-Pitch Music Sig-\nnalBased onTime-Space Clustering Using Gaussian\nKernel 2-Dimensional Model, Proc.IEEE Interna-\ntional Conf .onAcoustics Speec h,andSignal Pro-\ncessing (ICASSP2005) .\n[8]Kapanci E.,PfefferA., Signal-to-Score Music\nTranscription Using Graphical Models, Proc.19th\nInt.Joint Conf .onArtif.Intel. (IJCAI) ,Edinb urgh,\nUK, August 2005.\n[9]Klapuri A.,Multiple Fundamental Frequenc yEsti-\nmation byHarmonicity andSpectral Smoothness, \nIEEE Trans. Speec hand Audio Processing ,11(6),\n804-816, 2003.\n[10] Klapuri A.,Multiple Fundamental Frequenc yEs-\ntimation bySumming Harmonic Amplitudes, Pro-\nceedings oftheSeventh International Confer ence on\nMusic Information Retrie val,ISMIR 2006 pp.216-\n221, Victoria, BC,Canada, USA, 2006.\n[11] Peeling P.,LiC.,Godsill S.,Poisson point process\nmodeling forpolyphonic music transcription, Jour-\nnaloftheAcoustical Society ofAmerica Expr essLet-\nters121(4):EL168-EL175, April 2007."
    },
    {
        "title": "Assessment of Perceptual Music Similarity.",
        "author": [
            "Alberto Novello",
            "Martin F. McKinney"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415050",
        "url": "https://doi.org/10.5281/zenodo.1415050",
        "ee": "https://zenodo.org/records/1415050/files/NovelloM07.pdf",
        "abstract": "This paper extends a study on music similarity perception presented at ISMIR last year, in which subjects ranked the similarity of excerpt-pairs presented in triads [1]. The larger number of subjects and stimuli in the current study required a modification of the methodological strategy. We use here two nested incomplete block designs in order to cover the full set of song-excerpts comparisons (triads) while limiting the experimental time per subject. In addition to the two variable factors of the previous experiment, tempo and genre, we examine here the effect of prevalent instrument timbre. We found that 69 of 78 subjects where significantly consistent in their judgments of repeated triads. Furthermore, we found significant acrosssubject consistency on all 10 repeated triads. A significant difference was found in the distributions of interand intra-genre excerpt distances. The stress values in the Shepard’s plot shows evidence of increased complexity in the present study compared to the previous smaller study. 1 INTRODUCTION Recently, there has been an increasing interest in music similarity, both in the applicative [2] and research [5][6] fields. Various theoretical [3][4] and experimental works [5][6] have concentrated on which dimensions underlie listeners’ perception of similarity. These studies were run on a small number of stimuli or on a limited number of genres, making it difficult to extend conclusions to the large corpus of Western music. One of the most challenging problems in conducting an experiment on music similarity perception is dealing with the trade off between experimental time and the number of stimuli required for a complete representation of the complexity of the musical world. Our recent study showed that a method combining triadic comparisons and Balanced Incomplete Block Design (BIBD) limited the reasonable experiment duration per subject to a reasonable length (< 1 hour) while examining 18 excerpts. Here, we show how it is possible to further optimize the experimental design using two nested BIBDs to increase the number of stimuli, and thus to examine a broader range of musical styles. c⃝2007 Austrian Computer Society (OCG). 2 METHOD We employed a method using triadic comparisons of songexcerpts, because it is a straightforward procedure for subjects and it alleviates problems associated with scale interpretation. We used two nested BIBD to achieve triad reduction: one to create an incomplete but overlapping set of genres for each subject, the second to create a set of triads within each genre-set. The first BIBD was calculated to determine the musical genres for each subject: we used quadratic comparisons (4 genres) per subject. The BIBD formula shows in this case, the number of genre-sets, b: b = λn(n −1) k(k −1) . (1) With n=13 genres, k=4 (quadratic) and λ=3 (each genre pair appeared in three subject designs), we obtain b=39 genre-set, one for each subject. For each genre-set, a BIBD on excerpts was generated. With 6 excerpts per genre, the number of excerpts per genre-set is 24. We used k=3 (triadic) and λ=2 (each excerpt-pair appears twice) reaching 184 triads per genreset (subject). We added ten repeated triads for each subject for evaluation of within and across subject consistency.",
        "zenodo_id": 1415050,
        "dblp_key": "conf/ismir/NovelloM07",
        "keywords": [
            "music similarity perception",
            "subjects ranked the similarity",
            "excerpts presented in triads",
            "nested incomplete block designs",
            "tempo and genre",
            "prevalent instrument timbre",
            "significant consistent judgments",
            "acrosssubject consistency",
            "Shepards plot",
            "increased complexity"
        ],
        "content": "ASSESSMENT OF PERCEPTUAL MUSIC SIMILARITY\nAlberto Novello\nPhilips Research Laboratories, Eindhoven\nThe Netherlands\nalberto.novello@philips.comMartin McKinney\nPhilips Research Laboratories, Eindhoven\nThe Netherlands\nmartin.mckinney@philips.com\nABSTRACT\nThis paper extends a study on music similarity perception\npresented at ISMIR last year, in which subjects ranked\nthe similarity of excerpt-pairs presented in triads [1]. The\nlarger number of subjects and stimuli in the current study\nrequired a modiﬁcation of the methodological strategy.\nWe use here two nested incomplete block designs in or-\nder to cover the full set of song-excerpts comparisons (tri-\nads) while limiting the experimental time per subject. In\naddition to the two variable factors of the previous exper-\niment, tempo and genre, we examine here the effect of\nprevalent instrument timbre. We found that 69 of 78 sub-\njects where signiﬁcantly consistent in their judgments of\nrepeated triads. Furthermore, we found signiﬁcant across-\nsubject consistency on all 10 repeated triads. A signif-\nicant difference was found in the distributions of inter-\nand intra-genre excerpt distances. The stress values in the\nShepard’s plot shows evidence of increased complexity in\nthe present study compared to the previous smaller study.\n1 INTRODUCTION\nRecently, there has been an increasing interest in music\nsimilarity, both in the applicative [2] and research [5][6]\nﬁelds. Various theoretical [3][4] and experimental works\n[5][6] have concentrated on which dimensions underlie\nlisteners’ perception of similarity. These studies were run\non a small number of stimuli or on a limited number of\ngenres, making it difﬁcult to extend conclusions to the\nlarge corpus of Western music.\nOne of the most challenging problems in conducting an\nexperiment on music similarity perception is dealing with\nthe trade off between experimental time and the number\nof stimuli required for a complete representation of the\ncomplexity of the musical world.\nOur recent study showed that a method combining tri-\nadic comparisons and Balanced Incomplete Block Design\n(BIBD) limited the reasonable experiment duration per\nsubject to a reasonable length ( <1 hour) while examining\n18 excerpts. Here, we show how it is possible to further\noptimize the experimental design using two nested BIBDs\nto increase the number of stimuli, and thus to examine a\nbroader range of musical styles.\nc/circlecopyrt2007 Austrian Computer Society (OCG).2 METHOD\nWe employed a method using triadic comparisons of song-\nexcerpts, because it is a straightforward procedure for sub-\njects and it alleviates problems associated with scale in-\nterpretation. We used two nested BIBD to achieve triad\nreduction: one to create an incomplete but overlapping set\nof genres for each subject, the second to create a set of\ntriads within each genre-set.\nThe ﬁrst BIBD was calculated to determine the musical\ngenres for each subject: we used quadratic comparisons (4\ngenres) per subject. The BIBD formula shows in this case,\nthe number of genre-sets, b:\nb=λn(n−1)\nk(k−1). (1)\nWith n=13 genres, k=4 (quadratic) and λ=3 (each genre\npair appeared in three subject designs), we obtain b=39\ngenre-set, one for each subject.\nFor each genre-set, a BIBD on excerpts was gener-\nated. With 6 excerpts per genre, the number of excerpts\nper genre-set is 24. We used k=3 (triadic) and λ=2 (each\nexcerpt-pair appears twice) reaching 184 triads per genre-\nset (subject). We added ten repeated triads for each subject\nfor evaluation of within and across subject consistency.\n2.1 The Web Experiment\nWe performed a web experiment which involved 78 sub-\njects, running the total genre BIBD design twice. We had\n59 males and 19 females and 50 musicians and 28 non-\nmusicians. The average subject age was 28 years. After\nlistening to a triad of excerpts, the subject had to choose\nthe most similar and dissimilar pair among the three pos-\nsibilities. The stimuli were 15-second excerpts of West-\nern popular music covering a range of 13 musical styles.\nGenre classiﬁcation was based on the ”allmusic” website\n[7]. The song-excerpts belonged to one of two tempo cat-\negories: fast, for excerpts whose tempo was faster than\n120 BPM for a quarter note; and slow, for excerpts whose\ntempo was slower than 100 BPM for a quarter note. The\nexcerpts were also selected to fall into one of three tim-\nbre categories depending on their dominant musical in-\nstrument. The timbre categories were vocal, guitar and\npiano allowing us to ﬁnd excerpts containing these instru-\nments in all selected genres.2.2 Analysis\nThe data analysis consisted of three main stages: within-\nsubject consistency, across-subject consistency, and Mul-\ntidimensional Scaling (MDS). We used the Kendal Coef-\nﬁcient of Concordance (KCC) [8] to evaluate consistency\nfor both within and across subjects.\nTo model the multidimensional perceptual space, we\nﬁrst built a dissimilarity matrix of all subjects’ rankings,\nassigning the value ’2’ for the least similar pair, ’1’ to the\nmiddle pair and ’0’ for the most similar. The ALSCAL\nmultidimensional scaling algorithm [9] was used to es-\ntimate the coordinates of the excerpt positions best ﬁt-\nting the original data using a range of dimensionality or-\nders. The Shepard’s plot shows stress as a function of the\nnumber of dimensions. The optimal number of dimen-\nsions necessary to achieve an acceptable ﬁt is typically\nthe smallest number given the stress value is less than 0.2.\n3 RESULTS\nWe calculated the within-subject consistency using the ten\nrepeated triads of each subject. 69 subjects showed signif-\nicant consistency at the 0.5 signiﬁcance level in their re-\npeated rankings, among these 4 subjects where very close\nto signiﬁcance and only one subject performed very low\nin consistency. We also calculated the across-subject con-\nsistency on the repeated triads rankings. Figure 1 shows\nthat the across subjects consistency is signiﬁcant on all 10\ntesting triads, which is in line with our previous result of\nsigniﬁcant across-subject consistency in 97 over 102 tri-\nads.\nFigure 1 . Across-subject consistency per triad\nWe calculated the Shepard’s plot, shown in Figure 2\nfrom our MDS analysis. The best compromise here, be-\ntween dimension-order and stress value is six dimensions,\ngiving a stress value of 0.175, while the previous study\nshowed three dimensions to be the optimal choice.\nFrom the coordinates of the excerpt positions in the six-\ndimensional space, we calculated inter- and intra-genre\ndistances. The two distributions show signiﬁcant differ-\nence, in agreement with our past results, conﬁrming the\nimportance of genre a factor in subjects’ ranking.\nFigure 2 . Stress value vs number of dimensions\n4 DISCUSSION\nThe results conﬁrm the ﬁndings of the previous study [1]:\nthere is signiﬁcant across-subject consistency on excerpt\nsimilarity ranking; most subjects also show signiﬁcant con-\nsistency in repeated triads. However the stress values in\nthe Shepard’s plot shows evidence of increased complex-\nity in the present study, most likely due to the lager set\nof stimuli. Thus the perception of music similarity in the\ncurrent study is not easily mapped to three dimensions as\nin the previous study.\n5 REFERENCES\n[1] Novello A., McKinney .M, Kohlrausch .A,\n“Perceptual Representation of Music Similar-\nity”, 246-249, ISMIR 2006 Proceedings.\n[2] Pampalk E., “Computational Models of Music\nSimilarity and their Application in Music In-\nformation Retrieval”, Ph.D. dissertation 2006.\n[3] Deliege I.,“Introduction, Similarity Perception\n- Categorization - Cue Abstraction”, Music\nPercept. , 18(3), 233-43, (2001).\n[4] Ockelford A., “On similarity, derivation and\nthe cognition of musical structure”, Psychol-\nogy of Music , 32(1), 23-74, (2004).\n[5] Chupchik G. C., “Similarity and preference\njudgements of musical stimuli”, Scand. J. Psy-\nchol. , 23, 273-282, (1982).\n[6] Lamont A., Dibben, N., “Motivic Structure\nand the Perception of Similarity”, Music Per-\ncept., 18(3), 245-74, (2001).\n[7] http://www.allmusic.org/.\n[8] Kendall M., “Rank Correlation Methods”,\nCharles Grifﬁn, London (1975).\n[9] Young F. W., Lewyckyj R., “ALSCAL User’s\nGuide ( 5thEd.)”, L.L. Thurstone Psychomet-\nric Laboratory, University of North Carolina,\nChapel Hill, NC (1996)."
    },
    {
        "title": "A Stochastic Representation of the Dynamics of Sung Melody.",
        "author": [
            "Yasunori Ohishi",
            "Masataka Goto",
            "Katunobu Itou",
            "Kazuya Takeda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414732",
        "url": "https://doi.org/10.5281/zenodo.1414732",
        "ee": "https://zenodo.org/records/1414732/files/OhishiGIT07.pdf",
        "abstract": "In this paper, we propose a stochastic representation of a sung melodic contour, called stochastic phase representation (SPR), which can characterize both musical-note information and the dynamics of singing behaviors included in the melodic contour. The SPR is constructed by fitting probability distribution functions to F0 trajectories in the F0-∆F0 phase plane. Since fluctuations in singing can be easily separated by using SPR, we applied SPR to a melodic similarity measure for query-by-humming (QBH) applications. Our experimental results showed that the SPR-based similarity measure was superior to a conventional dynamic-programming-based method. 1 INTRODUCTION The goal of this study is to build a model that can represent the dynamics of various singing behaviors (e.g., fluctuations in a musical note and continuous transitions between notes) in a sung melodic contour. Although a symbolic melodic contour (a sequence of musical notes) can be easily modeled by a discrete-time stochastic representation such as n-grams, this representation cannot be used for modeling a sung melody because it is difficult to represent the singing dynamics of its melodic contour, such as vibrato and overshoot. The dynamic representation for modeling a sung melody is important for defining an appropriate melodic similarity between sung melodies, which is useful for various applications such as query-byhumming (QBH) and automatic clustering of songs. Most previous studies including symbolic melodic similarities [1, 2] and melodic similarities for sung melodies [3, 4, 5, 6] focused on the retrieval performance. For sung melodies, for example, a melodic contour was represented by a discrete symbolic sequence of musical notes [3, 4] or a sequence of pitch histograms for unstable pitch contours [5, 6]. Since they did not model the dynamics at all, their melodic similarities are sometimes too sensitive to singing behaviors that may differ among singers. Therefore, we propose a novel stochastic graphical representation of the dynamic properties of sung melodic contours, called stochastic phase representation (SPR). This representation is a generative model of melodic contours and can separate the dynamics of various singing behaviors from an original musical note sequence. By using this c⃝2007 Austrian Computer Society (OCG). 0 1 2 3 4 5 6 7 4500 5000 5500 6000 6500 F0 [cent] 4500 5000 5500 6000 4500 6000 1000 500 0 500 -1000 1000 500 0 -500 -1000 6500 Circle (Vibrato) Spiral (Overshoot) [cent] ∆F0 [cent / sec] (a)  F0 contour 5000 8 [sec] (b)  Phase plane 5500 6500 [cent] (c)  Stochastic phase representation ∆F0 [cent / sec] Figure 1. Schematic view of constructing stochastic phase representation (SPR). The original F0 contour (a) is mapped onto the F0-∆F0 phase plane (b). By fitting Gaussian mixture models to trajectories on the phase plane, stochastic representation of the F0 dynamics (c) can be constructed. representation, we also define a melodic similarity measure for QBH applications. In our experiments, we show the effectiveness of this similarity measure based on SPR. 2 STOCHASTIC PHASE REPRESENTATION (SPR) FOR MELODIC CONTOUR Figure 1 shows an example of an SPR constructed from singing melodic contours represented as trajectories of the fundamental frequency (F0). We assume that the F0 trajectories are generated by a dynamic system and represented in a two-dimensional phase plane, ⃗f(x, ˙x), where x is the F0 and ˙x is its differential. That is, ⃗f(x, ˙x) represents the local direction of an F0 trajectory. A fluctuation in a sung melody can be modeled by a damped oscillation of the dynamic system and appears as a curling trajectory around a certain target point, i.e., an attractor of the system. The advantage of this modeling is that typical singing behaviors can be characterized by the shape of curling trajectories. As shown in Fig. 1(b), for example, a vibrato within a musical note appears as a circular pattern because it has the quasi-periodic modulation of the F0, and an overshoot after a note change appears as a spiral pattern because the F0 of the overshoot transitionally exceeds the F0 of a (target) musical note just after the note change. Here, the location of each attractor corresponds to the F0 of its target musical note. Therefore, we model the curling trajectories by fitting a Gaussian mixture model (GMM) so that the likelihood of observing the given trajectories becomes the maximum. We refer to this GMM-based representation of the F0 trajectories (sung melodic contours) as stochastic phase representation (SPR) shown in Fig. 1(c). The F0 of musical notes is represented by the location of the local maxima of the SPR, and the singing behavior of those notes is represented by the shape around the local maxima. Because each (target) note and its relative length in a melodic contour are captured as the location and its height of the corresponding local maximum, respectively, the divergence between GMM-based distributions in the phase plane is expected to be a robust melodic similarity measure that can reduce variations by singing behaviors and focuses on the original (target) melodic information. 3 EXPERIMENTS The potential of SPR was preliminarily evaluated on a small QBH application. The song database consists of 50 short excerpts from 25 pop songs of the RWC Music Database (RWC-MDB-P-2001) [7]. The average length of those excerpts is 12 s. For query melodies, 75 subjects listened to each of the above 50 excerpts and then sang its melody with lyrics [8]. The number of recorded samples was 3,750 (75 × 50), but we used 3,257 samples after excluding samples whose melody was extremely different from the original melody. 1 The F0 contour of the query melodies was estimated for every 10 ms by using YIN [9]. The F0 contour of the 50 excerpts in the song database was manually annotated [10]. Both F0 contours were represented in cents so that one equal-tempered semitone corresponds to 100 cents, and then normalized by subtracting the average F0 value over each contour. Finally, the similarity between a query melody and each excerpt in the song database was calculated by using a histogram-intersection distance [11] between their discretized SPRs. SPRs were modeled by 16-mixture GMMs and converted into discretized SPRs where F0 and ∆F0 were uniformly partitioned into square cells (100 cent F0 × 25 cent/sec ∆F0) and relative occurrences (frequencies) within square cells were calculated. However, since this discretized-SPR-based distance did not take into account the temporal order of notes, we divided a long contour into several short segments so that short segments of the query can be compared with the corresponding short segments of each database excerpt in order. Their similarity was calculated by the cumulative sum of their distances. We thus investigated the performance improvement by increasing the number of segments. As for the baseline performance, we also evaluated a tradi1 Since all songs in RWC-MDB-P-2001 were original compositions, the subjects were not familiar with these melodies. Table 1. Percentage of Mean Reciprocal Rank (MRR) DTW Proposed # of segment 1 2 4 8 MRR [%] 64.3 45.6 57.1 65.6 71.1 0",
        "zenodo_id": 1414732,
        "dblp_key": "conf/ismir/OhishiGIT07",
        "keywords": [
            "stochastic phase representation (SPR)",
            "melodic similarity measure",
            "query-by-humming (QBH)",
            "sung melodic contour",
            "F0 trajectories",
            "Gaussian mixture model (GMM)",
            "F0 dynamics",
            "damped oscillation",
            "curling trajectory",
            "attractor"
        ],
        "content": "A STOCHASTICREPRESENTATIONOF THE DYNAMICSOF SUNG MELODY\nYasunoriOHISHI\nGraduateSchool of\nInformation Science,\nNagoyaUniversity\nohishi@sp.m.is.nagoya-u.ac.jpMasatakaGOTO\nNational Institute of\nAdvancedIndustrial Science\nand Technology(AIST)\nm.goto@aist.go.jpKatunobuITOU\nFacultyof Computer and\nInformationSciences,\nHosei University\nitou@k.hosei.ac.jpKazuya TAKEDA\nGraduateSchool of\nInformationScience,\nNagoyaUniversity\nkazuya.takeda@nagoya-u.jp\nABSTRACT\nIn this paper, we propose a stochastic representation of a\nsungmelodiccontour,called stochasticphaserepresenta-\ntion (SPR) , which can characterize both musical-note in-\nformationandthedynamicsofsingingbehaviorsincluded\nin the melodic contour. The SPR is constructed by ﬁt-\ntingprobabilitydistributionfunctionstoF0trajectoriesin\nthe F0- ∆F0 phase plane. Since ﬂuctuations in singing\ncan be easily separated by using SPR, we applied SPR\nto a melodic similarity measure for query-by-humming\n(QBH)applications. Ourexperimentalresultsshowedthat\nthe SPR-based similarity measure was superior to a con-\nventionaldynamic-programming-based method.\n1 INTRODUCTION\nThe goal of this study is to build a model that can rep-\nresent the dynamics of various singing behaviors (e.g.,\nﬂuctuations in a musical note and continuous transitions\nbetween notes) in a sung melodic contour. Although a\nsymbolic melodic contour (a sequence of musical notes)\ncan be easily modeled by a discrete-time stochastic rep-\nresentationsuchas n-grams,this representationcannot be\nused for modeling a sung melody because it is difﬁcult\nto represent the singing dynamics of its melodic contour,\nsuch as vibrato and overshoot. The dynamic representa-\ntionformodelingasungmelodyisimportantfordeﬁning\nanappropriatemelodicsimilaritybetweensungmelodies,\nwhichisusefulforvariousapplicationssuchasquery-by-\nhumming (QBH) and automatic clustering of songs.\nMostpreviousstudiesincludingsymbolicmelodicsim-\nilarities [1, 2] and melodic similarities for sung melodies\n[3,4,5,6]focusedontheretrievalperformance. Forsung\nmelodies,forexample,amelodiccontourwasrepresented\nbyadiscretesymbolicsequenceofmusicalnotes[3,4]or\nasequenceofpitchhistogramsforunstablepitchcontours\n[5, 6]. Since they did not model the dynamics at all, their\nmelodicsimilaritiesaresometimestoosensitivetosinging\nbehaviorsthat may differamong singers.\nTherefore,weproposeanovelstochasticgraphicalrep-\nresentationofthedynamicpropertiesofsungmelodiccon-\ntours, called stochastic phase representation (SPR) . This\nrepresentation is a generative model of melodic contours\nand can separate the dynamics of various singing behav-\niorsfromanoriginalmusicalnotesequence. Byusingthis\nc°2007 Austrian Computer Society (OCG).\n0 1 2 3 4 5 6 74500 5000 5500 6000 6500 \nF0 [cent] \n4500 5000 5500 6000 4500 6000 1000 \n500 \n0\n500 \n-1000 \n1000 \n500 \n0\n-500 \n-1000 6500 Circle \n(Vibrato) Spiral \n(Overshoot) \n[cent] ∆F0 [cent / sec] (a)  F0 contour \n5000 8 [sec] \n(b)  Phase plane 5500 \n6500 [cent] \n(c)  Stochastic phase representation ∆F0 [cent / sec] Figure1.Schematicviewofconstructingstochasticphaserep-\nresentation (SPR). The original F0 contour (a) is mapped onto\nthe F0- ∆F0 phase plane (b). By ﬁtting Gaussian mixture mod-\nels to trajectories on the phase plane, stochastic representation\nof the F0 dynamics (c) can be constructed.\nrepresentation, we also deﬁne a melodic similarity mea-\nsure for QBH applications. In our experiments, we show\ntheeffectivenessofthissimilaritymeasurebasedonSPR.\n2 STOCHASTICPHASE REPRESENTATION\n(SPR)FOR MELODIC CONTOUR\nFigure1showsanexampleofanSPRconstructedfrom\nsingingmelodiccontoursrepresentedastrajectoriesofthe\nfundamental frequency (F0). We assume that the F0 tra-\njectories are generated by a dynamic system and repre-\nsented in a two-dimensional phase plane, ~f(x;˙x), where\nxis the F0 and ˙xis its differential. That is, ~f(x;˙x)repre-\nsents the local direction of an F0 trajectory. A ﬂuctuation\nin a sung melody can be modeled by a damped oscilla-\ntion of the dynamic system and appears as a curling tra-\njectory around a certain target point, i.e., an attractor of\nthe system. The advantage of this modeling is that typi-\ncalsingingbehaviorscanbecharacterizedbytheshapeof\ncurling trajectories. As shown in Fig. 1(b), for example,\na vibrato within a musical note appears as a circular pat-\ntern because it has the quasi-periodic modulation of the\nF0, and an overshoot after a note change appears as a spi-ral pattern because the F0 of the overshoot transitionally\nexceedstheF0ofa(target)musicalnotejustafterthenote\nchange. Here, the location of each attractor corresponds\nto the F0 of its targetmusical note.\nTherefore, we model the curling trajectories by ﬁtting\na Gaussian mixture model (GMM) so that the likelihood\nofobservingthegiventrajectoriesbecomesthemaximum.\nWe refer to this GMM-based representation of the F0 tra-\njectories(sungmelodiccontours)as stochasticphaserep-\nresentation (SPR) shown in Fig. 1(c). The F0 of musical\nnotes is represented by the location of the local maxima\noftheSPR,andthesingingbehaviorofthosenotesisrep-\nresented by the shape around the local maxima. Because\neach(target)note and its relativelength in a melodic con-\ntour are captured as the location and its height of the cor-\nresponding local maximum, respectively, the divergence\nbetween GMM-based distributions in the phase plane is\nexpected to be a robust melodic similarity measure that\ncanreducevariationsbysingingbehaviorsandfocuseson\nthe original (target)melodic information.\n3 EXPERIMENTS\nThe potential of SPR was preliminarily evaluated on\na small QBH application. The song database consists of\n50 short excerpts from 25 pop songs of the RWC Music\nDatabase (RWC-MDB-P-2001) [7]. The average length\nof those excerpts is 12 s. For query melodies, 75 subjects\nlistenedtoeachoftheabove50excerptsandthensangits\nmelody with lyrics [8]. The number of recorded samples\nwas 3,750 (75 £50), but we used 3,257 samples after\nexcludingsampleswhosemelodywasextremelydifferent\nfrom the original melody.1\nThe F0 contour of the query melodies was estimated\nfor every 10 ms by using YIN [9]. The F0 contour of the\n50 excerpts in the song database was manually annotated\n[10]. Both F0 contours were represented in cents so that\none equal-tempered semitone corresponds to 100 cents,\nand then normalized by subtracting the average F0 value\novereach contour.\nFinally, the similarity between a query melody and\neachexcerptinthesongdatabasewascalculatedbyusing\na histogram-intersection distance [11] between their dis-\ncretizedSPRs. SPRsweremodeledby16-mixtureGMMs\nand converted into discretized SPRs where F0 and ∆F0\nwere uniformly partitioned into square cells (100 cent F0\n£25cent/sec ∆F0)andrelativeoccurrences(frequencies)\nwithin square cells were calculated.\nHowever, since this discretized-SPR-based distance\ndid not take into account the temporal order of notes, we\ndivided a long contour into several short segments so that\nshortsegmentsofthequerycanbecomparedwiththecor-\nrespondingshortsegmentsofeachdatabaseexcerptinor-\nder. Theirsimilaritywascalculatedbythecumulativesum\nof their distances. We thus investigated the performance\nimprovement by increasing the number of segments. As\nfor the baseline performance, we also evaluated a tradi-\n1Since all songs in RWC-MDB-P-2001 were original compositions,\nthe subjects were not familiarwith these melodies.Table1. Percentage of Mean Reciprocal Rank (MRR)\nDTW Proposed\n#of segment 1 2 4 8\nMRR[%] 64.345.6 57.1 65.6 71.1\n0 0.2 0.4 0.6 0.8 100.2 0.4 0.6 0.8 1\nRecall Precision Proposed (2 segments)DTW \nProposed (1 segment) \nProposed (4 segments)\nProposed (8 segments)\nFigure2. ROCcurvesof similarity measures.\ntional dynamic time warping (DTW) matching technique\nusing F0 contours.\n4 RESULTSAND DISCUSSIONS\nThe obtained QBH results of the mean reciprocal rank\n(MRR) and ROC curves are shown in Table 1 and Fig.\n2. The proposed distance using the original contours was\ninferiortothebaselineDTW.However,ifweusedthepro-\nposed cumulative distance after dividing each query con-\ntour and each database excerpt into eight segments, the\nMRRperformanceandtheROCcurvewereimprovedand\nwere better than the DTW.\nThese preliminary results showed that our histogram-\nbased distance using SPR is promising for measuring\nmelodic similarity. In the future, we plan to evaluate it\nin detail on a larger database. Although SPR has great\npotential for representing and generating singing dynam-\nics, we have not tested it yet. Future work will include\nthe evaluation of its ability to automatically detect partic-\nular singing behaviors such as vibrato and overshoot, and\nthe generation of melodic contours that reﬂect personal\nsinging behaviors.\n5 REFERENCES\n[1]Typke, R. et al., “Using transportation distances for measuring\nmelodicsimilarity,” Proc.ISMIR ,2003.\n[2]Grachten, M. et al., “Melodic Similarity: Looking for a Good Ab-\nstractionLevel,” Proc.ISMIR ,2004.\n[3]Hu, N. et al., “A probabilistic model of melodic similarity,” Proc.\nICMC,2002.\n[4]Pauws,S.,“CubyHum: Afullyoperationalquerybyhummingsys-\ntem,”Proc.ISMIR , 2002.\n[5]Adams,N.H.etal.,“TimeSeriesAlignmentforMusicInformation\nRetrieval,” Proc.ISMIR , 2004.\n[6]Song, J. et al., “Mid-Level Music Melody Representation of\nPolyphonic Audio for Query-by-Humming System,” Proc. ISMIR ,\n2002.\n[7]Goto, M. et al., “RWC music database: Popular, classical, and jazz\nmusicdatabases,” Proc.ISMIR , 2002.\n[8]Goto, M. et al., “AIST Humming Database:Music Database for\nSinging Research,” IPSJ (MUS) , Vol. 2005, No. 82, pp. 7-12, 2005\n(inJapanese).\n[9]de Cheveigne, A. et al., YIN, “a fundamental frequency estimator\nforspeech and music,” JASA,Vol.111,No.4, pp.1917-1930, 2002.\n[10]Goto,M.,“AISTAnnotationfortheRWCMusicDatabase,” Proc.\nISMIR, 2006.\n[11]Kashino, K. et al., “A Quick Search Method for Audio and Video\nSignals Based on Histogram Pruning,” IEEE Trans. Multimedia ,\nVol.5,No.3, pp.348-357, 2003."
    },
    {
        "title": "MusicSun: A New Approach to Artist Recommendation.",
        "author": [
            "Elias Pampalk",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417487",
        "url": "https://doi.org/10.5281/zenodo.1417487",
        "ee": "https://zenodo.org/records/1417487/files/PampalkG07.pdf",
        "abstract": "MusicSun is a graphical user interface to discover artists. Artists are recommended based on one or more artists selected by the user. The recommendations are computed by combining 3 different aspects of similarity. The users can change the impact of each of these aspects. In addition words are displayed which describe the artists selected by the user. The user can select one of these words to focus the search on a specific direction. In this paper we present the techniques used to compute the recommendations and the graphical user interface. Furthermore, we present the results of an evaluation with 33 users. We asked them, for example, to judge the usefulness of the different interface components and the quality of the recommendations. 1 INTRODUCTION Popular music recommendation services include Amazon’s personalized recommendation lists, personalized Internet radio (e.g. Last.fm and Pandora), and services that make music blogs more accessible (e.g. the Hype Machine). 1 In contrast to most existing recommendation services MusicSun uses neither collaborative filtering nor manually annotated data. Instead we use techniques to automatically extract information from audio and web pages. MusicSun is basically a query-by-example interface. A user selects one or more artists and is given a list of similar artists. In addition, MusicSun offers the users several options to modify the recommendations. In particular, the users can (1) choose which vocabulary the interface uses to describe artists, (2) select a word from a list of words summarizing the query which they consider most relevant to their search, and (3) choose which aspects of similarity they are most interested in (the options are: audiobased sound similarity, web-based sociocultural similarity, or similarity with respect to the selected word). Related work in terms of discovering music includes a number of very different approaches. One example is the query-by-example interface (for songs) presented in [1] where an “aha” slider can be used to filter results from the same genre and obtain more interesting recommendations. Related work also includes work on visualizing music collections, enabling the user to easily browse and discover new music, e.g., [5, 7, 8, 9, 10, 12, 15, 16]. In terms of discovering music a notable approach is the work 1 http://last.fm, http://pandora.com, http://hypem.com c⃝2007 Austrian Computer Society (OCG). presented in [3] which efficiently combines web services to give high quality music recommendations. MusicSun is closely related to the MusicRainbow user interface [14] with respect to the techniques used and the focus on discovering new artists. However, MusicSun gives the users more options to focus their search and also requires them to make more choices. 2 TECHNIQUES The MusicSun interface is generated using audio (tracks) and the associated artist names as input. Using the artist names we crawl the web (using Google), and parse the retrieved pages using 4 different vocabularies. Using this data extracted from the web we compute two of the three aspects of similarity we use and summarize individual artists and groups of artists with words. Using the tracks and audio similarity techniques we compute the third similarity aspect. In this section we briefly describe these techniques and how we combine them. Finally, we list some example recommendations.",
        "zenodo_id": 1417487,
        "dblp_key": "conf/ismir/PampalkG07",
        "keywords": [
            "MusicSun",
            "graphical user interface",
            "discover artists",
            "recommendations",
            "combining 3 different aspects of similarity",
            "users can change the impact of each aspect",
            "users can select one of these words",
            "evaluate with 33 users",
            "ask them to judge the usefulness",
            "quality of the recommendations"
        ],
        "content": "MUSICSUN: A NEW APPROACH TO ARTIST RECOMMENDATION\nElias Pampalk and Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST)\nIT, AIST, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\nABSTRACT\nMusicSun is a graphical user interface to discover artists.\nArtists are recommended based on one or more artists se-\nlected by the user. The recommendations are computed by\ncombining 3 different aspects of similarity. The users can\nchange the impact of each of these aspects. In addition\nwords are displayed which describe the artists selected by\nthe user. The user can select one of these words to focus\nthe search on a speciﬁc direction.\nIn this paper we present the techniques used to com-\npute the recommendations and the graphical user inter-\nface. Furthermore, we present the results of an evaluation\nwith 33 users. We asked them, for example, to judge the\nusefulness of the different interface components and the\nquality of the recommendations.\n1 INTRODUCTION\nPopular music recommendation services include Amazon’s\npersonalized recommendation lists, personalized Internet\nradio (e.g. Last.fm and Pandora), and services that make\nmusic blogs more accessible (e.g. the Hype Machine).1\nIn contrast to most existing recommendation services Music-\nSun uses neither collaborative ﬁltering nor manually an-\nnotated data. Instead we use techniques to automatically\nextract information from audio and web pages.\nMusicSun is basically a query-by-example interface. A\nuser selects one or more artists and is given a list of sim-\nilar artists. In addition, MusicSun offers the users several\noptions to modify the recommendations. In particular, the\nusers can (1) choose which vocabulary the interface uses\nto describe artists, (2) select a word from a list of words\nsummarizing the query which they consider most relevant\nto their search, and (3) choose which aspects of similar-\nity they are most interested in (the options are: audio-\nbased sound similarity, web-based sociocultural similar-\nity, or similarity with respect to the selected word).\nRelated work in terms of discovering music includes\na number of very different approaches. One example is\nthe query-by-example interface (for songs) presented in\n[1] where an “aha” slider can be used to ﬁlter results from\nthe same genre and obtain more interesting recommen-\ndations. Related work also includes work on visualizing\nmusic collections, enabling the user to easily browse and\ndiscover new music, e.g., [5, 7, 8, 9, 10, 12, 15, 16]. In\nterms of discovering music a notable approach is the work\n1http://last.fm, http://pandora.com, http://hypem.com\nc/circlecopyrt2007 Austrian Computer Society (OCG).presented in [3] which efﬁciently combines web services\nto give high quality music recommendations. MusicSun\nis closely related to the MusicRainbow user interface [14]\nwith respect to the techniques used and the focus on dis-\ncovering new artists. However, MusicSun gives the users\nmore options to focus their search and also requires them\nto make more choices.\n2 TECHNIQUES\nThe MusicSun interface is generated using audio (tracks)\nand the associated artist names as input. Using the artist\nnames we crawl the web (using Google), and parse the\nretrieved pages using 4 different vocabularies. Using this\ndata extracted from the web we compute two of the three\naspects of similarity we use and summarize individual artists\nand groups of artists with words. Using the tracks and au-\ndio similarity techniques we compute the third similarity\naspect. In this section we brieﬂy describe these techniques\nand how we combine them. Finally, we list some example\nrecommendations.\n2.1 Web-Crawling, Vocabularies, and Summarization\nTo extract “community information” from the web we use\nthe approach suggested in [18]. For each artist we query\nGoogle using the artist’s name and the terms “music” and\n“review” as constraints. We retrieve the top 50 ranked\npages per artist and parse them for any words they contain.\nWe parse the retrieved pages using 4 manually com-\npiled vocabularies containing words suitable to describe\nmusic (see Table 1). The vocabularies are based on our\nprevious work [14] where we used 3 vocabularies which\nimplied a hierarchy of concepts.\nFor each artist we count how often words from the vo-\ncabularies occur on the retrieved web pages and compute\nthet f id f weight for each word (see [13] for a more de-\ntailed description of the speciﬁc implementation we use).\nUsing these weights we compute similarities as described\nin the next section. In addition we use this information to\nselect words to summarize each artist. We apply the sum-\nmarization technique suggested in [6]. The basic idea is\nto select words that not only occur frequently on the re-\nspective web pages, but also are suitable to distinguish the\ngiven set of artists from all other artists (for further details\nsee [13]).\nThere are mainly two limitations to this approach. First\nof all, we assume that artist names are unique identiﬁers.\nHowever, many artist have ambiguous names. Further-\nmore, the approach only works for artists which are men-\ntioned on a larger number of web pages.V ocabulary Examples Words\nGenre/styles rock, alternative rap, sunshine pop 255\nInstruments/types jazz guitar, female, orchestra 167\nMoods/adjectives smooth, angry, contemporary 452\nCountries/regions Afro-Cuban, Nashville, European 93\ntotal number of words: 967\nTable 1 . Description of the 4 vocabularies used.\n2.2 Similarity\nMusicSun’s recommendations are based on combining 3\nsimilarity aspects. By default these are weighted equally.\nHowever, their weight can be adjusted using sliders. The\nﬁrst aspect is sound similarity which we compute by ana-\nlyzing audio contents. The second aspect is sociocultural\nsimilarity which is computed by analyzing web pages. (The\nsociocultural similarity is high if the artist names occur on\nthe same web pages, or if the same words are used on\npages they occur on.) Third, we compute the similarity\nwith respect to the word the user has selected to focus the\nsearch in a speciﬁc direction.\nAudio-based similarity: The artist similarity is computed\nas suggested in [14] which is based on computing the sim-\nilarity of tracks as described in [11]. To compute the dis-\ntance between a set of artists (the user’s query) and artist X\n(a possible candidate for a recommendation), we compute\nthe average distance of X to all artists in the set.\nWeb-based similarity: As described above we compute\nthet f id f weighting. The distance between length normal-\nized t f id f vectors is computed using the Euclidean norm.\nTo compute the distance between a set of artists and artist\nX, we compute the average distance of X to all artists in\nthe set.\nWord-based similarity (“sun ray”): One of the main fea-\ntures of the user interface is the option to focus the search\nby selecting one of 9 words that summarize the query\nartists. For example, when searching for artists similar\nto ABBA the user can choose to focus the search, for ex-\nample, on “pop” or “Swedish”. (One of the 9 options is al-\nways randomly chosen by default.) The similarity with re-\nspect to the selected word is computed based on the t f id f\nweighting. For a given word, the most relevant artist is\nthe one with the highest t f id f weighting. (Thus the word-\nbased similarity is independent of the query artists.)\nCombined similarity: First, given the query we compute\nthe similarity of every recommendation candidate accord-\ning to each of the three similarity aspects. Second, we\naggregate the similarity ranks by computing the weighted\naverage of the rank of each artist. Alternatively, this could\nbe done by ﬁrst normalizing the computed similarities and\nby combining them linearly (see e.g. [2, 12]). The main\nadvantage of using the ranks is that no normalization is\nneeded.\n2.3 Recommendation Examples\nTable 2 shows some example recommendations. For each\nof the 4 examples one of the 9 words summarizing the\nartist is selected (and shown to the right of the artist’sMadonna →Pop\n1 Britney Spears\n2 Lisa Stansﬁeld\n3 George Michael\n4 Whitney Houston\n5 Maria CareyMadonna →Singing\n1 Whitney Houston\n2 Lisa Stansﬁeld\n3 Maria Carey\n4 Britney Spears\n5 Macy Gray\nEminem→Controversial\n1 Death Row\n2 Black Eyed Peas\n3 The Streets\n4 Dr. Dre\n5 Ice-TGilberto Gil →Political\n1 Caetano Veloso\n2 Chico Buarque\n3 Caetano Veloso &\nGilberto Gil\n4 David Byrne\n5 Jorge Ben\nTable 2 . MusicSun example recommendations.\nname). The similarity weights were set to their default\nvalues (all weights equal). The top 5 recommendations\nare shown. For example, Jorge Ben is the ﬁfth recom-\nmendation when using Gilberto Gil as query and setting\nthe focus on “political”.2Changing the focus (e.g. in\nthe case of Madonna from “pop” to “singing”) does not\ncompletely change the results because the audio and web-\nbased part of the similarity computation (2/3 of the weight\nin the default settings) still produce the same results.\n3 USER INTERFACE\nThe components of the MusicSun interface are shown in\nFigure 1. A demonstration video is available online.3The\nuser starts by entering an artist name in the search box.\nWhen the user activates the search box a list slides in from\nthe left. After ﬁnding an artist the user drags it into the sun\n(a circle in the center of the screen), the search box slides\naway, and the main elements of the interface slide back\ninto position and are displayed as shown in the Figure 1.\nAs soon as one or more artists are located in the sun\ntherays are labeled with words to summarize the query\nartists. The triangular shape of each ray encodes the fol-\nlowing information with respect to the word it represents:\nIf the side of the triangle facing the sun is longer, then the\nrespective word describes the artists better. If the length\nof the ray is longer, then there are more artists in the col-\nlection which can also be described using the respective\nword. One of the rays is randomly preselected by the sys-\ntem. The user can select a different ray by clicking on it.\nOnce a ray is selected, it spins itself into the rightmost po-\nsition, indicating that it is currently being used to modify\nthe recommendations.\nThe user can listen to artists by clicking on their names.\nA second click plays the next song from the same artist.\nFrom each song only a 20 second excerpt is played which\nis selected by the RefraiD chorus detection function [4].\nThe currently playing song is displayed in the upper left\narea together with simple playback controls.\nThe user can drag artists from the recommendation list\ninto the sun, or drag them to the area surrounding the sun\n2Jorge Ben is often described as less political than all his contem-\nporaries. This is an example where a negation loses its meaning when\nfocusing only on individual words.\n3http://pampalk.at/musicsun/1 - Search box to enter (partial) artist names\nvia the keyboard. A search window\nslides in from the left when activated.\n2 - Basic audio playback controls (stop &\nskip song). Playback starts by clicking\non an artist.\n3 - Sliders controlling the weights on each\nsimilarity aspect.\n4 - On/off switches for the vocabularies. In\nthe screenshot all switches are on.\n5 - Mouse over help text is display here.\nThis information is particularly\nimportant for (3) and (4).\n6 - Query artists are placed inside the sun.\n7 - The user can choose one of 9 rays. In\nthe screenshot “rap” is selected.\n8 - Storage area: the users can place artists\nhere which they want to remember or\nkeep out of the recommendation list.\n9 - List of recommendations\n10 - History buttons (undo/redo actions)\n11 - Previous/next page of recommendations\nFigure 1 . Screenshot of the MusicSun user interface with 50 Cent, Outkast, and Eminem selected as query.\n(“storage area”). This storage area can be used, for exam-\nple, to keep track of previously found artists. Artists can\nalso be dragged back into the recommendation list.\nThe two small circles displayed next to each artist in\nthe recommendation list encode information about the num-\nber of pieces used for the audio analysis (ﬁrst circle), and\nthe number of web pages found (second circle). An empty\ncircle indicates that the system did not have enough data,\nthus the recommendation is probably ﬂawed. A half empty\ncircle means that the results are probably questionable. A\nfull circle means that the amount of data used is about av-\nerage. In case a Google search for an artists yielded an\nextremely high number of pages the circle is shown in a\nblueish color. This might indicate either a very famous\nartist, or an ambiguous name.\nIn the recommendation list each artist is described with\nsome words. These words are computed in the same way\nas the rays. More relevant words are located on the left.\nIf the user changes the vocabularies, then the rays as well\nas these artist summaries are recomputed to contain only\nwords belonging to respective vocabularies. If the user\nchanges the slider settings the recommendation list is in-\nstantly recomputed.\n4 EVALUATION AND RESULTS\nTo evaluate the MusicSun interface we asked 33 volun-\nteers to try out the interface. 16 males and 17 females\nparticipated with an average age of 28.7 years (standard\ndeviation = 8.4). All participants used computers on a\ndaily basis. 27 participants had previously never used a\ntabled PC. 15 had musical training. 24 stated that they did\nnot spend any time in the last month actively searching for\nnew music. 10 primarily discovered artists through rec-\nommendations from friends. The second most frequently\nrecommendation source mentioned was radio followed byHard = Easy NA\nLearning to use MusicSun 4 1 28 0\nDiscovering artists 2 0 26 5\nLow = High NA\nQuality of the recommendations 3 2 27 1\nQuality of the artist summaries 4 3 16 10\nFun factor 1 1 30 1\nInterest in future usage 3 1 29 0\nUsefulness of optional components:\nsmall circles in recommendation list 13 4 11 5\nsimilarity sliders 9 4 19 1\ninformation encoded in ray shapes 8 2 20 3\nchoosing vocabularies 6 1 24 2\nTable 3 . Evaluation results: For all questions we used a 7-\npoint scale. Users who answered by choosing one of the\n3 points on the right or the left of the scale are grouped\ninto one category. The number of users who selected the\nexact center of the scale is marked with an equal sign. In\naddition, users had the option not to answer a question\nwhich is marked with “NA”.\nmovie soundtracks. All participants had a musical taste\nthat partially overlapped with the contents of the music\ncollection we used in the evaluation.\nThe participants were asked to use a tablet PC and pen\nas input device. The implemented interface was created\nbased on a collection of 999 artists from various genres.\nWhile trying out the interface we asked the users ques-\ntions which are summarized in Table 3. In average the\nparticipants spent about 20 minutes with the interface. In\nthe remainder of this section we summarize some of the\nqualitative ﬁndings.\nDescribing the unknown: For each artist in the recom-\nmendation list a few words summarizing the artist are dis-\nplayed right under the artist’s name. We asked the par-\nticipants to rate the quality of these summaries. 16 userssaid the quality was good. However most users ignored\nthese summaries when using the interface. 10 users were\nnot able to answer the question because they never looked\nat them. Instead of reading the summaries they searched\nfor artist names they were familiar with. In the vicinity\nof known artists they searched for unknown artists. In ad-\ndition, an important factor in choosing which of the un-\nknown artists to listen to was the interestingness of the\nrespective name.\nUsefulness of reliability indicators: The small circles next\nto each artist in the recommendation list indicate how re-\nliable the recommendations are. Overall, the users con-\nsidered this information least useful. Most users did not\nnotice them until we pointed them out in the interviews.\nHowever, a few users mentioned that they found it useful\nto know if a speciﬁc artist is very famous (which corre-\nlates with the number of web-pages found), and how many\npieces per artists were in the database. Only in one case a\nuser noticed that the circles actually served their purpose\n(to explain a failure in the recommendations). In this par-\nticular case the artist’s name was “Chess” and the circle\nindicating the number of web pages was blue.\nSimilarity weight sliders: Most users tried out the slid-\ners brieﬂy and then ignored them. One user found the\nresults in the recommendation lists better when the focus\nwas on web-based similarity. Another user said the same\nfor audio-based similarity. However, generally it was not\nobvious to the users what the differences in the rankings\nwere when focusing on either one of the two.\nThe word based slider was extensively used by three\nusers to focus their search on, for example, Italian music\n(this was done by increasing the weight of the word sim-\nilarity slider and selecting the respective ray). However,\nreliably extracting country information from unstructured\nweb pages is difﬁcult. For example, a frequent occurrence\nof the word Italy could also mean that the artist held a\nconcert in Italy.\nVocabulary chooser: In contrast to the similarity sliders\nthe users found the consequences of choosing a different\nvocabulary set easy to understand. Some users focused\nonly on country and region names, others only on genres\nand styles.\nRequested features: A few users asked to be able to select\nmore than one ray at once and to be able to select rays\npermanently (one of the biggest problems with the inter-\nface is that the rays are recomputed when the artists inside\nthe sun change, sometimes this means that a previously\nselected word disappears).\n5 CONCLUSIONS\nWe presented a new approach to artist recommendation\nwhich combines information extracted from web pages\nwith information extracted from audio. We built a new\nquery-by-example(s) user interface which allows the user\nto control several recommendation parameters. We con-\nducted an evaluation with 33 users. From the feedback\nwe conclude that there are two main directions for further\nimprovements of the user interface: ﬁnding better ways tolink unknown artists with known artists, and enabling the\nuser to select more than one word to focus the search on.\nAcknowledgments:\nThis work was supported by CrestMuse, Crest, JST.\nREFERENCES\n[1] J.-J. Aucouturier & F. Pachet, “Music similarity measures:\nWhat’s the use?” in ISMIR , 2002.\n[2] S. Baumann, T. Pohle, & V . Shankar, “Towards a socio-\ncultural compatibility of MIR systems,” in ISMIR , 2004.\n[3] O. Celma, M. Ram ´ırez, & P. Herrera, “Foaﬁng the music:\nA music recommendation system based on RSS feeds and\nuser preferences,” in ISMIR , 2005.\n[4] M. Goto, “A chorus-section detection method for musical\naudio signals and its application to a music listening sta-\ntion”, in IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , V ol.14, No.5, pp.1783-1794, 2006.\n[5] P. Knees, M. Schedl, T. Pohle, & G. Widmer, “An innova-\ntive three-dimensional user interface for exploring music\ncollections enriched with meta-information from the web,”\ninProc of ACM Intl Conf on Multimedia , 2006.\n[6] K. Lagus & S. Kaski, “Keyword selection method for char-\nacterizing text document maps,” in Proc of Intl Conf on\nArtiﬁcial Neural Networks , 1999.\n[7] P. Lamere, “Search inside the music,” Sun Microsystems\nLaboratories, Tech. Rep., 2006,\nhttp://blogs.sun.com/plamere/resource/sitm twopager.pdf.\n[8] F. M ¨orchen, A. Ultsch, M. N ¨ocker, & C. Stamm,\n“Databionic visualization of music collections according\nto perceptual distance,” in ISMIR , 2005.\n[9] R. Neumayer, M. Dittenbach, & A. Rauber, “PlaySOM\nand PocketSOMPlayer, alternative interfaces to large mu-\nsic collections,” in ISMIR , 2005.\n[10] E. Pampalk, “Islands of Music: Analysis, organization, and\nvisualization of music archives,” MSc thesis, Vienna Uni-\nversity of Technology, 2001.\n[11] E. Pampalk, “Computational models of music similarity\nand their application in music information retrieval,” PhD\nthesis, Vienna University of Technology, 2006.\n[12] E. Pampalk, S. Dixon, & G. Widmer, “Exploring music\ncollections by browsing different views,” in ISMIR , 2003.\n[13] E. Pampalk, A. Flexer, & G. Widmer, “Hierarchical orga-\nnization and description of music collections at the artist\nlevel,” in Proc of European Conference on Research and\nAdvanced Technology for Digital Libraries , 2005.\n[14] E. Pampalk & M. Goto, “Musicrainbow: A new user in-\nterface to discover artists using audio-based similarity and\nweb-based labeling,” in ISMIR , 2006.\n[15] I. Stavness, J. Gluck, L. Vilhan, & S. Fels, “The MUSIC-\ntable: A map-based ubiquitous system for social interac-\ntion with a digital music collection,” in Intl Conf on Enter-\ntainment Computing , 2005.\n[16] M. Torrens, P. Hertzog, & J.-L. Arcos, “Visualizing and\nexploring personal music libraries,” in ISMIR , 2004.\n[17] R. van Gulik, F. Vignoli, & H. van de Wetering, “Mapping\nmusic in the palm of your hand, explore and discover your\ncollection,” in ISMIR , 2004.\n[18] B. Whitman & S. Lawrence, “Inferring descriptions and\nsimilarity for music from community metadata,” in Proc of\nIntl Computer Music Conf , 2002."
    },
    {
        "title": "Combining Temporal and Spectral Features in HMM-Based Drum Transcription.",
        "author": [
            "Jouni Paulus",
            "Anssi Klapuri"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417257",
        "url": "https://doi.org/10.5281/zenodo.1417257",
        "ee": "https://zenodo.org/records/1417257/files/PaulusK07.pdf",
        "abstract": "To date several methods for transcribing drums from polyphonic music have been published. Majority of the features used in the transcription systems are “spectral”: parameterising some property of the signal spectrum in a relatively short time frames. It has been shown that utilising narrow-band features describing long-term temporal evolution in conjunction with the more traditional features can improve the overall performance in speech recognition. We investigate similar utilisation of temporal features in addition to the HMM baseline. The effect of the proposed extension is evaluated with simulations on acoustic data, and the results suggest that temporal features do improve the result slightly. Demonstrational signals of the transcription results are available at http://www.cs.tut.fi/sgn/arg/paulus/demo/. 1 1 INTRODUCTION Systems for automatic transcription of music have gained a considerable amount of research effort during the last few years. From the point of view of music information retrieval, these can be considered as tools for rising from the acoustic signal to a higher level of abstraction that correlates better with the content of interest. Here we focus on the transcription of drums: locating and recognising sound events created by drum instruments in music. Several methods have been proposed for drum transcription. Some of them are based on locating the onsets of prominent sound events, extracting a set of features from the locations of the onsets, and classifying the events using the features. Systems of this category include the method by Tanghe et al. [11] using support vector machines (SVMs) as classifiers, and a system using template adaptation and iterative musical pattern based error correction by Yoshii et al. [13]. As an extension to the systems relying only on acoustic data, Gillet and Richard have proposed a multi-modal system which uses also visual information of the drummer playing [4]. A system using hidden Markov models (HMMs) was presented in [8]. 1 This work was supported by the Academy of Finland, project No. 5213462 (Finnish centre of Excellence program 2006 2011). c⃝2007 Austrian Computer Society (OCG). Frequency Time Classifier Classifier Conventional features TRAPS Figure 1. The basic idea of temporal features illustrated. Instead of short wide-band frames of data, features are calculated from long narrow-band frames. (After [6].) In polyphonic music the presence of other instruments makes the transcription more difficult, since they are effectively noise for drum transcription systems. An alternative to the previous approaches is to try to separate the drums from polyphonic music, or to separate each drum to its own stream. The separation can be done blindly without any prior templates for the drums, as is done by Dittmar and Uhle [2], or by using a dictionary for different drums [9]. Several methods, both blind and dictionarybased, developed by FitzGerald et al. are detailed in [3]. For a more description of the earlier methods refer to [3]. Majority of the features used in the recognisers are “spectral”: parameterising some property of the signal spectrum in a relatively short (e.g., 10 ms) time frames. Features describing the temporal evolution of the signal are usually limited to the first temporal derivatives of spectral features, and in essence they still are short-time features. Some systems hand the responsibility of modelling the temporal evolution of the features over to an HMM architecture: different states describe different time instants of the modelled event. Hermansky and Sharma proposed an alternative for this in [6] in the form of using TRAPS (TempoRAl PatternS) features describing energy evolution at different subbands. The main idea behind TRAPS is illustrated in Figure 1. They showed that utilising the information of how the energy evolves on several subbands in one-second frames it was possible to improve the performance of a baseline speech recogniser which used only short-time cepstral features. Features from subband envelopes have been used earlier also in other music related applications, such as muinput signal features features spectral TRAPS TRAPS GMMs observation observation likelihoods likelihoods GMMs drum HMMs transition probabilities decoding model sequence proposed extension Figure 2. Block diagram of the full system with the proposed extension circled with dashed line. sical piece structure analysis [10], genre classification [7], and automatic record reviews [12]. However, the features used in these works were concentrated on the modulations of the envelopes whereas we are interested in certain events: the drum hits. We propose to utilise the information from temporal features in addition to the earlier HMM-based system [8], and show that they do increase the performance. Temporal features suit for drums, because drums are usually short events and do not have any “stable” state as e.g. harmonic sounds may have. The baseline HMM system is described in Section 2.1. The added temporal features are detailed in Section 2.2. Methods for combining the information from temporal features to the baseline system are described in Section 2.3. The performance of the resulting system is evaluated with simulations described in Section 3. Finally, conclusions are given in Section 4. 2 PROPOSED METHOD The block diagram of the system including the proposed extension is illustrated in Figure 2. The baseline system extracts a set of spectral features from the input signal and estimates observation likelihoods for all HMM states using Gaussian mixture models (GMMs). Finally, the transcription is obtained by finding out the best state and model sequence to explain the observed features. The extension adopts the idea from [6] and assumes that temporal features can provide information which can correct some of the errors made by the baseline system. The information provided by the proposed extension is added to the baseline system in the observation likelihood stage before decoding.",
        "zenodo_id": 1417257,
        "dblp_key": "conf/ismir/PaulusK07",
        "keywords": [
            "drums",
            "transcription",
            "polyphonic music",
            "HMM baseline",
            "temporal features",
            "speech recognition",
            "conventional features",
            "spectral features",
            "energy evolution",
            "genre classification"
        ],
        "content": "COMBINING TEMPORAL AND SPECTRAL FEATURES IN HMM-BASED\nDRUM TRANSCRIPTION\nJouni Paulus, Anssi Klapuri\nInstitute of Signal Processing\nTampere University of Technology\nABSTRACT\nTo date several methods for transcribing drums from poly-\nphonic music have been published. Majority of the fea-\ntures used in the transcription systems are “spectral”: pa-\nrameterising some property of the signal spectrum in a rel-\natively short time frames. It has been shown that utilising\nnarrow-band features describing long-term temporal evo-\nlution in conjunction with the more traditional features\ncan improve the overall performance in speech recogni-\ntion. We investigate similar utilisation of temporal fea-\ntures in addition to the HMM baseline. The effect of\nthe proposed extension is evaluated with simulations on\nacoustic data, and the results suggest that temporal fea-\ntures do improve the result slightly. Demonstrational sig-\nnals of the transcription results are available at\nhttp://www.cs.tut.ﬁ/sgn/arg/paulus/demo/.1\n1 INTRODUCTION\nSystems for automatic transcription of music have gained\na considerable amount of research effort during the last\nfew years. From the point of view of music information\nretrieval, these can be considered as tools for rising from\nthe acoustic signal to a higher level of abstraction that cor -\nrelates better with the content of interest. Here we focus\non the transcription of drums: locating and recognising\nsound events created by drum instruments in music.\nSeveral methods have been proposed for drum tran-\nscription. Some of them are based on locating the on-\nsets of prominent sound events, extracting a set of fea-\ntures from the locations of the onsets, and classifying the\nevents using the features. Systems of this category include\nthe method by Tanghe et al. [11] using support vector\nmachines (SVMs) as classiﬁers, and a system using tem-\nplate adaptation and iterative musical pattern based error\ncorrection by Yoshii et al. [13]. As an extension to the\nsystems relying only on acoustic data, Gillet and Richard\nhave proposed a multi-modal system which uses also vi-\nsual information of the drummer playing [4]. A system us-\ning hidden Markov models (HMMs) was presented in [8].\n1This work was supported by the Academy of Finland, project No .\n5213462 (Finnish centre of Excellence program 2006 - 2011).\nc/circlecopyrt2007 Austrian Computer Society (OCG).\nFrequency\nTime\nClassiﬁer\nClassiﬁerConventional\nfeatures\nTRAPS\nFigure 1 . The basic idea of temporal features illustrated.\nInstead of short wide-band frames of data, features are cal-\nculated from long narrow-band frames. (After [6].)\nIn polyphonic music the presence of other instruments\nmakes the transcription more difﬁcult, since they are ef-\nfectively noise for drum transcription systems. An alter-\nnative to the previous approaches is to try to separate the\ndrums from polyphonic music, or to separate each drum\nto its own stream. The separation can be done blindly\nwithout any prior templates for the drums, as is done by\nDittmar and Uhle [2], or by using a dictionary for different\ndrums [9]. Several methods, both blind and dictionary-\nbased, developed by FitzGerald et al. are detailed in [3].\nFor a more description of the earlier methods refer to [3].\nMajority of the features used in the recognisers are “spec-\ntral”: parameterising some property of the signal spec-\ntrum in a relatively short (e.g., 10 ms) time frames. Fea-\ntures describing the temporal evolution of the signal are\nusually limited to the ﬁrst temporal derivatives of spec-\ntral features, and in essence they still are short-time fea-\ntures. Some systems hand the responsibility of modelling\nthe temporal evolution of the features over to an HMM ar-\nchitecture: different states describe different time inst ants\nof the modelled event. Hermansky and Sharma proposed\nan alternative for this in [6] in the form of using TRAPS\n(TempoRAl PatternS) features describing energy evolu-\ntion at different subbands. The main idea behind TRAPS\nis illustrated in Figure 1. They showed that utilising the in -\nformation of how the energy evolves on several subbands\nin one-second frames it was possible to improve the per-\nformance of a baseline speech recogniser which used only\nshort-time cepstral features.\nFeatures from subband envelopes have been used ear-\nlier also in other music related applications, such as mu-input signalfeatures\nfeaturesspectral\nTRAPSTRAPS\nGMMsobservation\nobservationlikelihoods\nlikelihoodsGMMsdrum\nHMMstransition probabilities\ndecoding\nmodel sequence\nproposed extension\nFigure 2 . Block diagram of the full system with the pro-\nposed extension circled with dashed line.\nsical piece structure analysis [10], genre classiﬁcation [ 7],\nand automatic record reviews [12]. However, the features\nused in these works were concentrated on the modula-\ntions of the envelopes whereas we are interested in certain\nevents: the drum hits.\nWe propose to utilise the information from temporal\nfeatures in addition to the earlier HMM-based system [8],\nand show that they do increase the performance. Temporal\nfeatures suit for drums, because drums are usually short\nevents and do not have any “stable” state as e.g. harmonic\nsounds may have. The baseline HMM system is described\nin Section 2.1. The added temporal features are detailed in\nSection 2.2. Methods for combining the information from\ntemporal features to the baseline system are described in\nSection 2.3. The performance of the resulting system is\nevaluated with simulations described in Section 3. Finally ,\nconclusions are given in Section 4.\n2 PROPOSED METHOD\nThe block diagram of the system including the proposed\nextension is illustrated in Figure 2. The baseline system\nextracts a set of spectral features from the input signal and\nestimates observation likelihoods for all HMM states us-\ning Gaussian mixture models (GMMs). Finally, the tran-\nscription is obtained by ﬁnding out the best state and model\nsequence to explain the observed features. The extension\nadopts the idea from [6] and assumes that temporal fea-\ntures can provide information which can correct some of\nthe errors made by the baseline system. The information\nprovided by the proposed extension is added to the base-\nline system in the observation likelihood stage before de-\ncoding.\n2.1 Baseline HMM Recogniser\nThe baseline HMM system is the one published earlier\nin [8]. Each combination of the target drums is modelled\nwith a HMM and one HMM serves as a background model\nfor the situation when none of the target drums is playing.\nThese models are combined into a network whose idea is\nillustrated in Figure 3. At each time frame the system is\ncomb 1 comb N silence\nFigure 3 . The idea of the used HMM model network con-\nsisting of drum combinations (“comb 1” and “comb N”)\nand the silence model.\nin one state of one of the combination models. After the\nsystem exits a combination model it may enter another\ncombination or the background model. In the recognition\nphase, the best path through the models is searched using\ntoken passing algorithm [14].\nWhen handling polyphonic music signals, the input is\npassed through a sinusoids+residual -modelling. The mod-\nelled sinusoids are subtracted from the original signal and\nthe residual is regarded as the input signal for the fur-\nther processing. It is assumed that some components of\npitched instruments are modelled with the sinusoids and\ntheir contribution is reduced in the residual. As most of\nthe sound generated by the drums is stochastic, the mod-\nelling does not affect them much. Then the signal is di-\nvided into 23.2 ms frames with 50% overlap and the fol-\nlowing set of features is extracted: 13 mel-frequency cep-\nstral coefﬁcients and their ﬁrst order time differences, en -\nergy of the signal, spectral centroid, kurtosis, spread, sl ope,\nﬂatness, and roll-off, and log-energy from 8 octave-spaced\nbands.\nThe used HMM architecture uses four states for all of\nthe drum combinations and one state for the background\nmodel. The feature distributions in the states are modelled\nwith GMMs with two components in states belonging to\nthe drum models and 10 components in the background\nmodel. State transitions are allowed only to the state itsel f\nand to the next state.\n2.2 Temporal Features\nHermansky et al. used one-second frames of subband en-\nergy envelopes sampled at 100 Hz as the input to a multi-\nlayer perceptron (MLP) classiﬁer. Here we use only the\nmain idea of temporal features and divert from the original\nmethod. Even though the features we use are not exactly\nthe same as the ones in the original TRAPS publication,\nwe still use the term TRAPS to refer to them. [6]\nThe input signal is passed through a bank of 1/3-octave\nbandpass ﬁlters, and the following processing is applied\nto each of the resulting subband signals. The envelope\nof a subband signal is calculated by squaring and low-\npass ﬁltering with 80 Hz cutoff frequency. In the low-\nest bands where the bandwidth is less than 80 Hz, the\nlow-pass cutoff is lowered to match the bandwidth. The\nenvelope signals are sampled at 400 Hz and µ-law com-pression ( ˆx= log(1 + µx)/log(1 + µ)) is applied with\nµ= 100 . Finally, temporal difference is calculated. The\nmotivation for applying compression and differentiation i s\nto detect perceptually signiﬁcant level changes in the sub-\nband signal. The result of this processing are the bandwise\nenvelopes bi(t)from which the temporal features are cal-\nculated.\nThe actual temporal features from the envelopes are\ncalculated by diving them into 100 ms frames with 50%\noverlap, and applying hamming windowing. Then a time-\nshift invariant representation of the envelope within each\nframe is desired, meaning that the position of a drum event\nshould not have an effect on the extracted features. This is\nachieved by calculating the discrete Fourier transform and\nretaining only the magnitude spectrum. The information\nabout the location of the event within the frame is now\ndiscarded along with the phase spectrum. The magnitude\nspectrum is converted to a cepstrum-like format by µ-law\ncompressing it with µ= 1000 , applying discrete cosine\ntransform (DCT), and discarding a majority of the coefﬁ-\ncients. It was empirically noted that discarding the zeroth\ncoefﬁcient and retaining the following 5 produced a suit-\nable parameterisation. The compression is used to reduce\nthe large dynamic scale on the magnitude values before\nreducing the dimensionality and correlation by DCT.\nAthineos et al. parameterised the envelopes by frequency-\ndomain linear prediction in [1]. The parameterisation was\nefﬁcient, but for drum transcription application is has one\nmajor drawback: the resulting features were sensitive to\nthe absolute location of the event within the frame.\n2.3 Combining the Spectral and Temporal Features\nCombining the information from temporal features to the\nbaseline HMM recogniser can be done in several ways.\nHermansky et al. used a combining MLP having the out-\nputs of the bandwise MLPs as its input to yield the ﬁnal\nrecognition result [6].\nThe easiest way to utilise the TRAPS features would\nbe to concatenate the features from all bands to the feature\nvector used in the baseline recogniser. This approach has\ntwo major problems, however: explosion of the dimen-\nsionality of the feature vector and the highly correlated\nnature of the TRAPS features.\nInstead of using the TRAPS features as such one could\nfollow the example of Hermansky et al. [6] and train a de-\ntector classiﬁer producing posterior probabilities for ea ch\ntarget drum and for each subband. These bandwise pos-\nterior probabilities can be interpreted as features and con -\ncatenated to the HMM feature vectors. However, the HMM\nuses GMMs to model the features and the distribution of\nthe posterior probabilities does not ﬁt the model well. As\na result, this approach does not produce a good result.\nThe solution we propose concatenates the temporal fea-\ntures from all bands into one feature vector and trains just\none Bayesian GMM classiﬁer for each target drum instead\nof an own classiﬁer of each band. The feature vectors are\nsubjected to PCA retaining 90% of the variance prior to\ntraining GMMs from them. In experiments it was notedthat a relatively small amount of components sufﬁces in\nthe modelling: two components for modelling the pres-\nence of the target drum, and three for modelling the ab-\nsence of the target drum. For a target drum dthe GMMs\nproduce a posterior probability p(d)of the drum to be\npresent in the frame.\nAs the temporal features are modelled as detectors for\nthe target drums and the HMMs are for combinations of\nthe drums, the posteriors for different individual drums\nhave to be combined to one posterior for the combination.\nMaking the assumption that the drums are independent of\neach other, the probability of the drum combination Ccan\nbe approximated by\np(C) =/productdisplay\nd∈Cp(d)/productdisplay\nd/∈C(1−p(d)). (1)\nThe resulting probability for this combination p(C)is then\nadded to the observation probabilities of all the states of\nthe combination model by multiplying the probabilities\nbefore ﬁnding the optimal path through the models in de-\ncoding.\n3 EV ALUATIONS\nThe performance of the proposed system was evaluated\nwith simulations on acoustic data. The target drums were\nkick drum, snare drum, and hi-hat, resulting in eight com-\nbinations to be modelled including the background model.\nThe test material is divided into three subsets: “simple\ndrums” consisting of simple patterns, such as 8-beat and\nshufﬂe, performed mainly with the target drums, “com-\nplex drums” containing more complex patterns and also\nnon-target drums, and “RWC Pop” consisting of 45 pieces\nfrom RWC Popular music database [5]. The signals con-\nsisting only of drums were recorded using three different\ndrum sets and three different recording environments. The\nrecorded signals were processed with equalisation and mult i-\nband compression. The length of the drums-only material\nclips was restricted to 30 seconds, while 60 second clips\nwere taken from the RWC songs. The setup is described\nin more detail in [9].\nA transcribed event was judged to be correct if it de-\nviated less than 30 ms from the event in ground truth an-\nnotations. The used performance metrics consist of pre-\ncision rate, P, (ratio of correctly transcribed events to all\ntranscribed events), recall rate, R, (ratio of correctly tran-\nscribed events to all events in ground truth), and harmonic\nF-measure, F= 2RP/(P+R). For each material set the\nevaluations were run in 3-fold cross validation scheme:\n2/3 of the pieces used as training material and testing with\nthe remaining 1/3. The presented results are calculated\nover all folds.\nTo get perspective to the performance of the system,\nthe system from [11] is used as a reference. It classiﬁes\nevents found by onset detector using a SVM, hence it is\nreferred in the result tables as “SVM”.2. The reference\n2The used implementation was kindly provided by the MAMI con-\nsortiumhttp://www.ipem.ugent.be/MAMI/ .F-measure (%) simple complex RWC\ndrums drums Pop\nbaseline HMM 93.4 84.0 66.8\nHMM+TRAPS 92.9 85.2 69.7\nSVM[11] 85.5 76.4 65.1\nTable 1 . Total average F-measures of different methods\nand different material sets. The presented results are cal-\nculated over all three target drums.\nmethod was not trained for the material used in the eval-\nuations, but instead the provided models were used. The\noverall results of the evaluations are given in Table 1.\nDetailed results for the HMM-based systems are given\nin Table 2, where F-measure, precision, and recall rates\nare given for all three target drums for both the baseline\nsystem and the proposed extension with temporal features.\nIt can be seen that the proposed utilisation of temporal\nfeatures increases the performance slightly. Some demon-\nstrational signals from the simulations are available at\nhttp://www.cs.tut.ﬁ/sgn/arg/paulus/demo/.\nThe results for both the baseline and the reference sys-\ntem presented in Table 1 differ slightly from those re-\nported in [8]. This is because some corrections were made\nto the ground truth annotations and longer signal excerpts\nwere used in the evaluations.\n4 CONCLUSIONS AND FUTURE WORK\nWe have proposed to utilise temporal features in conjunc-\ntion with a HMM-based system for transcribing drums\nfrom polyphonic audio. This was shown to result in slight\nimprovement in transcription accuracy, which is consis-\ntent with the results obtained by Hermansky et al. [6]. It\nwas also noted that the proposed addition changed the type\nof the errors from insertions to deletions, which are less\ndisturbing when listening to the synthesised transcriptio n\nresult.\nThe proposed system can be easily used as a baseline\nsystem and extended by incorporating musicological mod-\nels. Such a model could be a regular N-gram model, a pe-\nriodic N-gram, or a model making decision based on both\nthe past and the future [13]. It would be preferable for\nthe system to be able to adapt to the target signals instead\nof using ﬁxed models. This could be accomplished by us-\ning the models created with the proposed method as initial\nmodels and adapting them based in the input signal.\n5 REFERENCES\n[1] M. Athineos and D. P. W. Ellis. Frequency-domain\nlinear prediction for temporal features. In ASRU, St.\nThomas, U.S. Virgin Islands, USA, 2003.\n[2] C. Dittmar and C. Uhle. Further steps towards drum\ntranscription of polyphonic music. In 116th AES Con-\nvention, Berlin, Germany, 2004.material metric kick drum snare drum hi-hat\nsimple P(%) 99.2 (92.4) 99.7 (98.2) 96.1 (94.9)\ndrums R(%) 93.7 (98.2) 89.2 (89.4) 87.5 (90.9)\nF(%) 96.4 (95.2) 94.2 (93.6) 91.6 (92.8)\ncomplex P(%) 94.4 (87.4) 86.5 (78.5) 85.9 (82.8)\ndrums R(%) 97.3 (97.6) 76.5 (81.1) 74.4 (78.2)\nF(%) 95.8 (92.2) 81.2 (79.8) 79.8 (80.4)\nRWC P(%) 82.8 (73.1) 70.3 (52.7) 78.6 (74.1)\nPop R(%) 76.5 (78.7) 61.3 (66.6) 56.8 (58.6)\nF(%) 79.5 (76.8) 65.5 (58.8) 66.0 (65.4)\nTable 2 . Detailed results for the HMM methods. Baseline\nresults are given in parentheses.\n[3] D. FitzGerald and J. Paulus. Unpitched percussion\ntranscription. In A. Klapuri and M. Davy, editors,\nSignal Processing Methods for Music Transcription .\nSpringer, 2006.\n[4] O. Gillet and G. Richard. Automatic transcription\nof drum sequences using audiovisual features. In\nICASSP, Philadelphia, PA, USA, 2005.\n[5] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz mu-\nsic databases. In ISMIR, Paris, France, 2002.\n[6] H. Hermansky and S. Sharma. TRAPS - classiﬁers of\ntemporal patterns. In ICSLP, Sydney, Australia, 1998.\n[7] M. F. McKinney and J. Breebaart. Features for audio\nand music classiﬁcation. In ISMIR, Baltimore, Mary-\nland, USA, 2003.\n[8] J. Paulus. Acoustic modelling of drum sounds with\nhidden Markov models for music transcription. In\nICASSP, Toulouse, France, 2006.\n[9] J. Paulus and T. Virtanen. Drum transcription with\nnon-negative spectrogram factorisation. In EUSIPCO,\nAntalya, Turkey, 2005.\n[10] G. Peeters, A. La Burthe, and X. Rodet. Toward au-\ntomatic music audio summary generation from signal\nanalysis. In ISMIR, Paris, France, 2002.\n[11] K. Tanghe, S. Dengroeve, and B. De Baets. An algo-\nrithm for detecting and labeling drum events in poly-\nphonic music. In MIREX, London, UK, 2005. ex-\ntended abstract.\n[12] B. Whitman and D. P. W. Ellis. Automatic record re-\nviews. In ISMIR, Barcelona, Spain, 2004.\n[13] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkuno. An error correction framework based on drum\npattern periodicity for improving drum sound detec-\ntion. In ICASSP, Toulouse, France, 2006.\n[14] S. J. Young, N. H. Russell, and J. H. S. Thornton. To-\nken passing: a simple conceptual model for connected\nspeech recognition systems. Tech Report CUED/F-\nINFENG/TR38, Cambridge, UK, 1989."
    },
    {
        "title": "A Probabilistic Framework for Matching Music Representations.",
        "author": [
            "Paul H. Peeling",
            "Ali Taylan Cemgil",
            "Simon J. Godsill"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417677",
        "url": "https://doi.org/10.5281/zenodo.1417677",
        "ee": "https://zenodo.org/records/1417677/files/PeelingCG07.pdf",
        "abstract": "In this paper we introduce a probabilistic framework for matching different music representations (score, MIDI, audio) by incorporating models of how one musical representation might be rendered from another. We propose a dynamical hidden Markov model for the score pointer as a prior, and two observation models, the first based on matching spectrogram data to a trained template, the second detecting damped sinusoids within a frame of audio by subspace methods. The resulting Bayesian framework is robust to local variations in tempo, and can be used for a wide variety of applications. We evaluate both methods in a score alignment context by inferring the posterior distribution of the current position in the score exactly. The spectrogram method is shown to infer the score position reliably with minimal computation, and the damped sinusoid model is able to pinpoint the positions of score events in the audio with a high level of timing accuracy. 1 INTRODUCTION Musical information is roughly represented in one of three ways: a score, which is a symbolic representation, a MIDI file, which represents discrete musical events with more precise timing information, and sampled audio, which is the most faithful representation of the sound produced. There are many applications for which we would like to match a number of pieces of music with different representations together. For example, score alignment [14, 10, 9] is the matching of a score representation to the audio representation of the same music. Often in practice, this problem can be reformulated as matching a MIDI representation to audio, assuming the MIDI is quantized to discrete positions and accurately represents the score. In all these applications, the underlying factor which is responsible for causing mismatches between different representations is an unknown tempo process. For example, in the score alignment problem, the tempo of a MIDI representation evolves independently from that of the audio, hence dynamic time warping (DTW) strategies have been popular [19, 4, 11]. Audio synchronization, where two audio representations with different tempi are matched, can c⃝2007 Austrian Computer Society (OCG). also be treated by these strategies [15]. Dynamic time warping (DTW) schemes rely on minimizing an explicit matching function by dynamic programming and may encounter difficulties when unexpected events occur, which are not captured in the matching criteria, for example, mistakes made by a player when performing a piece from a score, repeats made in a concert but not during rehearsals, improvisation sections, pauses and reruns, and so on. A complete probabilistic model for music representation enables inclusion of such types of events as a priori information and facilitates learning from data, hence potentially a more robust matching performance can be obtained. Moreover, modern and powerful inference techniques can be developed in cases where the model size becomes large so as not to admit exact computation. In this paper we introduce a probabilistic framework which will allow us to match different music representations in a Bayesian setting. We begin by considering the fundamental representation of music as the score, and construct a prior model of how this representation evolves in time during a performance. One such approach has been developed by Raphael [18], where a probabilistic dynamical model is applied to the tempo of the audio, with the expected timing of events based on the score in a score alignment context. Here we consider the evolution of the position of a ‘score position pointer’ through time, adopting an approach similar to that of [8, 2, 20]. We define the ‘score pointer’ as an unobserved random variable over score positions evolving according to unknown velocities (tempi). This model differs from previous approaches in the way the tempi are represented. In Section 2 we describe this dynamical model for the score pointer, formulated as a hidden Markov model [17]. Given the formulation it is conceptually straightforward to develop online, offline and fixed-lag applications using standard exact or approximate inference methodology. In the Bayesian setting, we also require an observation model which assigns a likelihood value to observed data from a different music representation given the current state of the score position pointer. In Section 3 we present two such probabilistic models for the generation of music audio from a score, or where practically more appropriate, MIDI. Our approach in this paper will be to constrain our models to allow for exact inference of the posterior dis-",
        "zenodo_id": 1417677,
        "dblp_key": "conf/ismir/PeelingCG07",
        "keywords": [
            "probabilistic",
            "framework",
            "matching",
            "music",
            "representations",
            "score",
            "MIDI",
            "audio",
            "tempo",
            "process"
        ],
        "content": "APROBABILISTICFRAMEWORK FORMATCHING MUSIC\nREPRESENTATIONS\nPaul Peeling A. TaylanCemgil Simon Godsill\nSignalProcessing and CommunicationsLaboratory,Departm entofEngineering,\nCambridgeUniversity,TrumpingtonStreet, CambridgeCB2 1 PZ, UnitedKingdom\n{php23,atc27,sjg }@eng.cam.ac.uk\nABSTRACT\nIn this paper we introduce a probabilistic framework for\nmatching different music representations (score, MIDI,\naudio) by incorporating models of how one musical rep-\nresentation might be rendered from another. We propose\na dynamical hidden Markov model for the score pointer\nas a prior, and two observation models, the ﬁrst based on\nmatching spectrogramdata to a trained template, the sec-\nond detecting damped sinusoids within a frame of audio\nby subspace methods. The resulting Bayesian framework\nis robust to local variationsin tempo, and can be used for\na wide varietyof applications. We evaluate both methods\ninascorealignmentcontextbyinferringtheposteriordis-\ntribution of the current position in the score exactly. The\nspectrogram method is shown to infer the score position\nreliablywith minimalcomputation,andthe dampedsinu-\nsoidmodelisabletopinpointthepositionsofscoreevents\nintheaudiowitha highleveloftimingaccuracy.\n1 INTRODUCTION\nMusicalinformationisroughlyrepresentedinoneofthree\nways: ascore,whichisasymbolicrepresentation,aMIDI\nﬁle, which represents discrete musical events with more\nprecise timing information, and sampled audio, which is\nthe most faithful representation of the sound produced.\nThere are many applications for which we would like to\nmatch a number of pieces of music with different repre-\nsentationstogether. Forexample,scorealignment[14,10,\n9] is the matching of a score representation to the audio\nrepresentation of the same music. Often in practice, this\nproblem can be reformulated as matching a MIDI repre-\nsentationtoaudio,assumingtheMIDIisquantizedtodis-\ncretepositionsandaccuratelyrepresentsthe score.\nInalltheseapplications,theunderlyingfactorwhichis\nresponsibleforcausingmismatchesbetweendifferentrep-\nresentations is an unknown tempo process. For example,\ninthescorealignmentproblem,thetempoofaMIDIrep-\nresentation evolves independentlyfrom that of the audio,\nhencedynamictimewarping(DTW)strategieshavebeen\npopular[19,4,11]. Audiosynchronization,wheretwoau-\ndio representationswith different tempi are matched, can\nc∝circlecopyrt2007AustrianComputerSociety(OCG).also betreatedbythese strategies[15].\nDynamic time warping (DTW) schemes rely on min-\nimizing an explicit matching function by dynamic pro-\ngrammingandmayencounterdifﬁcultieswhenunexpected\nevents occur, which are not captured in the matching cri-\nteria, for example, mistakes made by a player when per-\nforming a piece from a score, repeats made in a concert\nbut not during rehearsals, improvisation sections, pauses\nand reruns, and so on. A complete probabilistic model\nfor music representation enables inclusion of such types\nof events as a prioriinformation and facilitates learning\nfrom data, hence potentially a more robust matching per-\nformancecanbeobtained. Moreover,modernandpower-\nful inference techniquescan be developedin cases where\nthe model size becomes large so as not to admit exact\ncomputation.\nIn this paper we introduce a probabilistic framework\nwhich will allow us to match different music represen-\ntations in a Bayesian setting. We begin by considering\nthe fundamentalrepresentationofmusic as the score, and\nconstructapriormodelofhowthisrepresentationevolves\nin time during a performance. One such approach has\nbeendevelopedbyRaphael[18],whereaprobabilisticdy-\nnamical model is applied to the tempo of the audio, with\ntheexpectedtimingofeventsbasedonthescoreinascore\nalignment context. Here we consider the evolution of the\npositionofa ‘scorepositionpointer’throughtime, adopt-\ning an approach similar to that of [8, 2, 20]. We deﬁne\nthe‘scorepointer’asanunobservedrandomvariableover\nscore positions evolvingaccordingto unknownvelocities\n(tempi). This model differs from previous approaches in\nthe way the tempi are represented. In Section 2 we de-\nscribe this dynamical model for the score pointer, formu-\nlated as a hidden Markov model [17]. Given the formu-\nlationitisconceptuallystraightforwardtodeveloponlin e,\nofﬂine and ﬁxed-lag applications using standard exact or\napproximateinferencemethodology.\nIntheBayesian setting,wealso requireanobservation\nmodel which assigns a likelihood value to observed data\nfrom a different music representation given the current\nstateofthescorepositionpointer. InSection3wepresent\ntwo suchprobabilisticmodelsforthegenerationofmusic\naudiofromascore,orwherepracticallymoreappropriate,\nMIDI.Our approachin thispaperwill be to constrainour\nmodels to allow for exact inference of the posterior dis- \n6/noteheads.s2/noteheads.s27 8 1 2 5/noteheads.s2/noteheads.s2 /clefs.G/timesig.C44/noteheads.s23 4\nFigure1. State spaceofthescorepositionpointer rk.\ntributions, algorithms for which are provided in Section\n4. In futureworkwe will relaxthese constraintsfor more\nelaborateandrealisticmodels,thusrequiringapproximat e\ninference techniques. such as sequential Monte Carlo [5]\nor variational Bayes [12]. In Section 5 we demonstrate\nhow to apply the Bayesian models to the score alignment\nproblem,andcomparethetwoobservationmodelsonreal\npolyphonicpianoaudioextracts.\n2 SCOREPOINTER DYNAMICS\nWedeﬁnethescorepointer rk∈[1,2, . . ., R ]astheposi-\ntionin a musicalscore at time k,measuredasthe number\nof eighth notes, sixteenth notes etc. from the beginning\nof the score. For example,in the simple score in Figure.1\nwe have R= 8and the unit is an eighth note, the ﬁnest\nscore resolution. We represent tempo tk∈Timplicitly\nbytheprobability π(tk)thatthescorepointer rkmovesto\nthe nextstate. Roughly,whenthe tempois fast (slow) the\nprobabilityto moveto the next positionis higher(lower).\nThisleadstothefollowingsimpledynamics:\np(rk|rk−1, tk) =\n\nπ(tk)ifrk=n+ 1,rk−1=n\n1−π(tk)ifrk=n,rk−1=n\n0 otherwise\np(r1|t1) = 1/R∀n (1)\nwhere Ris the overall length of the score. The uniform\nprior (1) allows the performance to begin at any position\nin the score, which is useful for practical applications.\nClearly, one can allow for moreelaborate score transition\nstructuressuchasrepeats,improvisationsectionsandmis -\ntakes.\nThe number of frames for a score pointer transition to\ntake place is a random variable with a geometric distri-\nbution with probabilityof success π(tk). The variance of\nthis distribution is (1−π(tk))/π(tk)2which in practice\nis largeenoughto accountforsubstantial deviationsfrom\npredictedscoretransitions,includingtheperformanceha lt-\ning for some periodof time. Hence we only need to con-\nsiderasmallselectionofcoarsediscreteset oftempoval-\nues such as tk∈ {‘fast’, ‘medium’, ‘slow’ }to account\nfora widerangeofperformanceconditions.3 FREQUENCY DOMAINOBSERVATION\nMODELS OFAUDIO\nInthissection,wedescribetwomodelstoextractfrequency -\ndomainfeaturesfromaudioframes.\n3.1 GenerativeSpectrogramModel\nHere we introduce a model for generating two-element\nvectors corresponding to the real and imaginary parts of\nthe complex values sνreturned by the discrete Fourier\ntransform (DFT) in frequency bins ν= 1, . . . , W, based\nonthecurrentscoreposition rkandascalingparameter λk\nwhichrepresentstheoverallenergyinthesignalattime k.\nSee [13] for an existing approach to a Bayesian model of\nthespectrogram. Inthesequel,weomitthetimeindexfor\nsimplicity,pleaserefertothegraphicalmodelinFigure3.\nWe do this via latent scale parameters vνwhich describe\ntheenergyineachfrequencybin,asfollows1\np(vν|r, λ) =IG(vν;a/2,2/(λσν(r)a))(2)\np(sν|vν) =N(sν; 0, vνI) (3)\nThegainparameter λscalesaspectrogramtemplate σνto\nmatch sν, from which the scale parameters vνare drawn\naccordingto(2)with‘tightness’ a. Thespectrogramtem-\nplateshaveunitenergy,i.e.\nW/summationdisplay\nν=1σν= 1 (4)\nSee Figure 2 for an example, where high energy regions\ncorrespondtothefundamentalandpartialsofthenotebe-\ning played. The spectrogram values are then drawn from\na bivariate Gaussian (3) with covariance vνIwhich is in-\nvariantto phase.\nOnesimplemethodoflinkingframesofaudiotogether\nis shown in Figure 3. Realistically the energy in frame\nkwill increase if there is a note onset in the score tran-\nsition, hence we could add further dependencies λk∼\np(λk|λk−1, rk, rk−1), resulting in a changepoint model.\nSee[3]foraexampleofinferenceonchangepointmodels\nin a music transcription setting. Further links could also\nbe addedbetween the parameters vνin Figure3 to reﬂect\nthat the energyin a frequencybin vν,kdependson vν,k−1\ninthepreviousframeandsomedampingcoefﬁcient ρ. For\nthispaperhowever,Figure3representsamodelforwhich\nit will be possible to exactly infer the performance vari-\nablesrk, tkprovidedwe have observedthe energy λkfor\neach frame k= 1, . . ., K. We can estimate λkfrom the\ntotal energyinthefrequencybinsforframe k,i.e.\n1Deﬁnitions of probability distributions used in this paper\nIG(x;α, β) =(1/β)α\nΓ(α)x−α−1exp(−1\nβx)\nN(x;µ,Σ) =1\n|2πΣ|1/2exp„\n−1\n2(x−µ)TΣ−1(x−µ)«0500 1000 1500 2000 2500 3000 3500 4000−12−10−8−6−4−20\nFrequency ν / Hzlog σν\nFigure 2. Spectrogram template σνof a piano playing\nmiddle C (261.6 Hertz) with a sampling frequency of\n8000Hzanda framelengthof400samples(50ms)\nν= 1, . . ., Wt1 t2 . . . tK\nr1 r2 . . . rK\nλ1 λ2. . . λK\nvν,1 vν,2 . . . vν,K\nsν,1 sν,2 . . . sν,K\nFigure 3. Generative spectrogram model. A directed arc\nbetween nodes denotes that the second variable is condi-\ntionallydependentontheﬁrst.λk=W/summationdisplay\nν=1sT\nν,ksν,k (5)\nAs this is only an estimate, it would be possible to treat\nλkasanunobservedvariable,andusethespectrogramen-\nergy(5)asaproposalinanapproximateinferencescheme.\nHere,aswewillbeperformingexactinference,itwillalso\nbe usefulto integrateoutthe latentscale parameters νkin\n(2)and(3)resultingin aformofStudent’s t-distribution\np(sν|r, λ) =/integraldisplay\np(sν|vν)p(vν|r, λ)dvν\n=Γ((a+D)/2)\n(πaλσ ν(r))D/2Γ(a/2)/parenleftbigg\n1 +1\nasT\nνsν\nλσν(r)/parenrightbigg−(a+D)\n2\n=Ta(sν; 0, λσν(r))\nInpractice,wecantrainthespectrogramtemplatefora\nnote by taking the spectrogram of a training sample, nor-\nmalizing each frame so that (4) holds, and computingthe\nmeanvaluein eachbinacrossframes.\n3.2 DampedSinusoidal Model\nSubspacemethods[1]allowhighfrequencyresolutiones-\ntimatesof dampedsinusoidspresentin a signal,byﬁtting\na parameterizedmodeloftheform\nx(t) =M/summationdisplay\nm=1ame−ρmtcos(2πωm+φm)\nHence for a frame of audio, we obtain estimates of the\nfrequencies ωm, amplitudes am, phases φmand damping\ncoefﬁcients ρmfora modelof Msinusoids.\nIn this paper we will deﬁne a model based on the ob-\nserved frequency values ωmand amplitudes αm. Damp-\ningcoefﬁcients ρmcouldpotentiallybeusedtomodelin-\nstrument timbre, but will not be considered here. Given\nthe score position rfor the frame, we assume that the\nfrequency and amplitude values are drawn independently\nfroma distribution p(ωm, αm|r), i.e.\np(ω1:M, α1:M|r) =M/productdisplay\nm=1p(ωm, αm|r)\nFigure4 showsthefrequencyandamplitudedatafrom\nthe ESPRIT subspace method, together with a Gaussian\nmixture model (GMM) trained on the data. The parame-\ntersoftheGMMaredeterminedbymaximumaposteriori\n(MAP) estimation, with priors placed on the covariance\nof each Gaussian component so that the harmonic struc-\nture of the subspace data is captured. A uniform‘clutter’\ncomponentisincludedtoaccountforspuriousdetections.\nTo accountfor differentnote volumes,the amplitudedata44.5 55.5 66.5 77.5 8−3.5−3−2.5−2−1.5−1−0.50\nLog FrequencyLog Amplitude\nFigure 4. Subspace frequency-amplitudedata of a piano\nplaying middle C with a sampling frequency of 8000Hz,\na framelength of 200 samples and M= 4sinusoids.\nThe Gaussian mixture model trained on this data is indi-\ncated bythe positionsof the meansandcontoursofequal\nprobability. ThecovariancepriorsarediagonalandGaus-\nsian:σf∼ N(σf; 10−2,10−4)isthelogfrequencyprior,\nσα∼ N(σα; 100,10−1)isthelogamplitudeprior.\nin each frameis scaled so that the maximumsinusoid log\namplitudedetectedina frameis zero.\nA similar model, where the number of sinusoids de-\ntectedisPoissonratherthanﬁxedat Mhasbeenappliedto\npolyphonicmusictranscriptionin[16],andisamathemat-\nicallysoundmethodforavoidingdataassociationbetween\npartialsofa musicalnoteandthedetectedsinusoids.\n4 INFERENCE\nInourBayesianframework,matchingconsistsofinferring\nthe unknown score position rkat time k, integrating out\nthe tempo value tk. Typically we may wish to determine\nthe most likely score position at time k, given past ob-\nservations p(rk|y1:k), whichisknownasﬁlteringandcan\nbe carried out recursively online, or including all future\nobservations p(rk|y1:K), which is known as smoothing\nand must be carried out ofﬂine, or including some recent\nobservations p(rk|y1:k+N), which is known as ﬁxed-lag\nsmoothing and is practical if a certain amount of latency\nintheinferenceisacceptable. Wemayalsowishtopredict\nfuture values of the score position p(rk+N|y1:k)or infer\nthemostlikelyprogressionofscorepositions p(r1:K|y1:K)\nwhichis knownasthe Viterbipathandismost suitable to\nofﬂine matching. The computationsrequiredfor all these\nrelatedbutdistinctqueriescanbeviewedintermsofmes-\nsagepassingalgorithms,andwillbedescribedinthissec-\ntion.\nThe observations are yk={s1:W,k, λk}for the spec-\ntrogram model, and yk={ω1:M, α1:M}for the damped\nsinusoidmodel.\nBy Bayes’ theorem, the posterior distribution over theunknownvariables H1:K≡ {r1:K, t1:K}isgivenby\np(H1:K|y1:K) =p(y1:K|H1:K)p(H1:K)\np(y1:K)\nThe marginal ﬁltering density p(Hk|y1:k)can be com-\nputed by passing αk|k≡p(Hk|y1:k)p(y1:k)‘alpha’ mes-\nsagesbetweenneighbouringframes:\nα0|0=p(H0)\nαk|k−1=/summationdisplay\nHk−1p(Hk|Hk−1)αk−1|k−1\nαk|k=p(yk|Hk)αk|k−1\nWe then obtain the desired density up to a normalizing\nconstantbyintegratingovertempovalues tk∈T\np(rk|y1:k) =/summationdisplay\ntk∈Tp(rk, tk|y1:k)∝/summationdisplay\ntk∈Tαk|k\nThe marginal smoothing density p(Hk|y1:K)is com-\nputedofﬂinebypassing βk|k≡p(yk+1:K|Hk)‘beta’mes-\nsagesasfollows:\nβK|K+1= 1\nβk|k=p(yk|Hk)βk|k+1\nβk−1|k=/summationdisplay\nHkp(Hk|Hk−1)βk|k\np(Hk|y1:K)∝αk|kβk|k+1\nThe Viterbi path is computedin an analogousmanner,\nwhere messages from neighbouring frames and observa-\ntions are combined by taking the maximum rather than\nsumming,i.e.:\nαk|k−1= max\nHk−1p(Hk|Hk−1)αk−1|k−1\nβk−1|k= max\nHkp(Hk|Hk−1)βk|k\nargmax\nH1:Kp(H1:K|y1:K) = argmax\nk=1:Kαk|kβk|k+1\n5 APPLICATIONS\nThe matching framework introduced in this paper is able\ntoaddressawiderangeofknownapplicationsinmusicin-\nformation retrieval. We have chosen here to demonstrate\nscore alignment using the observation models discussed\nabove. Theaimofscorealignmentistoinferthescorepo-\nsition in an audio extract. For purposes of evaluation we\nsampleanaccurateMIDItranscriptionofthescoreovera\nﬁne set of times, and train our observation models based\non the pitch content of the MIDI. Our training data was\nobtained separately from test data by summing piano au-\ndio samples (RWC-MDB-I-2001 No. 01) for each pitch, \n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.0/accidentals.0/noteheads.s2/noteheads.s2/rests.0/accidentals.0 /noteheads.s2/noteheads.s2/noteheads.s2/accidentals.2/accidentals.0/noteheads.s2\n/noteheads.s2/noteheads.s2/rests.3 /rests.3/noteheads.s2/timesig.C44/accidentals.M2/accidentals.M2/accidentals.M2 /noteheads.s2 /clefs.G/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2 /noteheads.s2/accidentals.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/accidentals.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/accidentals.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.2 /noteheads.s2\n/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/accidentals.0/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/rests.4/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2 /accidentals.0/accidentals.0 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/accidentals.M2 /accidentals.M2/accidentals.M2/accidentals.M2/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n4\nFigure5. ScoreofanextractfromBach’sWell-Tempered\nKlavier: Book 1 Fugue 2 in C minor. Audio source:\nDaniel-BenPienaar\nSpectrogram Data\nTime / sFrequency / Hz\n0 2 4 6 8 10 12 1401000200030004000\n5010015020025030035040045055606570758085MIDI Data\nScore positionMIDI note\nFigure 6. Viterbi-path score matching using the spectro-\ngram model, a= 10000. 25 ms frames. Evenly spaced\nvertical bars in the spectrogram correspond to the score\npositions marked on the MIDI data. The variation in\nthe spacing of the score positions illustrates the changing\ntempothroughtheextract.\ndownsampled to 8 kHz, from the RWC Musical Instru-\nment Sound Database [7, 6]. The data in Figures 2 and\n4 were obtained from these samples. We demonstrateof-\nﬂine score matching on the extract in Figure 5. The mp3\naudio was downsampled to 8 kHz and divided into non-\noverlapping frames of 20ms length. Note that although\nin the score extract there are only two parts playing at a\ngiventime,whenwesampletheMIDIuptofoursimulta-\nneous notes may be playing due neighboring notes over-\nlapping in time. This is typical in legatopiano playing\nand thus consideringthis overlapresults in a robust score\nalignment. A quantization model describing the render-\ningofMIDIfromscorewouldneedtotakethiseffectinto\naccount. The score pointer transition probabilities π(tk)\narechosenas {0.1,0.3,0.5}forthetempovalues {‘fast’,\n‘medium’,‘slow’ }respectively.\nFigures 6 and 7 are snapshots of the score alignment\nsystem, showing the position of the score pointer in time\nwith respect to the observation data for the spectrogram\nand damped sinusoid models respectively. Animationsof\ntheseﬁguresareavailableonourwebsite2,withtheaudio\n2http://www-sigproc.eng.cam.ac.uk/∼php23/\npublications/ISMIR/2 4 6 8 10 12 14456789Subspace Data\nTime / sLog Frequency / Hz\n5010015020025030035040045055606570758085MIDI Data\nScore positionMIDI note\nFigure 7. Viterbi-path score matching using the damped\nsinusoid model, M= 7. 7.5 ms frames. Only the fre-\nquencydatafromthesubspacedetectorisshown.\nsignal playing simultaneously, from which it is clear that\nscore pointer is correctly aligned with the audio signal.\nThe damped sinusoid model gives better time-accuracy\nthanthespectrogrammodel,althoughthiscomesat asig-\nniﬁcant computationaloverhead. Thisis becausethe sub-\nspace method gives high-resolution frequency estimates,\nwhilethespectrogrammethodreturnsfrequencyestimates\nindiscretebins,theresolutionofwhichworsenswithshort er\nframes.\n6 CONCLUSIONSAND FUTUREWORK\nIn this paper we have considered a musical performance\nas the evolution of a score pointer over time. We have\ndeﬁned a simple dynamical model that governsthe prob-\nability of the pointer transitioning from one position in\nthe score to another. We have introduced two models of\nfrequency-domainrepresentationsof musical audiogiven\nthe set of notes present at the current score position. The\nﬁrst method is a generativemodel of spectrogramvalues,\nwhich is computationally efﬁcient and simple to train on\nnote samples, but has the usual time-frequencyresolution\nlimitation associated with the spectrogram. The second\nmethod models the output of a subspace detector, which\nreturns the frequencies and amplitudes of a chosen num-\nber damped sinusoids in a frame. This method has bet-\nter frequency resolution over shorter frame lengths, but\niscomputationallymoreintensive. Wehavedemonstrated\nthesemodelsinascore-alignmentapplication,withpromis -\ningresultsevenforsimplemodelslinkingframestogether\nso thatexactinferenceis possible.\nBased on our results we suggest two possible applica-\ntions of interest to the music information retrieval com-\nmunity. The computation requirements of the generative\nspectrogram model are sufﬁciently low to expect that a\nreal-time score-following system would be feasible withthis model. The damped sinusoid model is capable of\nmatching a score to audio with high time precision. With\nthe training data and models used in this paper, we were\nable to detect note onsets to a resolution of 7.5ms. This\nmethod could potentially be used to automatically anno-\ntate databases of audio where the score is known, with a\nhigh level of accuracy. Such databases are invaluable to\nresearchers working on audio onset detection and music\ntranscriptionwishing to evaluatethe performanceof their\nmethodsagainstgroundtruth.\nWearealsocurrentlyinvestigatingotherinterestingap-\nplications that can be formulated in this framework such\nas audio synchronization, score-guided source separation\nortranscription.\n7 REFERENCES\n[1] R. Badeau, R. Boyer, and B. David. EDS parametric\nmodeling and tracking of audio signals. In Proceed-\nings of the 5th International Conference on Digital\nAudioEffects ,Hamburg,Germany,September2002.\n[2] A.T.Cemgil,H.J.Kappen,andD.Barber.Generative\nmodel basedpolyphonicmusic transcription.In IEEE\nWorkshoponApplicationsofSignalProcessingtoAu-\ndioandAcoustics ,New Paltz,NY, October2003.\n[3] A. T. Cemgil, H. J. Kappen, and D. Barber. A gen-\nerative model for music transcription. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n14(2):679–694,March2006.\n[4] S.Dixon.Livetrackingofmusicalperformancesusing\non–linetimewarping.In Proceedingsofthe8thInter-\nnationalConferenceonDigitalAudioEffects ,2005.\n[5] A. Doucet, J. F. G. de Freitas, and N. J. Gordon, ed-\nitors.Sequential Monte Carlo Methods in Practice .\nSpringer-Verlag,NewYork,2000.\n[6] M. Goto. Development of the RWC Music Database.\nInProceedingsof the 18th InternationalCongress on\nAcoustics , volume 1, pages 553–556, April 2004. In-\nvitedPaper.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Music Genre Database and\nMusical Instrument Sound Database. In Proceedings\nof the4thISMIR ,pages229–230,October2003.\n[8] L. Grubb. A Probabilistic Method for Tracking a\nVocalist. PhD thesis, School of Computer Science,\nCarnegieMellonUniversity,Pittsburgh,PA, 1998.\n[9] H. Heijink, P. Desain, H. Honing, and L. Windsor.\nMake me a match: An evaluation of different ap-\nproaches to score-performance matching. Computer\nMusic Journal ,24(1):43–56,2000.\n[10] T. Hoshishiba, S. Horiguchi, and I. Fujinaga. Study\nof expression and individuality in music performanceusingnormativedataderivedfromMIDIrecordingsof\npiano music. In 4th International Conference on Mu-\nsic PerceptionandCognition ,pages465–470,McGill\nUniversity,FacultyofMusic,Montreal,1996.\n[11] N. Hu, R. B. Dannenberg, and G. Tzanetakis. Poly-\nphonic audio matching and alignment for music re-\ntrieval. In IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics , pages 185–188,\nNewYork,USA, 2003.\n[12] M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K.\nSaul. An introduction to variational methods for\ngraphicalmodels. MachineLearning , 37(2):183–233,\n1999.\n[13] K. Kashino and S.J. Godsill. Bayesian estimation of\nsimultaneous musical notes based on frequency do-\nmain modelling. In Proceedings of the International\nConferenceon Acoustics,SpeechandSignalProcess-\ning,volume4,pages305–308,May2004.\n[14] E. W. Large. Dynamic programming for the analysis\nof serial behaviors. Behavior Research Methods, In-\nstruments,andComputers , 25(2):238–241,1993.\n[15] M Muller, H Mattes, and F Kurth. An efﬁcient multi-\nscale approach to audio synchronization. In Proceed-\ningsof the 6th InternationalConferenceon Music In-\nformationRetrieval ,Victoria,Canada,2006.\n[16] P. H. Peeling, C. Li, and S. J. Godsill. Poisson point\nprocess modeling for polyphonicmusic transcription.\nJournal of the Acoustical Society of America Express\nLetters, pagesEL168–EL175,April2007.\n[17] L. R. Rabiner. A tutorial on hidden markov models\nand selected applications in speech recognition. In\nProceedingsof the IEEE , volume 22, pages257–286,\nFebruary1989.\n[18] C. Raphael. A hybrid graphical model for aligning\npolyphonicaudiowithmusicalscores.In Proceedings\nofthe5thInternationalConferenceonMusicInforma-\ntionRetrieval ,Barcelona,Spain,2004.\n[19] F.Soulez,X.Rodet,andD.Schwarz.Improvingpoly-\nphonic and poly–instrumental music to score align-\nment. InProceedingsof the 4th InternationalConfer-\nenceonMusicInformationRetrieval ,Baltimore,MD,\n2003.\n[20] N.Whiteley,A.T.Cemgil,andS.J.Godsill.Bayesian\nmodelling of temporal structure in musical audio. In\nProceedings of the 7th International Conference on\nMusicInformationRetrieval ,Victoria,Canada,2006."
    },
    {
        "title": "Sequence Representation of Music Structure Using Higher-Order Similarity Matrix and Maximum-Likelihood Approach.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416748",
        "url": "https://doi.org/10.5281/zenodo.1416748",
        "ee": "https://zenodo.org/records/1416748/files/Peeters07.pdf",
        "abstract": "In this paper, we present a novel method for the automatic estimation of the structure of music tracks using a sequence representation. A set of timbre-related (MFCC and Spectral Contrast) and pitch-related (Pitch Class Profile) features are first extracted from the signal leading to three similarity matrices which are then combined. We then introduce the use of higher-order (2nd and 3rd order) similarity matrices in order to reinforce the diagonals corresponding to common repetitions and reduce the background noise. Segments are then detected and a maximum-likelihood approach is proposed in order to derive simultaneously the underlying sequence representation of the music track and the most representative segment of each sequence. The proposed method is evaluated positively on the MPEG-7 “melody repetition” test set. 1 INTRODUCTION Music structure discovery (MSD) aims at estimating automatically the structure of a music track by analyzing its audio signal. It has become a major topic of interest in the recent years because it allows the development of new paradigms: active music listening (intra-document browsing [4]), acoustic browsing of music catalogues (fast browsing using automatically generated audio summaries [20] or using automatically located chorus, key-phrase, audiothumbnail [15] [5] [3]), music creation (automatic segmentation into cognitively similar parts [12], music mosaicing), media compression [12] and automatic music analysis (understanding music structure through acoustic analysis). MSD algorithms always start by extracting a set of features from the audio signal. The features are then used to detect repetitions of the signal content over time. This notion of “repetition” and “detection of repetition” is the basis of all MSD algorithms developed so far. It is also their main limitation since it does not allow detecting variations or evolutions of a part 1 . The choice of the features therefore plays a central role since it guides the kind of repetitions that can be observed: repetitions can be based on instrument-background repetitions (timbre-related features are for example used by [8]), repetitions of melody or chord-succession (pitch-related used by [3]) or repetitions 1 Note however that [11] takes tonality modulation into account. c⃝2007 Austrian Computer Society (OCG). of rhythm patterns (rhythm-related used by [20] [13]). In the current work both timbre-related and pitch-related features are used. The temporal structure of a music track can then be visualized using a recurrence plot more often called a similarity matrix in the case of music [9] which represents the similarity between each pair of features over time. In order to extract from this visual representation, a numerical representation of the structure of a track, two kinds of representation can be used leading to two different approaches [19]: the state and the sequence representation. The “state” representation (see left part of Fig. 1) considers that a music track is a succession of parts called states and that each time of a music track has emitted a specific state. A state is defined as a set of contiguous times, which contains similar acoustical information. A state does not need to be repeated later in the track. The notion of states is closely related to the notion of parts in popular music (introduction, verse, chorus and bridge) because for popular music the musical background is often constant during the duration of each part. In this case, the goal of MSD algorithms is to find the states that have been emitted at each time. The algorithms rely mainly on segmentation (novelty measure of [8]), partitional, agglomerative or spectral clustering algorithms [15] [6] [1] or hidden Markov models ([15], [20]). The “sequence” representation (see right part of Fig. 1) considers that there exist sequences of time in the music track that are repeated over the track. A sequence is defined as a set of successive times, which is similar to another set of successive times. However the times inside a given sequence do not need to be necessarily identical to each other. All the times of a music track do not belong necessarily to a sequence. The notion of sequence is closely related to the notion of melody (sequence of notes) or chord succession in popular music. These sequences are visible in a similarity matrix through the diagonals, which represent succession of pairs of times with high similarity. The sequence approach allows a more precise description than the state approach, since it allows to detect only the parts which are repeated melodies and are therefore cognitively more memorable. When considering the sequence representation, most approaches only attempts to detect the most representative audio extract from the similarity matrix in order to create a thumbnail [3], [5]. Few papers address the problem of estimating the actual sequence representation from the similarity matrix. When dealing with this problem most authors use Dynamic Time Warping or pattern matching techniques [7] [2] [16] [11]. Recent approaches combine Figure 1. Structure representation in a similarity matrix as [left part] states: we observe three states noted A, C and D. The times noted A belong to the state A. Note that the state C is not repeated later in the track. [right part] sequences: we observe two sequences noted abc and de. Note that a sequence cannot exist if it is not repeated later in the track. DTW with a hierarchical approach of the structure detection [18] [17]. Despite its efficiency, the DTW approach remains very heavy in computation time. In this paper, we propose a fast method for the estimation of the sequence of a music track based on a maximum-likelihood approach (parts 2.4 and 2.5). Other contributions of this paper are the simultaneous use of timbre and harmonic-related features combined into a unique similarity matrix (part 2.1) and the use of higher-order similarity matrices (part 2.2). Finally part 3 presents the evaluation of our system on the MPEG7 “melody repetition” test set. 2 PROPOSED METHOD",
        "zenodo_id": 1416748,
        "dblp_key": "conf/ismir/Peeters07",
        "keywords": [
            "sequence representation",
            "state representation",
            "recurrence plot",
            "timbre-related features",
            "pitch-related features",
            "higher-order similarity matrices",
            "maximum-likelihood approach",
            "Dynamic Time Warping",
            "pattern matching techniques",
            "Dynamic Time Warping with hierarchical approach"
        ],
        "content": "SEQUENCE REPRESENTATION OF MUSIC STRUCTURE USING\nHIGHER-ORDER SIMILARITY MATRIX AND MAXIMUM-LIKELIHOOD\nAPPROACH\nGeoffroy Peeters\nIrcam Sound Analysis/Synthesis Team - CNRS STMS\n1, pl. Igor Stranvinsky - 75004 Paris - France\nABSTRACT\nIn this paper, we present a novel method for the automatic\nestimation of the structure of music tracks using a sequence\nrepresentation. A set of timbre-related (MFCC and Spec-\ntral Contrast) and pitch-related (Pitch Class Proﬁle) features\nare ﬁrst extracted from the signal leading to three simi-\nlarity matrices which are then combined. We then intro-\nduce the use of higher-order (2nd and 3rd order) similarity\nmatrices in order to reinforce the diagonals corresponding\nto common repetitions and reduce the background noise.\nSegments are then detected and a maximum-likelihood ap-\nproach is proposed in order to derive simultaneously the\nunderlying sequence representation of the music track and\nthe most representative segment of each sequence. The\nproposed method is evaluated positively on the MPEG-7\n“melody repetition” test set.\n1 INTRODUCTION\nMusic structure discovery (MSD) aims at estimating auto-\nmatically the structure of a music track by analyzing its\naudio signal. It has become a major topic of interest in\nthe recent years because it allows the development of new\nparadigms: active music listening (intra-document brows-\ning [4]), acoustic browsing of music catalogues (fast brows-\ning using automatically generated audio summaries [20]\nor using automatically located chorus, key-phrase, audio-\nthumbnail [15] [5] [3]), music creation (automatic segmen-\ntation into cognitively similar parts [12], music mosaicing),\nmedia compression [12] and automatic music analysis (un-\nderstanding music structure through acoustic analysis).\nMSD algorithms always start by extracting a set of fea-\ntures from the audio signal. The features are then used\nto detect repetitions of the signal content over time. This\nnotion of “repetition” and “detection of repetition” is the\nbasis of all MSD algorithms developed so far. It is also\ntheir main limitation since it does not allow detecting varia-\ntions or evolutions of a part1. The choice of the features\ntherefore plays a central role since it guides the kind of\nrepetitions that can be observed: repetitions can be based\non instrument-background repetitions (timbre-related fea-\ntures are for example used by [8]), repetitions of melody or\nchord-succession (pitch-related used by [3]) or repetitions\n1Note however that [11] takes tonality modulation into account.\nc/circlecopyrt2007 Austrian Computer Society (OCG).of rhythm patterns (rhythm-related used by [20] [13]). In\nthe current work both timbre-related and pitch-related fea-\ntures are used.\nThe temporal structure of a music track can then be vi-\nsualized using a recurrence plot more often called a simi-\nlarity matrix in the case of music [9] which represents the\nsimilarity between each pair of features over time. In or-\nder to extract from this visual representation, a numerical\nrepresentation of the structure of a track, two kinds of rep-\nresentation can be used leading to two different approaches\n[19]: the state and the sequence representation.\nThe“state” representation (see left part of Fig. 1) con-\nsiders that a music track is a succession of parts called states\nand that each time of a music track has emitted a speciﬁc\nstate. A state is deﬁned as a set of contiguous times, which\ncontains similar acoustical information. A state does not\nneed to be repeated later in the track. The notion of states is\nclosely related to the notion of parts in popular music (in-\ntroduction, verse, chorus and bridge) because for popular\nmusic the musical background is often constant during the\nduration of each part. In this case, the goal of MSD algo-\nrithms is to ﬁnd the states that have been emitted at each\ntime. The algorithms rely mainly on segmentation (nov-\nelty measure of [8]), partitional, agglomerative or spectral\nclustering algorithms [15] [6] [1] or hidden Markov models\n([15], [20]).\nThe“sequence” representation (see right part of Fig. 1)\nconsiders that there exist sequences of time in the music\ntrack that are repeated over the track. A sequence is deﬁned\nas a set of successive times, which is similar to another set\nof successive times. However the times inside a given se-\nquence do not need to be necessarily identical to each other.\nAll the times of a music track do not belong necessarily to\na sequence. The notion of sequence is closely related to the\nnotion of melody (sequence of notes) or chord succession in\npopular music. These sequences are visible in a similarity\nmatrix through the diagonals, which represent succession of\npairs of times with high similarity. The sequence approach\nallows a more precise description than the state approach,\nsince it allows to detect only the parts which are repeated\nmelodies and are therefore cognitively more memorable.\nWhen considering the sequence representation, most ap-\nproaches only attempts to detect the most representative\naudio extract from the similarity matrix in order to cre-\nate a thumbnail [3], [5]. Few papers address the problem\nof estimating the actual sequence representation from the\nsimilarity matrix. When dealing with this problem most\nauthors use Dynamic Time Warping or pattern matching\ntechniques [7] [2] [16] [11]. Recent approaches combineFigure 1 . Structure representation in a similarity matrix as\n[left part] states: we observe three states noted A, C and D.\nThe times noted A belong to the state A. Note that the state\nC is not repeated later in the track. [right part] sequences:\nwe observe two sequences noted abc and de. Note that a\nsequence cannot exist if it is not repeated later in the track.\nDTW with a hierarchical approach of the structure detec-\ntion [18] [17]. Despite its efﬁciency, the DTW approach\nremains very heavy in computation time. In this paper, we\npropose a fast method for the estimation of the sequence\nof a music track based on a maximum-likelihood approach\n(parts 2.4 and 2.5). Other contributions of this paper are the\nsimultaneous use of timbre and harmonic-related features\ncombined into a unique similarity matrix (part 2.1) and the\nuse of higher-order similarity matrices (part 2.2). Finally\npart 3 presents the evaluation of our system on the MPEG-\n7 “melody repetition” test set.\n2 PROPOSED METHOD\n2.1 Feature extraction and similarity matrix\nThe ﬁrst stage of our system extracts features from the au-\ndio signal. For the reasons mentioned above (the fact that\nrepetitions can be related either to timbre or pitch observa-\ntions), three set of audio features are extracted:\n•13 Mel Frequency Cepstral Coefﬁcients (excluding\nthe 0th/ DC-component coefﬁcient),\n•12 Spectral Contrast coefﬁcients [14] (spectral con-\ntrasts and spectral valley coefﬁcients into 6 frequency\nbands [0,sr\n26],[sr\n26,sr\n25], ...[sr\n22,sr\n2]2),\n•12 Pitch Class Proﬁle coefﬁcients [10].\nThe frame analysis is performed with a window length of\n80ms and a hop size of 40ms. Each dimension of the fea-\ntures is then modeled over time by its mean values over a\nsliding window of 4s with hop size of 500ms.\nPrincipal Component Analysis is then applied to the three\nfeature sets separately. For each set, only the principal com-\nponents explaining more than 10% of the total variance are\nkept. The data are then projected on the retained principal\ncomponents leading to the ﬁnal features.\nFrom the three modiﬁed feature sets, we compute sepa-\nrately three similarity matrices using an Euclidean distance.\nThe matrices are then normalized to the range [0,1]and\nadded. We note S(tx, ty)the resulting matrix.\n2srstands for sampling rate.\nFigure 2 . Computation of a [left] 2nd order similarity ma-\ntrix [right] 3rd order similarity matrix\n2.2 Higher order similarity matrix\nWe note o(tx)the feature vector extracted at time t=tx.\nThe similarity matrix S(tx, ty)represents the similarity be-\ntween two times txandtythrough the computation of the\ndistance between the feature vectors extracted at time tx\nandty. Iftzis a repetition of tx, we observe a high value\natS(tz, tx). In the same way, if tzis a repetition of ty,\nwe observe a high value at S(tz, ty). By transitivity, since\no(tz)/similarequalo(tx)ando(tz)/similarequalo(ty), we should have o(ty)/similarequal\no(tx)and observe a high value at S(ty, tx). However, in\npractice, because repetitions are not exact repetitions and\nbecause of the noise in the features, this repetition can be\nmasked. The higher order similarity matrix uses the transi-\ntivity property to recover those missing values and empha-\nsize the repetitions.\nWe deﬁne a second order similarity matrix S2(tx, ty)as\nthe similarity between times txandtythrough all the times\ntz(see left part of Fig. 2):\nS2(tx, ty) =Z\ntzS(tx, tz)S(tz, ty)dtz (1)\nS2(tx, ty)measures the similarity between txandtyusing\nthe fact that if txis similar to tz, andtztotythentxandty\nshould be similar.\nIn the same way, we can deﬁne a third order similarity\nmatrix as the similarity between time txandtythrough all\nthe times tz1,tz2(see right part of Fig. 2):\nS3(tx, ty) =Z\ntz1Z\ntz2S(tx, tz1)S(tz1, tz2)S(tz2, ty)dtz1dtz2\n(2)\nS3(tx, ty)measures the similarity between txandtyusing\nthe fact that if txis similar to tz1,tz1totz2andtz2toty\nthentxandtyshould be similar.\nIn Fig. 3, we represent the 1st, 2nd and 3rd order similar-\nity matrix for the track “She Loves You” from The Beatles.\nUsing the 2nd and 3rd order matrices, the repetitions, espe-\ncially at the beginning, become more visible.\n2.3 Sequence representation\nFrom the higher-order similarity matrix we derive the se-\nquence representation. This is done in two steps. We ﬁrst\ndetect in the matrix sets of diagonals from which we derive\na set of segments (part 2.4). We then estimate the sequence\nrepresentation that best explains the detected segments (part\n2.5). In the rest of this part, we will use the following terms:1st order similarity matrix\n100200300400500100\n200\n300\n400\n5000.511.522.533.54\n2nd order similarity matrix\n100200300400500100\n200\n300\n400\n5000.20.40.60.8\n3rd order similarity matrix\n100200300400500100\n200\n300\n400\n5000.20.40.60.8Figure 3 . From left to right: 1st, 2nd, 3rd order similarity matrix on “She Loves You” from The Beatles\ndiagonal (line): a diagonal (line) is deﬁned as a possibly\ndiscontinuous set of points in the similarity (lag) ma-\ntrix,\nsegment: a segment is a set of successive (continuous) times\ndeﬁned by a starting and ending time. A diagonal in\nthe matrix deﬁnes two segments: the original (projec-\ntion on the x-axis) and the repetition one (projection\non the y-axis).\nsequence: a sequence is a set of segments representing sim-\nilar information occurring at various times. A se-\nquence is deﬁned by a “mother” segment (the most\ntypical segment) and a set of times which indicate at\nwhich times the “mother” segment is instantiated,\nsequence representation: a sequence representation is de-\nﬁned by a set of sequences.\n2.4 Diagonals (lines) and segments detection\n2.4.1 Matrix ﬁltering\nIn order to reinforce the diagonal elements in the matrix\nwhile removing the non-diagonal elements, a ﬁlter is ap-\nplied to the matrix. The matrix S(ti, tj)is ﬁrst converted to\na lag-matrix [3] L(lij, tj)withlij=ti−tj. The lag ma-\ntrix transforms a diagonal repetition into a vertical constant-\nlag–line. The ﬁlter we use is the combination of a hori-\nzontal high-pass ﬁlter and a vertical low-pass ﬁlter. The\nhigh-pass ﬁlter is a gaussian kernel ﬁlter which is the com-\nbination of two opposed sign gaussian function: g(t) =\ngσ+(t)−gσ−(t)3. In the experiment of part 3, we will\nuse the following parameters: σ+= 0.3s.,σ−= 2s.The\nlow-pass ﬁlter is a simple averaging ﬁlter with length 8s.\n2.4.2 Segment detection\nThe segments are detected from the resulting ﬁltered high-\norder lag matrix using a method close to the one proposed\nby Goto [11]. Despite the fact that this method does not\nallow to detect repetitions of segments with time variations\n(accelerando, ritardando. . . ), it was chosen because it is fast\nand most of the time reliable. We refer the reader to [11]\nfor details about the method.\n3gσ(t) =1√\n2πσe−(t−µ)2\n2σ22.5 Sequence estimation using a maximum likelihood\napproach\nThe goal of the sequence representation is to represent all\nthe segments detected in the matrix using the smallest pos-\nsible set of sequences (mother segments and repetition times).\nIn [19], we have proposed a method for solving this prob-\nlem. The segments were ﬁrst connected, then for each set\nof connected segments a mother segment was chosen. In\nthis paper, we present a new approach, faster and more reli-\nable, which allows solving the problems using a maximum\nlikelihood approach. For each candidate mother segment,\nwe measure how well it would “explain” the observed seg-\nments. We deﬁne Sseg(ti, tj)a matrix with values set to 1\nwhen a segment exist at (ti, tj)and to 0otherwise The algo-\nrithm presented below is applied to Sseg(ti, tj). The term\n“explain” is expressed by a score inspired by the “summary\nscore” proposed by [5].\nProposed algorithm. We deﬁne mijas a candidate mother\nsegment starting at time τiand ending at time τj> τi(note\nthatmijdoes not need to correspond to an existing seg-\nment). The times τiandτjdeﬁne a row corridor in the\nmatrix (see Fig. 4[A]). The summation over the length of\nthe corridor (over all the columns of the matrix) is noted\nσ(τ) =TX\nt=1Sseg(τ, t) (3)\nThe summation over the width of the corridor (over the\nrows of the matrix deﬁned by the corridor)\nsij(t) =τjX\nτ=τiSseg(τ, t)∀t∈[1, T] (4)\nUsing this notation, the “summary score” [5] for a seg-\nment mijwould bePτj\nτ=τiσ(τ)but would be computed us-\ning the feature-similarity matrix and not the segment-similarity\nmatrix. Because of the use of Sseg,σ(τ)indicates the num-\nber of segments which are repetitions of a sequence existing\nat time τ.\nFirst condition: If one segment crosses the corridor\n[τi, τj]during an interval t= [tx, ty]thensij(t) = 1 ∀t∈\n[tx, ty](see Fig. 4[A]). If two segments cross simultane-\nously the corridor during an interval t= [tx, ty]thensij(t) =\n2∀t∈[tx, ty](see Fig. 4[B]). Therefore sij(t)provides\nan information about the number of simultaneous segments\noccurring during the interval t= [tx, ty]. Since two se-\nquences cannot occurred simultaneously (only one mothersegment can be instantiated at a given time), values of sij(t)\nlarger than 1 should be avoided and τiandτjadapted in or-\nder to achieve that (by reducing the width of the corridor).\nThe ﬁrst condition is then: sij(t)≤1∀tandτiandτj\nshould be modiﬁed in order to fullﬁll that.\nSegmentation: When sij(t)≤1∀t, applying a simple\nthreshold ( sij(t)>0) allows to detect automatically the\nvarious segment occurrences of the mother segment mij.\nWe note [tk\nx, tk\ny]k∈[1, K]the starting and ending time of\nthekthsegment occurrence of the mother segment mij.\nSecond condition: However the condition sij(t) = 1\n∀t∈[tk\nx, tk\ny])does not guarentee that [tk\nx, tk\ny]is an instan-\ntiation of the mother segment mij.[tk\nx, tk\ny]could also be -\na part (the beginning or ending) of another sequence (see\nFig. 4[C]) - the succession of two non-overlapping seg-\nments (see Fig. 4[D] and Fig. 4[E]). For this reason, we\nneed to add a second condition. For this, we deﬁne a sec-\nond score which is speciﬁc to each interval k∈K:\nσijk(τ) =tk\nyX\nt=tkxSseg(τ, t)∀τ∈[τi, τj] (5)\nIn the ideal case σijk(τ)should be equal to 1∀τ∈[τi, τj]\n(such as sij(t)should be equal to 1∀t∈[tk\nx, tk\ny]). If there\nexists a value τ∈[τi, τj]such that σijk(τ) = 0 , it indicates\nthat the segment only partially covers the duration of mij\n(see Fig. 4[C]). If there exists a value τ∈[τi, τj]such that\nσijk(τ)>1, it indicates that the interval contains several\nsegments (see Fig. 4[D][E]). The second condition is then:\nσijk(τ) = 1 ∀τ∈[τi, τj]∀k∈K.τiandτjshould be\nmodiﬁed in order to fullﬁll that condition too.\nBest ﬁt approach: Theoretically the width of the corri-\ndor should be reduced until sij(t)≤1∀tandσijk(τ) =\n1∀τ∈[τi, τj]for all the k∈Kintervals. In practice,\nsince the detected segments are not perfect, this could lead\nto unnecessary reduction of the corridor width hence of the\nmother segment length. A best ﬁt approach between errors\nand corridor-width-reduction is therefore used. For this, we\ndeﬁne the following scores:\n•/epsilon1s(k): the number of frames fow which sij(t∈[tk\nx, tk\ny])\nis>1, relative to the length of the interval ( tk\ny−tk\nx)\n•εs: the number of intervals k∈Kfor which /epsilon1s(k)\nexceeds a given threshold Ts\n•/epsilon1σ(k): the number of frames for which σijk(τ∈\n[τi, τj])differs from 1, relative to the length of the\ninterval ( τj−τi)\n•εσ: the number of intervals k∈Kfor which /epsilon1σ(k)\nexceeds a given threshold Tσ\nThe corridor is reduced until both εsandεσfall below a\nthird threshold T.T= 0 indicates that we do not allow\nany overlap of sequences. T= 1 indicates that we allow\nany overlap or partial sequences (this is the summary score\nproposed by [5]).\nHow is the corridor reduced ? The corridor can be\nreduced by increasing τior decreasing τj. For a speciﬁc\ninterval k∈K, a value of σijk(τ)>1forτclose to τi\nindicates that an extra segment appears at the beginning ofthe interval [tk\nx, tk\ny]. In this case the width of the corridor\nijshould be reduced by increasing the value of τi. A value\nofσijk(t)>1forτclose to τjindicates that an extra seg-\nment appears at the end of the interval [tk\nx, tk\ny]and the width\nof the corridor should be reduced by decreasing τj. Each\ninterval [tk\nx, tk\ny]may require a different solution. Therefore\nthe global action is made taking into account the best global\naction. This is made according to a vote: each interval k\nvotes either to increase τior decrease τj.\nScore computation: After adaptation of τiandτj, a\nscore is assigned to the current mother segment mij. It is\ndeﬁned as the sum of the lengths of all explained segments\n[tk\nx, tk\ny], it represents the likelihood that this mother segment\nexplains the observed segments. This process is repeated\nfor each candidate mother segment. The candidate mother\nsegment with the highest score (maximum likelihood) is\nchosen as the ﬁrst (most important) mother segment.\nSegment cancellation: The segments belonging to (that\ncan be mostly explained by) this mother segment are then\ncanceled. In order to do that, the values of the segment\nsimilarity matrix inside the corridor deﬁned by the selected\nmother segment mijare canceled (set to 0). A new set of\nsegments is then derived from the analysis of the new seg-\nment similarity matrix and the process is repeated for the\ndetection of the next mother segment.\nIn theory any values of τiandτjcan be chosen as the\nstarting and ending time of a candidate mother segment.\nHowever, in order to save computation time, τiandτjare\nchosen from the set of detected segments.\n3 EV ALUATION\nIn this part we evaluate the performances of our algorithm.\nIn particular, we compare the inﬂuence of the choice of the\nfeature sets and the use of higher-order similarity matrix.\nThis is, as far as we now, the ﬁrst time an evaluation of\nsequence detection algorithm is performed.\n3.1 Test set\nFor the evaluation of our system we have used the “melody\nrepetition” part of the MPEG-7 test set. The MPEG-7 test\nset has been developed by the author for the task of music\nstructure discovery. It is composed of two parts: state an-\nnotations (this part is currently merged with the QMUL test\nset4and sequence annotations5. The sequence test set\nis composed of 11 songs annotated into all their repeated\nmelodies over time (up to 7 melodies).\n3.2 Performance measure\nEvaluating the performances of MSD algorithms is not an\neasy task. [1] and [18] already raised this issue and did pro-\nposals in the case of state representation (measuring seg-\nment boundaries and segment labels). In the case of se-\nquence representation, measuring segment boundaries makes\nfew sense: an annotated sequence ABCD can be detected\nas two sequences AB and CD or as A and BCD or as three\nsequences AB-, -BC- and -CD. Measuring sequence labels\n4available at http:// www.elec.qmul.ac.uk/ digitalmusic/ downloads/\n5available at http:// recherche.ircam.fr/ equipes/ analyse-synthese/\npeeters/ mpeg7audio msd testset/Figure 4 . Sequence detection by maximum likelihood al-\ngorithm: [A] corridor [τi, τj]of the candidate mother seg-\nment mijand corresponding function sij(t);sij(t)≤1\nindicates no simultaneous segments and allows segmenta-\ntion; [B] sij(t)>1indicates simultaneous segments and\nrequires further corridor width reduction; [C] σijk(τ) = 0\nfor the majority of [tk\nx, tk\ny]indicates that the interval con-\ntains a partial sequence; [D] σijk(τ)>1indicates that the\ninterval contains successive non-overlapping segments; [E]\nσijk(τ)/negationslash= 1 indicates that the interval contains either suc-\ncessive non-overlapping segments or no segments\n.\n0500 1000 1500 2000 2500 30000.5\n1\n1.5\n0500 1000 1500 2000 2500 30000.5\n1\n1.5\n0500 1000 1500 2000 2500 30000.5\n1\n1.5Figure 5 . Mapping between annotated and detected se-\nquence labels [top] annotated sequences ai(t)[middle]\nmapped annotated sequences ak(j)(t)[bottom] detected se-\nquences ej(t)on “Smells like teen spirit” by Nirvana. The\ncolors represent the various labels.\nrequires a previous mapping between annotated sequence\nlabels and estimated sequence labels. The number of la-\nbels may differ between both: - 1) the annotation may gives\nﬁner details than what can be detected - 2) the annotation\nmay group segments with different acoustical properties.\nThe ﬁrst case is more usual since - annotations tend to split\nmelodies into sub-melody (according to lyric changes) - es-\ntimation tends to merge successive repeated melodies into\na single one (if the verse is always repeated by the cho-\nrus then only the grouped verse-chorus segment will be de-\ntected). Therefore we allow mapping several annotated se-\nquences to a unique estimated sequence (but not the oppo-\nsite). We note ai(t)(ej(t)) the time vector having a value\nof1when the annotated (estimated) sequence i(j) exists at\ntimetand0otherwise. We assign each annotated sequence\nito the estimated sequence jwith the largest dot product:\nk(j) =argmaxi/angbracketleftai(t), ej(t)/angbracketright(jis the estimated sequence\nthat best explains i). The mapping process is illustrated on\nFig. 5. Finally, we assign a score to the estimated represen-\ntation. This score is the sum of all “mapped” sequence dot\nproducts normalized by the total duration of the annotation:\ns=P\nj­\nak(j)(t), ej(t)®\nP\njP\ntak(j)(t)(6)\nThis score indicates how much are the annotated sequences\nexplained by the estimated sequences.\n3.3 Results\nWe present the results of the sequence estimation for vari-\nous conﬁguration of our system into Tab. 1. The rows in-\ndicate the individual track scores. The last row indicates\nthe average score over the 11 tracks. We ﬁrst compare\nthe results obtained using the three individual feature sets\n(MFCC, Spectral Contrast and Pitch Class Proﬁle) with the\nones obtained when combining their normalized individual\nsimilarity matrices (“Combined features” column). In al-\nmost all cases, the results obtained with the combined fea-\ntures (54.8%) are better than the ones obtained with the in-\ndividual feature sets. We then compare the estimation of the\nsequences using a 1st order similarity matrix (54.8%) with\nthe estimation using higher-order similarity matrix (“HOS”\ncolumn). A 2nd order matrix has been used here. For\nthe two cases, we indicate the individual track score and\nthe number of detected segments. On average the use ofTable 1 . Comparison between various conﬁgurations of\nthe sequence estimation algorithm on the MPEG-7 “melody\nrepetition” test set.\nthe HOS makes the score decreases from 54.8% to 46.8%.\nHowever, for the Brubeck, Moby “Natural Blues”, Oasis\nand Pink Floyd tracks, the use of the HOS allows improv-\ning the estimation. For the Morisette and Nirvana tracks,\nthe scores decrease. These two tracks have in common a\nchord progression repeated many times over the track dura-\ntion. In this case, the use of HOS produces many diagonals\nin the matrix and masks the real melody repetitions.\n4 CONCLUSION AND FUTURE WORKS\nIn this paper we proposed a system for the automatic esti-\nmation of the structure of music tracks using the sequence\nrepresentation. Three sets of features were used (related\nto timbre and pitch) and combined into a unique similarity\nmatrix. During an experiment, we showed that the combi-\nnation of these three sets allows improving the estimation of\nthe structure. We introduced the notion of higher-order sim-\nilarity matrix, which allows taking into account higher or-\nder time repetitions in the computation of the matrix. How-\never, the use of it only brings improvement in few cases. We\nﬁnally presented a maximum likelihood approach to esti-\nmate the structure of the track from the segment detected in\nthe similarity matrix. This approach allows to solve the esti-\nmation problem in a global way by looking at the sequences\nthat best explain all observed segments and is much faster\nthan the usual DTW algorithms. Finally, we introduced the\nMPEG-7 “melody repetition” test set and evaluated our al-\ngorithm positively on it.\nIn our current system, most estimation errors originate\nfrom the segment detection part (not from the sequence\nestimation part). Further works will therefore concentrate\non adapting our sequence estimation algorithm to work di-\nrectly on the similarity matrix, i.e. without requiring a pre-\nvious detection of the segments. Also, it appeared that the\nstarting time of most detected sequences did not match the\nannotated one. The annotation tends to start at the begin-\nning of the lyrics. Information about voice presence and\nbeat/ measure positions should certainly allows improving\nthe location of this starting time. Finally, other measuresshould be studied for the evaluation of the quality of the\nsequence estimation.\n5 ACKNOWLEDGEMENTS\nPart of this work was conducted in the context of the French\nRIAM project Ecoute http://projet-ecoute.ircam.fr/.\n6 REFERENCES\n[1] S. Abdallah, K. Nolan, M. Sandler, M. Casey, and C. Rhodes.\nTheory and evaluation of a bayesian music structure extractor.\nInISMIR , London, UK, 2005.\n[2] J.-J. Aucouturier and M. Sandler. Finding repeating patterns in\nacoustic musical signals: applications for audio thumbnailing.\nInAES 22nd Int. Conf. on Virt., Synth. and Ent. Audio , 2002.\n[3] M. Bartsch and G. Wakeﬁeld. To catch a chorus: Us-\ning chroma-based representations for audio thumbnailing. In\nWASPAA , New Paltz, NY , USA, 2001.\n[4] G. Boutard, S. Goldszmidt, and G. Peeters. Browsing inside a\nmusic track, the experimentation case study. In Workshop on\nLSAS , Athens, Greece, 2006.\n[5] M. Cooper and J. Foote. Automatic music summarization via\nsimilarity analysis. In ISMIR , Paris, France, 2002.\n[6] M. Cooper and J. Foote. Summarizing popular music via\nstructural similarity analysis. In WASPAA , New Paltz, NY ,\nUSA, 2003.\n[7] R. Dannenberg. Pattern discovery techniques for music audio.\nInISMIR , Paris, France, 2002.\n[8] J. Foote. Automatic audio segmentation using a measure of\naudio novelty. In ICME , New York City, NY , USA, 1999.\n[9] J. Foote. Visualizing music and audio using self-similarity. In\nACM Int. Conf. on Multimedia , Orlando, Florida, USA, 1999.\n[10] T. Fujishima. Realtime chord recognition of musical sound:\na system using common lisp music. In ICMC , Bejing, China,\n1999.\n[11] M. Goto. A chorus-section detecting method for musical audio\nsignals. In ICASSP , 2003.\n[12] T. Jehan. Perceptual segment clustering for music description\nand time-axis redundancy cancellation. In ISMIR , Barcelona,\nSpain, 2004.\n[13] K. Jensen. Rhythm-based segmentation of popular chinese\nmusic. In ISMIR , London, UK, 2005.\n[14] D. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H. Cai. Mu-\nsic type classiﬁcation by spectral contrast. In ICME , Lausanne\nSwitzerland, 2002.\n[15] B. Logan and S. Chu. Music summarization using key phrases.\nInICASSP , Istanbul, Turkey, 2000.\n[16] N. Maddage, C. Xu, M. Kankanhalli, and X. Shao. Content-\nbased music structure analysis with applications to music se-\nmantic understanding. In ACM Int. Conf. on Multimedia , New\nYork, NY , USA, 2004.\n[17] M. Mueller and F. Kurth. Towards structural analysis of au-\ndio recordings in the presence of musical variations. Eurasip\nJournal on Advances in Signal Processing , ID 89686, 2007.\n[18] J. Paulus and A. Klapuri. Music structure analysis by ﬁnding\nrepeated parts. In ACM Multimedia , Santa Barbara, CA, 2006.\n[19] G. Peeters. Deriving musical structures from signal analysis\nfor music audio summary generation: Sequence and state ap-\nproach. In LNCS 2771 . Springer-Verlag, 2004.\n[20] G. Peeters, A. Laburthe, and X. Rodet. Toward automatic mu-\nsic audio summary generation from signal analysis. In ISMIR ,\nParis, France, 2002."
    },
    {
        "title": "Music Clustering with Constraints.",
        "author": [
            "Wei Peng 0001",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418087",
        "url": "https://doi.org/10.5281/zenodo.1418087",
        "ee": "https://zenodo.org/records/1418087/files/PengLO07.pdf",
        "abstract": "This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by “similar” artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The “correctness” of the clusters generated is tested using artist similarity provided by All Music Guide. 1 INTRODUCTION For those who listen to music through the Internet, how to navigate in the ocean of on-line music is an important issue. Nowadays, everything about music is on the web — audio, lyrics, artist discographies, artist biographies, reviews, and discussions. This raises an issue of whether the on-line music data can be efficiently accessed so that the user can benefit from the existence of such large volumes of data. A solution to the issue can be given by developing efficient music assistance programs, which integrate techniques for analyzing, summarizing, indexing, classifying, and grouping music data. This paper addresses the issue of clustering pop music into groups with respect to the artists. Clustering is the standard, effective tool for efficient organization, summarization, navigation and retrieval of a large amount of data. Information navigation by browsing through data clusters is more suitable for users who have vague information need and/or just wish to discover general contents of the data set. The use of instance-level constraints as the background information to improve data clustering has been widely studied in machine learning in the past few years. Instance-level constraints are generally pairwise and they are of two types: the positive constraint is one that specifies that two instances must belong to the same clusc⃝2007 Austrian Computer Society (OCG). ter, and the negative constraint is one that specifies that two instances must belong to different clusters. These instance-level constraints have been used in learning distance/dissimilarity measures [3,4,7,10,18], modifying objective criteria for cluster evaluation [1], and improving optimization procedures [2,16,17]. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). In this paper, we investigate the problem of content-based music clustering with such instance-level constraints. In particular, we adapt a generalized constraint clustering algorithm based on K-means and discuss approaches to automatically generate constraints. The rest of the paper is organized as follows: Section 2 introduces the algorithm for constraint-based clustering, Section 3 discusses various approaches to generate constraints in music applications, Section 4 describes the content-based feature extraction, Section 5 show our experimental results, and Section 6 provides conclusions and presents open questions. 2 CONSTRAINT-BASED CLUSTERING This section provides some background on the K-means algorithm and then discusses the constraint-based clustering algorithm following the exposition in [7].",
        "zenodo_id": 1418087,
        "dblp_key": "conf/ismir/PengLO07",
        "keywords": [
            "music",
            "clustering",
            "constraints",
            "pop",
            "music",
            "artists",
            "background",
            "knowledge",
            "user",
            "access"
        ],
        "content": "MUSIC CLUSTERING WITH CONSTRAINTS\nWei Peng Tao Li\nSchool of Computer Science\nFlorida International University\n{wpeng002,taoli }@cs.ﬁu.eduMitsunori Ogihara\nDepartment of Computer Science\nUniversity of Rochester\nogihara@cs.rochester.edu\nABSTRACT\nThis paper studies the problem of building clusters of\nmusic tracks in a collection of popular music in the pres-\nence of constraints. The constraints come naturally in the\ncontext of music applications. For example, constraints\ncan be generated from the background knowledge (e.g.,\ntwo artists share similar styles) and the user access pat-\nterns (e.g., two pieces of music share similar access pat-\nterns across multiple users). We present an approach\nbased on the generalized constraint clustering algorithm\nby incorporating the constraints for grouping music by\n“similar” artists. The approach is evaluated on a data\nset consisting of 53 albums covering 41 popular artists.\nThe “correctness” of the clusters generated is tested using\nartist similarity provided by All Music Guide.\n1 INTRODUCTION\nFor those who listen to music through the Internet, how to\nnavigate in the ocean of on-line music is an important is-\nsue. Nowadays, everything about music is on the web —\naudio, lyrics, artist discographies, artist biographies, re-\nviews, and discussions. This raises an issue of whether the\non-line music data can be efﬁciently accessed so that the\nuser can beneﬁt from the existence of such large volumes\nof data. A solution to the issue can be given by developing\nefﬁcient music assistance programs, which integrate tech-\nniques for analyzing, summarizing, indexing, classifying ,\nand grouping music data.\nThis paper addresses the issue of clustering pop mu-\nsic into groups with respect to the artists. Clustering is\nthe standard, effective tool for efﬁcient organization, su m-\nmarization, navigation and retrieval of a large amount of\ndata. Information navigation by browsing through data\nclusters is more suitable for users who have vague infor-\nmation need and/or just wish to discover general contents\nof the data set.\nThe use of instance-level constraints as the back-\nground information to improve data clustering has been\nwidely studied in machine learning in the past few years.\nInstance-level constraints are generally pairwise and the y\nare of two types: the positive constraint is one that spec-\niﬁes that two instances must belong to the same clus-\nc/circlecopyrt2007 Austrian Computer Society (OCG).ter, and the negative constraint is one that speciﬁes that\ntwo instances must belong to different clusters. These\ninstance-level constraints have been used in learning dis-\ntance/dissimilarity measures [3,4,7,10,18], modifying o b-\njective criteria for cluster evaluation [1], and improving\noptimization procedures [2, 16, 17].\nThe constraints come naturally in the context of mu-\nsic applications. For example, constraints can be gen-\nerated from the background knowledge (e.g., two artists\nshare similar styles) and the user access patterns (e.g., tw o\npieces of music share similar access patterns across multi-\nple users). In this paper, we investigate the problem of\ncontent-based music clustering with such instance-level\nconstraints. In particular, we adapt a generalized con-\nstraint clustering algorithm based on K-means and discuss\napproaches to automatically generate constraints. The res t\nof the paper is organized as follows: Section 2 introduces\nthe algorithm for constraint-based clustering, Section 3\ndiscusses various approaches to generate constraints in\nmusic applications, Section 4 describes the content-based\nfeature extraction, Section 5 show our experimental re-\nsults, and Section 6 provides conclusions and presents\nopen questions.\n2 CONSTRAINT-BASED CLUSTERING\nThis section provides some background on the K-means\nalgorithm and then discusses the constraint-based cluster -\ning algorithm following the exposition in [7].\n2.1 K-means Clustering\nThe problem of clustering data arises in many disciplines\nand has a wide range of applications. Intuitively, cluster-\ning is the problem of partitioning a ﬁnite set of points in\na multi-dimensional space into classes (called clusters ) so\nthat (i) the points belonging to the same class are “similar”\nand (ii) the points belonging to different classes aren’t [9 ].\nK-means is a popular clustering algorithm where the\ninput data set is partitioned into Kgroups, where the num-\nberKis speciﬁed by the user. The quality of partition into\nKclusters can be viewed as the quantization error as fol-\nlows:\nE=1\n2K/summationdisplay\nj=1/summationdisplay\ns∈Cj(¯cj−s)2. (1)HereC1,... ,C Kare the Kclusters and and ¯c1,... ,¯cK\ntheir centroids. The goal of K-means is to minimize this\nquantization error, which is accomplished iteratively by\nalternating between the allocation step and the evalua-\ntion step. In the former each data point is allocated to\nthe cluster whose centroid is the closest to it so as to\nminimize the quantization error with respect to the cur-\nrent centroids, while in the latter, the centroid of each\ncluster is updated based on the new allocation. The so-\nlution ofδE\nδ¯cj= 0,1≤j≤Kprovides the rule that\nsets the centroid ¯cjto be the mean of the data points in\nCjfor all j,1≤j≤K. Repetition of these alternating\nsteps monotonically decreases the average distance of data\npoints from their corresponding centroid. The algorithm\nconverges when there is no change in the data allocation.\n2.2 Constraint-based Clustering\nFollowing [4] we deﬁne the concept of constraint-based\nclustering for music similarity. We deal with positive con-\nstraints, which are given as a list of pairs that are expected\nto belong to the same cluster, and negative constraints,\nwhich are given as a list of pairs that are expected to be-\nlong to different clusters. We modify the the objective\nfunction so that penalty is added for each constraint that is\nnot satisﬁed. For a positive constraint (si,sj)(that is, si\nandsjmust be in the same cluster), the penalty (in the case\nwhere they go to different clusters) is the squared distance\nbetween their cluster centroids. For a negative constraint\n(si,sj)(that is, siandsjmust be in different clusters),\nthe penalty (in the case where they go to the same clus-\nters) is the squared distance between the centroids that are\nthe closest and the second closest to either siorsj. In\nboth cases, we use the centroids to determine the penalty\nso as to treat equally constraint violations within a cluste r,\nand we use squared distance since the quantization error\nis based on squared distance. Also, our choice of the two\ncentroids in the penalty for an unsatisﬁed negative con-\nstraint is based on the idea that the constraint would be\nsatisﬁed if one vector were assigned to the cluster of the\nclosest centroid and the other were to the cluster of the\nother centroid. Note that the closest centroid closest is th e\ncentroid of the cluster to which siandsjbelong.\nThe exact formula for the objective function is given\nbellow:\nCE =1\n2(E+PM+PC) (2)\n=1\n2\nK/summationdisplay\nj=1/summationdisplay\ns∈Cj(¯cj−s)2+PM+PC\n,\nPM =/summationdisplay\n(si,sj)∈Mpm\nij(1−∆(y(si),y(sj))), (3)\nPC =/summationdisplay\n(si,sj)∈Cpc\nij∆(y(si),y(sj)), (4)\npm\nij= (¯cy(xi)−¯cy(xj))2, (5)\npc\nij= (¯cy(xi)−¯c∗\nij)2. (6)HereMandCrespectively represent the set of positive\nconstraints and the set of negative constraints, pm\nijandpc\nij\nare respectively penalty parameters for the positive and fo r\nnegative constraints, and the value of y(si)is the index of\nthe cluster to which the data point sibelongs. Also, ∆\nis the Kronecker delta function deﬁned by: ∆(x,y) =\n1ifx=yand0otherwise. That is, the penalty pm\nijis\nadded only if (si,sj)∈Mbutsiandsjdo not belong\nto different clusters and the penalty pc\nijis added only if\n(si,sj)∈Cbutsiandsjbelong to the same cluster.\nFurthermore, ¯c∗\nijis the centroid that is the next closest to\neither siandsj.\nLike K-means, the constraint-based clustering algo-\nrithm is iterative, alternating between the allocation ste p\nand the centroid update step. In the allocation step, the\ngoal is to minimize the generalized constrained vector\nquantization error in Eq. 3. This is achieved by assign-\ning instances so as to minimize the proposed error term.\nFor pairs of instances in the constraint set, the quantiza-\ntion error CEis calculated for each possible combination\nof cluster assignments, and the instances are assigned to\nthe clusters so that CEis minimized. In the update step,\nthe centroids are cluster centroids. As in K-means, the\nﬁrst order partial derivatives of CE with respect to each\ncentroid is evaluated and the solution that makes all these\nderivatives equal to zero are obtained. Figure 1 presents\nthe procedure of the constraint-based clustering algorith m\n(see [4] for more detail).\nALGORITHM Constraint-based Clustering\nInput: Data set S, positive constraint set M,\nnegative constraint set C, number of clusters K\nOutput: Cluster assignment Y\n1:Initialization:\na. Create the mneighborhoods from MandC\nb.ifm≥K, initialize using weighted farthest-ﬁrst\ntraversal starting from the largest neighborhood\nelseinitialize with centroids of neighborhood sets\nand remaining clusters at random\n2:Iteration:\nwhile stopping criterion is not met\n2.1: Step I: Assign each data point to either cluster or\nnoise set such that it minimizes CE\n2.2: Step II: Update the centroids for each cluster\n3: Return Y\nFigure 1 . The Algorithm Description.\n3 CONSTRAINTS GENERATION\nThe constraints come naturally in the context of music ap-\nplications. The following gives a summary on different\napproaches to generating constraints:•Background Knowledge : Constraints can be gener-\nated from the background knowledge. If we already\nknow that two songs are of the same styles, or for-\nmally, if we know two songs have the same cluster\nlabels, then they must be in the same cluster (e.g.,\na positive constraint). Similarly, if it is known that\ntwo songs are of different styles, then they should\nbe in different clusters (e.g., a negative constraint).\n•User-access Patterns : It is well known that the per-\nception of music is subjective to individual users.\nDifferent users can have totally different opinion\nfor the same pieces of music. If two pieces of\nmusic share similar access patterns across multiple\nusers, they should be similar in human perception,\nand consequently should be put in the same cluster.\nThus user-access patterns can be used to generate\nconstraints for music clustering.\n•Subjective Similarity Measures : In the context of\nmusic information retrieval, when the initial re-\ntrieval results are unsatisﬁed, relevance feedback\nmethods will be applied to improve the quality of\nretrieval results through iteration of user relevance\nfeedback. User feedback can be regarded as a way\nof providing subjective similarity measures to the\nmusic and this can also be used to generate con-\nstraints.\n•Complementary and Diverse Music Information\nSources : Music data are naturally multi-modal, in\nthe sense that they are represented by multiple sets\nof features. For example, the personnel-related fea-\ntures (the producer, the supporting musicians, and\nthe record label), the acoustic features (which sum-\nmarize the voice and the background audio), and\nthe text features (.e.g., the song lyrics, the reviews).\nThese features are complementary and diverse, and\ncan be used to generate constraints for clustering.\nFor example, if two piece of music have the same\npersonnel-related features, then they can be consid-\nered to be similar based on content.\nIn the experiments in this paper, the constraints are\ngenerated by background knowledge, i.e., from the known\nclass labels. Exploring the effects of various constraint\ngeneration methods is one of our future goals.\n4 CONTENT-BASED FEATURE EXTRACTION\nThere has been a considerable amount of work in ex-\ntracting descriptive features from music signals for music\ngenre classiﬁcation and artist identiﬁcation [8,11,13–15 ].\nIn our study, we use timbral features along with wavelet\ncoefﬁcient histograms. The feature set consists of the fol-\nlowing three parts and totals 35 features.\n4.1 Mel-Frequency Cepstral Coefﬁcients (MFCC)\nMFCC is a feature set that is popular in speech process-\ning and is designed to capture short-term spectral-basedfeatures. To obtain the feature, the logarithm of the ampli-\ntude spectrum is computed for each frame based on short-\nterm Fourier transform, where the frequencies are divided\ninto thirteen bins using the Mel-frequency scaling. (The\n“cepstrum” is the name coined for this logarithm.) After\ntaking the logarithm of the amplitude spectrum, the fre-\nquency bins are grouped and smoothed according to Mel-\nfrequency scaling, which is design to agree with percep-\ntion. MFCC features are generated by decorrelating the\nMel-spectral vectors using discrete cosine transform. In\nthis study, we use the ﬁrst ﬁve bins, and compute the mean\nand variance of each over the frames.\n4.2 Short-Term Fourier Transform Features (FFT)\nThis is a set of features related to timbral textures and is\nnot captured using MFCC. It consists of the following ﬁve\ntypes. More detailed descriptions can be found in [15].\nSpectral Centroid is the centroid of the magnitude spec-\ntrum of short-term Fourier transform and is a measure of\nspectral brightness. Spectral Rolloff is the frequency be-\nlow which 85% of the magnitude distribution is concen-\ntrated. It measures the spectral shape. Spectral Flux is the\nsquared difference between the normalized magnitudes of\nsuccessive spectral distributions. It measures the amount\nof local spectral change. Zero Crossings is the number\nof time domain zero crossings of the signal. It measures\nnoisiness of the signal. Low Energy is the percentage of\nframes that have energy less than the average energy over\nthe whole signal. It measures amplitude distribution of the\nsignal.\n4.3 Daubechies Wavelet Coefﬁcient Histograms\n(DWCH)\nDaubechies wavelet ﬁlters are ones that are popular in im-\nage retrieval (see [5]). To extract DWCH features, the\ndb8ﬁlter with seven levels of decomposition is applied\nto thirty seconds of sound signals. After the decomposi-\ntion, the histogram of the wavelet coefﬁcients is computed\nat each subband. Then the ﬁrst three moments of a his-\ntogram, i.e., the average, the variance, and the skewness,\nare used [6,13] to approximate the probability distributio n\nat each subband. In addition, the subband energy, deﬁned\nas the mean of the absolute value of the coefﬁcients, is\nalso computed at each subband. A few trials reveal that of\nthe seven subbands of db8(1: 11025–22050 Hz, 2: 5513–\n11025Hz, 3: 2756–5513Hz, 4: 1378–2756Hz, 5: 689–\n1378Hz, 6: 334–689Hz, 7: 0–334Hz), subbands 1, 2, and\n4 show little variation. We thus choose to use only the\nremaining four subbands, 3, 5, 6, and 7, for our experi-\nments. In fact, the subbands match the models of sound\noctave-division for perceptual scales [12].\n5 EXPERIMENTS\nIn this section, we perform experiments to evaluate\nwhether the clustering algorithms based on minimizingJ a m e sT a y l o rS u z a n n eV\ne g a\nS i m o n &G a r f u n k e l\nCa r l yS im o nJ\no n iM\ni t c h e l l\nR\ni c kyL e eJ o n e sJ a\nc ks o nB r o w n eT\nh e E a g l e sF l e e t w o o dM a\nc\nB\nr u c eH\no r n s b y\nS t i n gS t e e l yD\na n\nE l t o nJ o h n Pe t e rGa b r\nielU t o p\niaYe s\nS h e r y lC r o w\nH\no o t i e &T\nh eB\nl o w ﬁ s hT\nh eB\ne a t l e sT\nh e R o l l i n gS t o n e sG r a n dF\nu n kR a i l r o a d\nG e n e s i s\nT\nh eM\no o d yB\nl u e sD\ne e pP\nu r p l eA\nCD\nCZ Z T\no p\nI\nn d i g oG i r l sM\ne l i s s aE t h e r i d g eD\ne r e k &T\nh eD\no m i n o s\nL e dZ e p p e l\ninJ\ni m iH\ne n d r i x\nB\nl a c kS a b b a t h\nA\nm y G r a n tB\na s i a\nB j\no r kE v e r y t h i n gB\nu tT\nh e G i r l\nJ\nu d i t hO w e n\nT\nR e xR a d n e yJ\no n e sT\nh eP\no l i c e\nS t e v i eW\no n d e r\nC l a s s 1\nC l a s s 2C l a s s 3\nFigure 2 . The artist similarity graph. The names in bold are “core” no des.\ndisagreement can be more powerful than unimodal meth-\nods.\n5.1 Data Description\nOur experiments are performed on the dataset consisting\nof 300 songs from 53 albums of a total of 41 artists.\nTo divide songs into classes, we choose to use the simi-\nlarity information among artists available at the All Music\nGuide artist pages (http://www.allmusic.com), assuming\nthat this information is provided by experts. By examin-\ning the All Music Guide pages of the 41 artists, if the name\nof an artist X appears on the list of artists similar to Y , it\nis considered that X is similar to Y . To form classes artists\nhaving a large number of neighbors are selected as core\nnodes. Core nodes that are neighbors to each other are put\ninto the same class. The other artists that are neighbors to\neach core nod are selected to be in the class of the core\nso that each class is separated by at least one node in be-\ntween. All the remaining artists are put in a separate class.\nTable 1 shows the classes of the artists and Figure 2 shows\nthe similarity graph along with the classes. Our goal is\nto classify each song out of the 300 into one of the four\nclasses corresponding to the artist by analyzing its audio\ncontents.\n5.2 Evaluation Measures\nAs discussed above, we use the cluster structures obtained\nfrom All Music Guide as labels to evaluate the clustering\nperformance. We use purity ,entropy , and accuracy as our\nperformance measures (see [19] for discussions of these).\nTo describe how these measures are calculated, let\nC1,... ,C Kbe the input classes, N1,... ,N Ktheir size,Class Members\n1 {Fleetwood Mac, Yes, Utopia, Elton John,\nGenesis, Steely Dan, Peter Gabriel }\n2 {Carly Simon, Joni Mitchell,\nSuzanne Vega, Ricky Lee Jones,\nSimon & Garfunkel, James Taylor }\n3 {AC/DC, Black Sabbath, ZZ Top,\nLed Zeppelin, Grand Funk Railroad,\nDerek & The Dominos }\n4 All the remaining artists\nTable 1 . Artist classes.\nandN=N1+· · ·+NK. Also, let D1,... ,D Kbe\nthe output clusters and M1,... ,M Ktheir size. For each\n(i,o),1≤i,o≤K, letHi,o=/bardblCi∩Do/bardbl.\nPurity measures how large a portion of each output\ncluster comes from a single input class [19]. For each\no,1≤o≤K, letp(o)be the index i,1≤i≤K,\nthat maximizes Hi,o, where a tie can be broken arbitrarily.\nThe purity of Dowith respect to C1,... ,C Kis deﬁned\nto beHp(o),o\nMo. The purity of D1,... ,D Kwith respect to\nC1,... ,C Kis then deﬁned to be the sum of individual pu-\nrity weighted proportionally to the size of output clusters .\nIn other words,\nPurity =K/summationdisplay\no=1Mo\nN·Hp(o),o\nMo\n=1\nNK/summationdisplay\no=1max{Hi,o|1≤o≤K}.(7)\nNote that p(1),... ,p (K)are determined independently\nand thus p(o)may have the same value for multiple valuesofo. Generally speaking, the higher the purity, the better\nthe clustering quality.\nEntropy measures how classes distributed on various\nclusters.\nEntropy =−1\nlogKK/summationdisplay\ni=1Ni\nNK/summationdisplay\no=1Hi,o\nNilogHi,o\nNi\n=−K/summationdisplay\ni=1K/summationdisplay\no=1Hi,o\nNlogKlogHi,o\nNi, (8)\nwhere the logarithm is base 2. Generally speaking, the\nsmaller the entropy value, the better the clustering qualit y.\nAccuracy measures under the assumption that there is a\none-to-one correspondence between the input classes and\nthe output clusters, how accurately the data allocation is.\nAccuracy = max\nπK/summationdisplay\ni=1Ni\nNmax\nqK/summationdisplay\ni=1Hi,q(i)\nNi\n=1\nNmax\nπK/summationdisplay\ni=1Hi,q(i), (9)\nwhere qranges over all permutations of {1,... ,K }. Gen-\nerally speaking, the larger the accuracy value, the better\nthe clustering quality.\nNote that purity and accuracy do not necessarily agree\nwith each other. Consider an example in which three\nclasses of size 10 each are given, the ﬁrst class is evenly\nsplit between ﬁrst and second clusters, and and the whole\nsecond and third classes are allocated to the third clus-\nter. The purity for the three clusters are 1.0,1.0, and0.5\nrespectively. Since the cluster sizes are 5,5, and20re-\nspectively, the purity for the clustering is 1.0·5\n30+ 1.0·\n5\n30+ 0.5·20\n30= 0.6667 . On the other hand, by letting\nclassicorrespond to cluster imaximizes the formula for\nthe accuracy to5+10\n30= 0.5.\n5.3 Analysis of the Results\n30 constraints (including 10 positive constraints and 20\nnegative constraints) are randomly generated from the\ncluster labels. We compare the results of constraint clus-\ntering with the results obtained when clustering is applied\non content without any constraints. Table 2 presents the\nexperimental results over ten independent trials.\nMeasurement Purity Entropy Accuracy\nWithout Constraints 0.436 0.731 0.438\nWith Constraints 0.471 0.723 0.472\nTable 2 . Performance comparison. The numbers are ob-\ntained by averaging over ten trials.\nWe observe that constraint-based clustering achieves\nbetter performance (i.e., higher purity and accuracy val-\nues and lower entropy values) than clustering without any\nconstraints, and that the performance of purity, entropy,0 5 10 15 20 25 30 35 40 450.430.440.450.460.470.480.49Accuracy\nNumber of Constraints\nFigure 3 . Comparisons of the clustering accuracy as a\nfunction of constraint size.\nand accuracy relative to the other is consistent in our com-\nparison, i.e., higher purity values correspond to lower en-\ntropy values, and to higher accuracy values. Note that dif-\nferent evaluation measures consider different aspects of\nthe clustering results. For example, the entropy measure\ntakes into account the entire distribution of the data in a\nparticular cluster and not just the largest class as in the\ncomputation of the purity. The accuracy considers the re-\nlationships among all pair class-clusters. We hope that\nthese different measures would provide enough informa-\ntion to understand the results of our experiments.\nFigure 3 illustrates the effects of the constraint size.\nThe X-axis of ﬁgure shows the number of constraints\nwhile the Y-axis shows the clustering accuracy. Here dif-\nferent constraint sizes are tested to investigate the effec t\nof the size of the constraint on the overall clustering per-\nformance. An approximate 1 : 2 ratio of the number of\npositive constraints to the number of negative constraints\nis maintained throughout the experiment. We observe that\nas the constraint set size increases, the accuracy measures\nsteadily improves and ﬂattens out after 40. Then, after\nthat, it looks as if the accuracy beings to decrease. This\nmay suggest that two many constraints may force our clus-\ntering algorithm to over-ﬁt.\nThe total number of constraints to specify relations be-\ntween data elements in an N-element data set is N(N+\n1)/2. In our case, N= 300 so the number is 44,850. The\nnumber of constraints we used is less than 0.1%of this\nand thus may look very small. However, the total number\nof class relations four a K-class data set is K2, which is\nin our case 16. Thus, with 40 constraints we can expect\nthat the class relations are represented at least twice on av -\nerage. The conspicuous decline in accuracy may suggest\nthat adding more than three constraints per class relation\ncan lower the performance.\n6 DISCUSSIONS AND OPEN QUESTIONS\nIn this paper, we study the problem on clustering music\nsongs in the presence of constraints. In particular, we\npresent a constraint-based clustering framework and dis-cusses various approaches to generate constraints. Exper-\nimental results on a data set consisting of 300 songs from\n41 artists of 53 albums show the effectiveness of our ap-\nproach.\nThere are several natural avenues for future research.\nThe ﬁrst natural direction is to investigate different ap-\nproaches for constraints generation. Second, another in-\nteresting direction is on music annotation. How can we\nautomatically and efﬁciently generate music style or simi-\nlarity information? Note we did not agree completely with\nthe artist similarity obtained from All Music Guide, but\nnonetheless used it as the ground truth to evaluate our al-\ngorithms in the experiments. Can we incorporate the opin-\nions from music experts or take into account the views\nfrom individual users? Third, it would also be interesting\nto evaluate the quality of the generated constraints. Fi-\nnally, can we determine the number of constraints?\nAcknowledgments\nTao Li is partially supported by NSF grants HRD-\n0317692 and IIS-0546280. Wei Peng is supported by\na Florida International University Presidential Graduate\nFellowship.\n7 REFERENCES\n[1] K. P. Bennett A. Demiriz and M. J. Embrechts. Semi-\nsupervised clustering using genetic algorithms. In\nProceedings of the 5th Conference on Artiﬁcial Neu-\nral Networks in Engineering (ANNIE’99) , pages 809–\n814, 1999.\n[2] S. Basu, A. Banerjee, and R. J. Mooney. Semi-\nsupervised clustering by seeding. In Proceedings of\nthe 19th International Conference on Machine Learn-\ning (ICML’02) , pages 27–34, 2002.\n[3] S. Basu, M. Bilenko, and R. J. Mooney. A proba-\nbilistic framework for semi-supervised clustering. In\nProceedings of the 10th International Conference on\nKnowledge Discovery and Data Mining (KDD’04) ,\npages 59–68, ACM, 2004.\n[4] M. Bilenko, S. Basu, and R. J. Mooney. Integrat-\ning constraints and metric learning in semi-supervised\nclustering. In Proceedings of the 21st International\nConference on Machine Learning (ICML’04) , pages\n81–88, 2004.\n[5] I. Daubechies. Ten lectures on wavelets . SIAM,\nPhiladelphia, PA, 1992.\n[6] A. David and S. Panchanathan. Wavelet-histogram\nmethod for face recognition. Journal of Electronic\nImaging , 9(2):217–225, 2000.\n[7] I. Davidson and S. S. Ravi. Clustering with con-\nstraints: feasibility issues and the k-means algorithm.InProceedings of the 5th International Conference on\nData Mining (SDM’05) , SIAM, 2005.\n[8] J. Foote and S. Uchihashi. The beat spectrum: a\nnew approach to rhythm analysis. In Proceedings of\nthe 2nd International Conference on Multimedia and\nExpo (ICME’01) , IEEE, 2001.\n[9] A. K. Jain and R. C. Dubes. Algorithms for Clustering\nData . Prentice Hall, Englewood Cliffs, NJ, 1988.\n[10] D. Klein, S. D. Kamvar, and C. D. Manning. From\ninstance-level constraints to space-level constraints:\nMaking the most of prior knowledge in data cluster-\ning. In Proceedings of the 19th International Confer-\nence on Machine Learning (ICML’02) , pages 307–\n314, 2002.\n[11] J. Laroche. Estimating tempo, swing and beat loca-\ntions in audio recordings. In Proceedings of the Work-\nshop on Applications of Signal Processing to Audio\nand Acoustics (WASPAA’01) , 2001.\n[12] G. Li and A. A. Khokhar. Content-based indexing and\nretrieval of audio data using wavelets. In Proceedings\nof the International Conference on Multimedia and\nExpo (ICME’00) , pages 885–888, IEEE, 2000.\n[13] T. Li, M. Ogihara, and Q. Li. A comparative study\non content-based music genre classiﬁcation. In Pro-\nceedings of the 26th International Conference on De-\nvelopment in Information Retrieval (SIGIR’03) , pages\n282–289, ACM, 2003.\n[14] L. Rabiner and B. H. Jiang. Fundamentals of Speech\nRecognition . Prentice-Hall, NJ, 1993.\n[15] G. Tzanetakis and P. E. Cook. Musical genre classiﬁ-\ncation of audio signals. IEEE Transactions on Speech\nand Audio Processing , 10(5):293–302, 2002.\n[16] K. Wagsta and C. Cardie. Clustering with instance-\nlevel constraints. In Proceedings of the 17th Interna-\ntional Conference of Machine Learning (ICML’00) ,\npages 1103–1110, 2000.\n[17] K. Wagsta, C. Cardie, S. Rogers, and S. Schroedl.\nConstrained k-means clustering with background\nknowledge. In Proceedings of the 18th International\nConference of Machine Learning (ICML’01) , pages\n577–584, 2001.\n[18] E. P. Xing, A. Y . Ng, M. I. Jordan, and S. Russell.\nDistance metric learning, with application to cluster-\ning with side-information. In Neural Information Pro-\ncessing 15 (NIPS’03) , pages 505–512, 2003.\n[19] Y . Zhao and G. Karypis. Empirical and theoretical\ncomparisons of selected criterion functions for doc-\nument clustering. Machine Learning , 55(3):311–331,\n2004."
    },
    {
        "title": "An Application of Empirical Mode Decomposition on Tempo Induction from Music Recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Sergios Theodoridis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418103",
        "url": "https://doi.org/10.5281/zenodo.1418103",
        "ee": "https://zenodo.org/records/1418103/files/PikrakisT07.pdf",
        "abstract": "This paper presents an application of Empirical Mode Decomposition (EMD) on the induction of notated tempo from music recordings. At a first stage, EMD is employed as a means to segment music recordings into segments that exhibit similar rhythmic characteristics. At a second stage, EMD is used in order to analyze the diagonals of the Self-Similarity Matrix of each segment, so as to estimate the tempo of the recording. The proposed method has been employed on various music genres with music meters of 2 4, 3 4 and 4",
        "zenodo_id": 1418103,
        "dblp_key": "conf/ismir/PikrakisT07",
        "keywords": [
            "Empirical Mode Decomposition",
            "music recordings",
            "notated tempo",
            "rhythmic characteristics",
            "Self-Similarity Matrix",
            "tempo estimation",
            "music genres",
            "music meters",
            "method employed",
            "segmentation"
        ],
        "content": "ANAPPLICATIONOF EMPIRICALMODE DECOMPOSITION ON\nTEMPO INDUCTIONFROMMUSIC RECORDINGS\nAggelos PikrakisandSergiosTheodoridis\nDept. of Informatics and Telecommunications\nUniversity of Athens,Greece\n{pikrakis, stheodor }@di.uoa.gr, http://www.di.uoa.gr/dsp\nABSTRACT\nThispaperpresentsanapplicationofEmpiricalModeDe-\ncomposition (EMD) on the induction of notated tempo\nfrommusicrecordings. Ataﬁrststage,EMDisemployed\nas a means to segment music recordings into segments\nthat exhibit similar rhythmic characteristics. At a second\nstage, EMD is used in order to analyze the diagonals of\nthe Self-Similarity Matrix of each segment, so as to esti-\nmate the tempo of the recording. The proposed method\nhas been employed on various music genres with music\nmeters of2\n4,3\n4and4\n4. Tempo has been assumed to re-\nmain approximately constant throughout each recording,\nranging from 60bpm up to 220bpm.\n1 INTRODUCTION\nTempo extraction is generally acknowledged to be useful\nin a variety of Music Information Retrieval applications\nand has been often studied in close relationship with beat\ntracking. A recently published study and comparison of\nwell known algorithms ispresented in[1].\nMost proposed approaches assume that tempo remains\napproximatelyconstantthroughouttherecordingwhichis\nalso the case with our method. In addition, our method\nfocuses on music recordings where music meter can be\none of2\n4,3\n4and4\n4, with tempo ranging from 60bpm up to\n220bpm. Thispaperisanattempttoapplyarelativelynew\ntransform,calledEmpiricalModeDecomposition(EMD),\nin the context of tempo extraction. EMD was originally\nintroduced in the context of non-stationary time-series a-\nnalysis[2]anditsrelationshipwithdyadicﬁlterbankswas\nlaterinvestigated[3]. TheoriginofEMDisalgorithmicin\nnature and lacks a solid theoretical framework. To bridge\nthis theoretical gap, certain attempts have been made re-\ncently [4]. This fact has not, however, discouraged re-\nsearchersfromapplyingEMDespeciallyinthecontextof\ngeophysical and biosignal processing. To our knowledge,\nthe EMD technique has so far met very limited recogni-\ntioninthe context of MusicInformationRetrieval [5].\nIn this paper EMD is used in conjunction with Self\nSimilarity Analysis of music recordings [6] in order to\nachieve tempo induction. Previous work in the ﬁeld has\nc/circlecopyrt2007 AustrianComputer Society (OCG).shownthatthediagonalsoftheSelfSimilarityMatrixcan\nreveal signal periodicities and that tempo also manifests\nitself as a signal periodicity, e.g., [7]. In particular, if the\nmean value of each diagonal is computed, then the result-\ning sequence of mean values exhibits certain minima that\ncorrespond to inherent signal periodicities. We propose\nthat if EMD is used to decompose this sequence, signal\nperiodicities manifest themselves more clearly in the re-\nsulting components. By processing these components, it\nis possible a) to achieve a rough segmentation of a mu-\nsicrecordingintoclustersofsegmentsthatexhibitsimila r\nrhythmic characteristics and b) to extract reliable tempo\nestimatesfrom the generated clustersof segments.\nThe reason we chose EMD is because, by its algorith-\nmic nature, it considers signals at the level of their local\noscillations and examines the evolution of a signal be-\ntween consecutive local extrema, e.g., consecutive local\nminima. Thisﬁtsnicelywiththenatureofthesequenceof\nmean values of diagonals that is extracted from the SSM\nofmusicrecordings,ifthissequenceistreatedasasignal.\nThe paper is organized as follows: the next section de-\nscribes the feature extraction stage, Section 3 proposes\nhow EMD can be used to provide a rough segmentation\nof the recording and Section 4 describes how EMD can\nbe applied on the resulting segments in order to achieve\ntempoinduction. ResultsarepresentedinSsection5;con-\nclusions and ideas forfuturework aregiven inSection 6.\n2 FEATURE EXTRACTION\nAt a ﬁrst step, the music recording is split into overlap-\nping long-term segments. Each long-term segment is 5\nseconds long with 4seconds overlap between successive\nwindows. The energy envelop of each long-term window\nisthenextractedbymeansofashort-termprocessingtech-\nnique. Suggestedvaluesforthelength, wsandhopsize hs\nof the short-term window are 95ms and 5ms respectively.\nThe extracted energy envelop is then used to generate the\nSelf-SimilarityMatrix(SSM)ofeachsegment[6]. Tothis\nend, the Euclidean Distance function is used as the simi-\nlaritymetric.\nOncetheSSMisgenerated,themeanvalueofeachdi-\nagonal is computed. Let B(k)denote the mean value of\nthek-th diagonal, k= 1... D, where Dis the total num-\nber of diagonals. If B(k)is treated as a function of k,it can be observed that signal periodicities appear as lo-\ncal minima (valleys) of B. This can be seen in Figure 1.\nThe deeper a valley, the stronger the periodicity. In the\n0 0.5 1 1.5 2 2.5 300.0050.010.0150.020.0250.030.0350.040.0450.05\nlag (secs)Mean value of the k−th diagonaltempo range\n81.5 bpm\n(notated tempo, 1/4 note)\n163 bpm\n(1/8 note)music meter lag\n(2/4)\nFigure 1. Plot of Bfor a segment stemming from a2\n4\nrecording with notated tempo ≈81.5 bpm. For conve-\nnience, only periodicities up to 3seconds are shown. It\ncanbeseenthatnotatedtempodoesnotcorrespondtothe\ndeepest valley.\nsequel, we will also refer to index kas the “lag” index.\nThe relative positions of lags that correspond to valleys\nreveal useful information about the rhythmic characteris-\ntics of a segment. Obviously, if B(k1)is a local mini-\nmum,thecorrespondingperiodicity,measuredinseconds,\nis(k1−1)∗hs.\nPrevious work in the ﬁeld ([7]) has suggested that no-\ntated tempo also manifests itself as a periodicity, i.e., a\nvalleyof B,althoughnotalwaysthedeepestone,asisthe\ncase in Figure 1. In the sequel, we treat the function Bas\nthe“rhythmic signature” of the long-term segment from\nwhich it isextracted.\n3 SIGNATURE CLUSTERING USING EMD\nOurnextgoalistogroup “rhythmicsignatures” intoclus-\nters and compute the mean signature of each cluster. This\nis because reliable tempo estimates cannot be extracted\nfrom all signatures, due to the fact that certain regions of\na music recording contain introductory or transitive parts\nthat distort periodicities. If a number of signatures form\nacluster,thisisindicativeofanunderlyingrhythmicsim-\nilarity and it is expected that it will yield more reliable\ntempo estimates.\nIn order to perform clustering, EMD is applied sepa-\nratelyoneachsignature. ThebasicstepsofEMD,givena\nsignal x(t)can besummarized as follows [2],[3]:\n1. Identifyall extrema of x(t)\n2. Interpolate between minima (resp. maxima), en-\ndingupwithsome“envelope” emin(t)(respectively\nemax(t)).3. Computetheaverage a(t) = (emin(t)+emax(t))/2\n4. Extractthedetail d(t) =x(t)−a(t),alsoknownas\ntheIMF.Iterateontheresidual a(t)untilastopping\ncriterionissatisﬁed,i.e., a(t)isreasonablyzeroev-\nerywhere [3].\nLetBmdenote the signature of the m-th long-term seg-\nment,m= 1... M,where Misthetotalnumberoflong-\ntermsegments. If cmisthenumberofcomponents(IMFs)\ngenerated by the EMD for Bm, then, at a ﬁrst step, all\nBm’sthathavegeneratedthesamenumberofcomponents\naregroupedtoformasinglecluster. Forexample,thesig-\nnature in Figure 1 is decomposed into 5components and\nwill be part of a cluster where all signatures have 5com-\nponents. Ifnoothersignatureyieldsﬁvecomponents,this\nsignaturewill formacluster of itsown.\nAt a next step, all signatures in a cluster are further\nexamined in order to form sub-clusters. To this end, let\nIMF ibe the i-th component of a signature Bmthat has\nbeen assigned to some cluster, where i= 1... m Kand\nmKis the number of components of Bm. By the nature\nof EMD, IMF mK(i.e., the last component, also known\nas the residual) only captures the slowly varying nature\nof the signature and does not provide any useful informa-\ntion about inherent periodicities. This is why we choose\nto ignore this component while reﬁning the clusters that\nhave been already formed. To continue, the energy of\nall remaining IMF i’s is computed and components are\nsorted in descending order, according to their energy val-\nues. The resulting order is then used to form sub-clusters\nwithin every cluster, i.e. segments that yield the same\norderof components are considered to be similar. For\nthe example of Figure 1, the components (excluding the\nlastone)areorderedintermsofenergyvaluesasfollows:\n{2nd,3rd,4th,1st}.\nFigures 2 and 3 show the two components with higher\n0 0.5 1 1.5 2 2.5 3−8−6−4−202468x 10−3\nlag (secs)Mean value of the k−th diagonaltempo range\n81.5 bpm\n(notated tempo, 1/4 note)\n163 bpm\n(1/8 note)\nFigure 2. The second component of the signature in Fi-\ngure1.\nenergy values for the example in Figure 1. When this re-\nﬁned clustering is complete, the mean signature for each\nclusteriscomputedbysimplyaveragingthe Bm’sofeach0 0.5 1 1.5 2 2.5 3−8−6−4−202468x 10−3\nlag (secs)Mean value of the k−th diagonaltempo range\n81.5 bpm\n(notated tempo, 1/4 note)\nFigure 3. The third component of the signature in Figure\n1.\ncluster. This is possible because all signatures have the\nsame length due to the ﬁxed size of the long-term win-\ndow. At the end of this stage, the initial music recording\nhas been split into regions that are determined by the re-\nspective clusters. Certain regions may, of course, contain\nnon-adjacent segments.\n4 TEMPO INDUCTION\nWe then focus on clusters that consist of more than two\nsignatures. The mean signature Rlof the l-th cluster is\nthen used to provide two separate tempo estimates for the\nsegments belonging to the respective cluster. To this end,\nRlis decomposed using EMD. The energy of each com-\nponent of Rlis computed and the resulting components\nare again sorted in descending order, according to their\nenergy values. Then, we focus on the two components\nthat possess the higher energy values and from each com-\nponent a tempo estimate is extracted, thus yielding two\ntempo estimates per cluster. For convenience, let us as-\nsume that the components in Figures 2 and 3 are the two\nhigh-energy components of a mean signature R. It can\nbe observed, that the component in Figure 2 exhibits two\ndominantperiodicitiesinsidethetempolimits,whereasin\nthe component of Figure 3 only one periodicity appears\ninside the tempo region and this periodicity refers to the\nnotated tempo value. By the nature of EMD, if IMF i1\nandIMF i2are two components with i1< i2,IMF i2is\nexpected to capture longer periodicities. Therefore, it is\nnot a surprise that the presence of the periodicity of the\n1\n8note in Figure 2 is weakened in Figure 3, whereas the\nopposite holds forthe longer periodicity of the1\n4note.\nIn order to extract a tempo estimate from each one of\nthe two components, the following procedure is applied\non each component:\n1. Allvalleysofthecomponentaredetected,including\nvalleys totheright of thetempo region.\n2. Each valley in the tempo region, is then examinedagainst all valleys with larger lags. Let kmbe the\nlagofavalleyinthetemporegionand kibethelag\nofthevalleyagainstwhichitisexamined. Theratio\nki\nkmis then computed. If the roundoff error of this\nratioissmallerthan 0.1,thenk2isconsideredtobe\namultipleof km,i.e.,kmistreatedasafundamental\nperiodicity and kias its multiple. This procedure is\nrepeated for all possible pairs, yielding a set Lkm\nof multiples (for lag kmin the tempo region). In\ntheend,thefollowingsumiscomputedfor km,i.e.,\nPkm=ckm+/summationtext\n∀ki∈Lkmcki,where ckiisthevalue\nof theEMD component forlag ki.\n3. The above step is repeated for all valleys that fall\nwithin the allowable tempo limits. The valley with\nthe highest sum is selected as the winner and the\ncorresponding lag as the periodicity of the tempo\nestimate.\nAfter tempo extraction has been completed for all clus-\nters,alltempoestimatesareplacedinahistogramandthe\ntempocorrespondingtothehighestpeakisselectedasthe\ntempo of the music recording. It is often the case, that\nthe histogram exhibits lobes around peaks because EMD\ntends to slightly displace periodicities. This is why an av-\neragingofthelobes(histogramsmoothing)isneededprior\ntoselecting thehighest peak.\n5 RESULTS\nTheproposedmethodhasbeenappliedonavarietyofmu-\nsic genres, including western pop/rock music and Greek\nTraditionalmusic. Atotalof 400recordingswerestudied.\nThetempooftheserecordingswasnotatedfrommusicol-\nogists. The complete list of the titles of recordings, along\nwiththerespective notated tempi isavailable at\nhttp://www.di.uoa.gr/pikrakis/tempo.html .\nRecordings werechosen onthebasisofthefollowingcri-\nteria:\n•Notated tempo remains approximately constant th-\nroughout each recording. The tempo ranges from\n60bpm up to 220bpm.\n•Music meter remains constant throughout each re-\ncording and can be one of the following:2\n4,3\n4and\n4\n4.\n•Instrumentationvaries,includingnon-percussivere-\ncordings and absence ofvocals.\nTable1providesaroughdistributionofrecordingsamong\ngenres and musicmeters.\n5.1 Performance of the clustering scheme\nAs it was described in Section 3, we choose to estimate\ntempofromclustersconsistingofatleast 3signatures. At\nan average, 25%of the signatures in each audio record-\ning is grouped into such clusters. This suggests that, on\naverage, 25%of the length of an audio recording takesMusicMeter\nBroad MusicGenre2\n43\n44\n4\nContemporary Pop/Rock 20%5%40%\nTraditional Greek Folk music 10%15%10%\nTable 1. Distribution of music tracks among genres and\nmusicmeters.\npart in the tempo extraction process. This is because the\nclustering criteria are quite strict, in the sense that cer-\ntain components returned by the EMD contain a very low\npercentage of the energy of the signature and could actu-\nally be omitted, thus loosening the clustering criteria. Fo r\nthe recordings that we studied, the number of signatures\nin these clusters can vary signiﬁcantly depending on the\nrecording, with large clusters containing approximately\n20signatures.\n5.2 Performance of thetempo extraction algorithm\nWhen the proposed method yields correct tempo estima-\ntes, the average accuracy of the extracted tempo value\nlies within 3.5%of the respective notated tempo value.\nThis is due to histogram smoothing (see Section 4) and\nthe displacement of valleys that is ususally more appar-\nent in EMD components that capture longer periodicities.\nTable 2 summarizes the cases where the algorithm fails\nto return the notated tempo (percentages refer to the to-\ntal number of recordings of the music corpus). It can\nMusicMeter\nBroad MusicGenre2\n43\n44\n4\nContemp. Pop/Rock (2xbpm) 3%0%2%\nContemp. Pop/Rock (1\n2bpm) 1.5%0%3%\nContemp. Pop/Rock (3xbpm) 01.5%0%\nGreek Folk Dances (2xbpm) 1%0%3%\nGreek Folk Dances (1\n2bpm) 2%0%2%\nGreek Folk Dances (3xbpm) 0%0%0%\nGreek Folk Dances (1.5xbpm) 2%0%0%\nTable 2. Distributionof tempo induction failures.\nbe seen that in the majority of cases, the returned tempo\nvalueistwiceorhalfthenotatedvalue,withtheexception\nof fast contemporary music recordings of music meter3\n4,\nwhere the returned value is three times the notated value,\ni.e., coincides with the periodicity that corresponds to a\nwholemusicmeter. Anotherinterestingcaseofconfusion\nstemsfromGreekFolkmusicofmeter2\n4wherethedotted\nquarter-note is often perceived by humans as a dominant\nperiodicity. For certain recordings of this particular typ e\nof music, our method also returns as notated tempo the\nperiodicity of the dotted quarter-note. Table 2 suggests\nthat the notated tempo was successfully inducted (within\na3.5%accuracy) from 79%of the recordings. It has to\nbe noticed that, in the vast majority of failures, notated\ntempo is present in the histogram of tempo values (prior\nto selecting the winner peak) as the second highest peakandquiteoftenitsheightisveryclosetothehighestpeak.\nThe study in [1] suggests that there exists an upper bound\nof approximately 80%for such algorithms. However, it\nhastobenoticedthatthealgorithmsin[1]werecompared\non a different dataset and in addition our paper treats a\nsubset of music meters encountered in [1]. Since this is\nthe ﬁrst time our work is reported, further improvements\nareexpected by reﬁnements intheclustering stage.\n6 CONCLUSIONS\nThispaperpresentedtheapplicationofEMDontempoex-\ntraction of music recordings. EMD was used in a twofold\nmanner: a)asameanstogeneratesimilaritiesamongrhy-\nthmic signatures, thus yielding a rough audio segmenta-\ntionandb)asadecompositionwhosecomponentsempha-\nsize periodicities that are related with the rhythmic char-\nacteristicsofthemusicrecording. Theresultsareveryen-\ncouraging, indicating that EMD is a very promising tool\nfor Music Information Retrieval tasks. In the future, we\nwill revisit the clustering criteria so that the segmentati on\nstage covers a larger percent of the music recording and\nwillalsoinvestigateextendingourapproachtonon-binary\nmeters such as7\n8,9\n8and5\n4. We will also study the perfor-\nmance of the method in connection with perceived tempo\nvalues from musicrecordings.\n7 REFERENCES\n[1] F. Gouyon et al., “An experimental comparison of\naudio tempo induction algorithms”, IEEE Transac-\ntionsonAudio,SpeechandLanguageProcessing ,vol.\n14(5), pp. 1832-1844, Sept. 2006.\n[2] N.E. Huang et al., “The empirical mode decomposi-\ntion and Hilbert spectrum for nonlinear and nonsta-\ntionary time series analysis, Proceedings of R. Soc.\nLondon, vol. 454, pp. 903-995, 1998.\n[3] P. Flandrin, G. Rilling and P. Concalves, “Empirical\nmode decomposition as a ﬁlter bank”, IEEE Signal\nProcessingLetters ,vol11(2),pp.112-114,Feb.2004.\n[4] E. Delechelle, J. Lemoine and O. Niang, “Empiri-\ncal mode decomposition: an analytical approach for\nsifting process”, IEEE Signal Processing Letters , vol.\n12(11), pp. 764-767, Nov. 2005.\n[5] P. Heydarian and J. D. Reiss, “Extraction Of Long-\nTermStructuresInMusicalSignalsUsingTheEmpir-\nicalModeDecomposition”, ProceedingsofDAFX-05 ,\nSep. 2005, Madrid, Spain.\n[6] J. Foote, “Visualizing Music and Audio using Self-\nSimilarity”, Proceedings of ACM Multimedia , pp. 77-\n80, 1999, Orlando, FL, USA, ACM Press.\n[7] A. Pikrakis, I. Antonopoulos and S. Theodoridis,\n“Music Meter and Tempo Tracking from Raw Poly-\nphonic Audio”, Proceedings of ISMIR 2004 , pp. 192-\n197, Sep. 2004, Barcelona, Spain."
    },
    {
        "title": "Indexing Music Collections Through Graph Spectra.",
        "author": [
            "Alberto Pinto",
            "Reinier H. van Leuken",
            "M. Fatih Demirci",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416744",
        "url": "https://doi.org/10.5281/zenodo.1416744",
        "ee": "https://zenodo.org/records/1416744/files/PintoLDWV07.pdf",
        "abstract": "Content based music retrieval opens up large collections, both for the general public and music scholars. It basically enables the user to find (groups of) similar melodies, thus facilitating musicological research of many kinds. We present a graph spectral approach, new to the music retrieval field, in which melodies are represented as graphs, based on the intervals between the notes they are composed of. These graphs are then indexed into a database using their laplacian spectra as a feature vector. This laplacian spectrum is known to be very informative about the graph, and is therefore a good representative of the original melody. Consequently, range searching around the query spectrum returns similar melodies. We present an experimental evaluation of this approach, together with a comparison with two known retrieval techniques. On our test corpus, a subset of a well documented and annotated collection of Dutch folk songs, this evaluation demonstrates the effectiveness of the overall approach. 1 INTRODUCTION Singing songs has always been an important way of passing on stories and expressing emotions, religious beliefs or social values. Most of these folk songs were transferred orally, often significantly changing over time and location. This resulted in the existence of many versions of the same songs, often displaying considerable variations. In Onder de groene linde, a collection of Dutch folk songs has been assembled by Ate Doornbosch, a Dutch radio broadcaster and researcher [1]. By recording many singers in the countryside during a period of over three decades, he captured this cultural heritage counting more than 7300 songs on tape. A large part of these melodies and songs has now been transcribed to music notation. The collection is becoming available to the general public and research community [7]. Content-based music retrieval opens up this great resource in such a way that both audiences can access it better. Through music information retrieval, songs belonging to the same class of songs can be grouped, or songs with only slight variations can be found. It can help identify the composer of a song, or asc⃝2007 Austrian Computer Society (OCG). sist in any other scholarly musicological task. We have three main contributions in this paper. Firstly, we introduce a new approach to music retrieval in which the music is represented as graphs, and the matching is based on specific features of these graphs. Our graph representation encodes the interval structure of a melody; it is a global time-independent signature of the melody, displaying the network of connections that exists between the pitch classes. Secondly, we introduce our indexing approach, which is new to music retrieval. To compute similarity between melodies, an algebraic structure is associated to each graph: an n × n matrix, with n equal to the number of vertices in the graph. Thirdly, we evaluate our method on a test corpus of Dutch folk songs. In this evaluation, we compare our method to two other methods: one approach specifically targeted towards folk song collections, and one approach using a time-independent structural approach as well. In this comparison, our method outperforms the other methods in terms of three well-known performance measures, namely, nearest neighbor, first and second tier. 2 RELATED WORK Melodic similarity has been investigated by many authors from very different points of view, using different kind of song collections as dataset. One of the most complete and recent studies has been performed by M¨ullensiefen and Frieler, who explored the concept melodic similarity within a collection of folk songs [10]. Using a collection of 577 Luxembourg folk songs, they empirically established an optimal similarity measure (the Opti3) that combines several known methods into one unifying expression. Out of 50 implemented musical similarity measures, taking into account all sorts of musical features, a weighted combination of methods was chosen to create one measure that best reflected the results of an extensive human listening experiment. Since their approach is specifically targeted towards a collection of folk songs, we compare it to our method as well. Another example of a representation/matching/indexing paradigm is the weighted point set on which the Earth Mover’s Distance or Proportional Transportation Distance can be applied. This kind of approach has been applied to test music similarity as well [13]. The notes of a melody are encoded as weighted points in a two-dimensional space where pitch and onset time are the axes; the duration of a note determines its weight. Similarity between two melodies can now be computed by measuring the effort it takes to transform one weighted pointset into the other. 3 REPRESENTATION tion of a melodic line that actually makes sense from a musical point of view. With this aim, we start looking just at melodies, not considering the rhythm. Melodies are generally studied from a pitch sequence/contour point of view. Our approach is different: we take as a starting point the interval structure, by which we mean the network of connections between pitches. We remark that melodies use only a subset of all possible connections, and with different frequencies. To model such relationships we use graphs, which have various and significant applications throughout mathematics, computer science, and physics. As such, the graph is a projection of the time-dependent concept of melody to a time-independent concept of intervallic structure. The next level of abstraction is to leave out pitch class information so that only the “interval connectivity” of the melody remains, and this means that certain operations such as inversion, transposition, retrogradation, other kind of permutations in the pitch class set and (some) shifting of fragments does not affect the graph. In this perspective what we are modelling is a global, time-independent signature of the melody [11], [8]. Melodies that display a similar interval behaviour have similar graphs, for example melodies in which there are one or two central notes (with many connections) and a number of peripheral notes (few connections). Let M be a melodic sequence of length m = |M| and consider the sequence of pitches {pj}j∈I, {I = 1, ..., m}. Then let V = Z12 be the (metric) space of pitches, or pitch classes, in the 12-tone system. We define the graph G with vertex set VG = V and edge set whose elements are the edges aj such that aj : \u001a pj →pj+1 for every couple (pj, pj+1) ⊆M pm →p1 for the couple (pm, p1) where j = 1, . . . , m −1 (see also [2] and [5]). The arrow am : pm →p1 does not represent an actual interval in the melody but it has been added for symmetry reasons and in order to take into account the relationship between the last and the first note as well, which otherwise would not have been reflected in the model. 4 INDEXING The graph representation described up to now is a geometric one. In order to allow computations with this representation, we need to associate an algebraic structure to it. The most common algebraic structure to represent a graph is the adjacency matrix. The adjacency matrix A(G) of a graph G is a square matrix of size equal to the order of the graph and where the entry (i, j) represents the number of oriented edges from vertex i to vertex j. This adjacency matrix therefore contains all the information to reconstruct the connectivity of the graph. A matrix closely related to the adjacency matrix is the laplacian matrix L(G), computed as L(G) = D(G)−A(G), where D(G) is the degree matrix of G. The degree matrix is also a square matrix of size equal to the order of the graph, but all values are zero except for those on the main diagonal. Here, the entry (i, i) represents the number of outgoing edges of vertex i. Given the laplacian matrix of a melody graph, the question remains how to compute the similarity to another melody. For this purpose, we first compute the eigenvalues of the laplacian matrix and sort them by magnitude. 1 Hereby, we obtain the laplacian spectrum of the graph, that is known to reflect a number of important properties of the graph. These properties include the diameter (related to the second smallest eigenvalue), mean distance, minimum degree and algebraic connectivity. Furthermore, the spectrum is invariant under permutations of the matrix (i.e. swapping columns or rows). Together with the absence of pitch information stored in the matrix, this makes the representation invariant under transpositions and note permutation. This is an important property, because as pointed out before, our concept of similarity is also independent from note permutation and transposition. Our main motivation for encoding the topology of a graph using the laplacian matrix comes from the fact that laplacian matrices are more natural, more important, and more informative than other matrices about the input graphs [9]. Previously, Godsil and McKay [4] and more recently Haemers and Spence [6] have also shown that the laplacian matrix has more representational power than the adjacency matrix, in terms of resulting in fewer cospectral graphs. Recall that two graphs are called cospectral (or, isospectral) if they have the same eigenvalues. Given a query graph and a large database, the objective of an indexing algorithm is to efficiently retrieve a small set of candidate matches, that share topological similarity with the query. As pointed out, we encode the topology of a graph through its laplacian spectrum, which is used as a signature for the database object. This spectrum can be seen as a point in a high dimensional space. To compute similarity between two graphs, we compute the Euclidean distance between their signatures, which is inversely proportional to the structural similarity of the graphs. Therefore, for a given query, retrieving its similar graphs can be reduced to a nearest neighbor search among a set of points. A set of candidate matches can now be found without having to inspect the entire database. For more details on this indexing strategy, the reader is referred to [3]. 1 Since the graphs are directed, the laplacian matrix is not necessarily symmetric. Consequently, some of the eigenvalues may be complex numbers and there exist multiple strategies for sorting these. As in [12], we sort these eigenvalues by modulus. CRITERIA NN 1st tier 2nd tier LAPLACIAN 66% 44% 63% ADJACENCY 58% 28% 48% OPTI3 40% 39% 56% EMD 64% 33% 50% PTD 64% 30% 46% Table 1. Nearest neighbour (NN), first tier and second tier results on the Onder de groene linde collection, computed using Laplacian spectra (L) Adjacency spectra (A) of the graphs. The results are compared to the methods Opti3, EMD and PTD. 5 EXPERIMENTS In “Onder de groene linde”, a large number of Dutch folk songs is preserved. This collection consists of more than 7300 songs recorded on tape. These songs are documented and annotated in great detail, and illustrated by sheet music examples. We experimented on a subset of this resource, that consists of 141 songs, of which we used the first phrase. These songs have been classified in 18 classes or melody groups, that relate to the concept of melody norms. At the Meertens Institute (a research institute for Dutch language and culture in Amsterdam) the concept of melody norm 2 is used to group historically or “genetically” related, orally transmitted melodies. Because the contents of folk song collections such as OGL are highly fragmented, it is impossible to trace back the history of melodies and to find all variants that are derived from a common ‘ancestor’ melody. What can be done, is to find related groups of melodies within the collection, based on both melodic similarity and available meta data, and link them to melody norms. A search engine would speed up this process of relating melodies considerably. As a ground truth in our experiments, we used a classification of the melodies into melody groups, that serve as candidates for the melody norms to be assigned in a later stage. For all the melodies in our test corpus, a graph has been constructed as described in Section 3. We evaluated retrieval performance with these graphs using both the adjacency and the laplacian spectra. The results are summarised in Table 1. For both experiments, we computed some retrieval statistics, namely nearest neighbor, first and second tiers, each averaged over all possible queries. These are frequently used in information retrieval. The first figure is the percentage of correct nearest neighbors (NN), i.e. the number of cases in which the top ranked database item, discarding the query itself, belongs to the same class as the query. We also computed the first tier, i.e. how many melodies of the query’s class are returned within the first K −1 matches, where K is the size of the query class. A similar performance figure is the second tier, i.e. how many melodies of the query’s class are returned within the first 2(K −1) matches. The laplacian 2 Equivalent with “tune family” and “Melodietyp”. spectral method performs best with a NN score of 66%, a 1st tier score of 44% and a 2nd tier score of 63%. Although these performance figures show in general the efficacy of the method, there are some interesting cases in particular we would like to point out here. In Figure 1 there is a special case of an “almost false” positive: for query OGL19205 (belonging to “Heer Halewijn 3rd version”), the nearest neighbor is OGL19107, that belongs to the group “Heer Halewijn 4th version”. However, the nearest neighbor is somehow related to the query; coincidentally they share the same graph representation, as is shown in Figure 3. This example shows how two melodies can be identical from the interval connectivity point of view but can also be perceptually quite different. This may represent the main limitation of this method in perceptual similarity tasks. The second example (Figure 2) shows the nearest neighbors for the query song OGL19406. Both examples may suggest also that in the case of folksongs people tend to remember more the interval connectivity than the actual intervals of the melody. Furthermore, we experimented with weighting the edges based on the interval they represent. For this purpose, two different sets of weights were used: one reflecting the difference in notes on the chromatic scale (ignoring differences in octaves) and one reflecting the harmonics of the interval, giving larger weights to consonant intervals and smaller weights to dissonant intervals. During this round of experiments, these methods did not improve the results obtained with normal laplacian spectra. Using the same test corpus and performance measures we compared our method to the optimal distance measure that was established by M¨uellensiefen and Frieler [10]. These results are also presented in Table 1, under the name Opti3. This distance measure is a weighted combination of three distance measures, each working on different feature sets. These measures are harmcore (using harmonic correlation), rhythfuzz (using fuzzified rhythm values) and ngrukkon (taking into account characteristic motives). This combined distance measure was established empirically out of 50 building blocks, by searching for a weighted combination whose performance best reflected the results of an extensive human listening experiment. Consequently, this method has been fitted to the data set at hand, explaining why the results are not optimal in our experiment. We also compared our method to the Earth Mover’s Distance (EMD). This distance measure takes two weighted point sets as input, and measures the minimum amount of work needed to transform one into the other by moving weight. The EMD is used in a number of different contexts; in the musical case, as pointed out in [13], the (2 dimensional) weighted point set is represented by the score itself, where the weight assigned to each note is its duration. However, since our method only takes into account the global melodic structure, we projected the weighted points on the pitch axis prior to computing the transportation distances. The “Proportional Transportation Distance” (PTD) is a modification of the EMD in order to get a similarity measure based on weight transportation such that the surplus of weight between two point sets is taken into account. Figure 1. Example of false positive for the query song “Heer Halewijn” (3rd version) OGL19205 with its NN, OGL19107, instance of “Heer Halewijn” (4th version). Figure 2. Example of true positive for the query song “In Frankrijk buiten de poorten” (2nd version) OGL19406 with its NN, OGL41709. Figure 3. Graph representation of the folk songs OGL19205 and OGL19107 (see Figure 1). The two letters in each circle represent the pitch classes respectively in the first and in the second song. 6 CONCLUDING REMARKS We presented a graph spectral approach that is new to music retrieval. Our method is focussed on the intervallic structure of the melody. This structure is encoded in a graph whose vertices correspond to the 12 pitch classes and whose edges reflect the interval sequence of the melody; an edge is added to the graph if the pitch classes of the corresponding vertices appear consecutively in the melody. The graphs are indexed into a database using their laplacian spectra, a feature vector that reflects the original topology and graph structure to a large extent. We evaluated our approach using a subset of a large collection of Dutch folk songs. On this test corpus, our method clearly outperforms existing methods. It is our intention to investigate this method further, for instance by weighting the edges with the duration of the target note and to extend the test corpus. Furthermore we feel that the results can improve by incorporating more detailed musical features. 7 ACKNOWLEDGMENTS This research was supported by the FP6 IST project 5115722 PROFI and WITCHCRAFT NWO project 640-003501. The authors wish to thank Peter van Kranenburg (Universiteit Utrecht) and Ellen van der Grijn (Meertens Instituut). Many thanks also to Daniel M¨ullensiefen and Klaus Frieler for making available the SIMILE package, which includes Opti3. 8 REFERENCES [1] Onder de groene linde. Uitgeverij Unipers, 19871991. [2] B´ela Bollob´as. Modern graph theory. SpringerVerlag, New York, 1998. [3] M.F. Demirci, R.H. van Leuken, and R.C. Veltkamp. Indexing through laplacian spectra. Computer Vision and Image Understanding, To appear, 2007. [4] C.D. Godsil and B.D. McKay. Constructing cospectral graphs. In Aequationes Mathematicae, pages 257– 268, 1982. [5] Chris Godsil and Gordon Royle. Algebraic Graph Theory, volume 207 of Graduate Texts in Mathematics. Springer Verlag, 2001. [6] W. H. Haemers and E. Spence. Enumeration of cospectral graphs. Eur. J. Comb., 25(2):199–211, 2004. [7] Meertens Instituut. Nederlandse liederenbank. http://www.liederenbank.nl/. [8] P. Leonardo. A Graph Topological Representation of Melody Scores. Leonardo Music Journal, 12(1):33– 40, 2002. [9] B. Mohar. The laplacian spectrum of graphs. In Sixth International Conference on the Theory and Applications of Graphs, pages 871–898, 1988. [10] D. M¨ullensiefen and K. Frieler. Optimizing measures of melodic similarity for the exploration of a large folk song database. In Proc. of ISMIR, 2004. [11] Alberto Pinto and Goffredo Haus. A novel xml music information retrieval method using graph invariants. ACM Transactions on Information Systems, To appear, 2007. [12] A. Shokoufandeh, D. Macrini, S. Dickinson, K. Siddiqi, and S.W. Zucker. Indexing hierarchical structures using graph spectra. Pattern Analysis and Machine Intelligence, 27(7), 2005. [13] Rainer Typke, Frans Wiering, and Remco C. Veltkamp. Transportation distances and human perception of melodic similarity. Musicae Scientiae, Discussion Forum 4A, 2007 (special issue on similarity perception in listening to music), p. 153-182.",
        "zenodo_id": 1416744,
        "dblp_key": "conf/ismir/PintoLDWV07",
        "keywords": [
            "content-based music retrieval",
            "melodies",
            "graph spectral approach",
            "laplacian spectrum",
            "database indexing",
            "range searching",
            "query spectrum",
            "feature vector",
            "experimental evaluation",
            "test corpus"
        ],
        "content": "INDEXING MUSIC COLLECTIONS THROUGH GRAPH SPECTRA\nAlberto Pinto*, Reinier H. van Leuken, M. Fatih Demirci, Frans Wiering, Remco C. Veltkamp\nDepartment of Information and Computing Sciences - Universiteit Utrecht (The Netherlands) and\n* Dipartimento di Informatica e Comunicazione - Universit `a degli Studi di Milano (Italy)\npinto@dico.unimi.it, {reinier, mdemirci, Frans.Wiering, Remco.Veltkamp }@cs.uu.nl\nABSTRACT\nContent based music retrieval opens up large collections,\nboth for the general public and music scholars. It basically\nenables the user to ﬁnd (groups of) similar melodies, thus\nfacilitating musicological research of many kinds. We\npresent a graph spectral approach, new to the music re-\ntrieval ﬁeld, in which melodies are represented as graphs,\nbased on the intervals between the notes they are com-\nposed of. These graphs are then indexed into a database\nusing their laplacian spectra as a feature vector. This lapla-\ncian spectrum is known to be very informative about the\ngraph, and is therefore a good representative of the orig-\ninal melody. Consequently, range searching around the\nquery spectrum returns similar melodies.\nWe present an experimental evaluation of this approach,\ntogether with a comparison with two known retrieval tech-\nniques. On our test corpus, a subset of a well documented\nand annotated collection of Dutch folk songs, this eval-\nuation demonstrates the effectiveness of the overall ap-\nproach.\n1 INTRODUCTION\nSinging songs has always been an important way of pass-\ning on stories and expressing emotions, religious beliefs\nor social values. Most of these folk songs were trans-\nferred orally, often signiﬁcantly changing over time and\nlocation. This resulted in the existence of many versions\nof the same songs, often displaying considerable varia-\ntions. In Onder de groene linde , a collection of Dutch folk\nsongs has been assembled by Ate Doornbosch, a Dutch\nradio broadcaster and researcher [1]. By recording many\nsingers in the countryside during a period of over three\ndecades, he captured this cultural heritage counting more\nthan 7300 songs on tape. A large part of these melodies\nand songs has now been transcribed to music notation.\nThe collection is becoming available to the general pub-\nlic and research community [7]. Content-based music re-\ntrieval opens up this great resource in such a way that both\naudiences can access it better. Through music information\nretrieval, songs belonging to the same class of songs can\nbe grouped, or songs with only slight variations can be\nfound. It can help identify the composer of a song, or as-\nc/circlecopyrt2007 Austrian Computer Society (OCG).sist in any other scholarly musicological task.\nWe have three main contributions in this paper. Firstly,\nwe introduce a new approach to music retrieval in which\nthe music is represented as graphs, and the matching is\nbased on speciﬁc features of these graphs. Our graph rep-\nresentation encodes the interval structure of a melody; it\nis a global time-independent signature of the melody, dis-\nplaying the network of connections that exists between the\npitch classes.\nSecondly, we introduce our indexing approach, which\nis new to music retrieval. To compute similarity between\nmelodies, an algebraic structure is associated to each graph:\nann×nmatrix, with nequal to the number of vertices in\nthe graph.\nThirdly, we evaluate our method on a test corpus of\nDutch folk songs. In this evaluation, we compare our\nmethod to two other methods: one approach speciﬁcally\ntargeted towards folk song collections, and one approach\nusing a time-independent structural approach as well. In\nthis comparison, our method outperforms the other meth-\nods in terms of three well-known performance measures,\nnamely, nearest neighbor, ﬁrst and second tier.\n2 RELATED WORK\nMelodic similarity has been investigated by many authors\nfrom very different points of view, using different kind of\nsong collections as dataset.\nOne of the most complete and recent studies has been\nperformed by M ¨ullensiefen and Frieler, who explored the\nconcept melodic similarity within a collection of folk songs\n[10]. Using a collection of 577 Luxembourg folk songs,\nthey empirically established an optimal similarity measure\n(the Opti3) that combines several known methods into one\nunifying expression. Out of 50 implemented musical sim-\nilarity measures, taking into account all sorts of musical\nfeatures, a weighted combination of methods was cho-\nsen to create one measure that best reﬂected the results\nof an extensive human listening experiment. Since their\napproach is speciﬁcally targeted towards a collection of\nfolk songs, we compare it to our method as well.\nAnother example of a representation/matching/index-\ning paradigm is the weighted point set on which the Earth\nMover’s Distance or Proportional Transportation Distance\ncan be applied. This kind of approach has been applied to\ntest music similarity as well [13]. The notes of a melodyare encoded as weighted points in a two-dimensional space\nwhere pitch and onset time are the axes; the duration of a\nnote determines its weight. Similarity between two melodies\ncan now be computed by measuring the effort it takes to\ntransform one weighted pointset into the other.\n3 REPRESENTATION\nOur goal is to provide a sufﬁciently abstract representa-\ntion of a melodic line that actually makes sense from a\nmusical point of view. With this aim, we start looking\njust at melodies, not considering the rhythm. Melodies\nare generally studied from a pitch sequence/contour point\nof view. Our approach is different: we take as a starting\npoint the interval structure, by which we mean the net-\nwork of connections between pitches. We remark that\nmelodies use only a subset of all possible connections,\nand with different frequencies. To model such relation-\nships we use graphs, which have various and signiﬁcant\napplications throughout mathematics, computer science,\nand physics. As such, the graph is a projection of the\ntime-dependent concept of melody to a time-independent\nconcept of intervallic structure. The next level of abstrac-\ntion is to leave out pitch class information so that only\nthe “interval connectivity” of the melody remains, and this\nmeans that certain operations such as inversion, transpo-\nsition, retrogradation, other kind of permutations in the\npitch class set and (some) shifting of fragments does not\naffect the graph. In this perspective what we are modelling\nis a global, time-independent signature of the melody [11],\n[8]. Melodies that display a similar interval behaviour\nhave similar graphs, for example melodies in which there\nare one or two central notes (with many connections) and\na number of peripheral notes (few connections).\nLetMbe a melodic sequence of length m=|M|and\nconsider the sequence of pitches {pj}j∈I,{I= 1, ..., m}.\nThen let V=Z12be the (metric) space of pitches, or\npitch classes, in the 12-tone system. We deﬁne the graph\nGwith vertex set VG=Vand edge set whose elements\nare the edges ajsuch that\naj:/braceleftbiggpj→pj+1 for every couple (pj, pj+1)⊆M\npm→p1 for the couple (pm, p1)\nwhere j= 1, . . . , m −1(see also [2] and [5]).\nThe arrow am:pm→p1does not represent an actual\ninterval in the melody but it has been added for symmetry\nreasons and in order to take into account the relationship\nbetween the last and the ﬁrst note as well, which otherwise\nwould not have been reﬂected in the model.\n4 INDEXING\nThe graph representation described up to now is a geo-\nmetric one. In order to allow computations with this rep-\nresentation, we need to associate an algebraic structure to\nit. The most common algebraic structure to represent a\ngraph is the adjacency matrix.The adjacency matrix A(G)of a graph Gis a square\nmatrix of size equal to the order of the graph and where\nthe entry (i, j)represents the number of oriented edges\nfrom vertex ito vertex j. This adjacency matrix therefore\ncontains all the information to reconstruct the connectivity\nof the graph. A matrix closely related to the adjacency\nmatrix is the laplacian matrix L(G), computed as L(G) =\nD(G)−A(G), where D(G)is the degree matrix of G. The\ndegree matrix is also a square matrix of size equal to the\norder of the graph, but all values are zero except for those\non the main diagonal. Here, the entry (i, i)represents the\nnumber of outgoing edges of vertex i.\nGiven the laplacian matrix of a melody graph, the ques-\ntion remains how to compute the similarity to another melo-\ndy. For this purpose, we ﬁrst compute the eigenvalues\nof the laplacian matrix and sort them by magnitude.1\nHereby, we obtain the laplacian spectrum of the graph,\nthat is known to reﬂect a number of important proper-\nties of the graph. These properties include the diame-\nter (related to the second smallest eigenvalue), mean dis-\ntance, minimum degree and algebraic connectivity. Fur-\nthermore, the spectrum is invariant under permutations of\nthe matrix (i.e. swapping columns or rows). Together\nwith the absence of pitch information stored in the matrix,\nthis makes the representation invariant under transposi-\ntions and note permutation. This is an important property,\nbecause as pointed out before, our concept of similarity\nis also independent from note permutation and transposi-\ntion.\nOur main motivation for encoding the topology of a graph\nusing the laplacian matrix comes from the fact that lapla-\ncian matrices are more natural, more important, and more\ninformative than other matrices about the input graphs [9].\nPreviously, Godsil and McKay [4] and more recently Hae-\nmers and Spence [6] have also shown that the laplacian\nmatrix has more representational power than the adjacency\nmatrix, in terms of resulting in fewer cospectral graphs.\nRecall that two graphs are called cospectral (or, isospec-\ntral) if they have the same eigenvalues.\nGiven a query graph and a large database, the objective\nof an indexing algorithm is to efﬁciently retrieve a small\nset of candidate matches, that share topological similarity\nwith the query. As pointed out, we encode the topology of\na graph through its laplacian spectrum, which is used as\na signature for the database object. This spectrum can be\nseen as a point in a high dimensional space. To compute\nsimilarity between two graphs, we compute the Euclidean\ndistance between their signatures, which is inversely pro-\nportional to the structural similarity of the graphs. There-\nfore, for a given query, retrieving its similar graphs can be\nreduced to a nearest neighbor search among a set of points.\nA set of candidate matches can now be found without hav-\ning to inspect the entire database. For more details on this\nindexing strategy, the reader is referred to [3].\n1Since the graphs are directed, the laplacian matrix is not necessar-\nily symmetric. Consequently, some of the eigenvalues may be complex\nnumbers and there exist multiple strategies for sorting these. As in [12],\nwe sort these eigenvalues by modulus.CRITERIA NN 1sttier 2ndtier\nLAPLACIAN 66% 44% 63%\nADJACENCY 58% 28% 48%\nOPTI3 40% 39% 56%\nEMD 64% 33% 50%\nPTD 64% 30% 46%\nTable 1 . Nearest neighbour (NN), ﬁrst tier and second tier\nresults on the Onder de groene linde collection, computed\nusing Laplacian spectra (L) Adjacency spectra (A) of the\ngraphs. The results are compared to the methods Opti3,\nEMD and PTD.\n5 EXPERIMENTS\nIn “Onder de groene linde”, a large number of Dutch folk\nsongs is preserved. This collection consists of more than\n7300 songs recorded on tape. These songs are documented\nand annotated in great detail, and illustrated by sheet mu-\nsic examples. We experimented on a subset of this re-\nsource, that consists of 141 songs, of which we used the\nﬁrst phrase. These songs have been classiﬁed in 18 classes\normelody groups , that relate to the concept of melody\nnorms .\nAt the Meertens Institute (a research institute for Dutch\nlanguage and culture in Amsterdam) the concept of melody\nnorm2is used to group historically or “genetically” re-\nlated, orally transmitted melodies. Because the contents\nof folk song collections such as OGL are highly frag-\nmented, it is impossible to trace back the history of melo-\ndies and to ﬁnd all variants that are derived from a com-\nmon ‘ancestor’ melody. What can be done, is to ﬁnd re-\nlated groups of melodies within the collection, based on\nboth melodic similarity and available meta data, and link\nthem to melody norms. A search engine would speed\nup this process of relating melodies considerably. As a\nground truth in our experiments, we used a classiﬁcation\nof the melodies into melody groups , that serve as candi-\ndates for the melody norms to be assigned in a later stage.\nFor all the melodies in our test corpus, a graph has been\nconstructed as described in Section 3. We evaluated re-\ntrieval performance with these graphs using both the ad-\njacency and the laplacian spectra. The results are sum-\nmarised in Table 1. For both experiments, we computed\nsome retrieval statistics, namely nearest neighbor, ﬁrst and\nsecond tiers, each averaged over all possible queries. These\nare frequently used in information retrieval.\nThe ﬁrst ﬁgure is the percentage of correct nearest neigh-\nbors (NN) , i.e. the number of cases in which the top ranked\ndatabase item, discarding the query itself, belongs to the\nsame class as the query. We also computed the ﬁrst tier ,\ni.e. how many melodies of the query’s class are returned\nwithin the ﬁrst K−1matches, where Kis the size of the\nquery class. A similar performance ﬁgure is the second\ntier, i.e. how many melodies of the query’s class are re-\nturned within the ﬁrst 2(K−1)matches. The laplacian\n2Equivalent with “tune family” and “Melodietyp”.spectral method performs best with a NN score of 66%, a\n1sttier score of 44% and a 2ndtier score of 63%.\nAlthough these performance ﬁgures show in general\nthe efﬁcacy of the method, there are some interesting cases\nin particular we would like to point out here. In Figure 1\nthere is a special case of an “almost false” positive: for\nquery OGL19205 (belonging to “Heer Halewijn - 3rdver-\nsion”), the nearest neighbor is OGL19107, that belongs to\nthe group “Heer Halewijn - 4thversion”. However, the\nnearest neighbor is somehow related to the query; coin-\ncidentally they share the same graph representation, as is\nshown in Figure 3. This example shows how two melodies\ncan be identical from the interval connectivity point of\nview but can also be perceptually quite different. This may\nrepresent the main limitation of this method in perceptual\nsimilarity tasks. The second example (Figure 2) shows the\nnearest neighbors for the query song OGL19406. Both ex-\namples may suggest also that in the case of folksongs peo-\nple tend to remember more the interval connectivity than\nthe actual intervals of the melody.\nFurthermore, we experimented with weighting the ed-\nges based on the interval they represent. For this purpose,\ntwo different sets of weights were used: one reﬂecting the\ndifference in notes on the chromatic scale (ignoring differ-\nences in octaves) and one reﬂecting the harmonics of the\ninterval, giving larger weights to consonant intervals and\nsmaller weights to dissonant intervals. During this round\nof experiments, these methods did not improve the results\nobtained with normal laplacian spectra.\nUsing the same test corpus and performance measures\nwe compared our method to the optimal distance mea-\nsure that was established by M ¨uellensiefen and Frieler\n[10]. These results are also presented in Table 1, under the\nname Opti3. This distance measure is a weighted combi-\nnation of three distance measures, each working on dif-\nferent feature sets. These measures are harmcore (using\nharmonic correlation), rhythfuzz (using fuzziﬁed rhythm\nvalues) and ngrukkon (taking into account characteristic\nmotives). This combined distance measure was estab-\nlished empirically out of 50 building blocks, by search-\ning for a weighted combination whose performance best\nreﬂected the results of an extensive human listening ex-\nperiment. Consequently, this method has been ﬁtted to\nthe data set at hand, explaining why the results are not op-\ntimal in our experiment. We also compared our method\nto the Earth Mover’s Distance (EMD). This distance mea-\nsure takes two weighted point sets as input, and measures\nthe minimum amount of work needed to transform one\ninto the other by moving weight. The EMD is used in\na number of different contexts; in the musical case, as\npointed out in [13], the (2 dimensional) weighted point\nset is represented by the score itself, where the weight as-\nsigned to each note is its duration. However, since our\nmethod only takes into account the global melodic struc-\nture, we projected the weighted points on the pitch axis\nprior to computing the transportation distances. The “Pro-\nportional Transportation Distance” (PTD) is a modiﬁca-\ntion of the EMD in order to get a similarity measure basedon weight transportation such that the surplus of weight\nbetween two point sets is taken into account.\nFigure 1 . Example of false positive for the query song\n“Heer Halewijn” ( 3rdversion) OGL19205 with its NN,\nOGL19107, instance of “Heer Halewijn” ( 4thversion).\nFigure 2 . Example of true positive for the query song\n“In Frankrijk buiten de poorten” ( 2ndversion) OGL19406\nwith its NN, OGL41709.\nFigure 3 . Graph representation of the folk songs\nOGL19205 and OGL19107 (see Figure 1). The two let-\nters in each circle represent the pitch classes respectively\nin the ﬁrst and in the second song.\n6 CONCLUDING REMARKS\nWe presented a graph spectral approach that is new to mu-\nsic retrieval. Our method is focussed on the intervallic\nstructure of the melody. This structure is encoded in a\ngraph whose vertices correspond to the 12 pitch classes\nand whose edges reﬂect the interval sequence of the melody;\nan edge is added to the graph if the pitch classes of the cor-\nresponding vertices appear consecutively in the melody.\nThe graphs are indexed into a database using their lapla-\ncian spectra, a feature vector that reﬂects the original topol-\nogy and graph structure to a large extent.\nWe evaluated our approach using a subset of a large\ncollection of Dutch folk songs. On this test corpus, our\nmethod clearly outperforms existing methods. It is our in-\ntention to investigate this method further, for instance by\nweighting the edges with the duration of the target note\nand to extend the test corpus. Furthermore we feel that the\nresults can improve by incorporating more detailed musi-\ncal features.7 ACKNOWLEDGMENTS\nThis research was supported by the FP6 IST project 511572-\n2 PROFI and WITCHCRAFT NWO project 640-003501.\nThe authors wish to thank Peter van Kranenburg (Univer-\nsiteit Utrecht) and Ellen van der Grijn (Meertens Insti-\ntuut). Many thanks also to Daniel M ¨ullensiefen and Klaus\nFrieler for making available the SIMILE package, which\nincludes Opti3.\n8 REFERENCES\n[1]Onder de groene linde . Uitgeverij Unipers, 1987-\n1991.\n[2] B ´ela Bollob ´as. Modern graph theory . Springer-\nVerlag, New York, 1998.\n[3] M.F. Demirci, R.H. van Leuken, and R.C. Veltkamp.\nIndexing through laplacian spectra. Computer Vision\nand Image Understanding , To appear, 2007.\n[4] C.D. Godsil and B.D. McKay. Constructing cospec-\ntral graphs. In Aequationes Mathematicae , pages 257–\n268, 1982.\n[5] Chris Godsil and Gordon Royle. Algebraic Graph\nTheory , volume 207 of Graduate Texts in Mathemat-\nics. Springer Verlag, 2001.\n[6] W. H. Haemers and E. Spence. Enumeration of\ncospectral graphs. Eur. J. Comb. , 25(2):199–211,\n2004.\n[7] Meertens Instituut. Nederlandse liederenbank.\nhttp://www.liederenbank.nl/.\n[8] P. Leonardo. A Graph Topological Representation of\nMelody Scores. Leonardo Music Journal , 12(1):33–\n40, 2002.\n[9] B. Mohar. The laplacian spectrum of graphs. In Sixth\nInternational Conference on the Theory and Applica-\ntions of Graphs , pages 871–898, 1988.\n[10] D. M ¨ullensiefen and K. Frieler. Optimizing measures\nof melodic similarity for the exploration of a large folk\nsong database. In Proc. of ISMIR , 2004.\n[11] Alberto Pinto and Goffredo Haus. A novel xml music\ninformation retrieval method using graph invariants.\nACM Transactions on Information Systems , To appear,\n2007.\n[12] A. Shokoufandeh, D. Macrini, S. Dickinson, K. Sid-\ndiqi, and S.W. Zucker. Indexing hierarchical structures\nusing graph spectra. Pattern Analysis and Machine In-\ntelligence , 27(7), 2005.\n[13] Rainer Typke, Frans Wiering, and Remco C.\nVeltkamp. Transportation distances and human per-\nception of melodic similarity. Musicae Scientiae, Dis-\ncussion Forum 4A, 2007 (special issue on similarity\nperception in listening to music), p. 153-182."
    },
    {
        "title": "Meaningfully Browsing Music Services.",
        "author": [
            "Tim Pohle",
            "Peter Knees",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417777",
        "url": "https://doi.org/10.5281/zenodo.1417777",
        "ee": "https://zenodo.org/records/1417777/files/PohleKSW07.pdf",
        "abstract": "We present a browser application that offers the user an enhanced access to the content of music web services. Most importantly, the technique we apply aims at making it feasible to add to the automated suggestion of similar artists some intentional spin, or direction. At the heart of the algorithm, automatically derived artist descriptions are analyzed for common topics or aspects, and each artist is described by the extent to which it is associated with each of these topics. The browser application enables the user to formulate a query by means of these underlying topics by simply adjusting slider positions. The best matching artist is shown, and its web page found on the web music service is displayed. 1 INTRODUCTION Today, it is common for online music shops and music web services to have functions for convenient browsing and discovery of new music. One of these functions is the suggestion of similar artists. Such a similar artists function is quite useful, but it has the drawback that the process of discovering new artists may turn into a trial-and-error process, as it may not be immediately obvious if the suggested artists are closer to the user’s particular likes (or dislikes). Recently, we have suggested a procedure to describe artists by the aspects (or topics) they are commonly associated with [1]. Based on such a description, it is possible to give the similar artist suggestions an intentional spin, or direction. In the demonstration presented here, we have implemented a browser application that enables the user to access the content of a music web service with this technique. The recommendation tools offered by the music service’s web interface are also available, so that the application allows the user to get an overall impression how the new technique could complement existing approaches. 2 RELATED WORK There are music recommendation services (such as musiclens.de) that allow for browsing music by adjusting slider c⃝2007 Austrian Computer Society (OCG). values. The main difference is that the approach presented here automatically derives the browsable categories from the given data, thus no time-consuming manual annotation is necessary. Also, our approach does not work on the track level but on the artist level. 3 ALGORITHM In this section, we give an overview of the implemented procedure. Details can be found in [1]. The outcome of the procedure when applied to the data used in this demonstration is given in the next section. The outline of the implemented procedure is as follows: 1) Obtain artist descriptions. For each artist that is in the repository, a description is automatically extracted from the web. This is done by querying a web search engine with +“artist name” +music + review. The found pages are analyzed for the occurrence of particular music-related words, and a TF×IDF vector is calculated [2]. The output of this step is a long list of words associated with each artist. Each of the words has a weight associated. 2) Analyse artist descriptions for common properties. The artist descriptions (long vectors of weighted terms) are compressed to few – ideally meaningful – concepts that make a high-level interaction feasible. For this step, we apply Non-Negative Matrix Factorization (NMF) to yield eight main concepts [3]. The input of NMF are the TF×IDF vectors of all artists, and the most important output are eight vectors that allow a projection of each highdimensional artist vector down to eight dimensions. 3) Represent each artist as a mixture of the common properties. This means to apply the transformation calculated in the previous step to obtain a compressed representation for each artist. After this transformation, each artist is described by the extent to which it is associated with each of the eight concepts. The last step above yields a vector for each artist. The user query also is represented by a vector of the same length, so it is possible to calculate a similarity between the query vector and each artist by applying the cosine similarity measure. The user query can be seen as a usergenerated artist description. Figure 1. Demonstration: The music service browser. Each artist is represented as a vector giving the extent to which this artist is associated with each of the categories represented as sliders on the top. When the user selects an artist (via Quick Selection), its representation is transferred to the sliders and its last.fm web page is shown. When the user modifies the slider positions, the suggested artists (in the Matches box) are changed and a different artist’s page is shown. 4 IMPLEMENTATION For the demonstration, we use list of more than 2.000 artist names that have associated web pages on the music web service last.fm 1 . For each of these artists, up to 100 web pages are retrieved and analyzed as described in Section 3. The artist data is analyzed for eight concepts. For each of the found concepts, the terms with the highest weights are given in Table 1. In the browser application, each of these categories is represented by a slider that is labelled with a manually defined description of the respective concept (Figure 1).",
        "zenodo_id": 1417777,
        "dblp_key": "conf/ismir/PohleKSW07",
        "keywords": [
            "browser application",
            "enhanced access",
            "music web services",
            "similar artists",
            "intentional spin",
            "automatically derived artist descriptions",
            "Non-Negative Matrix Factorization (NMF)",
            "cosine similarity measure",
            "last.fm web page",
            "slider positions"
        ],
        "content": "MEANINGFULLYBROWSING MUSICSERVICES\nTim Pohle1,Peter Knees1,Markus Schedl1andGerhard Widmer1,2\n1)Dept. ofComputationalPerception\nJohannesKeplerUniversity,Linz,Austria\n2)AustrianResearch InstituteforArtiﬁcial Intelligence(O FAI)\nVienna, Austria\nABSTRACT\nWe present a browser application that offers the user an\nenhanced access to the content of music web services.\nMost importantly,thetechniquewe applyaimsat making\nit feasible to add to the automated suggestion of similar\nartists some intentional spin, or direction. At the heart of\nthealgorithm,automaticallyderivedartistdescriptions are\nanalyzed for common topics or aspects, and each artist is\ndescribedbytheextenttowhichitisassociatedwitheach\nof these topics. The browser application enables the user\nto formulate a query by means of these underlyingtopics\nby simply adjusting slider positions. The best matching\nartist is shown, and its web page found on the web music\nserviceisdisplayed.\n1 INTRODUCTION\nToday, it is common for online music shops and music\nweb services to have functions for convenient browsing\nanddiscoveryofnewmusic. Oneofthesefunctionsisthe\nsuggestion of similar artists. Such a similar artists func-\ntionisquiteuseful,butithasthedrawbackthattheprocess\nof discovering new artists may turn into a trial-and-error\nprocess, as it may not be immediately obviousif the sug-\ngested artists are closer to the user’s particular likes (or\ndislikes). Recently, we have suggested a procedureto de-\nscribeartistsbytheaspects(ortopics)theyarecommonly\nassociatedwith[1]. Basedonsuchadescription,itispos-\nsible to give the similar artist suggestions an intentional\nspin, or direction. In the demonstration presented here,\nwe have implemented a browser application that enables\ntheusertoaccessthecontentofamusicwebservicewith\nthis technique. The recommendationtools offered by the\nmusic service’s web interface are also available, so that\nthe application allows the user to get an overall impres-\nsion how the new technique could complement existing\napproaches.\n2 RELATED WORK\nThere are music recommendationservices (such as musi-\nclens.de)thatallowforbrowsingmusicbyadjustingslider\nc/circlecopyrt2007AustrianComputerSociety(OCG).values. Themaindifferenceisthattheapproachpresented\nhere automatically derives the browsable categories from\nthe given data, thus no time-consuming manual annota-\ntion is necessary. Also, our approach does not work on\nthetracklevelbutontheartist level.\n3 ALGORITHM\nIn this section, we give an overview of the implemented\nprocedure. Details can be found in [1]. The outcome of\ntheprocedurewhenappliedtothedatausedinthisdemon-\nstration is given in the next section. The outline of the\nimplementedprocedureisasfollows:\n1) Obtain artist descriptions. For each artist that is\nin the repository, a description is automatically extracte d\nfrom the web. This is done by querying a web search\nengine with +“artist name ” +music + review . The\nfound pages are analyzed for the occurrenceof particular\nmusic-related words, and a TF×IDFvector is calculated\n[2]. The output of this step is a long list of words asso-\nciated with each artist. Each of the words has a weight\nassociated.\n2) Analyse artist descriptions for common properties.\nThe artist descriptions (long vectors of weighted terms)\nare compressed to few – ideally meaningful – concepts\nthat make a high-level interaction feasible. For this step,\nwe apply Non-Negative Matrix Factorization (NMF) to\nyield eight main concepts [3]. The input of NMF are the\nTF×IDFvectorsofallartists,andthemostimportantout-\nput are eight vectorsthat allow a projectionof each high-\ndimensionalartist vectordowntoeightdimensions.\n3) Represent each artist as a mixture of the common\nproperties. Thismeansto applythe transformationcalcu-\nlatedintheprevioussteptoobtainacompressedrepresen-\ntationforeachartist. Afterthistransformation,eachart ist\nis described by the extent to which it is associated with\neachoftheeightconcepts.\nThe last step aboveyields a vector for each artist. The\nuser query also is represented by a vector of the same\nlength, so it is possible to calculate a similarity between\nthe query vector and each artist by applying the cosine\nsimilarity measure. The user querycan be seen as a user-\ngeneratedartist description.Figure 1. Demonstration: The music service browser.\nEach artist is represented as a vector giving the extent to\nwhich this artist is associated with each of the categories\nrepresented as sliders on the top. When the user selects\nan artist (via Quick Selection ), its representation is trans-\nferred to the sliders and its last.fmweb page is shown.\nWhentheusermodiﬁesthesliderpositions,thesuggested\nartists (in the Matchesbox) are changed and a different\nartist’spageisshown.\n4 IMPLEMENTATION\nFor the demonstration, we use list of more than 2.000\nartist names that have associated web pages on the mu-\nsic web service last.fm1. For each of these artists, up to\n100 web pagesare retrievedand analyzed as described in\nSection 3. The artist data is analyzed for eight concepts.\nForeachofthefoundconcepts,thetermswiththehighest\nweights are given in Table 1. In the browser application,\neach of these categories is represented by a slider that is\nlabelledwithamanuallydeﬁneddescriptionoftherespec-\ntiveconcept(Figure1).\n4.1 Browser Usage\nWhen using the browser, the user may immediately se-\nlect an artist that is known to her by selecting this artist\nin theQuick Selection box. When doing so, the artist’s\npageonthewebserviceisdisplayed,andtheinternalrep-\nresentationoftheselectedartististransferredtothesli der\npositions. Then,the usermayincrease ordecreasethe in-\nﬂuenceofeach aspect onthe artist suggestion. Forexam-\nple, in Figure1, the chosenseed artist was Metallica ,and\nthe amountof Dark Metal hasbeenincreased. The artists\nthatbestmatchthecurrentsliderpositionsarelistedinth e\nMatchesbox. Whenthe suggestedartistschangeduringa\nslider adjustment, both the content of the Matchesbox is\n1To obtain the artist names, weused audioscrobbler.netprogressivemetal idm blackmetal\ndreamtheater anticon deicide\npowermetal warpmorbidangel\nprogmetal rephlex blackmetal\nprogressiverock bjork dismember\npoprock composers rappers\nmadonna research gangsta\nrockandroll theatrical drdre\nsingersongwriter newyork rap\nsongwriter horror hiphop\ntrance screamo\nprogressivetrance emo\nprogressivehouse punkrock\npauloakenfold epitaph\nbreakbeat hardcore\nTable 1. Theeightcategoriesthatarefoundfortheartists\navailable in the demo application. For each category\nthe ﬁve terms with the largest weight are given. In the\nbrowser,eachcategoryisrepresentedbyoneslider.\nupdated, and the page associated with the currently best-\nmatchingartist isshown.\n5 CONCLUSIONS\nWe present a demonstration application that implements\na new approach for browsing music services and ﬁnding\nsimilar artists in a directedway. Based on the demonstra-\ntion, the user can get an impression how the new tech-\nniquemaycomplementexistingrecommendationtoolsby\noffering an additional way for high-level interaction with\na musicrepository.\n6 ACKNOWLEDGEMENTS\nThisresearchissupportedbytheAustrianFondszurF¨ order -\nungderWissenschaftlichenForschung(FWF)underproject\nnumberL112-N04,andbythe EU6thFP projectS2S2\n(“SoundtoSense,SensetoSound”,IST-2004-03773).The\nAustrian ResearchInstitute forArtiﬁcial Intelligenceal so\nacknowledges ﬁnancial support by the Austrian Federal\nMinistriesBMBWK andBMVIT.\n7 REFERENCES\n[1] T. Pohle, P. Knees, M. Schedl, and G. Widmer,\n“Building an interactive next-generationartist recom-\nmender,”in Proc.5thCBMI,2007.\n[2] B.WhitmanandS.Lawrence,“InferringDescriptions\nandSimilarityforMusicfromCommunityMetadata,”\ninProc.Intern.ComputerMusicConference ,2002.\n[3] WeiXu,XinLiu,andYihongGong,“DocumentClus-\ntering Based On Non-negative Matrix Factorization,”\ninProc.SIGIR03 ,Toronto,Canada,2003."
    },
    {
        "title": "Musical Memory of the World Data Infrastructure in Ethnomusicological Archives.",
        "author": [
            "Polina Proutskova"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416316",
        "url": "https://doi.org/10.5281/zenodo.1416316",
        "ee": "https://zenodo.org/records/1416316/files/Proutskova07.pdf",
        "abstract": "Ethnomusicological archives build the musical memory of the world, covering the geographical and the historical aspects of music worldwide. This article gives a brief description of the nature and the functionality of ethnomusicological archives. It reflects the current state of data infrastructure (policy and technology), addressing issues of access to archives’ holdings, of online visibility of music collections and of interoperability between archives. An outlook of a mutual involvement and a resulting influence at each others work between MIR community and ethnomusicological archives is given.",
        "zenodo_id": 1416316,
        "dblp_key": "conf/ismir/Proutskova07",
        "keywords": [
            "Ethnomusicological archives",
            "musical memory",
            "geographical",
            "historical aspects",
            "worldwide",
            "nature",
            "functionality",
            "data infrastructure",
            "access",
            "online visibility"
        ],
        "content": "MUSICAL MEMORY OF THE WORLD  –  DATA \nINFRASTRUCTURE IN ETHNOMUSICOLOGICAL ARCHIVES \n Polina Proutskova  \n Goldsmiths,  \nUniversity of London, \nComputing department   \nABSTRACT \nEthnomusicological archives build the musical memor y \nof the world, covering the geographical and the his torical \naspects of music worldwide.  \n    This article gives a brief description of the n ature and \nthe functionality of ethnomusicological archives. I t \nreflects the current state of data infrastructure ( policy and \ntechnology), addressing issues of access to archive s’  \nholdings, of online visibility of music collections  and of \ninteroperability between archives.  \n    An outlook of a mutual involvement and a result ing \ninfluence at each others work between MIR community  \nand ethnomusicological archives is given. \n1. ETHNOMUSICOLOGICAL ARCHIVES \nResearch on non-Western music is underrepresented i n \nMIR research today. One of the main reasons for it \naccording to Downie [1] is that the music data is n ot \navailable, incomplete or non-standard: \n„we do not yet have comprehensive recording sets of \nAfrican tribal songs nor Inuit throat music  “ [1], p. 303  \n   It means that the MIR community has no access to  the \nrespective field recordings. These recordings exist1 – but \nthey are hidden in the archives. \nEthnomusicological archives build a systematic, wel l \ndocumented repository of music recordings, covering  \ngeographically all regions of the world, often up t o a \nmicro scale of a single village, and historically r eaching \nin some cases as far back as the end of the 19 th century. \nThey are what is left of our musical memory, since many \noral music traditions are fading away due to changi ng \nenvironments, languages and cultural influences 2.          \n    Ethnomusicological archives hold recordings on all \ndifferent kinds of media: wax cylinders, all kinds of gra-\nmophone records, tapes, minidisks, compact disks, a nd \nhard disk recordings. The media are sometimes in a bad \ncondition and must be restored. The playback qualit y of \nrare early recordings is sometimes very poor. The m edia \n                                                           \n1 See e.g. recordings by Jean-Jacques Nattiez for Inu it throat singing. \nThere are numerous collections of recordings from d ifferent African \ncultures, see e.g. [2] p4 for Vienna Phonogram Arch ive \n2 for further discussion of the relevance of ethnomus icological \narchives today see [4] \n© 2007 Austrian Computer Society (OCG). \n  can be very sensitive to temperature and humidity, thus \nspecial conditions are needed to ensure their endur ance. \nMultiplication and recovering strategies have been \ndeveloped to guarantee the preservation of the cont ent 3. \n   Trying to capture as much of the social context of mu-\nsic making as possible, ethnomusicological archives  pre-\nserve beside the audio/video recordings of music, i nter-\nviews and environments also photos, field notes, bo oks \nand publications on music cultures, music instrumen ts, \ncultural and domestic objects brought from field tr ips.     \n \n Items Hours (incl. \ncommercial) Original \ncollections \nBerlin Phonogram \nArchive >150.000  18.000 >1.000 \nLibrary of Congress, \nArchive of Folk \nCulture  >100.000 >4.000 \nNational Sound Ar-\nchive (British Libra-\nry), World and Tra-\nditional Music section  300.000  370 \nArchives for Tradi-\ntional Music, Indiana \nUniversity 128.550 >250.000 2.000 \nTable 1 : Amount of recordings in leading \nethnomusicological archives  \n \n   Many archives are currently digitising their col lections. \nThe level of digitisation of audio data varies from  1% \n(National Sound Archive, London, original ethnomusi co-\nlogical collections) to 50% (Berlin Phonogram Archi ve). \nThe amount of attached accompanying information \n(metadata) is still considerably lower then that. \n   In contrast to recordings of classical or popula r music, \nrecordings in ethnomusicological archives cannot be  \nretrieved by composer name, because they are often \ntraditional, or by performer name, because the \nperformers are usually unknown to the searchers. Th e \nmain criterion to search for an ethnomusicological \nrecording is the cultural origin of the music. It a lso may \nbe the geographic place where the recording was mad e, \nthe language, the name of the collector, a social f unction \nor context (like a specific ritual), etc. Often the  searches \nare performed combining several criteria [6] p54. \n                                                           \n3 see  IASA-TC-03 report of the International Associa tion of Sound- \nand Audiovisual Archives    \n \n2. DATA INFRASTRUCTURE – POLICY AND \nTECHNOLOGY \nThe implementation of physical as well as of online  \naccess to recordings and to metadata differs signif icantly \nbetween archives and countries.  Restrictions are s ome-\ntimes caused by the possessive attitude of some \ncollectors and archivists. Another reason for restr icted \naccess can be a state policy (guarding the national  \nheritage). If the archive is part of a large instit ution, \ngeneral restrictive access policies of the institut ion are \nlikely to affect the work of the archive. \n    Yet the main reason to restrict the access to t he \nrecordings in the archives is a complicated proprie tary \nrights situation. The songs are often traditional ( no \ncomposer), the performers living far away from the \nWestern law space. The right to play and reproduce the \nrecording usually remains by the collector. Unfortu nate-\nly, field researchers are often reluctant to make t heir \nrecordings accessible in general, without them bein g \nasked for permission for each use. In many cases th e \nownership is unknown (orphan works) or the owner \ncannot be located, which also restrains archives fr om \nmaking recordings accessible.1 \n    Many archives catalogue their holdings electron ically, \nbut don’t make their catalogues available online, t he \nmost prominent examples being Berlin Phonogram \nArchive and The Archive of Folk Culture of Library of \nCongress. Yet there have been some promising \ndevelopments in the last years, see links [7]–[13] for \nexisting or soon-to-come online catalogues.  \n      The first networks of archives are being esta blished \ncurrently [11]–[13], thus there is an urgent need t o \nexchange, share and integrate metadata [6], p40. Th e \ndiversity of archives, of types of archived objects  and of \narchives’ metadata structures makes that a great \nchallenge. A certain amount of metadata standardisa tion \nbecomes inevitable, though it is hard for the archi ves to \nadmit and accept it. The Ethnographic Thesaurus pro ject \n[14] might offer the foundation for a common thesau rus \nfor ethnomusicological recordings. \n3. MIR AND ETHNOMUSICOLOGICAL ARCHIVES  \nMutual involvement would push the boundaries of bot h \nMIR research and the work of ethnomusicological \narchives. Analysis and retrieval of non-Western mus ic \nrecordings raises new MIR tasks: instead of searchi ng \nfor similarities, we might be more interested in de tecting \ndistinctive musical features specific to a music tr adition \nor a given collection; instead of genre classificat ion, \nwhich is meaningless across cultural borders, we mi ght \nwant to train classification engines to recognize \ncultural/geographical affiliation of recordings. Ed itorial \nmetadata like cultural origin or social context is often \nessential for retrieval and is usually well documen ted in \n                                                           \n1For further discussion of intellectual property rig hts and ethics for \nethnomusicological recordings see e.g. [5]  ethnomusicological collections. This may suggest it s \ncloser integration into MIR tools.  \n    MIR could offer automated annotation tools to t he \narchives: it would be useful to add information lik e song \nlength, playing instruments, number and gender of \nsingers, etc. automatically. MIR could also introdu ce \nnew search strategies to the archives, involving bo th \nmetadata and sound analysis. \n    Ethnomusicological archives could provide new t est \ndata sets for MIR algorithms containing difficult, non-\ncommon-practice music examples. Being public \norganizations devoted to collecting, maintaining an d \ndocumenting musics of the world, they in fact shoul d be \nour main partners in research issues. \n4. REFERENCES \n[1] Downie, J. Stephen. 2003. Music information \nretrieval. Annual Review of Information Science \nand Technology 37,  ed. Balise Cronin, 295-340. \nMedford, NJ: Information Today 2003  \n[2] Kowar, Helmut. Die Musikethnologischen \nBestände des Phonogrammarchivs , Das \naudiovisuelle Archiv  Nr45 , AGAVA, Vienna 1999  \n[3] Nettl, Bruno. The study of ethnomusicology: thirty-\none issues and concepts , University of Illinois \nPress, 2005 \n[4] Seeger, Anthony. The role of sound archives in \nethnomusicology today, Ethnomusicology , Spring/ \nSummer 1986, University of Illinois Press 1986 \n[5] Seeger, Anthony. Ethnomusicologists, archives, \nprofessional organisations, and the shifting ethics  of \nintellectual property, Yearbook for Traditional \nMusic , Vol. 28, 87-107, Los Angeles 1996  \nWeb references: \n[6] ethnoArc, Linked European archives for ethno-\nmusicological research,  D4, February 2007  \nhttp://www.ethnoarc.org/documents/D4%20Metadata%2 \n0Requirements%20and%20Specification.pdf   \n[7] WebFolk Project of the Bulgarian Academy of  \nSciences  http://arts.bas.bg/EN/Default.htm  \n[8] National Sound Archive of the British Library  \nhttp://cadensa.bl.uk/cgi-bin/webcat  \n[9] Archives for Traditional Music, Indiana University  \nhttp://www.iucat.iu.edu/authenticate.cgi?status=rem ote& \nselect1=WEBSERVER  \n[10] Alan Lomax Archive  http://www.lomaxarchive.com/  \n[11] Smithsonian Global Sound  \nhttp://www.smithsonianglobalsound.org/  \n[12] DISMARC  www.dismarc.org  \n[13] ethnoArc  http://www.ethnoarc.org/  \n[14] The Ethnographic Thesaurus project \nhttp://www.afsnet.org/thesaurus/"
    },
    {
        "title": "MAP Adaptation to Improve Optical Music Recognition of Early Music Documents Using Hidden Markov Models.",
        "author": [
            "Laurent Pugin",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415922",
        "url": "https://doi.org/10.5281/zenodo.1415922",
        "ee": "https://zenodo.org/records/1415922/files/PuginBF07.pdf",
        "abstract": "Despite steady improvement in optical music recognition (OMR), early documents remain challenging because of the high variability in their contents. In this paper, we present an original approach using maximum a posteriori (MAP) adaptation to improve an OMR tool for early typographic prints dynamically based on hidden Markov models. Taking advantage of the fact that during the normal usage of any OMR tool, errors will be corrected, and thus ground-truth produced, the system can be adapted in real-time. We experimented with five 16th-century music prints using 250 pages of music and two procedures in applying MAP adaptation. With only a handful of pages, both recall and precision rates improved even when the baseline was above 95 percent. 1 INTRODUCTION Optical music recognition (OMR) systems create encodings of the musical content in digital images automatically. For libraries, they constitute very promising solutions for building searchable digital libraries of previously inaccessible material, especially historical documents. Using OMR tools in large-scale digitisation project remains a challenge, however [4], mainly because of the inconsistent performance of most OMR tools. Learning-based approaches, such as the one adopted by Gamera [9], are well suited to such projects, but other approaches, e.g., coupling multiple recognisers, have also been considered recently [3]. When performing OMR on early music sources, one major problem is the extremely high variability exhibited in the data. In printed documents, the font shape may vary considerably from one print to another, and the printing techniques of the time as well as the texture of the paper used resulted in frequent printing irregularities. The physical documents are often degraded, introducing various kinds of noise, and the scanning settings (e.g., brightness or contrast) are not necessarily consistent across all documents. Five examples of 16th-century music prints we used for this study, shown in Figure 1, demonstrate c⃝2007 Austrian Computer Society (OCG). the variability in font shape, document degradation, and scanning parameters. Given such a range of documents, there is no guarantee that an OMR system can be trained to perform well on new document, even with a learningbased approach. Furthermore, as a considerable amount of labelled data is required to train a sufficiently reliable system [10], building a system from scratch for every new document encountered is not practical. Similar problems have been encountered previously in speech recognition, where the amount of data and time needed to build a recogniser is considerable. One common approach to solve the problem is to use so-called adaptation techniques. With these techniques, when a new speaker has to be recognised, a system that was previously trained on a large set of other speakers can be optimised for the new speaker using only a small set of new examples. One widely used approach for adaptation in speech is maximum a posteriori (MAP) adaptation [8], a technique that has also been applied in other domains such as handwriting recognition (on-line [2] or off-line [14]), audio transcription [7], and video annotation [1]. In OMR, it is difficult to imagine an application where the recognition errors would not have to be hand-corrected before using the output. For example, if the tool is used to build a digital library or to perform music analysis, it is absolutely necessary to have a correct representation of the musical text. This property makes adaptation techniques of prime interest for OMR because the normal usage of any OMR application software provides the handcorrected data for adaptation and performance improvement. As soon as a page has been recognised and corrected by the user during an OMR process, adaptation can be run so that the subsequent pages will require fewer corrections. In this paper, we present our experiments in using MAP adaptation in Aruspix, an OMR system for early typographic prints based on hidden Markov models (HMMs) [11]. The main goals of the study were to see whether MAP adaptation works in this context, how the adaptation process has to be organised, and how much data is needed to reap benefits from adaptation. We also compared the results obtained with those obtained when training the system from scratch, i.e., without using an adaptation technique. (a) RISM 1528-2 (Attaignant, Paris, 1528) (b) RISM 1532-10 (Moderne, Lyon, 1532) (c) RISM V-1421 (Figliuoli di Gardano, Venezia, 1572) (d) RISM V-1433 (Basa, Roma, 1585) (e) RISM M-0582 (Le Roy & Ballard, Paris, 1598) Figure 1: Examples of prints used for the experiments 2 OMR INFRASTRUCTURE Aruspix provides an infrastructure that handles the complete OMR process. It performs the indispensable preprocessing operations, such as deskewing the image, normalising the size, removing image borders, binarising and cleaning the image, detecting staff positions, and pre-classifying some of the elements (music, ornate letters, lyrics, and title elements). The core of the recognition is performed using HMMs, an original approach to OMR. The Aruspix infrastructure also includes an editor designed especially for early music, which makes it an end-user application as well as a research tool. The machine learning component of Aruspix (training and recognition) is based on the Torch machine learning library [5]. Recognition is performed using continuousdensity HMMs with a 2-pixel sliding window (for a normalised staff height of 100 pixels). At each position, a feature vector of 7 values is extracted [12], and the sequences of feature vectors are then used to build a set of HMMs using embedded training over the whole staff. 3 MAP ADAPTATION The general schema to enable MAP adaptation in Aruspix is to build, in a preliminary phase, a book-independent (BI) system using as large a set of learning data as possible, taken from many different books. The BI system gives acceptable results in general but is not optimised for any book in particular. When a page has been recognised and corrected by the user, the BI system is then adapted with MAP adaptation using the corrected page as an example. This means that, through usage, a book-dependent (BD) system for the book currently being processed can be obtained very quickly. When training the HMMs for the BI system, the expectation-maximisation (EM) algorithm is used to determine the parameter vector λ that maximises P(X|λ), where X is the observed data, i.e.: λ = argmax λ P(X|λ) (1) The principle of MAP adaptation [6] is to find the parameters λM that maximises the posterior probability P(λ|X) using the prior knowledge about the model parameter distribution of the already trained model P(λ): λM = argmax λ P(λ|X) = argmax λ P(X|λ)P(λ) (2) To obtain a λM estimate for the BD model, the EM algorithm is applied, and the value obtained enables the means µ of the BI HMMs to be adapted, while the variances, the transitions, and weights are usually unchanged [14]. EM runs with a heuristic weighting factor τ on the relative importance of the new adaptation data. High values of τ privilege the BI model, while low values privilege the new adaptation data. This weighting factor has to be determined empirically. 4 EXPERIMENTS For our experiments, we used microfilms of sixteenthcentury music prints held at the Marvin Duchow Music Library at McGill University and the Isham Memorial Library at Harvard University. They were scanned as 8-bit greyscale TIFFs at a resolution of 400 dots per inch. The BI system was trained using 457 pages taken from music books produced by printers from Italy, France, Belgium, and Germany between 1529 and 1595. This set of pages was transcribed and represents a total of 2,710 staves and 95,845 characters. Using this model, Aruspix was trained to recognise 220 different musical symbols (note values from longa to semi-fusa, rests, clefs, accidentals, custodes, dots, bar lines, coloured notes, ligatures, etc.). To experiment with MAP adaptation, we used 5 books printed in France and Italy between 1528 and 1598 (RISM 1528-2, 1532-10, V-1421, V-1433 and M-0582) [13]. For each of them, 50 pages were transcribed and corrected in Aruspix (250 pages in total). We took the first 40 pages for the training set and reserved the 10 remaining pages for the test set. We decided to select the first pages for adaptation because it is in this order that the data would become available in a digitisation workflow, but to verify our results, we performed a 5-fold cross-validation on one of the books, M-0582. In two books, the pages transcribed contain new symbols not represented in the BI model, 10 in M-0582 and 7 in V-1433, which required special treatment. We experimented with MAP adaptation using cumulative procedures, common when experimenting with this technique in an off-line architecture [14]. Unlike incremental MAP adaptation, which generates new BD models for every page of adaptation data using only the new page and the BD model from the previous page, in cumulative MAP adaptation, the BD model is generated using the BI model and the complete set of adaptation data up to that point. We tried two approaches in particular. The first is using embedded adaptation with the Viterbi algorithm on whole staves, which is similar to embedded training when training HMMs from scratch [11]. We call it the embedded cumulative MAP (EC-MAP) adaptation. The second approach is to adapt the models for each symbol individually, taking the advantage of the fact that our ground-truth data are aligned. This approach, which we call isolated cumulative MAP (IC-MAP) adaptation, is uncommon because in other domains, the adaptation data are not usually aligned. Finally, we trained a new model from scratch for each of the five books, using the data in a cumulative way, in order to compare with the MAP adaptation results. The MAP factor τ was empirically optimised in both EC-MAP and IC-MAP. The best results were obtained when the factor was decreased as the amount of data increased, reflecting the intuitive assumption that the more data we have for MAP adaptation, the less we need to rely on the original model. During the MAP adaptation process, the new symbols in M-0582 and V-1433 were necessarily trained separately before being inserted into the system. 5 RESULTS The results were evaluated by calculating recall and precision on the best-aligned subsequence of recognised symbols [10]. We computed a baseline by testing the BI model on the five test sets.",
        "zenodo_id": 1415922,
        "dblp_key": "conf/ismir/PuginBF07",
        "keywords": [
            "Optical music recognition (OMR)",
            "early typographic prints",
            "hidden Markov models",
            "maximum a posteriori (MAP)",
            "dynamic adaptation",
            "ground-truth production",
            "real-time adaptation",
            "five 16th-century music prints",
            "two procedures",
            "large-scale digitisation project"
        ],
        "content": "MAP ADAPTATION TO IMPROVE OPTICAL MUSIC RECOGNITION OF\nEARLY MUSIC DOCUMENTS USING HIDDEN MARKOV MODELS\nLaurent Pugin John Ashley Burgoyne Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Tech nology\nSchulich School of Music of McGill University\nMontr ´eal, Qu ´ebec, Canada H3A 1E3\n{laurent,ashley,ich }@music.mcgill.ca\nABSTRACT\nDespite steady improvement in optical music recognition\n(OMR), early documents remain challenging because of\nthe high variability in their contents. In this paper, we\npresent an original approach using maximum a posteri-\nori (MAP) adaptation to improve an OMR tool for early\ntypographic prints dynamically based on hidden Markov\nmodels. Taking advantage of the fact that during the nor-\nmal usage of any OMR tool, errors will be corrected, and\nthus ground-truth produced, the system can be adapted in\nreal-time. We experimented with ﬁve 16th-century mu-\nsic prints using 250 pages of music and two procedures in\napplying MAP adaptation. With only a handful of pages,\nboth recall and precision rates improved even when the\nbaseline was above 95 percent.\n1 INTRODUCTION\nOptical music recognition (OMR) systems create encod-\nings of the musical content in digital images automati-\ncally. For libraries, they constitute very promising solu-\ntions for building searchable digital libraries of previou sly\ninaccessible material, especially historical documents. Us-\ning OMR tools in large-scale digitisation project remains\na challenge, however [4], mainly because of the incon-\nsistent performance of most OMR tools. Learning-based\napproaches, such as the one adopted by Gamera [9], are\nwell suited to such projects, but other approaches, e.g.,\ncoupling multiple recognisers, have also been considered\nrecently [3].\nWhen performing OMR on early music sources, one\nmajor problem is the extremely high variability exhibited\nin the data. In printed documents, the font shape may vary\nconsiderably from one print to another, and the printing\ntechniques of the time as well as the texture of the pa-\nper used resulted in frequent printing irregularities. The\nphysical documents are often degraded, introducing vari-\nous kinds of noise, and the scanning settings (e.g., bright-\nness or contrast) are not necessarily consistent across all\ndocuments. Five examples of 16th-century music prints\nwe used for this study, shown in Figure 1, demonstrate\nc/circlecopyrt2007 Austrian Computer Society (OCG).the variability in font shape, document degradation, and\nscanning parameters. Given such a range of documents,\nthere is no guarantee that an OMR system can be trained\nto perform well on new document, even with a learning-\nbased approach. Furthermore, as a considerable amount\nof labelled data is required to train a sufﬁciently reliable\nsystem [10], building a system from scratch for every new\ndocument encountered is not practical.\nSimilar problems have been encountered previously in\nspeech recognition, where the amount of data and time\nneeded to build a recogniser is considerable. One com-\nmon approach to solve the problem is to use so-called\nadaptation techniques. With these techniques, when a new\nspeaker has to be recognised, a system that was previously\ntrained on a large set of other speakers can be optimised\nfor the new speaker using only a small set of new exam-\nples. One widely used approach for adaptation in speech\nis maximum a posteriori (MAP) adaptation [8], a tech-\nnique that has also been applied in other domains such as\nhandwriting recognition (on-line [2] or off-line [14]), au -\ndio transcription [7], and video annotation [1].\nIn OMR, it is difﬁcult to imagine an application where\nthe recognition errors would not have to be hand-corrected\nbefore using the output. For example, if the tool is used\nto build a digital library or to perform music analysis, it\nis absolutely necessary to have a correct representation of\nthe musical text. This property makes adaptation tech-\nniques of prime interest for OMR because the normal us-\nage of any OMR application software provides the hand-\ncorrected data for adaptation and performance improve-\nment. As soon as a page has been recognised and cor-\nrected by the user during an OMR process, adaptation can\nbe run so that the subsequent pages will require fewer cor-\nrections.\nIn this paper, we present our experiments in using MAP\nadaptation in Aruspix, an OMR system for early typo-\ngraphic prints based on hidden Markov models (HMMs)\n[11]. The main goals of the study were to see whether\nMAP adaptation works in this context, how the adaptation\nprocess has to be organised, and how much data is needed\nto reap beneﬁts from adaptation. We also compared the re-\nsults obtained with those obtained when training the sys-\ntem from scratch, i.e., without using an adaptation tech-\nnique.(a) RISM 1528-2 (Attaignant, Paris, 1528)\n(b) RISM 1532-10 (Moderne, Lyon, 1532)\n(c) RISM V-1421 (Figliuoli di Gardano, Venezia, 1572)\n(d) RISM V-1433 (Basa, Roma, 1585)\n(e) RISM M-0582 (Le Roy & Ballard, Paris, 1598)\nFigure 1: Examples of prints used for the experiments\n2 OMR INFRASTRUCTURE\nAruspix provides an infrastructure that handles the com-\nplete OMR process. It performs the indispensable pre-\nprocessing operations, such as deskewing the image, nor-\nmalising the size, removing image borders, binarising and\ncleaning the image, detecting staff positions, and pre-cla s-\nsifying some of the elements (music, ornate letters, lyrics ,\nand title elements). The core of the recognition is per-\nformed using HMMs, an original approach to OMR. The\nAruspix infrastructure also includes an editor designed es -\npecially for early music, which makes it an end-user ap-\nplication as well as a research tool.\nThe machine learning component of Aruspix (training\nand recognition) is based on the Torch machine learning\nlibrary [5]. Recognition is performed using continuous-\ndensity HMMs with a 2-pixel sliding window (for a nor-\nmalised staff height of 100 pixels). At each position, a\nfeature vector of 7 values is extracted [12], and the se-\nquences of feature vectors are then used to build a set of\nHMMs using embedded training over the whole staff.\n3 MAP ADAPTATION\nThe general schema to enable MAP adaptation in Aruspix\nis to build, in a preliminary phase, a book-independent\n(BI) system using as large a set of learning data as pos-\nsible, taken from many different books. The BI system\ngives acceptable results in general but is not optimised forany book in particular. When a page has been recognised\nand corrected by the user, the BI system is then adapted\nwith MAP adaptation using the corrected page as an ex-\nample. This means that, through usage, a book-dependent\n(BD) system for the book currently being processed can\nbe obtained very quickly.\nWhen training the HMMs for the BI system, the expec-\ntation-maximisation (EM) algorithm is used to determine\nthe parameter vector λthat maximises P(X|λ), where X\nis the observed data, i.e.:\nλ= argmax\nλP(X|λ) (1)\nThe principle of MAP adaptation [6] is to ﬁnd the parame-\ntersλMthat maximises the posterior probability P(λ|X)\nusing the prior knowledge about the model parameter dis-\ntribution of the already trained model P(λ):\nλM= argmax\nλP(λ|X) = argmax\nλP(X|λ)P(λ)(2)\nTo obtain a λMestimate for the BD model, the EM algo-\nrithm is applied, and the value obtained enables the means\nµof the BI HMMs to be adapted, while the variances,\nthe transitions, and weights are usually unchanged [14].\nEM runs with a heuristic weighting factor τon the rela-\ntive importance of the new adaptation data. High values\nofτprivilege the BI model, while low values privilege\nthe new adaptation data. This weighting factor has to be\ndetermined empirically.\n4 EXPERIMENTS\nFor our experiments, we used microﬁlms of sixteenth-\ncentury music prints held at the Marvin Duchow Music\nLibrary at McGill University and the Isham Memorial Li-\nbrary at Harvard University. They were scanned as 8-bit\ngreyscale TIFFs at a resolution of 400 dots per inch. The\nBI system was trained using 457 pages taken from mu-\nsic books produced by printers from Italy, France, Bel-\ngium, and Germany between 1529 and 1595. This set\nof pages was transcribed and represents a total of 2,710\nstaves and 95,845 characters. Using this model, Aruspix\nwas trained to recognise 220 different musical symbols\n(note values from longa tosemi-fusa , rests, clefs, acciden-\ntals, custodes , dots, bar lines, coloured notes, ligatures,\netc.).\nTo experiment with MAP adaptation, we used 5 books\nprinted in France and Italy between 1528 and 1598 (RISM\n1528-2, 1532-10, V-1421, V-1433 and M-0582) [13]. For\neach of them, 50 pages were transcribed and corrected in\nAruspix (250 pages in total). We took the ﬁrst 40 pages\nfor the training set and reserved the 10 remaining pages\nfor the test set. We decided to select the ﬁrst pages for\nadaptation because it is in this order that the data would\nbecome available in a digitisation workﬂow, but to verify\nour results, we performed a 5-fold cross-validation on one\nof the books, M-0582. In two books, the pages transcribed\ncontain new symbols not represented in the BI model, 10in M-0582 and 7 in V-1433, which required special treat-\nment.\nWe experimented with MAP adaptation using cumula-\ntive procedures, common when experimenting with this\ntechnique in an off-line architecture [14]. Unlike incre-\nmental MAP adaptation, which generates new BD models\nfor every page of adaptation data using only the new page\nand the BD model from the previous page, in cumulative\nMAP adaptation, the BD model is generated using the BI\nmodel and the complete set of adaptation data up to that\npoint. We tried two approaches in particular. The ﬁrst is\nusing embedded adaptation with the Viterbi algorithm on\nwhole staves, which is similar to embedded training when\ntraining HMMs from scratch [11]. We call it the embed-\nded cumulative MAP (EC-MAP) adaptation. The second\napproach is to adapt the models for each symbol individu-\nally, taking the advantage of the fact that our ground-truth\ndata are aligned. This approach, which we call isolated\ncumulative MAP (IC-MAP) adaptation, is uncommon be-\ncause in other domains, the adaptation data are not usually\naligned. Finally, we trained a new model from scratch for\neach of the ﬁve books, using the data in a cumulative way,\nin order to compare with the MAP adaptation results.\nThe MAP factor τwas empirically optimised in both\nEC-MAP and IC-MAP. The best results were obtained\nwhen the factor was decreased as the amount of data in-\ncreased, reﬂecting the intuitive assumption that the more\ndata we have for MAP adaptation, the less we need to rely\non the original model. During the MAP adaptation pro-\ncess, the new symbols in M-0582 and V-1433 were nec-\nessarily trained separately before being inserted into the\nsystem.\n5 RESULTS\nThe results were evaluated by calculating recall and preci-\nsion on the best-aligned subsequence of recognised sym-\nbols [10]. We computed a baseline by testing the BI model\non the ﬁve test sets.\n5.1 MAP adaptation vs training from scratch\nFor all ﬁve sets, MAP adaptation improved both recall\nand precision rates (see tables 1 and 2), even where the\nbaseline was above 95% (V-1421). Using all 40 pages of\nthe training sets, MAP adaptation gives better results than\ntraining the models from scratch (TS) for all sets but one.\nIn several cases, training from scratch failed to achieve\neven the baseline recall or precision. The only book where\nit yields better results is M-0582, the most degraded book\nin our set (see ﬁgure 1e). The severe degradation may ex-\nplain why in the end, training from scratch can outperform\nMAP adaptation.\nTable 2 shows the adaptation and training curves for the\ncross-validated results on M-0582. Other than the fact that\ntraining from scratch outperforms MAP adaptation after\nabout 30 pages, these two plots are also representative of\nthe curve shapes we obtained for the other sets. We canTable 1: Recall results with 40 pages\nBook Base. TS IC-MAP EC-MAP\n1528-2 84.93 89.67 88.36 91.61\n1532-10 76.53 87.86 86.24 89.23\nV-1421 95.82 93.84 96.33 97.01\nV-1433 86.32 91.70 90.82 92.62\nM-0582 72.44 90.26 86.31 88.44\nTable 2: Precision results with 40 pages\nBook Base. TS IC-MAP EC-MAP\n1528-2 96.57 95.57 95.77 97.11\n1532-10 94.24 93.29 95.30 95.98\nV-1421 97.18 94.95 97.28 97.05\nV-1433 95.48 95.56 71.18 97.25\nM-0582 84.54 93.19 92.19 90.14\nsee that MAP adaptation improves the results after only a\nhandful of pages (between 5 and 10) and that no further\nsigniﬁcant improvement is obtained after 20 pages.\n5.2 EC-MAP vs IC-MAP adaptation\nOverall, IC-MAP adaptation does not give as good results\nas EC-MAP adaptation. Nevertheless, it merits consid-\neration because it runs so much faster than the EC-MAP\nadaptation. Table 3 show the mean adaptation time for\nboth adaptation procedures. On our 2.7 GHz PowerPC\nG5 processor, IC-MAP takes less than one second per\npage on average, and it increases linearly as the number\nof pages increases. In comparison, EC-MAP takes about\none minute per page and increases exponentially. For this\nreason, IC-MAP is the most suitable approach to perform\nreal-time adaptation, e.g., within a real-world digitisat ion\nworkﬂow. As soon a page is corrected, the model can be\nadapted before recognising the next page.\nThe main drawback to IC-MAP is that it requires a\ngood alignment of the adaptation data. One book in our\nset (V-1433) had poorly aligned data because it had been\nprinted using a much wider font than the others (see ﬁg-\nure 1d). We can see in table 2 that the precision decreased\nwith IC-MAP for the particular book.\n6 CONCLUSIONS AND FUTURE WORK\nTo deal with the high variability in early music documents,\nwe recommend the use of MAP adaptation. For the books\nTable 3: Mean adaptation time\n# of pages IC-MAP EC-MAP\n1 >1 sec 1 min\n5 5 sec 7 min\n10 9 sec 20 min\n20 18 sec 50 min\n40 38 sec 1 h 45 min 70 75 80 85 90 95 100\n 5  10  15  20  25  30  35  40Recall\nNumber of pagesBaseline\nTraining from scratch\nEmbedded cumulative MAP adaptation\nIsolated cumulative MAP adaptation\n 70 75 80 85 90 95 100\n 5  10  15  20  25  30  35  40Precision\nNumber of pagesBaseline\nTraining from scratch\nEmbedded cumulative MAP adaptation\nIsolated cumulative MAP adaptation\nFigure 2: Cross-validated results for M-0582\nused for our experiments, MAP adaptation reduced the\nrecognition failures by a factor of nearly two on aver-\nage without losing precision. Precision was, in fact, in-\ncreased. Our experiments showed that only a couple of\npages are needed to beneﬁt from MAP adaptation. In\ncomparison, when training new models from scratch, at\nleast 30 pages are needed to outperform the results ob-\ntained with adaptation. We also presented an original ap-\nproach in applying MAP adaptation to isolated symbols\n(IC-MAP), which computes very quickly (about 1 sec-\nond per page) and could be used in real-time within a\ntypical OMR process. Such an infrastructure will speed\nup the OMR workﬂow by exploting the required human\nediting process to improve machine recognition, although\nto obtain the greatest beneﬁt, this infrastructure must in-\nclude efﬁcient user interfaces for error correction. This\napproach also opens new perspectives for other tasks where\nthe data present high variability, such as music manuscript s\nor other types of early documents.\n7 ACKNOWLEDGEMENTS\nWe would like to thank the Canada Foundation for Inno-\nvation and the Social Sciences and Humanities Research\nCouncil of Canada for their ﬁnancial support. We also\nwould like to thank Marnie Reckenberg for her contribu-\ntion to the project.8 REFERENCES\n[1] M. Barnard and J.-M. Odobez. Robust playﬁeld segmenta-\ntion using MAP adaptation. In Proceedings of the 17th In-\nternational Conference on Pattern Recognition , volume 3,\npages 610–13, Cambridge, United Kingdom, 2004.\n[2] A. Brakensiek, A. Kosmala, and G. Rigoll. Writer adapta-\ntion for online handwriting recognition. In Pattern Recog-\nnition: 23rd DAGM Symposium, Munich, Germany, Septem-\nber, 2001, Proceedings , volume 2191 of LNCS , pages 32–37.\nSpringer, Berlin, 2001.\n[3] D. Byrd and M. Schindele. Prospects for improving OMR\nwith multiple recognizers. In Proceedings of the 7th Inter-\nnational Conference on Music Information Retrieval , pages\n41–46, Victoria, Canada, 2006.\n[4] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fujinaga,\nB. Harrington, and K. MacMillan. Optical music recognition\nsystem within a large-scale digitization project. In Proceed-\nings of the 1st International Conference on Music Informa-\ntion Retrieval , 2000.\n[5] R. Collobert, S. Bengio, and J. Mari ´ethoz. Torch: a modular\nmachine learning software library. Technical Report IDIAP-\nRR 02-46, IDIAP, Martigny, Switzerland, October 2002.\n[6] J.-L. Gauvain and C.-H. Lee. Maximum a posteriori estima-\ntion for multivariate Gaussian mixture observations of Mar-\nkov chains. IEEE Transactions on Speech and Audio Pro-\ncessing , 2(2):291–98, 1994.\n[7] M. Goto. A predominant-F0 estimation method for CD\nrecordings: MAP estimation using EM algorithm for adap-\ntive tone models. In Proceedings of the International Confer-\nence on Acoustics, Speech, and Signal Processing , volume 5,\npages 3365–68, Salt Lake City, UT, 2001.\n[8] C.-H. Lee, C.-H. Lin, and B.-H. Juang. A study on speaker\nadaptation of the parameters of continuous density hidden\nMarkov models. IEEE Transactions on Signal Processing ,\n39(4):806–14, 1991.\n[9] K. MacMillan, M. Droettboom, and I. Fujinaga. Gamera:\nOptical music recognition in a new shell. In Proceedings of\nthe International Computer Music Conference , pages 482–\n85, 2002.\n[10] L. Pugin. Lecture et traitement informatique de typographies\nmusicales anciennes. Un logiciel de reconnaissance de par-\ntitions par mod `eles de Markov cach ´es. Ph.D. Dissertation,\nUniversity of Geneva, 2006.\n[11] L. Pugin. Optical music recognition of early typographic\nprints using hidden Markov models. In Proceedings of the\n7th International Conference on Music Information Re-\ntrieval , pages 53–56, Victoria, Canada, 2006.\n[12] L. Pugin, J. A. Burgoyne, and I. Fujinaga. Goal-directed\nevaluation for the improvement of optical music recognition\non early music prints. In Proceedings of the ACM/IEEE Joint\nConference on Digital Libraries , pages 303–4, Vancouver,\nCanada, 2007.\n[13] R ´epertoire international des sources musicales (RISM). Sin-\ngle Prints Before 1800 . Series A/I. B ¨arenreiter, Kassel,\n1971–81.\n[14] A. Vinciarelli and S. Bengio. Writer adaptation techniques\nin HMM based off-line cursive script recognition. Pattern\nRecognition Letters , 23:905–16, 2002."
    },
    {
        "title": "Multipitch Analysis with Harmonic Nonnegative Matrix Approximation.",
        "author": [
            "Stanislaw Andrzej Raczynski",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417809",
        "url": "https://doi.org/10.5281/zenodo.1417809",
        "ee": "https://zenodo.org/records/1417809/files/RaczynskiOS07.pdf",
        "abstract": "This paper presents a new approach to multipitch analysis by utilizing the Harmonic Nonnegative Matrix Approximation, a harmonically-constrained and penalized version of the Nonnegative Matrix Approximation (NNMA) method. It also includes a description of a note onset, offset and amplitude retrieval procedure based on that technique. Compared with the previous NNMA approaches, specific initialization of the basis matrix is employed – the basis matrix is initialized with zeros everywhere but at positions corresponding to harmonic frequencies of consequent notes of the equal temperament scale. This results in the basis containing nothing but harmonically structured vectors, even after the learning process, and the activity matrix’s rows containing peaks corresponding to note onset times and amplitudes. Furthermore, additional penalties of mutual uncorrelation and sparseness of rows are placed upon the activity matrix. The proposed method is able to uncover the underlying musical structure better than the previous NNMA approaches and makes the note detection process very straightforward. 1 INTRODUCTION The problem of automatic polyphonic music transcription (extracting underlying musical structure from sampled music) has been addressed numerous times, and it still seems there is a long way to go before arriving at a robust and universal technique. This paper tries to lay another brick towards this goal. Automatic music transcription of recorded music is usually a two stage process. The first stage is the event detection phase, where music events (note onsets, note offsets, pitch changes) are detected and identified. In the second stage, these events are transformed into a musical score. This paper focuses on the event detection stage, main part being multipitch analysis, which aims to uncover the fundamental frequencies of simultaneously played harmonic sounds. It is a difficult task, since each sound, besides the fundamental tone, consists of many harmonic tones, some of them having the same frequencies as the fundamental frequencies of other sounds (e.g. in the case of c⃝2007 Austrian Computer Society (OCG). tonal music). It is necessary to distinguish between the fundamental tones and their overtones. A large variety of methods has been used to tackle the multipitch analysis problem (e.g. [1], [2], [4], [9], [11], [12], [13]; an exhaustive list of methods would be very long and we are not going to include it here, but for a good summary, see [6]), but so far none of them solving the problem in a satisfactorily precise and universal way. While our lab has recently developed a powerful method based on Harmonic Temporal Structured Clustering (HTC) for this purpose [4], the procedure proposed in this paper is built upon a method from the family of Nonnegative Matrix Approximations (NNMA), which, under different names and in different varieties, has recently received much attention, also from the music transcription community. To the best of our knowledge, however, none of the NNMA-based methods were developed specifically for analysis of musical signals. As it will be shown later in this paper, nature of music can be exploited to increase the transcription potential of the algorithm. The goal of this paper was to propose a NNMA variation most suitable for multipitch analysis. Different variations and extensions of the NNMA algorithm have been used for multipitch analysis: the regular NNMA [13], its penalized versions, such as the Nonnegative Sparse Coding (NNSC) [2, 1], or NNMA with basis vectors extended to contain spectrotemporal signatures (a number of consequent data frames), such as Nonnegative Matrix Factor 2-D Deconvolution (NMF2D) and Sparse Nonnegative Matrix Factor 2-D Deconvolution (SNMF2D) [11]. These methods have, however, a few drawbacks. They do not guarantee to yield basis vectors with harmonic structure. NMF2D and SNMF2D use a single signature for every note (of a single instrument), making use of the shift-similarity of logarithmic frequency scale spectra of notes played on a single instrument. This might be an oversimplification resulting in an inadequate model. The note spectra are similar, but not identical, with significant differences for some specific instruments (e.g. flute). Moreover, spectrotemporal atoms cannot account for different note lengths, which results in multiple activity peaks when notes are longer than the signature, and lower activity peaks when notes are shorter than the signature. All of the previous work published on that subject do not propose a complete transcription procedure, simply reporting good results after visual comparison of the activities and symbolic data used to generate the analyzed music. The paper is organized as follows. Section 2 presents a theoretical introduction to the Nonnegative Matrix Approximation and its extension through the addition of constraints and penalties placed upon both the basis matrix and the activity matrix. An overview of the proposed procedure in given in section 3, including description of the proposed Harmonic Nonnegative Matrix Approximation (HNNMA) technique (3.3) and note detection method (3.4). The procedure is evaluated and compared with the results of regular NNMA methods in section 4. 2 THEORETICAL BACKGROUND",
        "zenodo_id": 1417809,
        "dblp_key": "conf/ismir/RaczynskiOS07",
        "keywords": [
            "harmonic nonnegative matrix approximation",
            "Harmonic Temporal Structured Clustering (HTC)",
            "Nonnegative Matrix Approximation (NNMA)",
            "penalized versions of NNMA",
            "basis matrix initialization",
            "mutual uncorrelation",
            "sparseness of rows",
            "activity matrix",
            "multipitch analysis",
            "Harmonic Nonnegative Matrix Approximation (HNNMA)"
        ],
        "content": "MULTIPITCH ANALYSIS WITH HARMONIC NONNEGATIVE MATRIX\nAPPROXIMATION\nStanisław A. Raczy ´nski Nobutaka Ono Shigeki Sagayama\nThe University of Tokyo\nGraduate School of Information Science and Engineering\nE-mail:{raczynski,onono,sagayama }@hil.t.u-tokyo.ac.jp\nABSTRACT\nThis paper presents a new approach to multipitch analysis\nby utilizing the Harmonic Nonnegative Matrix Approx-\nimation, a harmonically-constrained and penalized ver-\nsion of the Nonnegative Matrix Approximation (NNMA)\nmethod. It also includes a description of a note onset, off-\nset and amplitude retrieval procedure based on that tech-\nnique. Compared with the previous NNMA approaches,\nspeciﬁc initialization of the basis matrix is employed – the\nbasis matrix is initialized with zeros everywhere but at po-\nsitions corresponding to harmonic frequencies of conse-\nquent notes of the equal temperament scale. This results in\nthe basis containing nothing but harmonically structured\nvectors, even after the learning process, and the activity\nmatrix’s rows containing peaks corresponding to note on-\nset times and amplitudes. Furthermore, additional penal-\nties of mutual uncorrelation and sparseness of rows are\nplaced upon the activity matrix. The proposed method\nis able to uncover the underlying musical structure better\nthan the previous NNMA approaches and makes the note\ndetection process very straightforward.\n1 INTRODUCTION\nThe problem of automatic polyphonic music transcription\n(extracting underlying musical structure from sampled mu-\nsic) has been addressed numerous times, and it still seems\nthere is a long way to go before arriving at a robust and\nuniversal technique. This paper tries to lay another brick\ntowards this goal.\nAutomatic music transcription of recorded music is usu-\nally a two stage process. The ﬁrst stage is the event detec-\ntion phase, where music events (note onsets, note offsets,\npitch changes) are detected and identiﬁed. In the second\nstage, these events are transformed into a musical score.\nThis paper focuses on the event detection stage, main part\nbeing multipitch analysis, which aims to uncover the fun-\ndamental frequencies of simultaneously played harmonic\nsounds. It is a difﬁcult task, since each sound, besides\nthe fundamental tone, consists of many harmonic tones,\nsome of them having the same frequencies as the funda-\nmental frequencies of other sounds (e.g. in the case of\nc/ci∇clecopy∇t2007 Austrian Computer Society (OCG).tonal music). It is necessary to distinguish between the\nfundamental tones and their overtones.\nA large variety of methods has been used to tackle the\nmultipitch analysis problem (e.g. [1], [2], [4], [9], [11],\n[12], [13]; an exhaustive list of methods would be very\nlong and we are not going to include it here, but for a\ngood summary, see [6]), but so far none of them solv-\ning the problem in a satisfactorily precise and universal\nway. While our lab has recently developed a powerful\nmethod based on Harmonic Temporal Structured Cluster-\ning (HTC) for this purpose [4], the procedure proposed in\nthis paper is built upon a method from the family of Non-\nnegative Matrix Approximations (NNMA), which, under\ndifferent names and in different varieties, has recently re -\nceived much attention, also from the music transcription\ncommunity. To the best of our knowledge, however, none\nof the NNMA-based methods were developed speciﬁcally\nfor analysis of musical signals. As it will be shown later in\nthis paper, nature of music can be exploited to increase the\ntranscription potential of the algorithm. The goal of this\npaper was to propose a NNMA variation most suitable for\nmultipitch analysis.\nDifferent variations and extensions of the NNMA algo-\nrithm have been used for multipitch analysis: the regular\nNNMA [13], its penalized versions, such as the Nonneg-\native Sparse Coding (NNSC) [2, 1], or NNMA with basis\nvectors extended to contain spectrotemporal signatures (a\nnumber of consequent data frames), such as Nonnegative\nMatrix Factor 2-D Deconvolution (NMF2D) and Sparse\nNonnegative Matrix Factor 2-D Deconvolution (SNMF2D)\n[11]. These methods have, however, a few drawbacks.\nThey do not guarantee to yield basis vectors with har-\nmonic structure. NMF2D and SNMF2D use a single sig-\nnature for every note (of a single instrument), making use\nof the shift-similarity of logarithmic frequency scale spe c-\ntra of notes played on a single instrument. This might\nbe an oversimpliﬁcation resulting in an inadequate model.\nThe note spectra are similar, but not identical, with signif -\nicant differences for some speciﬁc instruments (e.g. ﬂute) .\nMoreover, spectrotemporal atoms cannot account for dif-\nferent note lengths, which results in multiple activity pea ks\nwhen notes are longer than the signature, and lower activ-\nity peaks when notes are shorter than the signature. All\nof the previous work published on that subject do not pro-\npose a complete transcription procedure, simply reportinggood results after visual comparison of the activities and\nsymbolic data used to generate the analyzed music.\nThe paper is organized as follows. Section 2 presents\na theoretical introduction to the Nonnegative Matrix Ap-\nproximation and its extension through the addition of con-\nstraints and penalties placed upon both the basis matrix\nand the activity matrix. An overview of the proposed\nprocedure in given in section 3, including description of\nthe proposed Harmonic Nonnegative Matrix Approxima-\ntion (HNNMA) technique (3.3) and note detection method\n(3.4). The procedure is evaluated and compared with the\nresults of regular NNMA methods in section 4.\n2 THEORETICAL BACKGROUND\n2.1 Deﬁnitions and basic properties\nFor clarity, the following notation was used in this paper:/vextendsingle/vextendsingle·/vextendsingle/vextendsingleis a sum of all the elements of a matrix, ⊙is the\nHadamard product (calculated element-wise) and 1is a\nmatrix (of appropriate dimensions) containing nothing but\nones.\nA few easy to prove properties were later used. If A∈\nRN×M, then:\n∇A/vextendsingle/vextendsingleA⊙A/vextendsingle/vextendsingle= 2A, (1)\n∇A/vextendsingle/vextendsingleATA/vextendsingle/vextendsingle= 2AT1, (2)\n∇A/vextendsingle/vextendsingleBA/vextendsingle/vextendsingle=BT1, (3)\n∇A/vextendsingle/vextendsinglef1(A) +f2(A)/vextendsingle/vextendsingle=∇A/vextendsingle/vextendsinglef1(A)/vextendsingle/vextendsingle+∇A/vextendsingle/vextendsinglef2(A)/vextendsingle/vextendsingle,(4)\n∇A/vextendsingle/vextendsingleB⊙(Cυ(A))/vextendsingle/vextendsingle=υ′(A)⊙/parenleftbig\nCTB/parenrightbig\n, (5)\nwhere υ:R+→R+is an element-wise function and υ′\nis its derivative, and f1,f2:R+,N×M→R+,N×Mare\nany matrix functions.\n2.2 Generalized Nonnegative Matrix Approximation\nGeneralized Nonnegative Matrix Approximation (described\nin [3] and later developed in [14]), is a method for decom-\nposition of a nonnegative (having only nonnegative ele-\nments) matrix X(later referred to as the data matrix) into\na multiplication of two, also nonnegative, matrices Aand\nS(later refereed to as the basis matrix and the activity ma-\ntrix, respectively):\nX∼=AS=/tildewideX. (6)\nThe Generalized NNMA solves this problem by mini-\nmizing a Bregman divergence between the data matrix X\nand its approximation /tildewideX. A Bregman divergence between\ntwo matrices is deﬁned as\nD(P,Q) =/vextendsingle/vextendsingleϕ(P)−ϕ(Q)−ϕ′(Q)⊙(P−Q)/vextendsingle/vextendsingle,(7)\nwhere ϕ:S⊆R→Ris a strictly convex function with\ncontinuous ﬁrst derivative, calculated here for each ele-\nment of a matrix separately. If ϕ(p) =plogp−p, thenthe Bregman divergence becomes the I-divergence (gen-\neralized Kullback-Leibler divergence):\nDKL(P,Q) =/vextendsingle/vextendsingle/vextendsingle/vextendsingleP⊙logP\nQ−P+Q/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (8)\nwhere the logarithm and the division are calculated element -\nwise. This situation leads to the simple NNMA, also known\nas Nonnegative Matrix Factorization (NMF) [8]. Lee and\nSeung in [8] has proposed a very fast algorithm for mini-\nmizing the I-divergence that can be derived by using aux-\niliary functions. A function G(P,P′)is an auxiliary func-\ntion for function F(P)if:\n1.G(P,P) =F(P),\n2.G(P,P′)≥F(P).\nMaking use of the convexity of ϕ[14], it can be shown\nthat:\nG(A,A′) =/vextendsingle/vextendsingle/vextendsingle/vextendsingleϕ(X) +˜X−X\n−X\n˜X′⊙/bracketleftbigg/parenleftbigg\nA′⊙logA\nA′/parenrightbigg\nS+ϕ(˜X′)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle,(9)\nwhere /tildewideX′=A′S, is an auxiliary function for\nF(A) = DKL(X,AS) =DKL(X,/tildewideX)\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingleX⊙logX\n˜X−X+˜X/vextendsingle/vextendsingle/vextendsingle/vextendsingle(10)\nand\nG(S,S′) =/vextendsingle/vextendsingle/vextendsingle/vextendsingleϕ(X) +/tildewideX−X\n−X\n/tildewideX′⊙/bracketleftbigg\nA/parenleftbigg\nS′⊙logS\nS′/parenrightbigg\n+ϕ(/tildewideX′)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle,(11)\nwhere this time /tildewideX′=AS′, is an auxiliary function for\nF(S) =DKL(X,AS). (12)\nIt can also be shown [14] that F(S′)is non-increasing un-\nder the update\nS′←arg min\nSG(S,S′). (13)\nTo solve this optimization problem, we calculate the\ngradient of the auxiliary function and force it to zero:\n∇SG(S,S′) =∇S/vextendsingle/vextendsingle/vextendsingle/vextendsingleϕ(X) +/tildewideX−X\n−X\n/tildewideX′⊙/bracketleftbigg\nA/parenleftbigg\nS′⊙logS\nS′/parenrightbigg\n+ϕ(/tildewideX′)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle=0.(14)\nUsing properties (3), (4) and (5), we can easily calculate\nthat gradient as:\n∇S/vextendsingle/vextendsingle/vextendsingle/vextendsingleAS−X\n/tildewideX′⊙/bracketleftbigg\nA/parenleftbigg\nS′⊙logS\nS′/parenrightbigg/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle=0, (15)AT1−S′\nS⊙/parenleftbigg\nAT/parenleftbiggX\n/tildewideX′/parenrightbigg/parenrightbigg\n=0, (16)\nS=S′⊙AT/parenleftBig\nX\neX′/parenrightBig\nAT1. (17)\nThis suggest a multiplicative update rule:\nS←S⊙AT/parenleftbigX\nAS/parenrightbig\nAT1. (18)\nWe can come up with a similar update rule for the basis\nmatrixA:\n∇AG(A,A′) =0, (19)\n1ST−A′\nA⊙/parenleftbiggX\n/tildewideX′ST/parenrightbigg\n=0, (20)\nA←A⊙X\nASST\n1ST. (21)\nSo, by using simple multiplicative rules from (21) and\n(18), we can ﬁnd matrices AandSthat minimize the I-\ndivergence from equation (8).\n2.3 Penalized NNMA\nBecause H(P,P′) =G(P,P′)+α(P)is an auxiliary func-\ntion for F(P) +α(P)(the proof is straightforward), the\nNNMA algorithm can be extended by placing additional\npenalties upon both estimated decomposition matrices, ex-\npressed as additional element in the objective function.\n∇AH(A,A′) =∇AG(A,A′) +∇Aα(A), (22)\n∇AH(A,A′) =1ST−A′\nA⊙/parenleftbiggX\nX′ST/parenrightbigg\n+∇Aα(A).\n(23)\nWhile it is very difﬁcult to solve this non-linear equa-\ntion with respect to A, the following approximation can\nbe used [14]:\n∇Aα(A)∼=∇Aα(A)/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nA=A′, (24)\nwhich is asymptotically true, as difference between Ain\nconsequent iterations tends to 0. Now:\n1ST−A′\nA⊙/parenleftbiggX\nX′ST/parenrightbigg\n+∇A′α(A′) =0, (25)\nwhich yields an update rule:\nA←A⊙X\nASST\n1ST+∇Aα(A). (26)\nSimilarly:\n∇SHS(S,S′)∼=∇SG(S,S′) +∇S′β(S′), (27)\nS←S⊙ATX\nAS\nAT1+∇Sβ(S). (28)\nIt must be noted that the new update rules may result in\nthe matrices AandSbecoming negative, so caution must\nbe taken while constructing the objective function.3 MULTIPITCH ANALYSIS PROCEDURE\n3.1 Overview of the procedure\nThe NNMA algorithm decomposes the data matrix X,\nwhich does not contain musical data, but rather some mid-\nlevel representation of it. In most cases its columns are\npower spectra of consecutive frames of time-domain mu-\nsical data. In the proposed procedure a constant-Q trans-\nform is used. The central frequencies of the constant-Q\nﬁlters can be set to correspond to the frequencies of the\nnotes of the most common twelve-tone equal tempera-\nment (12-TET) scale or can further divide each semitone,\nwhich is a very useful property for analyzing musical sig-\nnals. What is more, the dimensionality of a constant-Q-\ntransformed data is much lower than the dimensionality\nof a Fourier-transformed data, which makes the computa-\ntion of the NNMA faster. After calculating the constant-Q\ntransform, the resulting data is fed through the HNNMA\nalgorithm, which decomposes it to a product of the basis\nmatrix and activity matrix. Activity matrix is analyzed in\nthe last part of the procedure – the note detector, described\nin section 3.4.\n3.2 Matrix initialization in HNNMA\nBecause zero-valued elements of basis vectors will remain\nzero-valued throughout the learning process, we could ini-\ntialize them to have zeros everywhere but at the positions\nof fundamentals of notes from a speciﬁc range of the 12-\nTET scale and their harmonics. Furthermore, that would\nguarantee that the basis vectors are sorted by their fun-\ndamental frequencies, and that corresponding rows in the\nactivity matrix contain activities of consequent notes fro m\nthat range, resulting in a harmonicaly-constrained NNMA.\nThis would make analysis of the results of the algorithm\nstraightforward – one would only have to analyze the note\nactivities and ﬁnd peaks corresponding to instances of thes e\nnotes.\nIn the proposed procedure, after initialization, each ba-\nsis vector is multiplied by the normalized mean value of\nthe constant-Q transform of the data at the bin correspond-\ning to this note. This should discourage the HNNMA from\nlearning these notes and using them to reconstruct the an-\nalyzed data. During the learning process, each row of the\nactivity matrix is, as it is usually done in the learning pro-\ncess of the NNMA methods, normalized to unit squared\nsum, while the basis matrix is simply normalized by its\nmaximal value to let the basis vectors, that correspond to\nnotes not existing in analyzed music, freely decrease.\n3.3 Additional penalties in HNNMA\nHNNMA extends the regular NNMA to include additional\npenalties on the activity matrix S. We would like to ﬁnd\nsuch an activity matrix that would:\n1. be sparse, i.e. each row should contain only very\nfew non-zero elements (to reduce the low-valued\nnoise),Figure 1 . Basis vectors after analysis of the Ode to Joy .\nIts harmonic structure is clearly visible. Vertical dotted\nlines indicate expected harmonic peaks positions.\n2. contain mutually uncorrelated rows (to reduce the\ninter-row crosstalk, like e.g. octave errors).\nThe above can be reformulated, accordingly, in terms\nof a objective function β:\nβ(S) =−µ1/vextendsingle/vextendsinglelog(1 + S⊙S)/vextendsingle/vextendsingle\n+µ2/parenleftbig/vextendsingle/vextendsingleSTS/vextendsingle/vextendsingle−/vextendsingle/vextendsingleS⊙S/vextendsingle/vextendsingle/parenrightbig\n. (29)\nThe ﬁrst element,/vextendsingle/vextendsinglelog (1 + S⊙S)/vextendsingle/vextendsingle, is one of the often\nused sparseness measures [5]. The second one is a mea-\nsure of correlation between every pair of different matrix\nrows:/vextendsingle/vextendsingleSTS/vextendsingle/vextendsingle−/vextendsingle/vextendsingleS⊙S/vextendsingle/vextendsingle=/summationdisplay\ni/summationdisplay\nj/negationslash=isT\nisj, (30)\nwheresT\niis thei-th row and siis its transposition. Using\nproperties (1), (2) and (5) from section 2.1, we can easily\ncalculate the gradient:\n∇Sβ(S) =−2µ1S/(1 +S⊙S) + 2µ2S(1−I).(31)\nSimilar penalties could be used for the basis matrix, but\nour experiments with sparsity, column uncorrelation and\ncolumn shift-similarity showed that these constraints do\nnot improve procedure’s accuracy when the basis matrix\nwas initialized in the way described in the next subsection.\nWhen the matrix was initialized with traditional noise, the\nconstraints would result in basis vectors containing peaks ,\nalthough the structure was not always purely harmonic.\nThus, either further, much more complex constraint are\nrequired to enforce this structure, or we could take the ad-\nvantage of the multiplicative nature of HNNMA algorithm\nupdate rules.\n3.4 Note detector\nBefore analysis, each row of the activity matrix is mul-\ntiplied by the height of the peak at the fundamental fre-\nquency in corresponding basis vector (all rows are being\nnormalized to unit squared sum during the learning pro-\ncess). This should make the activities of notes that doNote activities\nTime [s]0 5 10 15 20d e f# g# a# c’ d’ e’ f#’ g#’\nFigure 2 . Note activities ( S) after analysis of the Ode to\nJoy. Peaks correspond to notes detected in the signal\nnot exist in the analyzed music signiﬁcantly smaller than\nthe activities of notes that occur in the music. After that\neach row with values higher than some arbitrarily chosen\nthreshold are normalized to the maximal value in each of\nthem.\nIt turns out that in preliminary experiments the result-\ning activities clearly correspond to notes in the analyzed\nmusical piece. However, if the notes were played shortly\none after another, their peaks blend to form a single peak\nwith multiple sub-peaks. Because of that, a simple thresh-\nolding is not enough. A still simple, but much more ro-\nbust thresholding method was used. First, the activities\nare thresholded to detect peaks and blended peaks (e.g.\ntwo blended peaks depicted on Figure 3). Then, for each\ndetection all local maxima and local minima are found.\nSome of the maxima correspond to actual sub-peaks, while\nsome are just ﬂuctuations in the note activity. Two thresh-\nolds are set between the highest local maximum and the\nlowest local minimum. All maxima that are above the\nhigher threshold (upper ligth-gray range on Figure 3) and\nhas at least one minimum lower than the lower threshold\n(lower light-gray range on picture 3) are marked as sub-\npeaks and are assumed to correspond to individual notes.\nIn similar fashion, all minima under the lower threshold\nthat lay between two sub-peaks are assumed to be the off-\nset time of the note corresponding to the left sub-peak and\nonset time of the note corresponding to the right sub-peak.\nThe beginning and the end of the blended group of sub-\npeaks are assumed to be the onset time of the ﬁrst note in\nthe group and the offset time of the last note in the group.\nThe last step of the note detection process is acceptance\ndecision for each of the detected peaks and sub-peaks. A\npeak is accepted and regarded a note only if its width mul-\ntiplied by its height is greater than some threshold.\n4 EXPERIMENTAL RESULTS\n4.1 Experiment conditions\nTo validate our approach, we tested our procedure on a\nfew recordings. All analyzed recordings were played onFigure 3 . Results of note detection for a quarter note and\nan eighth blended together (example of real data). Dark-\ngray circles mark the detected sub-peaks and the offset\ntime of the ﬁrst note.\nComposer Title Notes Acc. Corr.\nL. Beethoven Symphony in D minor,\nOp. 125, No. 9 (last\nmovement, Ode to joy )101 96% 86%\nF. Chopin Nocturne in E# major,\nOp. 9, No. 2 (part)328 87% 72%\nF. Chopin Nocturne in Bb minor,\nOp. 9, No. 1 (part)358 70% 74%\nJ. S. Bach Minuet No 4 in G 102 97% 100%\nTable 1 . Piano pieces used for algorithm evaluation\npiano, as listed in Table 1, but experiments show equally\ngood results for acoustic guitar and violin (though lower\ndetection accuracy for violin).\nDuring the experiments, all the parameters of the algo-\nrithm were kept constant in order to evaluate its robust-\nness, though by ﬁne-tuning the parameters for each musi-\ncal piece separately, much better results can be obtained.\nIt has been noted that the best results were achieved by\napplying this method for shorter blocks of data (30-60 s),\ninstead of the whole song at once.\nThe input data was ﬁrst mixed down to a monaural sig-\nnal and resampled to 11025 kHz. The constant-Q trans-\nform was calculated for frames shifted 12 ms. During\nlearning, µ1= 1,µ2= 10 were used. During note detec-\ntion phase, before normalization of activities, rows havin g\nmaximal values lower than 0.125of the maximal value of\nthe activity matrix Swere set to zero. The main detection\nthreshold was set at 0.25, the lower threshold to 0.25of\ndifference between the lowest minimum and the highest\nmaximum and the higher threshold to 0.75of that differ-\nence.\nAfter learning the basis matrix contained very well struc-\ntured vectors, each one having a stronger peak for the fun-\ndamental tone and weaker peaks for the harmonics (Figure\nFigure 4 . Three bars from the middle of Chopin’s Noc-\nturne in E# major, Op. 9, No. 2\n(a) Note activities obtained by NNMA\n(b) The ﬁrst 4 bars of the played score, as a reference\nFigure 5 . Note activities after note detection for Ode to\nJoywith grey squares being the original notes\n1). The results of note detection for few example pieces\nof music are presented in Table 1. Correctness of tran-\nscription is the ratio of the difference between the number\nof notes in analyzed music and the number of deletions, to\nthe number of notes in analyzed music. Accuracy is the ra-\ntio of the difference between the number of detected notes\nand the number of insertions, to the number of detected\nnotes. The ﬁrst and the last musical piece were relatively\neasy to analyze, containing notes from a rather short range\n(about 2 octaves). The two Chopin’s nocturnes were, on\nthe other hand, very difﬁcult – played with a big dynamic\nand wide range of note lengths, and containing notes from\nwithin 5-6 octaves (see e.g. Figure 4).\n4.2 Comparison with previous methods\nFigure 6 depicts the basis matrix obtained using standard\nNNMA (NMF) method after fundamental frequency esti-\nmation and basis vector sorting. It does not contain clear\nharmonic structure – many of the vectors have two (or\nmore) dominant peaks, sometimes with highest peaks be-\ning the overtones instead of the fundamental, sometimes\nhaving the same fundamental frequency as different basis\nvectors (Figure 6). Slightly better results are obtained by\nutilizing different penalized NNMA methods proposed in\nthe literature (e.g NNSC [1] or Local Nonnegative Ma-\ntrix Factorization [10]), however, the basis matrix never\ncontains as highly harmonically structured vectors as the\nones obtained with the proposed method. The activities\nobtained with these methods contain a lot of ﬂuctuations\nand assigning note names to the activities depends on the\nhighly dubious operation of fundamental frequency esti-\nmation of the basis vectors.Frequency binBasis vector nr\n0 50 100 150 1  3  5  7  9  11  13  15  17  19\nFigure 6 . Basis obtained with standard NMF. Note de-\ntection relies on pitch estimation of these, often multi-\npeaked, basis vectors.\nIt seems that the activity matrix in the proposed proce-\ndure contains very easy to analyze and at the same time\nalmost complete information about the underlying mu-\nsical structure of the analyzed signal. This method is a\ngood compromise between full basis estimation methods\n(such as NMF and other NNMA-based approaches) and\nmethods that use pre-learned basis vectors (e.g. [12] or\n[9]). The achieved results are similar to the results of dif-\nferent recently developed music transcription techniques\n(e.g. [4]), but by ﬁne-tuning the method’s parameters,\neven greater accuracy could be achieved. The proposed\nprocedure uses a relatively simple method of analyzing\nthe activity matrix, making room for future research in\nmore advanced techniques, such as modeling the tempo-\nral envelopes of notes or using models of musical rhythm\nand harmony.\n5 CONCLUSION\nIn this paper, we discussed the use of Harmonic Non-\nnegative Matrix Approximation for multipitch analysis of\npolyphonic music signals. By initializing the basis ma-\ntrix with harmonic structure and using new penalties of\nsparsity and uncorrelation of rows of the activity matrix,\nthis approach yielded higher note detection accuracy com-\npared with previous extensions of the Nonnegative Matrix\nApproximation algorithm. The future work includes im-\nproving the post-processing of the HNNMA results by in-\ncorporating models of musical rhythm and harmonicity.\n6 REFERENCES\n[1] Abdallah, S.A. and Plumbley, M.D. “Polyphonic mu-\nsic transcription by non-negative sparse coding of\npower spectra,” Proc. 5th International Conference on\nMusic Information Retrieval , pp. 318–325, Barcelona,\nSpain, 2004.\n[2] Abdallah, S.A. and Plumbley, M.D. “Unsupervised\nanalysis of polyphonic music by sparse coding,” IEEETrans. on Neural Networks , vol. 17, no. 1, pp. 179–\n196, 2006.\n[3] Dhillon, I.S. and Sra, S. “Generalized Nonnegative\nMatrix Approximations with Bregman Divergences,”\nProc. Neural Information Processing Systems, Van-\ncouver, USA 2005.\n[4] Kameoka, H., Nishimoto, T., Sagayama, S. “Multi-\npitch Analyzer Based on Harmonic Temporal Struc-\ntured Clustering,” IEEE Trans. on Audio, Speech and\nLanguage Processing , vol. 15, no. 3, pp. 982–994,\nMar, 2007.\n[5] Karvanen, J. and Cichocki, A. “Measuring sparseness\nof noisy signals,” Proc. 4th International Symposium\non Independent Component Analysis and Blind Signal\nSeparation , 2003.\n[6] Klapuri, A.P. “Automatic Music Transcription as We\nKnow it Today,” Journal of New Music Research , vol.\n33, no. 3, pp. 269–282, 2004.\n[7] Lee, D.D. and Seung, H.S. “Algorithms for Non-\nnegative Matrix Factorization,” Advances in Neural\nInformation Processing Systems , vol. 13, pp. 556–562,\n2001.\n[8] Lee, D.D. and Seung, H.S. “Learning the parts of\nobjects by nonnegative matrix factorization,” Nature ,\nvol. 401, no. 6755, pp. 788–791, 1999.\n[9] Lepain, P. “Polyphonic Pitch Extraction from Musical\nSignals,” Journal of New Music Research , vol. 28, no.\n4, pp. 296–309, 1999.\n[10] Li, S.Z. and Hou, X.W. and Zhang, H.J. and Cheng,\nQ.S. “Learning spatially localized, parts-based rep-\nresentation,” Proc. IEEE Conf. Computer Vision and\nPattern Recognition , pp. 1–6, 2001.\n[11] Schmidt, M.N. and Mørup M. “Sparse Non-negative\nMatrix Factor 2-D Deconvolution for Automatic Tran-\nscription of Polyphonic Music,” Proc. 6th Interna-\ntional Symposium on Independent Component Anal-\nysis and Blind Signal Separation , Charleston, USA,\n2006.\n[12] Sha, F. and Saul, L.K. “Real-Time Pitch Determina-\ntion of One or More V oices by Nonnegative Matrix\nFactorization,” Advances in Neural Information Pro-\ncessing Systems , vol. 17, 2005.\n[13] Smaragdis, P. and Brown. J.C. “Non-Negative Ma-\ntrix Factorization for Polyphonic Music Transcrip-\ntion,” Proc. 2003 IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , New York,\nUSA, 2003.\n[14] Sra, S. and Dhillon, I.S. “Nonnegative Matrix Ap-\nproximations: Algorithms and Application,” Techni-\ncal Report Tr-06-27, Computer Sciences, University\nof Texas, Austin, USA 2006."
    },
    {
        "title": "The Music Ontology.",
        "author": [
            "Yves Raimond",
            "Samer A. Abdallah",
            "Mark B. Sandler",
            "Frederick Giasson"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1323755",
        "url": "https://doi.org/10.5281/zenodo.1323755",
        "ee": "https://zenodo.org/records/1323755/files/BrioWeissenberger2018LITMUS.pdf",
        "abstract": "LITMUS (Linked Irish Traditional Music) is a two-year cultural heritage linked data project at the Irish Traditional Music Archive in Dublin, Ireland. It focuses on the creation of a linked data ontology specific to Irish traditional music and dancethe first such ontology based around a music primarily propagated by oral transmission. While efforts to accurately represent, describe, and organise traditional music have numerous challenges, the methodologies used to develop the ontology centre around text-based sources from traditional musicians album notes, mirroring language used by musicians when introducing tunes and songs in both formal and informal performance settings. Using practitioners own language will benefit the eventual application of the ontology within traditional music collections in Ireland, and have potential applications to other European and non-European folk/traditional music collections with similar considerations.",
        "zenodo_id": 1323755,
        "dblp_key": "conf/ismir/RaimondASG07",
        "keywords": [
            "LITMUS",
            "linked Irish Traditional Music",
            "cultural heritage",
            "linked data project",
            "ontology",
            "Irish traditional music",
            "oral transmission",
            "text-based sources",
            "practitioners own language",
            "traditional music collections"
        ],
        "content": "To cite this article: Weissenberger, L. K., (2018). Linked data in music and its potential for ITMA/traditional music web resources. Brio, 55(1), 52-57.  This article post-print is provided with permission of Brio, the journal of the UK/Ireland chapter of the International Association of Music Libraries, Archives, and Documentation Centres (IAML). See http://iaml-uk-irl.org/brio-contents. \tLINKED DATA IN MUSIC AND ITS POTENTIAL FOR ITMA/TRADITIONAL MUSIC WEB RESOURCES  Lynnsey Weissenberger  Introduction The Linked Irish Traditional Music project1 at the Irish Traditional Music Archive (Dublin, Ireland) is an ambitious two-year undertaking to create linked data tools for better describing Irish traditional music and dance. Our main focus is on developing an ontology specific to Irish traditional music (song and instrumental) as well as dance, along with creating better organisational structures to support ongoing efforts to describe and make these traditions accessible across the web.   In this project, the scope of ‘Irish traditional music and dance’ is that used in the definition used by the Irish Traditional Music Archive (ITMA):2   The Archive understands ‘Irish traditional music’ as a broad term which encompasses oral-tradition song, instrumental music and dance of many kinds and periods. It interprets the term in the widest possible sense, and always tries to include rather than exclude material. Items are collected if they could be considered traditional in any way – in origin, or in idiom, or in transmission or style of performance, etc. – or if they are relevant to an understanding of traditional music and its contexts. \tLinked open data (LOD) has shown great promise in cultural heritage and digital humanities applications, making cultural heritage materials—those found within libraries, museums, and archives—accessible to wider audiences via the semantic web. Recent projects involving music linked data include the DOREMUS (DOing REusable MUSic Data) project,3 the ongoing Linked Jazz project,4 as well as the large-scale Europeana Sounds project,5 for which the Irish Traditional Music Archive provided content.   There are numerous challenges when representing digital cultural heritage materials within linked data ontologies, with ontology development necessitating ‘double experts’ 6 in both ontology design and subject domains such as music. This paper will provide an overview of LITMUS to date, present some considerations for documenting and describing Irish traditional music and dance within a linked data ontology, and consider where the LITMUS project might take ITMA in the future.   Description and Access Challenges Music traditions propagated primarily through oral transmission have additional considerations and present unique representational challenges outside current knowledge organization frameworks, the majority of which are based upon the norms of Western Classical and Western Popular music.7 Among the few music ontologies developed, none adequately express orally-based traditions like Irish \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1 https://litmus.itma.ie - LITMUS, Linked Irish Traditional Music, project results (Marie Skłodowska-Curie Action grant number 750814) were generated with the assistance of EU financial support. 2 For the full statement, see: https://www.itma.ie/about/our-work/definitions 3 http://www.doremus.org/ 4 https://linkedjazz.org/ 5 http://www.eusounds.eu/ 6 David Stuart, Practical ontologies for information professionals (London: Facet Publishing, 2016). 7 For more discussion on this topic, see: L.K. Weissenberger,‘Traditional musics and ethical considerations of knowledge and documentation processes’, Knowledge Organization 42:5 (2015), 290-295 [http://www.ergon-verlag.de/isko_ko/downloads/ko_42_2015_5_e.pdf]  traditional music and dance.8 An ontology based upon the considerations of oral transmission will allow such items to be described and related to one another using terms musicians and dancers themselves use, and will reflect more accurate relationships than current music ontologies allow.  The LITMUS project must overcome challenges related to documenting traditional Irish music and dance practice. Relationships and terminology are made more difficult due to the informal nature of oral transmission. As collector and scholar Hugh Shields explained, ‘… terms to describe traditional singing have grown up in a haphazard way: a fact which disturbs its students more than its practitioners.’9 Several challenges for describing and organising Irish traditional music and dance include the general disagreement among practitioners on common terminology or conflicting uses of terms, as well as terms with more than one applied meaning. An example of this is the term ‘jig’, of which there are multiple applied meanings:  1. As a dance type, the jig can be rendered within the idioms of sean-nós (‘old style’ percussive) dance forms, competition-based solo step-dance using both heavy shoes (a treble jig) or soft shoes (beginner’s dances called light jigs), group dances such as within the competition step-dance (such as a dance choreographed for 8 dancers, termed an ‘8-hand jig’, for example), ceili dances (Haymaker’s jig), set dances within the solo step-dance tradition which are rendered in jig time, 6/8, such as St Patrick’s Day, and the group-based meaning of set dances, where couples dance sets and figures.  2. Next, we also must remember that the term ‘jig’ applies equally to the dance tunes in 6/8, played both for dancers and separate from them; the term might also be seen as an umbrella term for dance tunes and dances that are based in 6/8 and 9/8 metres, where three quavers are grouped to form one beat. While dancers use shoe type to differentiate between types of jigs, musicians use tune rhythms to denote either a ‘single jig’, where crotchet-quaver combinations predominate, or ‘double jig’, where groups of three quavers predominate.10  3. Jigs can also be sung (termed ‘jig songs’), usually in the Irish language, and several examples are to be found here: An Rogaire Dubh and Na Ceannabháin Bhána, just to list a 6/8 jig song and a 9/8 slip jig song. Within the above terminology example, relationships between different meanings of the same term illustrate the interwoven nature of Irish instrumental music, song, and dance. Relationships between people, music, place, and other aspects of the traditions are predominantly based in oral transmission and the kinds of associations that produces.   Ontology Development Overview Irish Traditional Music Archive staff provide guidance and expertise with regard to source materials, examples, contextual information, and bibliographic data throughout the development of the ontology, which is still ongoing. For the development of classes and entities, along with properties, our approach is to model a small number of highly-complex use cases to try and account for the scope and range of eventual representation in linked data. Two exemplar use cases are 1) the air/song with numerous titles, a few of which are ‘Danny Boy’ and ‘(London)Derry Air’, and 2) the song, instrumental slow air, hornpipes, and set dance known as An Lon Dubh / The Blackbird.11 Eventually, these graph models will be converted into classes and entities using OWL (Web Ontology Language).  In addition to developing the classes and entities within the ontology, the properties12 are a particular focus. The LITMUS ontology’s properties are derived both from existing ontologies and also from these written or oral liner notes. The methodology for developing the ontology properties centres around content analysis of album liner notes, which is rich in contextual detail. Selected recordings represent \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t8 For more detailed explanations, see: L.K. Weissenberger, Stories, songs, steps, and tunes: a linked data ontology for Irish traditional music and dance. Paper presented at the ISKO-UK biennial conference, London, UK, 2017 [http://doi.org/10.5281/zenodo.1002056].   9 Hugh Shields, Narrative singing in Ireland: lays, ballads, come-all-yes and other songs (Dublin: Irish Academic Press, 1993), v. 10 In everyday practice, it might be unusual to hear musicians use ‘single jig’ or ‘double jig’ when discussing tunes, however the terms would appear within compilation books of tunes and tutors, as well as between scholar-musicians of Irish traditional music. 11 An Lon Dubh is rarely performed as a song today. It is heard as a non-metred instrumental air as well as metred dance tunes derived from the air, and a set dance with regional and stylistic variants in dance steps. 12 Properties describe relationships; Stuart’s (2016) definition is ‘attributes that describe classes’. musicians from a variety of eras, geographic locations, instrumental and vocal traditions, and both solo and group recordings. The length and detailed nature of some album notes made them better candidates for analysis over those where only tune or song names/lyrics were provided. For older recordings where the musician can be heard talking and giving information about the tunes, these musings were recorded as ‘oral liner notes’.   A small example of liner notes used can be seen in Fig. 1, taken from an album of Johnny “Batt” Henry’s fiddle music, recorded over the span of almost two decades (1964, 1977-8, 1973, and 1981) and compiled in an album released in 2012. The notes are by renowned fiddle player James Kelly, who has an encyclopaedic memory of people, places, and tunes.   Language used within the album notes appears fairly consistent from the 1970s up to the albums released within the past few years. For example, musicians and singers note which tunes/songs are related to the one they are performing. Likewise, they often explain what makes the particular version of tune or song performed on the album different from the others. Musicians frequently provide sources for their tunes, such as where they first heard the tune or from whom they learned their version. In explaining these relationships, album notes reference titles, or events, or geographic places, other musicians, as well as collectors and published collections.  This methodology for developing ontology properties allows us to analyse language used by practitioners over time to express relationships found in transmission and learning of Irish traditional music. Anecdotally speaking, the language contained within the album notes mirrors language used when introducing tunes and songs in both formal and informal performance settings. Using practitioners’ own language will benefit the application of the ontology within traditional music collections in Ireland, as well as when applied to other European and non-European music collections.   Looking to the Future  The Linked Irish Traditional Music (LITMUS) project’s overall aim is to improve searching and access to web-based Irish traditional music, song and dance resources through the development of a linked data ontology, and eventual framework. Once completed, the LITMUS ontology will facilitate research in a variety of disciplines–including musicology/ethnomusicology, ethnochoreology, digital humanities, and library and information science–as well as enable discovery of new resources for students and performers of Irish music and dance worldwide. As global interest in Irish traditional music remains high, this will necessitate further digitisation efforts and investment in the Irish Traditional Music Archive’s digital library infrastructure to reach users worldwide.  For future projects stemming from LITMUS, we look to incorporate aspects of crowdsourced knowledge and aim for closer collaboration with the traditional music and dance communities. A similar project to LITMUS, the Linked Jazz project led by Cristina Patuelli of the Pratt Institute, derived many of their relationship data for musician-musician relationships from oral history interviews of jazz musicians; transcriptions were either made by Linked Jazz researchers or crowdsourced by the public. For large-scale linked data implementation, strategic crowdsourced knowledge generation could prove a tremendous asset to our diverse and ever-expanding collections.  Although tailored to Irish traditional music, it is hoped that LITMUS will provide a working model for other European and non-European traditional musics with similar considerations. Properties are potentially the most transferrable aspect of the ontology for other traditional/folk music outside the Irish tradition as the types of relationships and descriptors/attributes useful for describing relationships in Irish traditional music and dance may very well extend to other European and non-European music/dance traditions also reliant upon oral transmission. Collaborative efforts between the Irish Traditional Music Archive and other institutions with folk or traditional music and dance materials may be fertile ground for further development and implementation of the ontology.   Abstract LITMUS (Linked Irish Traditional Music) is a two-year cultural heritage linked data project at the Irish Traditional Music Archive in Dublin, Ireland. It focuses on the creation of a linked data ontology specific to Irish traditional music and dance–the first such ontology based around a music primarily propagated by oral transmission. While efforts to accurately represent, describe, and organise traditional music have numerous challenges, the methodologies used to develop the ontology centre around text-based sources from traditional musicians’ album notes, mirroring language used by musicians when introducing tunes and songs in both formal and informal performance settings. Using practitioners’ own language will benefit the eventual application of the ontology within traditional music collections in Ireland, and have potential applications to other European and non-European folk/traditional music collections with similar considerations.   Dr. Lynnsey Weissenberger is the Marie Skłodowska-Curie Fellow at the Irish Traditional Music Archive where she leads the LITMUS linked data project, and is an avid Irish traditional fiddle player and harper."
    },
    {
        "title": "A Study on Attribute-Based Taxonomy for Music Information Retrieval.",
        "author": [
            "Jeremy Reed",
            "Chin-Hui Lee"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414910",
        "url": "https://doi.org/10.5281/zenodo.1414910",
        "ee": "https://zenodo.org/records/1414910/files/ReedL07.pdf",
        "abstract": "We propose an attribute-based taxonomy approach to providing alternative labels to music.  Labels, such as genre, are often used as ground-truth for describing song similarity in music information retrieval (MIR) systems. A consistent labelling scheme is usually a key in determining quality of classifier learning in training and performance in testing of an MIR system.  We examine links between conventional genre-based taxonomies and acoustical attributes available in text-based descriptions of songs. We show that the vector representation of each song based on these acoustic attributes enables a framework for unsupervised clustering of songs to produce alternative labels and quantitative measures of similarity between songs. Our experimental results demonstrate that this new set of labels are meaningful and classifiers based on these labels achieve similar or better results than those designed with existing genrebased labels.",
        "zenodo_id": 1414910,
        "dblp_key": "conf/ismir/ReedL07",
        "keywords": [
            "attribute-based taxonomy",
            "alternative labels",
            "music information retrieval",
            "song similarity",
            "acoustical attributes",
            "unsupervised clustering",
            "quantitative measures",
            "classifier learning",
            "conventional genre-based taxonomies",
            "MIR systems"
        ],
        "content": "A STUDY ON ATTRIBUTE-BASED TAXONOMY FOR \nMUSIC INFORMATION RETRIEVAL\nJeremy Reed Chin-Hui Lee \nSchool of Electrical and Computer Engineering  \nGeorgia Institute of Technology \nAtlanta, GA 30332 \njeremy.reed@gatech.edu School of Electrical and Computer Engineering  \nGeorgia Institute of Technology \nAtlanta, GA 30332 \nchl@ece.gatech.edu \nABSTRACT \nWe propose an attribute-based taxonomy approach to \nproviding alternative labels to music.  Labels, such as \ngenre, are often used as ground-truth for describing song \nsimilarity in music information retrieval (MIR) systems.  \nA consistent labelling scheme is usually a key in \ndetermining quality of classifier learning in training and \nperformance in testing of an MIR system.  We examine \nlinks between conventional genre-based taxonomies and \nacoustical attributes available in text-based descriptions \nof songs. We show that the vector representation of each \nsong based on these acoustic attributes enables a \nframework for unsupervised clustering of songs to \nproduce alternative labels and quantitative measures of \nsimilarity between songs. Our experimental results \ndemonstrate that this new set of labels are meaningful \nand classifiers based on these labels achieve similar or \nbetter results than those designed with existing genre-\nbased labels. \n1. INTRODUCTION \nIn recent years, the performance in genre recognition has \nreached an asymptotic maximum level of around 75-\n85% [1]. Many researchers have blamed ground-truth \nlabelling procedures and debated the significance of the \ngenre recognition task. Largely, two factors are often \ncited as limitations in designing reasonable training and \ntesting databases for the task of genre recognition, and \nmore generally, music similarity. The first is the \ninconsistent labelling of music, both in terms of how a \nparticular artist or piece is classified and in the various \nclass labels used in the labelling procedure [2]. In part, \nthis can be explained by the tendency for people to use \nmore finely grained categories for particular classes they \nenjoy [3]. For example, a listener of contemporary \nmusic may use ten to twenty different subclasses of rock, \nbut may only have one general definition for classical. \nMeanwhile, a classical aficionado may use romantic, \nbaroque, etc., but describe all forms of rock, hip-hop, \netc. as contemporary.  \nThe second factor affecting database design is that \ncurrent genre labels are extracted from record companies and web-base retailers, who assign labels at \nthe album or artist level rather than to individual songs \n[4]. Several questions arise because of this issue. If only \none label is given per artist/album, does that really mean \nthe entire collection is indeed similar? In the case that \nmultiple labels are given to an artist, how does one \ndecide which song has a particular label(s)? For \ninstance, allmusic.com1 has six styles for Eric Clapton, \nincluding hard rock and adult contemporary. Obviously, \nfinding example songs that would fall under both \ncategories is rare. This also demonstrates that by \nallowing fuzzy classification the database design issue is \nnot solved; i.e., each label given in a multiple-class \nobject still needs to be correctly applied. \nAnother problem in building databases for MIR \npurposes is that a person's concept of musical genre and \nmusical similarity does not include just auditory cues \n[5]. For example, allmusic.com lists heavy metal and \nthrash as styles for Metallica's \"Load\" album, even \nthough many die-hard metal fans criticized the album for \nits alternative rock leanings. On possible solution to \ndeal with this issue is to incorporate non-acoustically-\nbased algorithms, such as collaborative filtering [6]; \nhowever, there is still a desire to separate the testing and \ntuning of these two sub-systems. Further, collaborative \nfiltering approaches can only be used when textual data \nexists and is therefore unavailable for new music. \nAnother solution is to define genre labels based solely \non music theory [7]. However, there is evidence that \nboth musicians and non-musicians do not use such deep \nlevels of musical understanding [8]. \nGiven these issues, the previously used genre labels \nare inconsistent, lack precision, and are biased by non-\nacoustical cues. These issues have given some \nresearchers the conclusion that genre is not a solvable \nproblem and that instead, focus should be concerned \nwith general notions of similarity. However, as pointed \nout by McKay and Fujinaga, the issues concerning genre \nalso influence similarity performance metrics [4]. For \nexample, if non-acoustic factors influence the perception \nof similarity, user studies must be able to isolate when a \ndecision is based on non-acoustical information if the \nend result is to test auditory-based classifiers. In \naddition, there is plenty of literature demonstrating that \n                                                           \n1 http://allmusic.com © 2007 Austrian Computer Society (OCG). \n   \n \npeople use genre when searching for music [9] and that \ndescriptions of genres are well-formed [3]. \nThis paper proposes a novel construction for musical \ndatabases using clustering techniques on musical \nattributes, which are connected to specific acoustic \nqualities from either a surface-level (e.g., timbre) or a \nhigh-level (e.g., song form). By building taxonomy from \na consistently-applied set of acoustic labels, non-\nacoustical information does not bias the performance \nmetrics. Remaining songs can then be described in terms \nof their distances from each respective class, allowing \nfor fuzzy classification that is based on a \"relevance \nscore\" in terms of similarity of attributes. Section 2 of \nthis paper describes the overall construction of the new \nlabelling scheme and illustrates how acoustical \nproperties guide the building of taxonomy. Section 3 \ndescribes how discriminative training reinforces cluster \nintra-similarity and increases the inter-cluster distances.  \nSection 4 demonstrates the increased performance \nbetween the proposed unsupervised clustering and the \ntraditional genre labels given at the artist level.  Section \n5 demonstrates that song-level genre labels still possess \nlarge variations in acoustic attributes. Conclusions and \nfuture considerations are given in Section 6. \n2. CLUSTERING PROCEDURE \n2.1. Taxonomy Construction \nDue to inconsistencies found in previous genre labels, \nthis section describes a new labelling procedure for \nproducing ground-truth similarity labels. This new \nprocedure is designed such that the following criteria are \nmet: \n· Labels are consistent: taxonomy labels are built \nfrom attributes that are assigned in a consistent \nmanner by human subjects.  \n· Appropriate precision: taxonomies are \nmeaningful in terms of precision and not too \ngeneral (e.g., 2 genres: classical and popular) \n· Acoustically meaningful: taxonomies for \nacoustically-based algorithms can only hope in \nlearning information contained in the audio \nsignal, whether at the surface level (e.g., \ntimbre, rhythm) or at a high-level (e.g., song \nform, tonal progression). \nIn order to meet these requirements, previous genre \nlabels are ignored and a new set of labels is developed in \nan unsupervised fashion using acoustically meaningful \ndescriptors from the Music Genome Project1 (MGP). \nThe rationale behind using MGP information is that \nattributes given to each song are musically motivated \nand text-normalization, such as stemming and stop-word \nfiltering [10], is easy given the small attribute set. The \n                                                           \n1 www.pandora.com example in Figure 1 demonstrates that the attribute list \ncan be seen as musical \"subjects.\" The list presents \nseveral surface-level features such as rhythm and \ninstrumentation (timbre). In addition, there are high-\nlevel acoustic features dealing with tonality and song \nstructure. \n \nbasic rock song structures \na subtle use of vocal harmony \nrepetitive melodic phrasing \nabstract lyrics \na twelve-eight time signature \nextensive vamping \nmixed acoustic and electric instrumentation \na vocal-centric aesthetic \nmajor key tonality \nelectric guitar riffs \nacoustic rhythm guitars \ntriple note feel  \nFigure 1. Example: Music Genome Project attributes. \n \nThere is information that is not relevant to many MIR \nacoustic-based similarity applications. For example, \nlyric identification is often seen as a separate task and \nnot relevant for the majority of acoustic-similarity \nalgorithms, which try link songs based on the musical \nqualities and not verbal subject matter. Fortunately, \ngiven the small attributes set provided by MGP, which is \naround 500 attributes, text normalization is an easy \nprocedure. A musically meaningful word list was \nmanually created from the original set of attributes, \nresulting in 375 words. Since this is an initial study and \nthe authors felt that the creation of a \"final\" word list \nshould be agreed upon by the MIR community, the \ntendency was to leave words that could be meaningful. \nFor example, the word \"off\" was left in the list because \n\"off beat\" carries a very definite musical quality, even \nthough \"off\" is a very commonly filtered word in text \nprocessing tasks. The authors also acknowledge that \nsources other than MGP might provide useful \ninformation. However, the source selection should \ninsure the criteria stated here are meet. \n2.2. Latent Semantic Analysis \nLatent semantic analysis (LSA) [11] represents a \ndocument collection with a term-document matrix where \nrows correspond to individual terms and columns \nrepresent documents. By including bigrams, which is the \nappearance of two consecutive words, each column \nvector has size M = J + J*J, with J equal to the number \nof unigrams (i.e., number of words in the lexicon). \nSpecifically, each element of the term-document matrix, \nW, is \n      ( )\njji\ni jincw,\n,1e-=                       (1) \nwhere ci,j is the number of times word i appears in \ndocument j and nj is the word count of document j. The   \n \nsize of the matrix W is M x N, where N represents the \nnumber of documents.  The term \u0001i is the normalized \nentropy for term i and is given by \n      \u0001\n=-=N\nji\niji\ni ttc\nN 1,loglog1e            (2) \nwhere ti is the total number of times term i appears in the \ntraining database. The entropy gives a measure of \nindexing power, where a value close to one indicates the \nword appears in very few documents. \nThe resulting matrix is very sparse, and therefore, \nfeature reduction is performed through singular value \ndecomposition (SVD), which is very close to eigenvalue \ndecomposition [12]. In SVD, W, is decomposed into \nTUSV WW =»ˆ                         (3) \nwhere U is MxQ, S is QxQ, V is NxQ and Q is the rank \nof the original matrix, W. The left-singular matrix, U, \nand the right-singular matrix, V, represent the term and \ndocument space, respectively. The matrix S is a diagonal \nmatrix of singular values, which represent the variation \nalong the Q axes. By keeping Q0\u0001Q singular values, the \nword-document space can be converted into a lower-\ndimensional \"concept\" space [11]. \n2.3. Song Clustering \nThe cosine similarity is a natural measure of distance \nand is given by \n( ) ( )\nSvSvvSvSvSv ddK\nj iT\nj i\nj i j i2\n, cos , = =      (5) \nwhere ||·|| represents the L2 norm. Song documents with \nK(di,dj) close to one are very similar and can be viewed \nas very relevant in terms of attributes or \"subjects.\" \nBottom-up clustering was performed on the database \nsuch that each song, di, in a cluster had a similarity of \nK(di,dj) > \u0002L, where \u0002L is a similarity threshold and dj is \nthe cluster mean. These clusters are based on the musical \ndescriptions given by the Music Genome Project and do \nnot contain non-acoustical features acting as \"taxonomic \nnoise.\" Unless stated otherwise, clusters which contain \nmore than 10 songs were kept for later analysis. \n3. DISCRIMINATIVE TRAINING REFINEMENT \nThe discriminative training (DT) technique described \nhere provides smaller inter-cluster similarity, while \nincreasing intra-cluster similarity Specifically, the \ncolumns of an MxK matrix, R, are updated in a training \nprocedure to minimize misclassification, where K \nrepresents the number of clusters found in the previous \nsection [13]. The training database is the set of songs in \neach of the retained clusters from Section 2.3 and each \nsong is labelled with its corresponding cluster.  This \ntype of classifier is effective when the parametric form \nof the class distribution is unknown and when optimality in terms of estimating distributions is not necessarily \nequivalent to optimality in terms of classifier design \n[14]. \nThe formulation starts by assigning the M-\ndimensional song vector x according to the \nmisclassification function, dj(x,R): \n()Rxd jj\nj, min arg ˆ=      \n   () ()]. , [min arg RxG Rxgj\nj+ - =           (6)   \nwhere the jth column vector in R represents the jth \ncluster. The function gj(·) is called the discriminate \nfunction and is often the dot product between x and the \ncluster mean of the jth cluster, rj: \n ()\u0001\n==×=M\niiji j j xr xr Rxg\n1,            (7) \nLikewise, the function Gj(·) is called the anti-discriminate \nfunction between x and the K-1 competing classes \n() ()h\nh/1\n1,,11, \u0002\n\u0003\u0004\n\u0005\n\u0006\u0007\n-=\u0001\n££¹ Kkjkj j RxgKRxG  (8) \nwhere \u0003 is a positive number. As \u0003 \u0002 \u0003, the anti-\ndiscriminate function is dominated by the most \ncompeting class. The interpretation of (6)-(8) is to \nassign a vector to the class to which it is most similar \nwhen the scores from competing classes are also \nconsidered.  \nIn order to find the optimal operating point given the \ntraining data, a smooth, differentiable 0-1 loss function \nis defined as \n()() ( )q g + - +=RxdRxl\njj, exp 11,        (9) \nwhere \u0004 and \u0005 are parameters that control the slope and \nshift of the sigmoid function, respectively. The overall \nempirical loss for the training database is \n() ( )( ) \u0001\u0001\n= =Î =0\n1 1,1N\niK\njj i i j\noCxRxlNRL 1      (10) \nwhere No is the number of training samples from the \nclusters retained from Section 2.3 and 1(·) represents the \nindicator function, and Cj represents class i. Because \nlj(x,R) is a smooth, differentiable function, the overall \nempirical loss is minimized directly and the column \nvectors in R are updated using a gradient descent \noptimization procedure [13]. By combining the \nmisclassification feature, dj, into the objective function, \nthe minimization of L(R) increases the separation \nbetween classes while decreasing the distance between \nsamples in a cluster. By using the discriminative training \nprocedure, terms that strengthen intra-class similarity \nand increase inter-class distance are given more weight. \nFurther, unlike LSA, term weights may obtain negative \nvalues, indicating that their inclusion in a test vector \nindicates it does not belong to the class in question.   \n \n4. EXPERIMENTAL RESULTS \nThe USPop2002 dataset [15] was used because of its \npopularity in MIREX Contests [1] and because many \nsongs are described by the MGP. Of the 8764 tracks in \nthe dataset, 3108 song descriptions were found. After \ntext normalization, a list of 375 words was used in the \nanalysis. \n4.1. Song Clustering \nSVD was performed on the word-document matrix \nfollowing LSA. Singular values were kept based on the \npercentage volume of the full matrix.  Specifically, if the \nsingular values are ordered such that \u00041 \u0005 \u00042 \u0005 … \u0005 \u0004Q \nthen the minimum number, Qo, is found such that \n vol Q\niiQ\nii\nt\nll\n£\n\u0001\u0001\n==\n12120\n                        (11) \nIt was found that the number of clusters and the \ncluster sizes increase as \u0002L decreases because the criteria \nfor similarity is relaxed. The number of clusters did not \nchange much as \u0002vol was varied, which demonstrates that \nthe attribute lists are largely noise-free. A possible \nreason is that (after text normalization), the final \nattribute set is fairly independent in meaning. \nThe larger clusters were examined when \u0002vol = 0.80 \nand \u0002L was varied. When \u0002L = 0.95, the largest cluster \ncontained six songs: \n1. \"Tiger\" by Abba \n2. \"The Ballad of El Goodo\" by Big Star \n3. \"Flowers\" by New Radicals \n4. \"Simple Kind of Life\" by No Doubt \n5. \"How's it Going to Be\" by Third Eye Blind \n6. \"She Takes Her Clothes Off\" by Stereophonics \n \nWhile this might seem a bit eclectic, all songs had \nvery similar attribute lists. Specifically, all songs were \ndefined by a basic rock song structure, mixed acoustic \nand electric instrumentation, a vocal harmony, and \nmajor key tonality. In addition, all contain acoustic \nrhythm guitars except \"Tiger,\" which contains an \nacoustic rhythm piano. \nThe other clusters in the \u0002L = 0.95 were fairly small \n(3-4 songs) and usually contained a single artist (e.g., \nAC/DC, Nirvana, Blink 182). However, as these clusters \ngrew, they started to obtain more variety in artists and \nindividual artists also appeared in multiple categories. \nClear cluster definitions also started to emerge, such as a \ngroup based on “basic rock structure,” one with \n“electronica and disco influences”, and a third \ncontaining “thin orchestration and radio friendly \nstylings.” 4.2. Discriminative Trained Clusters \nAs stated in Section 2.4, discriminative training provides \nmaximum separation in terms of misclassification by \nemphasizing features to strengthen within-class \nsimilarity, while pushing separate classes farther apart. \nFurther, discriminative training on LSA unigram vectors \nhas been shown to be as effective as trigram models \n[13]. To understand the difference in class separation \nbetween the unsupervised clusters versus more \ntraditional taxonomies, the empirical risk was examined \nfor both cases.  The unsupervised clusters with more \nthan 10 songs at thresholds of \u0002vol = 0.85 and \u0002L = 0.80 \nwere fed to the DT classifier. In addition, a separate \nclassifier was trained, but the labels for each song came \nfrom the genre label that could be found for the artist at \nallmusic.com. \nFigure 2 shows the empirical loss as defined in (10). \nConvergence is quicker when using the unsupervised \nmusical attribute clusters than clustering using genre \nlabels. While genre does eventually reach a minimum of \naround 4% after 16000 iterations, the empirical loss is \nstill higher than the unsupervised approach. \n \nFigure 2. Empirical loss for the discriminatively \ntrained song description clusters. \nThe drop in empirical loss from the discriminative \ntraining procedure is due to better separation between \nthe different clusters. This can be seen from the change \nin the misclassification function described in (6), as is \nshown in Error! Reference source not found.. \nSpecifically, the distribution is shifted left, indicating \nfewer misclassifications. \n \nFigure 3.  Histogram of the d-values in Eq. (6) before \nand after GPD training for the unsupervised clusters.   \n \nAnother reason for the drop in empirical loss is that \nfeatures which better represent a cluster are emphasized. \nFurther, features that negatively correlate with a class \nwill have a negative weight and other features, which do \nnot increase performance, are given values close to zero. \nThe example mean vector in Figure 4 demonstrates the \neffect of discriminative training on term weights. From a \nqualitative standpoint, the terms with the biggest \npositive and negative weight indicate this class contains \nmany songs with acoustic guitars riffs and do not feature \nbreathy (vocals), antiphony, or minor tonality. \n \n \nFigure 4. Attribute weights for an example cluster \nmean before and after discriminative training. \n5. DESCRIPTORS FOR TRADITIONAL TAXONOMY \nThe previous section demonstrated that the unsupervised \nclusters performed better in providing acoustic-based \ndescriptors than the traditional genre labels given in \nMIR applications. As online retailers obtain more of the \nmarket share for music distribution, one could expect for \nindividual songs to obtain genre labels. However, \ncognitive studies show that even in terms of individual \nsongs, humans often make genre decisions based on \nnon-acoustical cues [5]. To test the impact of assigning \ngenre labels at the song level, a classification experiment \nwas performed using attributes from MGP. \nSome attributes contained typical genre labels, such \nas \"hip-hop roots\" and \"basic rock song structure.\" \nSongs with these attributes were manually labelled to the \nprescribed genre and only songs with a single genre \nattribute were used. A total of 25 genres were found and \neach genre was considered in a flat hierarchy. This was \ndone to better understand the acoustic attributes that \ndescribed potential sub-classes. Only genres with more \nthan 40 songs were retained, resulting in the following \ngenres: rock, pop-rock, r&b, rap, and country. LSA and \ndiscriminative training were performed on the retained \nsongs.  Further, bigram and unigram information was \nexamined.  In addition, a comparison was made between \nperforming LSA prior to SVD and using simple word \ncounts (i.e., no entropy-normalization was performed). \nThe empirical loss in Figure 5 suggests that entropy \nnormalization on unigrams slows convergence. Given the nature of musical attributes, this is not surprising. \nOne difference between MIR and text information \nretrieval is that rarity does not necessarily translate to \nbetter retrieval results. For example, an important \ndescriptor in human similarity judgements is major and \nminor tonality [5]. However, since almost every piece of \nWestern music contains either/both major and minor \ntonality, the indexing power given to the terms is very \nlow since it occurs in almost every song attribute list. \nFurther, bigram counts appear to hurt overall accuracy. \nThe most likely reason for this is that many bigrams are \nunimportant and better text normalization might improve \nperformance. \n \n \nFigure 5. Empirical loss for unigram and bigram word \ncounts and unigram and bigram entropy-normalized \ncounts using MGP genre labels. \nTo test the generalization ability of the genre mean \nvectors found, another set of 5825 songs from MGP was \nclassified. The resulting confusion matrix for the raw \nunigram counts is shown in Table 1. \nThe overall accuracy was found to be 80.26%, \nindicating that musical attributes do not consistently \nappear within a genre. These initial results indicate that \neven musicologists are unable to correlate acoustic \ndescriptors on any temporal scale with genre \ntaxonomies. If the gap between low-level cognitive \nfeatures cannot describe the high-level attribute of genre \nassociation, there is little hope that low-level acoustic \nattributes will perform better.  These results should not \nbe interpreted as saying that genre recognition is an \nimpossible task. Instead, this indicates that non-\nacoustical information may be necessary in order to \naccomplish the genre recognition task. \n \n C PR RB RA RK \nC 49.65 2.8 2.45 0 45.1 \nPR 1.81 15.03 8.81 0 74.35 \nRB 0 14.35 38.12 0 47.54 \nRA 3.31 0.74 2.21 87.5 6.25 \nRK 1.34 4.72 1.68 0.09 92.16 \nTable 1. Confusion matrix for MGP genre labels.  C = \ncountry, PR = pop-rock, RB = r&b, RA = rap, RK = \nrock   \n \n6. CONCLUSIONS \nCognitive studies have shown that genre and other \nsimilarity labels often ascribed to music contain some \nnon-acoustical information, which is impossible for an \nacoustic classifier to learn. With the use of web-services, \ninformation on acoustic attributes can be gathered and \ntaxonomy can be created. By seeking specific acoustic-\nbased information, databases can be built such that \nground-truth labels are given at the song level, based on \nmeasurable acoustic attributes, and exclude non-acoustic \ninformation. This reduces the potential of over-\ngeneralizing these systems to match an unrealizable \nperformance level.  Further, acoustic similarity can be \ndefined in terms of relevance scores based on acoustic \nattributes. \nThis paper has presented an approach for designing \nacoustic MIR databases. First, musical similarity \nground-truth labels were built by matching musical \nattributes using LSA and document clustering \ntechniques. Further class separation was achieved using \na discriminative training procedure, which not only finds \nterms that are important for the class in question, but \nalso finds terms whose inclusion in a song indicates dis-\nsimilarity. Second, the variation within a genre and \nbetween different genres was examined using a similar \nprocedure, but with ground-truth assigned by traditional \ngenre labeling procedures. The results indicate that more \ninformation is used to assign genres than acoustic \ninformation. \n7. REFERENCES \n[1] \"MIREX 2006 Results,\" [Web site] 2006, \n[2007 March 24], Available: \nhttp://www.music-\nir.org/mirex2006/index.php/MIREX200\n6_Results. \n[2] J.-J. Aucouturier and F. Pachet. \"Representing \nmusical genre: a state of the art,\" J. New Music \nResearch, vol. 32, no. 1, pp. 1-12, 2003. \n[3] H. G. Tekman and N. Hortacsu. \"Aspects of \nstylistic knowledge: what are the different \nstyles and why do we listen to them?\" \nPsychology of Music, vol. 30, pp. 23-47, 2002. \n[4] C. McKay and I. Fujinaga. \"Musical genre \nclassification: is it worth pursuing and how can \nit be improved?\" ISMIR 2006, Victoria, \nCanada, pp. 101-107, 2006. \n[5] D. Huron. Sweet Anticipation: Music and the \nPsychology of Expectation. MIT Press, 2006. \n[6] B. Whitman and P. Smargdis. \"Combining \nmusical and cultural features for intelligent \nstyle detection,\" ISMIR 2002, pp. 47-52, 2002. \n[7] J. Reed and C.-H. Lee, \"A study on music \ngenre classification based on universal acoustic models,\" ISMIR 2006, Victoria, Canada, pp. \n89-94, 2006. \n[8] A. Lamont and N. Dibben, \"Motivic structure \nand the perception of similarity,\" Music \nPerception, vol. 18, no. 3, pp. 245-274, Spring \n2001. \n[9] J.H. Lee and J.S. Downie. \"Survey of music \ninformation needs, uses, and seeking behaviors: \npreliminary findings,\" ISMIR 2004, Barcelona, \nSpain, 2004. \n[10] C.D. Manning and H. Schütze. Foundations of \nStatistical Natural Language Processing.  MIT \nPress, 1999. \n[11] J. Bellegarda, \"Exploiting latent semantic \ninformation in statistical language modelling,\" \nProc. IEEE, vol. 88, no. 3, pp. 1279-1296, \nAugust 2000. \n[12] G. Strang. Linear Algebra and its Applications. \nHarcourt, Inc., 1998. \n[13] H.-K. Kuo and C.-H. Lee. \"Discriminative \ntraining of natural language call routers,\" IEEE \nTrans. Speech and Audio Proc., vol. 11, no. 1, \npp. 24-35, Jan. 2003. \n[14] B.-H. Juang, W. Chou, and C.-H. Lee, \n\"Minimum classification error rate methods for \nspeech recognition,\" IEEE Trans. Speech and \nAudio Proc., vol. 5, no. 3, pp. 257-265, May \n1997. \n[15] A. Berenzweig, B. Logan, D. Ellis, and B. \nWhitman. \"A large-scale evaluation of acoustic \nand subjective music-similarity measures.\" \nComputer Music Journal, vol. 28, iss 2, pp. 63-\n76, June 2004."
    },
    {
        "title": "Algorithms for Determining and Labelling Approximate Hierarchical Self-Similarity.",
        "author": [
            "Christophe Rhodes",
            "Michael A. Casey"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418095",
        "url": "https://doi.org/10.5281/zenodo.1418095",
        "ee": "https://zenodo.org/records/1418095/files/RhodesC07.pdf",
        "abstract": "We describe an algorithm for finding approximate sequence similarity at all scales of interest, being explicit about our modelling assumptions and the parameters of the algorithm. We further present an algorithm for producing section labels based on the sequence similarity, and compare these labels with some expert-provided ground truth for a particular set of recordings. 1 INTRODUCTION",
        "zenodo_id": 1418095,
        "dblp_key": "conf/ismir/RhodesC07",
        "keywords": [
            "algorithm",
            "sequence similarity",
            "approximate",
            "modelling assumptions",
            "parameters",
            "section labels",
            "expert-provided ground truth",
            "recordings",
            "comparison",
            "explicit"
        ],
        "content": "ALGORITHMS FOR DETERMINING AND LABELLING APPROXIMATE\nHIERARCHICAL SELF-SIMILARITY\nChristophe Rhodes, Michael Casey\nDepartment of Computing, Goldsmiths\nUniversity of London, SE14 6NW\nABSTRACT\nWe describe an algorithm for ﬁnding approximate se-\nquence similarity at all scales of interest, being explicit\nabout our modelling assumptions and the parameters of\nthe algorithm. We further present an algorithm for produc-\ning section labels based on the sequence similarity, and\ncompare these labels with some expert-provided ground\ntruth for a particular set of recordings.\n1 INTRODUCTION\nMethods for detecting similar regions in music record-\nings have many applications, for example in music sum-\nmarization; song identiﬁcation; audio compression; and\ncontent-based music query systems. Approaches to sim-\nilarity detection and segmentation of musical audio have\nbeen based on many audio features, such as timbre or ‘the\nway it sounds’ [1, 2], chroma or harmonic features [3], or\npartial transcription [4].\nWe present in this paper a top-down method for gener-\nating a tree of regions within a track related by similarity,\nwhere that similarity is deﬁned by the user’s choice of au-\ndio feature and processing method, by an acceptable error\nrate, and by the predicate for determining whether two se-\nquences match; we further present a method for assigning\nlinear structure labels to regions given such a tree. We\ndiscuss our motivation in section 1.1 and related work in\nsection 1.2, before presenting our algorithms in section 2.\nSome preliminary experimental validation is presented in\nsection 3, and we conclude in section 4.\n1.1 Motivation\nThe initial motivation for this work was provided by the\nCHARM1project, with an inquiry about ﬁnding simi-\nlar regions in audio tracks, with particular reference to\nalmost-literal repeats in recordings of Chopin Mazurkas.\nIn fact, with a known score, the approach to solving that\ntask would likely be very different from a use of the al-\ngorithms presented here: an approach based on aligning\nthe score (or a MIDI version) to the recoded audio [5, 6],\nand looking for discontinuities (which would indicate a\n1Centre for the History and Analysis of Recorded Music.\nc/circlecopyrt2007 Austrian Computer Society (OCG).abcdefg h ij\nabkdelm h in\n0010011 001\n120120-2 -1 0-2\nFigure 1 . The upper two rows contain two substrings of\nlength ten being considered for matching, with the per-\ncharacter match summarized in the third row ( 0for match-\ning,1for a mismatch at a given position). The lowest row\nshows the alignment for a match score of 1and a substi-\ntution penalty of 2. A sequence alignment would prefer a\nmatch of length 2or5, whereas given an error rate of1\n3we\nwish to consider the ﬁrst nine characters as our preferred\nmatch.\nrepeat or an omitted section). However, in more general\ncontexts, it is important to be able to identify repeated sec -\ntions with less a priori knowledge than a notated score\nwith written-out repeated sections: this paper considers\nprimarily working directly from recorded audio, though\nthe techniques described are applicable to ﬁnding struc-\nture in music in transcribed formats (such as MIDI).\nA secondary motivation behind the approach that we\ntook is to minimize the number of parameters in the algo-\nrithm, and to have those parameters which remain have a\nstraightforward interpretation in terms of the original se -\nquence being investigated for self-similarity.\nWe ﬁrst aim to identify pairs of regions which match\neach other (with a certain allowed error rate). We make\nsome assumptions about the structure of the matches that\nwe are interested in, the primary assumption being that\nthe matched regions are arranged in a hierarchical fashion:\nthat boundaries on large scales are not crossed by smaller-\nscale matches. Note that we do not wish to claim that all\nmusical structures are arranged in a single hierarchy, but\nthat when working with one particular kind of structure\n(induced over one particular audio feature) it is likely tha t\na hierarchical arrangement is a reasonable approximation.\nOnce we have identiﬁed the regions related by pairwise\nsimilarity, we also wish to summarize this information in\nsome simple way; in order to compare the results from our\nalgorithm with ground truth from the CHARM project, we\nderive structure labels from the pairwise similarity data.\nNote that a simple application of sequence alignment\nas commonly used in bioinformatics [7] is not appropri-ate for this problem, as our hierarchical criterion leads us\nto prefer long acceptable matches over short ‘better’ ones\n(see ﬁgure 1).\n1.2 Related Work\nMany existing methods of determining areas of track self-\nsimilarity are based on the S-matrix [8] containing a mea-\nsure of dissimilarity for short-time feature vectors; this\nlarge object, related to recurrence plots [9], is then in-\nvestigated for diagonal lines of low dissimilarity. Equiv-\nalently, the time-lag matrix as used in [10] and [11] is a\nrotation of half of the S-matrix, and regions related by\nsimilarity are indicated by horizontal lines. These author s\nthen post-process their matrices (in whatever orientation )\nby operations inspired by image processing ( e.g.erosion\nand dilatation), to attempt to enhance the relevant regions\nand eliminate noise; then lags corresponding to repeated\nsegments are detected by averaging the dissimilarity for a\ngiven lag and thresholding. Where these previous works\nuse short-time audio features and simple smoothing tech-\nniques, others (such as [12, 13]) generate a smoothed S-\nmatrix by applying dynamic time warping to regions in-\ntermediate in size between individual audio frames and\nlikely segment sizes.\nThe problem of assigning structure labels to tracks is\naddressed in some of these works. In some, the task at\nhand was to detect speciﬁc kinds of segment (the cho-\nrus in [10], for example), and so only that subtask was\naddressed, though some treatment of transitive closure of\npairwise relations is discussed. In [11], heuristic method s\nfor converting from pairwise-similar regions to structure\nlabels are discussed, along with methods for dealing with\noverlaps; the method of [14] for building an ‘explanation’\nof pairwise or clustered structure resembles the structure\nlabels that we generate, though the explanation is sensitiv e\nto the order in which the pairwise clusters are processed;\nin [15] an explicit cost function for explanations is intro-\nduced. There is a discussion of structure labelling and\ntree similarity from a bottom-up viewpoint in [13]; the la-\nbelling suffers from overlap conﬂicts which are resolved\nby an ordering by repetition count in a potentially lossy\nway, in contrast to the scheme described in section 2.2 be-\nlow.\n2 ALGORITHMS\nWe take as given a string Sof symbols over a given alpha-\nbet of length L, and a matchp predicate, which evaluates\nwhether two substrings of a given length (from the same\nalphabet) match. The matching is such that a certain per-\ncharacter error rate 0≤α≤1is acceptable, and that per-\ncharacter error rate is a constant for all substring lengths :\nthis formulation of matching allows a form of memoiza-\ntion, in that if the number of mismatches between the start\nof the substrings and position p < l exceeds αl, the max-\nimum permitted errors for a match of length l, then the\nmaximum number of permitted errors αl′for matches ofababbcdc0 0 101 1 0 0abcbcbdc0 p l\ns1s2\nFigure 2 . An illustration of matchp ’s behaviour. In this\nexample, for a prospective match of length l= 8, with\nα=1\n4, we ﬁnd after checking the sixth character that we\nhave exceeded the maximum allowed number of errors ( 2)\nfor matches of length 8, and so that we need not check this\npair of start points until lis smaller than 6(and eventually\na match will be found for these start positions at l= 4.\nlength l′forp≤l′< lwill also be exceeded by at most\nposition p(see ﬁgure 2 for an illustration of this); this pis\na secondary return value of matchp .\nWe also assume a substr subroutine which extracts or\notherwise indicates a substring of a string given a start\npoint and a length.\nAlthough we have described the algorithm in terms of\na string over a ﬁnite alphabet of symbols, it is straightfor-\nward to adapt it to a vector of continuous observations of\narbitrary dimensions in a ﬁnite metric space, with matchp\nadapted to consider a normalized distance measure be-\ntween observations instead of a boolean comparison be-\ntween symbols.\n2.1 Generating Pairwise Matches\nThe ﬁrst piece of our overall algorithm is the nextPossi-\nblePair function described in algorithm 1, which ﬁnds the\nnext possible pair of start indices s1,s2(given the current\nvalues) for a match of length lin a string of total length L.\nAlgorithm 1 nextPossiblePair (s1,s2,l,L )→s′\n1,s′\n2\nifs1=L−2lthen\nreturn⊥\nelse if s2=L−lthen\nreturn s1+ 1,s1+ 1 + l\nelse\nreturn s1,s2+ 1\nend if\nAlgorithm 2 is a brute-force method for ﬁnding the\nlongest matching matching regions in a string; it is too\nslow for our purposes: for length lmatches in a string of\nlength L, there are\nL−2l/summationdisplay\ni=0(L−2l+ 1−i) =1\n2(L−2l+ 1) (L−2l+ 2)\npossible start pairs, each of which will do O(l)work to\nperform the matchp operation. In the worst (no match)case, we do this for all l0≤l≤L\n2, giving overall work of\nO(L4).\nAlgorithm 2 Longest pairwise match, brute force\nforldownfrom/floorleftbigL\n2/floorrightbig\ntol0do\n(s1,s2)←(0,l)\nrepeat\nifmatchp (substr (S,s1,l),substr (S,s2,l))then\nreturn s1,s2,l\nend if\n(s1,s2)←nextPossiblePair (s1,s2,l,L)\nuntil(s1,s2) =⊥\nend for\nEven if the constant terms in front of the highest-order\nterms are small, this is prohibitively expensive for string s\ncorresponding to audio tracks at, say, one symbol per sec-\nond. We can, however, improve on this with relatively\nlittle effort, making this search practical for the sizes of\nstrings that we are dealing with. We can build a cache Aij,\nindexed by start positions i,j, of positions pat which the\nmatchp predicate discovered that the per-character error\nrate for a match of length lwould be greater than the per-\nmitted error rate. This then implies that the per-character\nerror rate for any smaller match l′≥pmust also be larger,\nso we do not need to call matchp again with those start in-\ndices until the length of the putative match is less than p.\nAlgorithm 3 Longest pairwise match, cacheing\nAij←/floorleftbigL\n2/floorrightbig\n+ 1for all 0≤i,j < L\nforldownfrom/floorleftbigL\n2/floorrightbig\ntol0do\n(s1,s2)←(0,l)\nrepeat\nifl < A s1s2then\n(m,p)←matchp (substr (S,s1,l),substr (S,s2,l))\nend if\nifmthen\nreturn s1,s2,l\nelse\nAs1s2←p\nend if\n(s1,s2)←nextPossiblePair (s1,s2,l,L)\nuntil(s1,s2) =⊥\nend for\nWe thus amortise the O(l)work of matchp , overl−p∼\n(1−α)lcomparisons (where αis the allowed error rate),\nthus reducing the overall complexity of the algorithm to\nO(L3)at a cost of O(L2)space; note that the smaller α\nis, the lower the constant of proportionality in front of the\nL3.\nAt no extra cost in work, we can turn this into an algo-\nrithm for ﬁnding all relevant matches at all length scales\nof interest by tracking two more pieces of information: the\nmatches themselves, and the inferred boundaries; when a\nmatch is found, the new boundaries alter the generation of\nall subsequent possible s1,s2pairs.Algorithm 4 nextPair (s1,s2,l,L,B )→s′\n1,s′\n2\nlocal predicate admissiblePair (s1,s2,l,B ):\n∄i: [(s1< i < s 1+l)∨(s2< i < s 2+l)]∧(i∈B)\ns1,s2←nextPossiblePair (s1,s2,l,L)\nif(s1,s2) =⊥then\nreturn⊥\nelse if admissiblePair (s1,s2,l,B)then\nreturn s1,s2\nelse\nreturn nextPair( s1, s2, l, L, B )\nend if\nIn algorithm 4, the admissiblePair local predicate de-\ntermines whether the proposed pair of regions (designated\nby start indices s1,s2and length l) overlaps any already-\ndetected boundaries (in B). One simple way of imple-\nmenting this simply is to represent the string Sas a linked\nlist of regions between boundaries, performing list splic-\ning in constant time when new boundaries are identiﬁed.\nAlgorithm 5 additionally ensures that once two regions\nhave been identiﬁed as being pairwise related, then no\npairs of subregions from those regions will be considered.\nThis will not prevent us from ﬁnding relevant substruc-\nture, however, as any such will necessarily have at least\none pair of regions not so excluded.\nAlgorithm 5 All pairwise matches\nAij←/floorleftbigL\n2/floorrightbig\n+ 1for all 0≤i,j < L\nB←{} ;M←{}\nforldownfrom/floorleftbigL\n2/floorrightbig\ntol0do\n(s1,s2)←(0,l)\nrepeat\nifl < A s1s2then\n(m,p)←matchp (substr (S,s1,l),substr (S,s2,l))\nend if\nifmthen\nAij←0fors1≤i < s 1+l,s2≤j < s 2+l\nB←B∪{s1,s1+l,s2,s2+l}\nM←M∪{(s1,s2,l)}\nelse\nAs1s2←p\nend if\n(s1,s2)←nextPair (s1,s2,l,L,B )\nuntil(s1,s2) =⊥\nend for\nreturn M\n2.2 Assigning Labels\nThe algorithm in section 2.1 generates a set of pairwise-\nmatched regions M, which because of the hierarchical as-\nsumption deﬁnes a tree of similarity. This tree contains all\nthe information that is needed; however, structure labels\nare a good way of summarizing this information (see e.g.\n[13]).Figure 3 . Illustration of transitive closure: the top half of\nthe diagram represents detected pairwise matches, while\nthe line at the bottom is the division into regions. Note\nthe leftmost small region, which is induced by the pair-\nwise similarity of a region which to another which itself\ncontains a match.\nAs a ﬁrst step towards producing a summary, we will\ndivide up the sequence into regions whose points share the\nsame symmetries, those symmetries being the transitive\nclosure of the pairwise similarities (see ﬁgure 3). Then, to\ngenerate structure labels, we will assign labels to regions\nin decreasing order of size until each pairwise match from\nthe original detection contains at least one label (and unti l\nall unlabelled regions are sufﬁciently small). This has an\neffect similar to the heuristics given in the structure anal -\nysis section of [11] and the cluster splitting in [14].\nAlgorithm 6 Transitive closure from pairwise matches M\nTi←{} for all 0≤i < L\nfor(s1,s2,l)inMordered by ascending size do\nTs1:s1+l←Ts1:s1+l∪Ts2:s2+l∪{(s1,s2,l)}\nTs2:s2+l←Ts1:s1+l\nend for\nreturn T\nAlgorithm 6 illustrates computation of the transitive\nclosure of pairwise matches; because of our hierarchical\nconstraint of these pairwise matches, we can simply it-\nerate over all matches in order of increasing size, as we\nknow that no part of a larger match can be contained in\na smaller match. The vector Tis then segmented into re-\ngions of related similarity, where a region is deﬁned as\na contiguous set of entries where the set of transforma-\ntions is the same and none of those transformations has a\nboundary in that region. This segmentation by transforma-\ntions contains the equivalent information to the pairwise\nsimilarities, but is in a form that is easier to interpret.\nWe then label this segmentation by sorting by size of\nsegment (resolving ties by grouping related segments to-\ngether), and assigning labels in decreasing segment size,\ncontinuing until both every pairwise match detected has\nhad at least one label assigned to a subregion, and until\nthe region size is under some salient length l′\n0(which can\nbut need not be the same as l0in section 2.1).3 EXPERIMENTAL DETAILS\nOur test corpus consists of twenty-seven recordings of the\nmazurka in A minor, Op. 7 Nr 2 by Chopin. Table 1 il-\nlustrates the structure of the mazurka on various levels:\nthe notated score is in four sections, labelled A,B,Cand\nD. SectionsA,BandDare sixteen bars long, and are\nnotated to be repeated; section Cis eight bars long and is\nonly played once. Additionally, the repeats of BandD\nhave small differences in the ﬁnal bar, and a da capo al\nﬁneis speciﬁed, so section Ais notated to be played again\n(once) at the end.\nWe convert the audio recordings into a sequence of\nsymbolic labels by performing an initial segmentation by\ntimbral features into ﬁve segment classes according to the\nmethod of [16], and generating a string with one segment\nlabel per second. This segmentation can be an accurate\nstructural segmentation in itself for certain kinds of mu-\nsic [17] but in the case of solo piano music, where tim-\nbral changes do not indicate structural changes directly,\nthe effect of this prior segmentation is to perform tem-\nporal smoothing of the audio features, allowing a lower\nvalue of αand allowing us not to have to perform dynamic\ntime warping. It is important to note that this preprocess-\ning step is independent of the algorithms described herein,\nwhich can be used on any sequential data with a normal-\nized distance measure.\nTable 2 presents some experimental results, where we\nused a threshold error rate of α=1\n12and a minimum\nlength l0=l′\n0= 10 corresponding to a time of 10s. Our\nalgorithm working on the processed audio as described\nabove gives labellings corresponding with the (corrected)\nground truth in nine of the twenty-seven cases, where we\ntreat our ‘CCD’ sequence as equivalent to the ground truth\n‘CDD’ for reasons discussed below.\nThe ﬁrst thing to note is that there is more structure\nto this Mazurka than is evident from the ‘ground truth’\nlabelling: the ﬁrst row in table 1 describes the similarity\nrelationships on an eight-bar metrical grid. This substruc -\nture explains why we have accepted ‘CCD’ from our al-\ngorithm as equivalent to ‘CDD’ in the ground truth, as it\ncorresponds to the deded section in the actual score: and\nthere is no way of distinguishing from just the audio that it\nis notated in ‘CDD’ fashion. Further, we see some of this\neight-bar substructure being detected by the algorithm in\nthe recordings by Smith (1975) and Indjic (2001); indeed,\nthe algorithmic answers for those two recordings are a fair\nreﬂection of the performance in question.\nThere are other classes of discrepancy between the al-\ngorithmic labels and the ground truth: in three cases, the\nalgorithm has failed to label the ‘orphan’ segment in the\ndeded section (and in some others, there is another single\nmissing segment); in several cases, there is an unmatched\nlabel at the end of the string, presumably corresponding\nto silence. Because of the way our algorithm is struc-\ntured, the single label for the inner sections of the Franc ¸o is\nrecordings (without repeats) is as correct as it can be.\nFinally, we note that in the light of recent revelationsa b a b c b c b d e d e d a b\nA A B B C D D A\n/bardbl:A:/bardbl /bardbl :B:/bardbl C /bardbl :D:/bardbl d.c.\nTable 1 . The structure of Chopin’s mazurka in A minor, Op. 7 Nr 2. The t op line corresponds to eight-bar units, allowing\nfor small differences in the musical material at the beginni ng and end of the eight bars. The middle line corresponds to th e\nground truth labels provided by an expert for a performance c orresponding to the notated score represented in the bottom\nline.\nRecording Algorithmic labels Ground Truth (a) (b) (c) (d)\nAshkenazy (1981) ABCCCDDA AABBCDDA ×\nBiret (1990) ABCCDDEB AABBCDDA ×\nBlock (1995) AABBCDCE AABBCDDA ×\nBrailowsky (1960) AABBCCDA AABBCDDA\nChiu (1999) ABCCDDBE AABBCDDA ××\nClidat (1994) AABBCCA AABBCDDA ×\nCortot (1951) ABCCDEEFAG AABBCDDA ××\nFalvay (1989) ABCCDEEFG AABBCDDA ×\nFiorentino (1962) AABBCCDA AABBCDDA\nFli`ere (1977) AABBCCDA AABBCDDA\nFranc ¸ois(1956) ABA ABCDA\nFranc ¸ois (1966) AABA ABCDA ×\nFriedman (1930) ABBCCD AABBCDDA* ×\nHatto (1997) ABBCDDA AABBCDDA*\nIndjic (2001) ABCBCBDDEB AABBCDDA* ××\nKapell (1951) AABBCCDA AABBCDDA\nLuisada (1990) AABBCCA AABBCDDA ×\nMagaloff (1977) AABBCCDA AABBCDDA\nPobłolcka (1999) AABBCCDAB AABBCDDAB\nRubinstein (1939) AABCCDA AABCDDA\nRubinstein (1952) AABBCCDA AABBCDDA\nRubinstein (1966) ABCCDEEFA AABBCDDA ×\nShebanova (2002) AABBCCA AABBCDDA ×\nSmith (1975) ABABCBCBDDEABF AABBCDDA ××\nTs’ong (1993) ABCCDDEA AABBCDDA ×\nTs’ong (2005) AABCCAD AABBCDDA ××\nUninsky (1959) ABCBCDDCE AABBCDDA* ××\nTable 2 . The labels derived using the algorithms presented in secti on 2 for the set of 27 performances of Chopin’s Mazurka\nOp. 7 Nr 2 in A minor. Note that four of the ground truth labels p rovided by an expert listener (starred) are incorrect,\nand should read ABBCDDA. Various classes of discrepancy bet ween ground truth and algorithmic labels are summarized\non the right of the table: (d) indicates labelling silence at the end of the track; (c) indicates missing one segment; (b) i s\nmarked if the algorithm has labelled structure at a ﬁner deta il than the ground truth, and (a) is for other errors, most oft en\nfrom failure to detect a pairwise match.about the discography of Hatto [18], we can assess from\nthese results a certain amount about the sensitivity of the\nalgorithm to the audio processing chain used in this case,\nas the input audio of the Hatto (1997) and Indjic (2001)\nrecordings is for all practical purposes identical, while t he\naudio processing has some random elements. The differ-\nence in algorithmic labels between those two recordings\nthus indicates that our method as presented is sensitive to\nfeatures of the audio processing chain.\n4 CONCLUSIONS\nWe have presented methods for detecting and labelling hi-\nerarchical structure in sequential data, with a small set\nof parameters which are straightforwardly interpretable;\nthe preliminary results from these methods on a stringent\ntest are encouraging. The method as presented assumes\namatchp predicate which works character-by-character;\nhowever, real music often undergoes temporal alterations\nbetween regions of similar material. In this investigation ,\nwe dealt with that issue by having an elaborate processing\nchain; however, there is nothing to stop a different matchp\npredicate including some dynamic time warping: the chal-\nlenge would be to preserve efﬁciency. A more straightfor-\nward reﬁnement to the method presenting here would be\nsimply to prohibit pairwise matches from being detected\nas starting or ending on a mismatched character, which\ncould be incoroporated into the initialization of Aij. Fi-\nnally, we note again that the methods presented here are\nnot speciﬁc to audio processing, and are in use in analysis\nof a large database of MIDI performance transcriptions.\nAcknowledgments\nWe thank Craig Sapp and Raphael Clifford for helpful\ndiscussions. This work was supported by EPSRC grant\nGR/S84750/01.\n5 REFERENCES\n[1] Jean-Julien Aucouturier, Franc ¸cois Pachet, and Mark\nSandler. The Way It Sounds: Timbre Models For\nAnalysis and Retrieval of Polyphonic Music Signals.\nIEEE Transactions of Multimedia , 2005.\n[2] Samer Abdallah, Katy Noland, Mark Sandler, Michael\nCasey, and Christophe Rhodes. Theory and evaluation\nof a Bayesian music structure extractor. In Joshua D.\nReiss and Geraint A. Wiggins, editors, Proc. ISMIR ,\npages 420–425, 2005.\n[3] Mark A. Bartsch and Gregory H. Wakeﬁeld. To Catch\na Chorus: Using Chroma-Based Representations for\nAudio Thumbnailing. In Proc. WASPAA , 2001.\n[4] N. Maddage, X. Changsheng, M. Kankanhalli, and\nX. Shao. Content-based Music Structure Analysis\nwith Applications to Music Semantics Understanding.\nIn6th ACM SIGMM MIR Workshop , October 2004.[5] Ferr ´eol Soulez, Xavier Rodet, and Diemo Schwarz.\nImproving Polyphonic and Poly-Instrumental Music\nto Score Alignment. In Proc. ISMIR , pages 143–148,\n2003.\n[6] Christopher Raphael. A Hybrid Graphical Model for\nAligning Polyphonic Audio with Musical Scores. In\nProc. ISMIR , pages 387–394, 2004.\n[7] M. S. Waterman and M. Eggert. A New Algorithm\nfor Best Subsequence Alignments with Application to\ntRNA-rRNA Comparisons. Journal of Molecular Bi-\nology , 197:723–728, 1987.\n[8] Jonathan Foote. Visualizing Music and Audio using\nSelf-Similarity. In ACM Multimedia (1) , pages 77–80,\n1999.\n[9] J.-P. Eckmann, S. O. Kamphorst, and D. Ruelle. Re-\ncurrence Plots of Dynamical Systems. Europhysics\nLetters , 5:973–977, 1987.\n[10] Matasaka Goto. A Chorus Section Detection Method\nfor Musical Audio Signals and Its Application to a\nMusic Listening Station. IEEE Transactions on Audio,\nSpeech and Language Processing , 14(5):1783–1794,\n2006.\n[11] L. Lu, M. Wang, and H. Zhang. Repeating pattern\ndiscovery and structure analysis from acoustic music\ndata. In 6th ACM SIGMM MIR Workshop , October\n2004.\n[12] Meinard M ¨uller and Frank Kurth. Towards Structural\nAnalysis of Audio Recordings in the Presence of Mu-\nsical Variations. Advances in Signal Processing , 2007.\n[13] Wei Chai. Automated Analysis of Musical Structure .\nPhD thesis, MIT, 2005.\n[14] R. Dannenberg and N. Hu. Discovering musical struc-\nture in audio recordings. In Music and Artiﬁcal Intel-\nligence: Second International Conference , Edinburgh,\n2002.\n[15] Jouni Paulus and Anssi Klapuri. Music Structure\nAnalysis by Finding Repeated Parts. In 1st ACM AM-\nCMM workshop , 2006.\n[16] Samer Abdallah, Mark Sandler, Christophe Rhodes,\nand Michael Casey. Using duration models to reduce\nfragmentation in audio segmentation. Machine Learn-\ning, 62(2-3):485–515, 2006.\n[17] Christophe Rhodes, Michael Casey, Samer Abdal-\nlah, and Mark Sandler. A Markov-Chain Monte Carlo\nApproach to Musical Audio Segmentation. In Proc.\nICASSP , volume V , pages 797–800, 2006.\n[18] Nicholas Cook and Craig Sapp. Purely coinci-\ndental? Joyce Hatto and Chopin’s Mazurkas.\nhttp://www.charm.rhul.ac.uk/content/\ncontact/hatto article.html ."
    },
    {
        "title": "Algorithms for Polyphonic Music Retrieval: The Hausdorff Metric and Geometric Hashing.",
        "author": [
            "Christian André Romming",
            "Eleanor Selfridge-Field"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417615",
        "url": "https://doi.org/10.5281/zenodo.1417615",
        "ee": "https://zenodo.org/records/1417615/files/RommingS07.pdf",
        "abstract": "We consider two formulations of the computational problem of transposition-invariant, time-offset tolerant, meterinvariant, and time-scale invariant polyphonic music retrieval. We provide algorithms for both that are scalable in the sense that space requirements are asymptotically linear and queries are efficient for large databases of music. The focus is on cases where a query pattern M consisting of m events is to be matched against a database N consisting of n events, and m ≪n. The database is assumed to be polyphonic, and the algorithms support polyphonic queries. We are interested in finding exact and proximate occurrences of the query pattern. The first problem considered is that of finding the minimum directed Hausdorff distance from M to N. We give a (2 + ǫ)-approximation algorithm that solves this problem in O (nm) query time and O (n) space. The second problem is that of finding all maximal subset matches of M in N, and we give an algorithm that solves this problem in O \u0000m3 (k + 1) \u0001 query time and O \u0000w2n \u0001 space, where w represents the maximum window size and k is the number of matches. Using the same method, the problem can be solved in O (m (k + 1)) query time and O (wn) space if we do not require the time-scale invariance property. The latter query time is asymptotically optimal for the given problem. 1 INTRODUCTION We consider two formulations of the computational problem of content-based, polyphonic music retrieval. We suggest algorithms for both, and they are linked in the sense that they employ the same point-set representation of music. The first problem formulation, referred to as the minimum Hausdorff distance problem, considers the dissimilarity of two point sets to be related to the Euclidian distance between points in the set. The second, which we call the maximal subset matching problem, takes the similarity to be related to the number of points in one set that are in the same location as points in the other. Although these two problems might seem similar, the methods required to solve them are inherently different. c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417615,
        "dblp_key": "conf/ismir/RommingS07",
        "keywords": [
            "content-based",
            "polyphonic",
            "music retrieval",
            "minimum directed Hausdorff distance",
            "maximal subset matches",
            "time-scale invariant",
            "polyphonic queries",
            "point-set representation",
            "Euclidian distance",
            "location"
        ],
        "content": "ALGORITHMSFORPOLYPHONICMUSIC RETRIEVAL:THE\nHAUSDORFF METRICAND GEOMETRICHASHING\nChristianAndré Romming\nStanford University\nDept. ofComputerScience\nromming@cs.stanford.eduEleanorSelfridge-Field\nStanford University\nDept. ofMusic\nesﬁeld@stanford.edu\nABSTRACT\nWe considertwo formulationsofthe computationalprob-\nlemoftransposition-invariant,time-offsettolerant,me ter-\ninvariant, and time-scale invariant polyphonic music re-\ntrieval. Weprovidealgorithmsforboththatarescalablein\nthe sense that space requirements are asymptotically lin-\near and queries are efﬁcient for large databases of music.\nThefocusisoncaseswherea querypattern Mconsisting\nofmevents is to be matched against a database Ncon-\nsisting of nevents,and m≪n. The databaseisassumed\nto be polyphonic, and the algorithms support polyphonic\nqueries. We are interested in ﬁnding exact and proximate\noccurrences of the query pattern. The ﬁrst problem con-\nsideredisthatofﬁndingtheminimumdirectedHausdorff\ndistance from MtoN. We give a (2 +ǫ)-approximation\nalgorithm that solves this problem in O(nm)query time\nandO(n)space. The second problem is that of ﬁnd-\ning all maximal subset matches of MinN, and we give\nan algorithm that solves this problem in O/parenleftbig\nm3(k+ 1)/parenrightbig\nquery time and O/parenleftbig\nw2n/parenrightbig\nspace, where wrepresents the\nmaximum window size and kis the number of matches.\nUsing the same method, the problem can be solved in\nO(m(k+ 1))querytime and O(wn)space if we do not\nrequirethetime-scaleinvarianceproperty. Thelatterque ry\ntimeisasymptoticallyoptimalforthegivenproblem.\n1 INTRODUCTION\nWe considertwo formulationsofthe computationalprob-\nlemofcontent-based,polyphonicmusicretrieval. Wesug-\ngest algorithms for both, and they are linked in the sense\nthattheyemploythesamepoint-setrepresentationofmu-\nsic. The ﬁrst problemformulation,referredto as the min-\nimum Hausdorff distance problem,considers the dissimi-\nlarity of two point sets to be related to the Euclidian dis-\ntancebetweenpointsintheset. Thesecond,whichwecall\nthemaximal subset matching problem, takes the similar-\nitytoberelatedtothenumberofpointsinonesetthatare\ninthesamelocationaspointsintheother. Althoughthese\ntwoproblemsmightseemsimilar,themethodsrequiredto\nsolvethemare inherentlydifferent.\nc/circlecopyrt2007AustrianComputerSociety(OCG).1.1 RelatedWork\nItisarguedbyDiLorenzoandDiMaio[3]thattheHaus-\ndorffdistancehas signiﬁcantrelationshipswith perceive d\nmusical similarity. The Hausdorff distance as formulated\ninthispaperisrelatedtotheEarthMoverDistancestudied\nby Typke [9]. The main differenceslie in the metric used\ntomeasuresimilarityandtheindexingmethodsemployed\nin order to make queries efﬁcient. Another related ap-\nproachis that taken by Lubiw and Tanur[5], whose mea-\nsureofsimilarityisbasedontheconsonanceofeventsthat\noverlap in time. The maximal subset matching problem\n(and variants of it) has been studied by several groups.\nSome of the most notable algorithms are the SIA fam-\nily, an excellent overview of which is given by Mered-\nith [6]. A recent randomized algorithm building on the\nSIA algorithmsis MSM, developedby Clifford et al. [2],\nwhich achieves a running time of O(nlogn). Further-\nmore, problem P2 described by Ukkonen et al. [10] is\nclosely related to the maximal subset matching problem\nas deﬁned here. The most notable differencesare that we\nalso requirenotedurationsto matchandthat we allowar-\nbitraryscalingofthepatternset inadditiontotranslatio n.\n1.2 Representation\nWerepresentmusicasthree-dimensionalpointsets,where\nthe three dimensions are pitch, duration, and onset time.\nPoint-setrepresentationsofmusicarecommoninthecon-\ntext ofmelodicsimilarityalgorithms[2, 9]. Anotherpop-\nular representationformat is sets of line segments, where\nthe length of the line segments speciﬁes the durationof a\nmusical event [5, 10]. We choose the point-set represen-\ntation for two main reasons. First, it is extensible in the\nsense that more features such as e.g. harmony and beat\ncan be included by simply adding more dimensions. The\nalgorithms in this paper will still work after such a mod-\niﬁcation (with slightly higher running times). Second, a\npoint-set representation makes it easier to apply some of\nthe ideas used in this paper, such as the Hausdorff metric\nand geometric hashing. Only notes are explicitly repre-\nsented,andornamentsandgracenotesareignored.\nLet us ﬁrst give the details of the point-set represen-\ntation we use. The pitch of an event is encoded using\nHewlett’s base-40 notation [4], so that translations in the\npitchdimensionleaveallintervalsunchanged. Alogarith-micscaleisusedtorepresentduration,sothattranslation s\nin the duration dimension correspondto scaling of actual\ndurations. For example, the distance from a quarter note\ntoaneighthnoteisthe sameasthedistancefromawhole\nnotetoa halfnote.\nWe identify two different ways of representing onsets,\neach with different musical properties. The ﬁrst is to use\na measure-relative scale, that is, we let all measures have\na ﬁxed length (independent of meter) and set onset co-\nefﬁcients accordingly. For example, a note that begins\nhalf-waythroughthe ﬁfth measurewill haveonset 4.5·l,\nwhere lis the measure-length constant. The second is to\nuse an absolute scale. In this system, the meter informa-\ntion is ignored,and the onset coefﬁcient of a point would\nbe proportionalto the aggregatedurationsince the begin-\nningofthework. Usingeitherthe measure-relativeorthe\nabsolute scale, translationsin the onset dimensionclearl y\ncorrespondto a time-shift of the music. To see the differ-\nencebetweenthetwoscales,observethatapieceofmusic\nin 4/4 meter will have different onset coefﬁcients from a\npiece with exactlythe same notesin 3/4 meter onlywhen\nthemeasure-relativescaleis used.\nAlthough translations have clear meanings in each of\nthe dimensionsin isolation, the effect in the onset dimen-\nsionofatranslationuponthedurationdimensionisslightl y\nmore involved. Let us assume that a translation of one\npositive unit in the duration dimension corresponds to a\ndoubling of the duration of each note. Observe that note\nonsets are left unchanged by this transformation. Using\nthe measure-relativescale, this transformation can equiv -\nalently be seen as a change in meter. This is illustrated\nin Fig. 1: The duration of each note in fragment (b) is\ntwice that of the corresponding note in fragment (a), but\nthe relative position of each note is the same in both se-\nquences. We will exploit this feature when approaching\nthe issue of time-scale invariance. An artifact of this rep-\nresentation is that fragment (a) is indistinguishable from\nfragment(c),whichimpliesthatthoughwemightachieve\nthe meter-invariance property, this interpretation is mus i-\ncally ambiguous. We would lose meter invariance if we\nuseabsoluteonsets: Themeterinformationisignored,and\nso in order to achieve the transformation from (a) to (b)\nin Fig. 1 (ignoring the time signature and the bar lines),\nthetranslationinthedurationdimensionwouldhavetobe\ncombined with a scaling by a factor of two in the onset\ndimension. Our algorithms can be used with both scale\ntypes.\n1.3 Time-ScaleInvariance\nOne element of the richness of polyphonic music lies in\nmelodic imitation between voices. Imitations can be ex-\nactineveryrespect,butcommonlythemelodyvariesfrom\none iteration to the next by octave or by transposition.\nIn these cases, a query in which pitch and duration are\ndeﬁned should be adequate for retrieval. Since the need\nfor pitch-invarianceis widely accepted, we turned our at-\ntention to time-scale invariance. An imitative device in\nwidespreaduseinEuropeanartmusicofseveralcenturies\nFigure1.Threerelatedfragments. (a)Asimplenotesequence.\n(b) The note sequence translated in the duration dimension. (c)\nThe note sequence from (a) shown in a different meter with\nmeasure-relative onsets left unchanged.\nwas the mensuration canon, in which an imitation could\nbe contracted or elongated by doubling or halving note-\nvalues. Contrapuntal techniques involving the augmen-\ntation or diminution of note-durations often noted in the\nworkofJ. S.Bach,left asigniﬁcantmarkonmotivicpro-\nceduresinlaterorchestralmusic.\n1.4 Windowing\nWindowingreferstotheconceptofsplittingthemusicinto\n(overlapping)segmentswhoseeventsﬁt intoa certainon-\nset interval (referred to as window length ). This idea is\nsomewhat analogous to the concept of n-grams, which is\nused extensively in string matching. The n-grams tech-\nnique has been applied in monophonicmusic queryalgo-\nrithms, but does not easily generalize to polyphonic mu-\nsic,wherethetargetsequencemayoccurinanyvoiceand\nmay even move between voices [2]. Windowing is made\npossible by the fact that query patterns are local in time,\nthat is, theyusuallyspan onlya limitedrangein theonset\ndimension.\nWe use two forms of windowing. In section 2.1 we\nsuggest that the runningtime of our algorithmcan be im-\nproved if one of the subroutines queries a data structure\nconsistingofa small windowofpointsratherthanthe en-\ntiremodelpointset. Inthemaximalsubsetmatchingalgo-\nrithmpresentedin section3.1windowingis fundamental:\nThepreprocessingstepessentiallytakeswindowsasinput,\nand the algorithm makes assumptions about the relative\nlengthsofthesewindowsandthequerypatternlengths.\n2 MINIMUM HAUSDORFFDISTANCE\nPROBLEM\nWe begin by specifying the ﬁrst of the two problems we\nstudy in this paper, which is that of ﬁnding the minimal\ndirected Hausdorff distance between a pattern Mand a\nmodel Nover all possible translations of the pattern M.\nFormally,thedistancewewantto ﬁndis\nmin\nw∈R3/parenleftbigg\nmax\nm∈M/parenleftbigg\nmin\nn∈Nd(m+w, n)/parenrightbigg/parenrightbigg\n,(1)\nwhere wis any translation vector and dis the Euclidean\ndistance function. To understandwhat is meant by Haus-\ndorffdistance, considera particulartranslationof the pa t-Figure2.The concept of Hausdorff distance.\nternM. For each point in M, consider the distance to\nthe closest point in N. The directed Hausdorff distance\nis the maximum of these distances. This is illustrated in\nFig. 2(a): The crosses represent the point set Nand the\ndotsrepresentthepatternpointset M. TheHausdorffdis-\ntance from the dots to the crosses is the furthest distance\nfrom a dot to the cross that is nearest to it. Another way\nofseeingthisdistanceisasthesmallestradiusthatcircle s\naround each of the dots can have in order for each of the\ncircles to contain at least one cross. As stated earlier, we\nallow the pattern point set Mto undergo any translation\nw. Such a translation that decreases the Hausdorff dis-\ntance is shown in Fig. 2(b): The dots have been shifted\nhorizontally to the right, and this has decreased the dis-\ntance from m1to the nearest cross from datodb. The\ntask is to determinethe translationvectorwhichgivesthe\nsmallest Hausdorffdistance. The example presentedhere\nis for a two-dimensional representation, but the problem\nclearlygeneralizestoanypointdimension.\n2.1 Our Algorithm\nThere are two main steps in our algorithm. The ﬁrst step\nis toalignthe pattern set Mand the model set Nwith\nrespect to some pair of points (a, b), where a∈Mand\nb∈N. Concretely,thismeansthat weapplya translation\nwtoeverypointinthepatternset Msuchthat a+w=b.\nThe next step is to ﬁnd the nearest neighbor of each of\nthe pattern points in the model point set under this trans-\nlation. The Hausdorff distance is then computed based\non this. We repeat this procedure for each of the points\nin the model set N. Alg. 1 gives the details of this pro-\ncedure. We will provide details about the complexity of\nthe algorithm, and also show that it is indeed a (2 +ǫ)-\napproximationtothedirectedHausdorffdistanceproblem\noutlinedintheprevioussection.\nSubsequent to the alignment, the nearest neighbor is\nfoundinthemodelset Nforeachofthepointsinthepat-\ntern set M. We use the approach proposed by Arya et al\nin[1],whichduringpreprocessingofthedatadecomposes\nthe set Ninto cells in a so-called BBD-tree structure. At\nquery time, the cell to which the query point belongs is\nfound,andaprioritysearchroutineprocessesnearbycells\nuntil the nearest neighbor is found. Note that this algo-\nrithmactuallyﬁndsan approximate nearestneighbor,that\nis, it returns any point that is at most (1 +ǫ)times as\nfar away from the query point as its true nearest neigh-\nbor, where ǫ >0is a constant of our choosing. The\ntime required for each such nearest neighbor computa-Algorithm1 MinimumHausdorffdistancealgorithm\n1. Pickanypoint pinM\n2. Foreverypoint niinN\n(a) Translate Mby(ni−p)\n(b) Foreverypointin mjinM\n(c) Findthe nearestneighbor njofmjinN\ni. Computethedistance dij=d(mj, nj)\n(d) Set distance i,di= max j{dij}\n3. Outputthe minimalalignmentdistance, mini{di}\ntion is O/parenleftBig\nd/ceilingleftbig\n1 + 6d\nǫ/ceilingrightbigdlogn/parenrightBig\n, where dis the point di-\nmension (in our case d= 3). The preprocessing time for\nthe data structure is O(dnlogn), and the space require-\nmentis O(dn).\n2.2 Analysis\nFor each alignment, there are (m−1)nearest neighbor\nqueriestoprocess,andthereisatotalof ncandidatealign-\nments. Thetotal time complexityof ouralgorithmis thus\nO(mnlogn). Theonlypreprocessingrequirediscreating\nthe BBD-tree forthe modelpoints,and thespace require-\nment is linear in the size of the model point set. The idea\nof exploiting the spread in the onset dimension through\nwindowing (see section 1.4) can be used to reduce the\ntime complexityof queries. To do this, we requirean up-\nper bound on the potential difference in onset between a\nquerypointanditsnearestneighborinthemodelset. This\nwill determine a window length and hence a maximum\nsize that the set of model points that fall within the limits\nof such a window can have. If we build the BBD-tree for\neach window,the querytime is reducedto O(nmlogw),\nwhere wis largest possible window point set size. Note\nthat although wmay be large, it is inherently linked to\nthe representation and does not grow with either mnor\nn. It can thus be regarded as a constant, and the asymp-\ntoticrunningtimeofouralgorithmisthus O(mn). Time-\nscale invariance is achieved in a brute-force way by re-\npeating the query process for different meters applied to\nthe query sequence (see section 1.2). We do not discuss\nthe details about the set of applicable meters here, other\nthan to note that the set of applicable meters is ﬁnite and\nrelativelysmall. Wealsoomittheproofthatthealignment\nmethodguaranteesthatthedistanceoutputisatmosttwice\nthe trueHausdorffdistance. From thepromisesof the ap-\nproximatenearestneighborsubroutineitthusfollowsthat\nouralgorithmproducesadistancewhichisatmost (2 +ǫ)\ntimesthetruedistance,where ǫ >0isarbitrarilysmall.\n2.3 Example\nIn order to illustrate the algorithmoutlinedabove,we ap-\nply our algorithm to the melody “Fuggi, fuggi, da questo\ncielo”, which belongs a melodic family studied by Tagli-\navini [8]. The result of matching this theme against three\nother membersof this melodic family is shown in Fig. 3.Theexampleismeanttoillustratemeteraspectsoftheal-\ngorithm as well as to show the output when perceptually\nsimilar pieces are compared. Note that our algorithm is\ndesignedforpolyphonicmusic,andinparticularinstances\nwherethemodelsareentireworksofmusicthatareorders\nofmagnitudelargerthanthequery.\nThe query consists of the ﬁrst 19 events of “Fuggi,\nfuggi, da questo cielo”; the last event has been left out so\nthatthequeryisnolongerthananyofthemodelmelodies.\nNotethatthequeryisin4/4meterwhereasthethreemod-\nels are in 6/8, 6/8, and 2/4 meter respectively. The query\nmelody matches each of the models when the onsets of\nthe query events are scaled according to 2/4 meter (see\nsection 1.2). Applying this meter has the effect of dou-\nblingthenumberofmeasures. Thenumbersaboveeachof\nthe models show the matches (i.e. geometrically nearest\nneighbors) of each query event in the optimal alignment\nof the query. Note that more than one query note can be\nmatchedtoamodelnote. Thetranslationvectorthatgives\nthe optimal alignment is also given, where the three co-\nefﬁcients are onset, pitch, and duration,respectively. Th e\nﬁrst two matches have a shift in the duration dimension\nof1, meaning that every query note has been doubled in\nduration. This shift happensbecause the algorithmaligns\nthe ﬁrst note of the query (an eighth note) with notes in\nthe models, and in these two cases the best matches hap-\npen to be alignments with quarter notes. For model (b)\nit turns out that the distance would be smaller if we did\nnot translate the durations. As the algorithm is a (2 +ǫ)-\napproximation, however, we are guaranteed that the dis-\ntance output is at most about twice the optimal. Hence,\nmatch (a) is in any case better than match (b), since the\ndistanceofmodel(b)mustbeat least ∼1.4. Ineachcase\nthenotematchesthatarefurthestapart,i.e. thosethatdic -\ntate theHausdorffdistance,aregiven.\n3 MAXIMAL SUBSET MATCHING\nThis problem is a variation of problem P2 proposed by\nUkkonen et al. [10]. In their representation, events are\nline segmentsin two-dimensionalpitch-timespace where\nevent durations are proportional to line segment lengths.\nTheproblemistoﬁndalltranslationsofapattern Msuch\nthat a subset of the onset eventsof Mmatch onset events\ninN. They give an algorithm that solves this problem\ninO(nmlogm)time and O(m)space basedon the idea\nof sweeplining N. The problem we consider here is the\nsame, with three important modiﬁcations: First, we will\nallow time-scalings (i.e. scaling in the onset dimension)\nofMin addition to translations. Second, we require that\nthedurationsofeacheventmatch(inadditiontopitchand\nonset). Third, we are only interested in subsets of size\ntwo orgreater. Thereasonforthelast modiﬁcationis that\nin our representation, any event can be transformed into\nany other by a translation. Thus, for every query there\narenmtrivial subset matchesof size one that result from\nsimply translating each event in Mto each event in N.\nA solution to the maximal subset matching problem is a\nFigure4.Twostepsofthemaximalsubsetmatchingalgorithm.\nTranslation(gray)withrespecttopoint uandsubsequentscaling\n(black) withrespect topoint v.\nlist of ordered triplets of the form /a\\}bracketle{tv, s, M′/a\\}bracketri}ht, where vis\na translation vector, sis a scaling factor, M′∈Mis the\nsubset of points that match points in Nunder translation\nvandscaling s,and|M| ≥2.\n3.1 OurAlgorithm\nOur algorithm employsthe idea of geometric hashing, an\nexcellent overview of which is given in [11]. The main\nidea istopreprocessthe modelssothat therepresentation\nis invariant to translation and scaling. In order to keep\nthe preprocessingtime anddata structuresize reasonable,\nwe window the models as described in section 1.4. For\neach model point u, we consider each other point vthat\nlies within a time window Wcentered at u. We refer to\neach such pair of points /a\\}bracketle{tu, v/a\\}bracketri}htas abasis. For each basis\nwe translate the window point set Wso that uis at the\norigin. ThisisillustratedbythegrayarrowsinFig. 4. We\nthen scale the onsets of all points in the set by a positive\nfactor such that the difference in the onset of uandvis\n1. In other words, we scale all onsets by the inverse of\nthe absolute value of the onset of v. This is shown by the\nblack arrows in Fig. 4. The pitch and duration are left\nunchangedbythisscaling. We nowcomputea hashvalue\nof the points in the point set using a hash function that\nassignsauniquevaluetoeachpossiblepointlocation. The\nhash values are then used as indices in a hash table, the\nentriesofwhicharereferredtoas bins. Eachbinconsists\nof a list of labels, which contain information about the\nbasis for instances of the aforementioned process where\na point in the window set ended up in this speciﬁc bin\nlocation. Forexample,to eachof the lists associated with\nthe bins for (the translated and scaled) points u,v, andw\nin Fig. 4, we would add the label identifying the basis\n/a\\}bracketle{tu, v/a\\}bracketri}ht. We repeat this processfor all pairs of pointsin all\nwindows.\nProcessing a query follows a similar procedure. For\na pair of query points, we translate and scale as in the\npreprocessing step. We then consider the lists of labels\nof the bins into which the query points fall. Each label\nthat occurs in one of these lists identiﬁes a subset match,\nsince for this particular translation and scaling the query\nandthemodelhaveatleastoneeventincommon. Allthat\nremains is to aggregatethe lists, i.e. for each label create\na set of all the points whose bins contain that label. Note\nthat every possible label will be present in the bin at theFigure3.Example of the minimum Hausdorff distance algorithm. Top: \" Fuggi, fuggi da questo cielo\", a canzonette from c. 1625 as\nnotatedbyGherardoPedali. (a)\"Moldau\"themeusedbySmeta nainhissymphonicsuiteMaVlast. (b)AtranscriptionbyCou ssemaker\n(a noted musicologist) from c. 1850 of a popular Flemish song , \"Ik zag Cecilia komen\". (c) A transcription by Tagliavini f rom a\ntwentieth-century arrangement byMelchiade Benni called t he \"BallodiBaraben\" orthe \"Ballo diMantova\".\nAlgorithm2 Maximumsubsetmatchingalgorithm\nPreprocessing:\n1. Foreachwindow Wucenteredat point u∈N\n(a) Foreachpair v∈N∩Wu, where u/\\e}atio\\slash=v\ni. Translate Wby−u\nii. Scale Wby1\n|onset(v)|\niii. Foreach w∈W\\ {u}\nA. Addlabel /a\\}bracketle{tu, v/a\\}bracketri}httothe binat location w\nQuery:\n1. Foreach /a\\}bracketle{ta, b/a\\}bracketri}ht, where a, b∈Manda/\\e}atio\\slash=b\n(a) Translate Mby−a\n(b) Scale Mby1\n|onset(b)|\n(c) Foreach c∈M\\ {a}\ni. Foreachlabel /a\\}bracketle{tu, v/a\\}bracketri}htinthebinat location c\nA. Add ctothematchset for /a\\}bracketle{tu, v/a\\}bracketri}ht\n(d) Outputlabelswhosematchsets areofsize ≥2\norigin,correspondingtosubsetmatchesofcardinality 1as\ndescribedearlier. To ﬁnd all maximalsubset matches,we\nrepeatthisprocessforall pairsofquerypoints.\n3.2 Analysis\nWe now turn to the time and space complexity of the al-\ngorithm. Denoteby lthe maximumpattern lengthandby\nǫthe smallest unit in the duration dimension. By using\na window of length 2lcentered at the ﬁrst basis point we\nare guaranteed to ﬁnd all subset matches. Furthermore,\nletwbethemaximumnumberofeventsthatoccurin any\nwindow. An event forms a basis with at most (w−1)\nother events. For each basis, at most wnew entries are\ncreated in the hash table, and so an upper bound on the\ntime and space complexity of the preprocessing step is\nO/parenleftbig\nw2n/parenrightbig\n. Although the complexity seems high for big\nwindowlengths,forapplicationswhereonecan deﬁneanupper bound on pattern lengths the time and space com-\nplexity of the algorithm grows linearly with the database\nsize.\nLet us now consider the query time complexity. Each\nelement in the model set Nis the ﬁrst element of a ba-\nsis at most wtimes. For each basis, at most one entry is\nmade into the data structure per bin. Hence, each bin’s\nlist will have length at most wn. For a query of size m\nthere will be msuch lists to consider per basis. By us-\ning a hash table, the histogram step (step 1(c) of alg. 2)\ncan be completed in O(wnm)time. The total running\ntime of the query algorithm is thus O/parenleftbig\nwnm3/parenrightbig\n, since the\ncomputationisrepeatedfor O/parenleftbig\nm2/parenrightbig\nbases. Astherunning\ntimeofouralgorithmisinfactproportionaltothenumber\nof matches, we can restate it as O/parenleftbig\nm3(k+ 1)/parenrightbig\n, where\nkis the number of maximal subset matches. This entails\nthat the running time does not depend on the size of the\ndatabase. Ouralgorithmisthusasymptoticallyoptimalup\nto am2factor, and this at least partly justify the higher\nspace requirements of this relative to previous methods.\nFinally, we also observe that we were not interested in\ntime-scale invariance, the basis in the preprocessing step\nand queries would consist of one point. Thus we would\ngetO(wn)space and O(m(k+ 1))query time, as we\nwould only need to consider a single basis in the query.\nThisquerytimeisasymptoticallyoptimalassumingmatch\neventenumerationisrequired.\n3.3 Example\nAsanexampleofthemaximalsubsetmatchingalgorithm\ngiven in this section, we consideran excerptfrom the ap-\npendix to the Goldberg Variations by J. S. Bach (BWV\n1087, No. 14) shown in Fig. 5. The second phrase in\nthe soprano(a seriesof elevensixteenthnotes) islater re-\npeatedinthetenorasquarternotes. Takingeitherofthese\ntwo note sequences as the query, the algorithm returns aFigure5.Excerpt from the appendix tothe Goldberg Variationsby J.S. Bach(BWV1087, No. 14).\ncompletematchforeachbasis(seeprevioussection),cor-\nresponding to the scaled occurrence. Similarly, the ﬁrst\nseven eighth-notes in the alto are repeated in the bass as\nhalf notes, with one difference: The 6th note in the se-\nquence is a C# in the eighth-note occurrence and a C in\nthehalf-noteoccurrence. Applyingthealgorithmtoeither\nof these sequences thus gives a match of size six corre-\nsponding to the scaled occurrence. This will be the case\nfor any basis that does not have the 6th note as the ﬁrst\nnote. The relative onsets of the 6th note are the same in\nthe sequences, and so the bases with the 6th note as the\nsecondnotewill alsohavea matchofsize six.\n4 DISCUSSION ANDCONCLUSIONS\nWehaveimplementedourtwoalgorithmsontheJavaplat-\nform. The input format for both the database and queries\nis the Kern data format, and for testing our algorithms\nwe have made extensive use of data from the KernScores\nwebsite ( http://kern.humdrum.net ) [7], includ-\ning music by J. S. Bach, Corelli, and Beethoven, as well\nasragtimepiecescomposedbyScottJoplin. Thesoftware\nproduces on-the-ﬂy output using the GUIDO NoteServer\n(http://www.noteserver.org ).\nWe continue to explore the contexts in which each of\nthealgorithmsareuseful. Provisionally,weexpectthatth e\nHausdorff distance algorithm should be effective in situ-\nations where there is noise present in the encoded music.\nFuture research will focus on ways to improve the space\nrequirements of the time-scale invariant maximal subset\nmatchingalgorithm. Furthermore,athree-dimensionalrep -\nresentationofmusicisinsufﬁcienttocaptureallperceive d\nfeatures of music. More work is required to determine if\nthe point-set model can be extended to encompass more\nfeatures, such as harmony and beat information, for ex-\nample.\n5 ACKNOWLEDGEMENTS\nWe are thankful to Don Anthony for typesetting musical\nexamples for this paper, to Walter B. Hewlett for use-\nful discussions, and to Jürgen Kilian for maintaining the\nGUIDONoteServerwebsite.\n6 REFERENCES\n[1] S.Arya,D.M.Mount,N.S.Netanyahu,R.Silverman,\nand A. Y. Wu. “An Optimal Algorithm for Approxi-mate Nearest Neighborhood Searching,” Proc. Symp.\nDiscrete Algorithms ,pp.573-582,1994.\n[2] R. Clifford, M. Christodoulakis, T. Crawford, D.\nMeredith, and G. A. Wiggins. “A Fast, Randomised,\nMaximumSubsetMatchingAlgorithmforDocument-\nLevel Music Retrieval,” Proceedings of the 7th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR2006),pp.153-155,2006.\n[3] P.DiLorenzoandG.DiMaio.\"TheHausdorffMetric\nin the Melody Space: A New Approach to Melodic\nSimilarity,\" Ninth International Conference on Music\nPerceptionandCognition ,Bologna,2006.\n[4] W. B. Hewlett. “A Base-40 Number-LineRepresenta-\ntionofMusicalPitchNotation,\" Musikometrika4 ,pp.\n1-14,1992.\n[5] A. Lubiw and L. Tanur. “Pattern Matching in Poly-\nphonic Music as a Weighted Geometric Transla-\ntion Problem,” Proceedings of the 5th International\nConference on Music Information Retrieval (ISMIR\n2004),pp.289-296,2004.\n[6] D. Meredith. “Point-Set Algorithms for Pattern Dis-\ncovery and Pattern Matching in Music,” In T. Craw-\nford and R. C. Veltkamp (Eds.), Proceedings of\nthe Dagstuhl Seminar on Content-Based Retrieval ,\nDagstuhl,Germany,2006.\n[7] C. Sapp. “Online Database of Scores in the Hum-\ndrum File Format,” Proceedings of the 6th Interna-\ntionalConferenceonMusicInformationRetrieval (IS-\nMIR2005),pp.664-665,2005.\n[8] L. F. Tagliavini. “’Il ballo di Mantova’, ovvero,\n’Fuggi, fuggi, da questo cielo’, ovvero, ’Ce-\ncilia’, ovvero,” in Max Lutolf zum 60. Geburtstag:\nFestschrift(Basel: Wiese),pp.135-175,1994.\n[9] R.Typke.“MusicRetrievalbasedonMelodicSimilar-\nity,”Doctoralthesis.UtrechtUniversity,2007.\n[10] E.Ukkonen,K.Lemstrom,andV.Makinen.“Geomet-\nric Algorithms for Transposition Invariant Content-\nBased Music Retrieval,” Proceedingsof the 4th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR2003),pp.193-199,2003.\n[11] H. J. Wolfson and I. Rigoutsos. “Geometric Hashing:\nAn Overview,” IEEE Computational Science and En-\ngineering ,4(4),pp.10-21,1997."
    },
    {
        "title": "Improving the Classification of Percussive Sounds with Analytical Features: A Case Study.",
        "author": [
            "Pierre Roy",
            "François Pachet",
            "Sergio Krakowski"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417875",
        "url": "https://doi.org/10.5281/zenodo.1417875",
        "ee": "https://zenodo.org/records/1417875/files/RoyPK07.pdf",
        "abstract": "There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we conducted experiments with so-called analytical features, an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. These analytical features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attack portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with standard approaches.",
        "zenodo_id": 1417875,
        "dblp_key": "conf/ismir/RoyPK07",
        "keywords": [
            "automatically classifying sounds",
            "supervised classification",
            "analytical features",
            "general bag-of-frame scheme",
            "improvement in performance",
            "evaluation and selection",
            "arbitrary audio classification problems",
            "Pandeiro sounds",
            "classification of entire sounds",
            "classification of attack portions"
        ],
        "content": "IMPROVING THE CLASSIFICATION OF PERCUSSIVE \nSOUNDS WITH ANALYTICAL FEATURES: A CASE STUDY\nPierre Roy Fran çois Pachet Serg io Krakowski1\nSony CSL\n6, rue Amy ot\n75 005 Paris\nroy@csl.sony.frSony CSL Paris\n6, rue Amyot\n75 005 Paris\npachet@csl.so ny.frSony CSL Paris\n6, rue Amy ot\n75 005 Paris\nskrako@gmail .com\nABSTR ACT\nThere  is an increasing  need for automatic ally classify ing \nsounds  for MIR and interac tive music applica tions. In the \ncontext of supervised classific ation, we conducted experi-\nments with so-called  analy tical features, an approach that \nimproves  the performan ce of the general bag-of-fram e \nschem e without loosing  its generality. These  analytical \nfeatures are better,  in a sense  we define precisely  than \nstanda rd, general features, or even than ad hoc features \ndesigned by hand  for specific  problems. Our method  al-\nlows us to build a large number of thes e features, evaluate \nand select them  automatic ally for arbitr ary audio classi-\nficatio n problems.\nWe present  here a specific  study concerning  the analy-\nsis of Pande iro (Brazilian  tambourin e) sounds.  Two  prob-\nlems are considered:  the classificati on of entire sounds , \nfor MIR applicati ons, and the classifica tion of attack por-\ntions of the sound  only, for interac tive music applica-\ntions. We evaluate precisel y the gain obtained  by analy ti-\ncal features on these  two problems , in comparis on with \nstanda rd approaches.\n1.Acoustic  Features  \nMost  audio classificati on approaches use either one of \nthese  two paradig ms: a general scheme, called  bag-of-\nframes, or ad hoc approaches.\nThe bag-of-frame  appro ach [65535], [65535] consists \nin considerin g the sig nal in a blind  way, using a systemat-\nic and general scheme : the signal is sliced  into consecu-\ntive, possibly  overlapping  frames (typically  of 50ms), \nfrom  which a vector of audio features is computed.  The \nfeatures are supposed  to represent  characteristic informa-\ntion of the signal for the problem at hand.   These  vecto rs \nare then aggregated (hence the “bag”) and fed to the rest \nof the chain.  First, a subset of available  features is identi-\nfied, using some  feature selectio n algorithm.  Then  the \nfeature set is used to train a classifier, from  a databa se of \nlabeled signals (trainin g set). The classifier thus obtained \nis then  usually tested  against another databa se (test  set) to \nassess  its performan ce.\nMPEG7-audio ([65535]) as well as [65535], [65535] \nare standa rd sources  for audio features. These  features \ncontain statis tical informatio n from  the temporal  domain \n(e.g. Zero-crossing rate), spect ral domain  (e.g. Spect ral-\nCentroid), or more  perceptive  aspec ts (such as sharpness, \nrelative  loudness, etc.) and are mostly  of low dimension-\nality.The bag-of-fram e approach has been used extensiv ely \nin the MIR domain,  for instance  by [65535]. A large pro-\nportion of MIR related  papers has been devoted  to study-\ning the details  of this chain  of process: feature identifica-\ntion [65535]; feature aggregation [65535]; feature selec-\ntion [65535], [65535], [65535]; classifier  compa rison or \ntuning [65535], [65535]. This approach performs  well on \nsome  problems,  e.g. speech music discriminati on. How-\never, it shows  limitations when appli ed to “difficult” \nproblems  such as genre classific ation, which works  well \non abstra ct, large categories  (Jazz  vs. Rock),  but works \npoorly for more  precise  class problems  (e.g. Be-bop vs. \nHard-bop).  \nIn these cases, the natural tendency  is usually to look \nfor ad hoc appro aches, which aim at extracting  “manu al-\nly” from  the signal the characteristi cs most  appropriate \nfor the problem  at hand,  and exploit  them  accordingly. \nThis can be done  either by defining ad hoc features, inte-\ngrated in the bag-of-fram e approa ch (e.g. the 4-Hertz \nmodulation  energy used in some  speech/mus ic classifiers , \n[65535]), or by defining complete ly dif ferent schem es for \nclassify ing, e.g. the analy sis-by-synthesis approach de-\nsigned for drum sound classificati on [65535], and further \ndeveloped  by [65535] and [65535].\nThe bag-of-frame  approach relies on generic features \nthat do not alway s capture the relevant  perceptive  charac-\nteristics  of the signals to be classified.  Some  classifier, \nlike kernel method s [65535] including Support Vecto r \nMachines  ([65535], [65535]) transfo rm the feature space \nto increase  inter-clas s separability . Howev er, the increas-\ning sophis tication of feature selection algorithms  or clas-\nsifiers canno t compensate  any initial loss of informati on.\nTo find better features than the generic ones,  one can \nfind inspiration  in the way human experts actual ly invent \nad hoc  features. The papers  quoted above  use a number of \ntricks and techniqu es to this aim, combined  with intu-\nitions  and musical knowled ge. For instance,  one can use \nsome  front- end system to normalize  a signal, or pass it \nthroug h some  filter, add pre-processing  to isolate  the \nmost  salient characteristics of the signal.\nWe have i ntroduced  in [65535] the EDS system, which \nautomates  feature invention.  It used an evolutionary  al-\ngorithm which  explores  quickl y a very large spa ce (about \n1020) of ad hoc features. The features are built by com-\nposing  - in the sense of functional composition - elemen-\ntary operat ors. We call these  features analy tical becaus e \nthey are described  by an explicit  composition of func-\ntions,  as opposed to other  forms  of signal reduction,  such \nas arbitra ry computer  programs.  In the rest of this article \nwhen we refer to analy tical features, we me an features in-\nvented by the EDS system.\n1 The work of Sergio Krakowski is partially supported by a CAPES scholarship.© 2007 Aust rian Computer Society (OCG).2.PAN DEIRO SOUND CLASSIFICATION\nThe Pand eiro is a Brazi lian frame drum (a type of tam-\nbourin e) used in particular in Brazilian  popular music \n(samba,  côco, capoeira,  chôro). As it is the case for many \npopu lar music instr uments, there is no official method for \nplaying the Pand eiro. Howev er, the third  author,  a profes-\nsional  Pand eiro player, has developed  such a method,  as \nwell as a notation of the Pande iro, that we use in this pa-\nper. This method  is based  on a classifica tion of Pand eiro \nsounds  in exactly  six categori es (see Figure 1): tung : \nBass  sound, also known  as open  sound;  ting: Higher \npitched  bass sound, also open;  PA (or big pa): A slap \nsound , close to the Conga slap; pa (or small  pa): A \nmedium sound  produced  by hitting  the Pand eiro head  in \nthe center;  tchi : The  jingle sound; tr: A tremolo of jingle \nsounds.\nThe need for automatic ally analyzing Pandeiro  sounds \nis twofold.  First, MIR applicati ons, for education  notably, \nrequire the ability to automatic ally transc ribe Pande iro \nsolos.\n \n \n  tung                  ting                tchi\n \n \n    tr                               PA                            pa\nFigure  1. The gestures to produce the six basic Pan-\ndeiro sounds.\nThe second need is more  original, and consists in de-\nveloping  real time interac tion systems  that expand  the \npossibiliti es of the percussionist,  to allow him to increase \nits musical “powers”.  In this case, we need to analyze ro-\nbustly and quickly  Pande iro sounds , to trigger various \nevents (see, e.g.[65535]).\nWe there fore define two different analy sis problems, \ncorrespo nding  to these two applica tions.\nThe first problem  consists in classifying complete \nsounds  (150ms duration)  in the  six basi c classes.  The sec-\nond problem,  more  difficult but more  useful for real time \napplica tions, consists in classifying sounds  using as less \npossible  informati on, typically  only the attack (about \n3ms, that is 128 samples  at 44 kHz),  to allow a subse-\nquent triggering of a musical event. To this aim we must \nbuild a reliable and very fast classifier. \n2.1.Availa ble sound  databases\nWe have  recorded 2448 complete  Pande iro sounds  (408 \nof each 6 types) that constitute the full sound database. \nThey  were produced  with the same instru ment and \nrecorded  on a Shure Beta 98 microphone linked  to a \nMOTU Traveller sound card. \nIn order  to classify the sounds,  it is impor tant to finely \nlocate them  in time. To this aim, we designed a robust at-\ntack identifier, which  work s as follows:  the incomin g sig-\nnal is divid ed in non-overlapping  frames of 1.4ms (64 \nsamples  at 44kHz). A loudness  value is computed for \neach frame, generating the “loudness  curve”.  An attack is \nrepor ted when a peak in this curve is found. The identifier is previo usly calibrated,  in order  to distin guish betw een \nnoise peaks and real attacks.\nFor each attack, we record an audio file containin g the \nattack fram e itself and the followin g fram e. This  file pop-\nulates the attac k databa se.\n2.2.Experiments: train ing and testing bases\nWe compa re analytical features to a “reference feature \nset” [65535], containing  standard  acoustic  features from \ne.g. Mpeg7-audio. We systematically  evaluate the perfor-\nmance  of two classifiers:  one built with the reference set, \nthe other  built with EDS analytical features.\nEach  experi ment is in turn divid ed in two parts.  First, \nclassifiers  are trained  on trainin g samples  and tested  on \nthe test sampl es. To  this ai m, databases  are systematically \ndivided in two parts,  2/3 for the training , and 1/3 for the \ntest. The samples  are chosen randomly , to avoid  artifa cts \n(e.g. evolut ion of the membran e during the recording ses-\nsion,  small  variati ons in the player gestures). In the sec-\nond part,  classifiers  are trained  and tested  only on the test \ndatabase, using 10-fold cross-validation.\nThis procedur e aims at showin g that the advanta ge ob-\ntained by analytical features is consistent,  and do not de-\npend on the conditions of experi ments. The cross-valida-\ntion using only the test database  is motivated  by the fact \nthe EDS alread y uses the training  databa se for evaluating \nthe analytical features. So reusing it for trainin g the clas-\nsifiers could produce biases  (althou gh we are not sure \nwhy and how).\nFinally, for the attack problem , we build an experiment \nin which the signal itself is used as a feature (this is pos-\nsible because these  signals are very short). The aim is to \nconfirm that the signal is not a good feature.\n2.3.Choosing  the clas sifiers\nThere  is a vast literatur e on supervised learning  al-\ngorithms  [65535] with no clear  winn er. We conducted  ex-\nperiments with various  classifiers , to avoid  biases  (e.g. \nSVM, kNN, J48). For the sake of clarity , we report only \nthe results with Support  Vecto r Machin es [65535], which \nturned out to be the b est and most stable. (We use Weka’s \nSMO with a polynomial  kernel.)\nWe used EDS in a fully automated  way for the creation \nand selection of analytical features. For each problem , we \nran the genetic search until no improvem ents were found \nin feature fitness. For the complete  sound problem , EDS \nevaluated about 40,000 features. For the attack problem \nEDS evaluated about 200,000 features.\n2.4.Feature  Selection\nTo compare the two  appro aches (general versus analy tical \nfeatures) in a fair mann er, it is important to train classi-\nfiers on spaces  with identical  dimension.  For the full \nsounds, all reference features could be computed,  yielding \na feature set of dimension  100. We have therefore  select-\ned 100 scalar  analytical features among  the 23,200 com-\nputed by EDS.\nIn the case of attacks, not all reference features were \ncomputable , because the y are too smal l: only 17 reference \nfeatures could be computed and evaluated, with a total di-\nmension  of the feature set of 90. W e there fore selected  90 \nanalytical features among  the 53,500 EDS created  for at-\ntacks.We used two feature selection methods . Firstly, Infor-\nmation  Gain  Ratio (IGR) [65535], which corresponds to \nWeka’s  AttributeSelection algorithm  with the following \nparameters:  the evaluator is a InfoGainAttributeEval and \nthe search is a Ranker, which allows  us to determin e a \npriori the dimension  of the feature set. Secondly , we de-\nveloped  an algorithm  suited to multi-class  problems.  The \nidea is to select a feature set that “covers”  optimally  the \nclasses to learn from the viewpoint  of indiv idual features, \nthat is, essentiall y of their F-measure. The algorithm  iter-\nates over all classes and selects features with the best F-\nmeasure for a given class.Finally, we present r esults obt ained for  various siz es of \nfeature sets from  1 to 100. This is an import ant aspec t in \nthe context of real-tim e systems , wher e we want  to mini-\nmize the number of features to compute in real time. As \nwe will see, EDS finds not only better  features but also \nfeature sets of lesser dimension.\n2.5.Results  and comments\nFigure 2 and Figure 3 show the results obtained:\nFeatur e Set Dimension\nExperiment Des cript ion1009075502515105321\nReferenceIGRTrain/Test99,999,999,699,59999,599,192,888,565,256\nReferenceIGR10-fold XV99,999,599,599,599,198,698,4928260,559,3\nEDSIGRTrain/Test99,999,998,598,398,998,399,19868,936,136,9\nEDSIGR10-fold XV99,999,999,998,89898,498,297,864,73621,2\nReferenceEDS FSTrain/Test99,999,999,999,899,199,198,998,893,680,867,2\nReferenceEDS FS10-fold XV99,999,699,699,498,698,498,898,393,478,161,6\nEDSEDS FSTrain/Test99,999,998,999,999,999,699,59989,988,873,8\nEDSEDS FS10-fold XV99,999,998,999,799,699,599,49991,389,573,6\nFigure 2. Results on full sounds. IGR stands for Info. Gain Ratio. EDS FS denotes our F-measure-\nbased FS algorithm. Train/Te st and 10-fold XV denote the experiments described in Section 2.2\nFeatur e Set Dimension\nExperiment De scription9075502515105321\nReferenceIGRTrain/Test91,891,389,676,678,367,564,356,151,149\nReferenceIGR10-fold XV92,691,288,879,973,267,464,744,242,434,5\nEDSIGRTrain/Test95,193,392,377,772,56361,354,754,556,9\nEDSIGR10-fold XV94,993,892,480,878,962,46155,155,954,9\nReferenceEDS FSTrain/Test91,991,59187,786,783,483,671,755,643,9\nReferenceEDS FS10-fold XV91,991,590,286,185,278,98268,548,639\nEDSEDS FSTrain/Test94,994,49492,191,487,990,188,680,472,1\nEDSEDS FS10-fold XV94,59493,391,491,48989,58880,169,2\nSignal 77.776.973.364.164.26059.258.157.544\nFigure  3. Results obtained with on attacks. See above for abbreviations. The “Signal” line gives the \nperformance of classifiers using the input signal directly as a feature.\nFor the two problems , analytical features found by \nEDS improve  the classificati on performance . The full \nsound problem is relativel y easy. The use of the full refer-\nence  feature set (dime nsion  100) yields a precisi on of \nabout  99,9%.  With  the same dimension,  analytical fea-\ntures yields the same  precisi on. The gain becomes  inter-\nesting if we consider  feature sets of smaller dimension:  2 \nanaly tical features yield a precision  of 89,5% versus  78% \nfor general features.\nAttac k problems  are more  difficult and interestin g. \nAnal ytical features are better  than general ones,  in partic-\nular for small  feature sets. For attacks, 3 analytical fea-\ntures perform  better than the 15 best general features. \nNote that the gain depends  on feature selection. IGR \ndoes  not select the best EDS features for small feature \nsets (this is a known  result [65535]). However , our fea-\nture selection algorithm  yields better results for all sizes \nof the feature set, see Figure 4. This show s again the diffi-\nculty in interpretin g directly  the precisi on of classifiers.  \nThe performan ce gain brought by analytical features \nfor small  feature sets has a lot of advantag es, in  parti cular \nfor real-time applicati ons. The 3 features that yield a pre-cision  greater than that obtained  with 15 reference  fea-\ntures are the followin g:\nAbs (Log (Percentile (Square (BpFi lter (x, 764,  3087)), 64)))\nCentroid (MelBands (Derivation (HpFilter (Power (Normalize (x), 3), 100)), 6))\nAbs (Sum (Arcsin (Mfcc (Hann (HpFilter (x, 19845)), 20 ))))\n50556065707580859095100\n9075502515105321RefEDS\nFigure 4. Analytical vs. reference features on attacks\nThis particular result allows  us to consider  real-time \nimplementation s: on a 3GHz  Pentium IV PC, the compu-\ntation  of the 3 features for a 2,8 ms signal takes  about 3 ms, to be compared  to the computation of 15 generic \nfeatures, which takes 9 ms, that is 3 times longer.\n3.Conclusion\nWe have  applied the EDS method  for creatin g audio fea-\ntures, called  analy tical, to the classificati on of Pandeiro \nsounds.  In both cases  studied (full sounds , or only a por-\ntion of the attack) analytical features do improve the per-\nforman ce of classificati on, as compared to results ob-\ntained  with generic, Mpeg-7 like features, in a bag-of-\nframe approach. The gain is notable  both in terms  of clas-\nsification precision  and feature set size.  Moreover,  the \nuse of analy tical features to improve  classifica tion algo-\nrithms  may be combined  with other  optimizatio n process-\nes, such as boosting, bagging or ad hoc approaches.\n5.     REFEREN CES\n[65535] Aucouturier, J.-J. and Pachet, F. Tools and \nArchitecture for the Evaluation of Similarity Meas-\nures : Case Study of Timbre Similarity. ISMIR 2004.\n[65535] Aucouturier, J.-J, Defreville, B. and Pachet, F. \nThe bag-of-frame approach to audio pattern recogni-\ntion: A sufficient model for urban soundscapes but \nnot for polyphonic music. JASA, 2007.\n[65535] Blum , A. and Langley, P. Selection of Relevant \nFeatures and Examples in Machine Learning, Artifi-\ncial Intelligence, 1997 pp.24 5-271, Dec. 1997.\n[65535] Boser, B. Guyon, I. and Vapnik V. A training al-\ngorithm for optimal margin classifiers. In D. \nHaussler, editor, 5th Annu al ACM Workshop on \nCOLT, pp.144-152, Pittsburgh, PA. ACM Press. \n1992.\n[65535] Fiebrink, R. and Fujinaga, I. Feature Selection \nPitfalls and Music Classification. ISMIR 2006, \npp. 340-341\n[65535] Kim, H.G., Moreau, N. and T. Sikora Mpeg7 Au-\ndio and Beyond: Audio Content Indexing and Retriev-\nal. Wiley & Sons. 2005.\n[65535] Krarkowski, S. Pandeiro+, music video available \nat: \nhttp://www.skrako.com/eng/pop_video.html?aguas\n[65535] McEnnis, D. McKay, C., Fujinaga, I. Depalle, P. \njAudio: a feature extraction library, Ismir 2005.\n[65535] McKinney, M.F. and Breebart, J. Features for au-\ndio and music classification. ISMIR 2003.\n[65535] Peeters, G. and Rodet, X. Automatically selecting \nsignal descriptors for sound classification. Proceed-\nings of the 2002 ICMC, Goteborg (Sweden). 2002.\n[65535] Peeters, G. A large set of audio features for sound \ndescription in the Cuidado project\n[65535] Quinlan, J.R. C4.5: Programs for machine learn-\ning. Morgan Kaufmann. 1993\n[65535] Sandvold, V. Gouyon, F. Herrera, P. Percussion \nclassification in polyphonic audio recordings using \nlocalized sound models, ISMIR 2004\n[65535] Scheirer, Eric D. and Slaney, Malcolm Construc-\ntion and evaluation of a robust multifeature \nspeech/music discriminator. Proc. ICASSP ’97. 1997\n[65535] Schölkopf, B. and Smola, A. Learning with Ker-\nnels, MIT Press, Cambridge, MA. 2002.\n[65535] Shawe-Taylor, J. and Cristianini, N. Support \nVector Machines and other kernel-based learning \nmethods - Cambridge University Press. 2000.[65535] West, K., Cox, S., Features and Classifiers for the \nautomatic classification of musical audio signals, IS-\nMIR 2004.\n[65535] Witten, I.H. Eibe, F. Data Mining: Practical Ma-\nchine Learning Tools and Techniques. M. Kaufmann \nPublisher, 2nd Edition. 2005\n[65535] Yoshii, K., Goto, M., and Okuno, H.G.  AdaMast: \nA Drum Sound Recognizer based on Adaptation and \nMatching of Spectrogram Templates, ISMIR 2004\n[65535] Zils A., Pachet F., Delerue O., Gouyon F. Auto-\nmatic Extraction of Drum Tracks from Polyphonic \nMusic Signals. Proceedings of WEDELMUSIC, Dec. \n2002\n[65535] Pachet, F. and Roy, P. Exploring billion s of audio \nfeatures. In Eurasip, editor, Proc. of CBMI 07, 2007"
    },
    {
        "title": "Comparative Analysis of Multiple Musical Performances.",
        "author": [
            "Craig Stuart Sapp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417693",
        "url": "https://doi.org/10.5281/zenodo.1417693",
        "ee": "https://zenodo.org/records/1417693/files/Sapp07.pdf",
        "abstract": "A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible influence between performances. 1 INTRODUCTION In the Mazurka Project 1 conducted at CHARM during the past two years along with Nicholas Cook and Andrew Earis, we have collected over 2,500 recorded performances for 49 of Chopin’s mazurkas—on average over 50 performances for each mazurka. Keeping track of differences and similarities between numerous performances is difficult when comparing recordings heard weeks, months or even years apart. And remembering the distinguishing features of 50 individual performances of a composition would be taxing on anyone’s memory. Often the surface acoustics of a performance (such as reverb, microphone placement, piano model, recording/playback noise) are more noticeable and memorable than the actual performance, so identifying related performances solely by ear can sometimes be difficult. A written score contains only the most basic of expressive instructions. The composer relies on the performer to interpret the work according to implicit rules as well as the written instructions. The unwritten rules of a composition are transmitted aurally between performers as well as passed down from teacher to student. These performance conventions can apply to specific pieces, genres, composers or entire time periods. Performances may involve combining interpretations from several sources, such as 1 http://mazurka.org.uk c⃝2007 Austrian Computer Society (OCG). teachers or other admired pianists; or conversely, it could be a reaction against convention. To help in the exploration of influences between performances, basic descriptions of tempo and dynamics are extracted from each performance of a work which can then be correlated against each other. A single global similarity measurement for this data could miss interesting smallerscale structures. Therefore, the following plots were developed which display the closest performance to the reference at all possible timescales. In the most interesting variation of the plot, each performance is assigned a color, and when a particular performance is most similar to the reference, its color is filled in the corresponding point in the plot. As a result of looking at all time spans, patterns of color emerge which can give clues to the relative importance of other performances to the reference performance of the plot. 2 RAW DATA Two types of data are used for comparative analysis: beat duration and loudness. There are many other facets of performance which are being ignored, such as individual note timings, voicing, pedaling, and articulation. However, tempo and overall loudness level at the beats are easier to extract from audio data than many other expressive features and form a reasonable expressive baseline. Both tempo and loudness data are extracted beat by beat throughout a performance, and the data can be plotted against the sequence of beats as illustrated in Figure 1. While the data is extracted by beat from the performances for this paper, we are also working on extracting individual note times and dynamics (including off-beats as well as hand synchrony). Such fine-grained performance information may prove useful in characterizing similarities or differences between performances. Beat durations are extracted by first recording taps in real-time while listening to a performance in an audio editor called Sonic Visualiser developed at the Centre for Figure 1. Average tempo and dynamic graphs for 35 performances of mazurka in B minor, 30/2. Digital Music at Queen Mary, University of London. 2 The resulting taps are not aligned precisely to true beat onsets in mazurkas due to a lag in response by the listener— typically with a standard deviation of 60–80 ms (compared to about 30 ms for following a steady tempo). Therefore, audio analysis plugins are used to assist in adjusting the taps onto the exact attack times of notes played on the beats. 3 By repeating data entry for the same performance in an independent manner, the alignment error is reduced to a standard deviation of around 11 ms. Defining a data error as a difference in beat localization by more than 50 ms, the measured data-entry error rate was about 1% for recordings made after 1980 and 3% for recordings in good condition from the early 1920’s. At timing resolutions around 10 ms, defining beat location can become difficult in piano music, particularly due to attack-time differences between the left and right hands (hand synchrony). In these cases, the best procedure is to define the beat location in a consistent manner in the analogous places in each performance. Since the melody usually contains more expressive timing, it is useful to define the beat as the time at which the melody note is played rather than using the less-expressive accompaniment. For comparisons of musical dynamics between performances, a smoothed version of the raw power calculated for the audio signal every 10 ms is sampled at each beat location. The raw power in decibels in a sample of audio is given by the equation: raw power = 10 log10",
        "zenodo_id": 1417693,
        "dblp_key": "conf/ismir/Sapp07",
        "keywords": [
            "Beat duration",
            "Loudness",
            "Correlation",
            "Reference performance",
            "Sub-sequences",
            "Performance conventions",
            "Coloring",
            "Patterns",
            "Similarity measurement",
            "Fine-grained performance information"
        ],
        "content": "COMPARATIVE ANALYSIS OF MULTIPLE MUSICAL PERFORMANCES\nCraig Stuart Sapp\nRoyal Holloway, University of London\nCentre for the History and Analysis of Recorded Music (CHARM)\nABSTRACT\nA technique for comparing numerous performances of\nan identical selection of music is described. The basic\nmethodology is to split a one-dimensional sequence into\nall possible sequential sub-sequences, perform some op-\neration on these sequences, and then display a summary\nof the results as a two-dimensional plot; the horizontal\naxis being time and the vertical axis being sub-sequence\nlength (longer lengths on top by convention). Most types\noftimewisedataextractedfromperformancescanbecom-\nparedwiththistechnique,althoughthecurrentfocusison\nbeat-level information for tempo and dynamics as well as\ncommixtures of the two. The primary operation used on\neachsub-sequenceiscorrelationbetweenareferenceper-\nformanceandanalogoussegmentsofotherperformances,\nthenselectingthebestcorrelatedperformancesforthesum-\nmary display. The result is a useful navigational aid for\ncoping with large numbers of performances of the same\npiece of music and for searching for possible inﬂuence\nbetween performances.\n1 INTRODUCTION\nIn the Mazurka Project1conducted at CHARM during\nthe past two years along with Nicholas Cook and An-\ndrewEaris,wehavecollectedover2,500recordedperfor-\nmancesfor49ofChopin’smazurkas—onaverageover50\nperformances for each mazurka. Keeping track of differ-\nences and similarities between numerous performances is\ndifﬁcultwhencomparingrecordingsheardweeks,months\nor even years apart. And remembering the distinguish-\ning features of 50 individual performances of a compo-\nsition would be taxing on anyone’s memory. Often the\nsurfaceacousticsofaperformance(suchasreverb,micro-\nphoneplacement,pianomodel,recording/playbacknoise)\naremorenoticeableandmemorablethantheactualperfor-\nmance, so identifying related performances solely by ear\ncan sometimes be difﬁcult.\nAwrittenscorecontainsonlythemostbasicofexpres-\nsive instructions. The composer relies on the performer\ntointerprettheworkaccordingtoimplicitrulesaswellas\nthewritteninstructions. Theunwrittenrulesofacomposi-\ntionaretransmittedaurallybetweenperformersaswellas\npassed down from teacher to student. These performance\nconventions can apply to speciﬁc pieces, genres, com-\nposers or entire time periods. Performances may involve\ncombining interpretations from several sources, such as\n1http://mazurka.org.uk\nc/circlecopyrt2007 Austrian Computer Society (OCG).teachers or other admired pianists; or conversely, it could\nbe a reaction against convention.\nTohelpintheexplorationofinﬂuencesbetweenperfor-\nmances,basicdescriptionsoftempoanddynamicsareex-\ntracted from each performance of a work which can then\nbecorrelatedagainsteachother. Asingleglobalsimilarity\nmeasurement for this data could miss interesting smaller-\nscale structures. Therefore, the following plots were de-\nveloped which display the closest performance to the ref-\nerence at all possible timescales.\nIn the most interesting variation of the plot, each per-\nformanceisassignedacolor,andwhenaparticularperfor-\nmanceismostsimilartothereference,itscolorisﬁlledin\nthe corresponding point in the plot. As a result of looking\nat all time spans, patterns of color emerge which can give\nclues to the relative importance of other performances to\nthe reference performance of the plot.\n2 RAW DATA\nTwo types of data are used for comparative analysis: beat\nduration and loudness. There are many other facets of\nperformance which are being ignored, such as individual\nnote timings, voicing, pedaling, and articulation. How-\never,tempoandoverallloudnesslevelatthebeatsareeas-\nier to extract from audio data than many other expressive\nfeatures and form a reasonableexpressive baseline.\nBoth tempo and loudness data are extracted beat by\nbeat throughout a performance, and the data can be plot-\ntedagainstthesequenceofbeatsasillustratedinFigure1.\nWhilethedataisextractedbybeatfromtheperformances\nfor this paper, we are also working on extracting individ-\nual note times and dynamics (including off-beats as well\nashandsynchrony). Suchﬁne-grainedperformanceinfor-\nmation may prove useful in characterizing similarities or\ndifferences between performances.\nBeat durations are extracted by ﬁrst recording taps in\nreal-time while listening to a performance in an audio ed-\nitor called Sonic Visualiser developed at the Centre for\nFigure 1. Average tempo and dynamic graphs for 35 per-\nformances of mazurka in B minor, 30/2.Digital Music at Queen Mary, University of London.2\nTheresultingtapsarenotalignedpreciselytotruebeaton-\nsetsinmazurkasduetoalaginresponsebythelistener—\ntypically with a standard deviation of 60–80 ms (com-\nparedtoabout30msforfollowingasteadytempo). There-\nfore, audio analysis plugins are used to assist in adjusting\nthetapsontotheexactattacktimesofnotesplayedonthe\nbeats.3Byrepeatingdataentryforthesameperformance\nin an independent manner, the alignment error is reduced\nto a standard deviation of around 11 ms. Deﬁning a data\nerror as a difference in beat localization by more than 50\nms, the measured data-entry error rate was about 1% for\nrecordingsmadeafter1980and3%forrecordingsingood\ncondition from the early 1920’s.\nAttimingresolutionsaround10ms,deﬁningbeatloca-\ntion can become difﬁcult in piano music, particularly due\ntoattack-timedifferencesbetweentheleftandrighthands\n(hand synchrony). In these cases, the best procedure is\nto deﬁne the beat location in a consistent manner in the\nanalogous places in each performance. Since the melody\nusuallycontainsmoreexpressivetiming,itisusefultode-\nﬁnethebeatasthetimeatwhichthemelodynoteisplayed\nrather than using the less-expressive accompaniment.\nFor comparisons of musical dynamics between perfor-\nmances, a smoothed version of the raw power calculated\nfor the audio signal every 10 ms is sampled at each beat\nlocation. The raw power in decibels in a sample of audio\nis given by the equation:\nraw power = 10 log10/parenleftBigg\n1\nN/summationdisplay\nnx2\nn/parenrightBigg\n(1)\nwhere Nis the number of audio-samples in sequence x\nbeing considered. The raw power measurements are then\nsmoothed with an exponential smoothing ﬁlter described\nby the following difference equation:\ny[n] =α x[n] + (1−α)y[n−1] (2)\nwhere αis a constant set to 0.2 in the case of 44100 Hz\naudio data with power measurements made every 10 ms.\nThe exponential smoothing ﬁlter is applied twice to the\nrawpowerdata: onceintheforwarddirectionandoncein\nthetime-reverseddirection. Thiskeepsthesmootheddata\ncentered at its original time location. To extract a loud-\nness level for a particular beat in the audio, the smoothed\npowervalueabout70msafterthatonsetisused—tocom-\npensate for a loss of high-frequency information in the\nsmoothed data which delays the maximum amplitude lo-\ncation of note attacks.\n3 ANALYSIS TOOLS\n3.1 Correlation\nNormalizedcorrelation,or Pearsoncorrelation,isdeﬁned\ninEquation3. Thisformofcorrelationyieldsvaluesinthe\nrange from −1.0to+1.0, with 1.0being an exact match,\nand0.0indicating no predictable relation between the se-\nquences being compared.\nr(x, y) =/summationdisplay\nn(xn−¯x)(yn−¯y)\n/radicalbigg/summationdisplay\nn(xn−¯x)2/summationdisplay\nn(yn−¯y)2(3)\n2http://www.sonicvisualiser.org\n3http://sv.mazurka.org.ukwhere xandyare number sequences of the same length;\n¯xand ¯yare average values of each number sequences x\nandy.\nCorrelationisausefulwaytomeasurethesimilaritybe-\ntween two shapes such as comparing different performers\ntempo and dynamic curves as shown in Figure 1.\n3.2 Scape plot\nCorrelation values are difﬁcult to interpret in isolation, so\nthe following plotting method is one way of presenting\nthe data in a more human-readable format. Scape plots\ntake their name from the word landscape since they show\nsmall-scale features analogous to the foreground in a pic-\nture, as well as large-scale features similar to the back-\nground. And like a painting, the interesting parts of the\nscape plot usually lie somewhere in the middle-ground.\nConsider a simple example illustrated in Figure 2. A\nmusical performance consists of six beats which are la-\nbeled: A,B,C,D,E,andF.Thesesixbeatscanbechopped\nup into 21 unique sub-sequences ( n-grams). Firstly, the\nelements can be considered in isolation. Next they can\nbe grouped by sequential pairs: AB, BC, CD, DE, EF.\nThenbythrees: ABC,BCD,CDE,DEF;byfours: ABCD,\nBCDE, CDEF; by ﬁves: ABCDE, BCDEF; and ﬁnally\nonesequencecoveringtheentireperformance: ABCDEF.\nAll of these possible sub-sequences of the basic six-beat\nperformance,canbearrangedontopofeachothertoform\nthe arrangement shown in Figure 2.\nOriginally the scape plotting method was designed for\nstructural analysis of harmony in musical scores ([2] and\n[3]). However, it has also been applied to audio-based\nharmony analysis[1] and timbral analysis[4].\nFigure2. Scapeplottingdomain(left)andanexampleap-\nplication of averaging in each cell (right), where the orig-\ninal data sequence is (7,6,2,5,8,4).\n4 COMPARATIVE PERFORMANCE SCAPES\nWhat operation is done in each cell of a scape plot is ar-\nbitrary. The plot on the right in Figure 2 shows the ap-\nplication of averaging in each cell. In the following sub-\nsections, the calculation for each cell is done using the\nfollowing steps:\n•Choose one performance to be the reference for a\nparticular plot.\n•For each cell in the scape plot, measure the corre-\nlation between the reference performance and all\nother performances, then make note of the perfor-\nmance which yields the highest correlation value.\n•Color the cell with a unique hue assigned to that\nhighest-correlating performance.\nNotethattheactualcorrelationvaluesarethrownawayin\nthis variation of the scape plot. This is primarily becauseFigure 3. Timescapes for two performances of mazurka\nin C major, 24/2, showing teacher/student pairing, each\nshowing large regions of best-correlation to each other\n(out of 35 performances).\nthe plots would become too complex and confusing if it\nwere kept (for example displayed as gray-scale mask on\nthe indexed performance colors). Other plot variants may\ndisplay raw correlation values such as one that correlates\nhalf-sinearchestoperformancedataforidentifyingphras-\ning structure.\n4.1 Timescapes\nFigure3demonstratesapairofsimilarperformancesfound\nin the set for mazurka in C major, Op. 24, No. 2. Mutual\nbest matching seen in this ﬁgure indicates a strong link\nbetween two performances and is less likely to be caused\nby chance. However, other structures seen in this ﬁgure\nare more likely to be random links to other performances\nwith no interesting relationships. The total area covered\nin a plot by a particular performance is also an indication\nofsigniﬁcance,butlesssothanmutualsimilaritybetween\ntwoparticularperformances. Inthiscasetheperformance\non the left contains an area of 76% from another partic-\nular performance, and that performance in turn contains\n58% by area of the original performance. Who was inﬂu-\nenced by whom cannot be deduced from the plots. They\nonly show that there is a strong relationship between the\ntwo performances in this case. Clues as to what is going\nFigure 4. Same performances as in Figure 3, but with the\naverage of all performancesincluded (black).\nFigure 5. Timescapes for three performances of mazurka\nin B minor, Op. 30, No. 2. showing early, middle and late\ncareer performances by ArthurRubinstein.\non can be gleaned from the fact that the performance on\nthe left was recorded in 1999 and the one on the right in\n2005; also the performer on the right did post-graduate\nstudies with the performer represented on the left.\nIt is often useful to include the average of all perfor-\nmances in the collection of a piece of music being ana-\nlyzedsothatminorandrandomrelationshipsbetweenper-\nformancesarehiddenbythesimilaritytotheaverageper-\nformance which is usually quite strong. Figure 4 demon-\nstrates the effect of including the average performance\nalong with the other real performances (compare to Fig-\nure 3).\nIn all ﬁve mazurkas examined comprehensively so far,\nall performers for which we have multiple recordings of\nshow very strong relations to each other, regardless of the\namountoftimebetweentherecordings. InFigure5,three\nrecordings of Arthur Rubinstein are displayed—an early,\nmiddle and late career sampling covering a time period\nof 25 years. In each case, the closest performance to the\nreference is another Rubinstein performance.\n4.2 Dynascapes\nBeat-level tempo is fairly unique to each performer, and\nwhenthereisastrongmutualsimilaritybetweenperform-\ners,itisusuallynotlikelytobeacoincidence. Fordynam-\nics (beat-level amplitude measurements in this case), the\nuniquenessislesspronounceddueinparttothecomposer\nwritingbasicloudnessguidessuchas forteorpianointhe\ncomposition or data extraction accuracy. Dynamics (as\nextractedinthisstudy)arelessuniquetoasingleindivid-\nualperformer,andagreaterlikelihoodofrandompatterns\nFigure 6. Two dynascapes of mazurka in C /sharpminor, 63/3,\nshowing early/late career pairing of performers.make the plots more difﬁcult to interpret than when using\ntempo data. Also, it is possible that tempo expressivity is\nmore static between performances, while loudness is eas-\nier to consciously control.\nHowever, Figure 6 shows some nice mutually similar\ndynascapes for the same performer, recorded almost 40\nyears apart. In this case, the performer is closest to his\ndynamic interpretations in these two performance than to\nany of the other 58 performance of the same work which\nwereexamined. Alsoconsiderthattheperformanceswere\nrecorded in very different technological eras, the ﬁrst in\nthetime of78 rpm records, whilethe laterone inthe 33.3\nrpm era.\n4.3 Scape plots of parallelfeature sequences\nFor Pearson correlation calculations, the ordering of the\ndata is not signiﬁcant as long as the sequence order is\nidentical for both performances. But to generate multi-\nfeature scape plots with a structure similar to the single-\ndata forms, the independent values are interleaved in the\ncorrecttimeordersothatthestructureinthescapeplotre-\nmainsanalogoustothesingle-sequenceplots. Tocombine\ntempoanddynamicsforcomparisonbetweenperformers,\nthe time series of each feature are interleaved. Here are\nexamples of two data sequences for tempo and dynamics\nto be mixed:\nt= (t1, t2, t2, t4, ..., t n) (4)\nd= (d1, d2, d2, d4, ..., d n) (5)\nTo mix them together with equal strength, create an-\nother sequence of joint features which interleaves tempo\nand dynamic values:\nJ= (Jt,1, Jd,1, Jt,2, Jd,2, ..., J t,n, Jd,n)(6)\nTominimizetheeffectofmixingunrelateddatainsuch\namannerforthecorrelationcalculations,thestandardde-\nviationandmeanofthetwosetsofdatashouldbeequiva-\nlent. Inthiscasethetempovaluesareleftunchangedsince\nthey contain more performance information to start with:\nJt,n=tn (7)\nwhiletheloudnesssequence’sstandarddeviationandmean\nare adjusted to match that of the tempo sequence:\nJd,n=st/parenleftbiggdn−¯d\nsd/parenrightbigg\n+¯t (8)\nwhere sxmeans the standard deviation of a sequence x,\nand ¯xrepresents the mean value of a sequence x. The\njoint sequence can either be created globally, or locally\nbasedonthesub-sequencedata(thelatterwouldnotwork\nwell at small timescales).\nFigure 7 demonstrates the beneﬁt of ﬁnding a perfor-\nmance match which is probably not random. When only\ntime data is compared, there is little direct matching be-\ntween the two performances. Comparing dynamics alone\ngives a stronger match between the performances, but is\ndifﬁculttoascertainifthematchisrelevantduetothelim-\nitedrangefordynamicsbetweenperformances. However,\nwhen both time and dynamic data are processed in paral-\nlel into a scape plot, the match between the performance\nbecomes clear, and is likely to show a direct relation be-\ntween the performance rather than a random occurrence.\nFigure 7. Tempo, dynamics and joint data plots. Black\nregionsindicatemutualbestmatches. Stripedregionindi-\ncates a third performer common to both.\n5 CONCLUSIONS AND FUTURE WORK\nSigniﬁcanceofcorrelationmeasurementsisdifﬁculttoas-\nsess in performance data since it is hard to statistically\nmodel a performer. So the precise meanings of the color\npatterns which emerge are not easy to pin down. Scape\nplots are a step towards identifying signiﬁcant relations\nand can show where in a performance similarities are oc-\ncurring.\nThe most difﬁcult aspect of the plots is determining\nhow relevant the best matches between performances are.\nLargepatchesofcolordoseemtobemoresigniﬁcant,but\nnot always. In particular, if a patch of color starts from a\npointandwidensasitrisesinaplot,itismostlikelydueto\nchance. Mutual best-matches between performers seems\nto be a good indication of signiﬁcance, and sharp bound-\nariesbetweencolorregionsalsotendtoindicatemoresig-\nniﬁcant matches.\nTempo data in particular can be a superposition of sev-\neral types of performance features. In mazurkas, for ex-\nample,thelow-frequencytempocomponent(phrasing)can\nbe controlled independently by the performer from the\nhigh-frequency mazurka metrical pattern (where the ﬁrst\nbeat is typically shorter than the other two) and time ac-\ncentuation of notes. Thus, it would be useful to identify\nandextractsingleperformancefeaturesandcomparethem\nin isolation as well as in composite.\n6 REFERENCES\n[1] G´omez,EmiliaandJordiBonada.“Tonalityvisualisa-\ntionofpolyphonicaudio”, ProceedingsoftheInterna-\ntionalComputerMusicConference ,Barcelona,Spain,\n2005.\n[2] Sapp, Craig. “Harmonic visualizations of tonal mu-\nsic”,Proceedings of the International Computer Mu-\nsic Conference , Havana, Cuba, 2001. pp. 423-430.\n[3] Sapp, Craig. “Visual hierarchical key analysis”, Com-\nputers in Entertainment 3/4 (October 2005). ACM\nPress; New York.\n[4] Segnini, Rodrigo and Craig Sapp. “Scoregram: Dis-\nplaying gross timbre information from a score”, Pro-\nceedingsoftheInternationalSymposiumonComputer\nModeling and Retrieval , Pisa, Italy, 2005."
    },
    {
        "title": "Web-Based Detection of Music Band Members and Line-Up.",
        "author": [
            "Markus Schedl",
            "Gerhard Widmer",
            "Tim Pohle",
            "Klaus Seyerlehner"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418325",
        "url": "https://doi.org/10.5281/zenodo.1418325",
        "ee": "https://zenodo.org/records/1418325/files/SchedlWPS07.pdf",
        "abstract": "We present first steps towards the automatic detection of music band members and instrumentation using web content mining techniques. To this end, we combine a named entity detection method with rule-based linguistic text analysis. We report on preliminary evaluation results and discuss limitations of the current method. 1 INTRODUCTION AND CONTEXT Automatic extraction of textual information about music artists can be used, for example, to enrich music information systems, for automatic biography generation, to build relationship networks, or to define similarity measures between artists, a key concept in music information retrieval. Here, we present an approach to finding the members of a given music band and the respective instruments they play. In this preliminary work, we restrict instrument detection to the standard line-up of most Rock bands, i.e. we only check for singer(s), guitarist(s), bassist(s), drummer(s), and keyboardist(s). 2 METHODS Basically, our approach comprises four steps: web retrieval, named entity detection, rule-based linguistic analysis, and rule selection. Web Retrieval Given a band name B, we use Google to retrieve the URLs of the 100 top-ranked web pages, whose content we then retrieve via wget 1 . Trying to restrict the query results to those web pages that actually address the music band under consideration, we add domain-specific keywords to the query, which yields the following four query schemes: • “B”+music (abbreviated as M in the following) • “B”+music+review (MR) • “B”+music+members (MM) • “B”+lineup+music (LUM) Discarding all markup tags, we eventually obtain a plain text representation of each web page. 1 http://www.gnu.org/software/wget c⃝2007 Austrian Computer Society (OCG). Named Entity Detection There is a large amount of literature on the topic of named entity detection. A good introduction can be found, for example, in [1]. For this work, we follow a quite simple approach. First, we extract all 2-, 3-, and 4-grams from the plain text representation of the web pages. 2 Subsequently, some basic filtering is performed. We exclude those N-grams whose substrings contain only one character and retain only those N-grams whose tokens all have their first letter in upper case and all remaining letters in lower case. Finally, we use the iSpell English Word Lists 3 to filter out those N-grams which contain at least one substring that is a common speech word. The remaining Ngrams are regarded as potential band members. Rule-based Linguistic Analysis Having determined the potential band members, we perform a simple linguistic analysis to obtain the actual instrument of each member. Similar to the approach proposed in [3] for finding hyponyms in large text corpora, we define the following rules and apply them on the potential band members.",
        "zenodo_id": 1418325,
        "dblp_key": "conf/ismir/SchedlWPS07",
        "keywords": [
            "web retrieval",
            "named entity detection",
            "rule-based linguistic analysis",
            "rule selection",
            "domain-specific keywords",
            "plain text representation",
            "N-grams",
            "iSpell English Word Lists",
            "potential band members",
            "instrument"
        ],
        "content": "WEB-BASED DETECTION OF MUSIC BAND MEMBERS AND LINE-UP\nMarkus Schedl1Gerhard Widmer1,2Tim Pohle1Klaus Seyerlehner1\n1Department of Computational Perception, Johannes Kepler University, Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence, Vienna, Austria\nABSTRACT\nWe present ﬁrst steps towards the automatic detection of\nmusic band members and instrumentation using web con-\ntent mining techniques. To this end, we combine a named\nentity detection method with rule-based linguistic text anal-\nysis. We report on preliminary evaluation results and dis-\ncuss limitations of the current method.\n1 INTRODUCTION AND CONTEXT\nAutomatic extraction of textual information about music\nartists can be used, for example, to enrich music informa-\ntion systems, for automatic biography generation, to build\nrelationship networks, or to deﬁne similarity measures be-\ntween artists, a key concept in music information retrieval.\nHere, we present an approach to ﬁnding the members of\na given music band and the respective instruments they\nplay. In this preliminary work, we restrict instrument de-\ntection to the standard line-up of most Rock bands, i.e.\nwe only check for singer(s), guitarist(s), bassist(s), drum-\nmer(s), and keyboardist(s).\n2 METHODS\nBasically, our approach comprises four steps: web re-\ntrieval, named entity detection, rule-based linguistic anal-\nysis, and rule selection.\nWeb Retrieval\nGiven a band name B, we use Google to retrieve the URLs\nof the 100 top-ranked web pages, whose content we then\nretrieve via wget1. Trying to restrict the query results\nto those web pages that actually address the music band\nunder consideration, we add domain-speciﬁc keywords to\nthe query, which yields the following four query schemes:\n•“B”+music (abbreviated as M in the following)\n•“B”+music+review (MR)\n•“B”+music+members (MM)\n•“B”+lineup+music (LUM)\nDiscarding all markup tags, we eventually obtain a plain\ntext representation of each web page.\n1http://www.gnu.org/software/wget\nc/circlecopyrt2007 Austrian Computer Society (OCG).Named Entity Detection\nThere is a large amount of literature on the topic of named\nentity detection. A good introduction can be found, for\nexample, in [1]. For this work, we follow a quite simple\napproach. First, we extract all 2-, 3-, and 4-grams from\nthe plain text representation of the web pages.2Subse-\nquently, some basic ﬁltering is performed. We exclude\nthose N-grams whose substrings contain only one charac-\nter and retain only those N-grams whose tokens all have\ntheir ﬁrst letter in upper case and all remaining letters in\nlower case. Finally, we use the iSpell English Word Lists3\nto ﬁlter out those N-grams which contain at least one sub-\nstring that is a common speech word. The remaining N-\ngrams are regarded as potential band members.\nRule-based Linguistic Analysis\nHaving determined the potential band members, we per-\nform a simple linguistic analysis to obtain the actual in-\nstrument of each member. Similar to the approach pro-\nposed in [3] for ﬁnding hyponyms in large text corpora,\nwe deﬁne the following rules and apply them on the po-\ntential band members.\n1.Mplays the I\n2.Mwho plays the I\n3.R M\n4.Mis the R\n5.M, the R\n6.M(I)\n7.M(R)\nIn these rules, Mis the potential band member, Iis the in-\nstrument, and Ris the role Mplays within the band (singer,\nguitarist, bassist, drummer, keyboardist). For IandR, we\nuse synonym lists to cope with the use of multiple terms\nfor the same concept (e.g. percussion anddrums ). We\nfurther count on how many of the web pages each rule\napplies for each MandI(orR).\nRule Selection\nThese counts are document frequencies (DF) since they\nindicate, for example, that on 24web pages Ralf Scheep-\nersis said to be the singer of the band Primal Fear ac-\ncording to rule 6(on6pages according to rule 3, and so\non). To reduce uncertain information, we ﬁlter out those\nrules whose DF is below a threshold expressed as a frac-\ntion of the DF of the highest scored rule (according to\nthe DF score of all applying rules for the band under con-\nsideration).4Finally, for every instrument, the rule with\n2We assume no artist name to comprise more than four single names.\n3http://wordlist.sourceforge.net\n4In our experiments we used 0.2of the maximum DF as threshold.the highest DF is selected and the respective (member,\ninstrument)-pair is predicted.\n3 EVALUATION AND RESULTS\nTo evaluate our approach, we compiled a ground truth\nbased on one author’s private music collection. Since this\nis a quite labor-intensive task, we restricted the collection\nto51bands, with a strong focus on the genre Metal . The\nchosen bands vary strongly with respect to their popular-\nity (some are very well known, like Metallica , but most\nare largely unknown, like Powergod ,Pink Cream 69 , or\nRegicide ). We gathered the current line-up of the bands\nby consulting Wikipedia5,allmusic6,Discogs7, or the\nband’s web site. Finally, our ground truth contained 240\nmembers with their respective instruments.\nWe use three different string comparison methods to eval-\nuate our approach. First, we perform exact string match-\ning. Addressing the problem of different spelling for the\nsame artist (e.g. the drummer of Tiamat ,Lars Sk ¨old, is\noften referred to as Lars Skold ), we also evaluate the ap-\nproach on the basis of a canonical representation of each\nband member. To this end, we perfom a mapping of sim-\nilar characters to their stem, e.g. ¨a,`a,´a,˚a, æ toa. Fur-\nthermore, to cope with the fact that many artists use nick-\nnames or abbreviations of their real names, we apply an\napproximate string matching method. According to [2],\nthe so-called Jaro-Winkler similarity is well suited for per-\nsonal ﬁrst and last names since it favors strings that match\nfrom the beginning for a ﬁxed preﬁx length (e.g. Edu\nFalaschi vs. Eduardo Falaschi , singer of the Brazilian\nband Angra ). We use a level two distance function based\non the Jaro-Winkler distance metric, i.e. the two strings to\ncompare are broken into substrings (ﬁrst and last names,\nin our case) and the similarity is calculated as the com-\nbined similarities between each pair of tokens. We assume\nthat the two strings are equal if their Jaro-Winkler similar-\nity is above 0.9. For calculating the distance, we use the\nopen-source Java toolkit SecondString8.\nTable 1 shows the overall recall of the (band member,\ninstrument)-pairs on the ground truth. A (member, instru-\nment)-pair is only considered as correct if both the mem-\nber and the instrument are predicted correctly. As for the\ninﬂuence of the query scheme, no signiﬁcant difference\ncould be made out between the M, the MR, and the MM\nsettings. In contrast, the LUM scheme performed much\nworse, so we will exclude it in future experiments.\nTo estimate the goodness of the results given in Table 1,\nwe analyzed, for the best performing query scheme M,\nhow many of the actual band members (according to the\nground truth) occur at least once in the retrieved web pages,\ni.e. for every band B, we calculated the recall, on the\nground truth, of the N-grams extracted from B’s web pages.\nWe veriﬁed that no band members were erroneously dis-\ncarded in the N-gram selection process. Over all band\nmembers, we obtained recall values of 56.00%,57.64%,\nand63.44% using exact matching, similar character map-\nping, and Jaro-Winkler distance, respectively. Taking these\nupper limits into account, the recall values given in Table 1\n5http://www.wikipedia.org\n6http://www.allmusic.com\n7http://www.discogs.com\n8http://secondstring.sourceforge.netTable 1 . Recall, in percent, of the (member, instrument)-\npairs on the ground truth for different query schemes and\nstring distance functions.\nexact similar char L2-JaroWinkler\nM 34.76 37.14 39.05\nMR 34.11 36.45 37.85\nMM 35.98 36.92 37.38\nLUM 26.64 27.57 27.57\nare quite promising for this preliminary study.\nTaking a qualitative look on the results, good performance\nwas achieved for those bands whose principal members\nspent a long time in the band and are still members, re-\ngardless of the popularity of the band. For example, all\n(member, instrument)-pairs were correctly identiﬁed for\nthe very famous Iron Maiden , but also for the less known\nEdguy andPink Cream 69 . On the other hand, our ap-\nproach obviously has problems with heavy band member\nﬂuctuations, especially if a very famous member left the\nband after years of participation. A good example of this\nisNightwish , whose long-term singer Tarja Turunen left\nthe band in 2006. Moreover, since we restricted instru-\nment detection to the ﬁve most popular ones used in Rock\nbands, the approach cannot deal with bands like Apoca-\nlyptica , comprising three cellists and one drummer.\n4 FUTURE WORK\nAs for future work, we will try to improve performance by\nusing more sophisticated rules and named entity detection\napproaches. Furthermore, we aim at deriving complete\nband histories (by searching for dates when a particular\nartist joined or left a band). This would allow us to create\ntime-dependent relationship networks that could be used\nto derive a similarity measure. One possible application\nfor this research is the creation of a domain-speciﬁc search\nengine for music artists, which is our ultimate aim.\n5 ACKNOWLEDGEMENTS\nThis research is supported by the Austrian Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung (FWF) under\nproject number L112-N04 and by the Vienna Science and\nTechnology Fund (WWTF) under project number CI010\n(Interfaces to Music).\n6 REFERENCES\n[1] J. Callan and T. Mitamura. Knowledge-Based Extrac-\ntion of Named Entities. In Proc. of the 11th Intl.\nConf. on Information and Knowledge Management ,\nMcLean, V A, USA, November 2002.\n[2] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A\nComparison of String Distance Metrics for Name-\nMatching Tasks. In Proc. of the IJCAI-03 Workshop on\nInformation Integration on the Web , Acapulco, Mex-\nico, August 2003.\n[3] M. A. Hearst. Automatic Acquisition of Hyponyms\nfrom Large Text Corpora. In Proc. of the 14th Conf.\non Computational Linguistics - Vol. 2 , Nantes, France,\nAugust 1992."
    },
    {
        "title": "A Qualitative Assessment of Measures for the Evaluation of a Cover Song Identification System.",
        "author": [
            "Joan Serrà"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415174",
        "url": "https://doi.org/10.5281/zenodo.1415174",
        "ee": "https://zenodo.org/records/1415174/files/Serra07.pdf",
        "abstract": "The evaluation of effectiveness in Information Retrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process. This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identification. We present a concrete example and then try to elucidate which metrics work best to evaluate such a system. We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves. 1 INTRODUCTION Before the final implementation of any Information Retrieval (IR) engine, we must carefully consider the quality of the end-product of our efforts. This step can be described as a performance evaluation of a proposed solution. IR techniques can be essentially seen as heuristics: we try to guess something as similar as possible to the right answer. So we have to measure how close to it we can come. Furthermore, evaluation methods are used in a comparative way to measure whether certain changes lead to any improvement in system performance. In particular, when tuning algorithm parameters, it is important to choose the evaluation measure that rewards what we think is a right answer (choosing a valid measure). In the next sections we concentrate on evaluating an IR engine. More precisely, we focus on the evaluation of the effectiveness of a particular Music Information Retrieval (MIR) system. Our goal is to decide which measures are valid (construct validity) for a specific situation which is presented in subsequent sections. This concrete case is related with Cover Song 1 Identification, a very active research topic within the last few years in the MIR community [3, 4, 6], as it provides a direct way of evaluating music similarity algorithms. Some efforts are then being devoted to compare and evaluate different alternatives for this purpose (as MIREX 2 ). 1 According to Wikipedia, in popular music, a cover song (a cover version, or simply cover) is a new rendition, performance or recording of a previously recorded song. 2 http://www.music-ir.org/mirex2006/index.php/Audio Cover Song c⃝2007 Austrian Computer Society (OCG). 2 EVALUATION MEASURES We focus on the situation where a retrieval engine has an input query and it provides an output list of documents (preferably relevant to the query). We find in the literature some measures from binary classification that might be useful for our purpose: False / True Positives and Negatives (TP, FP, TN, FN), Sensitivity and Specificity. We also consider here the Fallout Rate, the Receiver Operating Characteristic (ROC) curve, and the Lift Curve [8]. Finally, some popular IR measures we analyze are: Precision, Recall, the Precision-Recall curve, the Break-even Point, the F-measure and Average Precision (AP). We also consider Reciprocal Rank (RR), Discounted Cumulative Gain (DCG) and Binary Preferencebased measure (bpref and bpref-10) [1, 2, 5, 7]. We do not study here other measures such as Spearman’s Rho or Kendall’s Tau, because our data does not fit to the models they were thought for. Basically, we do not have a true measure of similarity for the ground truth (our cover songs, originally, are not ranked from more similar to less similar, we just only know if they are a cover of a given query or not). 3 CASE STUDY: COVER SONG IDENTIFICATION SYSTEM In this section we study the situation where there is a song database (D, the document collection), and we have to come up with an algorithm that, given a song title (query q), yields a list of potential cover songs (A, a list of their titles in descending order of similarity). Here, the query song is not retrieved (that is: q ̸∈A). We should note that there is not a ground truth for song similarity. As a ground truth data, we label 3 all songs, indicating if they correspond to the same group (the same label is attached to the original song and covers of it) or not. Thus, our judgements are based on binary relevance. For our concrete problem, we have a database of 2054 songs (|D| = 2054), labelled into 451 different groups (or “canonical” song versions). The average number of covers per song is 4.24, ranging from 1 (the original song + 1 cover) to 14. Since the maximum number of covers 3 Do not confuse the song titles (which are not relevant for us), with the label we attach to them after listening. a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 |Rq| q1 ⇒A1 ⋆ 1 q2 ⇒A2 ⋆ ⋆ ⋆ ⋆ 7 q3 ⇒A3 ⋆ ⋆ ⋆ ⋆ 7 q4 ⇒A4 ⋆ ⋆ ⋆ ⋆ 14 q5 ⇒A5 ⋆ ⋆ ⋆ ⋆ 14 q6 ⇒A6 4 Table 1. Test answer set example. It consists of 6 manually labelled answer sets (Ai) answering 6 hypothetical queries (qi). These answer sets are composed of 14 ranked documents (Ai = {a1, . . . , a14}), and they are ordered from most valuable (A1) to less valuable (A6). The “⋆” symbol in (i, j) cell denotes that the aj document is relevant for the i-th query. Last column (|Rq|) denotes the total number of covers for the query qi that can be found in the database. per canonical version is 14, the length of the answer set is set to this number in order to be able to present to a potential user all the relevant songs in a single output list (|A| = 14). A cutoff like this is typically introduced in an IR system because of the paginated presentation of search results.",
        "zenodo_id": 1415174,
        "dblp_key": "conf/ismir/Serra07",
        "keywords": [
            "evaluation",
            "Information Retrieval",
            "Music Information Retrieval",
            "Cover Song Identification",
            "performance evaluation",
            "heuristics",
            "Music Similarity Algorithms",
            "MIREX",
            "construct validity",
            "cover version"
        ],
        "content": "AQUALITATIVEASSESSMENT OF MEASURES FORTHEEVALUATION\nOF ACOVER SONG IDENTIFICATIONSYSTEM\nJoanSerr `a\nMusicTechnologyGroup\nUniversitatPompeu Fabra\njserra@iua.upf.edu\nABSTRACT\nTheevaluationofeffectivenessinInformationRetrieval\nsystems has been developed in parallel to its evolution,\ngeneratingagreatamountofproposalstoachievethispro-\ncess. This paper focuses on a particular task of Music\nInformation Retrieval: a system for Cover Song Identi-\nﬁcation. We present a concrete example and then try to\nelucidate which metrics work best to evaluate such a sys-\ntem. Weendupwithtwoevaluationmeasuressuitablefor\nthisproblem: bprefandNormalizedLift Curves .\n1 INTRODUCTION\nBefore the ﬁnal implementation of any Information Re-\ntrieval(IR)engine,wemustcarefullyconsiderthequality\nof the end-product of our efforts. This step can be de-\nscribed as a performance evaluation of a proposed solu-\ntion. IR techniques can be essentially seen as heuristics:\nwe try to guess something as similar as possible to the\nright answer. So we have to measure how close to it we\ncan come. Furthermore,evaluationmethodsare used in a\ncomparativewaytomeasurewhethercertainchangeslead\nto any improvement in system performance. In particu-\nlar, when tuning algorithm parameters, it is important to\nchoosetheevaluationmeasurethatrewardswhatwethink\nisa rightanswer(choosinga validmeasure).\nInthenextsectionsweconcentrateonevaluatinganIR\nengine. Moreprecisely,we focuson the evaluationof the\neffectiveness of a particular Music Information Retrieval\n(MIR) system. Our goal is to decide which measures are\nvalid (construct validity) for a speciﬁc situation which is\npresentedinsubsequentsections.\nThis concrete case is related with Cover Song1Iden-\ntiﬁcation, a very active research topic within the last few\nyearsin the MIR community[3, 4, 6], as it providesa di-\nrect way of evaluatingmusic similarity algorithms. Some\neffortsarethenbeingdevotedtocompareandevaluatedif-\nferentalternativesforthispurpose(asMIREX2).\n1According to Wikipedia, in popular music, a cover song (a cov er\nversion, or simply cover) is a new rendition, performance or recording\nof apreviously recorded song.\n2http://www.music-ir.org/mirex2006/index.php/Audio CoverSong\nc/circlecopyrt2007AustrianComputerSociety(OCG).2 EVALUATION MEASURES\nWe focus on the situation where a retrieval engine has an\ninput query and it provides an output list of documents\n(preferablyrelevanttothequery).\nWe ﬁnd in the literature some measures from binary\nclassiﬁcationthatmightbeusefulforourpurpose: False/\nTrue Positives and Negatives (TP,FP,TN,FN),Sensitiv-\nityandSpeciﬁcity . Wealsoconsiderherethe FalloutRate ,\ntheReceiver Operating Characteristic (ROC) curve, and\ntheLift Curve [8]. Finally, some popularIR measureswe\nanalyzeare: Precision ,Recall,thePrecision-Recallcurve ,\ntheBreak-even Point , theF-measure andAverage Preci-\nsion(AP). We also consider Reciprocal Rank (RR),Dis-\ncounted Cumulative Gain (DCG) andBinary Preference-\nbasedmeasure (bprefandbpref-10)[1, 2, 5, 7].\nWe do not study here other measures such as Spear-\nman’sRho orKendall’sTau , becauseourdata doesnotﬁt\nto the modelsthey were thoughtfor. Basically, we do not\nhaveatruemeasureofsimilarityforthegroundtruth(our\ncover songs, originally, are not ranked from more similar\nto less similar, we just only know if they are a cover of a\ngivenqueryornot).\n3 CASESTUDY: COVER SONG\nIDENTIFICATION SYSTEM\nInthissectionwestudythesituationwherethereisasong\ndatabase ( D, the document collection), and we have to\ncome up with an algorithm that, given a song title (query\nq), yields a list of potential cover songs ( A, a list of their\ntitles in descending order of similarity). Here, the query\nsongisnotretrieved(thatis: q/ne}ationslash∈A).\nWeshouldnotethatthereisnotagroundtruthforsong\nsimilarity. As a ground truth data, we label3all songs,\nindicatingif theycorrespondto thesame group(thesame\nlabel is attached to the original song and covers of it) or\nnot. Thus,ourjudgementsare basedonbinaryrelevance.\nFor our concrete problem,we have a database of 2054\nsongs( |D|= 2054),labelled into 451differentgroups(or\n“canonical” song versions). The average number of cov-\ners per song is 4.24, ranging from 1 (the original song\n+ 1 cover) to 14. Since the maximum number of covers\n3Do not confuse the song titles (which are not relevant for us) , with\nthe label weattach to them after listening.a1a2a3a4a5a6a7a8a9a10a11a12a13a14 |Rq|\nq1⇒A1 ⋆ 1\nq2⇒A2 ⋆⋆⋆ ⋆ 7\nq3⇒A3 ⋆⋆⋆ ⋆ 7\nq4⇒A4 ⋆ ⋆ ⋆ ⋆ 14\nq5⇒A5 ⋆ ⋆⋆⋆ 14\nq6⇒A6 4\nTable 1. Test answer set example. It consists of 6 manually labelled answer sets ( Ai) answering 6 hypothetical queries\n(qi). These answer sets are composed of 14 ranked documents ( Ai={a1, . . . , a 14}), and they are ordered from most\nvaluable ( A1) to less valuable ( A6). The “ ⋆” symbol in (i, j)cell denotes that the ajdocument is relevant for the i-th\nquery. Last column( |Rq|)denotesthetotalnumberofcoversforthequery qithatcanbefoundinthedatabase.\nper canonical version is 14, the length of the answer set\nis set to this number in order to be able to present to a\npotential user all the relevant songs in a single output list\n(|A|= 14). Acutofflike thisis typicallyintroducedinan\nIRsystembecauseofthepaginatedpresentationofsearch\nresults.\n3.1 Preliminaryhypotheses\nTheorder(ranking)inwhichthedocumentsarepresented\nin an answer set Awill be relevant (as the algorithm at-\ntempts to partially deﬁne a similarity metric, and there-\nfore,toprovidethemostsimilarsongsatthebeginningof\nthe list). From this we hypothesize that rank-based mea-\nsures(like RRorDCG) shouldperformwell.\nAnother main objective of the desired algorithm is to\nmaximize the amount of retrieved covers (like in any au-\ndio identiﬁcation task). Then, we can also argue that Re-\ncall-basedmeasuresﬁtourrequirements. Notethatinour\nexperimentswe will know recall (as all the documentsin\nthecollectionwherewewant tosearchforare labelled).\n3.2 Test framework\nWe manually annotate and rank several synthetic sets of\nprototypicalanswersto differentqueriesin orderto try to\nelucidatewhichmeasurebest ﬁtsourcriteria.\nForasetofqueries Sq={q1, . . ., q Nq},wedeﬁneaset\nof answer sets Sa={A1, . . . , A Nq}, where each Ak=\n{ak,1, ak,2, . . . , a k,14}. In our experiments, the number\nof sets Sqis equal to 30 and Nqranges from 4 to 8. We\nmanually label the retrieved documents ak,jwith a “ ⋆”\nsymbol,whichdenotesif we considerthemtobe relevant\nornottothequery.\nAn example of such an answer set is shown in table 1.\nThis set has been chosen because it representsthe typical\nsituation we want to highlight (regarding ranking and re-\ncall of the answer sets), and it will be the main reference\nforourdiscussioninnextsubsection.\nWe intentionally rank the answers Akfrom most to\nleast importantfor us. This is the way we deﬁne the rele-\nvanceoftheanswersets. Thisalsohelpstoobservewhich\nmeasuresaremoresuitable. Forinstance,wepreferofour\nsystem to retrieve a single cover song if there is only oneinthecollection D,ratherthanretrievingfouroutofseven\n(seeA1andA2in table1), even if they are ranked in the\nﬁrst positions. Also,ona situationwiththesamepercent-\nage of retrievedsongs( |Ra|/|Rq|, where Racorresponds\nto the set of relevantdocumentsfor theanswer set A, and\nRqcorrespondsto theset ofall relevantdocumentsin the\nentire collection D), it would be desirable that they were\nranked at the ﬁrst positions (see A2andA3). Notice that\nA6is the worst answer because it does not retrieve any\nrelevantdocument.\n3.3 Evaluation measures for a Cover Song Identiﬁca-\ntionsystem\nWe implemented all the cited measures and tested sev-\neral synthetic sets of potential answers according to the\nmentioned framework. Table 2 presents the results corre-\nsponding to the example answer set of table 1. We now\nelaboratesomecommentsonresultssuchasthese.\nAll the measures TP,FP,FNandTNprovide us with\nimportantinformationbut are not suitable forour task, as\nthese featuresdonot take into accountthe rank(position)\nofthe correctlyclassiﬁed instances. Furthermore,theydo\nnot consider the total number of relevant documents per\nquery ( |Rq|), so that they do not care, for instance, about\nthedifferenceinretrievingtheonlypossibleitemofa set,\nor one of the largest labelled group (we want the former\nto havea higherrewardthanthelatter).\nAccuracy ,Speciﬁcity andFallout Rate suffer from the\nsame kind of problems as the measures cited above, and,\nin addition, as our data is extremely skewed (in IR sys-\ntems,over99.9 %ofthedocumentsareusuallyinthenot-\nrelevant category [1]), they allow few discernment and\nthey are not discriminative enough between answer sets\n(forinstance, A4andA5).\nIf we plot the ROCandLiftcurves, we ﬁnd the same\nproblems,as theyare basedin TP,FP,FNandTN. How-\never,we have comewith a useful variant(the Normalized\nLift Curve , shown in ﬁgure 1), consisting of plotting the\npercentage of positive examples normalized by the total\nnumber of relevant queries ( %Pos.Ex/ |Rq|) versus the\nratio of examplesnormalized by the length of the answer\nset (%Ex./|A|). Figure 1 provides an easy interpretable\ncurve.Measure A1 A2 A3 A4 A5 A6\nTP 1 4 4 4 4 0\nFP 13 10 10 10 10 14\nFN 0 3 3 10 10 14\nTN 2040 2037 2037 2030 2030 2036\nAccuracy 0.994 0.994 0.994 0.990 0.990 0.991\nSensitivity 1.000 0.571 0.571 0.285 0.285 0.000\nSpeciﬁcity 1.000 0.998 0.998 0.995 0.995 0.998\nFalloutrate 0.006 0.005 0.005 0.005 0.005 0.007\nPrecision 0.071 0.286 0.286 0.286 0.286 0.000\nRecall 1.000 0.571 0.571 0.286 0.286 0.000\nBreak-evenpoint 0.3 0.7 0.3 0.5 0.4 0.1\nAP 0.250 0.950 0.307 0.500 0.496 0.000\nF-measure 0.133 0.381 0.381 0.286 0.286 0.000\nRR 0.018 0.145 0.038 0.074 0.095 0.000\nDCG 0.721 3.974 1.987 3.203 2.371 0.000\nbpref -2.000 0.550 0.143 0.235 0.194 0.000\nbpref-10 0.727 0.563 0.395 0.256 0.232 0.000\nbpref* 0.800 0.564 0.428 0.260 0.239 0.000\nTable 2. Results for differentmeasuresfor the test case examplesh own in table 1. The columnscorrespondto the value\noftheevaluationmeasureforthe answerset Aiinthe forementionedexampleset.\n00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91\n% Examples% Positive ExamplesA1\nA2\nA3\nA4\nA5\nA6\navg\nFigure 1.Normalized Lift Curves for the test answer set.\nDash-dotted lines are the test answers ( Ai) and the solid\nline correspondstheiraverage. Performanceisas goodas\nthesteepnessofthecurvethatapproximatestopoint(0,1).\nPrecision andRecallvalues seem to be good for our\npurposes ( Recallbetter than Precision , as we had previ-\nously hypothesized), but they fail in taking into account\ntheposition(rank)ofthe correctlyretrieveditems(forin -\nstance, we cannot distinguish between A2andA3or be-\ntween A4andA5, whichhavethesame |Ra|and|Rq|).\nThePrecision-Recall Curve (ﬁgure 2) gives an idea\nabout the ranking of the items, but it does not measure\nifwehaveretrievedallthepossibleelements. Inaddition,\ntherearesomeproblemswheninterpolatinganswerswith\njust one relevant document ( A1). We also noticed someinterpolationproblemswhenthereareequallyspacedrele-\nvantdocumentsintheanswerset( A4). Also,the Precision-\nRecallCurve hastheproblemthatitisnotjustavalue,and\nthus,itisabitdifﬁculttointerpretitinsomeparticulars it-\nuations(lookingatﬁgure2,whichanswerisbetter: A3or\nA5?).\n00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91\n% Recall% PrecisionA1\nA2\nA3\nA4\nA5\nA6\navg\nFigure2.Precision-RecallCurves forthetest answerset.\nDash-dotted lines are the test answers ( Ai) and the solid\nline correspondsto theaverageofthem.\nAPseemsbetterthan Precision orRecallalone(weare\nable to distinguish between differently ranked answers),\nbut we feel that ranking matters a lot (see A2). It also\ndoes not consider if we have retrieved all possible ele-\nments( Rq).\nF-measure and other measuresobtainedby combiningPrecision andRecallalso sufferfromtheirdrawbacks.\nRegarding RRandDCG, we ﬁnd again that ranking\nmattersalot. Wehaveproblemsindistinguishingbetween\nA4andA5forthe formerand between A2andA3forthe\nlatter. Thesemeasuresalso donotconsider |Rq|.\nIn general, we can see that we need a measure which\ncombines two different aspects: ranking and recall, as\nstated in our preliminary hypotheses. Looking at table 2,\nit would seem that we can come with such a single value\njust averaging recall and rank-based values. We decided\nthereforetoimplementsomemeasurescombining APand\nRecallwithaweightedmeanandwiththeharmonicmean\n(in anF-measure fashion). We also tried with RRand\nRecalland with DCGandRecall. These new measures\nworkwell forseveraltest sets, but,intheend, bpref-10or\nbpref∗ﬁt betterwithourexpectancies.\nBpref∗stands for a variant of bpref[2] that seems to\nperform well for practically all the answer sets we have\ntested. When testing bpref,we foundoneof theproblems\nmentioned in the above reference for A1(the number of\nrelevant documents being very small), but we solved it\nwith the variant mentioned in the same article ( bpref-10)\nand also with a new one ( bpref∗). This last leads to quite\nsatisfactory results. The formulation of bpref∗is similar\ntobpref-10:\nbpref∗=1\n|Rq||Ra|/summationdisplay\nj=1/parenleftbigg\n1−Nnr(j)\n|A|+|Rq|/parenrightbigg\n(1)\nWhere |Rq|is the total number of relevant documents\ninthecollection, |Ra|isthenumberofrelevantdocuments\nin the answer set, and Nnr(j)is the number of judged\nnon-relevantdocumentsrankedbeforethe j-threlevantre-\ntrieveddocumentinthe orderedset Ra.\nIfweconsiderthefactthat bprefvariantshavetheaddi-\ntionalpropertyofdealingwithunjudgedinformation(not\nlabelledelements),whichallowsus,forinstance,tointro -\nduce outliers to our database without affecting our evalu-\nation measure [2], bpref∗becomes our choice of prefer-\nence.\n4 CONCLUSIONS\nWe have presented a particular MIR system focused on\nCover Song Identiﬁcation and discussed the suitability of\ndifferent measures for evaluating it. Even though a com-\nplete formal analysis has not been presented, the discus-\nsionhasguidedustoassessthe prosandconsofdifferent\nmeasures, and to select those that seem to be more suit-\nabletoourproblem(theonestomonitorwhenperforming\nseveralimplementationsofaCoverSongIdentiﬁcational-\ngorithm).\nFurthermore, we have come with an adaptation of a\nparticularevaluationmeasure, bpref∗, which we thinkre-\nﬂects in many ways the retrieval performance we wanted\nto care about when considering the possible answer sets\nofourtest case. Moreover,wehavefoundthatusing Nor-\nmalizedLift Curves canbeveryinformativeforourtask.In a broader sense, it is very difﬁcult to tell someone\nwhich evaluation measure to use. Sometimes we take for\ngranted that people know the differences between mea-\nsures such as those we have dealt with. In some cases\nthese differencesarenotso important. Inothercases, one\nperhaps just take the measure that other people uses for\na similar purpose or a similar database. This is good for\ncomparison,butdoesnotimplythatthechosenmeasureis\ngoingtobethemostappropriateone,andapossibleprob-\nlem arises when tuning algorithms to a value that you do\nnotknowif it measuresthe “information”youwant.\nChoosing an evaluation measure strongly depends on\nthe problem you are focused, and every case has to be\nstudiedindependently.So,wehighlyencourageresearcher s\nto make such analysis and reﬂections as we have made\nherewhenfacingtheevaluationofaconcreteIRproblem.\n5 ACKNOWLEDGEMENTS\nThe author wishes to thank his colleagues at the MTG\n(UPF), specially Emilia G´ omez and Perfecto Herrera for\ntheir constant support and helpful reviews, and Vanessa\nMurdockat Yahoo! Research (Barcelona).\nThis research has been partially funded by the EU-IP\nprojectPHAROS4.\n6 REFERENCES\n[1] R. Baeza-Yates and B. Ribeiro Neto. Modern Infor-\nmationRetrieval . ACMPressBooks,1999.\n[2] C. Buckley and E. M. Voorhees. Retrieval evaluation\nwithincompleteinformation. SIGIR’04 , (27),2004.\n[3] D. P. W. Ellis and G. E. Polliner. Identifying cover\nsongs with chroma features and dynamic program-\nming beat tracking. Proc. ICASSP , April 2007. (sub-\nmitted,4pp)- Seealso MIREX’06poster.\n[4] E. G´ omez, B. S. Ong and P. Herrera. Automatic tonal\nanalysisfrommusic summariesforversionidentiﬁca-\ntion.Conv. of the Audio Engineering Society (AES) ,\nOctober2006.\n[5] C. D. Manning, R. Prabhakar and H. Schutze.\nAn introduction to Information Retrieval . Cam-\nbridge University Press, Cambridge, England,\npreliminary draft ed., 2007. Online version at\nhttp://www.informationretrieval.org (last access:\nApril2007).\n[6] M. Marolt. A mid-level melody-based representation\nforcalculatingaudiosimilarity. Proc.ISMIR ,2006.\n[7] E. M. Voorheesand L. P. Buckland. Commonevalua-\ntion measures. in Proc. of Text Retrieval Conference ,\n2006.Appendix.\n[8] N. Ye. The handbook of Data Mining . Lawrence Erl-\nbaumAssociates, 2003.\n4http://www.pharos-audiovisual-search.eu/"
    },
    {
        "title": "From Rhythm Patterns to Perceived Tempo.",
        "author": [
            "Klaus Seyerlehner",
            "Gerhard Widmer",
            "Dominik Schnitzer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418373",
        "url": "https://doi.org/10.5281/zenodo.1418373",
        "ee": "https://zenodo.org/records/1418373/files/SeyerlehnerWS07.pdf",
        "abstract": "There are many MIR applications for which we would like to be able to determine the perceived tempo of a song automatically. However, automatic tempo extraction itself is still an open problem. In general there are two tempo extraction methods, either based on the estimation of interonset intervals or based on self similarity computations. To predict a tempo the most significant time-lag or the most significant inter-onset-interval is used. We propose to use existing rhythm patterns and reformulate the tempo extraction problem in terms of a nearest neighbor classification problem. Our experiments, based on three different datasets, show that this novel approach performs at least comparably to state-of-the-art tempo extraction algorithms and could be useful to get a deeper insight into the relation between perceived tempo and rhythm patterns. 1 INTRODUCTION The tempo is a basic and highly descriptive property of a song, and it is a feature that music users perceive in an intuitive and direct way. Tempo is one of the parameters a user would love to have under control. For instance, one can imagine that depending on her mood, a user would like to be able to choose faster or slower music. Therefore the perceived tempo, the perceived speed of music, would be a perfect and highly intuitive parameter for various new interfaces to music collections. The automatic extraction of tempo information directly from digital audio signals has attracted a lot of research. Published tempo determination methods generally proceed in two stages: extracting low-level information related to apparent periodicities in the signal (we will use the generic term ‘‘rhythm patterns” for this information), and determining some assumed tempo from this information. To be useful in MIR applications, the tempo that is inferred by an algorithm should match the tempo human listeners intuitively perceive when listening to the music, not some ‘theoretical’ tempo that might be deduced from a written score of the piece. It is in this sense that we will use the term perceived tempo here, to denote the tempo that most human listeners would assign to a piece. (Of course, tempo may be perceived differently and may be c⃝2007 Austrian Computer Society (OCG). ambiguous in certain cases, but in general, there will be substantial agreement.) In the evaluation of our tempo extraction method in this paper, we will use music collections manually annotated with tempo values; we will have to assume that the annotations correspond to the tempo that most listeners would perceive. Despite a lot of literature on the subject, the human perception of rhythms, periodicity and pulsation is not yet very well understood. The same is true for the relation between rhythm and tempo perception. Compared to rhythm patterns, which we consider to be low-level features, the common approach in tempo identification is to analyse the extracted rhythm patters (even if they not usually called so – see section 2.2 below), assuming that one of the periodicities present in the rhythm patterns corresponds to the perceived tempo. This is usually determined by using some simple peak picking algorithm to predict the most salient tempo. However, the relation between rhythmic patterns and tempo perception might be more complex, and simple peak picking algorithm might not be the most appropriate choice. While a lot of scientific work has focused on onset detection and rhythm pattern extraction, there has been little effort to gain further insight into this relation between rhythm and perceived tempo to improve the tempo estimation step. We propose to view this relation as a sort of machine learning problem. We model the overall tempo extraction process as a two stage process of rhythm pattern extraction and tempo estimation, and will investigate whether the prediction of perceived tempo from rhythm patterns can be learned by the computer. 2 BASIC NOTIONS",
        "zenodo_id": 1418373,
        "dblp_key": "conf/ismir/SeyerlehnerWS07",
        "keywords": [
            "tempo",
            "perceived",
            "tempo",
            "perception",
            "automatic",
            "tempo",
            "extraction",
            "method",
            "tempo",
            "determination"
        ],
        "content": "FROM RHYTHM PATTERNS TO PERCEIVED TEMPO\nKlaus Seyerlehner1, Gerhard Widmer1,2, Dominik Schnitzer1,2\n1Department of Computational Perception\nJohannes Kepler University Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence, Vienna\nABSTRACT\nThere are many MIR applications for which we would like\nto be able to determine the perceived tempo of a song au-\ntomatically. However, automatic tempo extraction itself is\nstill an open problem. In general there are two tempo ex-\ntraction methods, either based on the estimation of inter-\nonset intervals or based on self similarity computations.\nTo predict a tempo the most signiﬁcant time-lag or the\nmost signiﬁcant inter-onset-interval is used. We propose\nto use existing rhythm patterns and reformulate the tempo\nextraction problem in terms of a nearest neighbor classi-\nﬁcation problem. Our experiments, based on three dif-\nferent datasets, show that this novel approach performs at\nleast comparably to state-of-the-art tempo extraction algo-\nrithms and could be useful to get a deeper insight into the\nrelation between perceived tempo and rhythm patterns.\n1 INTRODUCTION\nThe tempo is a basic and highly descriptive property of a\nsong, and it is a feature that music users perceive in an\nintuitive and direct way. Tempo is one of the parameters a\nuser would love to have under control. For instance, one\ncan imagine that depending on her mood, a user would\nlike to be able to choose faster or slower music. Therefore\ntheperceived tempo , the perceived speed of music, would\nbe a perfect and highly intuitive parameter for various new\ninterfaces to music collections.\nThe automatic extraction of tempo information directly\nfrom digital audio signals has attracted a lot of research.\nPublished tempo determination methods generally proceed\nin two stages: extracting low-level information related to\napparent periodicities in the signal (we will use the generic\nterm ‘‘rhythm patterns” for this information), and deter-\nmining some assumed tempo from this information. To\nbe useful in MIR applications, the tempo that is inferred\nby an algorithm should match the tempo human listen-\ners intuitively perceive when listening to the music, not\nsome ‘theoretical’ tempo that might be deduced from a\nwritten score of the piece. It is in this sense that we will\nuse the term perceived tempo here, to denote the tempo\nthat most human listeners would assign to a piece. (Of\ncourse, tempo may be perceived differently and may be\nc/circlecopyrt2007 Austrian Computer Society (OCG).ambiguous in certain cases, but in general, there will be\nsubstantial agreement.) In the evaluation of our tempo ex-\ntraction method in this paper, we will use music collec-\ntions manually annotated with tempo values; we will have\nto assume that the annotations correspond to the tempo\nthat most listeners would perceive.\nDespite a lot of literature on the subject, the human\nperception of rhythms, periodicity and pulsation is not yet\nvery well understood. The same is true for the relation be-\ntween rhythm and tempo perception. Compared to rhythm\npatterns, which we consider to be low-level features, the\nperceived tempo is a more abstract higher level feature. A\ncommon approach in tempo identiﬁcation is to analyse the\nextracted rhythm patters (even if they not usually called\nso – see section 2.2 below), assuming that one of the pe-\nriodicities present in the rhythm patterns corresponds to\nthe perceived tempo. This is usually determined by using\nsome simple peak picking algorithm to predict the most\nsalient tempo. However, the relation between rhythmic\npatterns and tempo perception might be more complex,\nand simple peak picking algorithm might not be the most\nappropriate choice. While a lot of scientiﬁc work has fo-\ncused on onset detection and rhythm pattern extraction,\nthere has been little effort to gain further insight into this\nrelation between rhythm and perceived tempo to improve\nthe tempo estimation step. We propose to view this rela-\ntion as a sort of machine learning problem. We model the\noverall tempo extraction process as a two stage process\nofrhythm pattern extraction andtempo estimation , and\nwill investigate whether the prediction of perceived tempo\nfrom rhythm patterns can be learned by the computer.\n2 BASIC NOTIONS\n2.1 Perceived Tempo\nPerceived tempo is not a well-deﬁned mathematical con-\ncept, since the perception of the tempo of a song strongly\ndepends on the listener. It is well known that a human’s\npreferred tempo range is around 100 to 120 bpm. But\nwhat makes perceived tempo estimation really difﬁcult is\nthe fact that the perceived tempo of certain pieces might\nbe ambiguous, and human subjects may indeed perceive\ndifferent tempi in the same piece, as has been conﬁrmed\nin tapping experiments [10]. Not unexpectedly, many of\nthe discrepancies in users’ tapping choices relate to differ-\nent choices of the main metrical level; thus, users’ tempojudgments often differ by a factor of two or three.\nInterestingly, it is not yet clear if ambiguities known\nfrom tapping experiments really correspond to ambigui-\nties in tempo perception. According to Chua et al. [1, 7],\nthe’Foot-tapping’ tempo is not always the same as the\nperceived tempo. This is supported by Zhu et al. [3], who\ncarried out a user study where the subjects had to deter-\nmine the tempo with respect to some reference clips. They\nﬁnally conclude that the perception biases between differ-\nent individuals are not large, so that the notion of a unique\nperceived tempo might still be practically useful.\nTo deal with the ambiguity problem, some authors have\nproposed to extract two main tempi as well as their rela-\ntive strength (e.g., [10, 5]). For a tempo descriptor that\nshould support user interaction, this is rather problematic.\nImagine a user interface having a tempo slider, such that\na user can inﬂuence the tempo by selecting another tempo\nrange. Such an interface obviously presupposes a unique\ntempo value.\nBecause of our interest in a useful tempo descriptor for\nMIR applications, we decided to focus on the estimation\nof the major orstrongest perceived tempo of a piece, the\ntempo most people would decide for. This seems reason-\nable, as many pieces have unambiguous perceived tempo.\nFor instance, DJs rely on beat databases to increase the\nspeed of the songs during an event from slow to fast music.\nIn section 4.1 we will make use of such publicly available\ntempo information from beat databases to create a repre-\nsentative ground truth dataset.\n2.2 Rhythm Patterns\nAll tempo extraction algorithm have more or less the same\ncommon structure. In a ﬁrst stage, information related to\npotential periodicities is extracted from the digital audio\nsignal ( rhythm pattern extraction process ), and in a sec-\nond ﬁnal stage a tempo is estimated from the extracted\npattern ( tempo estimation process ). For a comprehensive\noverview on tempo extraction algorithms we refer to [4],\nwho summarize the results of the ISMIR’04 tempo ex-\ntraction contest. A detailed description of various state-\nof-the art tempo extraction algorithms can also be found\non the MIREX’06 homepage1. In this section we give\na brief overview of various representations of rhythmic\nstructures one can extract from music signals. This is of\ninterest since most of the tempo extraction algorithms do\nnot explicitly generate a representation of the rhythm pat-\nterns, but instead directly try to estimate the tempo in a\nconsecutive tempo estimation block. Therefore it is often\nnot obvious how rhythm information is represented. Our\napproach, in contrast, depends on an explicit representa-\ntion of rhythm information, and a basic overview on dif-\nferent types of rhythm patterns will illustrate what types\nof patterns (or probably combinations of those) might be\nuseful.\nOne common method is to automatically detect onsets\nand then perform an analysis of inter-onset intervals (IOI).\n1http://www.music-ir.org/mirex2006For event-based approaches the detection of note onsets is\nof major importance. Dixon [6] and Gouyon et al. [13]\ngive interesting overviews of features suitable for onset\ndetection. For most of the IOI based methods an inter-\nonset histogram can be generated, which represents the\nessential rhythm information [9]. Another common ap-\nproach is to extract periodicities based on self-similarity\ncomputations. Self-similarity based approaches in general\ntry to detect periodicities by comparing the audio signal to\ndelayed versions of the original signal. This can be done\nby using a set of comb ﬁlters that cover the range of pos-\nsible tempi. The output of a comb ﬁlter bank can then\nbe interpreted as a rhythm pattern. The tempo is ﬁnally\nestimated by predicting the tempo corresponding to the\ncomb ﬁlter with the highest response. Another variant of\nthe self-similarity approach is the detection of periodici-\nties based on the Autocorrelation Function (ACF) . The\nautocorrelation measures the similarity of a signal to a de-\nlayed version of the original signal. The autocorrelation\nfunction represents this self-similarity relation for differ-\nent time lags. For music signals the peaks in the ACF\nreﬂect the occurrence of regular musical events. Thus the\nACF itself is some sort of rhythm pattern. The ACF can\nbe computed based on the time domain representation or\nin the frequency domain, on a frame basis. Foote et al.\n[2] cut the audio signal into frames, perform an FFT for\neach frame and ﬁnally derive the so-called Beat Spec-\ntrum from the self-similarity matrix of the FFT frame rep-\nresentation. Another interesting self-similarity approach\nisdetrended ﬂuctuation analysis (DFA). The DFA can\nmeasure two-point correlations and is especially suitable\nfor non-stationary signals like music. In [8] DFA is used\nto generate a rhythm pattern useful for genre classiﬁca-\ntion. Pampalk et al. [11, 12] measure the ﬂuctuations of\nthe loudness in twenty different frequency bands to cap-\nture detailed rhythm information. The Fluctuation Pat-\nterns are a common descriptor for content-bases audio\nsimilarity computations and will be used in our evalua-\ntions in section 4.\n3 REFORMULATING THE\nTEMPO ESTIMATION PROBLEM\nWithin the tempo extraction process the estimation of the\ntempo based on the extracted rhythm pattern is a crucial\nstep. A perfect rhythm pattern would capture all periodic\nelements in a piece, and their relative strengths. Estimat-\ning the tempo from a rhythm pattern means picking the\n”correct” periodicity. Which periodicity is most signiﬁ-\ncant depends on the human perception and is up to now\nnot yet fully understood. Various strategies have been im-\nplemented to pick the correct tempo, but most tempo ex-\ntraction algorithms just pick the highest peak from the beat\npattern.\nThe basic idea of our approach is based on the assump-\ntion that songs having a similar rhythmic structure are\nlikely to have a similar (perceived) tempo as well. Thus\nif one has got a set of rhythm patterns manually annotatedFigure 1 . Histograms of ground truth tempo values in 5 bpm steps for the ballroom ,songs andpopdataset.\nwith the correct perceived tempo, the mapping of an un-\nseen rhythm pattern to a perceived tempo can be realized\njust by looking for similar rhythm patterns in the anno-\ntated training set. Using the Nmost similar rhythm pat-\nterns from the annotated training set, we can predict the\nperceived tempo by predicting the most frequent tempo\nof these Nrhythm patterns ( nearest neighbor classiﬁca-\ntion). Thus, we learn the mapping from rhythm patterns\nto perceived tempo from a ground truth dataset using an\ninstance-based machine learning approach.\nWhile this approach is straightforward, we still have\nto deﬁne a distance metric that measures how similar two\nrhythm patterns are. For our experiments we decided to\nuse the correlation coefﬁcient. For two rhythm patterns /vector x\nand/vector ythe correlation is given by equation (1).\nr=sxy\nsxsy=/summationtextn\ni=1(xi−¯x)(yi−¯y)/radicalbig/summationtextn\ni=0(xi−ˆx)2/summationtextn\ni=0(yi−ˆy)2(1)\nTwo different types of rhythm patterns are used in our\nexperiments, the Autocorrelation Function (ACF) and the\nFluctuation Patterns (FPs) .\n•Autocorrelation Function (ACF)\nIn our experiment, the Autocorrelation Function is\ncomputed from a log-magnitude 40-channel Mel-\nfrequency spectrogram for an 8 kHz down-sampled\nmono version of the original audio signal with a 32\nms window and 4 ms hop between frames, exactly\nas described in [14]. For each frequency channel\nthe ﬁrst-order difference is half-wave rectiﬁed and\nﬁnally summed across the frequency bands. Af-\nter high-pass ﬁltering the resulting onset signal to\nremove the d.c. offset the signal is autocorrelated\nout to a maximum lag of 4 seconds. Finally we\nsmoothen the ACF using an average ﬁlter with a\nwindow size of 20, because the ACF can be ex-\ntremely ”spiky” and even pretty similar rhythm pat-\nterns could be judged as rather dissimilar by our dis-\ntance measure. The resulting autocorrelation func-\ntion is then used as a representation of the periodic-\nities in the original audio signal.\n•Fluctuation Patterns (FP)\nThe Fluctuation Pattern of a songs describes the am-\nplitude modulation of the loudness for 20 frequencybands, spaced according to the Bark scale. The\nFPs were originally designed for rhythm-based au-\ndio similarity computations and were never expected\nto be useful for automatic tempo extraction. Our\nimplementation is based directly on [11] and [12].\nThe only modiﬁcation we apply is to reduce the\nrhythmic information captured for 20 different fre-\nquency bands to just one by simply summing across\nthe bands. This reduces the dimensionality from\n1200 to only 60 dimensions, which is expected to be\nbeneﬁcial for the nearest-neighbor classiﬁer to be\nused in our experiments — nearest-neighbor meth-\nods are notorious for their sensitivity to high-dimen-\nsional feature spaces.\nIn the next section we report on our ground truth data\nand the results obtained from our experiments.\n4 EXPERIMENTS\n4.1 Ground truth data\nOur results are based on three different datasets. The ﬁrst\ntwo datasets have been used in [4] for tempo extraction\nevaluations and should help make the results of our ex-\nperiments more comparable to previous work. The ball-\nroom dataset is a well known dance music collection from\nBallroomDancers.com and consists of 698 audio excerpts\n— each of about 30 seconds of playing time. The songs\ndataset, also used in [4], is publicly available2. This data-\nset contains 465 audio clips (20 seconds each) from nine\ngenres (Rock, Classic, Electronica, Latin, Samba, Jazz,\nAfroBeat, Flamenco, Balkan and Greek).\nThe third dataset, pop, was generated by ourselves, by\ncrawling publicly available beat databases from the web\nand retrieving the tempo information for thousands of pop-\nular songs. The authors’ private collections were then\nsearched for songs for which the tempo information was\nnow available, using a approximative string matching al-\ngorithm. The resulting dataset was checked manually for\nplausibility. The ﬁnal set consists of 545 full length songs\nbelonging to various different popular genres. The dis-\ntributions of the annotated tempi of the test databases are\nshown in Figure 1.\n2http://www.iua.upf.es/mtg/ismir2004/contest/tempoContest/Figure 2 . Comparative experimental results on ballroom andsongs datasets.\nFigure 3 . Visualisation of errors of S2 on popdataset, sorted in ascending order according to ground truth tempo. Top\npanel: prediction errors. Bottom panel: ground truth tempo.\n4.2 Learning and evaluation methods\nTo estimate the tempo for a given song from one of our\ndatasets, we use a simple k-nearest-neighbor ( k-NN) clas-\nsiﬁer, which searches for the kmost similar songs in the\ndatabase and predicts the tempo that appears most fre-\nquently within these ksongs. In our experiments, we used\nk= 5. Evaluating the method on a whole music collec-\ntion is done via a strategy known as leave-one-out cross\nvalidation: the tempo estimate of one single audio excerpt\nis computed by using all other examples from the dataset\n(except the audio excerpt we estimate the tempo of) as\ntraining examples. Since there are no duplicates in the\ndatasets, there is no general advantage of this approach\ncompared to the tempo estimation algorithms evaluated in\n[4].\nTo make the results comparable to [4], we deﬁne the\ntempo estimation to be correct if the predicted tempo esti-\nmate is within 4% (the precision window) of the ground-\ntruth tempo. For visualizing the errors, (see Fig. 3) we\nintroduce a slightly modiﬁed error measure compared to\n[4], which is deﬁned in equation (2), where tdenotes the\nground truth tempo and ˆtthe estimated tempo of a piece.\nCompared to the error measure in [4], which is based on\nthe logarithm, incorrect tempo estimates exceeding half\nor double the ground truth tempo will be judged in a more\nlinear way.e=/braceleftBigg\nˆt\nt−1 ˆt > t\n−(t\nˆt−1) ˆt <=t(2)\nCorrect or almost correct estimates will yield an error\nvalue close to zero, whereas tempo estimates double or\nhalf the ground truth value are considered to be equally er-\nroneous. Positive values indicate that the tempo extraction\nalgorithm predicts too fast a tempo, while negative values\nindicate that the predicted tempo is too slow. In particular,\npredicting twice the correct tempo gives an error of 1.0,\npredicting half the tempo gives −1.0.\n4.3 Results\nThe results obtained from the datasets ballroom andsongs\ncan easily be compared to the results obtained in [4]. The\ndetailed evaluation results for eleven algorithms from six\ndifferent participants in the ISMIR’04 tempo induction\ncontest, namely Miguel Alonso (A1, A2), Simon Dixon\n(D1, D2, D3), Anssi Klapuri (KL), Eric Scheirer (SC),\nGeorge Tzanetakis (T1, T2, T3) and Christian Uhle (UH)\ncan be found on the web3. We will denote our own al-\ngorithms as S1 (the NN approach using Fluctuation Pat-\nterns ) and S2 (NN prediction based on the Autocorrelation\nFunction ), respectively. Figure 2 illustrates the results for\n3http://www.iua.upf.es/mtg/ismir2004/contest/tempoContestFigure 4 . Visualization of the relation of annotated tempo and the Fluctuation Patterns for the ballroom dataset.\nFigure 5 . Visualization of the relation of annotated tempo and the Autocorrelation Function for the popdataset (ACF\ndata binarized to enhance visibility of patterns).\ntheballroom dataset and for the songs dataset. For the\nballroom dataset the k-NN approach clearly outperforms\nthe other tempo extraction algorithms. S1 achieves an ac-\ncuracy of 78.51% and S2 an accuracy of 73.78% . For the\nsongs dataset we obtain accuracies of 40.86% for S1 and\n60.43% for S2, which amounts to rank 4 (S1) and rank 1\n(S2) in this comparison, respectively. Thus, we may con-\nclude that our learning approach performs roughly at the\nsame level as the best current tempo identiﬁcation algo-\nrithms, at least on these two data sets.\nOn our own data set popwe obtain a classiﬁcation ac-\ncuracy of 68.8% for S1 and 74.5% for S2. Since no\ncomparision is possible for our own dataset, Figure 3 just\nillustrates the estimation errors of S2. As can be seen,\nthe errors are almost exclusively due a commitment to the\n‘wrong’ metrical level — the errors are either 1.0or−1.0.\nAlso, the errors occur mainly in those parts of the music\ncollection where there are extreme tempi and (thus) few\nsimilar pieces (i.e., pieces with similar rhythm patterns).\nFinally, joining all three datasets into one large set of\nmore diverse styles and running our algorithms on this set\ngives accuracies of 64.06% for S1, and 68.91% for S2,\nwhich shows that the good performance on the individual\nsets is not just due to the narrow stylistic range of the sets.Our tempo estimation NN-approach is based on the as-\nsumption that songs having similar rhythm patterns tend\nto have the same perceived tempo. To check if this is a\nreasonable assumption, we can sort all the rhythm patterns\naccording to the annotated ground truth tempo and visual-\nize the result. Figure 4 shows the Fluctuation Patterns for\nall the 698 instances of the ballroom dataset. Each column\nrepresents the FP of the corresponding audio excerpt. The\ntempo curve in ﬁgure 4 indicates the increasing tempo for\neach sample from the dataset. One can visually observe\nthat audio clips of similar annotated tempo tend to have\nsimilar rhythm patterns. Figure 5 visualizes the Autocor-\nrelation Function for the popdataset. The patterns of the\nACF seem to change smoothly as the annotated tempo in-\ncreases. In ﬁgure 5, instance 251 is marked. This instance\nis rather dissimilar compared to its neighbors. A further\ninvestigation of the corresponding song Five - When The\nLights Go Out led to the conclusion that the ground truth\nannotation of this song ( 104 bpm ) is incorrect and should\nbe changed to about 140 bpm . This case illustrates that\none can even visually explore mistakes in the ground truth\nannotation based on this representation.4\n4The outlier is not well visible in the printed version of Figure 5 – not\neven in the enlarged excerpt –, but it clearly sticks out in the coloured5 CONCLUSIONS\nThe reformulation of the tempo estimation step in terms\nof a nearest neighbor classiﬁcation problem permits to\nlearn the relation of rhythm patterns and perceived tempo.\nThus, we can make the implicit tempo information from\nrhythm patterns explicit, such that it can be used in form of\na tempo descriptor for MIR applications. We have demon-\nstrated this both for the Autocorrelation Function and for\nFluctuation Patterns. Experimental results based on three\ndifferent datasets indicate that this approach performs at\nleast equally well as other state-of-the-art tempo identiﬁ-\ncation methods.\nA visualization of the datasets supports our basic as-\nsumption that songs with similar rhythm patterns tend to\nhave the same perceived tempo. The proposed visual-\nization of rhythm patterns ordered according to the asso-\nciated perceived tempo reveals interesting structures and\nmight be a useful representation to get some more insight\nin the relation of rhythm patterns and perceived tempo.\nAlso, possible annotation failures in the ground truth data\ncan be visually explored using the proposed representa-\ntion.\nAs with each instance based learning approach, the ef-\nfectiveness of the tempo extraction strongly depends on\nthe training instances, the similarity measure, and the abil-\nity of the rhythm patterns to capture periodicity informa-\ntion. To be useful in terms of a general tempo extrac-\ntion algorithm a large annotated training set is needed that\ncovers various tempo ranges and various genres and musi-\ncal styles. This may be a limiting factor in practical MIR\napplications. We do, however, hope that the kind of ap-\nproach proposed here, if investigated further, could help\nin getting deeper insights in the relation of rhythm and\ntempo perception.\n6 ACKNOWLEDGMENTS\nThis research was supported by the Austrian Fonds zur\nF¨orderung der Wissenschaftlichen Forschung (FWF) un-\nder grant L112-N04.\n7 REFERENCES\n[1] Chua, B.Y. Ku, G. ”Improved Perceptual Tempo De-\ntection of Music”, Proceedings of the 11th Interna-\ntional Multimedia Modelling Conference (MMM’05) ,\nMelbourne, Australien, 2005.\n[2] Foote, J. Uchihashi, S. ”The Beat Spectrum: A new\nApproach to Rhythm Analysis”, Proceedings of the\nIEEE International Conference on Multimedia Expo\n(ICME’01) , Tokyo, Japan, 2001.\n[3] Zhu, J. Lu, L. ”Perceptual Visualization of A Music\nCollection”, Proceedings of the IEEE International\nConference on Multimedia Expo (ICME’05) , Amster-\ndam, The Netherlands, 2005.\ncomputer display.[4] Gouyon, F. Klapuri, A. Dixon, S. Alonso, M. Tzane-\ntakis, G. Uhle, C. Cano, P. ”An experimental com-\nparison of audio tempo induction algorithms”, IEEE\nTransactions on Audio, Speech and Language Pro-\ncessing , vol. 14, no. 5, pp. 1832-1844, 2006.\n[5] McKinney, M.F. Moelants, D. ”Deviations from the\nresonance theory of tempo induction”, Proceedings\nof the Conference on Interdisciplinary Musicology\n(CIM’04) , Graz, Austria, 2004.\n[6] Dixon, S. ”Onset Detection Revisited”, Proceedings\nof the International Conference on Digital Audio Ef-\nfects (DAFx’06) , Montreal, Canada, 2006.\n[7] Chua, B.Y. Lu, G. ”Determination of Perceptual\nTempo of Music”, Lecture Notes in Computer Science ,\nvol. 3310, pp. 61-70, 2005.\n[8] Jennings, H.D. Ivanov, P.C. Martins, A.M. Silva, P.C.\nViswanathan, G.M. ”Variance ﬂuctuations in nonsta-\ntionary time series: a comparative study of music gen-\nres”, Physica A: Statistical and Theoretical Physics ,\nvol. 336, Issue 3-4, pp. 585-594, 2004.\n[9] McKinney, M.F. Moelants, D. ”Extracting the Percep-\ntual Tempo From Music”, Proceedings of the Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’04) , Barcelona, Spain, 2004.\n[10] McKinney, M.F. Moelants, D. ”Tempo Perception and\nMusical Content: What makes a piece fast, slow or\ntemporally ambiguous?”, Proceedings of the Interna-\ntional Conference on Music Perception and Cognition\n(ICMPC’04) , Evanston, USA, 2004.\n[11] Pampalk, E. ”Islands of Music: Analysis, Organiza-\ntion, and Visualization of Music Archives”, MSc The-\nsis, Technical University of Vienna, 2001.\n[12] Pampalk, E. Rauber, A. Merkl, D. ”Content-based or-\nganization and visualization of music archives”, Pro-\nceedings of the 10th ACM International Conference on\nMultimedia , Juan les Pins, France, pp. 570-579, 2002.\n[13] Gouyon F., Widmer G., Serra X., Flexer A. ”Acoustic\nCues to Beat Induction: A Machine Learning Perspec-\ntive”, Music Perception , vol. 24, Issue 2, pp. 177-188,\n2006.\n[14] Ellis, D.P.W. ”Beat Tracking with Dynamic Program-\nming”, Music Information Retrieval Evaluation eX-\nchange (MIREX’06) , 2006."
    },
    {
        "title": "A Demonstrator for Automatic Music Mood Estimation.",
        "author": [
            "Janto Skowronek",
            "Martin F. McKinney",
            "Steven van de Par"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417669",
        "url": "https://doi.org/10.5281/zenodo.1417669",
        "ee": "https://zenodo.org/records/1417669/files/SkowronekMP07.pdf",
        "abstract": "Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music’s emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic predicition of music mood is possible. 1 INTRODUCTION In the development of an automatic music mood classifier, we must treat the high degree of subjectivity associated with mood. One can do this by developing personalized models requiring comprehensive user feedback for training, or one can develop generalized models by minimizing the subjectivity involved in the evaluation of music mood. Here we apply the latter approach. A first step in minimizing subjectivity is to deliberately define the goal of the classification algorithm: it should model how people would describe the mood expressed in the music (affect attribution) and not how they would actually feel when they are listening to the music (affect induction). See also [1] for a detailed discussion on affect attribution and induction. A second step is to define mood classes on which users show relative agreement when applying them to music. In addition from an application point of view the mood classes should be easy to use and important to the users. Our previous work [8] reported on the identification of those consistent-, importantand easy-mood classes; some issues will be recapitulated in Section 2. As a third step the ground-truth database used for training and testing the desired classifier should contain only music that people can clearly and consistently assign to or exclude from a mood category. This issue will be discussed in more detail in Sections 3 and 4. c⃝2007 Austrian Computer Society (OCG). 2 CLASS DEFINITION In previous work [8] we searched for mood classes that enable the development of the above mentioned generalized mood classification models. The basic idea was to ask subjects to evaluate music excerpts using 33 candidate labels used in literature (e.g. [1, 6]), followed by a data analysis in order to identify those mood classes on which subjects show a certain agreement and which subjects assessed as important and easy to use. As a measure for across subject consistency we computed the Cronbach’s coefficient α and as a measure for the importance and easiness of the classes we used questionnaires. In [8] we identified 8 mood classes that fulfilled our criteria (α ≥0.7, at least important, at least easy to use). A more detailed analysis suggested a slight modification to that selection of mood classes. First, some mood classes that were highly appreciated in the questionnaires got an α slightly below 0.7. Therefore we lowered our threshold to 0.65. Second, there were very high correlations between some classes, whose meaning of the class names (adjectives) were quite close. Thus we decided to merge them. Third, we loosened the criteria importance and easiness due to the small number of subjects (10): a mood class was selected if at least one of the two criteria was fulfilled. Hence the new set of mood classes comprised 12 categories: arousing-awakening, angry-furiousagressive, calming-soothing, carefree-lighthearted-lightplayful, cheerful-festive, emotional-passionate-touchingmoving, sad, loving-romantic, powerful-strong, restlessjittery-nervous, peaceful, tender-soft. 3 MATERIAL COLLECTION In a follow-up experiment subjects labelled a large number of music excerpts using the 12 identified mood classes. The idea was to select those excerpts that got a clear and consistent rating by the subjects to compile a ground-truth database for training and testing a mood classification algorithm. In our previous experiment [8] many excerpts did not fulfill our criteria for conveying a clear mood. Hence the new set-up aimed at obtaining a large number of labelled excerpts to ensure a sufficient number of training material, resulting in compromises with respect to the number of involved subjects and an unbalanced distribution of excerpts across music genres. As in the first experiment, we selected the excerpts such that their moods are likely to be constant by avoiding drastic changes in the music (structure, tempo, instrumentation etc.) We distributed 1059 excerpts from 12 music genres across 12 subjects such that each excerpt was rated by 6 subjects; each subject had to rate 530 excerpts and the sets per subject were mutually overlapping. The subjects were students and employees working at our laboratory(age between 23 and 40, 8 different nationalities, musical practise from 0 to 21 years). Three of them participated already in the previous experiment. We collected the subjective judgments on a 4-point scale: not, slightly, moderately, definitively that mood. 4 CLASSIFICATION ALGORITHM In contrast to the mood classes defined by Lu et al. [2], our mood classes are not mutually exclusive. Consequently, we implemented an individual detector for each mood class (binary decision: 1 = that class, 2 = not that class). For that purpose we had to decide for each excerpt and each mood category, whether the excerpt belongs to class 1 or class 2 or whether it should be excluded. An excerpt was accepted if it was relatively consistently rated across subjects (standard deviation ≤1 point on the scale) and if its mean rating was not in an area of ambiguity, which we defined between not that mood and slightly that mood. Depending on the mood label between 5% (sad) and 20% (restless) of the 1059 excerpts were accepted for class 1, between 27% (emotional) and 66% (angry) for class 2. While the consistency criterion was often fulfilled, the unambiguity criterion excluded most of the excerpts. The used feature extraction algorithm computes every 743ms a feature vector comprising 4 general feature types: basic signal describing features and their temporal modulation [3], perceptually relevant tempo and rhythm based features [4], features based on chroma and key information [5] as well as features that evaluate the occurences of percussive sound events in the music [7]. The classification stage is based on quadratic discriminant analysis and used a randomized 80/20 split of the training and test data in combination with bootstrap repetitions in order to estimated the classification performance. Results are shown in Table 1. While some classes show about 75-80 % correct classifciation (e.g. carefree, loving), the performance of other classes (e.g. angry, calming) is with about 90% rather good. In general these results show that an automatic estimation of music mood, as we defined it in Section 1, is possible. 5 CONCLUSIONS The aim of this work is the development of a music mood classification algorithm. The applied procedure of groundtruth database design was intended to minimize the subjectivity that is involved in the perception of music mood. Under these constraints the achieved classification results as well as the behavior of our real-time demonstrator show that music mood estimation is to a certain extend possible. While a subjective evaluation of the classification alMood Class Performance arousing-awakening 85.1 ± 1.7 % angry-furious-agressive 90.1 ± 1.6 % calming-soothing 90.9 ± 2.4 % carefree-lighthearted-light-playful 77.1 ± 1.7 % cheerful-festive 79.8 ± 1.9 % emotional-passionate-touching-moving 82.2 ± 1.1 % loving-romantic 80.2 ± 1.7 % peaceful 88.5 ± 1.7 % powerful-strong 80.7 ± 1.8 % sad 85.0 ± 1.8 % restless-jittery-nervous 90.6 ± 1.5 % tender-soft 89.5 ± 0.6 % Table 1. Classification performance (mean ± standard error across bootstrap repetitions) of the individual mood detectors. gorithm on abitrary music is currently being performed, open issues for future research are to further investigate why so many excerpts seem to be ambiguous and how to deal with such music pieces. 6 REFERENCES [1] M. Leman, V. Vermeulen, L. De Voogdt, D. Moelants, M. Lesaffre, Prediction of Musical Affect Using a Combination of Acoustic Structural Cues, J. of New Music Research, Vol. 34(1), 39-67, 2005. [2] L. Lu, D. Liu, H. Zhang, Automatic Mood Detection and Tracking of Music Audio Signals, IEEE transactions on audio, speech, and language processing, Vol. 14(1), 5-18, 2006. [3] M. McKinney, J. Breebart, Features for Audio Music Classification, Proceedings of 4th International Conference on Music Information Retrieval, 2003. [4] M. McKinney, D. Moelants, Extracting the perceptual tempo from music audio, Proceedings of 5th International Conference on Music Information Retrieval, Barcelona, 2004. [5] S. van de Par, M. McKinney, A. Redert, Musical Key Extraction from Audio using Profile Training, Proceedings of 7th International Conference on Music Information Retrieval, Victoria, 2006. [6] J. Russell, A circumplex model of affect, J. Personality & Social Psychology, Vol. 39, 1161-1178, 1980. [7] J. Skowronek, M. McKinney, Features for audio classification: Percussiveness of sounds, Intelligent Algorithms in Ambient and Biomedical Computing, Philips research Book Series Vol. 7, Springer, 2006. [8] J. Skowronek, S. van de Par, M. McKinney, Groundtruth for automatic music mood classification, Proceedings of 7th International Conference on Music Information Retrieval, Victoria, 2006.",
        "zenodo_id": 1417669,
        "dblp_key": "conf/ismir/SkowronekMP07",
        "content": "A DEMONSTRATOR FOR AUTOMATIC MUSIC MOOD ESTIMATION\nJanto Skowronek, Martin McKinney, Steven van de Par\nPhilips Research Laboratories\nHightech Campus 36, 5656 AE Eindhoven, The Netherlands\nABSTRACT\nInterestinautomaticmusicmoodclassiﬁcationisincreas-\ning because it could enable people to browse and man-\nage their music collections by means of the music’s emo-\ntional expression complementary to the widely used mu-\nsic genres. We continue our work on designing a well de-\nﬁnedground-truthdatabaseformusicmoodclassiﬁcation\nand show a demonstrator of automatic mood estimation.\nWhile a subjective evaluation of this algorithm on arbi-\ntrarymusicisongoing,theinitialclassiﬁcationresultsare\nencouraging and suggest that an automatic predicition of\nmusic mood is possible.\n1 INTRODUCTION\nInthedevelopmentofanautomaticmusicmoodclassiﬁer,\nwe must treat the high degree of subjectivity associated\nwith mood. One can do this by developing personalized\nmodels requiring comprehensive user feedback for train-\ning,oronecandevelopgeneralizedmodelsbyminimizing\nthesubjectivityinvolvedintheevaluationofmusicmood.\nHere we apply the latter approach. A ﬁrst step in min-\nimizing subjectivity is to deliberately deﬁne the goal of\nthe classiﬁcation algorithm: it should model how people\nwould describe the mood expressed in the music ( affect\nattribution ) and not how they would actually feel when\nthey are listening to the music ( affect induction ). See also\n[1] for a detailed discussion on affect attribution and in-\nduction. Asecondstepistodeﬁnemoodclassesonwhich\nusersshowrelativeagreementwhenapplyingthemtomu-\nsic. In addition - from an application point of view - the\nmood classes should be easy to use andimportant to the\nusers. Ourpreviouswork[8]reportedontheidentiﬁcation\nof those consistent-, important- and easy-mood classes;\nsome issues will be recapitulated in Section 2. As a third\nstep the ground-truth database used for training and test-\ning the desired classiﬁer should contain only music that\npeople can clearly and consistently assign to or exclude\nfrom a mood category. This issue will be discussed in\nmore detail in Sections 3 and 4.\nc/circlecopyrt2007 Austrian Computer Society (OCG).2 CLASS DEFINITION\nIn previous work [8] we searched for mood classes that\nenable the development of the above mentioned general-\nized mood classiﬁcation models. The basic idea was to\nasksubjectstoevaluatemusicexcerptsusing33candidate\nlabels used in literature (e.g. [1, 6]), followed by a data\nanalysis in order to identify those mood classes on which\nsubjects show a certain agreement and which subjects as-\nsessed as important andeasy to use . As a measure for\nacross subject consistency we computed the Cronbach’s\ncoefﬁcient αandasameasurefortheimportanceandeasi-\nnessoftheclassesweusedquestionnaires. In[8]weiden-\ntiﬁed8moodclassesthatfulﬁlledourcriteria( α≥0.7,at\nleastimportant , at leasteasy to use ).\nA more detailed analysis suggested a slight modiﬁca-\ntion to that selection of mood classes. First, some mood\nclasses that were highly appreciated in the questionnaires\ngot an αslightly below 0.7. Therefore we lowered our\nthreshold to 0.65. Second, there were very high correla-\ntions between some classes, whose meaning of the class\nnames (adjectives) were quite close. Thus we decided to\nmerge them. Third, we loosened the criteria importance\nandeasinessdue to the small number of subjects (10): a\nmood class was selected if at least one of the two criteria\nwas fulﬁlled. Hence the new set of mood classes com-\nprised 12 categories: arousing-awakening, angry-furious-\nagressive, calming-soothing, carefree-lighthearted-light-\nplayful, cheerful-festive, emotional-passionate-touching-\nmoving, sad, loving-romantic, powerful-strong, restless-\njittery-nervous, peaceful, tender-soft.\n3 MATERIAL COLLECTION\nIn a follow-up experiment subjects labelled a large num-\nberofmusicexcerptsusingthe12identiﬁedmoodclasses.\nThe idea was to select those excerpts that got a clear and\nconsistentratingbythesubjectstocompileaground-truth\ndatabase for training and testing a mood classiﬁcation al-\ngorithm. In our previous experiment [8] many excerpts\ndid not fulﬁll our criteria for conveying a clear mood.\nHence the new set-up aimed at obtaining a large number\noflabelledexcerptstoensureasufﬁcientnumberoftrain-\ning material, resulting in compromises with respect to the\nnumber of involved subjects and an unbalanced distribu-\ntion of excerpts across musicgenres.\nAs in the ﬁrst experiment, we selected the excerpts\nsuch that their moods are likely to be constant by avoid-ing drastic changes in the music (structure, tempo, instru-\nmentationetc.) Wedistributed1059excerptsfrom12mu-\nsic genres across 12 subjects such that each excerpt was\nrated by 6 subjects; each subject had to rate 530 excerpts\nand the sets per subject were mutually overlapping. The\nsubjectswerestudentsandemployeesworkingatourlab-\noratory(age between 23 and 40, 8 different nationalities,\nmusical practise from 0 to 21 years). Three of them par-\nticipated already in the previous experiment.\nWecollectedthesubjectivejudgmentsona4-pointscale:\nnot,slightly,moderately ,deﬁnitively that mood .\n4 CLASSIFICATION ALGORITHM\nIncontrasttothemoodclassesdeﬁnedbyLuetal. [2],our\nmood classes are not mutually exclusive. Consequently,\nweimplementedanindividualdetectorforeachmoodclass\n(binary decision: 1 = that class , 2 =not that class ). For\nthat purpose we had to decide for each excerpt and each\nmood category, whether the excerpt belongs to class 1 or\nclass 2 or whether it should be excluded. An excerpt was\naccepted if it was relatively consistently rated across sub-\njects (standard deviation ≤1point on the scale) and if\nits mean rating was not in an area of ambiguity, which\nwedeﬁnedbetween notthatmood andslightlythatmood .\nDepending on the mood label between 5% (sad) and 20%\n(restless) of the 1059 excerpts were accepted for class 1,\nbetween 27% (emotional) and 66% (angry) for class 2.\nWhiletheconsistencycriterionwasoftenfulﬁlled,theun-\nambiguity criterion excluded most of the excerpts.\nThe used feature extraction algorithm computes every\n743msafeaturevectorcomprising4generalfeaturetypes:\nbasic signal describing features and their temporal modu-\nlation [3], perceptually relevant tempo and rhythm based\nfeatures [4], features based on chroma and key informa-\ntion[5]as wellasfeaturesthatevaluate theoccurencesof\npercussive sound events in the music [7].\nThe classiﬁcation stage is based on quadratic discrim-\ninant analysis and used a randomized 80/20 split of the\ntrainingandtestdataincombinationwithbootstraprepeti-\ntions in order to estimated the classiﬁcation performance.\nResults are shown in Table 1. While some classes show\nabout 75-80 % correct classifciation (e.g. carefree, lov-\ning), the performance of other classes (e.g. angry, calm-\ning) is with about 90% rather good. In general these re-\nsultsshowthatanautomaticestimationofmusicmood,as\nwe deﬁned it in Section 1, ispossible.\n5 CONCLUSIONS\nTheaimofthisworkisthedevelopmentofamusicmood\nclassiﬁcationalgorithm. Theappliedprocedureofground-\ntruth database design was intended to minimize the sub-\njectivitythatisinvolvedintheperceptionofmusicmood.\nUnder these constraints the achieved classiﬁcation results\naswellasthebehaviorofourreal-timedemonstratorshow\nthat music mood estimation is to a certain extend possi-\nble. While a subjective evaluation of the classiﬁcation al-Mood Class Performance\narousing-awakening 85.1±1.7 %\nangry-furious-agressive 90.1±1.6 %\ncalming-soothing 90.9±2.4 %\ncarefree-lighthearted-light-playful 77.1±1.7 %\ncheerful-festive 79.8±1.9 %\nemotional-passionate-touching-moving 82.2±1.1 %\nloving-romantic 80.2±1.7 %\npeaceful 88.5±1.7 %\npowerful-strong 80.7±1.8 %\nsad 85.0±1.8 %\nrestless-jittery-nervous 90.6±1.5 %\ntender-soft 89.5±0.6 %\nTable1. Classiﬁcationperformance(mean ±standarder-\nror across bootstrap repetitions) of the individual mood\ndetectors.\ngorithm on abitrary music is currently being performed,\nopen issues for future research are to further investigate\nwhy so many excerpts seem to be ambiguous and how to\ndeal with such music pieces.\n6 REFERENCES\n[1] M. Leman, V. Vermeulen, L. De Voogdt, D. Moe-\nlants, M. Lesaffre, Prediction of Musical Affect Using\naCombinationofAcousticStructuralCues ,J.ofNew\nMusic Research, Vol. 34(1), 39-67, 2005.\n[2] L. Lu, D. Liu, H. Zhang, Automatic Mood Detection\nand Tracking of Music Audio Signals , IEEE transac-\ntions on audio, speech, and language processing, Vol.\n14(1), 5-18, 2006.\n[3] M. McKinney, J. Breebart, Features for Audio Music\nClassiﬁcation , Proceedings of 4th International Con-\nference on Music InformationRetrieval, 2003.\n[4] M. McKinney, D. Moelants, Extracting the percep-\ntualtempofrommusicaudio ,Proceedingsof5thInter-\nnational Conference on Music Information Retrieval,\nBarcelona, 2004.\n[5] S. van de Par, M. McKinney, A. Redert, Musical Key\nExtraction from Audio using Proﬁle Training , Pro-\nceedingsof7thInternationalConferenceonMusicIn-\nformation Retrieval, Victoria, 2006.\n[6] J.Russell, Acircumplexmodelofaffect ,J.Personality\n& Social Psychology, Vol. 39, 1161-1178, 1980.\n[7] J.Skowronek,M.McKinney, Featuresforaudioclas-\nsiﬁcation: Percussiveness of sounds , Intelligent Al-\ngorithms in Ambient and Biomedical Computing,\nPhilips research Book SeriesVol. 7, Springer, 2006.\n[8] J. Skowronek, S. van de Par, M. McKinney,\nGroundtruth for automatic music mood classiﬁcation ,\nProceedingsof7thInternationalConferenceonMusic\nInformation Retrieval, Victoria, 2006."
    },
    {
        "title": "Similarity Based on Rating Data.",
        "author": [
            "Malcolm Slaney",
            "William White"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416402",
        "url": "https://doi.org/10.5281/zenodo.1416402",
        "ee": "https://zenodo.org/records/1416402/files/SlaneyW07.pdf",
        "abstract": "This paper describes an algorithm to measure the similarity of two multimedia objects, such as songs or movies, using users’ preferences. Much of the previous work on query-by-example (QBE) or music similarity uses detailed analysis of the object’s content. This is difficult and it is often impossible to capture how consumers react to the music. We argue that a large collection of user’s preferences is more accurate, at least in comparison to our benchmark system, at finding similar songs. We describe an algorithm based the song’s rating data, and show how this approach works by measuring its performance using an objective metric based on whether the same artist performed both songs. Our similarity results are based on 1.5 million musical judgments by 380,000 users. We test our system by generating playlists using a content-based system, our rating-based system, and a random list of songs. Music listeners greatly preferred the ratings-based playlists over the content-based and random playlists. 1 INTRODUCTION This paper describes a means to evaluate the similarity of two multimedia objects using users’ stated preference or rating data about items in the collection. We use a large music collection to illustrate this work, but the same idea applies to any collection of objects where many users report their preference of an object. Most work on similarity uses content-based algorithms. A specialized algorithm looks at the content (usually music) calculates various musically-inspired measures of the sound to form a feature vector, and then compares two features vectors to make a decision about similarity [1]. This similarity measurement is the heart of conventional queryby-example (QBE) systems, such as QBIC [2]. But, even measuring similarity with human raters is difficult [4]. Our work ignores the content. Instead we look at a large group of users and ask if these users, with a wide range of personal musical interests, rate two pieces of music in the same manner. If jazz, classical, blues, and hiphop lovers all rate two songs in the same way, whether c⃝2007 Austrian Computer Society (OCG). they hate it or love it, then the songs are most certainly similar. This paper hypothesizes that ratings data from a large number of users, averaging over many different kinds of tastes, produces an accurate measure of similarity. We compare our approach to a traditional content-based approach. We test this approach by generating a list of songs which are most similar to the query. We show the superiority of our approach by asking for human judgments and by counting songs identified as similar by virtue that they are performed by the same artist (a weak, but highly objective measure of similarity, as we will discuss in Section",
        "zenodo_id": 1416402,
        "dblp_key": "conf/ismir/SlaneyW07",
        "keywords": [
            "users preferences",
            "multimedia objects",
            "query-by-example (QBE)",
            "music similarity",
            "content-based algorithms",
            "content analysis",
            "objective metric",
            "objective measure of similarity",
            "content-based system",
            "random list of songs"
        ],
        "content": "SIMILARITY BASED ON RATING DATA\nMalcolm Slaney\nYahoo! Research\n2821 Mission College Blvd.\nSanta Clara, CA 95054\nmalcolm@ieee.orgWilliam White\nYahoo! Media Innovation\n1950 University Ave.\nBerkeley, CA 94704\nwwhite@yahoo-inc.com\nABSTRACT\nThis paper describes an algorithm to measure the similar-\nity of two multimedia objects, such as songs or movies,\nusing users’ preferences. Much of the previous work on\nquery-by-example (QBE) or music similarity uses detailed\nanalysis of the object’s content. This is difﬁcult and it is\noften impossible to capture how consumers react to the\nmusic. We argue that a large collection of user’s pref-\nerences is more accurate, at least in comparison to our\nbenchmark system, at ﬁnding similar songs. We describe\nan algorithm based the song’s rating data, and show how\nthis approach works by measuring its performance using\nan objective metric based on whether the same artist per-\nformed both songs. Our similarity results are based on 1.5\nmillion musical judgments by 380,000 users. We test our\nsystem by generating playlists using a content-based sys-\ntem, our rating-based system, and a random list of songs.\nMusic listeners greatly preferred the ratings-based playlists\nover the content-based and random playlists.\n1 INTRODUCTION\nThis paper describes a means to evaluate the similarity of\ntwo multimedia objects using users’ stated preference or\nrating data about items in the collection. We use a large\nmusic collection to illustrate this work, but the same idea\napplies to any collection of objects where many users re-\nport their preference of an object.\nMost work on similarity uses content-based algorithms.\nA specialized algorithm looks at the content (usually mu-\nsic) calculates various musically-inspired measures of the\nsound to form a feature vector, and then compares two fea-\ntures vectors to make a decision about similarity [1]. This\nsimilarity measurement is the heart of conventional query-\nby-example (QBE) systems, such as QBIC [2]. But, even\nmeasuring similarity with human raters is difﬁcult [4].\nOur work ignores the content. Instead we look at a\nlarge group of users and ask if these users, with a wide\nrange of personal musical interests, rate two pieces of mu-\nsic in the same manner. If jazz, classical, blues, and hip-\nhop lovers all rate two songs in the same way, whether\nc\r2007 Austrian Computer Society (OCG).they hate it or love it, then the songs are most certainly\nsimilar.\nThis paper hypothesizes that ratings data from a large\nnumber of users, averaging over many different kinds of\ntastes, produces an accurate measure of similarity. We\ncompare our approach to a traditional content-based ap-\nproach. We test this approach by generating a list of songs\nwhich are most similar to the query. We show the superi-\nority of our approach by asking for human judgments and\nby counting songs identiﬁed as similar by virtue that they\nare performed by the same artist (a weak, but highly ob-\njective measure of similarity, as we will discuss in Section\n5.3.)\n2 MOTIVATION\nThere are many situations where it is useful to know which\nmedia is similar to other media. We would like to be able\nto ﬁnd songs similar to a user’s interests, or to create a\nplaylist that sequences a list of songs in a pleasing manner.\nWe are interested in analyzing all types of media.\nWith the emergence of digital media, users are now\nfaced with the problem of too much choice. There is\nsimply more media available to the user now than they\nwill ever be able to consume. Traditional media-discovery\nmethods such as searching for a track by artist, album or\ntitle are no longer sufﬁcient. The user is restricted to what\nthey already know, which is a constantly shrinking piece\nof an ever increasing media pie.\nUsing genre as the primary discovery mechanism is\nalso not sufﬁcient, as genre is often difﬁcult to pin down,\nwith two people often classifying the same piece of music\ninto completely different genres. Clearly, there is an ur-\ngent need for new methods, which allow users to discover\nnew media that is fresh and exciting to them.\nPersonalized recommendation systems are one of the\nmore popular mechanisms for facilitating media discov-\nery. However, most recommendation systems need to get\na fair amount of information about the user and what she\nlikes before they become effective. An advantage of our\napproach is that it does not require any user-speciﬁc rat-\nings to be effective. Our method can start with a single\nseed song, without the need to build an elaborate user pro-\nﬁle.\nGenerating a list of music based upon their similarityto a speciﬁc track facilitates a more interactive musical\nexperience. By creating similarity-based playlists, we can\nprovide a highly targeted experience, appropriate for the\nsituation, engaging for the user, and easy for them to ini-\ntiate.\nOne can imagine using such a technology to generate\nthe perfect on-demand mood music experience. You wake\nup on a lazy Sunday morning and start things off with\n“Girl From Ipanema” by Stan Getz. Similar music would\nfollow for a couple hours, easing you into the day. Then\nafter lunch, you make a single adjustment—changing the\nseed track to “We Will Rock You” by Queen. The mood\npicks up and the tracks that follow make up the perfect\nsoundtrack for watching Sunday afternoon football over\nbeers with friends. By focusing on similarity to a given\nsource track, we can deliver music that the user not only\ngenerally likes, but will want to hear right now.\nIn practice, one would combine this approach with a\nconventional content-based analysis approach (like our pre-\nvious work on genre-gram based similarity [9], or the best\nof the MIREX work on similarity search [1]) since we do\nnot have rating data for everything.\nOur approach is different from collaborative ﬁltering,\nwhich uses rating data to ﬁnd songs that a user likes [6].\nCollaborative ﬁltering is usually structured as a rating-\nprediction problem. Given a collection of users’ ratings,\ncollaborative ﬁltering builds a model of the song-rating\ndatabase. This allows one to predict a new user’s ratings\non any new song based on what similar users like. This\nmight be done using nearest-neighbor approaches [8], or\nby clustering users and then examining the average rating\nin the cluster [3].\nAn Internet radio service called Pandora uses a hand-\nlabeled meta-data feature, i.e. “Angry Lyrics” and “Back-\nbeat Hand Claps,” to ﬁnd similar songs [11]. Then based\non user feedback, they adjust the relative weights of the\ndifferent parameters to ﬁnd music for each listener. This\nis different from our work since our underlying feature\nvector is based on musical preferences.\nOur work is similar to Whitman’s work [12], which\nuses text from the web to judge artist similarity. They use\nrich descriptions about the artist and their work with hun-\ndreds of words written by a small number of web authors.\nWe, on the other hand, use less than 3 bits per user (usu-\nally one point on a ﬁve-star scale), a relatively inexpensive\namount of data to collect, because we believe similarity is\nbetter judged by more listeners.\n3 RATINGS-BASED SIMILARITY APPROACH\nConsider three listeners: Ujazz,Urock,Uclassical . They\nmight rate three songs as follows on a ﬁve-point scale (0\nmeans they hate it, 5 means they love it) as shown in Table\n1.\nFrom this small snippet of data, we can infer that songs\ns1ands3are similar because jazz and rock lovers like\nthem and the classical listener does not. (We can not say\nanything about song s2.)s1s2s3\nUjazz 5 0 5\nUrock 5 0 5\nUclassical 0 5 0\nTable 1 . A sample table of user ratings for three songs\nand three users.\nThe algorithm we describe below quantiﬁes these sim-\nilarities and differences among users.\n4 ALGORITHMS\nOur approach starts with data about users’ preferences for\ndifferent media (i.e. songs or ﬁlms). The variable r(u; s)\ndescribes user u’s rating for object s. The vector ~ rsis an\nNu-dimensional vector of all the rating data for object s.\nThese ratings range from 0, which means never play this\nagain, to 100, which means the user “can’t get enough of\nit.” In a large database of users and media, many ratings\nwill be undeﬁned.\nTo compute the similarity of two objects, we compute\nthe normalized vector\n~ rs0= (~ rs\u0000b)=j~ rs\u0000bj (1)\nwhere bis a bias value that will eventually represent the\nimplicit rating for an unrated object. This normalization is\nimportant because it allows songs with different numbers\nof raters to be compared. A ratings database of this size\nis necessarily sparse—almost nobody will rate all songs.\nWe discuss the effect this has on our system in Section\n5.4. By default, we give unlabeled songs in our system an\nimplicit rating of b.\nThe similarity of two objects, s1ands2is written\ns=~ rs10\u0001~ rs20: (2)\nThis is equivalent to the cosine metric used when compar-\ning the word-frequencies in conventional text-based infor-\nmation retrieval. It is also monotonically related to the\nEuclidean distance between the two vectors since\nja\u0000bj2=jaj2\u0000j2abj+jbj2= 2\u0000j2abj (3)\nand the last step follows since the magnitude of our ratings\nvectors is equal to 1.\nGiven a query we generate a list of similar songs, or a\nplaylist, by ﬁnding the songs that have the highest similar-\nity, as judged by Equation 2.\nWe compare the ratings-based similarity to a content-\nbased scheme using a genre-gram [10]. In the original\nwork on genre-grams, an hand-crafted collection of fea-\ntures is combined to form a feature vector. Different posi-\ntions in this vector space indicate different musical genres.\nWe extended this idea in our previous work [9] by using\na multi-dimensional linear discriminant analysis (LDA)\nto rotate the coordinate axis and generate an optimal 28-\ndimensional subspace. Similarity in genre space is de-\nﬁned as the Euclidean distance between the positions ofSong 1Song 2\n100 150 200 250 300100\n150\n200\n250\n300Figure 1 . A portion of the similarity matrix for 200 of the\nsongs in our database.\ntwo points in genre space. In this work we used Euclidean\ndistance in feature space since the results were similar to\nthe LDA result.\n5 RESULTS\n5.1 Database\nWe tested the ratings-based similarity algorithm using a\ndatabase of 1,449,335 ratings of jazz songs provided by\nusers of the Yahoo! Music service and described by Mar-\nlin [5]. From this list of jazz songs we choose the 1000\nsongs with the most ratings. The number of ratings per\nsong ranged from “Sunrise” by Norah Jones with 98,658\nratings to a song by Os Nosos with 118 ratings. 380,911\nusers contributed to these ratings, with one user rating 913\nsongs and many users rating only one of these jazz songs.\nThus we restricted our initial study to music which fell\nunder the category “Jazz,” and then restricted it even more\nto those tunes for which we had ratings data. Essentially\nthis gave us a set of tunes which are not speciﬁc enough to\nbe one speciﬁc type of jazz (i.e. bebop or fusion), but\njazzy enough to be labeled “Jazz.” This explains why\nNora Jones, a “contemporary pop” artist, got top ranking\nin our results.\nFigure 1 shows a portion of the similarity matrix for\nour 1,000 song database. Pixels representing two songs\nthat are most similar show up darker in this matrix. (The\ndiagonal is set to zero so we can more easily see the other\nsongs.) The squares along the diagonal represent groups\nof songs, that happen to be listed in order in our database,\nfrom the same artist or album. These songs tend to be\nhighly similar. (Note: The Yahoo! Music system allows\nusers to rate artists too, but we did not use this data in our\nanalysis, only the per-song rating data.)Rank Song Title Artist\nQuery Those Sweet Words Norah Jones\n1 Shoot The Moon Norah Jones\n2What Am I To You? Norah Jones\n3 Sunrise Norah Jones\n4 Seven Years Norah Jones\nTable 2 . Rating-based similarity: Most similar tracks for\nthe query song “Those Sweet Words.” The 4 most similar\nsongs are all recorded by Norah Jones.\nRank Song Title Artist\nQuery Those Sweet Words Norah Jones\n5 Come Live You Life\nWith MePeter Cincotti\n6 Dansons La Gigue Patricia Barber\n7 Noa Noa Wolfgang Dauner\n8 Ain’t Nobody Here But\nUs ChickensLouis Jordan\nTable 3 . Rating-based similarity: Most similar tracks for\nthe query “Those Sweet Words,” as in Table 2 but not in-\ncluding songs by Norah Jones.\n5.2 Sample Playlists\nTables 2, 3, and 4 shows results for both algorithms for\nthe query song “Those Sweet Words” by Norah Jones. In\nTable 2, we demonstrate our ratings-based measure and\nshow the top 5 songs. Then in Table 3 we remove all songs\nby Norah Jones and show the next four selections. Finally,\nin Table 4, we show the results from a content-based ap-\nproach. In all cases, we can only measure similarity when\nwe have enough ratings data, the 1,000-song subset of jazz\nsongs in our database.\n5.3 Artist-based Similarity Test\nFor the purposes of an artist-based test, we say songs are\nsimilar (a binary decision) if they are recorded by the same\nartist. We use the artist of a song as a weak, but objective\nmeasure of two songs similarity. This metric is potentially\nweak for two reasons. First, an artist’s career can span\nRank Song Title Artist\nQuery Those Sweet Words Norah Jones\n1 More Harry Connick,\nJr.\n2 I Really Love You Cy Coleman\n3 Sway Peter Cincotti\n4 I Wants To Stay Here Ella Fitzgerald\n5 Feelin’ The Same Way Norah Jones\nTable 4 . Content-based similarity: Most similar tracks for\nthe query “Those Sweet Words” by Nora Jones.0 20 40 60 802.262.282.32.322.342.36\nBias ValueArtist Matches per QueryFigure 2 . Average number of artist matches per query\nfor the 50 most similar songs—computed as a function of\nthe normalization bias, or the implicit rating of unlabeled\ndata. Higher numbers are better because they indicate\na greater agreement between the ratings-based similarity\nmeasure and the artist who recorded the best matches.\nmany decades and musical styles. Second, and harder to\ntest, our data is not complete and the data that is missing\nis not random (see Section 5.5). It is possible that users\nmight rate all songs in an album, or their ratings might\nreﬂect their interest in the artist and not the song. Yet in\nspite of these two ﬂaws, the metric is easy to compute. It\nallows us to study parameter variations and test the entire\ncollection of song-similarity judgments, which we can not\ndo with human listeners.\n5.4 Implicit Rating\nThe dot-product formalism we deﬁned in Equation 2 does\nnot deﬁne what happens when data is missing. A common\nsolution in collaborative ﬁltering evaluates the dot product\nonly over those users who rate both songs. Since Equation\n2 computes a sum over all users, dropping data is equiva-\nlent to multiplying by 0, or assuming all the missing data\nis equal to 0. This means that the bias term in Equation 1\nhas special signiﬁcance as the implicit rating for a song.\nWe measured the effect of changing the bias value when\nnormalizing the rating data by measure the same-artist rate\nfor the genereated playlist. The results in Figure 2 show\nthat a bias of 0 is the best.\nAt the start of our work, we hypothesized that a bias of\n50 might be most effective. We rationalized that positive\nand negative results might be equally important for mea-\nsuring similarity. This proves not to be true, perhaps be-\ncause the rating data is not uniform [5], or because people\nwho like a song are the best people to evaluate similarity.\n5.5 Missing Data\nA histogram of all data in our entire music-rating database,\nFigure 3, shows peaks for the lowest and highest ratings.\nYet a histogram of rating data when a small number of\n0 25 50 75 10000.10.20.30.40.5\nRating ValueRating ProbabilityFigure 3 . Probability of rating over our entire database\n(717 million ratings of 136,000 songs given by 1.8 million\nusers of Yahoo! Music services and collected between\n2002 and 2006). (Adapted with permission from Marlin\n[5].)\n0 25 50 75 10000.10.20.30.40.5\nRating ValueRating Probability\nFigure 4 . Probability of rating for 35,786 users who each\nrated 10 songs that we chose at random. (Adapted with\npermission from Marlin [5].)Content Ratings00.511.522.5Artist Matches per QueryFigure 5 . Average number of artist matches per query for\nthe 50 most similar songs for a content-based algorithm\non the left and our rating-based approach on the right.\nHigher numbers are better because they indicate a greater\nagreement between the ratings-based similarity measure\nand the artist that recorded the best matches.\nusers are asked to rate all songs in a sample set, Figure 4,\nshows a different pattern, with users disliking most songs\nand only giving their highest ratings to a select few songs.\nThe difference between Figures 3 and 4 is that the Fig-\nure 3 shows the ratings of songs that users hear (either by\ntheir choice, based on a search, or recommended to them\nby the service) and they choose to rate. This is signif-\nicant because users tend to really hate or really like the\nsongs they hear. The latter ﬁgure is a measure of what\nusers think of allsongs in the database. For this ﬁgure, a\nsmall number of volunteers rated a completely at-random\nset of songs, exposing them to songs to which they would\nnormally not listen to.\nFor whatever reason the best default rating is close to\nzero. Perhaps it is because they do not rate songs that\nthey do not care about or because in practice users are not\nexposed to songs that they will not like.\n5.6 Artist-Based Similarity Test\nFigure 5 shows a test comparing the content-based and\nratings-based similarity measures. In this test we judged\nwhich algorithm produced a better, or more similar playlist,\nby asking which list had more songs by the artist that per-\nformed the query song. The rating-based similarity mea-\nsure described in this paper works almost twice as well as\nthe content-based scheme we use as a benchmark. One\ncan argue that this content-based scheme might not be as\ngood as others in the literature, but this certainly shows\nthat the rating-based scheme is competitive, and we be-\nlieve this approach will only get better with more data.\nFigure 6 . A portion of the screen showing two of the three\nplaylists in our user test.\nApproach Most Similar V otes Least Similar V otes\nRandom 1 13\nContent 1 4\nRating 16 1\nTable 5 . Results of a user test comparing three differ-\nent means of calculating similar songs. Users were asked\nto vote for the playlist with the songs most similar to the\nquery, and to vote for the playlist with the songs least sim-\nilar.\n5.7 User Test\nFinally we also performed a small user test to conﬁrm our\nanalytic results. Users were shown a display with a single\nquery song, and three lists of songs: similar songs based\non content, similar songs based on ratings, and a random\nlist of songs (Figure 6). The songs were in order of their\nsimilarity, but were not identiﬁed in any manner. The or-\nder of the three lists was randomized, and each list showed\nthe 10 most similar songs based on the query. For each\nsong there is only a button that the user can press to hear\na 30-second snippet of audio—other than the query song,\nthe songs were not identiﬁed.\nTable 5 shows our performance with human judgments.\nBy a wide margin, the 18 listeners judged the playlist gen-\nerated by the rating-based approach to produce the most\nsimilar list of songs, and the random list to be least simi-\nlar. This is true whether the listener liked or disliked jazz.\nThis result is important because it shows that a little bit\nof data from a large number of people expressing their\npreference for music can be used to measure a completely\ndifferent question—are two songs musically similar?\n6 CONCLUSIONS\nSimilarity is both difﬁcult to measure and highly personal.\nA classical-music lover will recognize the underlying sim-\nilarity of a Bach recording on period pieces and WendyCarlos’ “Switched on Bach” recordings; but he will prob-\nably love one and hate the other—they are not very simi-\nlar in his mind. A teenage lover of hip-hop will instantly\nhate both renditions. Perhaps this explains why positive\nratings are more important for similarity judgments than\nnegative—lovers of a particular style of music are more\ndiscriminating. We believe that averaging the musical in-\nterests of many users—380,911 in this study—is the best\nanswer to the “what is similar” problem.\nWe have described a method that computes the simi-\nlarity of two songs by comparing users’ ratings or pref-\nerences for the two different pieces of music. We used\na large collection of rating data to judge the similarity of\n1000 jazz songs. Using both a simple, but objective mea-\nsure of similarity, whether the songs were recorded by the\nsame artist, and a small user test we demonstrated the su-\nperior performance of the rating data over an approach\nbased on analysis of the content. Content-based methods\nwill always be necessary for songs that are new, or are\nnot popular enough to warrant a large number of listeners.\nThe rating-based approach can be applied to any data—\nmusic, books, ﬁlms—where we have rating data.\n7 ACKNOWLEDGMENTS\nWe appreciate the help we have received from Mike Mull\nand Ben Marlin who provided the data, and helped us to\nanalyze and understand this data. We appreciate the assis-\ntance of Sara Anderson and the anonymous reviewers in\nimproving the presentation of this work.\n8 REFERENCES\n[1] J. Stephen Downie. The Music Information\nRetrieval Evaluation eXchange (MIREX). In\nD-Lib Magazine, 12 (Issue 12), 2006:\n[2] Myron Flickner, Harpreet Sawhney, Wayne\nNiblack, Jonathan Ashley, Qian Huang, By-\nron Dom, Monika Gorkani, Jim Hafner, De-\nnis Lee, Dragutin Petkovic, David Steele, Pe-\nter Yanker. Query by image and video content:\nThe QBIC System. Computer , 28(9), pp. 21–\n32, 1995.\n[3] Thomas Hofmann and Jan Puzicha. Latent\nclass models for collaborative ﬁltering. in IJ-\nCAI ’99: Proceedings of the Sixteenth Inter-\nnational Joint Conference on Artiﬁcial Intelli-\ngence , San Francisco, CA, USA, pp. 688–693,\n1999.\n[4] M. Cameron Jones, J. Stephen Downie, An-\ndreas F. Ehmann. Human similarity judg-\nments: Implications for the design of formal\nevaluations. Proceedings of the 2007 Interna-\ntional Society of Music Information Retrieval,\nVienna, 2007.[5] Benjamin M. Marlin, Richard S. Zemel, Sam\nRoweis, and Malcolm Slaney. Collaborative\nﬁltering and the missing at random assump-\ntion. To be published in Proceedings of the\n23rd Conference on Uncertainty in Artiﬁcial\nIntelligence. 2007.\n[6] David M. Pennock, Eric Horvitz, Steve\nLawrence, and C. Lee Giles. Collaborative\nﬁltering by personality diagnosis: A hybrid\nmemory- and model-based approach, in Pro-\nceedings of the 16th Conference on Uncer-\ntainty in Artiﬁcial Intelligence (UAI-2000), pp.\n473–480, Stanford, CA, June 2000.\n[7] P. Resnick, N. Iacovou, M. Suchak, P.\nBergstorm, and J. Riedl. GroupLens: An open\narchitecture for collaborative ﬁltering of Net-\nnews. In Proceedings of ACM 1994 Con-\nference on Computer Supported Cooperative\nWork, pages 175–186, Chapel Hill, North Car-\nolina, 1994. ACM.\n[8] B. M. Sarwar, G. Karypis, J. A. Konstan,\nand J. Riedl. Item-based collaborative ﬁlter-\ning recommendation algorithms. In Proc. of\nthe 10th International World Wide Web Con-\nference (WWW10), Hong Kong, May 2001.\n[9] Malcolm Slaney and William White. Measur-\ning playlist diversity for recommendation sys-\ntems. In Proceedings of the 1st ACM Workshop\non Audio and Music Computing Multimedia ,\nSanta Barbara, California, USA, 27 October\n2006.\n[10] George Tzanetakis, Georg Essl and Perry\nCook. Automatic musical genre classiﬁcation\nof audio signals. In Proceedings International\nSymposium for Audio Information Retrieval\n(ISMIR) , October, 2001.\n[11] Tim Westergreen. Pandora Town Hall,\nCCRMA, Stanford University, 29 November\n2006.\n[12] Brian Whitman and Steve Lawrence. Infer-\nring descriptions and similarity for music from\ncommunity metadata. In Voices of Nature,\nProceedings of the 2002 International Com-\nputer Music Conference. pp 591–598. 16–21\nSeptember 2002."
    },
    {
        "title": "Annotating Music Collections: How Content-Based Similarity Helps to Propagate Labels.",
        "author": [
            "Mohamed Sordo",
            "Cyril Laurier",
            "Òscar Celma"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415708",
        "url": "https://doi.org/10.5281/zenodo.1415708",
        "ee": "https://zenodo.org/records/1415708/files/SordoLC07.pdf",
        "abstract": "In this paper we present a way to annotate music collections by exploiting audio similarity. Similarity is used to propose labels (tags) to yet unlabeled songs, based on the content–based distance between them. The main goal of our work is to ease the process of annotating huge music collections, by using content-based similarity distances as a way to propagate labels among songs. We present two different experiments. The first one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. On the one hand, our approach shows that using a music collection annotated at 40% with styles, the collection can be automatically annotated up to 78% (that is, 40% already annotated and the rest, 38%, only using propagation), with a recall greater than 0.4. On the other hand, for a smaller music collection annotated at 30% with moods, the collection can be automatically annotated up to 65% (e.g. 30% plus 35% using propagation). 1 INTRODUCTION Manual annotations of multimedia data is an arduous task, and very time consuming. Automatic annotation methods, normally fine-tuned to reduced domains such as musical instruments or limited to sound effects taxonomies, are not mature enough to label with great detail any possible sound. Yet, in the music domain the annotation becomes more complex due to the time domain frame. The purpose of making music easily accessible implies a condition of describing music in such a way that machine learning can understand it [1]. Specifically, these two steps must be followed: to build music descriptions which can be easily maintained, and to exploit these descriptions to build efficient music access systems that help users find music in large collections. There are a lot of ways to describe music content, but we can basically classify the descriptors in three groups: editorial meta-data, cultural meta-data, and acoustic meta-data [1]. As a paradigmatic example, the Music Genome Project is a big effort to “capture the essence of music at the fundamental level” by using over 400 attributes to describe c⃝2007 Austrian Computer Society (OCG). songs. To achieve this, more than 40 musicologists have been annotating thousands of files since 2000. Based on this knowledge, a well–known system named Pandora 1 creates playlists by exploiting these human–based annotations. It is clear that helping these musicologists can reduce both time and cost of the annotation task. Thus, the main goal of our work is to ease the process of annotating music collections, by using content-based similarity distance as a way to propagate labels among songs. 2 RELATED WORK Nowadays, content-based retrieval systems can not classify, identify and retrieve as well as humans can. This is a common problem in the multimedia field, like in image or video annotation. But in the latter fields many attempts have been made [2][3]. Semantic audio annotation, however, has not been as studied as image or video annotation, except the work by Whitman [4] or Barrington et al. [5][6]. Barrington et al. have made significant advances in semantic annotation of songs for music information retrieval (MIR) using MFCC’s to describe music content and HMM’s trained on timbre and rhythm for computing similarity between songs. Their idea was basically based on other work that represented image semantic annotation as a supervised multiclassification problem [7] . In the MIR field, only a few works are dealing with the problem of detecting mood using audio content. Although some results are promising (e.g [8], [9]), there is no standard or clearly defined proposals about the categories and the features to use. In the following experiments, we will check if tags about styles and moods can be propagated using content–based (CB) similarity. 3 EVALUATION The goal of this paper is to prove empirically how contentbased similarity can help to propose labels to yet unlabeled songs, and thus reducing the hard effort of manually annotating songs. For our purpose, the content–based similarity can be seen as a black box. That is to say, given a 1 http://www.pandora.com seed song, the module returns a list of the ith most similar songs. This study employs a CB module that considers not only timbrical features (e.g. MFCC), but some musical descriptors related to rhythm, tonality, etc. [11]. We present two different experiments. The first one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. The problem with the Magnatune collection is that there is only one human that annotated the tracks, when normally a ground truth of this nature should be pair–reviewed. Yet, we validated a large amount of the annotated songs by listening to them.",
        "zenodo_id": 1415708,
        "dblp_key": "conf/ismir/SordoLC07",
        "keywords": [
            "annotation",
            "multimedia",
            "music",
            "content-based",
            "retrieval",
            "semantic",
            "audio",
            "annotation",
            "MIR",
            "Mood"
        ],
        "content": "ANNOTATING MUSIC COLLECTIONS: HOW CONTENT-BASED\nSIMILARITY HELPS TO PROPAGATE LABELS\nMohamed Sordo, Cyril Laurier, `Oscar Celma\nMusic Technology Group\nUniversitat Pompeu Fabra\n{msordo, claurier, ocelma }@iua.upf.edu\nABSTRACT\nIn this paper we present a way to annotate music collec-\ntions by exploiting audio similarity. Similarity is used to\npropose labels (tags) to yet unlabeled songs, based on the\ncontent–based distance between them. The main goal of\nour work is to ease the process of annotating huge music\ncollections, by using content-based similarity distances as\na way to propagate labels among songs.\nWe present two different experiments. The ﬁrst one\npropagates labels that are related with the style of the piece,\nwhereas the second experiment deals with mood labels.\nOn the one hand, our approach shows that using a mu-\nsic collection annotated at 40% with styles, the collection\ncan be automatically annotated up to 78% (that is, 40%\nalready annotated and the rest, 38%, only using propaga-\ntion), with a recall greater than 0.4. On the other hand, for\na smaller music collection annotated at 30% with moods,\nthe collection can be automatically annotated up to 65%\n(e.g. 30% plus 35% using propagation).\n1 INTRODUCTION\nManual annotations of multimedia data is an arduous task,\nand very time consuming. Automatic annotation methods,\nnormally ﬁne-tuned to reduced domains such as musical\ninstruments or limited to sound effects taxonomies, are\nnot mature enough to label with great detail any possible\nsound. Yet, in the music domain the annotation becomes\nmore complex due to the time domain frame.\nThe purpose of making music easily accessible implies\na condition of describing music in such a way that ma-\nchine learning can understand it [1]. Speciﬁcally, these\ntwo steps must be followed: to build music descriptions\nwhich can be easily maintained, and to exploit these de-\nscriptions to build efﬁcient music access systems that help\nusers ﬁnd music in large collections. There are a lot of\nways to describe music content, but we can basically clas-\nsify the descriptors in three groups: editorial meta-data,\ncultural meta-data, and acoustic meta-data [1].\nAs a paradigmatic example, the Music Genome Project\nis a big effort to “capture the essence of music at the fun-\ndamental level” by using over 400 attributes to describe\nc/circlecopyrt2007 Austrian Computer Society (OCG).songs. To achieve this, more than 40 musicologists have\nbeen annotating thousands of ﬁles since 2000. Based on\nthis knowledge, a well–known system named Pandora1\ncreates playlists by exploiting these human–based anno-\ntations. It is clear that helping these musicologists can\nreduce both time and cost of the annotation task.\nThus, the main goal of our work is to ease the process\nof annotating music collections, by using content-based\nsimilarity distance as a way to propagate labels among\nsongs.\n2 RELATED WORK\nNowadays, content-based retrieval systems can not clas-\nsify, identify and retrieve as well as humans can. This is\na common problem in the multimedia ﬁeld, like in image\nor video annotation. But in the latter ﬁelds many attempts\nhave been made [2][3].\nSemantic audio annotation, however, has not been as\nstudied as image or video annotation, except the work by\nWhitman [4] or Barrington et al. [5][6]. Barrington et\nal. have made signiﬁcant advances in semantic annota-\ntion of songs for music information retrieval (MIR) using\nMFCC’s to describe music content and HMM’s trained\non timbre and rhythm for computing similarity between\nsongs. Their idea was basically based on other work that\nrepresented image semantic annotation as a supervised multi-\nclassiﬁcation problem [7] .\nIn the MIR ﬁeld, only a few works are dealing with the\nproblem of detecting mood using audio content. Although\nsome results are promising (e.g [8], [9]), there is no stan-\ndard or clearly deﬁned proposals about the categories and\nthe features to use. In the following experiments, we will\ncheck if tags about styles and moods can be propagated\nusing content–based (CB) similarity.\n3 EVALUATION\nThe goal of this paper is to prove empirically how content-\nbased similarity can help to propose labels to yet unla-\nbeled songs, and thus reducing the hard effort of manually\nannotating songs. For our purpose, the content–based sim-\nilarity can be seen as a black box. That is to say, given a\n1http://www.pandora.comseed song, the module returns a list of the ithmost similar\nsongs. This study employs a CB module that considers\nnot only timbrical features (e.g. MFCC), but some musi-\ncal descriptors related to rhythm, tonality, etc. [11].\nWe present two different experiments. The ﬁrst one\npropagates labels that are related with the style of the piece,\nwhereas the second experiment deals with mood labels.\nThe problem with the Magnatune collection is that there is\nonly one human that annotated the tracks, when normally\na ground truth of this nature should be pair–reviewed. Yet,\nwe validated a large amount of the annotated songs by lis-\ntening to them.\n3.1 Propagation of music style labels\nThe ground truth for the style experiment consists of 29\ndifferent labels (like Rock,Instrumental ,Classical ,Relax-\ning, etc.), and 5481 annotated songs.\nThe evaluation process was the following, for a par-\ntially annotated collection (10%,..., 50%), we use the CB\nmodule to get the ith–similar (i=10, 20 and 30) songs —\nand their tags— to a given one, to propose tags based on\nthe tags from these similar songs. However, we did not\npropose those tags that appeared with a frequency less\nthan 20%.\n3.1.1 Evaluation metrics\nThe metrics used to evaluate the styles experiments were\ninitially Precision/Recall and F2-Measure (giving more\nweight to Recall). In our case, Recall seems to be more\ninformative since our purpose is to know how well the\ntags can be propagated. However, neither P nor R take\ninto account the frequencies (i.e. ranking) of the tags ob-\ntained from the similar songs. Thus, we used the Spear-\nman’s rank correlation coefﬁcient, or Spearman ρ, which\nis deﬁned as:\nρ= 1−6/summationtextd2\ni\nn(n2−1)(1)\nWhere direpresents the distance between each rank of\npair of values —in our case labels in the ground truth and\nlabels in the proposed tags— To compute the distances we\nassume that the frequency of manually annotated labels is\nequal to 1.\n3.1.2 Results\nFor the style experiment, we ran different conﬁgurations\nand we computed the average metrics. A special case is\nwhen using the 100% annotated songs (see the results in\nTable 1). This experiment is used to test whether the CB\nsimilarity is good for propagating labels. There are four\ndifferent conﬁgurations when retrieving the most similar\nsongs to a given one: do not apply any constraint, or ﬁlter\nby artist/album. The constraints, then, are: ﬁltering the\nsimilarity results by same Artist, same Album, or by same\nArtist and Album. The latter case makes only sense when\nthe songs appears in compilations, various artists albums,\netc. When ﬁltering by artist or by album we make surethat the most similar songs to a given one are not from the\nsame artist or the same album. That of course decreases\nthe Precision/Recall measure. We can see from the results,\nthat to achieve more precision and recall when applying\na constraint, we need to increase the number of similar\nsongs, which makes sense because we are not taking into\naccount similar songs that are closer to a given one.\nSims. Constraint P R F2 ρ\nNone 0.56 0.84 0.72 0.51\n10 Artist 0.41 0.58 0.51 0.23\nAlbum 0.50 0.71 0.62 0.34\nArtist & Album 0.43 0.59 0.53 0.19\nNone 0.56 0.82 0.71 0.49\n20 Artist 0.48 0.61 0.56 0.26\nAlbum 0.53 0.72 0.64 0.35\nArtist & Album 0.48 0.61 0.56 0.24\nNone 0.60 0.77 0.70 0.45\n30 Artist 0.50 0.58 0.55 0.28\nAlbum 0.56 0.67 0.63 0.37\nArtist & Album 0.50 0.59 0.55 0.27\nTable 1 . Experiments with the 100% annotated collec-\ntion. The Precision/Recall measure, the F2-measure and\nthe Spearman ρmeasure are proportional to the number of\nsimilar songs. When constraints are present, these mea-\nsures decrease.\nNow, table 2 shows the results of propagating a par-\ntially annotated collection. The Spearman ρcoefﬁcient,\nas well as Precision/Recall and F2-measure, grows when\nincreasing the percentage of songs annotated in the col-\nlection. Interestingly enough, the values decrease when\nincreasing the number of neighbours (from 10 to 30) for a\ngiven song.\nAnnotation Sims. P R F2 ρ\n10 0.32 0.29 0.30 0.24\n20% 20 0.22 0.17 0.19 0.16\n30 0.08 0.05 0.06 0.06\n10 0.57 0.59 0.58 0.43\n40% 20 0.56 0.52 0.53 0.41\n30 0.49 0.39 0.42 0.34\n10 0.61 0.67 0.64 0.47\n50% 20 0.61 0.61 0.61 0.45\n30 0.57 0.51 0.53 0.41\nTable 2 . Experiments with the 20%, 40% and 50% an-\nnotated collection. The Precision, Recall and F2-measure\nand the Spearman ρvalues grow with a higher percent-\nage of annotated songs, and a smaller number of similar\nsongs.\nFinally, we propose another experiment that is to auto-\nmatically annotate songs in a music collection by means\nof the propagation process. The results are presented in\nTable 3. It is clear that the percentage of songs automati-\ncally annotated by CB similarity increases when the num-\nber of already annotated songs grows. But, we can seean interesting exception here, that is the 40% annotated\ncollection performs better (up to 38.68% new propagated\nlabels, with a low Recall 0.4) than the 50% one. This\ncould be due to the random process of splitting the ground\ntruth and the test set from the collection. Furthermore,\nwe can see how the percentage of songs automatically an-\nnotated is inversely proportional to the number of similar\nsongs used by the CB similarity module (in contrast with\nthe results from the 100% annotated collection, see Table\n1, when applying any constraint).\nPropagation with Recall\nAnnot. Sims. >0.8 >0.6 >0.4\n10 17.515% 21.365% 24.977%\n20% 20 8.666% 12.352% 15.453%\n30 2.554% 3.758% 5.145%\n10 28.01% 33.46% 38.68%\n40% 20 22.50% 28.92% 34.32%\n30 15.22% 20.82% 26.22%\n10 26.77% 31.62% 35.92%\n50% 20 22.66% 28.74% 33.37%\n30 17.48% 23.15% 28.44%\nTable 3 . Extending annotations of a music collection by\nmeans of CB similarity. We observe that the propagation\ngrows with a smaller number of similars and a higher per-\ncentage of annotated songs, except for the case of 40%\nand 50%.\n3.2 Propagation of mood labels\nFor the moods experiment, the ﬁrst issue is the choice of\nthe taxonomy. As advised by Juslin et al. in [10], in order\nto make our experiment and to build a ground truth that\nachieve the best agreement between people, we should\nconsider few categories. We used a reduced version of\nthe Magnatune online library. This collection offers a\nset of playlists based on mood2. We clustered the 150\nmood playlists to ﬁt in our few categories paradigm. The\nadjectives proposed by Juslin: happiness, sadness, anger\nand fear in [10] have been applied by Feng et al. in [9]\nand proved to give satisfying results. As the collection is\nmostly focused on popular and classical music, the “fear”\nadjective has been extended to a larger category called\n“mysterious”. Using Wordnet3we have joined the pos-\nsible playlists together in the following four categories :\nhappy, sad, angry and mysterious. Then, a listener was\nasked to validate each song label. We obtained a ground\ntruth database of 191 songs with the distribution in mood\nshown in Table 4. For each song, there is only one mood\nlabel. It is not an equal distribution but there is enough\ndata in each category to experiment with the CB similar-\nity.\n2http://www.magnatune.com/moods/\n3http://wordnet.princeton.edu/Mood Happy Sad Angry Mysterious\nSongs 67 61 34 29\nTable 4 . Mood distribution of the ground truth\nGT/Predicted Angry Happy Mysterious Sad\nAngry 27 7 1 1\nHappy 4 55 1 2\nMysterious 8 6 7 5\nSad 4 16 2 35\nTable 5 . Confusion matrix for the mood experiment with\na 100% annotated collection.\n3.2.1 Evaluation metrics\nTo evaluate the mood results, we used two measures. First\nwe wanted to check if the system was able to guess the\ncorrect mood label (there is only one possible label per\nsong). We evaluated the Precision just considering the ﬁrst\nresult using Precision at 1, also called P@1.\nP@1 =/braceleftbigg\n1, best proposed label =real label\n0, otherwise(2)\nWe averaged this value over all the examples. This\nmetric helps us to understand if the system can predict the\ncorrect mood label. However it does not take into account\nthe relative frequencies. Then another measure would be\nneeded to evaluate this aspect. We weighted the frequen-\ncies of the proposed label and normalized to compute a\nweighted Precision at 1, that we will call wP@1. It is\nequal to the frequency value of the correct label over the\nsum of all the proposed label frequencies:\nwP@1 =freq. correct label/summationtextfreq. proposed labels(3)\n3.2.2 Results\nTo have an overview of the system performance for each\nmood, we built a confusion matrix in Table 5. It has\nbeen computed using 100% of the collection annotated.\nEach row gives the predicted mood distribution (consid-\nering only the best label) for each mood in the ground\ntruth. Looking at the confusion matrix we observe that a\nCB similarity approach can propagate relatively well the\n“happy”, “angry”, and “sad” labels. However the “mys-\nterious” label does not give good results. We can explain\nthis by the fact that it might be the most ambiguous con-\ncept of these categories. Table 6 presents the average P@1\nand wP@1 values per mood.\nAngry Happy Mysterious Sad All\nP@1 0.72 0.89 0.27 0.61 0.62\nwP@1 0.65 0.62 0.22 0.59 0.52\nTable 6 . P@1 and wP@1 values averaged for each moodIt conﬁrms what we have in the confusion matrix, the\n“happy” category gives the best result. However looking\nat the values of wP@1, we note that if “happy” is the most\nguessed mood, the system gives more reliability to its re-\nsults about the label “angry”.\nIn our last experiment we wanted to evaluate how well\nthe mood labels can be propagated if we annotate just par-\ntially the collection. We computed the P@1 for 70%, 50%\nand 30% of the database and obtain the results written in\nTable 7. It shows that for 30% of the collection annotated,\nthe system can propagate correctly the tags up to 65% of\nthe collection.\nInitial annotation 70% 50% 30%\nP@1 0.60 0.44 0.5\nCorrectly annotated after prop. 88% 72% 65%\nTable 7 . Evaluation of the mood label propagation with\nthe initially percentage of annotated songs.\nAs the CB approach may not consider important as-\npects that can infer the mood, all these performances should\nbe improved by using dedicated descriptors and approach\nor meta-data, like information about the title, the style or\nthe lyrics.\n4 CONCLUSIONS AND FUTURE WORK\nOur objective was to test how the content–based similar-\nity can propagate labels. For styles, we have shown that\nwith a 40% annotated collection, we can reach a 78%\n(40%+38%) annotated collection with a recall greater than\n0.4, only using content–based similarity. In the case of\nmoods, with a 30% annotated collection we can automati-\ncally propagate up to 65% (30% +35%). These results are\nquite encouraging as content–based similarity can propa-\ngate styles and moods in a surprisingly effective manner.\nOf course there are some limitations as the example of the\n“mysterious” label, the concept has to be clearly encoded\nin the music for the content–based propagation to work.\nFor the moods we will try to experiment with a larger\ndatabase, different taxonomies and more concepts. With\nour current mood results it may not be possible to general-\nize but it shows the potential of the technique. In general,\nto enhance the performance of such an automatic annota-\ntion system we would use a hybrid approach combining\ncontent–based, user feedback and social networks infor-\nmations. But as shown by the satisfying results, our prop-\nagation system based on content–based similarity would\nalready ease a lot the annotation process of huge music\ncollections.\n5 ACKNOWLEDGEMENTS\nThis research has been partially supported by the e-Content\nplus project V ARIAZIONI4. We are also very grateful for\n4http://www.variazioniproject.comthe help of Jens Grivolla and Joan Serr `a, and their advice\nabout evaluation metrics.\n6 REFERENCES\n[1] Pachet, F. “Knowledge Management and Musical\nMetadata”, Encyclopedia of Knowledge Management .\n[2] Jeon, J. and Lavrenko, V. and Manmatha, R. “Au-\ntomatic image annotation and retrieval using cross-\nmedia relevance models”, 26th ACM SIGIR confer-\nence on Research and development in information re-\ntrieval pages 119-126 , Toronto, Canada, 2003.\n[3] Wenyin, L. and Dumais, S. and Sun, Y. and Zhang,\nH. and Czerwinski, M. and Field, B. “Semi-automatic\nimage annotation”, INTERACT2001, 8th IFIP TC,\nvolume 13, pages 9–13 , 2001.\n[4] Whitman, B.A. “Learning the meaning of music”,\nPhD Thesis , Massachusetts Institute of Technology,\n2005.\n[5] Barrington, L. and Chan, A. and Turnbull, D. and\nLanckriet, G. “Audio Information Retrieval Using\nSemantic Simlarity”, International Conference on\nAcoustic, Speech and Signal Processing (ICASSP) ,\nHawaii, 2007.\n[6] Turnbull, D. and Barrington, L. and Torres, D. and\nLanckriet, G. “Exploring the Semantic Annotation\nand Retrieval of Sound”, CAL Technical Report CAL-\n2007-01 , San Diego, 2007.\n[7] Carneiro, G. and Vasconcelos, N. “Formulating Se-\nmantic Image Annotation as a Supervised Learning\nProblem”, Computer Vision and Pattern Recognition,\nvolume 2 , IEEE Computer Society Conference, San\nDiego, 2005.\n[8] Lu,L Liu,D Zhang H.J. “Automatic mood detection\nand tracking of music audio signals”, IEEE transac-\ntions on audio, speech and language processing, vol-\nume 14, pages 5-18 , 2006\n[9] Feng, Y. and Zhuang, Y. and Pan, Y. “Music Informa-\ntion Retrieval by Detecting Mood via Computational\nMedia Aesthetics”, IEEE/WIC International Confer-\nence on Web Intelligence , Washington DC, 2003.\n[10] Juslin, P.N. and Sloboda, J.A. Music and Emotion:\nTheory and Research . Oxford University Press, 2001.\n[11] Cano, P. et al. “An Industrial-Strength Content-based\nMusic Recommendation System. 28th ACM SIGIR\nConference , Salvador, Brazil, 2005."
    },
    {
        "title": "Pitch Spelling with Conditionally Independent Voices.",
        "author": [
            "Gabi Teodoru",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414946",
        "url": "https://doi.org/10.5281/zenodo.1414946",
        "ee": "https://zenodo.org/records/1414946/files/TeodoruR07.pdf",
        "abstract": "We introduce a new approach for pitch spelling from MIDI data based on a probabilistic model. The model uses a hidden sequence of variables, one for each measure, describing the local key of the music. The spellings in the voices evolve as conditionally independent Markov chains, given the hidden keys. The model represents both vertical relations through the shared key and horizontal voice-leading relations through the explicit Markov models for the voices. This conditionally independent voice model leads to an efficient dynamic programming algorithm for finding the most likely configuration of hidden variables — spellings and harmonic sequence. The model is also straightforward to train from unlabeled data, though we have not been able to demonstrate any improvement in performance due to training. Our results compare favorably with others when tested on Meredith’s corpus, designed specifically for this problem. 1 INTRODUCTION We consider here the problem of pitch spelling from MIDI, as has been addressed by several others, including Meredith, [1], [2], Cambouropoulos [3], [4], Chew and Chen, [5], Longuet-Higgins [6], and our previous work [7]. The goal here is to provide the pitch spellings (is it F♯or G♭?) necessary to notate common practice music using a data source, such as MIDI, that does not distinguish between alternate spellings. The most immediate use for such an algorithm is to produce more readable music scores from MIDI data — at present, the pitch spellings from the commercial music notation programs we know of leave much to be desired. But pitch spelling will also be a part of the inevitable migration from MIDI, which, at present, constitutes the lion’s share of symbolically represented music, to more expressive symbolic representations. While, perhaps, not as deep a problem as harmonic analysis, pitch spelling is the most obvious observable attribute of harmony — thus pitch spelling provides a means to quantify the accuracy of a harmonic analysis in objective terms. We introduce a model that uses the notion of conditionally independent voices. That is, we model the musical voices as conditionally independent sequences, while dec⃝2007 Austrian Computer Society (OCG). pending on a common collection of key variables. More explicitly, we model the influence of harmony by a hidden Markov chain of local keys, one for each measure. Given the keys, the evolution of each voice in each measure occurs independently of the others, but is dependent on the key sequence. The voices are also modeled as Markov Chains. Thus we allow for interaction between the voices while clearly articulating the way in which this interaction occurs. Our model assumes that there are two primary issues that explain pitch spellings: voice leading and local harmony. These two sources of information are articulated in the music theory text [8]. One principle therein suggests using accidentals to show the direction of chromatic passing tones, thus capturing the “yearning” ascribed to accidentals in informal discussions by musicians. (This same notion is also discussed by the Russian composer Nikolai Rimsky-Korsakov in [9].) Another principle from [8] advises avoiding “remote” accidentals; thus B♭is preferred to A♯in C major, since the former is in the scale of the F major, which is near to C. This principle is captured by the key conditional nature of our model, with its implicit notion of the likelihood of various pitches in different keys. Of course, there are times when these two principles come into conflict with each other, as in the spelling of chromatic scales. While notational conventions prescribe solutions here, and in other cases, our model can only explain the spelling in terms of local key and voice leading. The most obvious distinction between our approach and the others mentioned is our formulation in terms of a generative probabilistic model. Within this context, we believe that the merits of various pitch spellings can best be weighed within the context of a hidden key, so we explicitly model key and simultaneously estimate this key sequence along with spelling. Furthermore, we clearly articulate our objective as the globally most likely configuration. All of the algorithms cited use some notion of “pitch closeness” in choosing spellings, as in Temperley’s line of fifths and Chew’s spiral array, though, in our case, this notion of closeness is in terms of the hidden key variable. These algorithms differ in their incorporation of voice leading. Temperley and Sleater, Meredith, and, to some extent Longuet-Higgins use voice leading, while Cambouropoulos, and Chew and Chen do not. Our approach is computationally efficient while capturing both notions of horizontal and, to some extent, vertical interaction between voices. Our model is automatically \u0000\u0001\u0000\u0000\u0001\u0000\u0002\u0001\u0002 \u0002\u0001\u0002 \u0003\u0001\u0003\u0001\u0003 \u0003\u0001\u0003\u0001\u0003 \u0004\u0001\u0004 \u0004\u0001\u0004 \u0005\u0001\u0005\u0001\u0005 \u0005\u0001\u0005\u0001\u0005 \u0006\u0001\u0006\u0001\u0006 \u0006\u0001\u0006\u0001\u0006 \u0007\u0001\u0007\u0001\u0007 \u0007\u0001\u0007\u0001\u0007 \b\u0001\b\u0001\b \b\u0001\b\u0001\b \u0001\t\u0001 \u0001\t\u0001",
        "zenodo_id": 1414946,
        "dblp_key": "conf/ismir/TeodoruR07",
        "keywords": [
            "pitch spelling",
            "MIDI data",
            "probabilistic model",
            "hidden sequence of variables",
            "local key",
            "conditionally independent Markov chains",
            "harmony",
            "voice model",
            "conditionally independent voices",
            "harmonic analysis"
        ],
        "content": "PITCH SPELLING WITH CONDITION ALL YINDEPENDENT VOICES\nGabi Teodoru\nSchool ofInformatics\nIndiana University\nateodoru@indiana.eduChristopher Raphael\nSchool ofInformatics\nIndiana University\ncraphael@indiana.edu\nABSTRA CT\nWeintroduce anewapproach forpitch spelling from\nMIDI data based onaprobabilistic model. The model\nuses ahidden sequence ofvariables, oneforeach mea-\nsure, describing thelocal keyofthemusic. Thespellings\ninthevoices evolveasconditionally independent Mark ov\nchains, giventhehidden keys.Themodel represents both\nvertical relations through theshared keyandhorizontal\nvoice-leading relations through theexplicit Mark ovmod-\nelsforthevoices. This conditionally independent voice\nmodel leads toanef\u0002cient dynamic programming algo-\nrithm for\u0002nding themost likelycon\u0002guration ofhid-\ndenvariables spellings andharmonic sequence. The\nmodel isalsostraightforw ardtotrain from unlabeled data,\nthough wehavenotbeen able todemonstrate anyim-\nprovement inperformance duetotraining. Our results\ncompare favorably with others when tested onMeredith' s\ncorpus, designed speci\u0002cally forthisproblem.\n1INTR ODUCTION\nWeconsider here theproblem ofpitch spelling from MIDI,\nashasbeen addressed byseveralothers, including Mered-\nith,[1],[2],Cambouropoulos [3],[4],ChewandChen,\n[5],Longuet-Higgins [6],andourprevious work[7].The\ngoal here istoprovide thepitch spellings (isitF]orG[?)\nnecessary tonotate common practice music using adata\nsource, such asMIDI, thatdoes notdistinguish between\nalternate spellings. The most immediate useforsuch an\nalgorithm istoproduce more readable music scores from\nMIDI data atpresent, thepitch spellings from thecom-\nmercial music notation programs weknowofleavemuch\ntobedesired. Butpitch spelling willalso beapartofthe\ninevitable migration from MIDI, which, atpresent, consti-\ntutes thelion'sshare ofsymbolically represented music,\ntomore expressi vesymbolic representations. While, per-\nhaps, notasdeep aproblem asharmonic analysis, pitch\nspelling isthemost obvious observ able attrib uteofhar-\nmonythus pitch spelling provides ameans toquantify\ntheaccurac yofaharmonic analysis inobjecti veterms.\nWeintroduce amodel thatuses thenotion ofcondition-\nallyindependent voices .That is,wemodel themusical\nvoices asconditionally independent sequences, while de-\nc\r2007 Austrian Computer Society (OCG).pending onacommon collection ofkeyvariables. More\nexplicitly ,wemodel thein\u0003uence ofharmon ybyahidden\nMark ovchain oflocal keys,oneforeach measure. Given\nthekeys,theevolution ofeach voice ineach measure oc-\ncurs independently oftheothers, butisdependent onthe\nkeysequence. The voices arealso modeled asMark ov\nChains. Thus weallowforinteraction between thevoices\nwhile clearly articulating thewayinwhich thisinteraction\noccurs.\nOur model assumes thatthere aretwoprimary issues\nthatexplain pitch spellings: voice leading andlocal har-\nmony.These twosources ofinformation arearticulated in\nthemusic theory text[8].One principle therein suggests\nusing accidentals toshowthedirection ofchromatic pass-\ningtones, thus capturing theyearning ascribed toacci-\ndentals ininformal discussions bymusicians. (This same\nnotion isalsodiscussed bytheRussian composer Nikolai\nRimsk y-Korsak ovin[9].)Another principle from [8]ad-\nvises avoiding remote accidentals; thus B[ispreferred\ntoA]inCmajor ,since theformer isinthescale oftheF\nmajor ,which isnear toC.This principle iscaptured bythe\nkeyconditional nature ofourmodel, with itsimplicit no-\ntionofthelikelihood ofvarious pitches indifferent keys.\nOfcourse, there aretimes when these twoprinciples come\ninto con\u0003ict with each other ,asinthespelling ofchro-\nmatic scales. While notational conventions prescribe solu-\ntions here, andinother cases, ourmodel canonly explain\nthespelling interms oflocal keyandvoice leading.\nThemost obvious distinction between ourapproach and\ntheothers mentioned isourformulation interms ofagen-\nerativeprobabilistic model. Within thisconte xt,webe-\nlievethatthemerits ofvarious pitch spellings canbest be\nweighed within theconte xtofahidden key,soweex-\nplicitly model keyandsimultaneously estimate thiskey\nsequence along with spelling. Furthermore, weclearly\narticulate ourobjecti veastheglobally most likelycon-\n\u0002guration. Allofthealgorithms cited usesome notion\nofpitch closeness inchoosing spellings, asinTemper -\nley'slineof\u0002fths andChew'sspiral array ,though, inour\ncase, thisnotion ofcloseness isinterms ofthehidden key\nvariable. These algorithms differintheir incorporation\nofvoice leading. Temperle yandSleater ,Meredith, and,\ntosome extent Longuet-Higgins usevoice leading, while\nCambouropoulos, andChewandChen donot.\nOurapproach iscomputationally ef\u0002cient while captur -\ningboth notions ofhorizontal and, tosome extent, vertical\ninteraction between voices. Our model isautomatically\u0000\u0001\u0000\u0000\u0001\u0000\n\u0002\u0001\u0002\u0002\u0001\u0002\n\u0003\u0001\u0003\u0001\u0003\u0003\u0001\u0003\u0001\u0003\n\u0004\u0001\u0004\u0004\u0001\u0004\n\u0005\u0001\u0005\u0001\u0005\u0005\u0001\u0005\u0001\u0005\n\u0006\u0001\u0006\u0001\u0006\u0006\u0001\u0006\u0001\u0006\n\u0007\u0001\u0007\u0001\u0007\u0007\u0001\u0007\u0001\u0007\n\b\u0001\b\u0001\b\b\u0001\b\u0001\b\n\t\u0001\t\u0001\t\t\u0001\t\u0001\t\n\n\u0001\n\n\u0001\n\u000b\u0001\u000b\u0001\u000b\u000b\u0001\u000b\u0001\u000b\n\f\u0001\f\f\u0001\f\n\r\u0001\r\u0001\r\r\u0001\r\u0001\r\n\u000e\u0001\u000e\u0001\u000e\u000e\u0001\u000e\u0001\u000e\n\u000f\u0001\u000f\u0001\u000f\u000f\u0001\u000f\u0001\u000f\n\u0010\u0001\u0010\u0010\u0001\u0010 Voice 3Midi 3Midi 2Midi 1\nVoice 1\nVoice 2Key\nFigur e1.The hidden variables ofthemodel, inopen\ncircles, aretheMark ovchain corresponding tothekey\nsequence, along with conditionally independent Mark ov\nchains foreach voice. The observ able variable arethe\nMIDI pitch classes denoted bysolid circles. Thekeyvari-\nables aredirectly related toeach variable within thesame\nmeasure, though wedonotdrawthisinour\u0002gure forthe\nsakeofclarity .\ntrainable from unlabeled data, though itisunclear ifEM-\ntype training isappropriate foroursituation, asdiscussed\ninalater section. Wepresent results onthedatabase col-\nlected byDaveMeredith asatestbed forseveraldiffer-\nentpitch spelling algorithms, presented atISMIR 05[10].\nOuroverall error islowerthan thebest presented in[10],\nMeredith' sps1303 algorithm, byafactor ofmore than 3,\nthough theresults varybetween composers.\n2MOTIVATION OFOUR MODEL\nWeassume wearegivenapartition ofourpiece intoacol-\nlection ofVvoices .This partition willbeobvious inthe\ncase ofvocal music ormonophonic instrumental music in\nwhich each part will beassociated with avoice. How-\never,thenotion ofvoice isoften meaningful inkeyboard\nandother musical domains aswell. Severalefforts inthe\nISMIR community ,[11],[12],[13],aswell asourown,\nhaveaddressed automatic voicing ofmusic thatdoes not\ncontain explicit parts. Both [11],[13],havefound this\nproblem tobereadily solvable bydynamic-programming-\ntype algorithms seeking apartition thatminimizes acost\nfunction. Wedoubt thattheresults ofthepitch-spelling\nalgorithm discussed here areparticularly sensiti vetothe\nwayinwhich thedata isvoiced when thenotion ofvoice\nissuspect.\nWeviewour data asacollection ofnotes whose\nonly pitch attrib utes weconsider arethepitch classes,\nf0;:::;11g,where theclasses represent theremainder\nwhen theMIDI pitch isdivided by12. Forinstance C\nandB]would berepresented byclass 0,etc.Wenotate the\npitch classes ofthenotes asfomvig,where m=1;:::;M\nindexesthemeasures ofthepiece, v=1;:::;Vindexes\nthevoices, andi=1;:::;I=I(m;v)indexesthenotes\nwithin themthmeasure ofthevthvoice. While, ofcourse,\nthenumber ofnotes inaparticular voice andmeasure will\nbevariable, forthesakeofclarity wewillsuppress thisde-\npendence inournotation andsimply writeIforI(m;v).\nWhile thefomvigaretheobserv able variables corre-\nsponding toourmusical surface,weexplain each variableastheresult oftwohidden variables ,KmandSmvi.Here\nKmisthelocal keyofthepiece 12possible tonics with\namode ofeither major orminor andSmviisthesolfege\nvariable describing thescale degree with possible modi\u0002-\ncations. KmandSmviareconnected, since thedomain of\nSmvidepends onthekeyKm.Explicitly ,wehave\nSmvi2frest;^1;:::;^7;]^1;]^2;]^4;]^5;]^6;[^2;[^3;[^5;[^6;[^7g\n(1)\nforthemajor mode and\nSmvi2frest;^1;:::;^7;]^6;]^7;]^1;]^3;]^4;[^2;[^4;[^5g(2)\nfortheminor mode, inwhich theabovenotation ofscale\ndegrees isrelati vetonatural minor ^6and^7refer to\nthesixth andseventh scale degrees ofthenatural minor\nscale, while]^6and]^7refer tothesixth andseventh scale\ndegrees ofthemelodic minor scale. Weassume thatKm\nandSmvidetermine thepitch class, which wedenote as\no(k;s),inthestraightforw ardway.Forinstance, Km=C\nmajor ,Smvi=]^4together imply pitch class 6(omvi=6),\nwhileKm=Eminor ,Smvi=]^6sayomvi=1.That is,\no(Cmajor;]^4)=6ando(Eminor ;]^6)=1.\nThe twodifferent versions ofsome solfe gevariables,\nsuch as[^3and]^2,areused todistinguish between the\nspellings ofthenote oureventual goal. Notationally\nspeaking, anysolfe gevariable with asharp (\u0003at) must be\nspelled byraising (lowering) thecorresponding scale tone.\nForinstance, if]^4appears inthekeyofCminor ,whose\nscale degree^4isF,then thenote would bespelled asF].\nNotationally ,the]accidental would beused only ifitis\nnecessary totrump thekeysignature, which may dif-\nferfrom thatofthelocal keyofthepassage. Similarly ,\nif]^2appears inEmajor ,whose scale degree^2isF],the\nnote would bespelled asFdouble sharp .Notationally ,\nthiswould alwaysappear with thedouble sharp symbol\nunless thekeysignature actually hadadouble sharp. We\nhope toneverseethislatter situation!\nWhile theobserv able pitch classes, o,depend deter -\nministically ontheKandSvariables, themodeling of\nKandSismore interesting. Naturally ,inspection ofac-\ntualmusic data would unco verboth vertical andhorizon-\ntaldependence among theSvariables. Arelati velysim-\nplemodel would ignore horizontal dependence andtreat,\nfora\u0002xed measure m,thefSmvigasarandom sample\nfrom some distrib ution depending onthemode ofKm,\nb(Km).This distrib ution might givethehighest proba-\nbility totonic triad notes, thesecond highest probability\ntotheremaining scale notes, andthelowest probability to\nnon-scale tones. Such modeling must takeinto account\nthemode ofthekey,since different non-scale tones exist\nforthetwomodes asineqs1,2.Wehavedescribed these\nkey-conditional random sample models asbag ofnotes\nmodels inearlier work[14],andused them forharmonic\nanalysis toreasonably good effect. Abagofnotes model\ncanbeused fornote-spelling, asin[7],byspelling each\nnote using thelocal keyandperhaps other variables \ne.g. inDmajor ,pitch class 6willbespelled asF]rather\nthan G[.However,such amodel ignores thevoice leadingtendencies, which describe horizontal motion often an\nimportant issue indetermining correct pitch spellings (e.g.\n]^5often movesto^6).\nInthisworkwecapture theharmonic nature ofpitch\nspelling bymodeling asequence ofhidden keys,onefor\neach measure, asaMark ovchain andallowing thevoices\ntodepend onthiskey.Tocapture thevoice leading ten-\ndencies, wemodel each voice asaMark ovchain whose\ntransition probabilities capture voiceleading patterns. The\nvoices areassumed tobeconditionally independent ,given\nthekeysequence, K=k,thereby assuming thatallinter-\naction between thevoices isaccounted forbyK.This\nassumption allowsustopartially decouple thevoices dur-\ningourcomputations.\nAswith allmodels, ourassumptions oversimplify the\ntrue state ofaffairs; however,wedomanage tocapture\nwhat weexpect tobethemost important considerations.\nForinstance, theMark ovchainKattempts toestimate the\ntime-v arying key,which isusually themost important el-\nement forpitch spelling other considerations aside, in\nDmajor F]ispreferred toG[.However,themodel isalso\nable tocapture theinertia oftheaccidental spellings,\nwhich haveastrong tendenc ytoresolv einthedirection\noftheir accidentals. Furthermore, keyandspelling enjoya\nkind ofsymbiosis which, webelie ve,enables each tohelp\nclarify theother .While theeffectofkeyonspelling is\nrather obvious andhasalready been mentioned, spelling\ntendencies canhelp in\u0003uence thechoice ofkey.Forin-\nstance, analternation between pitch classes 6and7in\nameasure may argueagainst thekeyofCmajor using\nbag ofnotes reasoning since both F]andG[areun-\nlikelyinCmajor .However,when seen as]^4resolving\nto^5,arelati velycommon occurrence, Cmajor becomes a\nmuch more reasonable hypothesis. Ourapproach capital-\nizesonthisinterplay between keyandspelling bydoing\nsimultaneous recognition ofboth attrib utes.\n2.1The Model\nAdirected acyclic graph representation ofourmodeling\nassumptions isgiveninFigure 1,which showsaMark ov\nchain forthekeyvariables ontoponeforeach mea-\nsure. The keyvariables in\u0003uence everyvariable inthe\nsame measure, though these dependencies arenotdrawn\ninthegraph forthesakeofclarity .Once thekeysequence\nis\u0002xed,each voice evolvesindependently from theother\nvoices asaMark ovchain, butstilldepending onthekey\nsequence. Thus thevoices interact, butonly through the\nkey.Both keyandsolfe gevariable together determine\ntheobserv able MIDI pitch classes, which wedenote with\nsolid circles.\nUsing theprinciples articulated sofar,aswell asa\ncouple ofothers tobediscussed, thejoint distrib ution onK;S,andO,p(k;s;o),canbefactored asfollo ws.\np(k;s;o)=p(k1)M\u00001Y\nm=1p(km+1jkm) (3)\n\u0002VY\nv=1p(smv1jkm\u00001;sm\u00001vI;km)(4)\n\u0002\u000eo(km;smv1)(omv1) (5)\n\u0002IY\ni=2p(smvijkm;smvi\u00001) (6)\n\u0002IY\ni=2\u000eo(km;smvi)(omvi) (7)\nwhere\n\u000eo(k;s)(o)=\u001a1o(k;s)=o\n0otherwise\nNote thenesting oftheproducts inthese equations. The\nbasic structure ofEqns. 3-7issimply theresult ofthe\nassumptions that thekeyvariables areaMark ovchain\nandthat thevoices areconditionally independent given\nthekeysequence. Additionally weassume thatthe\u0002rst\nsolfe gevariable inavoice,smv1depends only onthetwo\nnearest keyvariables, km;km\u00001,aswell asitspredeces-\nsor,sm\u00001vI.Similarly ,weassume thatwithin ameasure\n(i>1)asolfe gevariable depends only onthecurrent key,\nkm,anditspredecessor solfe gevariable, smvi\u00001.\nThese assumptions arefurther specialized inthefol-\nlowing, inwhich wewritek=(t;b),where tisthetonic\n(2f0;:::;11g)andbthemode (major/minor) ofthekey.\nFirst thewithin measure transitions aremodeled by\np(smvi+1jsmvi;km)=pS(smvi+1jsmvi;bm)(8)\nwhere thelatter member pS(s0js;b)depends ontheprevi-\noussolfe gevariable andthecurrent mode. Inaddition, we\nfurther re\u0002ne Eqn. 4to:\np(smv1jsm\u00001vI;km\u00001;km)\n=8\n>>>><\n>>>>:p0(smv1jbm) voicevempty\ninmeasm\u00001\npT(smv1jsm\u00001vI;km\u00001;km)km\u000016=km\npS(smv1jsm\u00001vI;bm) otherwise\nThus, there arethree situations. When there isnoobvi-\nousinformation regarding voice leading, aswhen avoice\nbegins from scratch, wechoose the\u0002rstnote ofthevoice\nfrom thedistrib utionp0(sjb),which depends only onthe\nmode ofthekeyofthemeasure. When wehaveakey\nchange, weusethetransition probability ,pT,which can\nbequite simply parameterized, though weomit thede-\ntails. Otherwise wedohaveuseful voice leading infor -\nmation andfollo wthesame assumptions ofEqn. 8using\npS(s0js;b).\nThekeytransition probabilities canalso besimpli\u0002ed\nbyforcing thetranslation invariant nature ofkeytransi-tions.\np(km+1jkm)=p(tm+1;bm+1jtm;bm)\n=p(bm+1jtm;bm)p(tm+1jbm+1;tm;bm)\n=pB(bm+1jbm)pT(tm+1\u0000tmjbm+1;bm)\nwhere tm+1\u0000tmistakenmodulo 12. HerepB(b0jb)\ndescribes theprobability distrib ution formode transi-\ntions, obviously favoring staying inthecurrent mode.\nAlsopT(\u0001tjb;b0)givestheprobability oftherelative dif-\nference intonic, conditioned ontheprevious and next\nmode. Ofcourse when thetwomodes areequal, asthe\nmost often willbe,wewillfavor\u0001t=0thekeystays\nthesame. Presumably ,theprobability of\u0001t=0willbe\nlesswhen themode changes, though itmay stillberel-\nativelyhigh tocapture therelati vefamiliarity ofmoving\nbetween amajor keyanditsparallel minor .\n3COMPUTING THE MOST LIKEL Y\nCONFIGURA TION\nInthissection weexploit theconditional independence of\nthevoices to\u0002ndthemost likelycon\u0002guration ofthehid-\ndenkeyandsolfe gevariables, giventhepitch class obser -\nvations. Theessential idea isthesame asinthecomputa-\ntionofthemost likelycon\u0002guration ofaBayesian belief\nnetw orkwe\u0002ndagroups ofhidden variables thatsep-\narate thepast andthefuture variables. Thus theproba-\nbilities ofthevarious paths tosuch agroup canbecom-\npared without consideration offuture evolution. While\npresented forthesakeoffull disclosure andcomplete-\nness, thissection canbeskipped without lossofcontinu-\nity.\nForeach measure m=1;:::;M,wede\u0002ne thevector\nXmtobecomposed ofKmaswell asallSmvivariables\n(thehidden variables). Similarly ,wede\u0002ne thevectorom\ntobethecollection ofallpitch class observ ations omviin\nmeasure m.Inaddition wepartition thehidden variables,\nXm,as\nYm=(Km;Sm1I;:::;SmVI)\nZm= theremaining Smvivariables inXm\nsothat thedisjoint YmandZmtogether compose Xm.\nFigure 1shades theYmvariables foreach depicted mea-\nsure .Ouralgorithm \u0002nds themost likelyhidden con\u0002gu-\nration byrecursi velycomputing thefunction\np\u0003(ym)= max\nx1;:::;xm\u00001;zmp(x1;:::;xm\u00001;ym;zm;o1;:::;om)\n= max\nx1;:::;xm\u00001;zmp(x1;:::;xm;o1;:::;om)\nusing theessential ideas from dynamic programming.\nEachymcon\u0002guration separates thepast from thefu-\nture inourmodel. That is,anypath connecting variables\noneither side ofymmust contain aymvariable. Exploit-\ningsuch separations isalwaysthecore idea ofdynamic\nprogramming.Tobegin,wenote that\np\u0003(y1)=max\nz1p(x1)p(o1jx1)\n= max\ns111:::s11I\u00001\ns121:::s12I\u00001\n.........\ns1V1:::s1VI\u00001p(k1)QV\nv=1p(s1v1:::s1vIjk1)QI\ni=1p(o1vijk1;s1vi)\n=p(k1)VY\nv=1max\ns1v1:::s1vI\u00001p(s1v1:::s1vIjk1)QI\ni=1p(o1vijk1;s1vi)\n=p(k1)VY\nv=1q\u0003\n1vI(s1vIjk1)\nwhere wede\u0002ne\nq\u0003\n1vi(s1vijk1)=max\ns1v1:::s1vi\u00001p(s1v1:::s1vijk1)Qi\nj=1p(o1vjjk1;s1vj)\nfori=1;:::;I.Wecancompute q\u0003\n1virecursi velyusing\ntheusual dynamic program argument:\nq\u0003\n1v1(s1v1jk1)=p(s1v1jk1)p(o1v1jk1;s1v1)\nand\nq\u0003\n1vi+1(s1vi+1jk1)= max\ns1v1:::s1vip(s1v1:::s1vi+1jk1)Qi+1\nj=1p(o1vjjk1;s1vj)\n=max\ns1vip(s1vi+1js1vi;k1)\np(o1vi+1jk1;s1vi+1)\n\u0002 max\ns1v1:::s1vi\u00001p(s1v1:::;s1vijk1)Qi\nj=1p(o1vjjk1;s1vj)\n=max\ns1vip(s1vi+1js1vi;k1)\np(o1vi+1jk1;s1vi+1)\nq\u0003\n1vi(s1vi)\nHaving computed p\u0003(y1)wecancompute thegeneral\np\u0003(ym)recursi velyaswell.\np\u0003(ym)= max\nx1;:::;xm\u00001;zmp(x1:::;xm)Qm\n\u0016=1p(o\u0016jx\u0016)\n= max\nx1:::;xm\u00002;zm\u00001;ym\u00001;zmp(x1:::xm\u00001)Qm\u00001\n\u0016=1p(o\u0016jx\u0016)\np(xmjym\u00001)\np(omjxm)\n=max\nym\u00001p\u0003(ym\u00001)max\nzmp(xmjym\u00001)p(omjxm)\n=max\nym\u00001p\u0003(ym\u00001)q\u0003\nm(ymjym\u00001) (9)\nwhere\nq\u0003\nm(ymjym\u00001)=max\nzmp(xmjym\u00001)p(omjxm)Thenq\u0003\nmcanbecomputed interms offactors thatdepend\nonly ontheindividual voices by\nq\u0003\nm(ymjym\u00001)\n=max\nzmp(xmjym\u00001)p(omjxm)\n= max\nsm11:::sm1I\u00001\nsm21:::sm2I\u00001\n.........\nsmV1:::smVI\u00001p(kmjkm\u00001)QV\nv=1p(smv1:::;smvIj\nkm\u00001;km;sm\u00001vI)QI\ni=1p(omvijkm;smvi)\n=p(kmjkm\u00001)VY\nv=1q\u0003\nmvI(smvIjkm;km\u00001sm\u00001vI)\nwhere\nq\u0003\nmvi(smvijkm;km\u00001sm\u00001vI)\n=max smv1:::smvI\u00001p(smv1:::;smvIjkm\u00001;km;sm\u00001vI)Qi\nj=1p(omvjjkm;smvj)\nq\u0003\nmvicanalsobecomputed recursi velywith only minor\nrevision ofthederivation ofq\u0003\n1vi,though wehaveomit-\ntedthelong butstraightforw ardcalculation forthesakeof\nbrevity.\nOnce therecursions havebeen computed totheendof\nthepiece, wehavep\u0003(yM)forallcon\u0002gurations ofyM.\nIfwede\u0002ne ^yM=argmax yMy\u0003(yM),then from eqn. 9\nweseethatp\u0003(^yM)istheprobability oftheglobally max-\nimizing con\u0002guration ofhidden variables. Itisasimple\nmatter toundo ourcalculations toidentify thisglobal\nmaximizer .Wedothisbysubstituting argmax formax\nineqn. 9:\n^ym\u00001=argmax\nym\u00001p\u0003(ym\u00001)q\u0003\nm(^ymjym\u00001)\nform=M\u00001:::;2.Having found theoptimal con-\n\u0002guration foreach oftheYmvariables, wecanundo the\nq\u0003\nmvicalculations to\u0002llinthemissing values oftheZm\nvariables.\n4TRAINING AND EXPERIMENTS\nSince ourmodel isaBayesian belief netw ork, wecan\ntrain themodels parameters using theusual junction tree\nandmessage passing paradigms. However,wefound it\nsimpler toadopt thefamiliar forw ard-backw ard(Baum-\nWelch) training algorithm tothisparticular case. Hav-\ningimplemented thetraining inthisway,wewere dis-\nappointed toseeaslight degradation inourresults when\ncompared tohand-set parameters. However,thisisnot\nreally surprising. Baum-W elch training isanexample of\ntheEMalgorithm which seeks tomaximize themarginal\nlikelihood oftheobserv eddata, having integrated outover\nallunobserv edvariables. Asmanyresearchers haveob-\nserved,thisisnotthecriterion wearereally interested in\noptimizing; wewould prefer tominimize thenumber ofFigur e2.The error rates ofthealgorithms grouped by\ncomposer .\nComposer Error Rate\nCorelli 0.081\nVivaldi 0.265\nTelemann 0.053\nHandel 0.024\nBach 0.102\nHaydn 0.449\nMozart 0.322\nBeetho ven 0.102\nTotal 0.175\nTable 1.Percentage Error rateforCIV algorithm\nerrors onatraining set.Especially with error rates aslow\nastheyareinthisdomain, itisnotreasonable toexpect an\nincrease inperformance using amodel trained byEM. In\nessence, training getscredit formaking more likelycer-\ntaincon\u0002gurations thatarealready correctly recognized.\nWithalready lowerror rates, these already correct con\u0002g-\nurations may well dominate thelearning process.\nMeredith [10]compares thesuccess rate ofseveral\npitch spelling algorithms. Wehaveused theexact same\ncorpus as[10]tojudge theperformance ofouralgorithm.\nMeredith' scorpus contains 216pieces byeight different\ncomposers, composed between theyears 1680-1810. A\nkeyfeature ofthiscorpus isthatithasthesame number of\nnotes foreach composer ,thus thenumber ofmovements\norpieces percomposer differs.\nWecompare theperformance ofourConditionally In-\ndependent Voices (CIV) algorithm with theother algo-\nrithms from [10]using error rateasourcriterion thatis,\nthenumber ofmisspelled notes divided bythenumber of\ntotal notes. Wehaveused theoverall error rate, aswell as\ntheerror rateforeach composer ,tofacilitate comparisons.\nMeredith[10 ]reimplemented thealgorithms asde-\nscribed bytheir authors. Inthisprocess, severalversions\nofeach algorithm were produced, considering from these\ntheversion giving thebestperformance onthetestset.Toasmall extent, thisprocess tunes thealgorithms onthetest\ndata, though westillexpect thereported performance will\ngeneralize toother similar testdata. Inallcases thenum-\nberoftuned parameters wasquite small, making over\u0002t-\nting oftherather largedata highly unlik ely.The error\nrates wepresent forthealgorithms of[10]aretakendi-\nrectly from thissource.\nAsitmay benoted from Figure 3,theCIV algo-\nrithm compares favorably with theother algorithms of\n[10]when aggre gated overtheentire corpus, aswell as\nhaving thebestresult on5ofthe8composers. Theprecise\nerrors forthealgorithm aregiveninTable 1bycomposer .\nInthese experiments, theparameters ofourmodel were\ntuned byhand using asubset ofthecorpus which acted as\nourtraining data, andincluded alloftheBeetho vencor-\npus(5symphon ymovements), about athird oftheHaydn\ncorpus (5string quartet movements), andanother third of\ntheMozart corpus (one concerto movement). While itis\nnotmethodologically ideal totune parameters onthetest\ndata, wehadnochoice buttodothat, aswedidnothave\nother pitch spelled data, andneeded totestontheentire\ncorpus tocompare ourresults with [10].Inourcase, as\nwith [10],thenumber ofparameters wasverysmall in\ncomparison tothesizeofthetestset.Thus webelie vethat\nourresults willgeneralize tosimilar data aswell asthose\nof[10],making thecomparisons valid.\nOuralgorithm hastrouble correctly spelling afewhar-\nmonic situations such as:Ge+6!Cadential 6/4inmajor ,\nviio7/V!Cadential 6/4inmajor ,secondary dominants\nandsecondary leading tone chords ingeneral, andalso\ndelayed resolution (G]!B!Ainstead ofG]!A).\nWewere expecting such errors since thepremises ofour\nmodel oversimplify thetruestate ofaffairssome what. As\nthislastexample shows,theresolution ofanote may not\nalwaysbethefollo wing note inthevoice, thus thwarting\nourmodel, which only hasone-step memory ofpitch.\nOther situations require adeeper notion oftheharmonic\nstate than provided bythelocal key,asintheGerman\naugmented sixth chord, which seems nearly impossible to\nspell correctly without recognizing itassuch. Itseems,\nhowever,thatsimple voice leading tendencies often give\nthesame result asadeeper harmonic analysis, thus ex-\nplaining thesuccess ofourmodel. Finally ,weanticipate\nthatouralgorithm might haveproblems with chordal \u0002g-\nurations (arpe ggiated chords) inwhich severalvoices are\nrepresented inasingle voice. This might be\u0002xedbypre-\nprocessing thedata with analgorithm thatwould turn the\n\u0002guration intovoices. (Wefound thistobeanaggra vating\ntendenc yofourownvoice recognition algorithm, which\nmay well beanasset here!) Itisworth noting that the\nMeredith testcorpus contains almost no\u0002gurations.\n5REFERENCES\n[1]Meredith D.Comparing Pitch Spelling Algorithms\nonaLargeCorpus ofTonal Music, Computer Music\nModeling andRetrie val,Second International Sympo-\nsium, 2004 CMMR, Esbjer g,Denmark, 2004.[2]Meredith D.Pitch Spelling Algorithms, Proceed-\nings oftheFifthTriennial ESCOM Confer ence pp.\n204-207, Hano ver,German y,2003\n[3]Cambouropoulos E.,Automatic Pitch Spelling: From\nNumbers toSharps andFlats, Proceedings ofVIII\nBrazilian Symposium onComputer Music Fortaleza,\nBrazil, 2001.\n[4]Cambouropoulos E.,Pitch Spelling: ACompu-\ntational Model, Music Perception 20(4):411-429,\n2003.\n[5]ChewE.,Chen, Y.-C.Determining Conte xt-De\u0002ning\nWindows: Pitch Spelling Using theSpiral Array,\nProceedings oftheFourth International Confer ence\nonMusic Information Retrie val,ISMIR 2003 pp.223-\n224, Baltimore, USA, 2003.\n[6]Longuet-Higgins H. C., The Perception of\nMelodies, Mental Processes: Studies inCogni-\ntive Science pp. 105-129. British Psychological\nSociety/MIT Press, London, England andCambridge,\nMass., 1987.\n[7]Stoddard J.,Raphael C.,Utgof fP.Well-tempered\nSpelling: Akey-invariant Pitch Spelling Algorithm,\nProceedings oftheFifthInternational Confer ence on\nMusic Information Retrie val, ISMIR 2004 pp.106-\n111, Barcelona, Spain, 2004.\n[8]Aldwell andSchachter ,Harmon yandVoice Lead-\ning,3rdEdition, pp.505-508, Thompson Schirmer ,\n2003.\n[9]Rimsk y-Korsak ovN., Practical Manual ofHar-\nmony,1886 translated from 12th Russian Edition by\nJoseph Achron, Carl Fischer ,1930.\n[10] Meredith D.,Wiggins G.,Comparing Pitch Spelling\nAlgorithms, Proceedings oftheFourth International\nConfer ence onMusic Information Retrie val, ISMIR\n2005 pp.280-287, London, 2005.\n[11] Lilian J.,Hoos H.,Voice Separation ALocal Op-\ntimisation Approach, Proceedings oftheThirdInter -\nnational Confer ence onMusic Information Retrie val,\nISMIR 2004 pp.39-46, Paris, 2004.\n[12] Cambouropoulos E.,From MIDI toTraditional Mu-\nsical Notation, Proceedings oftheAAAI Workshop on\nArti\u0002cial Intellig ence andMusic Austin, Texas,USA,\n2000.\n[13] Temperle yD.,The Cognition ofBasic Musical Struc-\ntures, MIT Press, Cambridge, USA, 2001.\n[14] Raphael C.,Harmonic Analysis with Probabilistic\nGraphical Models, Proceedings oftheFourth Inter -\nnational Confer ence onMusic Information Retrie val,\nISMIR 2003 pp.177-182, Baltimore, USA, 2003."
    },
    {
        "title": "Ensemble Learning for Hybrid Music Recommendation.",
        "author": [
            "Marco Tiemann",
            "Steffen Pauws",
            "Fabio Vignoli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417781",
        "url": "https://doi.org/10.5281/zenodo.1417781",
        "ee": "https://zenodo.org/records/1417781/files/TiemannPV07.pdf",
        "abstract": "We investigate ensemble learning methods for hybrid music recommenders, combining a social and a content-based recommender algorithm in an initial experiment by applying a simple combination rule to merge recommender results. A first experiment suggests that such a combination can reduce the mean absolute prediction error compared to the used recommenders’ individual errors. 1 INTRODUCTION Music recommender systems that are publicly available today apply one of two recommender paradigms: social recommenders predict preferences based purely on user preference data. Content-based recommenders represent songs as feature vectors, where the features can be assigned manually or extracted automatically directly from audio data. Users are characterized by profiles, and songs that are similar to a user’s profile are recommended. Hybrid recommenders combine recommender paradigms to improve the overall accuracy of predictions and reduce problems of specific recommender algorithms. To this end, hybrid recommenders usually combine social and content -based recommendation using one of a number of possible combination methods [1]. They can be categorized into recommenders that 1) integrate social and contentbased recommendation in a single algorithm or 2) combine the outputs of independent recommender algorithms. We follow the second approach. Our goal is to integrate many distinct independent recommenders (referred to as base recommenders in this paper) flexibly in a generalized hybrid recommender, using ensemble learning methods to create diverse base recommenders and to combine their output with established and novel combination methods. 2 APPLYING ENSEMBLE LEARNING In ensemble learning [3], several weak learners are created and used in regression or classification tasks. Their individual output is combined into a unified single output by applying a combination rule. Ensemble learning methods can often outperform single strong learners in practic⃝2007 Austrian Computer Society (OCG). cal applications. However, ensemble methods can only be applied successfully when the used weak learners are sufficiently diverse – weak learners must be able to correct errors made by other weak learners. Hence many ensemble learning methods actively select or create suitably diverse weak learners by techniques such as training data resampling. Some ensemble learning methods can also iteratively adapt to newly presented evidence by modifying either combination rules or weak learners as new evidence is successively presented. Ensemble learning is a promising concept for hybrid music recommendation. It can be used to flexibly integrate heterogeneous data sources and different algorithms. Weak learner diversification methods and combination rules with well explored properties are readily available in existing work on ensemble learning, and iterative algorithms can be used to optimize recommender accuracy as additional observations of user preferences becomes available. 3 A HYBRID MUSIC RECOMMENDER In this paper we focus on combining recommender output. We investigate to what extend commonly used social and content-based recommenders provide sufficiently diverse output to improve recommendations by combining them without applying diversification methods. A frequently used social recommender method is item-based collaborative filtering [4]. Using this method, first the similarity between two items i and j is computed using preference data of all users u ∈U considered. The Pearson correlation coefficient is frequently used to determine this similarity: s(i, j) = P u \u0000Ru,i −¯Ri \u0001 \u0000Ru,j −¯Rj \u0001 qP u \u0000Ru,i −¯Ri \u00012 P u \u0000Ru,j −¯Rj \u00012 (1) for all users u ∈U, where Ru,i, Ru,j are preferences of user u for items i and j respectively, and ¯Ri, ¯Rj are mean preferences for these items. Then, a predicted preference Pu,i for a target item i is computed as the weighted sum of preference values of user u for all items j ∈J that are correlated to item i and for which the user i’s preferences are known, scaled by the sum of similarity terms: Pu,i = P j s(i, j)Ru,j P j s(i, j) (2) A content-based recommender can be implemented similarly to this social recommender algorithm. We first construct a similarity matrix of songs using a song similarity measure introduced in [5]. Analogous to the social recommendation algorithm described above, recommendations are computed as given in Formula 2 using the referenced content-based similarity function s(i, j) instead of the correlation of user preference data. A combination rule using decision templates [2] is applied to combine the base recommender output. A decision template is a record of weak learner estimates for an observed value. It can be expressed as a decision rule in the form {Pr1, ..., Prn} →R for a set of base recommenders {r1, ..., rn}, where Prn is the prediction of recommender n and R is the observed preference for a particular recommendation. For classification, decision templates are created as averaged class probabilities over all instances classified into a particular class. This method has been shown to perform well for different classification tasks [2]. For the regression task of predicting preference values, we apply a variant of this method. Decision template rules are retained without averaging them for each user. To combine base recommenders for a new instance, their individual predictions are computed. Then the Euclidean distance of the vector of computed results to each stored decision template rule head is computed, and the nearest neighbor rule’s tail (the observed preference) is the resulting preference prediction. 4 EXPERIMENT We conducted a preliminary experiment to evaluate the performance of the described hybrid recommender. For this a music dataset containing a collection of 63,949 popular music pieces was used. For each song manually assigned metadata are provided and perceptual audio features were extracted for the content-based recommender described in Section 3. The dataset contains 1,139,979 play counts, the number of times that a song has been played by a user, for 6,939 participants. Participants that played less than 100 distinct songs, played less than 50 songs more than once, or played no song at least 10 times were discarded for the experiment to ensure that sufficient test and training data for cross-validation were available. After this procedure 735 participants remained. In order to avoid bias introduced by large play counts, these were capped at a value of 10. The evaluation task was to predict play counts for songs in the range [1,10] for each of the 735 users. For each user, 10-fold cross validation was used to split the available play counts into training and validation sets. The Mean Absolute Error (MAE) and the standard deviation σ of errors, averaged over all users u ∈U and scaled by the total count of evaluated instances n were computed for each recommender r. The MAE indicates the mean deviation of predicted from observed values and is the most frequently used recommender algorithm evaluation measure [4]. MAEr = 1 n X u X i ∥Ru,i −Pr,u,i∥ (3) In this initial experiment, the item-based social recommender reached a MAE of 2.608 (2.736) and σ 3.374, the content-based recommender reached a MAE of 2.884 (2.993) and σ 3.651, and the hybrid recommender a MAE of 2.349 1 and σ 2.817. We attribute the higher measured MAE for the social recommender compared to MAE values for other, ratings-based datasets (such as [4]) to the usage of implicit listening data and the high sparsity of the dataset. The presented hybrid recommender reduces the MAE by 0.259 compared to the best performing base recommender. 5 CONCLUSION AND FUTURE WORK We have introduced a hybrid recommender that uses a combination rule adapted from ensemble learning methods to combine base recommenders. In a preliminary experiment performed on observed listening data the technique leads to a MAE that is about 10% smaller than that of the best base recommender. This implies that a degree of diversity between social and content-based recommenders exists that can be exploited. Future work will focus on integrating more hetereogeneous data sources, applying diversification techniques on them, developing specifically suited combination rules for hybrid music recommenders, and collecting explicit user ratings for a comparative evaluation of such approaches. 6 REFERENCES [1] R. Burke. ”Hybrid Recommender Systems: Survey and Experiments.” User Modeling and User-Adapted Interaction, Volume 12, Issue 4, 2002, 331-370. [2] L. Kuncheva, J. Bedzek and R. Duin. ”Decision Templates for Multiple Classifier Fusion: an Experimental Comparison” Pattern Recognition, Volume 34, Issue 2, 2001, 299-314. [3] R. Polikar. ”Ensemble Based Systems in Decision Making.” IEEE Systems Magazine, Issue 3, 2006, 2145. [4] B. Sarwar, G. Karypis, J. Konstan and J. Riedl. ”Itembased Collaborative Filtering Recommendation Algorithms.” Proc. 10th International World Wide Web Conference, 2001. [5] F. Vignoli and S. Pauws. ”A Music Retrieval System Based on User-Driven Similarity and its Evaluation.” Proc. Sixth International Symposium on Music Information Retrieval, 2005. 1 MAE values for the base recommenders are rounded to the nearest possible play count to compensate for the inherent rounding effect of the decision template combination rule; original values are given in brackets.",
        "zenodo_id": 1417781,
        "dblp_key": "conf/ismir/TiemannPV07",
        "keywords": [
            "ensemble learning",
            "hybrid music recommenders",
            "social and content-based",
            "combination rule",
            "recommendation paradigms",
            "recommendation accuracy",
            "combination methods",
            "weak learners",
            "regression or classification tasks",
            "combination output"
        ],
        "content": "ENSEMBLE LEARNING FOR HYBRID MUSIC RECOMMENDATION\nMarco Tiemann, Steffen Pauws, Fabio Vignoli\nPhilips Research Europe\nHigh Tech Campus 34\n5656 AE Eindhoven, The Netherlands\n{marco.tiemann, steffen.pauws, fabio.vignoli }@philips.com\nABSTRACT\nWe investigate ensemble learning methods for hybrid mu-\nsic recommenders, combining a social and a content-based\nrecommender algorithm in an initial experiment by apply-\ning a simple combination rule to merge recommender re-\nsults. A ﬁrst experiment suggests that such a combination\ncan reduce the mean absolute prediction error compared\nto the used recommenders’ individual errors.\n1 INTRODUCTION\nMusic recommender systems that are publicly available\ntoday apply one of two recommender paradigms: social\nrecommenders predict preferences based purely on user\npreference data. Content-based recommenders represent\nsongs as feature vectors, where the features can be as-\nsigned manually or extracted automatically directly from\naudio data. Users are characterized by proﬁles, and songs\nthat are similar to a user’s proﬁle are recommended. Hy-\nbrid recommenders combine recommender paradigms to\nimprove the overall accuracy of predictions and reduce\nproblems of speciﬁc recommender algorithms. To this\nend, hybrid recommenders usually combine social and con-\ntent -based recommendation using one of a number of pos-\nsible combination methods [1]. They can be categorized\ninto recommenders that 1) integrate social and content-\nbased recommendation in a single algorithm or 2) com-\nbine the outputs of independent recommender algorithms.\nWe follow the second approach. Our goal is to integrate\nmany distinct independent recommenders (referred to as\nbase recommenders in this paper) ﬂexibly in a generalized\nhybrid recommender, using ensemble learning methods to\ncreate diverse base recommenders and to combine their\noutput with established and novel combination methods.\n2 APPLYING ENSEMBLE LEARNING\nInensemble learning [3], several weak learners are cre-\nated and used in regression or classiﬁcation tasks. Their\nindividual output is combined into a uniﬁed single output\nby applying a combination rule. Ensemble learning meth-\nods can often outperform single strong learners in practi-\nc/circlecopyrt2007 Austrian Computer Society (OCG).cal applications. However, ensemble methods can only be\napplied successfully when the used weak learners are suf-\nﬁciently diverse – weak learners must be able to correct er-\nrors made by other weak learners. Hence many ensemble\nlearning methods actively select or create suitably diverse\nweak learners by techniques such as training data resam-\npling. Some ensemble learning methods can also itera-\ntively adapt to newly presented evidence by modifying ei-\nther combination rules or weak learners as new evidence is\nsuccessively presented. Ensemble learning is a promising\nconcept for hybrid music recommendation. It can be used\nto ﬂexibly integrate heterogeneous data sources and dif-\nferent algorithms. Weak learner diversiﬁcation methods\nand combination rules with well explored properties are\nreadily available in existing work on ensemble learning,\nand iterative algorithms can be used to optimize recom-\nmender accuracy as additional observations of user pref-\nerences becomes available.\n3 A HYBRID MUSIC RECOMMENDER\nIn this paper we focus on combining recommender output.\nWe investigate to what extend commonly used social and\ncontent-based recommenders provide sufﬁciently diverse\noutput to improve recommendations by combining them\nwithout applying diversiﬁcation methods. A frequently\nused social recommender method is item-based collabo-\nrative ﬁltering [4]. Using this method, ﬁrst the similarity\nbetween two items iandjis computed using preference\ndata of all users u∈Uconsidered. The Pearson correla-\ntion coefﬁcient is frequently used to determine this simi-\nlarity:\ns(i, j) =/summationtext\nu/parenleftbig\nRu,i−¯Ri/parenrightbig/parenleftbig\nRu,j−¯Rj/parenrightbig\n/radicalBig/summationtext\nu/parenleftbig\nRu,i−¯Ri/parenrightbig2/summationtext\nu/parenleftbig\nRu,j−¯Rj/parenrightbig2(1)\nfor all users u∈U, where Ru,i,Ru,jare preferences of\nuserufor items iandjrespectively, and ¯Ri,¯Rjare mean\npreferences for these items. Then, a predicted preference\nPu,ifor a target item iis computed as the weighted sum\nof preference values of user ufor all items j∈Jthat are\ncorrelated to item iand for which the user i’s preferences\nare known, scaled by the sum of similarity terms:\nPu,i=/summationtext\njs(i, j)Ru,j/summationtext\njs(i, j)(2)A content-based recommender can be implemented simi-\nlarly to this social recommender algorithm. We ﬁrst con-\nstruct a similarity matrix of songs using a song similarity\nmeasure introduced in [5]. Analogous to the social recom-\nmendation algorithm described above, recommendations\nare computed as given in Formula 2 using the referenced\ncontent-based similarity function s(i, j)instead of the cor-\nrelation of user preference data.\nA combination rule using decision templates [2] is ap-\nplied to combine the base recommender output. A de-\ncision template is a record of weak learner estimates for\nan observed value. It can be expressed as a decision rule\nin the form {Pr1, ..., P rn} → Rfor a set of base recom-\nmenders {r1, ..., r n}, where Prnis the prediction of rec-\nommender nandRis the observed preference for a par-\nticular recommendation. For classiﬁcation, decision tem-\nplates are created as averaged class probabilities over all\ninstances classiﬁed into a particular class. This method\nhas been shown to perform well for different classiﬁcation\ntasks [2]. For the regression task of predicting preference\nvalues, we apply a variant of this method. Decision tem-\nplate rules are retained without averaging them for each\nuser. To combine base recommenders for a new instance,\ntheir individual predictions are computed. Then the Eu-\nclidean distance of the vector of computed results to each\nstored decision template rule head is computed, and the\nnearest neighbor rule’s tail (the observed preference) is\nthe resulting preference prediction.\n4 EXPERIMENT\nWe conducted a preliminary experiment to evaluate the\nperformance of the described hybrid recommender. For\nthis a music dataset containing a collection of 63,949 pop-\nular music pieces was used. For each song manually as-\nsigned metadata are provided and perceptual audio fea-\ntures were extracted for the content-based recommender\ndescribed in Section 3. The dataset contains 1,139,979\nplay counts , the number of times that a song has been\nplayed by a user, for 6,939 participants. Participants that\nplayed less than 100 distinct songs, played less than 50\nsongs more than once, or played no song at least 10 times\nwere discarded for the experiment to ensure that sufﬁcient\ntest and training data for cross-validation were available.\nAfter this procedure 735 participants remained. In order\nto avoid bias introduced by large play counts, these were\ncapped at a value of 10.\nThe evaluation task was to predict play counts for songs\nin the range [1,10] for each of the 735 users. For each\nuser, 10-fold cross validation was used to split the avail-\nable play counts into training and validation sets. The\nMean Absolute Error (MAE) and the standard deviation\nσof errors, averaged over all users u∈Uand scaled by\nthe total count of evaluated instances nwere computed for\neach recommender r. The MAE indicates the mean devi-\nation of predicted from observed values and is the most\nfrequently used recommender algorithm evaluation mea-sure [4].\nMAE r=1\nn/summationdisplay\nu/summationdisplay\ni/bardblRu,i−Pr,u,i/bardbl (3)\nIn this initial experiment, the item-based social recom-\nmender reached a MAE of 2.608 (2.736) and σ3.374,\nthe content-based recommender reached a MAE of 2.884\n(2.993) and σ3.651, and the hybrid recommender a MAE\nof 2.3491andσ2.817. We attribute the higher measured\nMAE for the social recommender compared to MAE val-\nues for other, ratings-based datasets (such as [4]) to the\nusage of implicit listening data and the high sparsity of\nthe dataset. The presented hybrid recommender reduces\nthe MAE by 0.259 compared to the best performing base\nrecommender.\n5 CONCLUSION AND FUTURE WORK\nWe have introduced a hybrid recommender that uses a\ncombination rule adapted from ensemble learning meth-\nods to combine base recommenders. In a preliminary ex-\nperiment performed on observed listening data the tech-\nnique leads to a MAE that is about 10% smaller than that\nof the best base recommender. This implies that a de-\ngree of diversity between social and content-based rec-\nommenders exists that can be exploited. Future work will\nfocus on integrating more hetereogeneous data sources,\napplying diversiﬁcation techniques on them, developing\nspeciﬁcally suited combination rules for hybrid music rec-\nommenders, and collecting explicit user ratings for a com-\nparative evaluation of such approaches.\n6 REFERENCES\n[1] R. Burke. ”Hybrid Recommender Systems: Survey\nand Experiments.” User Modeling and User-Adapted\nInteraction, Volume 12, Issue 4, 2002, 331-370.\n[2] L. Kuncheva, J. Bedzek and R. Duin. ”Decision Tem-\nplates for Multiple Classiﬁer Fusion: an Experimental\nComparison” Pattern Recognition, Volume 34, Issue 2,\n2001, 299-314.\n[3] R. Polikar. ”Ensemble Based Systems in Decision\nMaking.” IEEE Systems Magazine, Issue 3, 2006, 21-\n45.\n[4] B. Sarwar, G. Karypis, J. Konstan and J. Riedl. ”Item-\nbased Collaborative Filtering Recommendation Algo-\nrithms.” Proc. 10th International World Wide Web\nConference, 2001.\n[5] F. Vignoli and S. Pauws. ”A Music Retrieval System\nBased on User-Driven Similarity and its Evaluation.”\nProc. Sixth International Symposium on Music Infor-\nmation Retrieval, 2005.\n1MAE values for the base recommenders are rounded to the nearest\npossible play count to compensate for the inherent rounding effect of the\ndecision template combination rule; original values are given in brackets."
    },
    {
        "title": "Strike-A-Tune: Fuzzy Music Navigation Using a Drum Interface.",
        "author": [
            "Adam R. Tindale",
            "David W. Sprague",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418243",
        "url": "https://doi.org/10.5281/zenodo.1418243",
        "ee": "https://zenodo.org/records/1418243/files/TindaleST07.pdf",
        "abstract": "A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shuffles are used when users have no particular music in mind. We present a new interface and visualization system called StrikeA-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shuffles. We have designed an interface using an electronic drum to communicate with a visualization and playback system.",
        "zenodo_id": 1418243,
        "dblp_key": "conf/ismir/TindaleST07",
        "keywords": [
            "traditional music library system",
            "mouse and keyboard control",
            "song selection",
            "randomized playlist",
            "fuzzy navigation",
            "imprecise navigation",
            "user preference",
            "music library",
            "visualization system",
            "electronic drum"
        ],
        "content": "Strike-A-Tune: Fuzzy Music Navigation Using a Drum Interface\nAdam R. Tindale, David Sprague, and George Tzanetakis\nDepartment of Computer Science\nUniversity of Victoria\nart,dsprague,gtzan@csc.uvic.ca\nAbstract\nA traditional music library system controlled by a mouse\nand keyboard is precise, allowing users to select their de-\nsired song. Alternatively, randomized playlist or shufﬂes\nare used when users have no particular music in mind. We\npresent a new interface and visualization system called Strike-\nA-Tune for fuzzy music navigation. Fuzzy navigation is\nan imprecise navigation approach allowing users to choose\npreference related items. We believe this will help users to\nplay music they want to hear and re-discover infrequently\nplayed songs in their music library, thus combining the best\naspects of precision navigation and shufﬂes. We have de-\nsigned an interface using an electronic drum to communi-\ncate with a visualization and playback system.\n1. Motivation\nThere are many pieces of software available for organizing\nand playing music. With rare exception, these programs uti-\nlize the keyboard and mouse, devices that are very precise,\nfor navigating the music collection. For large collections\nof music, problems arise. Users will more often pick their\nfavourite artists over others. The other typical method of\nnavigation is random mode or the iTunes “Party Shufﬂe.”\nWe propose a new type of navigation: fuzzy navigation .\nPeople are often in the mood for music they like, without be-\ning concerned about the particular song playing. Two web-\nsites, Pandora.com and last.fm, have become very popular\nby playing music based on user-guided feedback. Strike-A-\nTune puts the user in direct control of the navigation utiliz-\ning the user’s own collection. The interface’s imprecision\nintroduces adjustable navigation variability. Due to the tac-\ntile feedback of a drum, the user is aware of the instrument\nin his or her hands [1].\n2. Previous Work\nTwo existing systems [3, 6] use a personal MP3 player to\nadjust an exercise routine by selecting music based on the\nuser’s performance. These interfaces provide fuzzy music\nnavigation but none are designed for music re-discovery.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of VictoriaThe Music Rainbow system [7], despite also using a circu-\nlar interface based on song similarity, is designed for music\ndiscovery and permits precision music selection.\nMurdoch and Tzanetakis’ [5] radio drum research looked\nat using a drum to navigate large music collections using\nrhythm based navigation and SOM dimensionality reduc-\ntion. For dimensionality reduction, 3D song positions would\nbe accessed by the 3D position of the drum sticks.\nThe CircleView system by Keim et al. [4] uses a visually\nsimilar approach to ours with segments of a circle repre-\nsenting an attribute, distance from the exterior representing\ntime, and colour representing the average data value for that\ntime. Appan et al.[2] also use the analogy of water ripples\nto display communication patterns over time.\n3. Drum Interface\nElectronic drum pads come equipped with a piezo sensor\nthat can transduce the movement of the drumhead into an\naudio signal. The drum software interface, implemented in\nMarsyas [10] takes an audio stream and segments it into dis-\ncrete hits. Once a hit is detected, a ﬁxed window of 4096\nsamples are captured to be processed. This window is sent\nto the feature extraction section of the software where the re-\nsulting feature vector is delivered to multiple Gaussian clas-\nsiﬁers. The output of the classiﬁers is collected and sent\nto the visualization via Open Sound Control. The system is\nbased on a real-time implementation of the system described\nin [9].\nThe categories of gesture used in the system are: Ra-\ndial Position(0-1) and Magnitude (0-1). The radial position\nrefers to the point on the drumhead where the drum was\nstruck. When classifying the difference between Normal\nand Rim, the system achieved 85.0% accuracy. Radial ac-\ncuracy was tested by dividing the radius of the drum into 2,\n3, and 4 section; the system achieved an accuracy of 89%,\n81.2%, and 73.2% respectively.\n4. The Disk Visualization\nThe disk visualization presents rings of song data with the\ncurrently playing/chosen song in the center of the disk (see\nFigure 1(i)). Rings closer to the center have fewer songs\npositioned on the ring but more song details. Shneider-\nman’s information visualization mantra of “Overview, zoom\nand ﬁlter, details-on-demand” is thus supported [8]. An\noverview of the music is provided by outside rings, songdata is ﬁltered based on the currently playing song, and\nsong details are provided on demand by highlighting the\nsong. Songs on a given ring are currently positioned evenly\naround the ring in song segments. The current implementa-\ntion presents 1 song in the center, 10 songs on the ﬁrst ring,\n100 songs on the second ring, and 1000 songs on the outside\nring. Ring number thus conveys 1 dimension of informa-\ntion while position on the ring convey a second dimension.\nRing position and segment order can also convey a single\nvariable, like song similarity (used in the current implemen-\ntation). Currently, songs on a ring are positioned according\nto similarity to the center song. Songs on rings closer to the\ncenter are also more similar to the center song than songs\non outer rings. Each song segment is coloured according to\na third nominal dimension (eg. genre). Strike-A-Tune sup-\nports up to three dimensions of song data for each library\nentry, in addition to traditional text based information.\nFigure 1. When a selection strike occurs, the selected song’s de-\ntailed information is displayed and a blue bar animates around\nthe selected ring until it arrives at the chosen song (ii). The cho-\nsen song is then highlighted (iii).\nThe disk visualization intuitively maps drum strike in-\nformation to visualization interactions. The radial position\nfrom the drum center of a simple “selection” strike maps to\nthe radius position on the disk. Radius positions (between 0\nand 1) are then discretized to a ring number. Strike magni-\ntude (0-1) represents a rotation around the chosen ring be-\ntween 0 and 360 degrees. A “test of strength” analogy is\nused to convey how magnitude impacts a selection. A zero\ndegree rotation selects ring segments aligned with the x axis\nof the center song. Stronger hits cause an animation to rotate\nclockwise around the ring until the chosen song is selected.\nThis animation both attracts attention to the selected ring\nsegment (movement is perceptually pre-attentive) and rein-\nforces the “test of strength” analogy (similar to a carnival\nhigh striker game). See Figure 1 for details.\nThe metadata of one of the authors 3207 song personal\nmusic library was outputted to a tab delimited text ﬁle. Each\nsong had 25 different pieces of metadata used to compute\ndistance metrics between songs. We chose to implement\nfuzzy music navigation based on song similarity. Each meta-\ndata category was weighted based on importance and string\nsimilarity was either determined by exact string matching or\nthe percentage of characters in common.5. Discussion\nThe Strike-A-Tune system offers many advantages compared\nto conventional shufﬂe or precise mouse and keyboard based\nmusic navigation systems (ie. precise navigation). Unlike\nboth precise and random music navigation, the beneﬁts of\nfuzzy navigation improves as the library size increases. Songs\nthe user has forgotten about have a chance at being played\n(unlike using precision navigation) but only songs similar\nthe user’s current preference will be played (unlike a ran-\ndomized playback). The disk visualization provides an overview\nof music data and details for songs in focus. Finally, the\nintuitive nature of the interface makes interacting with the\nsystem as simple as using a traditional mouse and keyboard,\nbut allows us to expand our interface in new directions of\nmusic exploration.\nReferences\n[1] Miguel Bruns Alonso and David V . Keyson. Musiccube:\nmaking digital music tangible. In CHI ’05: CHI ’05 ex-\ntended abstracts on Human factors in computing systems ,\npages 1176–1179. ACM Press, 2005.\n[2] Preetha Appan, Hari Sundaram, and Belle Tseng. Summa-\nrization and visualization of communication patterns in a\nlarge social network. In The 10th Paciﬁc-Asia Conference\non Knowledge Discovery and Data Mining , 2006.\n[3] Greg T. Elliott and Bill Tomlinson. PersonalSoundtrack:\ncontext-aware playlists that adapt to user pace. In CHI ’06:\nCHI ’06 extended abstracts on Human factors in computing\nsystems , pages 736–741. ACM Press, 2006.\n[4] Daniel A. Keim, J ¨orn Schneidewind, and Mike Sips. Circle-\nview: a new approach for visualizing time-related multidi-\nmensional data sets. In AVI ’04: Proceedings of the working\nconference on advanced visual interfaces , pages 179–182.\nACM Press, 2004.\n[5] Jennifer Murdoch and George Tzanetakis. Interactive\ncontent-aware music browsing using the radio drum. In Pro-\nceedings of the IEEE International Conference on Multime-\ndia and Expo , 2006.\n[6] Nuria Oliver and Fernando Flores-Mangas. MPTrain: a\nmobile, music and physiology-based personal trainer. In\nMobileHCI ’06: Proceedings of the 8th conference on\nHuman-computer interaction with mobile devices and ser-\nvices , pages 21–28. ACM Press, 2006.\n[7] Elias Pampalk and Masataka Goto. MusicRainbow: A new\nuser inferace to discover artists using audio-based similarity\nand web-based labeling, 2006.\n[8] Ben Shneiderman. The eyes have it: A task by data type\ntaxonomy for information visualizations. In VL ’96: Pro-\nceedings of the 1996 IEEE Symposium on Visual Languages ,\npage 336. IEEE Computer Society, 1996.\n[9] Adam R. Tindale, Ajay Kapur, George Tzanetakis, and\nIchiro Fujinaga. Retrieval of percussion gestures using tim-\nbre classiﬁcation techniques. In Proceedings of the 5th Inter-\nnational Conference on Music Information Retrieval , pages\n541–544, 2004.\n[10] George Tzanetakis. Marsyas: A framework for audio analy-\nsis.Organized Sound , 4(3):169–175, 2000."
    },
    {
        "title": "Identifying Words that are Musically Meaningful.",
        "author": [
            "David A. Torres",
            "Douglas Turnbull",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417175",
        "url": "https://doi.org/10.5281/zenodo.1417175",
        "ee": "https://zenodo.org/records/1417175/files/TorresTBL07.pdf",
        "abstract": "A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is inconsistently used by human annotators, or the word is not clearly represented by the underlying acoustic representation, the word can be considered as noisy and should be removed from the vocabulary to denoise the modeling process. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA) . Experimental results illustrate that, by identifying musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval. 1 INTRODUCTION Over the past two years we have been developing a computer audition system that can annotate songs with semantically meaningful words and retrieve relevant songs based on a text query. This system learns a joint probabilistic model between a vocabulary of words and acoustic feature vectors using a heterogeneous data set of song and song annotations. However, if a specific word is inconsistently used when annotating songs or is not represented well by the acoustic features, the system will not be able to model this noisy word well. In this paper, we explore the problem of vocabulary selection for semantic music annotation and retrieval. We will consider two concepts, human agreement and acoustic correlation, as indicators for picking candidate words. Previously, we collected semantic annotations of music using various methods: text-mining song reviews [15], conducting a human survey [16], and exploring the use of a human computation game [17, 19]. In all cases, we are forced to choose a vocabulary using ad-hoc methods. For example, text-mining the song reviews resulted in a list of over 1,000 candidate words which the authors manually pruned if there was a general consensus that a word was not ‘musically-relevant’. To collect the survey and game data, we built, a priori, a two-level hierarchical vocabulary by first considering a set of high-level semantic c⃝2007 Austrian Computer Society (OCG). categories (‘Instrumentation’, ‘Emotion’, ‘Vocal Characteristic’, ‘Genre’) and then listing low-level words (‘Electric Guitar’, ‘Happy’, ‘Breathy’, ‘Bebop’) for each semantic category. In both cases, a vocabulary required manual construction and included some noisy words that degraded the performance of our computer audition system. In this paper, we highlight two potential reasons why a word causes problems for our system. The first is related to the notion that aspects of music are subjective. That is, two individual listeners will use different words to describe the same piece of music. For example, a pre-teen girl might consider a Backstreet Boys song to be ‘touching and powerful’ whereas a dj at an indie radio station may consider it ‘abrasive and pathetic’. If we consider them as one population, the annotations will be in conflict with one another. To address this issue we introduce in Section 2 a measure of human agreement to evaluate how consistently our population uses a word to label a large set of songs. A second reason a word may be hard to model involves the expressive power of our chosen audio feature representation. For example, if we are interested in words related to long-term music structure (e.g., ‘12-bar blues’) and we only represent the audio using short-term (  0] (2) where I is an indicator function that is 1 if Aw,s is greater then zero, and 0 otherwise. That is, all word-song pairs are valid except the word-song pair that nobody associates with one another. We expect human agreement to be close to 1 for more ‘objective’ words such as words associated with instrumentation (‘cow bell’), and close to 0 for words that are more ‘subjective’ such as those that related to song usages (‘driving music’). 3 ACOUSTIC CORRELATION WITH CCA Canonical Correlation Analysis, or CCA, is a method of exploring dependencies between data which are represented in two different, but related, vector spaces. For example, consider a set of songs where each song is represented by both a semantic annotation vector and an audio feature vector. An annotation vector for a song is a real-valued (or binary) vector where each element represents the strength of association (e.g., Equation 1) between the song and a word from our vocabulary. An audio feature vector is a real-valued vector of statistics calculated from the digital audio signal. It is assumed that the two spaces share some joint information which can be captured in the form of correlations between the music data that live in these spaces. CCA finds a one-dimensional projection of the data in each space such that the correlations between the projections is maximized. More formally, consider two data matrices, A and S, from two different feature spaces. The rows of A contain music data represented in the audio feature space A. The corresponding rows of S contain the music data represented in the semantic annotation space S (e.g., annotation vectors). CCA seeks to optimize max wa∈A,ws∈S w′ aA′Sws (3) s.t. w′ aA′Awa = 1 w′ sS′Sws = 1. The objective in Problem 3 is the dot product between projections of data points. By itself, the objective function is unbounded since we can scale the w terms arbitrarily. Thus, we add the constraints to bound the length of the w terms and ensure the result is proportional to a correlation score. By analyzing the Lagrangian dual function of Problem 3, we find that it is equivalent to a pair of maximum eigenvalue problems, S−1 ss SsaS−1 aa Sasws = λ2ws (4) S−1 aa SasS−1 ss Ssawa = λ2wa (5) where \u0012 Saa Sas Ssa Sss \u0013 = \u0012 A′A A′S S′A S′S \u0013 and λ is the maximum of Problem 3. Note that the solution vector ws can be interpreted as a linear combination of words, learned from the music data, which are highly correlated with the audio representation. In the next section we modify Problem 3 so that a subset of words in our vocabulary is explicitly selected.",
        "zenodo_id": 1417175,
        "dblp_key": "conf/ismir/TorresTBL07",
        "keywords": [
            "musically meaningful vocabulary",
            "computer audition system",
            "semantic music annotation",
            "acoustic feature vectors",
            "joint probabilistic model",
            "human agreement",
            "acoustic correlation",
            "human survey",
            "human computation game",
            "musically-relevant"
        ],
        "content": "IDENTIFYING WORDS THAT ARE MUSICALLY MEANINGFUL\nDavid Torres1, Douglas Turnbull1, Luke Barrington2Gert Lanckriet2\nDept. of Computer Science and Engineering1\nDept. of Electrical and Computer Engineering2\nUniversity of California, San Diego\nABSTRACT\nAmusicallymeaningfulvocabularyisoneofthekeystones\ninbuildingacomputerauditionsystemthatcanmodelthe\nsemantics of audio content. If a word in the vocabulary is\ninconsistently used by human annotators, or the word is\nnot clearly represented by the underlying acoustic repre-\nsentation,thewordcanbeconsideredas noisyandshould\nbe removed from the vocabulary to denoise the model-\ning process. This paper proposes an approach to con-\nstruct a vocabulary of predictive semantic concepts based\nonsparse canonical component analysis (sparse CCA) .\nExperimental results illustrate that, by identifying musi-\ncallymeaningfulwords,wecanimprovetheperformance\nof a previously proposed computer audition system for\nmusic annotation and retrieval.\n1 INTRODUCTION\nOver the past two years we have been developing a com-\nputerauditionsystemthatcanannotatesongswithseman-\nticallymeaningfulwordsandretrieverelevantsongsbased\non a text query. This system learns a joint probabilistic\nmodel between a vocabulary of words and acoustic fea-\nture vectors using a heterogeneous data set of song and\nsong annotations. However, if a speciﬁc word is incon-\nsistentlyusedwhenannotatingsongsorisnotrepresented\nwell by the acoustic features, the system will not be able\nto model this noisyword well. In this paper, we explore\nthe problem of vocabulary selection for semantic music\nannotation and retrieval. We will consider two concepts,\nhuman agreement andacoustic correlation , as indicators\nfor picking candidate words.\nPreviously, we collected semantic annotations of mu-\nsicusingvariousmethods: text-miningsongreviews[15],\nconductingahumansurvey[16],andexploringtheuseof\na human computation game [17, 19]. In all cases, we are\nforced to choose a vocabulary using ad-hoc methods. For\nexample, text-mining the song reviews resulted in a list\nof over 1,000 candidate words which the authors manu-\nally pruned if there was a general consensus that a word\nwas not ‘musically-relevant’. To collect the survey and\ngame data, we built, a priori, a two-level hierarchical vo-\ncabulary by ﬁrst considering a set of high-level semantic\nc/circlecopyrt2007 Austrian Computer Society (OCG).categories (‘Instrumentation’, ‘Emotion’, ‘Vocal Charac-\nteristic’,‘Genre’)andthenlistinglow-levelwords(‘Elec-\ntricGuitar’,‘Happy’,‘Breathy’,‘Bebop’)foreachseman-\ntic category. In both cases, a vocabulary required manual\nconstructionandincludedsome noisywordsthatdegraded\nthe performance of our computer audition system.\nInthispaper,wehighlighttwopotentialreasonswhya\nword causes problems for our system. The ﬁrst is related\nto the notion that aspects of music are subjective. That\nis, two individual listeners will use different words to de-\nscribe the same piece of music. For example, a pre-teen\ngirlmightconsideraBackstreetBoyssongtobe‘touching\nand powerful’ whereas a dj at an indie radio station may\nconsider it ‘abrasive and pathetic’. If we consider them\nas one population, the annotations will be in conﬂict with\noneanother. ToaddressthisissueweintroduceinSection\n2 a measure of human agreement to evaluate how consis-\ntently our population uses a word to label a large set of\nsongs.\nAsecondreasonawordmaybehardtomodelinvolves\ntheexpressivepowerofourchosenaudiofeaturerepresen-\ntation. For example, if we are interested in words related\nto long-term music structure (e.g., ‘12-bar blues’) and we\nonlyrepresenttheaudiousingshort-term( <1sec)audio\nfeaturevectors,wemaybeunabletomodelsuchconcepts.\nAnother example is words that relate to a geographical\nassociation (e.g., ‘British Invasion’, ‘Woodstock’) which\nmayhavestrongculturalroots,butarepoorlyrepresented\nin the audio content.\nGivenanaudiofeaturerepresentation,wewouldliketo\nidentify the words that are represented well by the audio\ncontent before we try to model such words. To do this we\npropose the use of a method based on canonical correla-\ntion analysis (CCA) to measure acoustic correlation .\nCCAisamethodofexploringdependenciesacrosstwo\ndifferent, but related, vector spaces and has been used\nin applications dealing with multi-language text analysis\n[18], learning a semantic representations between images\nand text [4], and localizing pixels which are correlated\nwith audio from a video stream [6]. Similar to how prin-\ncipal component analysis (PCA) ﬁnds informative direc-\ntions in one feature space by maximizing the variance of\nprojected data, CCA ﬁnds directions (projections of the\ndata) across multiple spacesthat maximize correlation.\nGiven music data represented in both a semantic fea-\nture space and an acoustic feature space, we propose thatthese directions of high correlation can be used to ﬁnd\nwords that are strongly characterized by an audio repre-\nsentation. We do so by imposing constraints on CCA that\nexplicitly turn it into a vocabulary selection mechanism.\nThis CCA variant is called sparse CCA .\n2 HUMAN AGREEMENT\nRecently, we collected the Computer Audition Lab 500\n(CAL500) data set [16]: 500 songs by 500 unique artists\neachofwhichhasbeenannotatedaccordingtoa173-word\nvocabulary by a minimum of three individuals. Most of\nthe participants were paid, American, undergraduate stu-\ndents and the testing was conducted in a computer labo-\nratory at UC San Diego. We purposely collected multiple\nannotations for songs so that we could gauge how consis-\ntently a population of college students label music.\nUsingthisdataset,wecancalculateastatisticwerefer\nto ashuman agreement for each word in our vocabulary.\nThe agreement of a word-song pair ( w, s) is:\nAw,s=#(positive associations )w,s\n#(annotations )s.(1)\nFor example, if 3 out of 4 students label Elvis Presley’s\n‘Heartbreak Hotel’ as beinga ‘blues’ songs then\nA‘blues’, ‘heartbreak hotel’ = 0.75. We calculate the human\nagreement for a word by averaging over all the songs in\nwhich at least one subject has used the word to describe\nthe song. This can be written as\nAw=/summationtext\nsAw,s/summationtext\nsI[Aw,s>0](2)\nwhere Iisanindicatorfunctionthatis1if Aw,sisgreater\nthen zero, and 0 otherwise. That is, all word-song pairs\narevalidexcepttheword-songpairthatnobodyassociates\nwithoneanother. Weexpecthumanagreementtobeclose\nto 1 for more ‘objective’ words such as words associated\nwithinstrumentation(‘cowbell’),andcloseto0forwords\nthataremore‘subjective’suchasthosethatrelatedtosong\nusages (‘driving music’).\n3 ACOUSTIC CORRELATION WITH CCA\nCanonical Correlation Analysis, or CCA, is a method of\nexploringdependenciesbetweendatawhicharerepresented\nin two different, but related, vector spaces. For example,\nconsider a set of songs where each song is represented by\nboth asemantic annotation vector and anaudio feature\nvector. Anannotationvectorforasongisareal-valued(or\nbinary)vectorwhereeachelementrepresentsthestrength\nof association (e.g., Equation 1) between the song and a\nword from our vocabulary. An audio feature vector is a\nreal-valued vector of statistics calculated from the digi-\ntal audio signal. It is assumed that the two spaces share\nsomejointinformationwhichcanbecapturedintheform\nof correlations between the music data that live in these\nspaces. CCA ﬁnds a one-dimensional projection of thedata in each space such that the correlations between the\nprojections is maximized.\nMore formally, consider two data matrices, AandS,\nfrom two different feature spaces. The rows of Acon-\ntain music data represented in the audio feature space A.\nThecorrespondingrowsof Scontainthemusicdatarepre-\nsentedinthesemanticannotationspace S(e.g.,annotation\nvectors). CCA seeks to optimize\nmax\nwa∈A,ws∈Sw/prime\naA/primeSws (3)\ns.t. w/prime\naA/primeAwa= 1\nw/prime\nsS/primeSws= 1.\nTheobjectiveinProblem3isthedotproductbetweenpro-\njections of data points. By itself, the objective function\nis unbounded since we can scale the wterms arbitrarily.\nThus,weaddtheconstraintstoboundthelengthofthe w\ntermsandensuretheresultisproportionaltoacorrelation\nscore.\nByanalyzingtheLagrangiandualfunctionofProblem\n3,weﬁndthatitisequivalenttoapairofmaximumeigen-\nvalue problems,\nS−1\nssSsaS−1\naaSasws=λ2ws (4)\nS−1\naaSasS−1\nssSsawa=λ2wa (5)\nwhere/parenleftbiggSaaSas\nSsaSss/parenrightbigg\n=/parenleftbiggA/primeA A/primeS\nS/primeA S/primeS/parenrightbigg\nandλis the\nmaximum of Problem 3.\nNotethatthesolutionvector wscanbeinterpretedasa\nlinearcombinationofwords,learnedfromthemusicdata,\nwhicharehighlycorrelatedwiththeaudiorepresentation.\nIn the next section we modify Problem 3 so that a subset\nof words in our vocabulary is explicitly selected.\n3.1 Sparse CCA\nThe solution vectors, waandws, in Problem 3 can be\nconsidered dense since most of the elements of each vec-\ntor will be non-zero. In many applications it may be of\ninteresttolimitthenumberofnon-zeroelementsinthe w\nterms. This may aid in the interpretability of the result,\nparticularly when the coordinate axes of a vector space\nhaveadirectmeaning. Forexample,inbioinformaticsex-\nperiments, the input space may contain thousands of co-\nordinate axes corresponding to individual genes. We may\nwish to perform sparse analysis if we suspect that some\nphenomenon is dependent on a handful of these genes. In\nthis paper, we impose sparsity on the solution vector ws,\ncorresponding to the semantic space where each coordi-\nnate axis describes a word. Our goal is to ﬁnd a subset of\nwords in a vocabulary that are highly correlated to audio.\nWe expect that these words may be more objective than\nothers inthe vocabularysince theyare potentiallycharac-\nterized by correlations with the underlying audio signal,\nandthus,usingthesewordsmayimprovetheperformance\nof semantic music analysis systems.\nSparsity has been well studied in the ﬁelds of statis-\ntics and machine learning [22, 2, 14]. Imposing sparsityis theoretically achieved by constraining the zero norm of\na solution vector ||w||0, which is the number of non-zero\nelements in w(this is technically an abuse of terminol-\nogy as the zero norm is not a true mathematical norm).\nConstraining the zero-norm of a solution, because it is\na non-convex constraint, renders the problem intractable,\nNP hard in fact. Instead, most sparse methods relax the\ncardinalityconstraintsbyapproximatingthezero-normwith\na more mathematically tractable (i.e., convex) term such\nas the one-norm, ||w||1=/summationtext\ni|wi|[22] [2]. In this pa-\nper we approximate ||w||0by/summationtext\nilog(/epsilon1+|wi|), where\n0< /epsilon1/lessmuch1, avoids problems when one of the wiis zero.\nThis approximation has shown superior performance in\nsparse methods literature and it can be shown that the\ncardinality that results from this approximation is only\nO(1\nlog/epsilon1)greater than the zero-norm constrained solution\n[14] [20]. In practice we assume that /epsilon1is equal to ma-\nchine precision and set it to zero.\nWe impose sparsity on the semantic space Sby penal-\nizing the cardinality of the solution vector wsin Eq. 4.\nThis is equivalent to solving the problem\nmax\nws∈SS−1\nssSsaS−1\naaSasws−ρs/summationdisplay\nilog|ws,i|(6)\ns.t. w/prime\nsws= 1\nIfthelogtermintheobjectiveisremoved,theproblem\nissimplythevariationalcharacterizationoftheeigenvalue\nproblem in Eq. 4. The addition of the log term penal-\nizesthecardinalityofthesolutionvectorifitbecomestoo\nhigh. ρsmitigates how harsh that penalty is, so by setting\nρsone can control the sparsity of the solution.\nThenon-zeroelementsofthesparsesolutionvector ws\ncan be interpreted as those words which have a high cor-\nrelation with the audio representation. Thus, in the exper-\niments that follow, setting values of ρsand solving Prob-\nlem 6 reduces to a vocabularyselection technique.\nProblem 6 is still difﬁcult to solve because it requires\nmaximizing a convex objective. In other words, the prob-\nlem does not have a guaranteed global maximum. How-\never local solutions can be found by gradient descent or,\nas is done in our experiments, by solving a sequence of\nlinear approximations [14].\n4 REPRESENTING AUDIOAND SEMANTIC\nDATA\nIn this section we describe the audio and semantic rep-\nresentations, as well as describe the CAL500 [16] and\nWeb2131 [15] annotated music corpora that are used in\nour experiments. In both cases, the semantic information\nwillberepresentedusingasingleannotationvector swith\ndimension equal to the size of the vocabulary. The au-\ndiocontentwillberepresentedasmultiplefeaturevectors\n{a1, ...,aT}, where T depends on the length of the song.\nTheconstructionofthematrices AandStorunsparse\nCCA follows: Each feature vector in the music corpus is\nassociated with the label for its song. For example, fora given song, we duplicate its annotation vector sfor a\ntotal of Ttimes so that the song-label pair may be repre-\nsented as {(s,a1), ...,(s,aT)}. To construct Awe stack\nthefeaturevectorsforallsongsinthecorpusintoonema-\ntrix. Sis constructed by stacking all the corresponding\nannotation vectors into one matrix. If each song has ap-\nproximately600featurevectorsandwehave500hundred\nsongs, then both AandSwill have about 30,000 rows.\n4.1 Audio Representation\nEach song is represented as a bag-of-feature-vectors : we\nextract an unordered set of feature vectors for every song,\nby extracting one feature vector for each short-time seg-\nment of audio data. Speciﬁcally, we compute dynamic\nMel-frequency cepstral coefﬁcients (dMFCC) from each\nhalf-overlapping, medium-time ( ∼743 msec) segment of\naudio content [9].\nMel-frequency cepstral coefﬁcients (MFCC) describe\nthe spectral shape of a short-time audio frame in a con-\nciseandperceptuallymeaningfulwayandarepopularfea-\nturesforspeechrecognitionandmusicclassiﬁcation(e.g.,\n[11, 7, 13]). We calculate 13 MFCC coefﬁcients for each\nshort-time (23 msec) frame of audio. For each of the 13\nMFCCs, we take a discrete Fourier transform (DFT) over\nthe time series of 64 frames, normalize by the DC value\n(to remove the effect of volume) and summarize the re-\nsulting spectrum by integrating across 4 modulation fre-\nquency bands: (unnormalized) DC, 1-2Hz, 3-15Hz and\n20-43Hz. Thus, we create a 52-dimensional features vec-\ntor (4 features for each of the 13 MFCCs) for every 3/4\nsegment of audio content. For a ﬁve minute song, this\nresults in about 800 52-dimensional feature vectors.\nWe have also explored a number of alternative feature\nrepresentations, These include auditory ﬁlterbank tempo-\nral envelope [9], MFCCs (with and without instantaneous\nderivatives)[16],chromafeatures[3],andﬂuctuationpat-\nterns [10]. For our experiments we chose a DMFCC rep-\nresentation since it is compact compared with raw MFCC\nfeature representations and shows good performance on\nthe task of semantic music annotation and retrieval com-\npared with these other representations.\n4.2 Semantic Representation\nThe CAL500 is an annotated music corpus of 500 west-\nern popular songs by 500 unique artists. Each song has\nbeenannotatedbyaminimumof3individualsusingavo-\ncabulary of 174 words. We paid 66 undergraduate music\nstudents to annotate our music corpus with semantic con-\ncepts. Wecollectedasetofsemanticlabelscreatedspecif-\nically for a music annotation task. We considered 135\nmusically-relevant concepts spanning six semantic cate-\ngories: 29 instruments were annotated as present in the\nsong or not; 22 vocal characteristics were annotated as\nrelevant to the singer or not; 36 genres, a subset of the\nCodaich genre list [8], were annotated as relevant to the\nsong or not; 18 emotions, found by Skowronek et al. [12]\nto be both important and easy to identify, were rated onTop 3 words by semantic category\nAgreement Acoustic Correlation\noverall male lead vocals, drum set, female lead vocals rapping, at a party, hip-hop/rap\nemotion not angry/agressive, not weird, not tender/soft arousing/awakening, exciting/thrilling, sad\ngenre hip-hop/rap, electronica, world hip-hop/rap, electronica, funk\ninstrument male lead vocals, drum set, female lead vocals drum machine, samples, synthesizer\ngeneral electric texture, not danceable, high energy heavy beat, very danceable, synthesized texture\nusage driving, at a party, going to sleep at a party, exercising, gettingready to go out\nvocals rapping, emotional, strong rapping, strong, altered witheffects\nBottom 3 words by semantic category\nAgreement Acoustic Correlation\noverall at work, with the family, waking up not weird, not arousing, not angry/agressive\nemotion not powerful/strong, not emotional, weird not weird, not arousing, not angry/agressive\ngenre contemporary blues, roots rock, alternative folk classic rock, bebop, alternative folk\ninstrument trombone, tamborine, organ female lead vocals, drum set, acoustic guitar\ngeneral changing energy level, minor key tonality, low\nsong qualityconstant energy level, changing energy level, not\ncatchy\nusage at work, with the family, waking up going to sleep, cleaning the house, at work\nvocals falsetto, spoken, monotone high pitches, falsetto, emotional\nTable 1. Top and bottom 3 words by semantic category as calculated by agreement and acousticcorrelation.\na scale from one to three (e.g., ”not happy”, ”neutral”,\n”happy”); 15 song concepts describing the acoustic qual-\nities of the song, artist and recording (e.g., tempo, en-\nergy, sound quality); and 15 usage terms from [5], (e.g.,\n“Iwouldlistentothissongwhile driving,sleeping,etc. ”).\nThe 135 concepts are converted to the 174-word vocabu-\nlary by ﬁrst mapping bi-polar concepts to multiple word\nlabels(‘EnergyLevel’mapsto‘lowenergy’and‘highen-\nergy’). Then we prune all words that are represented in\nﬁve or fewer songs to remove under-represented words.\nLastly, we construct a real-valued 174-dimensional anno-\ntation vector by averaging the label frequencies of the in-\ndividualannotators. Detailsofthesummarizationprocess\ncan be found in [16]. In general, each element in the an-\nnotationvectorcontainsareal-valuedscalarindicatingthe\nstrength of association.\nTheWeb2131isanannotatedcollectionof2131songs\nandaccompanyingexpertsongreviewsminedfromaweb-\naccessiblemusicdatabase1[15]. Exactly363songsfrom\nWeb2131 overlap with the CAL500 songs. The vocab-\nulary consists of 317 words that were hand picked from\na list of the common words found in the corpus of song\nreviews. Common stop words are removed and the re-\nsulting words are preprocesses with a custom stemming\nalgorithm. We represent a song review as a binary 317-\ndimensional annotation vector. The element of a vector\nis 1 if the corresponding word appears in the song review\nand 0 otherwise.\n5 EXPERIMENTS\nBoth human agreement and acoustic correlation may be\nused to discover words that are musically meaningful and\n1AMG All Music Guide www.allmusic.comHuman Agr. Acoustic Cor.\nemotion 53.5 (26.7) emotion 127.9 (54.9)\ninstrument 53.9 (39.5) vocals 146.2 (50.0)\nvocals 88.2 (40.0) instrument 154.5 (39.9)\ngenre 118.6 (42.4) genre 156.7 (41.2)\nusage 152.3 (21.9) usage 162.5 (37.9)\nTable 2. Average rank of words in a semantic category\nwhen ranked by human agreement and acoustic correla-\ntion: Columnsaresorteddownwardinincreasingaverage\nrank. The average rank of the category and std. dev. are\nshown (lower is better). Note that the order of the cate-\ngories closely match across both columns.\nusefulinthecontextofsemanticmusicannotationandre-\ntrieval. In this section, we conduct three experiments to\nhighlight potential uses.\n5.1 Qualitative Analysis\nTable2showstheaveragerankofwordsinasemanticcat-\negory when words are ranked by human agreement and\nacoustic correlation. For human agreement, words are\nranked by their agreement score. For acoustic correla-\ntion,wordsarerankedbyhowlongtheyarekeptbysparse\nCCA as the vocabulary size is reduced. This experiment\nwas run on the CAL500 data set.\nA good rank in the human agreement metric suggests\nthat a word is less subjective. This is true by deﬁnition\nsince a good human agreement score means that people\nused that word consistently to describe music. Not sur-\nprisingly,wefoundthatmoreobjectivecategoriessuchas\ninstrumentation are highly ranked on this list and subjec-\ntive categories such as usage are ranked at the bottom.vocab.sz. 48824920314910350\n# CAL500 words 173118101856539\n# Web2131 words 315131102643811\n%Web2131 .64.52.50.42.36.22\nTable 3. The fraction of noisy web-mined words in a vo-\ncabularyasvocabularysizeisreduced: Asthesizeshrinks\nsparseCCAprunesnoisywordsandtheweb-minedwords\nare eliminated over higher quality CAL500 words.\nInterestingly, the ranks of semantic categories for the\nacoustic correlation metric matched closely with human\nagreement. Thisindicatesthatacousticcorrelationmaybe\nhoninginonobjectivewordstoo. Thiscouldbeexplained\nif acoustic correlation picks up on the same structure in\nthe audio that is being used by humans to make semantic\njudgements.\nFor a closer inspection at “musically relevant” words\ngivenbyourmethods,Table1showsthetop3andbottom\n3 ranked words for all semantic categories.\n5.2 Vocabulary Pruning using Sparse CCA\nSparse CCA can be used to perform vocabulary selection\nwherethegoalistoprunenoisywordsfromalargevocab-\nulary. To test this hypothesis we combined the vocabular-\niesfromtheCAL500andWeb2131datasetsandconsider\nthe subset of 363 songs that are found in both data sets.\nBased on our own informal user study, we found that\nthe Web2131 annotations are noisy as compared to the\nCAL500annotations. Weshowedsubjects10wordsfrom\neachdatasetandaskedthemwhichsetofwordswererel-\nevanttoasong. TheWeb2131annotationswerenotmuch\nbetter than selecting words randomly to from the vocabu-\nlary,whereasCAL500wordsweremostlyconsideredrel-\nevant.\nBecauseWeb2131wasfoundtobenoisierthanCAL500,\nwe expect sparse CCA to ﬁlter out more of the Web2131\nwords. That is, given progressively smaller vocabulary\nsizes, Web2131 should comprise a progressively smaller\nproportion of the vocabulary.\nTable 3 shows the results of this experiment. The ﬁrst\ncolumnofdatareﬂectsthevocabularyatfullsize. Thevo-\ncabularysizeis488andtheWeb2131vocabularyinitially\nout numbers the CAL500 words nearly two to one. Sub-\nsequent columns show the state of the vocabulary when\nvocabulary size is reduced from 203 words down to 50\nwords. Notice that the percentage of the Web2131 words\nthat comprise a vocabulary does, in fact, decrease. Noisy\nwords are being removed. If sparse CCA had no pref-\nerence for either Web2131 or CAL500 we would see a\nconstant proportion in Table3.\n5.3 Vocabulary Selection forMusic Retrieval\nHumanagreementandacousticrelevancecanalsobeused\nto prune noisy words from a vocabulary in order to im-\nprove the performance of semantic music analysis sys-\nFigure1. Comparisonofvocabularyselectiontechniques:\nWe compare vocabulary selection using human agree-\nment, acoustic correlation, and a random baseline, as it\neffects retrieval performance.\ntems. If a word is not well represented by the underlying\nacoustic and semantic representation, then attempting to\nmodel such a word will be fruitless.\nIn previous work, we have built a music retrieval sys-\ntem that can rank-order songs for a given word based on\nthe audio content [16]. At a high level, this system builds\nGaussianmixturemodelsofaudiofeaturevectorsforsongs\nassociatedwithagivenword. Thesemixturescanthenbe\napplied to the task of music retrieval.\nOne useful evaluation metric for this task is the area\nunder the ROC (AROC) curve. (A ROC curve is a plot of\nthetruepositiverateasafunctionofthefalsepositiverate\nas we move down a ranked list of songs.) For each word,\nthe AROC ranges between 0.5 for a random ranking and\n1.0 for a perfect ranking. Average AROC is found by av-\neraging AROC over all words in the vocabulary. Once a\nspeciﬁc vocabulary is set, we train our supervised model\nwith a training set of 450 of the 500 test songs from the\nCAL500 data set. Then we calculate average AROC per-\nformance using the 50 songs that were not used during\ntraining. TheaverageAROCscores,asafunctionofvary-\ning vocabulary size, are shown in Figure 1.\nWeﬁndthatpruningavocabularybothbasedonhuman\nagreement and acoustic correlation improve the retrieval\nperformance of our system. The performance of acous-\ntic correlation is markedly superior to a baseline method\nin which we randomly select words to be in a vocabulary.\nBased on experiment 5.2 these methods seem to remove\nnoisy words which are difﬁcult to model, and thus im-\nprove system performance.\n6 DISCUSSION\nWe have presented human agreement and acoustic corre-\nlation as metrics by which we can automatically, and in\nan unsupervised fashion, construct a musically meaning-ful vocabulary prior to building semantic models. Our re-\nsults indicate that vocabulary selection via these metrics\nremove noisy words that are difﬁcult to model and lead\nto improved performance of our semantic music analy-\nsis systems. In the absence of human labeled data where\nhuman agreement cannot be calculated, acoustic correla-\ntion using sparse CCA provides a principled approach for\npruning noisy words as is shown in Section 5.2.\nWhitman and Ellis have previously looked at vocabu-\nlary selection by training binary classiﬁers (e.g., Support\nVector Machines) on a heterogeneous data set of web-\ndocuments related to artists and the audio content pro-\nducedbytheseartists[21]. Byevaluatingtheperformance\nofeach‘word’classiﬁer,theyareabletorankorderwords\nfor the purpose of vocabulary selection. This idea is con-\nceptuallysimilartorankorderingwordsbyaverageAROC,\naswasshowninSection5.3,inthatbothevaluateavocab-\nulary a posteriori. i.e., after using a supervised learning\nframework. Moreover, such analysis evaluates the rele-\nvance of every word independent of every other word.\nInfutureresearch,wewillinvestigatetheimpactofus-\ningamusicallymeaningfulvocabularyforassessingsong\nsimilarity through semantic distance. We are interested in\ndeveloping a query-by-semantic-example system [1] for\nmusic which retrieves similar songs by ﬁrst representing\nthem in the semantic space and then rank-ordering them\nbased on a distance metric in that semantic space. We ex-\npectthathavingacompactsemanticrepresentation,which\ncan be found using sparse CCA, we will be able to im-\nprove retrieval performance.We also plan to explore the\npossibilityofextractingmeaningfulsemanticconceptsfrom\nweb-based documents through acoustic correlation.\n7 REFERENCES\n[1] LukeBarrington,AntoniChan,DouglasTurnbull,and\nGert Lanckriet. Audio information retrieval using se-\nmantic similarity. Technical report, 2007.\n[2] A. d’Aspremont, L. El Ghaoui, M.I. Jordan, and\nG.R.G. Lanckriet. A direct formulation for sparse pca\nusing semideﬁnite programming. Accepted for publi-\ncation in SIAM Review , 2006.\n[3] D. Ellis and G. Poliner. Identifying cover songs\nwith chroma features and dynamic programming beat\ntracking. IEEE ICASSP, 2007.\n[4] DavidR.Hardoon,SandorSzedmak,andJognShawe-\nTaylor. Canonical correlation analysis; an overview\nwith application to learning methods. Neural Compu-\ntation, 2004.\n[5] Xiao Hu, J. Stephen Downie, and Andreas F.\nEhmann. Exploiting recommended usage metadata:\nExploratory analyses. ISMIR, 2006.\n[6] Einat Kidron, Yoav Y. Schechner, and Michael Elad.\nPixels that sound. In IEEE Computer Vision and Pat-\ntern Recognition , 2005.[7] Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. ISMIR, 2000.\n[8] CoryMcKay,DanielMcEnnis,andIchiroFujinaga.A\nlarge publicly accessible prototype audio database for\nmusic research. ISMIR, 2006.\n[9] M. F. McKinney and J. Breebaart. Features for audio\nand music classiﬁcation. ISMIR, 2003.\n[10] E. Pampalk. Computational Models of Music Simi-\nlarity and their Application in Music Information Re-\ntrieval. PhD thesis, Vienna University of Technology,\nVienna, Austria, 2006.\n[11] L. Rabiner and B. H. Juang. Fundamentals of Speech\nRecognition . Prentice Hall, 1993.\n[12] Janto Skowronek, Martin McKinney, and Steven\nven de Par. Ground-truth for automatic music mood\nclassiﬁcation. ISMIR, 2006.\n[13] M. Slaney. Semantic-audio retrieval. IEEE ICASSP ,\n2002.\n[14] Bharath K. Sriperumbudur, David A. Torres, and Gert\nR. G. Lanckriet. Sparse eigen methods by d.c. pro-\ngramming. To appear in International Conference on\nMachine Learning , 2007.\n[15] D. Turnbull, L. Barrington, and G. Lanckriet. Mod-\nellingmusicandwordsusingamulti-classna ¨ıvebayes\napproach. ISMIR, 2006.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Towards musical query-by-semantic description\nusingtheCAL500dataset.In ToappearinSIGIR’07 ,\n2007.\n[17] D.Turnbull,R.Liu,L.Barrington,D.Torres,andGert\nLanckriet. UCSD CAL technical report: Using games\nto collect semantic information about music. Techni-\ncal report, 2007.\n[18] Alexei Vinokourov, John Shawe-Taylor, and Nello\nCristianini. Inferring a sementic representation of text\nvia cross-language correlation analysis. Advances in\nNeural Information Processing Systems , 2003.\n[19] LuisvonAhn.Gameswithapurpose. IEEEComputer\nMagazine , 39(6):92–94, 2006.\n[20] J.Weston,A.Elisseeff,B.Sch ¨olkopf,andM.Tipping.\nUse of the zero-norm with linear models and kernel\nmethods. Journalofmachinelearningresearch ,2003.\n[21] B. Whitman and D. Ellis. Automatic record reviews.\nISMIR, 2004.\n[22] HuiZou,TrevorHastie,andRobertTibshirani.Sparse\nprincipal component analysis. Journal of computa-\ntional and graphical statistics , 2004."
    },
    {
        "title": "A Game-Based Approach for Collecting Semantic Annotations of Music.",
        "author": [
            "Douglas Turnbull",
            "Ruoran Liu",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416464",
        "url": "https://doi.org/10.5281/zenodo.1416464",
        "ee": "https://zenodo.org/records/1416464/files/TurnbullLBL07.pdf",
        "abstract": "Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic information about songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., instruments, emotions, usages, genres) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback about the agreement amongst all players. We show that we can use the data collected during a two-week pilot study of Listen Game to learn a supervised multiclass labeling (SML) model. We show that this SML model can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content. 1 INTRODUCTION Collecting high-quality, semantic annotations of music is a difficult and time-consuming task. Previous methods have included hand-labeling music [3, 9], conducting surveys [14, 6, 8] and text-mining web documents [7, 15]. Each approach has drawbacks: human annotation methods are time consuming, costly, and as such, do not scale when attempting to annotate large music collections. Information mined automatically from web documents is often inconsistent with a true semantic description of the audio content. To collect large amounts of high quality annotation data at low cost, we propose using web-based games. von Ahn et. al. have created a suite of games (ESP Game [11], Peekaboom [13], Phetch [12]) for collecting semantic information about images. These ‘games with a purpose’ offer users an engaging platform for competition and collaboration while also collecting useful data about the image content. This data analysis technique is called human computation because it harnesses the collective intelligence of a large number of human participants to solve a c⃝2007 Austrian Computer Society (OCG). task that can not easily be automated. Using a game-based approach, a population of users can solve large problems (i.e., labeling all the images on the Internet) using voluntary contributions from individuals (i.e., playing a game to label a single image.) In this paper, we describe Listen Game, a multi-player, web-based game designed to collect associations between audio content and words. We show that this game is a powerful tool for collecting semantic music information by using the collected data to build a music information retrieval (MIR) application. In previous work [8], we presented a computer audition system that can automatically both annotate novel music with semantically meaningful words and retrieve relevant songs from a large database. Our system learns a supervised multi-class labeling (SML) model [1] by training on a set of audio content labeled with semantic annotations. We use the data collected from Listen Game to train our SML model. We then quantitatively evaluate the quality of the data by examining the accuracy of the SML model on the tasks of music annotation and retrieval. 2 COLLECTING MUSIC ANNOTATIONS A supervised learning approach to semantic music annotation and retrieval requires a large corpus of song-word associations. Early work in music classification (by genre [9, 5], emotion [4], instrument [2]) either used music corpora hand-labeled by the authors or made use of existing song metadata. While hand-labeling generally results in high quality labels, it does not easily scale to hundreds of labels per song over thousands of songs. Companies such as Pandora [14] employ dozens of musical experts whose full-time job is to tag songs with a large vocabulary of musically relevant words but, unfortunately, have little incentive to make their data publicly available. In [15], Whitman and Ellis collect a large number of web-documents and summarize their content using textmining techniques. From web-documents associated with artists, they learned binary classifiers for musically relevant words by associating words in the documents with the artists’ songs. In previous work [7], we mined expert music reviews associated with songs and demonstrated that we could learn a supervised multi-class labeling (SML) model over a large vocabulary of words. While web-mining is a more scalable approach than handlabeling, we found that the data collected was of low quality since the extracted words did not necessarily provide a good description of a song. In general, when writing reviews of songs, albums or artists, authors do not make explicit decisions about the relevance of each single word. In addition, many reviews contain social, historical or opinionated information that is not related to the song’s audio content [15]. A third approach uses surveys to collect semantic information about music. Moodlogic [6] customers annotate music using a standard survey containing questions about genre, instrumentation, emotional characteristics, etc. We used a similar approach [8] to collect the CAL500 data set of 500 songs, each of which has been annotated using a vocabulary of 174 words by a minimum of three people. Data collection took over 200 person-hours and resulted in approximately 300,000 individual word-song associations. Using a survey produced higher quality annotations than the web data but required that we pay test subjects for their time. Furthermore, surveys are tedious and time consuming. Despite financial motivation, test subjects quickly tire of lengthy surveys, resulting in inaccurate annotations. Human computation games motivate players to generate reliable annotations based on incentives built into the game. In the ESP Game [11] for example, a pair of unacquainted players are partnered up and each shown the same image. Both players are asked to “type what your partner is thinking”. Since they have no means of communicating, players invariably type words that have something to do with the common image they see. When two people independently suggest the same word to describe an image, the annotation is assumed to be reliable. Human computation games also address the issue of collecting lots of data by turning annotation into an entertaining task. The ESP Game has gathered over 10 million image annotations. Games build a sense of community and loyalty in users and can be highly addictive. Statistics from the ESP Game highlight that some people played in multiple 40 hour per week spans. Since they require little maintenance and run 24 hours a day, games can constantly collect new information from multiple players. Developing human computation games for annotating music is a useful approach for collecting semantic information. We believe that this approach has the potential for large-scale success because people enjoy talking about, sharing, discovering, arguing about and listening to music. 3 LISTEN GAME Image annotation often makes objective binary associations between an image and the objects (‘sailboat’), scene information (‘landscape’), and visual characteristics (‘red’) it represents. Our human computation game broaches the subjectivity inherent in many semantic labels that could be applied to music by allowing users to share their opinions, rather than be judged as correct or incorrect. Listen Game collects the strength of association between a word and a song, rather than an all-or-nothing binary label.",
        "zenodo_id": 1416464,
        "dblp_key": "conf/ismir/TurnbullLBL07",
        "keywords": [
            "human computation",
            "music domain",
            "semantic information",
            "music game",
            "music annotation",
            "music retrieval",
            "supervised multi-class labeling",
            "music information retrieval",
            "web-based games",
            "semantic music annotation"
        ],
        "content": "A GAME-BASED APPROACH FOR COLLECTING SEMANTIC\nANNOTATIONS OF MUSIC\nDouglas Turnbull1, Ruoran Liu1, Luke Barrington2, Gert Lanckriet2\nDept. of Computer Science and Engineering1\nDept. of Electrical and Computer Engineering2\nUniversity of California, San Diego\nABSTRACT\nGames based on human computation are a valuable\ntool for collecting semantic information about images. We\nshow how to transfer this idea into the music domain in\norder to collect high-quality semantic information about\nsongs. We present Listen Game , a online, multiplayer\ngame that measures the semantic relationship between\nmusic and words. In the normal mode, a player sees a\nlist of semantically related words (e.g., instruments, emo-\ntions, usages, genres) and is asked to pick the best and\nworst word to describe a song. In the freestyle mode,\na user is asked to suggest a new word that describes the\nmusic. Each player receives realtime feedback about the\nagreement amongst all players. We show that we can use\nthe data collected during a two-week pilot study of Lis-\nten Game to learn a supervised multiclass labeling (SML)\nmodel. We show that this SML model can annotate a\nnovel song with meaningful words and retrieve relevant\nsongs from a database of audio content.\n1 INTRODUCTION\nCollecting high-quality, semantic annotations of music is\na difﬁcult and time-consuming task. Previous methods\nhave included hand-labeling music [3, 9], conducting sur-\nveys [14, 6, 8] and text-mining web documents [7, 15].\nEach approach has drawbacks: human annotation meth-\nods are time consuming, costly, and as such, do not scale\nwhen attempting to annotate large music collections. In-\nformation mined automatically from web documents is of-\nten inconsistent with a true semantic description of the au-\ndio content.\nTo collect large amounts of high quality annotation\ndata at low cost, we propose using web-based games. von\nAhn et. al. have created a suite of games (ESP Game [11],\nPeekaboom [13], Phetch [12]) for collecting semantic in-\nformation about images. These ‘games with a purpose’\noffer users an engaging platform for competition and col-\nlaboration while also collecting useful data about the im-\nage content. This data analysis technique is called hu-\nman computation because it harnesses the collective intel-\nligence of a large number of human participants to solve a\nc/circlecopyrt2007 Austrian Computer Society (OCG).task that can not easily be automated. Using a game-based\napproach, a population of users can solve large problems\n(i.e., labeling all the images on the Internet) using volun-\ntary contributions from individuals (i.e., playing a game\nto label a single image.)\nIn this paper, we describe Listen Game , a multi-player,\nweb-based game designed to collect associations between\naudio content and words. We show that this game is\na powerful tool for collecting semantic music informa-\ntion by using the collected data to build a music infor-\nmation retrieval (MIR) application. In previous work [8],\nwe presented a computer audition system that can auto-\nmatically both annotate novel music with semantically\nmeaningful words and retrieve relevant songs from a large\ndatabase. Our system learns a supervised multi-class la-\nbeling (SML) model [1] by training on a set of audio con-\ntent labeled with semantic annotations. We use the data\ncollected from Listen Game to train our SML model. We\nthen quantitatively evaluate the quality of the data by ex-\namining the accuracy of the SML model on the tasks of\nmusic annotation and retrieval.\n2 COLLECTING MUSIC ANNOTATIONS\nA supervised learning approach to semantic music anno-\ntation and retrieval requires a large corpus of song-word\nassociations. Early work in music classiﬁcation (by genre\n[9, 5], emotion [4], instrument [2]) either used music cor-\npora hand-labeled by the authors or made use of existing\nsong metadata. While hand-labeling generally results in\nhigh quality labels, it does not easily scale to hundreds\nof labels per song over thousands of songs. Companies\nsuch as Pandora [14] employ dozens of musical experts\nwhose full-time job is to tag songs with a large vocabu-\nlary of musically relevant words but, unfortunately, have\nlittle incentive to make their data publicly available.\nIn [15], Whitman and Ellis collect a large number of\nweb-documents and summarize their content using text-\nmining techniques. From web-documents associated with\nartists , they learned binary classiﬁers for musically rele-\nvant words by associating words in the documents with\nthe artists’ songs. In previous work [7], we mined ex-\npert music reviews associated with songs and demon-\nstrated that we could learn a supervised multi-class la-\nbeling (SML) model over a large vocabulary of words.\nWhile web-mining is a more scalable approach than hand-labeling, we found that the data collected was of low qual-\nity since the extracted words did not necessarily provide a\ngood description of a song. In general, when writing re-\nviews of songs, albums or artists, authors do not make ex-\nplicit decisions about the relevance of each single word. In\naddition, many reviews contain social, historical or opin-\nionated information that is not related to the song’s audio\ncontent [15].\nA third approach uses surveys to collect semantic infor-\nmation about music. Moodlogic [6] customers annotate\nmusic using a standard survey containing questions about\ngenre, instrumentation, emotional characteristics, etc. We\nused a similar approach [8] to collect the CAL 500data\nset of 500songs, each of which has been annotated us-\ning a vocabulary of 174words by a minimum of three\npeople. Data collection took over 200 person-hours and\nresulted in approximately 300,000 individual word-song\nassociations. Using a survey produced higher quality an-\nnotations than the web data but required that we pay test\nsubjects for their time. Furthermore, surveys are tedious\nand time consuming. Despite ﬁnancial motivation, test\nsubjects quickly tire of lengthy surveys, resulting in inac-\ncurate annotations.\nHuman computation games motivate players to gener-\nate reliable annotations based on incentives built into the\ngame. In the ESP Game [11] for example, a pair of un-\nacquainted players are partnered up and each shown the\nsame image. Both players are asked to “type what your\npartner is thinking”. Since they have no means of com-\nmunicating, players invariably type words that have some-\nthing to do with the common image they see. When two\npeople independently suggest the same word to describe\nan image, the annotation is assumed to be reliable.\nHuman computation games also address the issue of\ncollecting lotsof data by turning annotation into an enter-\ntaining task. The ESP Game has gathered over 10million\nimage annotations. Games build a sense of community\nand loyalty in users and can be highly addictive. Statistics\nfrom the ESP Game highlight that some people played in\nmultiple 40hour per week spans. Since they require little\nmaintenance and run 24 hours a day, games can constantly\ncollect new information from multiple players. Develop-\ning human computation games for annotating music is a\nuseful approach for collecting semantic information. We\nbelieve that this approach has the potential for large-scale\nsuccess because people enjoy talking about, sharing, dis-\ncovering, arguing about and listening to music.\n3 LISTEN GAME\nImage annotation often makes objective binary associ-\nations between an image and the objects (‘sailboat’),\nscene information (‘landscape’), and visual characteris-\ntics (‘red’) it represents. Our human computation game\nbroaches the subjectivity inherent in many semantic la-\nbels that could be applied to music by allowing users to\nshare their opinions, rather than be judged as correct or\nincorrect. Listen Game collects the strength of association\nbetween a word and a song, rather than an all-or-nothing\nbinary label.3.1 Description of Game Play\nListen Game (www.listengame.org) is a multi-player, on-\nline, music annotation game. Players listen to a common\npiece of music, select good and bad semantic labels and\nget realtime feedback on the selection of all other play-\ners. In a regular round (Figure 1.a), the game server se-\nlects a 15-second music clip (chosen from 250 popular\nwestern songs) and six words or phrases associated with\na semantic category (e.g., instrumentation, usage, genre).\nThe words are randomly chosen from a predeﬁned 174-\nword vocabulary used in the CAL500 survey [8]. Each\nplayer’s game client, loaded in a standard web browser,\nplays the clip and displays the category and the words in a\nrandomly permuted order (to avoid order bias). The player\nthen chooses both the best word to describe the clip and\nthe worst word to describe the clip. Once the choices are\ncommitted, the game client displays instant feedback on\nthe choices made by all other players. A player’s score,\nS, is determined by the amount of agreement between the\nplayer’s choices and the choices of all other players:\nS= 100 ∗(fraction in agreement with best word )\n+ 100 ∗(fraction in agreement with worst word ).\nA player plays 7 regular rounds plus a freestyle round\n(Figure 1.b) where the game client plays a preview clip\nand displays a semantic category. The player is asked to\nenter a word or phrase that is an appropriate description\nof the preview clip. In the next regular round, the same\nmusic clip is played and the player’s suggested word is\npresented as one of the possible annotations which other\nplayers may select as the best or worst word. Using these\nnovel words from freestyle rounds, Listen Game can au-\ntomatically grow the predeﬁned vocabulary of musically\nrelevant terms. Upon ﬁnishing 8rounds, a game summary\ndisplays the player’s score, the songs played, and various\ngame statistics.\n3.2 Quality of Data\nWhile individual best/worst choices by players are binary,\nthe aggregate song-word associations are not binary. One\nmay interpret them as real-valued weights, proportional\nto the percentage of players who agree that a word does\n(or does not) describe a song. We calculate the semantic\nweight was a function of the ‘best’ votes, ‘worst’ votes\nand potential votes (the number of times a song-word pair\nis presented to any player):\nw=/braceleftbigg\n0, if #(Best) - #(Worst) <2\nw/prime,otherwise\nw/prime= max/parenleftBigg\n0,/bracketleftbigg#(Best)−#(Worst )\n#(Potential V otes )/bracketrightbigg/parenrightBigg\n.\nFor a song-word pair to be reliable, we require that at least\ntwo people make the association in any given round. We\nwould hope that with more data, we could raise the thresh-\nold for agreement signiﬁcantly.(a) Normal Round: players select best and worst words to describe the song\n (b) Freestyle Round: players enter their own word to describe the song\nFigure 1: Screenshots of Listen Game\n4 SUPERVISED MULTICLASS LABELING (SML)\nWe use the semantic song-word associations collected us-\ning Listen Game to train a SML model. The SML model\nwas developed by Carneiro et al. [1] for the tasks of image\nannotation and retrieval. In [8], we showed how to use the\nSML model to learn a Gaussian mixture model (GMM)\nover an audio feature space for each word in a predeﬁned\nvocabulary. We estimate these ‘word-level’ GMMs by\ncombining ‘song-level’ GMMs (one trained on the feature\nvectors extracted from a single song) using the mixture hi-\nerarchies expectation-maximization algorithm (MH-EM)\n[10]. We also extended MH-EM to allow for real-valued\nsemantic weights , rather than binary labels. While binary\nlabels are quite natural for images where the majority of\nwords are associated with objective semantic concepts,\nmusic is more subjective. For example, two listeners may\nnot always agree that a song is representative of a certain\ngenre or emotion. Listen Game directly reﬂects this no-\ntion by recording the votes of a large group of users on\nthe best and worst words to describe a song. Using our\nweighted MH-EM algorithm, we learn GMMs that reﬂect\nthe strength of the semantic associations between words\nand songs. We refer the reader to [8] for a full explanation\nof this system, as well as other details related to audio fea-\nture extraction and semantic representation.\n5 EVALUATION OF LISTEN GAME DATA\nPreviously we used a survey to collect the CAL500 data\nset of semantic weights between 500 songs (by 500 unique\nartists) and 174 words [8]. The 174 words are part of an\nhierarchical vocabulary with six high-level semantic cate-\ngories: genre, emotion, instrumentation, vocal character-\nistic, general song characteristics, and usage. We deter-\nmine the ‘strength of association’ for these 87,000 word-\nsong pairs by averaging the response of multiple individ-\nuals who annotated the song using a standard survey [8].\nMore recently, we conducted a two-week pilot study of\nListen Game. We reduced the vocabulary to 120 words by\neliminating ambiguous and less well known words. For\nthe experiments reported in Section 5.2, we require that\neach word has been used to describe a minimum of ﬁvesongs in the corpus, further reducing this vocabulary to\n82 words. A randomly selected set of 250 songs from\nthe CAL500 data set were used in the game. Players for\nListen Game were recruited using emails to the authors’\nfriends and families, a mass email to a Music-IR list and\nword-of-mouth referrals.\nDuring the two-week study, we collected the Listen250\ndata set: 26,000 annotations (best and worst votes) of\n250 songs using 120 words from 440 unique players.\n20 players played more than 30 eight-round games and\nﬁve (including one of the authors) played more than 100\ngames. In the freestyle round, players suggested 775 new\nwords not from the original 120-word CAL500 vocabu-\nlary. Some standouts include subgenres (‘psychedelic’,\n‘lounge’), usages (‘good for a hangover’,‘cooking’), ad-\njectives (‘airy’, ‘fun loving’) and slang (‘agro’, ‘mosh-\ning’).\n5.1 Qualitative Analysis\nIn Table 1, we present human- and machine-generated\nannotations of two songs. Human annotations are sum-\nTable 1: “Musical MadLibs”. Annotations generated di-\nrectly using semantic weights collected by Listen Game\nand automatically using the Listen250 SML model.\nNorah Jones - Don’t Know Why\nGenerated using Listen Game data\nThis is cool jazz ,soul song that is mellow andpositive . It features\nfemale vocal ,piano ,bass, and breathy ,aggressive vocals. It is a\nsong with a light beat andwith a catchy feel that you might like to\nlisten to while studying .\nAutomatically Generated using SML model\nThis is soft rock ,jazz song that is mellow andsad. It features piano ,\nsynthesizer ,ambient sounds , and monotone ,breathy vocals. It is\na song with a slow tempo andwith low energy that you might like\nto listen to while studying .\nRick James - Super Freak\nGenerated using Listen Game data\nThis is R&B y,funk song that is positive andcheerful . It features\nmale vocal ,piano ,acoustic guitar , and high-pitched ,aggressive\nvocals. It is a song with a catchy feel andwith a changing energy\nlevel that you might like listen to to while at a party .\nAutomatically Generated using SML model\nThis is popy,R&B song that is not mellow andcheerful . It features\nsequencer ,synthesizer ,male vocal , and spoken ,rapping vocals.\nIt is a song that is very danceable andwith a synthesized texture\nthat you might like to listen to while at a party .marized by ranking words within each semantic category\naccording to the semantic weights calculated by Listen\nGame. This results in labeling ‘Don’t know why’ by No-\nrah Jones as both ‘Cool Jazz’ and ‘Soul’ though these\nmay not be the best genres to describe this song. ‘Cool\nJazz’ was selected by multiple players in a round where\nthere happened to be no truly relevant words. After many\nrounds, the semantic weight of words appearing in rounds\nwith no clear choice would be reduced by votes for rel-\nevant words. We consider the Listen250 data set to be\nsparse, since there have only been on average two ‘po-\ntential votes’ for each of the 20,500 song-word pairs. The\nsecond set of annotations in Table 1 are automatically pro-\nduced by the SML model trained using Listen250 data.\n5.2 Quantitative Evaluation\nWe use per-word precision and recall (pwPrecision and\npwRecall) metrics to measure annotation performance.\nWe annotate each song with a ﬁxed number of words,\npicked by the SML model. For each word win our vo-\ncabulary, |wH|is the number of songs that have word w\nin the “ground truth” annotation, |wA|is the number of\nsongs that our model annotates with word w, and|wC|is\nthe number of “correct” words that have been used both in\nthe ground truth annotation and by the model. pwRecall is\n|wC|/|wH|and pwPrecision is |wC|/|wA|. The reported\nvalues in Table 2 are found by averaging these metrics\nover all the words in the vocabulary. While trivial mod-\nels can easily maximize one of these measures (e.g., by\nlabeling all songs with a certain word or, instead, none of\nthem), achieving excellent precision and recall simultane-\nously requires a truly valid model.\nMean average precision (meanAP) and mean area un-\nder the receiver operating characteristic (ROC) curve\n(meanAROC) metrics measure retrieval performance. We\ncalculate average precision (AP) by moving down the\nranked list of retrieved test songs and averaging the pre-\ncisions at every point where we correctly identify a new\nsong. An ROC curve plots the true positive rate as a func-\ntion of the false positive rate as we move down the ranked\nlist of songs. The area under the ROC curve (AROC) is\nupper bounded by 1.0. Random guessing results in AROC\nof 0.5. MeanAP and meanAROC are found by averaging\nAP and AROC across all words in our vocabulary.\nTable 2 compares the performance of three SML mod-\nels: Listen250 trained using 225 songs annotated using\nListen Game, CAL250 and CAL500 trained using 225 and\n450 songs respectively, annotated using responses to sur-\nveys. We evaluate all models with the CAL500 data using\n10-fold cross-validation. All differences are signiﬁcant\n(paired t-test with α= 0.05) with the exception of pwRe-\ncall and pwPrecision between CAL250 and CAL500. As\nexpected, the models CAL250 and CAL500, trained on\nsurvey data produce better annotation and retrieval per-\nformance than the model Listen250 trained with sparser\ngame data. The model CAL500, trained on more songs,\nachieves better retrieval performance than CAL250.\nWe would expect the performance of all models, but\nespecially Listen250, to improve with both more trainingTable 2: Model evaluation. The semantic information for\nCAL models was collected using a survey. The Listen\nmodel was trained on data collected by Listen Game. Each\nsong is annotated with 8 words.\nModel Annotation Retrieval\npwRecall pwPrecision meanAP meanAROC\nRandom 0.092 0.058 0.188 0.501\nListen250 0.188 0.289 0.368 0.661\nCAL250 0.215 0.333 0.410 0.701\nCAL500 0.224 0.338 0.429 0.722\nsongs and more accurate estimates of the word-song re-\nlationships. For example, we noticed an improvement in\nmeanAROC for Listen250 from 0.640 to 0.661 during the\nlast 4 days of our two-week pilot study during which time\nwe collected approximately 35% more data. By the end of\nour pilot study, we had shown each of our 20,500 word-\nsong pairs only twice to a player.\nAcknowledgements\nWe thank D. Torres, S. Dubnov, O. Lang, and M. Yaz-\ndani for helpful comments. We also thank everyone who\nplayed and provided feedback on Listen Game. This\nwork is supported by NSF IGERT DGE-0333451 and\nNSF grant DMS-MSPA 062540922.\n6 REFERENCES\n[1] G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos. Super-\nvised learning of semantic classes for image annotation and retrieval.\nIEEE Transactions on Pattern Analysis and Machine Intelligence ,\n29(3):394–410, 2007.\n[2] S. Essid, G. Richard, and B. David. Inferring efﬁcient hierarchical\ntaxonomies for music information retrieval tasks: Application to mu-\nsical instruments. ISMIR , 2005.\n[3] M. Goto. AIST annotation for RWC music database. ISMIR , 2006.\n[4] T. Li and M. Ogihara. Detecting emotion in music. ISMIR , 2003.\n[5] M. F. McKinney and J. Breebaart. Features for audio and music clas-\nsiﬁcation. ISMIR , 2003.\n[6] T. Sulzer. Moodlogic. http://www.moodlogic.com, 2007.\n[7] D. Turnbull, L. Barrington, and G. Lanckriet. Modelling music and\nwords using a multi-class na ¨ıve bayes approach. ISMIR , 2006.\n[8] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Towards\nmusical query-by-semantic description using the CAL500 data set.\nSIGIR , 2007.\n[9] G. Tzanetakis and P. R. Cook. Musical genre classiﬁcation of au-\ndio signals. IEEE Transactions on Speech and Audio Processing ,\n10(5):293–302, 7 2002.\n[10] N. Vasconcelos. Image indexing with mixture hierarchies. IEEE\nCVPR , pages 3–10, 2001.\n[11] L. von Ahn and L. Dabbish. Labeling images with a computer game.\nInACM CHI , 2004.\n[12] L. von Ahn, S. Ginosar, M. Kedia, R. Liu, and M. Blum. Improving\naccessibility of the web with a computer game. In ACM CHI Notes ,\n2006.\n[13] L. von Ahn, R. Liu, and M. Blum. Peekaboom: A game for locating\nobjects in images. In ACM CHI , 2006.\n[14] T. Westergren. Music genome project. www.pandora.com, 2007.\n[15] B. Whitman and D. Ellis. Automatic record reviews. ISMIR , 2004."
    },
    {
        "title": "A Supervised Approach for Detecting Boundaries in Music Using Difference Features and Boosting.",
        "author": [
            "Douglas Turnbull",
            "Gert R. G. Lanckriet",
            "Elias Pampalk",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415082",
        "url": "https://doi.org/10.5281/zenodo.1415082",
        "ee": "https://zenodo.org/records/1415082/files/TurnbullLPG07.pdf",
        "abstract": "A musical boundary is a transition between two musical segments such as a verse and a chorus. Our goal is to automatically detect musical boundaries using temporallylocal audio features. We develop a set of difference features that indicate when there are changes in perceptual aspects (e.g., timbre, harmony, melody, rhythm) of the music. We show that many individual difference features are useful for detecting boundaries. By combining these features and formulating the problem as a supervised learning problem, we can further improve performance. This is an alternative to previous work on music segmentation which has focused on unsupervised approaches based on notions of self-similarity computed over an entire song. We evaluate performance using a publicly available data set of 100 copyright-cleared pop/rock songs, each of which has been segmented by a human expert. 1 INTRODUCTION Most pop/rock songs have a standard structure: an introduction followed by alternating verses, choruses, and solos/bridges segments, and concluding with an outro. We define a musical boundary as the point in time where a song transitions between two of these segments. A boundary is often associated with one or more perceptual cues such as the introduction of a new instrument, a key change, or a drum fill. Our goal is to first design low-level features that encode information related to such cues, and second, use these features to develop a system that can automatically detect musical boundaries. Automatic boundary detection is useful for generating music thumbnails [10], efficient music browsing [9], music information retrieval [16], and as a front-end for semantic music analysis systems [14, 11]. There are three main contributions of our work. First, we describe a set of local difference features each of which is a time series that is designed to indicate (e.g., ‘peak’) when some aspect of the music changes. Each feature is calculated by sliding a difference window across the audio signal and comparing the information extracted from the first half of the window with the second half of the window. Each low-level feature is loosely related to a highc⃝2007 Austrian Computer Society (OCG). level musical concept such as timbre, harmony, melody, and rhythm. We individually evaluate each difference feature and report how well each feature can be used to detect musical boundaries. Second, we combine these features and propose a supervised learning approach based on the AdaBoost algorithm [3, 15, 5]. For each of our features, we create a large number of additional features by first smoothing the feature and then calculating the instantaneous derivatives of the smoothed versions of the feature. We then simultaneously sample each feature in this enlarged set of features to create a high-dimensional (∼800 dimensions) vector of feature values for each sample 1 . The samples that correspond to musical boundaries are labeled as belonging to the ‘boundary’ class while all others are labeled as belonging to the ‘non-boundary’ class. We learn a boosted decision stump (BDS) classifier using training data and report performance on test data. Viola and Jones [15] pioneered this technique and Dollar et al. [3] applied this to image boundary detection. Third, we propose two evaluation metrics, Median Time Difference and Boundary Hit Rate, in order to quantitatively evaluate musical boundary detection. We quantitatively evaluate both individual difference features and BDS classifiers using the RWC music database of 100 pop songs [6]. This data set now includes human annotated segmentations and can serve as a common test bed for future research [8]. Although we are unaware of previous work that explicitly addresses musical boundary detection, there has been a significant amount of work on music segmentation [9, 13, 10, 1, 11, 2, 4]. These two closely related problems are different in that musical boundary detection involves finding musical boundaries from temporally-local features whereas segmentation involves finding coherent segments within a song using notions of self-similarity (e.g. finding repetitive structures [9, 2, 4] or identifying similar spectral characteristics [13, 10, 1]). Our features are designed to indicate dissimilarity in the audio signal. In addition, our supervised approach has the benefit of allowing the user to explicitly specify their definition of a ‘boundary’ through the labels that they provide for the training data. 1 Throughout this paper, we will consider a sample to be a ‘feature sample’ rather than a ‘audio sample’ that is calculated 10 per second. 2 MUSICAL BOUNDARY FEATURES In this section, we describe various features that are useful for detecting musical boundaries. We loosely relate each to a high-level music concept for clarity, though these features are largely derived from a low-level spectral representation (e.g., short-time Fourier transforms) and often can be related to more than one high-level concept. Each feature is time series that is sampled at a given feature sampling rate (e.g., 10 samples/sec). We create the time series by sliding a window (5-10 seconds in length) across the audio signal with a hopsize equal to the feature sampling rate. The audio signal in the window is summarized with a statistic. The statistic may be a scalar, a vector, a matrix, a set of scalars/vectors, or a time series of scalars/vectors. A difference feature is time series of scalar values that is derived by sliding a window over an audio signal. A difference feature is computed by comparing the statistic calculated in the first half of the window with the statistic calculated in the second half of the window. When our statistic is a scalar, vector, or matrix, we will compute the Euclidean norm of the difference. When our statistic is a set or times series, we first estimate one Gaussian distribution (with full covariance) from values in the first half of the window and a second Gaussian distribution from the values in the second half of the window. We then compute the symmetric Kullback-Leibler (sKL) divergence between the two Gaussian distributions (See Section 2.2.3.4 of [12] for details). The sKL is a nonnegative scalar value that is large when the two estimated distributions greatly differ. One should note that by estimating Gaussians, we discards all temporal information when our statistic is a time series. We normalize each feature so that, over an entire song, the mean of the samples is equal to 0 and variance of the samples equal to 1. Normalization is needed for generalization of features across the different songs in our corpus. In addition, we find empirically that ‘smoothing’ a feature by passing a Gaussian window over the time series improves performance.",
        "zenodo_id": 1415082,
        "dblp_key": "conf/ismir/TurnbullLPG07",
        "keywords": [
            "musical boundary",
            "temporallylocal",
            "audiosignal",
            "timbre",
            "harmony",
            "melody",
            "rhythm",
            "supervised learning",
            "AdaBoost algorithm",
            "RWC music database"
        ],
        "content": "A SUPERVISED APPROACH FOR DETECTING BOUNDARIES IN MUSIC\nUSING DIFFERENCE FEATURES AND BOOSTING\nDouglas Turnbull1, Gert Lanckriet2\nComputer Science & Engineering1\nElectrical & Computer Engineering2\nUniversity of California, San Diego\nLa Jolla, CA 92093, USAElias Pampalk, Masataka Goto\nNational Institute of Advanced\nIndustrial Science and Technology (AIST)\nTsukuba, Ibaraki 305-8568, Japan\nABSTRACT\nA musical boundary is a transition between two musical\nsegments such as a verse and a chorus. Our goal is to au-\ntomatically detect musical boundaries using temporally-\nlocal audio features. We develop a set of difference fea-\ntures that indicate when there are changes in perceptual as-\npects (e.g., timbre, harmony, melody, rhythm) of the mu-\nsic. We show that many individual difference features are\nuseful for detecting boundaries. By combining these fea-\ntures and formulating the problem as a supervised learning\nproblem, we can further improve performance. This is an\nalternative to previous work on music segmentation which\nhas focused on unsupervised approaches based on notions\nof self-similarity computed over an entire song. We evalu-\nate performance using a publicly available data set of 100\ncopyright-cleared pop/rock songs, each of which has been\nsegmented by a human expert.\n1 INTRODUCTION\nMost pop/rock songs have a standard structure: an intro-\nduction followed by alternating verses, choruses, and so-\nlos/bridges segments, and concluding with an outro. We\ndeﬁne a musical boundary as the point in time where\na song transitions between two of these segments. A\nboundary is often associated with one or more perceptual\ncues such as the introduction of a new instrument, a key\nchange, or a drum ﬁll. Our goal is to ﬁrst design low-level\nfeatures that encode information related to such cues, and\nsecond, use these features to develop a system that can au-\ntomatically detect musical boundaries. Automatic bound-\nary detection is useful for generating music thumbnails\n[10], efﬁcient music browsing [9], music information re-\ntrieval [16], and as a front-end for semantic music analysis\nsystems [14, 11].\nThere are three main contributions of our work. First,\nwe describe a set of local difference features each of which\nis a time series that is designed to indicate (e.g., ‘peak’)\nwhen some aspect of the music changes. Each feature is\ncalculated by sliding a difference window across the audio\nsignal and comparing the information extracted from the\nﬁrst half of the window with the second half of the win-\ndow. Each low-level feature is loosely related to a high-\nc/circlecopyrt2007 Austrian Computer Society (OCG).level musical concept such as timbre, harmony, melody,\nand rhythm. We individually evaluate each difference fea-\nture and report how well each feature can be used to detect\nmusical boundaries.\nSecond, we combine these features and propose a su-\npervised learning approach based on the AdaBoost algo-\nrithm [3, 15, 5]. For each of our features, we create a large\nnumber of additional features by ﬁrst smoothing the fea-\nture and then calculating the instantaneous derivatives of\nthe smoothed versions of the feature. We then simultane-\nously sample each feature in this enlarged set of features\nto create a high-dimensional ( ∼800dimensions) vector\nof feature values for each sample1. The samples that cor-\nrespond to musical boundaries are labeled as belonging\nto the ‘boundary’ class while all others are labeled as be-\nlonging to the ‘non-boundary’ class. We learn a boosted\ndecision stump (BDS) classiﬁer using training data and\nreport performance on test data. Viola and Jones [15] pi-\noneered this technique and Dollar et al. [3] applied this to\nimage boundary detection.\nThird, we propose two evaluation metrics, Median\nTime Difference andBoundary Hit Rate , in order to quan-\ntitatively evaluate musical boundary detection. We quan-\ntitatively evaluate both individual difference features and\nBDS classiﬁers using the RWC music database of 100 pop\nsongs [6]. This data set now includes human annotated\nsegmentations and can serve as a common test bed for fu-\nture research [8].\nAlthough we are unaware of previous work that ex-\nplicitly addresses musical boundary detection, there has\nbeen a signiﬁcant amount of work on music segmentation\n[9, 13, 10, 1, 11, 2, 4]. These two closely related problems\nare different in that musical boundary detection involves\nﬁnding musical boundaries from temporally-local features\nwhereas segmentation involves ﬁnding coherent segments\nwithin a song using notions of self-similarity (e.g. ﬁnding\nrepetitive structures [9, 2, 4] or identifying similar spectral\ncharacteristics [13, 10, 1]). Our features are designed to\nindicate dissimilarity in the audio signal. In addition, our\nsupervised approach has the beneﬁt of allowing the user to\nexplicitly specify their deﬁnition of a ‘boundary’ through\nthe labels that they provide for the training data.\n1Throughout this paper, we will consider a sample to be a ‘feature\nsample’ rather than a ‘audio sample’ that is calculated 10 per second.2 MUSICAL BOUNDARY FEATURES\nIn this section, we describe various features that are useful\nfor detecting musical boundaries. We loosely relate each\nto a high-level music concept for clarity, though these fea-\ntures are largely derived from a low-level spectral repre-\nsentation (e.g., short-time Fourier transforms) and often\ncan be related to more than one high-level concept.\nEach feature is time series that is sampled at a given\nfeature sampling rate (e.g., 10 samples/sec). We create the\ntime series by sliding a window (5-10 seconds in length)\nacross the audio signal with a hopsize equal to the feature\nsampling rate. The audio signal in the window is sum-\nmarized with a statistic. The statistic may be a scalar, a\nvector, a matrix, a set of scalars/vectors, or a time series\nof scalars/vectors. A difference feature is time series of\nscalar values that is derived by sliding a window over an\naudio signal. A difference feature is computed by compar-\ning the statistic calculated in the ﬁrst half of the window\nwith the statistic calculated in the second half of the win-\ndow. When our statistic is a scalar, vector, or matrix, we\nwill compute the Euclidean norm of the difference. When\nour statistic is a set or times series, we ﬁrst estimate one\nGaussian distribution (with full covariance) from values\nin the ﬁrst half of the window and a second Gaussian dis-\ntribution from the values in the second half of the window.\nWe then compute the symmetric Kullback-Leibler (sKL)\ndivergence between the two Gaussian distributions (See\nSection 2.2.3.4 of [12] for details). The sKL is a non-\nnegative scalar value that is large when the two estimated\ndistributions greatly differ. One should note that by es-\ntimating Gaussians, we discards all temporal information\nwhen our statistic is a time series.\nWe normalize each feature so that, over an entire song,\nthe mean of the samples is equal to 0 and variance of the\nsamples equal to 1. Normalization is needed for general-\nization of features across the different songs in our corpus.\nIn addition, we ﬁnd empirically that ‘smoothing’ a feature\nby passing a Gaussian window over the time series im-\nproves performance.\n2.1 Timbre-related features\nTimbre describes sound qualities not related to melody or\nrhythm. For example, the instrumentation of a piece has a\nstrong impact on the perception of timbre. We use a set of\nsix scalar low-level audio statistics to describe the timbre\nof the audio contents within a window. (Section 2.2.5 of\n[12] for details.) These features are related to loudness,\nnoisiness, brightness, and percussiveness of the signal.\nWe compute six corresponding features, denoted Spec, by\nsliding a window over the musical signal. We also com-\npute six spectral difference features, denoted Spec-Diff,\nby sliding a difference window over a musical signal and\ncomputing the difference between the values calculated in\neach window.\nWe also use Mel-frequency cepstral coefﬁcients to de-\nscribe timbre. An MFCC vector encodes the spectral\nshape of a short-time (e.g., 20 msec) audio frame. Given a\ndifference window, we generate two time series of MFCC\nvectors (one for each half of the difference window) us-\ning the frames that are within the window. We esti-mate a Gaussian distribution using these time series of\nMFCC vectors and calculate one sKL value per window.\nA MFCC difference feature is then created by sliding the\ndifference over the song. We generate a set of four MFCC\ndifference features, denoted MFCC-diff, by using differ-\nent subsets of the MFCCs: the ﬁrst 5 MFCCs, the ﬁrst\n20 MFCCs, the 2nd to 5th MFCCs, and the 2th to 20th\nMFCCs. In addition, we will compute four MFCCDelta-\nDiff and four MFCCDelta2-Diff features using the instan-\ntaneous ﬁrst and second derivatives from the series of\nMFCC vectors.\n2.2 Harmony- and Melody-related features\nChromagrams are often used to encode information about\nkey and major/minor tonality. For each short-time frame,\nWe ﬁrst computing a chroma vector [8] that measures\nthe relative amount of energy in each pitch class (e.g.\nA,...,G#). Similar to the computation of the MFCC-\ndiff features, we estimate Gaussian distributions from the\nchroma vectors that are extracted from each half of a\ndifference windows and compute one sKL per window.\nWe generate a Chromagram difference feature, denoted\nChroma-diff, by sliding the difference window over the\nsong. We use 12-, 24-, and 36-dimensional chroma vec-\ntors where each element represents 1, 1/2, and 1/3 of a\npitch class, respectively. We also compute ChromaDelta-\nDiff and ChromaDelta2-Diff features using the ﬁrst and\nsecond derivatives from the time series of chroma vectors.\nIn an attempt to model changes in the melody, we es-\ntimate the fundamental frequency (F0) and the cumula-\ntive power (F0Power) in the harmonics of the estimated\nF0 for each short-time frame[7]. For both the time series\nof FO values and F0Power values, we calculate F0-Diff\nand F0Power-Diff features using the same technique we\nused to create MFCC-Diff and Chroma-Diff features.\n2.3 Rhythm-related features\nTo model rhythm, we use the ﬂuctuation patterns (FPs)\nwhich describe modulations in the loudness for a set of\nfrequency bands (see [12] for details). For each Mel-scale\nfrequency band, an FFT is used to compute modulation\nfrequencies of the loudness in dB within that band. Mod-\nulations around 4Hz are emphasized using a model of per-\nceived ﬂuctuation strength. The FP is a matrix where the\nrows correspond to the Mel-bands and the columns corre-\nspond to modulation frequencies. The values of a cell in\nthe matrix describe strength of the speciﬁc modulation for\na speciﬁc frequency band.\nFrom an FP computed on each window, we calculate\nseven statistics denoted by Flux (see Section 2.2.5.4 of\n[12] for details). These statistics are related to the per-\nceived tempo, strength of bass beats, overall strength of\nbeats, etc. To calculate difference features, we compute\none FP for the ﬁrst half of the difference window and one\nFP for the second half of the difference window. Similar\nto the Spec-Diff features, we compute the seven Flux-Diff\nfeatures by measuring the difference in values from the\nﬁrst half to the second half of the window. In addition,\nwe compute an eighth Flux-diff feature by calculating the\nFrobenius norm between the two FPs.Set Feature # CommentsTimbreSpec 6 Spectral features\nSpec-Diff 6 Differences between Spec Features\nMFCC-Diff 4 MFCCs 1-5,1-20,2-5,2-20\nMFCCDelta-Diff 4 1st Derivatives of MFCCs\nMFCCDelta2-Diff 4 2nd Derivatives of MFCCsHarmonyChroma-Diff 3 Chroma Vectors - 12, 24, 36 Pitch Classes\nChromaDelta-Diff 3 1st Derivatives of chroma Vectors\nChromaDelta2-Diff 3 2nd Derivatives of chroma VectorsMelodyF0 2 F0 and F0Power\nF0-diff 2 Differences between F0 FeaturesRhythmFlux 7 Fluctuation Pattern (FP) features\nFlux-Diff 8 Difference of FP features,\nFrobenius norm of difference between FPs\nTable 1 . Summary of features: For each song, we extract\n37difference features and 15 non-difference features.\n3 PEAK PICKING FOR BOUNDARY DETECTION\nOur various difference features are designed to peak when\none or more timbral, harmonic, melodic or rhythmic cues\noccurs in a song. Since these cues are often associated\nwith musical boundaries, we estimate the location of mu-\nsical boundaries by ‘picking the local peaks’ of a differ-\nence feature. The quality of the local peaks can be evalu-\nated by comparing them with a human-generated ‘ground-\ntruth’ segmentation of the music. Since difference fea-\ntures are often noisy and simple peak picking results in\nestimating too many boundaries, we propose a technique\ncalled local-peak-local-neighborhood (LPLN). First, we\nsmooth the feature (see Section 2) and then pick local\npeaks. For each local peak, we examine at a local neigh-\nborhood (e.g., +/- 0.5 seconds) and ﬁnd the sample with\nthe largest unsmoothed feature value. For each song,\nwe rank all of these LPLN samples according to the un-\nsmoothed feature value. Finally, we output the times cor-\nresponding to the top NLPLN samples, where Nis a\nuser-speciﬁed number. Empirically, we ﬁnd that this tech-\nnique does (slightly) improve performance over simple\npeak picking.\nPeak picking can be considered ‘unsupervised’ since\nwe assume, rather than learn, a relationship between peaks\nand boundaries. The two main drawbacks of this approach\nare that it is not obvious how to combine multiple features\nand that we estimate boundaries only at the peaks of our\ndifference features. In the next section, we will propose a\n‘supervised’ approach that uses the ‘ground truth’ bound-\naries from human segmentations to learn models which in-\ncorporate a large number of difference and non-difference\nfeatures. This approach is ﬂexible in that it does not de-\npend on the assumption that the boundaries occur at the\nlocal peaks of an individual feature.\n4 SUPERVISED BOUNDARY DETECTION USING\nBOOSTING\nWe can frame our musical boundary detection problem as\na supervised learning problem. The label for a sample is\n1 if a boundary occurs at that sample, and 0 otherwise.\n(Given a feature sample rate of 10 samples/second, there\nwill be between 7-15 ‘boundary’ samples and about 2000\ntotal samples for each song.) We create a 832-dimensional\nvector of feature values for each sample by calculating ad-\nditional features from the 52 features that are described inSection 2. We smooth each of these features three times\nusing different smoothing window widths (e.g., 1.6 sec-\nonds, 6.4 seconds and 25.6 seconds) to encode different\ntime resolutions. For each of the resulting 156 smoothed\nfeatures, we calculate the ﬁrst and second instantaneous\nderivatives and the absolute values of the ﬁrst and second\ninstantaneous derivatives. The derivative features are in-\ntended to encode information about the local optima (e.g.,\npeaks) of the features. We also include their absolute val-\nues since decision stumps are linear classiﬁers and de-\ntection of local optima depends on the magnitude of the\nderivatives. For example, a peak occurs when the magni-\ntude of the ﬁrst derivative is close to zero. We then simul-\ntaneously sample each of these 52 + 156 + 156 ∗4 = 832\nfeatures in order to generate a series of 832-dimensional\nvectors of feature values. Our training data is the set of\nlabeled samples from a corpus of songs, however, in prac-\ntice we will use only a small random subset of the non-\nboundary samples.\nWe learn a boosted decision stumps (BDS) classiﬁer\nfrom the training data [3, 15]. A decision stump is a\n‘weak’ classiﬁer that operates on one feature by setting a\nthreshold for that feature. For each sample, if the value of\nthe feature is greater than the threshold, then it is classiﬁed\nas one class, otherwise it is classiﬁed as the other class. At\neach iteration of our boosted learning algorithm, we pick\none decision stump with an associated weight and add it to\nour existing ensemble of decision stumps and weights. We\nuse the AdaBoost algorithm to determine the weights [5]\nfor each decision stump. When classifying a novel vector,\nwe evaluate each of the stumps and combine their ‘votes’\nusing the learned weights.\nGiven a novel song, we extract features, generate the\ntime series of vectors for feature values, and classify each\nvector using our BDS classiﬁer. The classiﬁer outputs\nboth a binary decision and a conﬁdence score of each sam-\nple belonging to the ‘boundary’ class. Often the classi-\nﬁer will predict many ‘boundary’ samples near one true\nboundary. We create a smoothed time series from the se-\nries of conﬁdence scores and pick peaks using the LPLN\ntechnique.\n5 EXPERIMENTAL SETUP\nWe evaluate both unsupervised (picking the peaks of\nindividual difference features) and supervised (combin-\ning multiple features and learning a BDS classiﬁer) ap-\nproaches for detecting musical boundaries. Our data set\nis 100 pop songs from the RWC music database (RWC-\nMDB-P-2001) [6]. Each song has been manually seg-\nmented by a music graduate student [8]. We deﬁne a true\nmusical boundary as a transition between ‘intro’, ‘verse’,\n‘chorus’, ‘bridge/solo’, and ‘outro’ segments. There are\nbetween 7-15 boundaries per song and each song is be-\ntween 2 and 6 minutes long.\nWe compare 260 unsupervised (52 features x 5 smooth-\ning window widths) and 4 supervised BDS models. For\neach model, we estimate the location of the top 10 musi-\ncal boundaries per song using the LPLN technique. We\nexplicitly ﬁx the number of boundaries since some (less\nsmoothed) models tend to over segment songs. To eval-\nuate a model, we use two sets of metrics: median timesModel Median Time (seconds) Boundary Hit Rates (ranges between 0.0 and 1.0)\nguess-to-true true-to-guess precision recall Hit-F\nBaselines\nDeterministic 8.85 (0.39) 6.40 (0.46) 0.039 (0.006) 0.052 (0.008) 0.043 (0.007)\nStochastic 9.67 (0.42) 8.80 (0.38) 0.043 (0.007) 0.056 (0.009) 0.048 (0.007)\nUnsupervised Approaches: Picking the peaks of individual difference features\nMFCCDelta-Diff(1-20), 3.2 sec 5.14 (0.49) 3.72 (0.56) 0.258 (0.015) 0.358 (0.022) 0.295 (0.017)\nf0-Diff(Power), 1.6 sec 8.07 (0.84) 6.36 (0.59) 0.140 (0.012) 0.195 (0.017) 0.160 (0.013)\nChroma-Diff(36), 1.6 sec 8.19 (0.71) 9.91 (0.92) 0.120 (0.010) 0.163 (0.014) 0.136 (0.011)\nSpec-Diff(Harm), 0.8 sec 7.11 (0.73) 15.87(1.53) 0.109 (0.011) 0.143 (0.017) 0.121 (0.012)\nFlux-Diff(Norm), 12.8 sec 6.87 (0.52) 3.71 (0.29) 0.096 (0.011) 0.131 (0.015) 0.109 (0.012)\nSupervised Approaches: training a boosted decision stump (BDS) classiﬁer\nBDS 200 4.29 (0.47) 1.82 (0.30) 0.327 (0.016) 0.462 (0.022) 0.378 (0.017)\nBDS 100 4.63 (0.46) 1.58 (0.24) 0.320 (0.014) 0.452 (0.020) 0.370 (0.016)\nBDS 50 5.06 (0.50) 2.19 (0.33) 0.300 (0.016) 0.424 (0.022) 0.346 (0.018)\nBDS 20 5.35 (0.53) 2.49 (0.37) 0.295 (0.016) 0.422 (0.023) 0.342 (0.018)\nTable 2 . Results: The reported value is the average of the metric over the 100 songs. The standard error is reported in\nparenthesis. For unsupervised models, we only report the best individual model (feature and smoothing width) from each\nof the ﬁve sets of difference features.\nbetween estimated and true boundaries, and boundary hit\nrates. Two median time metrics, true-to-guess and guess-\nto-true, respectively measure the median time between\neach true boundary to the closest estimate, and the median\ntime between each estimate to the closed true boundary.\nFor the boundary hit rate, we consider an estimate a ‘hit’\nwhen it is within half a second of a true boundary. We cal-\nculate the precision (the percentage of estimates that hit\na true estimate), the recall (the percentage of true bound-\naries that are hit), and Hit-F (the harmonic average of pre-\ncision and recall). Note that each true boundary can be\nhit by at most one estimate since we explicitly prevent es-\ntimates from being close to one another using the LPLN\ntechnique.\nFor all metrics, performance is averaged over the 100\nsongs in the data set. For each supervised model, we use\n10-fold cross validation. In addition to providing results\nfor both our unsupervised and supervised approaches, we\nalso report the performance of two baselines for compari-\nson. The deterministic baseline uniformly spaces 10 esti-\nmates over the course of the song. The stochastic baseline\ninvolves picking 10 samples randomly from each song to\nuse as our estimates for the boundaries. All results are\ngiven in Table 2.\n6 RESULTS\nFor the unsupervised approach, we compare 520 individ-\nual models by smoothing each of the 52 features pre-\nsented in Section 2 with one of ten smoothing window\nwith widths ranging from 0.1 seconds (no smoothing) to\n51.2 seconds. We rank (by Hit-F) the best performing in-\ndividual feature (and associated smoothing width) in each\nset of difference features. If we consider Hit-F metric, the\nbest model for each difference feature set performs signif-\nicantly better than the random baselines. All 12 MFCC-\nbased difference features outperform all other difference\nfeatures with respect to the hit rate metrics.\nFor the supervised approach, we produce results for\nBDS classiﬁers with 20, 50, 100, or 200 decision stumps.\nAll BDS classiﬁers signiﬁcantly out perform all unsuper-\nvised model. By examining the ﬁrst 20 features selected\nduring boosting, we ﬁnd that MFCC, F0, Spec, Chroma\nand Flux difference features are all represented. This sug-\ngests that these features are encoding different cues andthat, when combined, can produce superior performance.\nAnother interesting ﬁnding is that the three smoothing\nwidths and four derivatives are all represented in the ﬁrst\n20 features that are selected. This suggests that it is impor-\ntant to consider multiple time resolutions and to encode\n‘peaks’ when designing a supervised framework that uses\ndifference features.\nREFERENCES\n[1] S. A. Abdallah, K. Noland, M. Sandler, M. Casey, and C. Rhodes.\nTheory and evaluation of a bayesian music structure extractor. In\nISMIR , pages 420–425, 2005.\n[2] R. Dannenberg and N. Hu. Discovering musical structure in audio\nrecordings. In ICMAI , pages 43–57, London, UK, 2002.\n[3] P. Doll ´ar, Z. Tu, and S. Belongie. Supervised learning of edges and\nobject boundaries. In CVPR , June 2006.\n[4] J. Foote. Visualizing music and audio using self-similarity. In ACM\nMULTIMEDIA , pages 77–80, New York, NY , USA, 1999.\n[5] Y . Freund and R. Schapire. A decision-theoretic generalization\nof on-line learning and an application to boosting. In EuroCOLT ,\npages 23–37, 1995.\n[6] M. Goto. Development of the RWC music database. In Interna-\ntional Congress on Acoustics , pages 553–556, 2004.\n[7] M. Goto. A real-time music-scene-description system:\nPredominant-f0 estimation for detecting melody and bass lines in\nreal-world audio signals. Speech Communication , 43(4):311–29,\n2004.\n[8] M. Goto. AIST annotation for RWC music database. In ISMIR ,\nOctober 2006.\n[9] Masataka Goto. A chorus-section detection method for musical\naudio signals and its application to a music listening station. IEEE\nTransactions on Audio, Speech and Language Processing , 14(5),\nSeptember 2006.\n[10] M. Levy, M. Sandler, and M. Casey. Extraction of high-level musi-\ncal structure from audio data and its application to thumbnail gen-\neration. In ICASSP , volume 5, pages V13–V16, May 2006.\n[11] N. C. Maddage, C. Xu, M. S. Kankanhalli, and Xi Shao. Content-\nbased music structure analysis with applications to music seman-\ntics understanding. In ACM MULTIMEDIA , pages 112–119, New\nYork, NY , USA, 2004. ACM Press.\n[12] E. Pampalk. Computational Models of Music Similarity and their\nApplication in Music Information Retrieval . PhD thesis, Vienna\nUniversity of Technology, Vienna, Austria, 2006.\n[13] C. Rhodes, M. Casey, S. Abdallah, and M. Sandler. A markov-\nchain monte-carlo approach to musical audio segmentation. In\nICASSP , volume 5, pages V797–V800, May 2006.\n[14] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Towards\nmusical query-by-semantic description using the CAL500 data set.\nInSIGIR ’07 , 2007.\n[15] P. Viola and M. J. Jones. Robust real-time face detection. Int. J.\nComput. Vision , 57(2):137–154, 2004.\n[16] K. West and S. Cox. Finding an optimal segmentation for audio\ngenre classiﬁcation. In ISMIR , pages 680–685, 2005."
    },
    {
        "title": "Stereo Panning Features for Classifying Recording Production Style.",
        "author": [
            "George Tzanetakis",
            "Randy Jones",
            "Kirk McNally"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417537",
        "url": "https://doi.org/10.5281/zenodo.1417537",
        "ee": "https://zenodo.org/records/1417537/files/TzanetakisJM07.pdf",
        "abstract": "Recording engineers, mixers and producers play important yet often overlooked roles in defining the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classification systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classification tasks and are compared with the traditional Mel-Frequency Cepstral Coefficients. The proposed features can be viewed as a first attempt to capture extra-musical information related to the production process through music information retrieval techniques. 1 INTRODUCTION Starting in the 1960s the recording process for rock and popular music moved beyond the convention of recreating as faithfully as possible the illusion of a live performance. Facilitated by technological advances including multi-track recording, tape editing, equalization and compression, the creative contributions of record producers became increasingly important in defining the sound of artists, groups, and styles [4]. Although not as well known as the artists they worked with, legendary producers including Phil Spector, George Martin, Quincy Jones, and Brian Eno changed the way music was created. So far, research in music information retrieval has largely ignored information about the recording process, focusing instead on capturing information about pitch, rhythm and timbre. A common methodology is to extract features, quantifiable attributes of music signals, from recordings, then to classify these features into distinct groups using machine learning techniques. This two-part process has enabled tasks such as automatic identification of genres, albums and artists. The influence of the recording process on automatic classification has been acknowledged and termed the alc⃝2007 Austrian Computer Society (OCG). bum effect. The performance of artist identification systems degrades when music from different albums is used for training and evaluation [7]. Therefore, the classification results of such systems are not based entirely on the musical content. Various stages of production of the recorded artifact, including recording, mixing, and mastering, all have the potential to influence classification. This has led to research which attempts to quantify the effects of production on acoustic features. By detecting equalization curves used in album mastering, it is possible to compensate for the effects of mastering so that multiple instances of the same song on different albums can be better compared [3]. We believe that other information related to the recording process, specifically mixing, is an important component of understanding modern pop and rock music and should be incorporated rather than being removed from music information retrieval systems. Our goal is to explore stereo panning information as an aspect of the recording and production process. Stereo information has been utilized for source separation purposes [2, 9]. However, to the best of our knowledge, it has not been used in classification systems for audio signals. In this paper we show that stereo panning information is indeed useful for automatic music classification. 2 STEREO PANNING INFORMATION EXTRACTION In this section we describe the process of calculating stereo panning information for different frequencies based on the short-time Fourier transform (STFT) of the left and right channels. Using the extracted Stereo Panning Spectrum we propose features for classification.",
        "zenodo_id": 1417537,
        "dblp_key": "conf/ismir/TzanetakisJM07",
        "keywords": [
            "recording engineers",
            "mixers",
            "producers",
            "sound sources",
            "stereo panning",
            "production process",
            "audio classification",
            "audio features",
            "music information retrieval",
            "stereo panning information"
        ],
        "content": "STEREO PANNING FEATURES FOR CLASSIFYING RECORDING\nPRODUCTION STYLE\nGeorge Tzanetakis, Randy Jones, and Kirk McNally\nUniversity of Victoria\nComputer Science\ngtzan@cs.uvic.ca, rj@csc.uvic.ca, kmcnally@uvic.ca\nABSTRACT\nRecording engineers, mixers and producers play im-\nportant yet often overlooked roles in deﬁning the sound of\na particular record, artist or group. The placement of dif-\nferent sound sources in space using stereo panning infor-\nmation is an important component of the production pro-\ncess. Audio classiﬁcation systems typically convert stereo\nsignals to mono and to the best of our knowledge have not\nutilized information related to stereo panning. In this pa-\nper we propose a set of audio features that can be used to\ncapture stereo information. These features are shown to\nprovide statistically important information for non-trivial\naudio classiﬁcation tasks and are compared with the tra-\nditional Mel-Frequency Cepstral Coefﬁcients. The pro-\nposed features can be viewed as a ﬁrst attempt to capture\nextra-musical information related to the production pro-\ncess through music information retrieval techniques.\n1 INTRODUCTION\nStarting in the 1960s the recording process for rock and\npopular music moved beyond the convention of recreat-\ning as faithfully as possible the illusion of a live perfor-\nmance. Facilitated by technological advances including\nmulti-track recording, tape editing, equalization and com-\npression, the creative contributions of record producers\nbecame increasingly important in deﬁning the sound of\nartists, groups, and styles [4]. Although not as well known\nas the artists they worked with, legendary producers in-\ncluding Phil Spector, George Martin, Quincy Jones, and\nBrian Eno changed the way music was created.\nSo far, research in music information retrieval has largely\nignored information about the recording process, focusing\ninstead on capturing information about pitch, rhythm and\ntimbre. A common methodology is to extract features,\nquantiﬁable attributes of music signals, from recordings,\nthen to classify these features into distinct groups using\nmachine learning techniques. This two-part process has\nenabled tasks such as automatic identiﬁcation of genres,\nalbums and artists.\nThe inﬂuence of the recording process on automatic\nclassiﬁcation has been acknowledged and termed the al-\nc\r2007 Austrian Computer Society (OCG).bum effect. The performance of artist identiﬁcation sys-\ntems degrades when music from different albums is used\nfor training and evaluation [7]. Therefore, the classiﬁ-\ncation results of such systems are not based entirely on\nthe musical content. Various stages of production of the\nrecorded artifact, including recording, mixing, and mas-\ntering, all have the potential to inﬂuence classiﬁcation.\nThis has led to research which attempts to quantify the\neffects of production on acoustic features. By detecting\nequalization curves used in album mastering, it is possible\nto compensate for the effects of mastering so that multi-\nple instances of the same song on different albums can be\nbetter compared [3]. We believe that other information re-\nlated to the recording process, speciﬁcally mixing, is an\nimportant component of understanding modern pop and\nrock music and should be incorporated rather than being\nremoved from music information retrieval systems.\nOur goal is to explore stereo panning information as\nan aspect of the recording and production process. Stereo\ninformation has been utilized for source separation pur-\nposes [2, 9]. However, to the best of our knowledge, it has\nnot been used in classiﬁcation systems for audio signals.\nIn this paper we show that stereo panning information is\nindeed useful for automatic music classiﬁcation.\n2 STEREO PANNING INFORMATION\nEXTRACTION\nIn this section we describe the process of calculating stereo\npanning information for different frequencies based on the\nshort-time Fourier transform (STFT) of the left and right\nchannels. Using the extracted Stereo Panning Spectrum\nwe propose features for classiﬁcation.\n2.1 Stereo Panning Spectrum\nAvendano [2] describes a frequency-domain source iden-\ntiﬁcation system based on a cross-channel metric called\nthepanning index . We use the same metric as the basis\nfor calculating stereo audio features for classiﬁcation. For\nthe remainder of the paper the term Stereo Panning Spec-\ntrum (SPS) is used instead of the panning index as we feel\nit is a more accurate term. The SPS holds the panning\nvalues (between -1 and +1 with 0 being center) for each\nfrequency bin.The derivation of the SPS assumes a simpliﬁed model\nof the stereo signal. In this model each sound source is\nrecorded individually and then mixed into a single stereo\nsignal by amplitude panning. Stereo reverberation is then\nadded artiﬁcially to the mix. The basic idea behind the\nSPS is to compare the left and right signals in the time-\nfrequency plane to derive a two-dimensional map that iden-\ntiﬁes the different panning gains associated with each time-\nfrequency bin. By selecting time-frequency bins with sim-\nilar panning it is possible to separate particular sources\n[2]. In this paper we utilize the SPS directly as the basis\nfor extracting statistical features without attempting any\nform of source separation. Our SPS deﬁnition directly fol-\nlows Avendano [2].\nIf we denote the STFT of the left,right signals xl(t);xr(t)\nfor a particular analysis window as Xl(k);Xr(k), where\nk is the frequency index we can deﬁne the following sim-\nilarity measure:\n (k) = 2jXl(k)X\u0003\nr(k)j\njXl(k)j2+jXr(k)j2(1)\nwhere\u0003denotes complex conjugation. For a single source\nwith amplitude panning the similarity function will have\na value proportional to the panning coefﬁcient \u000bin those\ntime frequency regions where the source has energy. More\nspeciﬁcally if we assume the sinusoidal energy-preserving\npanning law: ar=p\n1\u0000a2\nlthen:\n (k) = 2\u000bp\n1\u0000\u000b2 (2)\nIf the source is panned to the center (i.e \u000b= 0:7071 ) then\nthe function will attain its maximum value of 1, and if\nthe source is completely panned to either side the function\nwill attain its minimum value of zero. The ambiguity with\nregards to the later direction of the source can be resolved\nusing the partial similarity measures:\n l=jXl(k)X\u0003\nr(k)j\njXl(k)j2; r=jXr(k)X\u0003\nl(k)j\njXr(k)j2(3)\nand their difference:\n\u0001(k) = l\u0000 r (4)\nwhere positive values of \u0001(k)correspond to signals panned\ntowards the left and negative values correspond to signals\npanned to the right. Thus we can deﬁne the following\nambiguity-resolving function:\n^\u0001(k) =8\n><\n>:+1;if\u0001(k)>0\n0;if\u0001(k) = 0\n\u00001;if\u0001(k)<0(5)\nShifting and multiplying the similarity function by ^\u0001(k)\nwe obtain the Stereo Panning Spectrum (or panning index)\nas:\nSPS (k) = [1\u0000 (k)]^\u0001(k) (6)\nTimeFrequency BinStereo Panning Spectrum \n100 200 300 400 500 600 700 800 900 1000 1100 120050100150200250300350400450500Figure 1 .Stereo Panning Spectrum of “Hell’s Bells” by\nACDC (approximately 28 seconds).\nTimeFrequency BinStereo Panning Spectrum\n0 200 400 600 800 1000 120050100150200250300350400450500\nFigure 2 .Stereo Panning Spectrum of “Supervixen” by\nGarbage (approximately 28 seconds)\nFigure 1 shows a visualization of the Stereo Panning\nSpectrum for the song “Hell’s Bells” by ACDC. The visu-\nalization is similar to a Spectrogram with the X-axis corre-\nsponding to time, measured in number of analysis frames,\nand the Y-axis corresponding to frequency bin. No pan-\nning is represented by gray, full left panning by black and\nfull right panning by white. The songs starts with four bell\nsounds that alternate between slight panning to the left and\nto the right, visible as changes in grey intensity. Near the\nend of the ﬁrst 28 seconds a strong electric guitar enters\non the right channel, visible as white.\nFigure 2 shows a visualization of the SPS for the song\n“Supervixen” by Garbage. Several interesting stereo ma-\nnipulations can be observed in the ﬁgure and heard when\nlistening to the song. The song starts with all instruments\ncentered for a brief period and then moves them to the left\nand right creating an explosion like effect. Most of the\nsound of a fast repetitive hi-hat is panned to the right (the\nwide dark bar over the narrow horizontal white bar) with\na small part of it panned to the left (the narrow horizontal\nwhite bar). Near the end of the ﬁrst 28 seconds the voice\nenters with the a crash cymbal panned to the right, visible\nas the large black area.\n2.2 Stereo Panning Spectrum Features\nIn this section we describe a set of features that summarize\nthe information contained in the Stereo Panning Spectrum\nthat can be used for automatic music classiﬁcation. The\nmain idea is to capture the amount of panning in different\nfrequency bands as well as how it changes over time.We deﬁne the Panning Root Mean Square for a partic-\nular frequency band as:\nPl;h=vuut1\nh\u0000l+ 1hX\nk=l[SPS (k)]2 (7)\nwherelis the lower frequency of the band, his the high\nfrequency of the band, and Nis the number of frequency\nbins. By using RMS we only consider the amount of pan-\nning without taking into account whether it is to the left\nor right. We consider the following 4-dimensional feature\nvector corresponding to an analysis window t:\n\b(t) = [Ptotal(t);Plow(t);Pmedium (t);Phigh(t)](8)\nThe PRMS values correspond to overall panning (0–22050\nHz), and panning for low (0–250 Hz), medium (250–2500\nHz) and high frequencies (2500–22050 Hz) respectively.\nTo capture the dynamics of panning information we\ncompute a running mean and standard deviation over the\npast M frames:\nm\b(t) =mean [\b(t\u0000M+ 1);::;\b(t)] (9)\ns\b(t) =std[\b(t\u0000M+ 1);::;\b(t)] (10)\nThis results in a 8-dimensional feature vector at the same\nrate as the original 4-dimensional one. For the experi-\nments M is set to 40 corresponding to approximately 0.5\nseconds. To avoid any duration effects on classiﬁcation we\nonly consider approximately the ﬁrst 30 seconds of each\ntrack, resulting in a sequence of 1000 8-dimensional fea-\nture vectors for each track. The tracks are stereo, 16-bit,\n44100 Hz sampling rate audio ﬁles and the STFT win-\ndow size is set to 1024 samples. The sequence of feature\nvectors is collapsed to a single feature vector represent-\ning the entire track by taking again the mean and standard\ndeviation across the ﬁrst 30 seconds resulting in the ﬁnal\n16-dimensional feature vector per track.\n3 EXPERIMENTS\nIn order to evaluate the effectiveness of the proposed fea-\ntures we considered two non-trivial tasks. As a sidenote,\nusing the proposed features it is trivial (although quite use-\nful) to detect mono recordings directly converted to stereo\nwithout remastering.\nThe ﬁrst classiﬁcation task we consider is distinguish-\ning two collections of rock music, one from the 1960s\nand another from the 1990s. In genre terms, these can\nbe loosely categorized as ‘garage’ and ‘grunge.’ Both of\nthese styles would be classiﬁed to the top-level genre of\nrock. To isolate the effects of recording production, we\nonly included albums which had as their main instrumen-\ntation the standard rock ensemble of electric guitar, elec-\ntric bass, drums and vocals. Albums with an excess of\nkeyboards or experimental studio techniques, late 1960s\nBeatles for example, were excluded. We used 227 tracks\nfrom the 1960s and 176 tracks from the 1990s. ExampleGarage/Grunge ZeroR NBC SMO J48\nSPSF 56.4 77.2 81 84.2\nSMFCC 56.4 74. 6 76.7 71.6\nSPSF+SMFCC 56.4 82.7 83.7 83.2\nTable 1 . Classiﬁcation accuracies for Garage/Grunge\nAcoustic/Electric ZeroR NBC SMO J48\nSPSF 51.3 99.4 99.7 99.1\nSMFCC 51.3 71.8 79.4 68.4\nSPSF+SMFCC 51.3 98.5 99.1 99.1\nTable 2 . Classiﬁcation accuracies for Acoustic/Electric\nJazz\n‘garage’ groups include The Byrds, The Kinks and Buddy\nHolly. Example ‘grunge’ groups include Nirvana, Pearl\nJam and Radiohead.\nThe second classiﬁcation task we consider is distin-\nguishing electric jazz from acoustic jazz. Both of these\nstyles would be classiﬁed to the top-level genre of jazz.\nAcoustic jazz tends to have relatively pronounced pan-\nning of the solo instruments (saxophone and trumpet) that\ndoesn’t vary over time. We used 175 electric jazz tracks\nand 184 acoustic jazz tracks. Example electric jazz groups\ninclude: Weather Report, Return to Forever, Medeski, Mar-\ntin and Wood, and Mahavishnu Orchestra. Example acous-\ntic jazz groups led by artists include: Miles Davis, John\nColtrane, Lee Morgan and Branford Marsalis.\nTables 1, 2 show the classiﬁcation accuracy results for\nthe Stereo Panning Spectrum Features and compares them\nwith the results obtained from stereo Mel-Frequency Cep-\nstrum Coefﬁcients (MFCC) (basically the MFCC of the\nleft and right channels concatenated) as well as their com-\nbination for the two tasks. MFCCs are the most com-\nmon feature front-end for evaluating timbral similarity [1].\nThe accuracies are in percentages and are computed us-\ning stratiﬁed 10-fold cross-validation. The ZeroR clas-\nsiﬁer is a simple baseline, NBS corresponds to a simple\nNaive Bayes classiﬁer, SMO corresponds to a linear Sup-\nport Vector Machine trained with Sequential Minimal Op-\ntimization and J48 is a decision tree. More information\nabout these representative classiﬁers can be found in [8]\nor any pattern recognition textbook. As can be seen the\nStereo Panning Spectrum Features (SPSF) perform well\nand for the acoustic vs electric jazz task achieve almost\nperfect classiﬁcation. As a sidenote the classiﬁcation ac-\ncuracy of mono MFCC were almost identical to the stereo\nMFCC therefore were not included in the Tables.\nIt is important to note that the proposed features only\ncapture stereo information and are not inﬂuenced by any\nspectral content or amplitude dynamics. For example ap-\nplying any amplitude changes to both channels doesn’t\nchange their values and the spectrum could be completely\naltered without changing the features as long as the changes\nare proportional to the panning coefﬁcients.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8020406080100120\nAverage Total RMS PanningHistogram for Acoustic Jazz\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8020406080100120\nAverage Total RMS PanningHistogram for Electric JazzFigure 3 .Histogram of mean overall panning for Acous-\ntic Jazz (Left) and Electric Jazz (Right)\nGr/Ga/Aj/Ej ZeroR NBC SMO J48\nSPSF 29.8 73.6 81 76.5\nSMFCC 29.8 56.4 65.9 52.3\nSPSF+SMFCC 29.8 75.2 87.4 79.9\nTable 3 . Classiﬁcation accuracies for four styles\nFigure 3 shows the histograms of a single feature: the\nmean total RMS panning for acoustic jazz (left) and elec-\ntric jazz (right). As can be seen acoustic jazz has lower\nbut more consistent panning values whereas electric jazz\nhas more pronounced and spread panning values.\nTable 3 shows the classiﬁcation results for all four styles\ncombined. Although somewhat artiﬁcial as a task, this\nprovides information about the robustness of the proposed\nfeatures as well as the value of combining the standard\nMFCC features with the proposed Stereo Panning Spec-\ntrum Features.\nResearchers interested in replicating these experiments\ncan obtain the complete lists of tracks and albums for both\nof these tasks by contacting the authors via email. The\ncode for the calculation of the SPS features has been inte-\ngrated into Marsyas1[6], an open source framework for\naudio processing with speciﬁc emphasis on Music Infor-\nmation Retrieval. The machine learning part of the exper-\niments were conducted using Weka2[8].\n4 CONCLUSIONS AND FUTURE WORK\nA new feature set based on the Stereo Panning Spectrum\nwas proposed and shown to be effective for two non-trivial\naudio classiﬁcation tasks. It has been argued that the ap-\nproach of modeling timbral similarity using MFCC has\nreached a “glass ceiling” [1]. We believe that information\nrelated to the recording process such as the stereo panning\ninformation used in this paper can help future audio MIR\nsystems escape this ceiling. More detailed features related\nto stereo information than the ones proposed in this paper\ncan be envisioned. For example, by clustering the panning\nvalues it might be possible to determine how many tracks\nwere used in the mix.\nWe are also interested in exploring other aspects of the\nstudio production process for MIR purposes. Examples\ninclude equalization, compression, and effects including\nreverberation and delay. One of the authors is a profes-\nsional studio recording engineer who teaches recording\n1http://marsyas.sourceforge.net\n2http://www.cs.waikato.ac.nz/ml/weka/techniques. We are planning to develop visualization and\nediting tools that can help reverse-engineer the stereo mix-\ning of audio recordings for pedagogical purposes.\nEngineers communicate about mixing with a particular\nlexicon of qualitative terms. A good example comes from\nan interview with Mix Magazine where Dave Pensado de-\nscribes one of his mixes as having “massive club bottom,\nhip hop sensibility in the middle, and this real smoothed-\nout, classy, Quincy Jones-type top.” [5]. Our hope is to\neventually be able to translate this type of discussion into\na more quantitative domain.\nAcknowledgments\nThe authors would like to thank the National Sciences\nand Engineering Research Council (NSERC) and Social\nSciences and Humanities Research Council (SSHRC) of\nCanada for funding this work, Perry Cook for suggesting\nthe idea of using stereo a long time ago, and Carlos Aven-\ndano for describing his method with sufﬁcient clarity and\ndetail to be re-implemented.\n5 REFERENCES\n[1] J.J. Aucouturier and F. Pachet. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences , 1(1), 2004.\n[2] C. Avendano. Frequency-domain source identiﬁca-\ntion and manipulation in stereo mixes for enhance-\nment, suppression and re-panning applications. In\nProc. IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics (WASPAA) , pages 55–\n58, 2003.\n[3] Y .E. Kim, D.S. Williamson, and S. Pilli. Towards\nquantifying the album effect in artist identiﬁca-\ntion. In Proc. Int. Conf. on Music Information Re-\ntrieval(ISMIR) , pages 393–394, 2006.\n[4] V Mooreﬁeld. The Producer as Composer . MIT Press,\n2005.\n[5] Dave “Hard Drive” Pensado. Interview. Mix Maga-\nzine, September 2001.\n[6] G. Tzanetakis and P. Cook. Marsyas: A framework for\naudio analysis. Organized Sound , 4(3), 2000.\n[7] B. Whitman, G. Flake, and S. Lawrence. Artist de-\ntection in music with minnowmatch. In Proc. IEEE\nWorkshop on Neural Networks for Signal Processing,\n2001 , pages 559–568, 2001.\n[8] I.H. Witten and E. Frank. Data Mining: Practical Ma-\nchine Learning Tools and Techniques . Morgan Kauf-\nmann, 2005.\n[9] J. Woodruff, B. Pardo, and R. Dannenberg. Remix-\ning stereo music with score-informed source separa-\ntion. In Proc. Int. Conf. on Music Information Re-\ntrieval(ISMIR) , 2006."
    },
    {
        "title": "Assessment of State-of-the-Art Meter Analysis Systems with an Extended Meter Description Model.",
        "author": [
            "Matthias Varewyck",
            "Jean-Pierre Martens"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417367",
        "url": "https://doi.org/10.5281/zenodo.1417367",
        "ee": "https://zenodo.org/records/1417367/files/VarewyckM07.pdf",
        "abstract": "An extended meter description model capturing the hierarchical metrical structure of Western music is proposed. The model is applied for the quantitative evaluation of four state-of-the-art automatic meter analysis algorithms of musical audio. Evaluation results suggest that the best beat trackers reach a reasonable level of performance, but that none of the tested algorithms has the potential to perform a reliable bar onset tracking. Moreover, the frontends of the best over-all systems not necessarily seem to have the front-ends best encoding the time signature in their output. Therefore, further improvements of these systems should be attainable by a better combination of ideas that can be borrowed from existing algorithms. 1 INTRODUCTION The temporal characteristics of music reside in an ensemble of perceivable periodic patterns at different time scales. These can be captured in a hierarchical metrical structure, called the meter [6]. The automatic extraction of metrical characteristics in musical audio is of direct importance to applications such as intelligent synchronization, standard editing, semi-automatic mixing and synchronizing audio effects but also higher-order applications such as chord recognition, structure detection and genre classification. Western music usually exhibits a prominent periodicity, called the beat. Many meter analysis algorithms try to track these beats or to determine the corresponding tempo (e.g. [1, 2, 3]). However, tapping experiments have demonstrated [9] that the beat level is a subjective concept. Consequently, focusing too much on the beat level, may not be such a good idea. Performing an analysis involving multiple levels therefore seems a better approach. Although a number of studies describe meter analysis on symbolic (e.g. MIDI, score) data, those based on musical audio remain rather limited. In this study, we focus on the latter. Goto & Muraoka [5, 11] considered a binary meter model with a bar, beat and intermediate level. Obviously, this model only works well if the Inter Timestamp Interval (ITI) between successive timestamps on one level is equal to two times the ITI on the lower level. Klapuri et al. [6] proposed a meter analysis involving three other levels: the c⃝2007 Austrian Computer Society (OCG). bar, beat and tatum level, with ITI ratios of one to nine between subsequent levels. Klapuri’s method can be applied to music with non-binary meters, but it may produce ambiguous irregular time signatures. In this paper, we propose an extended meter description model offering a more complete representation of the time signature of a polyphonic audio excerpt, hereafter called a song. The generality of the model was first inspected by creating meter annotated data. Now, these annotations are used for the quantitative assessment of four state-of-theart automatic meter analysis systems. The meter description model is introduced in section",
        "zenodo_id": 1417367,
        "dblp_key": "conf/ismir/VarewyckM07",
        "keywords": [
            "meter",
            "beat",
            "tempo",
            "metrical",
            "structure",
            "automatic",
            "meter",
            "analysis",
            "audio",
            "excerpt"
        ],
        "content": "ASSESSMENT OF STATE-OF-THE-ART METER ANALYSIS SYSTEMS\nWITH AN EXTENDED METER DESCRIPTION MODEL\nMatthias Varewyck, Jean-Pierre Martens\nDepartment of Electronics and Information Systems\nGhent University (Belgium)\nABSTRACT\nAn extended meter description model capturing the hier-\narchical metrical structure of Western music is proposed.\nThe model is applied for the quantitative evaluation of\nfour state-of-the-art automatic meter analysis algorithms\nof musical audio. Evaluation results suggest that the best\nbeat trackers reach a reasonable level of performance, but\nthat none of the tested algorithms has the potential to per-\nform a reliable bar onset tracking. Moreover, the front-\nends of the best over-all systems not necessarily seem to\nhave the front-ends best encoding the time signature in\ntheir output. Therefore, further improvements of these\nsystems should be attainable by a better combination of\nideas that can be borrowed from existing algorithms.\n1 INTRODUCTION\nThe temporal characteristicsof musicreside inan ensem-\nbleofperceivableperiodicpatternsatdifferenttimescales.\nThesecanbecapturedinahierarchicalmetricalstructure,\ncalled the meter [6]. The automatic extraction of metrical\ncharacteristics in musical audio is of direct importance to\napplications such as intelligent synchronization, standard\nediting, semi-automatic mixing and synchronizing audio\neffects but also higher-order applications such as chord\nrecognition, structure detection and genre classiﬁcation.\nWestern music usually exhibits a prominent periodic-\nity,calledthebeat. Manymeteranalysisalgorithmstryto\ntrackthesebeatsortodeterminethecorrespondingtempo\n(e.g. [1,2,3]). However,tappingexperimentshavedemon-\nstrated[9]thatthebeatlevelisasubjectiveconcept. Con-\nsequently,focusingtoomuchonthebeatlevel,maynotbe\nsuch a good idea. Performing an analysis involving mul-\ntiple levels therefore seems a better approach. Although\na number of studies describe meter analysis on symbolic\n(e.g. MIDI, score) data, those based on musical audio re-\nmain rather limited. In this study, we focus on the latter.\nGoto & Muraoka [5, 11] considered a binary meter\nmodel with a bar, beat and intermediate level. Obviously,\nthismodelonlyworkswelliftheInterTimestampInterval\n(ITI)betweensuccessivetimestampsononelevelisequal\nto two times the ITI on the lower level. Klapuri et al. [6]\nproposedameteranalysisinvolvingthreeotherlevels: the\nc/circlecopyrt2007 Austrian Computer Society (OCG).bar,beatandtatumlevel,withITIratiosofonetoninebe-\ntweensubsequentlevels. Klapuri’smethodcanbeapplied\nto music with non-binary meters, but it may produce am-\nbiguous irregular time signatures.\nInthispaper,weproposeanextendedmeterdescription\nmodelofferingamorecompleterepresentationofthetime\nsignatureofapolyphonicaudioexcerpt,hereaftercalleda\nsong. The generality of the model was ﬁrst inspected by\ncreatingmeterannotateddata. Now,theseannotationsare\nused for the quantitative assessment of four state-of-the-\nart automatic meter analysis systems.\nThe meter description model is introduced in section\n2. Section 3 explains how the annotations enabled us to\ndesignaﬂexiblemethodforthequantitativeevaluationof\nan automatic meter analysis system. Section 4 introduces\nthesongcollectionandthealgorithmsundertest(AUT’s).\nTheexperimentsaredescribedinsection5,whilethemost\nimportant conclusions are summarized in section 6.\n2 METER DESCRIPTIONMODEL\nThe basic hypotheses of the meter description model are\n(1)thattherealwaysexistsaquasiperiodicpatternmarked\nby main temporal accents with a mean ITI between 1.5\nand 6s, (2) that secondary accents characterize the per-\nceivable periodic patterns at smaller time scales, and (3)\nthat these secondary accents can be represented by time-\nstamps positioned on a uniform grid. Main accents are\nassociated with bar transitions. Tempo changes are re-\nstricted to changes in bar lengths.\nThe annotation starts by tapping along with bar on-\nsets, and continues with the manual selection of the most\nappropriate metrical pattern for each bar. As in [7], the\nmodel imposes constraints on the relations between suc-\ncessive timestamps on the same level and timestamps on\nsuccessivelevels. Ifthehighestlevelrepresentsthelargest\nITIandifitislabeledwiththelowestindexzero,thenthe\nmain constraints are: (1) the timestamps on level Lλare\ncopied to level Lλ+1and (2) extra timestamps on level\nLλ+1are obtained from those on level Lλby dividing\neach ITI on Lλinto 2 or 3 equal parts. During each bar,\nthisdivisionratioisthesameforallITIson Lλ. However,\nforoneLλ, we allow that both ratios occur causing two\ndifferent ITIs on Lλ.\nFurthermore, per bar, a salience is assigned to each\nlevel to represent the attentional strength of the temporalFigure 1. Example of a 3-bar annotated metrical pattern.\nThe darkness encodes the level’s salience.\npatternevokedonthatlevel. Majorandmediumsaliences\nare distinguished. One level of each bar must get a ma-\njor salience while the level just above and/or below can\nrecieve a medium salience. (see Figure 1).\n3 EVALUATION METHODOLOGY\nWeaimtocompareasequenceofhypothesizedtimestamps\nHm(m= 1. . . M)againstasequenceofannotatedtime-\nstamps An(n= 1. . . N) and to retrieve a quantitative\nperformance measure. The annotated timestamps consti-\ntute a subset of the multi-level annotation. Some interest-\ningselectionschemesaredescribedinthenextsubsection.\nForeach Anthesetof Hmfallingwithinagiventoler-\nance of Anis determined. The remaining Hmare con-\nsidered insertions. If the set is not empty, Hmclosest\ntoAnis considered as the one generated for Anwhereas\nthe others join the insertions. This simple approach pre-\nsumes that the tolerance is sufﬁciently small to ascertain\nthatnohypothesiscanbecloserthanthetolerancetomore\nthan one annotated timestamp. Given the number of an-\nnotated timestamps ( N), hypotheses ( M), deleted time-\nstamps ( D) and inserted hypotheses ( I) the performance\nmeasuresRecall(R),Precision(P)andF-measure(F)can\nbe computed.\nR=N−D\nN, P=M−I\nM, F=2RP\nP+R(1)\n3.1 Two types of experiments\nIn a ﬁrst type of experiments we compare the hypothe-\nsized timestamps with the annotated timestamps of one\nsingle annotated level across the complete song. One im-\nportant level is the beat level , deﬁned as the annotation\nlevel exhibiting the largest average salience. Another im-\nportant level is the best tracked level , deﬁned as the level\nthatisbestsupportedbythehypothesizedtimestamps. For\neach AUT it is the level offering the best compromise be-\ntween a high Fand a good R/Pbalance, i.e. the level\nminimizing the criterion\nE=4\nF+/vextendsingle/vextendsingle/vextendsingle/vextendsingle1\nP−1\nR/vextendsingle/vextendsingle/vextendsingle/vextendsingle(2)\nIn a second type of experiments the hypothesized time-\nstamps are compared with the annotated timestamps con-\nsidered to represent the correct timestamp sequence . In\ncaseofbeattrackingthe correctbeatsequence isassumedtobetheannotatedtimestampsatthemostsalientlevelof\neachbar,i.e. notallthecorrecttimestampshavetobelong\nto the same level across the song.\n3.2 Tolerance selection\nPreviousassessmentsoftemporalanalysisalgorithmswere\nmade using a tolerance of 17.5% [5, 6] or 10% ([6], bar\nlevel). Ascientiﬁcbasisfortheselectionofagoodthresh-\nold can be found in [4]. In that study, humans were asked\nto insert a missing fourth event in an isochronic sequence\nof length six. Deviations between the obtained and the\nmathematically correct position were about 2.5% of the\nITI, with an absolute minimum of 6 ms for small ITI’s.\nIn order to adhere to other meter analysis evaluations, we\nuse a threshold of 12.5% (5 times 2.5%) of the mean ITI,\nwith a minimum of 30 ms (5 times 6 ms). The condition\nthat a hypothesis cannot be within the tolerance of two\nsuccessive Anis then met if the ITI is larger than 60 ms.\n3.3 Time lag compensation\nSeveral mechanisms may result in a small but consistent\ntimelagbetweenthehypothesizedandtheannotatedtime-\nstamps. This time lag may not just be punished. It was\nfound experimentally that for a given AUT, the average\ntime lag per song exhibits a distribution with a mean Do\nthat is characteristic for the AUT and with a spread that\ncanpartiallyoriginatefromthedifferentcharacteristicsof\ndifferent onsets.\nTherefore, for a certain AUT, we ﬁrst determine Doas\nthe mean time lag across a collection of songs. Per song,\nthis value corresponds to the maximum cross-correlation\nbetweenthehypothesizedandthesmearedannotatedtime-\nstampsonthebeatlevel. Doisboundedbetween ±100ms.\nThe song dependent time lag D∈(Do-20ms, Do+20ms)\nis then determined by computing the cross-correlation a\nsecond time and ﬁnally Dis subtracted from the hypoth-\nesized timestamps before supplying these timestamps to\nthe evaluation program. The smearing converts each an-\nnotated timestamps to a gaussian with a spread equal to\nthe time tolerance on the corresponding level.\n4 EXPERIMENTAL FRAMEWORK\nThe evaluation set contains 30s excerpts of 161 different\nsongs: 120 excerpts were formerly used in the MIREX\n2006 beat tracking and tempo detection contests [8]. The\nremaining 41 were used in a human tapping experiment\n[9]. The average number of annotated levels per excerpt\nis 4.5, with a minimum of 2 and a maximum of 6. Both\nexcerpts and annotations can bedownloaded from [12].\nFour algorithms, which are believed to represent the\nstate of the art in meter analysis, were tested: one full\nmeter analysis algorithm (KLAPURI, [6]) and three beat\ntracking algorithms (DAVIES [1], DIXON [2] & ELLIS\n[3]). All algorithms consist of a front-end producing one\nor more accent functions highlighting the most important\nrhythmic events, and a timestamp inducing back-end.5 EXPERIMENTAL RESULTS\n5.1 Assessing beat tracking abilities\nIn a ﬁrst experiment we examined the beat tracking abili-\nties of the four algorithms. In Table 1, we have listed the\nPrecision, Recall and F-measure with respect to the best\ntracked level per song, as well as the number of times the\nbesttrackedlevelcoincideswiththebeatlevelofthesong.\nIt appears that there are signiﬁcant differences among the\nAUT Do(ms) PRFNbeat\nKLAPURI 00.890.800.843 100\nDAVIES -80.870.830.850 106\nDIXON -20 0.830.850.841 85\nELLIS 200.830.760.793 49\nTable1. Evaluationofbeattrackers: intrinsicdelay( Do),\nprecision (P), recall (R) & F-measure (F) w.r.t. the best\ntrackedlevel,andnumberofsongs(outof161)forwhich\nthe best tracked level is thebeat level.\nAUT’s. Note for instance that although the F-measure of\nDIXON and KLAPURI are quite similar, the former only\ntracks the beat level in about 50% of the songs. Figure 2\n(a) KLAPURI\n (b) DAVIES\n(c) DIXON\n (d) ELLIS\nFigure 2. Evaluation of beat trackers w.r.t. the complete\nmetricalstructure: nr. oftimesthebesttrackedlevelhasa\ngiven offset w.r.t. the beat level, and Recall ( /triangle) & Preci-\nsion (o) in the correspondingsong collections.\nshows that ELLIS, but also DIXON to some extent, tend\nto track the level below the beat level. If this happens,\ntheRecallandPrecisionstayatahighlevel,meaningthat\nthe best tracked level is well tracked. If a level above the\nbeat level is selected, the Precision is usually low, indi-\ncating that a large number of hypotheses did not coincide\nwith an annotated timestamp. To complete our analysis,\nwemeasuredtheperformancesoftheAUT’scomparedto\nthe correct beat sequence (deﬁned in 3.1). Results are in\ncolumns 2-4 of Table 2. Comparison with Table 1 learnsBEAT TRACKER FE + STIU\nAUT PRFPRF\nKLAPURI 0.720.750.731 0.480.630.534\nDAVIES 0.700.790.742 0.480.680.564\nDIXON 0.620.820.709 0.420.510.463\nELLIS 0.480.780.595 0.450.650.533\nTable 2. Evaluation of the full beat tracker and the front-\nend (FE) + simple timestamp induction unit (STIU) w.r.t.\nto the correct beat sequence.\nthatthereismainlyadropinprecisionwhichisaccording\ntothetendencyofthetestedbeattrackerstotrackthelevel\njust below the beat level (seeﬁgure 2).\n5.2 Role of front-end and back-end\nIn a second experiment, we tried to assess the role of\nthe front-ends and back-ends as individual components.\nTherefore,weimplementedasimpletimestampinduction\nunit to extract timestamps directly from the accent func-\ntion, without the inclusion of extra musical knowledge.\nThe simple timestamp induction unit (STIU) involves\na peak generation and peak rejection step. The ﬁrst step\nusesarobustleft-to-rightpeak-valleysearchalgorithm[10].\nThis ﬁrst step is controlled by two parameters: the min-\nimal time difference ∆Tbetween successive peaks and\nthe minimal relative drop in amplitude δAthat must be\nexceeded before searching for the next peak. The peak\nrejection step retains all peaks exceeding some amplitude\nthresholdTH.Persong,theSTIUisrunforfourvaluesof\n∆T(20, 30, 40, 50 ms), three values of δA(0.1, 0.2, 0.3)\nand a user deﬁnable number of values ( NTH) of TH.\nThe performance of the STIU operated with its best\n(∆T, δA, TH) combination was assessed. The results are\nincolumns5-7ofTable2. Comparisontothecorrespond-\ningﬁguresforthefullalgorithm(columns2-4)letuscon-\nclude that the back-ends generally do a good job. Only\nthe ELLIS back-end is not so superior to the STIU. Since\ntheﬁguresincolumn7ofTable2allrepresentresultsob-\ntainedwiththesameback-end,theyexposedifferencesin\nquality of the accent functions. They suggest that the ac-\ncent functions of KLAPURI, DAVIES and ELLIS carry a\ncomparable amount of beat information. DIXON’s front-\nend seems to be inferior to theothers in this respect.\n5.3 Assessing bar and tatumtracking abilities\nInathirdexperimentweassessedtheabilityofKLAPURI\ntotrackthebarandtatumtimestampsfoundonthetopand\nbottom level in the individual bars respectively. Table 3\nTracking level Do(ms) PRFM/N\nBar 5.8 0.450.450.451 0.997\nTatum 0 0.750.700.725 0.933\nTable 3. Evaluation of KLAPURI’s bar and tatum output\n(M/N = nr. of hypothesized versus annotated timestamps)shows that only 45% of the bar onsets are detected. The\ntatum detection results are similar to those for beat track-\ning. As for the beat level , we measured the position of\nthe best tracked level of the bar and tatum outputs with\nrespect to the bar level (the highest level in the hierarchy)\nand thetatum level (the lowest level in more than 50%\nof the bars). Figure 3 illustrates the results. The best\n(a) Bar\n (b) Tatum\nFigure 3. Evaluation of bar and tatum tracking: nr. of\ntimes the best tracked level has a given offset w.r.t. the\nbar/tatumlevel,andRecall( /triangle)&Precision(o)inthecor-\nresponding song collections.\ntracked level for the bar and tatum output is correct for\n79 and 125 songs respectively. The bar tracking results\ncan be explained as follows: the number of hypotheses is\nmore or less correct (see last column in Table 3), but a\nlarge part of these hypotheses do not occur in the vicin-\nity of a bar onset. However, they do occur very often in\nthe vicinity of a timestamp on a lower level. By selecting\nthat lower level, the evaluation program can signiﬁcantly\nraise its Precision. The Recall on its turn is then expected\ntoapproachtheratiobetweenthenumberofhypothesized\ntimestamps and the number of annotated timestamps on\nthat lower level. In case of a binary meter, the Recall at\nlevel 1 would then be close to 0.5 (we ﬁnd 0.43).\nForcompleteness,wealsotestedtheabilityoftheSTIU\ntoretrievethebaronsetsfromthefront-endoutputs. There-\nfore, TH is gradually increased. For a considerable num-\nberofsongs,level2continuestobethebesttrackedlevel.\nThisdemonstratesthatbaronsetsareoftennotmarkedby\nstrongpeaksinthefront-endoutput. TheELLISfront-end\nseems the only one producingsalient peaks at bar onsets.\n6 CONCLUSIONS\nFour state-of-the-art beat tracking algorithms were inves-\ntigated. Answers were provided to the following ques-\ntions: (1) Which metric level of the song agrees best with\nthe outputs of the algorithm? (2) How well do the out-\nputsagreewiththeannotatedtimestampsoccurringonthe\nmost salient level of each individual bar? (3) How much\nis the beat tracking performance affected by the front-\nand back-end of the algorithm? The experimental results\nshowedthatmostbeattrackershaveapreferencefortrack-\ningthebeatlevelofthesong,butforasigniﬁcantnumber\nofsongstheytracktheleveljustbelowthebeatlevel. Be-\ntween75and82%oftheannotatedbeatsarecorrectlyde-tectedwhile28to52%ofthehypothesizedbeatsarefalse\nalarms. The investigation of the bar and tatum tracking\nabilities shows that especially bar onset tracking is inade-\nquate to serve as a basis for high-order applications. All\nback-endsclearlyoutperformasimplebeatinductionunit.\nHowever, we found no evidence that the incorporation of\ninter-level dependencies in the back-end, as in Klapuri et\nal [6], leads to a higher performance.\n7 ACKNOWLEDGMENTS\nThis work was done in the context of the SEMA project,\nfunded by BOF, Ghent. Special thanks go to Anssi Kla-\npuri and Matthew Davies for providing their source code.\n8 REFERENCES\n[1] M.E.P. Davies and M.D. Plumbley, “Beat tracking\nwith a two state model” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP 2005) , vol 3, pp 241-244, 2005\n[2] S. Dixon, “Automatic extraction of tempo and beat\nfromexpressiveperformances”in JournalofNewMu-\nsic Research , vol 30, no. 1, pp 39-58, 2001\n[3] D.P.W. Ellis and G.E. Poliner, “Identifying ’cover\nsongs’ with beat-synchronous chroma features”, in\nMIREX audio cover song evaluation , 2006\n[4] A.FribergandJ.Sundberg,“Timediscriminationina\nmonotonic, isochronous sequence”, in Journal of the\nAc. Soc. of Am. , vol 98, no. 5, pp 2524-2531, 1995\n[5] M. Goto and Y. Muraoka, “Issues in evaluating beat\ntracking systems”, in Proceedings of IJCAI-97 Work-\nshop on Issues in AI and Music , pp 9-16, 1997\n[6] A.P.Klapuri,A.J.EronenandJ.T.Astola,“Analysisof\nthemeterofacousticmusicalsignals”,in IEEETrans-\nactions Speech and Audio Proc. , vol 14, no. 1, 2006\n[7] F. Lerdahl & R. Jackendoff, “A Generative Theory of\nTonal Music”, The MIT press,1982\n[8] D. Moelants and M.F. McKinney, “Tempo perception\nandmusicalcontent-whatmakesapiecefast,slowor\ntemporally ambiguous,” in 8th intern. conference on\nmusic perception & cognition , Evanston, IL, 2004\n[9] M. McKinney and D. Moelants,“Ambiguity in tempo\nperception: what draws listeners to different metrical\nlevels?”, Music Perception, 24(2), pp 155-166, 2006.\n[10] A. Vorstermans, J.P. Martens and B. Van Coile, “Au-\ntomatic segmentation and labeling of multi-lingual\nspeech data”, Speech Comm. 19, pp 271-294, 1996\n[11] M.Goto,“Anaudio-basedreal-timebeattrackingsys-\ntem for music with or without drum-sounds”, Journal\nof New Music Research 30(2),pp 159-171, 2001.\n[12] https://speech.elis.ugent.be/ (see downloads)"
    },
    {
        "title": "Applying Rhythmic Similarity Based on Inner Metric Analysis to Folksong Research.",
        "author": [
            "Anja Volk",
            "Jörg Garbers",
            "Peter van Kranenburg",
            "Frans Wiering",
            "Remco C. Veltkamp",
            "Louis P. Grijp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416830",
        "url": "https://doi.org/10.5281/zenodo.1416830",
        "ee": "https://zenodo.org/records/1416830/files/VolkGKWVG07.pdf",
        "abstract": "In this paper we investigate the role of rhythmic similarity as part of melodic similarity in the context of Folksong research. We define a rhythmic similarity measure based on Inner Metric Analysis and apply it to groups of similar melodies. The comparison with a similarity measure of the SIMILE software shows that the two models agree on the number of melodies that are considered very similar, but disagree on the less similar melodies. In general, we achieve good results with the retrieval of melodies using rhythmic information, which demonstrates that rhythmic similarity is an important factor to consider in melodic similarity. 1 INTRODUCTION In this paper we study rhythmic similarity in the context of melodic similarity as a first step within the interdisciplinary enterprise of the WITCHCRAFT 1 project (Utrecht University and Meertens Institute Amsterdam). The project aims at the development of a content based retrieval system for a large collection of Dutch folksongs that are stored as audio and notation. The retrieval system will give access to the collection Onder de groene linde hosted by the Meertens Institute to both the general public and musical scholars. The collection Onder de groene linde (short: OGL) consists of songs transmitted through oral tradition, hence it contains many variants for one song. In order to describe these variants the Meertens Institute has developed the concept of melody norm 2 which groups historically or ‘genetically’ related melodies into one norm (for more details see [4]). The retrieval system to be designed should assist in defining melody norms for the collection OGL based on the similarity of the melodies in order to support the study of oral transmission. In a first step similar melodies from a given test corpus have been manually classified into groups. These melody groups serve as pos1 What is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes 2 similar to “tune family” and “Melodietyp” c⃝2007 Austrian Computer Society (OCG). sible candidates for the melody norms to be assigned in a later stage. According to cognitive studies, metric and rhythmic structures play a central role in the perception of melodic similarity. For instance, in the immediate recall of a simple melody studied in [8] the metrical structure was the most accurately remembered structural feature. In this paper we demonstrate that melodies belonging to the same melody group can successfully be retrieved based on rhythmic similarity. Therefore we conclude that rhythmic similarity is a useful characteristic for the classification of folksongs. Furthermore, our results show the importance of rhythmic stability within the oral transmission of melodies, which confirms the impact of rhythmic similarity on melodic similarity suggested by cognitive studies. 2 DEFINING A MEASURE FOR SYMBOLIC RHYTHMIC SIMILARITY This section introduces our rhythmic similarity measure that is based on Inner Metric Analysis (IMA).",
        "zenodo_id": 1416830,
        "dblp_key": "conf/ismir/VolkGKWVG07",
        "keywords": [
            "melodic similarity",
            "folksong research",
            "rhythmic similarity",
            "Inner Metric Analysis",
            "SIMILE software",
            "melody norm",
            "oral tradition",
            "content based retrieval",
            "melody group",
            "metrical structure"
        ],
        "content": "APPLYING RHYTHMIC SIMILARITY BASED ON INNER METRIC\nANALYSIS TO FOLKSONG RESEARCH\nAnja Volk, J ¨org Garbers, Peter van Kranenburg, Frans Wiering, Remco C. Veltkamp, Louis P. Grijp*\nDepartment of Information and Computing Sciences\nUtrecht University and *Meertens Institute, Amsterdam\nvolk@cs.uu.nl\nABSTRACT\nIn this paper we investigate the role of rhythmic similar-\nity as part of melodic similarity in the context of Folksong\nresearch. We deﬁne a rhythmic similarity measure based\non Inner Metric Analysis and apply it to groups of simi-\nlar melodies. The comparison with a similarity measure\nof the SIMILE software shows that the two models agree\non the number of melodies that are considered very simi-\nlar, but disagree on the less similar melodies. In general,\nwe achieve good results with the retrieval of melodies us-\ning rhythmic information, which demonstrates that rhyth-\nmic similarity is an important factor to consider in melodic\nsimilarity.\n1 INTRODUCTION\nIn this paper we study rhythmic similarity in the context\nof melodic similarity as a ﬁrst step within the interdisci-\nplinary enterprise of the WITCHCRAFT1project (Utrecht\nUniversity and Meertens Institute Amsterdam). The project\naims at the development of a content based retrieval sys-\ntem for a large collection of Dutch folksongs that are stored\nas audio and notation. The retrieval system will give ac-\ncess to the collection Onder de groene linde hosted by the\nMeertens Institute to both the general public and musical\nscholars.\nThe collection Onder de groene linde (short: OGL )\nconsists of songs transmitted through oral tradition, hence\nit contains many variants for one song. In order to de-\nscribe these variants the Meertens Institute has developed\nthe concept of melody norm2which groups historically or\n‘genetically’ related melodies into one norm (for more de-\ntails see [4]). The retrieval system to be designed should\nassist in deﬁning melody norms for the collection OGL\nbased on the similarity of the melodies in order to sup-\nport the study of oral transmission. In a ﬁrst step simi-\nlar melodies from a given test corpus have been manually\nclassiﬁed into groups. These melody groups serve as pos-\n1What is Topical in Cultural Heritage: Content-based Retrieval\nAmong Folksong Tunes\n2similar to “tune family” and “Melodietyp”\nc/circlecopyrt2007 Austrian Computer Society (OCG).sible candidates for the melody norms to be assigned in a\nlater stage.\nAccording to cognitive studies, metric and rhythmic\nstructures play a central role in the perception of melodic\nsimilarity. For instance, in the immediate recall of a sim-\nple melody studied in [8] the metrical structure was the\nmost accurately remembered structural feature. In this pa-\nper we demonstrate that melodies belonging to the same\nmelody group can successfully be retrieved based on rhyth-\nmic similarity. Therefore we conclude that rhythmic sim-\nilarity is a useful characteristic for the classiﬁcation of\nfolksongs. Furthermore, our results show the importance\nof rhythmic stability within the oral transmission of melo-\ndies, which conﬁrms the impact of rhythmic similarity on\nmelodic similarity suggested by cognitive studies.\n2 DEFINING A MEASURE FOR SYMBOLIC\nRHYTHMIC SIMILARITY\nThis section introduces our rhythmic similarity measure\nthat is based on Inner Metric Analysis (IMA).\n2.1 Inner Metric Analysis\nInner Metric Analysis (see [2], [5]) describes the inner\nmetric structure of a piece of music generated by the ac-\ntual notes inside the bars as opposed to the outer metric\nstructure associated with a given abstract grid such as the\nbar lines. The model assigns a metric weight to each note\nof the piece (which is represented as symbolic data).\nThe details of the model have been described in [2] or\n[1]. The general idea is to search for all pulses (chains\nof equally spaced events) of a given piece and then to as-\nsign a metric weight to each note. The speciﬁc pulse type\nunderlying IMA is called local meter and is deﬁned as\nfollows. Let Ondenote the set of all onsets of notes in a\ngiven piece. We consider every subset m⊂Onof equally\nspaced onsets as a local meter if it contains at least three\nonsets and is not a subset of any other subset of equally\nspaced onsets. Let k(m)denote the number of onsets the\nlocal meter mconsists of minus 1 (we call k(m)thelength\nof the local meter m). Hence k(m)counts the number of\nrepetition of the period (distance between consecutive on-\nsets of the local meter) within the local meter. The metric\nweight of an onset ois calculated as the weighted sum ofthe length k(m)of all local meters mthat coincide at this\nonset ( o∈m).\nLetM(/lscript)be the set of all local meters of the piece of\nlength at least /lscript. The general metric weight of an onset,\no∈On, is as follows:\nW/lscript,p(o) =/summationdisplay\n{m∈M(/lscript):o∈m}k(m)p.\nIn all examples of this paper we have set the parameter\n/lscript= 2, hence we consider all local meters that exist in\nthe piece. In order to obtain stable layers in the metric\nweights of the folksongs we have chosen p= 3. Figure\n1 shows examples of metric weights of three melodies of\nthe melody group Deze morgen in 6/8. The weights are\ndepicted with lines such that the higher the line, the higher\nthe corresponding weight. The background gives the bar\nlines for orientation.\nFigure 1 . Metric weights of similar melodies in 6/8: three\nexamples from the melody group Deze morgen\n2.2 Deﬁning similarity based on IMA\nRhythmic similarity has been used extensively in the au-\ndio domain for classiﬁcation tasks. In contrast to this,\nsimilarity for symbolic data has been less extensively dis-\ncussed so far. Metric weights of short fragments of musi-\ncal pieces have been used in [1] to classify dance rhythms\nof the same meter and tempo using a correlation coefﬁ-\ncient. In this paper we measure the rhythmic-metric sim-\nilarity between two complete melodies. The similarity\nmeasure is carried out on the analytical information given\nby the metric weights. The application of the measure to\nfolk songs in the following section is a ﬁrst and simple ap-\nproach in so far as it does not contain the search for similar\nsegments that are shifted in time.\nIn a ﬁrst step we deﬁne for each of the two pieces the\nmetric weight of all silence events as zero and hence ob-\ntain the metric grid weight which assigns a weight to all\nevents. The silence events are inserted along the ﬁnest grid\nof the piece determined by the greatest common divisor of\nall time intervals between consecutive onsets.In a second step we adapt the grids of the pieces to\na common ﬁner grid by adding events ewith the weight\nzero. In the third step, the metric grid weight is split into\nconsecutive segments that cover an area of equal duration\nin the piece. These segments contain the weights to be\ncompared with the correlation coefﬁcient, we therefore\ncall them correlation windows . The ﬁrst correlation win-\ndow of each piece starts with the ﬁrst full bar, hence the\nweights of an upbeat are disregarded. For all examples of\nthis article we have set the size of the correlation window\nto one bar of the query.\nFor the computation of the similarity measure both grid\nweights are completely covered with correlations windows.\nLetwi, i=1,...,n denote the consecutive correlation win-\ndows of the ﬁrst piece and vj, j=1,...,m those of the sec-\nond piece. Let ck, k=1,...,min (n,m )denote the correlation\ncoefﬁcient between the grid weights that are covered by\nthe windows wkandvk. Then we deﬁne the similarity\nIMA c,sthat is deﬁned on the subsets of the two musi-\ncal pieces from the beginning until the end of the shorter\npiece as the mean of all correlation coefﬁcients:\nIMA c,s=1\nmin(n, m )min (n,m )/summationdisplay\nk=1ck\n3 RESULTS FOR THE TEST CORPUS\nOur current test corpus of digitized melodies from OGL\nconsists of 141 melodies. In a ﬁrst classiﬁcation attempt\nall melodies have been manually classiﬁed into groups of\nsimilar melodies.\n3.1 Results using IMA c,s\nTable 1 gives an overview over the results with the sim-\nilarity measure IMA c,s. For each melody group (listed\nin the ﬁrst column), an example query is presented (listed\nin the third column) with the corresponding ranks for all\nmembers of the melody group in the fourth column.3The\nlast column lists the mean of all group member ranks ac-\ncording to the example query. In addition to the example\nquery we have computed ranking lists using each member\nof the melody group once as the query. The second col-\numn lists the mean over all these ranks of melodies that\nbelong to the group. Hence it represents an average over\nthe distances between the group members.\nIn the following we investigate for the example queries\nthe reasons for the assignment of a low rank. Some melody\ngroups contain melodies of different meter types. Melodies\nthat are notated with a different meter than the query are\nresponsible for low ranks in the melody group Deze mor-\ngen(ranks 136, 137, 140 and 141), Halewijn 4 (ranks 85\nand 139), Halewijn 5 (rank 88), Frankrijk 2 (all ranks be-\ntween 100 and 128), Jonkheer 1 (ranks 96 and 129) and\nMoeder 1 (rank 92). For the melody group Deze morgen\nandFrankrijk 2 we have therefore created subgroups of\n3If two melodies have exactly the same similarity distance to the\nquery, they are both assigned the same rank.Melody Group Query Ranks for Mean\nGroup Mean group members Rank\nDeze 56.48 19914 1, 4, 5, 6, 8, 9, 11, 42.94\nmorgen 14, 17, 18, 27,\n136, 137, 140, 141\nHalewijn 2 12.58 19201 1, 2, 3, 4, 5, 5, 7, 10.18\n8, 11, 18, 48\nHalewijn 4 41.11 19107 1, 2, 3, 5, 6, 8, 26.45\n9, 16, 17, 85, 139\nHalewijn 5 32.53 19106 1, 2, 3, 10, 16, 30, 25.5\n54, 88\nFrankrijk 1 12.55 19301 1, 2, 3, 4, 5, 6, 6, 8.63\n6, 6, 23, 33\nFrankrijk 2 54.65 19304 1, 1, 3, 4, 5, 6, 7, 45.51\n8, 9, 10, 11, 12,\n13, 14, 15, 16, 17,\n18, 18, 20, 21, 22,\n23, 24, 25, 26, 28,\n29, 30, 32, 35, 62,\n77, 78, 79, 95, 97,\n100, 103, 105, 112,\n114, 120, 121, 122,\n123, 128\nJonkheer 1 51.39 22621 1, 2, 10, 13, 29, 39, 39.87\n96, 129\nMoeder 1 22.58 33006 1, 2, 3, 4, 6, 9, 10, 15.27\n11, 13, 17, 92\nTable 1 . Query results using IMA c,s\nmelodies belonging to the same meter as displayed in Ta-\nble 2 showing much better ranking lists.\nThe melody at rank 23 for the group Frankrijk 1 is no-\ntated with doubled note values. Furthermore, low ranks\nwere assigned to melodies that contain a meter change\nwithin the piece for Deze morgen (rank 27) and Frankrijk\n2a(ranks 62, 77, 78, 79 and 97). In summary, the main\nreason for a low rank according to IMA c,sis a very differ-\nent rhythmic structure expressed by a different meter no-\ntation in the transcription. On the other hand, the rhythmic\nstructure seems to be an important component of melodic\nsimilarity for many melody groups. For instance, among\nthe ﬁrst 15 ranks we ﬁnd 9 out of all 11 melodies for\nHalewijn 2 , similar good results are achieved for the groups\nDeze morgen 6/8 ,Halewijn 4 ,Frankrijk 1 ,Moeder 1 and\nFrankrijk 2b (see Table 4 for the complete list).\nAn improvement of this approach could be achieved by\nshifting the shorter melody along the longer and to search\nfor the most similar submelody. The similarity measure\nrhytGauss from the SIMILE package contains such a rou-\ntine, hence one might expect better results with rhytGauss .\nWhile IMA c,smeasures the similarity of metric weights\nthat reﬂect regularity patterns of the onsets of the notes,\nrhytGauss measures the similarity of Gaussiﬁcations4of\n4A gaussiﬁcation GRis a linear combination of gaussians centered at\nthe onsets of the given rhythm R. Hence rhytGauss can also be applied\nto unquantized data.Melody Group Query Ranks for Mean\nGroup Mean group members Rank\nDeze morgen 16.36 19914 1, 4, 5, 6, 8, 9, 11 11.08\nsubgroup 6/8 13, 14, 17, 18, 27\nFrankrijk 2a 31.56 19304 1, 1, 3, 4, 5, 6, 7, 8, 9, 24.88\nsubgroup 6/8 10, 11, 12, 13, 14, 15,\n16, 17,18, 18, 20, 21,\n22, 23, 24, 25, 26, 28,\n29, 30, 32, 35, 62,\n77, 78, 79, 97\nFrankrijk 2b 10.58 24105 1, 2, 3, 4, 5, 6, 7, 6.81\nsubgroup 3/4 8, 9, 10, 20\nTable 2 . Query results with IMA c,sfor subgroups of\nmelodies of the groups Frankrijk 2 andDeze morgen\nthe onset times (see [3]). In the following section we com-\npare our results to those of rhytGauss .\n3.2 Comparison of IMA c,storhytGauss\nThe SIMILE package (see [7] and [6]) contains the sim-\nilarity measure rhytGauss that is based on cross correla-\ntions of Gaussiﬁcations. The rhytGauss algorithm shifts\nthe shorter of the two melodies along the longer one and\ntakes the maximum of all similarity values as the ﬁnal sim-\nilarity value. Since rhytGauss takes tempo information\ninto account, all midi ﬁles of the melodies from the test\ncorpus have been set to the same tempo.\nMelody Query Ranks for group members Mean\nGroup Rank\nDeze morgen 19914 1, 2, 3, 4, 5, 6, 8, 11, 12, 13, 38.625\n16, 17, 117,122, 140, 141\nHalewijn 2 19201 1, 2, 3, 5, 6, 6, 9, 10, 15.54\n10, 11, 108\nHalewijn 4 19107 1, 2, 3, 64, 95, 95, 95 73.64\n106, 110, 118, 121\nHalewijn 5 19106 1, 2, 9, 11, 46, 73, 103, 125 46.25\nFrankrijk 1 19301 1, 2, 3, 3, 3, 3, 7, 8, 8, 47, 80 15\nJonkheer 1 22621 1, 2, 3, 4, 17, 21, 93, 102 30.375\nMoeder 1 33006 1, 2, 3, 4, 7, 8, 15, 18, 37, 24.72\n46, 131\nDeze morgen 19914 1, 2, 3, 4, 5, 6, 8, 11, 8.16\nsubgroup 6/8 12, 13, 16, 17,\nFrankrijk 2a 19304 1, 1, 3, 4, 5, 6, 7, 8, 9, 10, 32.41\nsubgroup 6/8 11, 11, 13, 14, 15, 16, 17,\n18, 19, 21, 23, 24, 31, 35,\n37, 38, 43, 45, 54, 61, 66,\n77, 92, 104, 112, 116\nFrankrijk 2b 24105 1, 2, 3, 4, 5, 5, 7, 8, 11, 17.36\nsubgroup 3/4 13, 140\nTable 3 . Query results using rhythGauss\nThe comparison of the mean ranks of the example queries\nfor both methods (see Tables 1, 2, and 3) reveals, with theexception of the groups Deze morgen andJonkheer 1 , bet-\nter (lower) mean ranks for IMA c,s.\nMelody Group Query IMA c,s rhytGauss\nDeze morgen 6/8 19914 9 (12 ) 10 (12 )\nHalewijn 2 19201 9 (11) 10 (11)\nHalewijn 4 19107 7 (11) 3 (11)\nHalewijn 5 19106 4 (8) 4 (8)\nFrankrijk 1 19301 9 (11) 9 (11)\nFrankrijk 2a 19304 31 (36) 26 (36)\nFrankrijk 2b 24105 10 (11) 10 (11)\nJonkheer 1 22621 4 (8) 4 (8)\nMoeder 1 33006 9 (11) 7 (11)\nTable 4 . Comparison of the number of melodies ranked\namong the ﬁrst 15 hits (for Frankrijk 2a among the ﬁrst\n40 hits). Numbers in brackets refer to the total number of\nmelodies belonging to the melody group.\nThe greatest difference between the ranking lists of the\ntwo measures can be observed in the results for the melody\ngroups Halewijn 4 andHalewijn 5 . For instance, the low\nranks of 103, 73 and 46 in the group Halewijn 5 according\ntorhytGauss correspond to much higher ranks according\ntoIMA c,s(3, 30 and 16).\nWith the exception of the groups Deze morgen andJonk-\nheer 1 the similarity measure IMA c,sobtains on average\nbetter results than rhytGauss despite the fact that the lat-\nter includes the search for the most similar subset in the\nlonger melody. The comparison of the ranks assigned\nto the same melody shows that rhytGauss assigns to 17\nmelodies a considerably higher rank5thanIMA c,s(on\naverage 20.65 ranks better). However, IMA c,sassigns\nto 30 melodies a considerably higher rank than rhytGauss\n(on average 49.9 ranks better).\nThe comparison of the number of songs that are found\nwithin the top 15 matches (top 40 matches for Frankrijk\n2a) as listed in Table 4 shows no great differences between\nthe two methods compared (with the exception of Hale-\nwijn 4 ). Hence the difference between the methods seems\nto apply to melodies that are more distant to the query.\n4 CONCLUSION\nOur results show that rhythmic similarity is an important\ningredient of the similarity between melodies that have\nbeen classiﬁed into groups of similar melodies.\nA further reﬁnement of our proposed rhythmic similar-\nity measure is the search for the most similar submelody\nwithin the longer melody by shifting the metric weight of\nthe shorter melody along the weight of the longer melody.\nWith the current approach of IMA c,sthe additional phra-\nses of the longer piece have in some cases a great impact\non the weight of the entire piece and may change the met-\nric weight of otherwise similar parts. In a further develop-\nment we could test whether using the analysis of the sub-\n5with a difference of more than 10 ranksmelody (deﬁned by the length of the shorter melody) leads\nto better results than using the subset of the weight of the\nentire piece. The comparison with the rhytGauss measure\nindicates on average better results for IMA c,s. However,\nthe compared methods agree on how many melodies are\nvery similar to the query. A more detailed investigation of\nthe examples that were ranked very differently will help\nto clarify which similarity measures are the most appro-\npriate for which query type within the retrieval system to\nbe designed within the WITCHCRAFT project. Prelim-\ninary ﬁndings using IMA c,son a larger corpus includ-\ning 1100 melodies from the Essen Folksong collection in-\ndicate promising results for the application of rhythmic\nsimilarity to Folksong research. Hence we conclude that\nrhythmic similarity is an important ingredient of melodic\nsimilarity.\n5 ACKNOWLEDGMENTS\nThis research has been funded by the Nederlandse Organi-\nsatie voor Wetenschappelijk Onderzoek within the WITCH-\nCRAFT-project NWO 640-003-501. We thank Daniel M ¨ul-\nlensiefen and Klaus Frieler for providing and assisting us\nwith the SIMILE package. We thank Ellen van der Grijn\nfrom the Meertens Institute for classifying the melodies of\nour test corpus into groups of similar melodies.\n6 REFERENCES\n[1] Chew, E., V olk, A., and Lee, C. ”Dance Music Clas-\nsiﬁcation Using Inner Metric Analysis”, Proceedings\nof the 9th INFORMS Computer Society Conference ,\nKluwer, 2005, pp. 355-370.\n[2] Fleischer (V olk), A. Die analytische Interpretation.\nSchritte zur Erschließung eines Forschungsfeldes am\nBeispiel der Metrik . dissertation.de - Verlag im Inter-\nnet Gmbh, Berlin, 2003.\n[3] Frieler, K. ”Beat and Meter Extraction using Gaussi-\nﬁed Onsets”. ISMIR Proceedings , Barcelona, 2004.\n[4] van Kranenburg, P., Garbers, J., V olk, A., Wiering, F.,\nGrijp, L.P., Veltkamp, R. C. ”Towards Integration of\nMIR and Folk Song Research” ISMIR Proceedings ,\nVienna, 2007.\n[5] Mazzola, G. The Topos of Music . Birkh ¨auser, 2002.\n[6] M ¨ullensiefen, D. and Frieler, K. ”Cognitive Adequacy\nin the Measurement of Melodic Similarity: Algorith-\nmic vs. Human Judgements”. Computing in Musicol-\nogy 13 , p. 147-177, 2004.\n[7] M ¨ullensiefen, D. and Frieler, K. The SIMILE algo-\nrithms documentation 0.3 . White Paper, 2006.\n[8] Sloboda, J.A. and Parker, D.H.H. ”Immediate recall of\nmelodies”. In Hower, P., Cross, I. and West, R. (eds),\nMusical structure and cognition , London, Academic\nPress, p.143-167, 1985."
    },
    {
        "title": "Keyword Generation for Lyrics.",
        "author": [
            "Bin Wei",
            "Chengliang Zhang",
            "Mitsunori Ogihara"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418369",
        "url": "https://doi.org/10.5281/zenodo.1418369",
        "ee": "https://zenodo.org/records/1418369/files/WeiZO07.pdf",
        "abstract": "This paper proposes a scheme for content based keyword generation of song lyrics. Syntactic as well semantic similarity is used for sentence level clustering to separate the topic from the background of a song. A method is proposed to search for a center in the semantic graph of WordNet for generating keywords not contained in original text. 1 INTRODUCTION The growth of Web lyrics databases such as lyrics.com and absolutelyric.com raises the issue of processing and understanding lyrics automatically. However, previous work (see [4]) suggests that lyrics are tough for natural language processing. There are three issues that are specific to keyword extraction, First, the “correct” outcome is usually subtle. Unlike other text data such as news, topic words in lyrics have a very small number of occurrences. Instead, a large portion will be devoted to the background. This makes the frequency information not so useful, or even misleading. Second, because lyrics are free-style, approaches based on word positions also face difficulty. Third, for some lyrics, meaningful keywords may not even be included in the original text. Some sort of induction is needed to find suitable keywords. To handle the first two issues, we propose to use a sentence-level clustering so as to separate the topic from the background and eliminate the position information irrelevant. To handle the third issues, we propose to uses various relation links of WordNet 1 , assuming that the center of the discovered links serves as the central keyword. Keywords generated for each intra-lyric cluster are compared among lyrics, enabling separation of lyrics on similar topics with different backgrounds. Our use of WordNet is more comprehensive than the existing WordNet lexical-cohesion-based approach of (Silber and McCoy [5]) called Lexical Chain, in that we not only focus on nouns and concept hierarchy but try to combine verbs and adjectives and logical relations in the analysis. The sentence-level clustering enables us to constrain our search in a compact subgraph of WordNet rather than the whole net. Also, our analysis is different in that the goal is not only to find the relations among words in 1 http://WordNet.princeton.edu c⃝2007 Austrian Computer Society (OCG). the lyrics but to introduce new words as the suitable keywords that may not necessarily occur in the original text. 2 KEYWORD GENERATION We use the Stanford parser for obtaining dependencies, where a dependency is a triple of the form (relation, governor, dependent). We then use the Lesk measurement for word similarity inthe WordNet, and the algorithm in [2] for word sense disambiguation.",
        "zenodo_id": 1418369,
        "dblp_key": "conf/ismir/WeiZO07",
        "keywords": [
            "content based",
            "keyword generation",
            "song lyrics",
            "syntactic similarity",
            "semantic similarity",
            "sentence level clustering",
            "topic separation",
            "background elimination",
            "WordNet",
            "lexical cohesion"
        ],
        "content": "KEYWORD GENERATION FOR LYRICS\nBin Wei\nU. Rochester\nComp. Sci. Dept.Chengliang Zhang\nU. Rochester\nComp. Sci. Dept.Mitsunori Ogihara\nU. Rochester\nComp. Sci. Dept.\nABSTRACT\nThis paper proposes a scheme for content based keyword\ngeneration of song lyrics. Syntactic as well semantic sim-\nilarity is used for sentence level clustering to separate the\ntopic from the background of a song. A method is pro-\nposed to search for a center in the semantic graph of Word-\nNet for generating keywords not contained in original text.\n1 INTRODUCTION\nThe growth of Web lyrics databases such as lyrics.com\nand absolutelyric.com raises the issue of processing and\nunderstanding lyrics automatically. However, previous\nwork (see [4]) suggests that lyrics are tough for natural\nlanguage processing. There are three issues that are spe-\nciﬁc to keyword extraction, First, the “correct” outcome is\nusually subtle. Unlike other text data such as news, topic\nwords in lyrics have a very small number of occurrences.\nInstead, a large portion will be devoted to the background.\nThis makes the frequency information not so useful, or\neven misleading. Second, because lyrics are free-style,\napproaches based on word positions also face difﬁculty.\nThird, for some lyrics, meaningful keywords may not even\nbe included in the original text. Some sort of induction is\nneeded to ﬁnd suitable keywords.\nTo handle the ﬁrst two issues, we propose to use a\nsentence-level clustering so as to separate the topic from\nthe background and eliminate the position information ir-\nrelevant. To handle the third issues, we propose to uses\nvarious relation links of WordNet1, assuming that the\ncenter of the discovered links serves as the central key-\nword. Keywords generated for each intra-lyric cluster are\ncompared among lyrics, enabling separation of lyrics on\nsimilar topics with different backgrounds.\nOur use of WordNet is more comprehensive than\nthe existing WordNet lexical-cohesion-based approach of\n(Silber and McCoy [5]) called Lexical Chain, in that we\nnot only focus on nouns and concept hierarchy but try to\ncombine verbs and adjectives and logical relations in the\nanalysis. The sentence-level clustering enables us to con-\nstrain our search in a compact subgraph of WordNet rather\nthan the whole net. Also, our analysis is different in that\nthe goal is not only to ﬁnd the relations among words in\n1http://WordNet.princeton.edu\nc/circlecopyrt2007 Austrian Computer Society (OCG).the lyrics but to introduce new words as the suitable key-\nwords that may not necessarily occur in the original text.\n2 KEYWORD GENERATION\nWe use the Stanford parser for obtaining dependencies,\nwhere a dependency is a triple of the form (relation, gov-\nernor, dependent). We then use the Lesk measurement for\nword similarity inthe WordNet, and the algorithm in [2]\nfor word sense disambiguation.\n2.1 Sentence Level Clustering\nAfter the previous steps, we obtain a set of senses and a\nset of dependencies for a given sentence. To combine the\nsimilarity of words in the sentence while preserving the\nsemantics, we view the dependency as a proper unit of\nmeaning, as most phases can be represented in this form.\nWe ﬁrst deﬁne the similarity between two dependencies\nas:\nSim(Da, Db) =/summationdisplay\nWi∈Da,Wj∈DbSim(Wi, Wj).\nWe then view a sentence as a collection of meaning units.\nThe similarity between sentences is thus deﬁned as:\nSim(Sa, Sb) = max\nDi∈Sa,Dj∈SbSim(Di, Dj).\nAfter obtaining pairwise sentence similarity, we use the\ndata-driven clustering algorithm in [1]. The algorithm ac-\ncepts a normalized distance matrix Pas input, and per-\nforms a random walk, with transition matrix P, either un-\ntil convergence or for a speciﬁed number of steps. It can\nautomatically decide the number of clusters by optimizing\nspectral properties of P.\n2.2 Candidate Keywords Selection\nWe then select candidate keywords inside the cluster.\nSince lyrics are often not well-formed and thus nouns\nalone do not offer adequate information, we take adjec-\ntives, adverbs, and verbs into consideration, too. The\nranking of the words inside a cluster is based on their sim-\nilarity to other members of the cluster:\nScore (Wi) =/summationdisplay\nWj∈CkSim(Wi, Wj)\nBased on this score, we keep only top ncandidates for the\nnext step (we set n= 10 in our work).2.3 Candidate Generation and Final Selection\nWordNet is a freely available electronic dictionary. In\nWordNet, each node stands for a set of synonyms, and\nnodes are connected through links that represent seman-\ntic relationships. These relations enable us to ﬁnd the\nwords logically implied by the original text (for example,\nwe may get the cause of a certain outcome by following\nthe cause link), which solves the “hidden keyword” issue.\nWe thus assume that if a sense is reached by many words\nin the original text, it is a good candidate for keyword.\nThe algorithm takes as input a set of words from the\nlyrics, and executes breadth-ﬁrst traversal on each word\nfor a given number of steps. To cross the boundary of\nPOS, we also view the glossary ﬁeld as a kind of link. We\nparse each glossary and extract the words that turn up as\nsubjects, verbs, or objects. We then simply choose their\nmost common senses and think of these senses as con-\nnected with the original sense. The words reachable from\na signiﬁcant percentage(30%) of words will be added to\nthe original word list as a candidate.\n2.4 Cluster Selection\nOur last step is to pick out a cluster for each lyric to stand\nfor the whole. In other words, we need to separate the\nparts concerning the topic from the rest. We assume that\nthe topics of a lyric are shared with other lyrics and thus\ncommonly appear, while the background cluster is spe-\nciﬁc to each song and thus is not so common. Thus we\nuse the degree of overlap for selecting a topic cluster. Let\ncijdenote the j-th cluster for the i-th lyric and wijits\nkeywods. The score for cijis:\nσij=/summationtext\nk∈Tmax /lscript/bardblcij∩ck/lscript/bardbl\n/bardblT/bardbl,\nwhere Tis the set of ksuch that for some /lscriptthe set wk/lscript\nhas at least three keywords with cij.\n3 EVALUATION\nWe perform our test on DigiTrad2, a database that con-\ntains approximately 8,000 folk songs and provides con-\ntent based (like “school”, “marriage”, etc) keywords. For\nthe ﬁrst test, we run our program on the same data sets\nused in [4]: SONG1 consisting of 408 songs composed\nof two clusters, labeled “murder” and “marriage” respec-\ntively in DigiTrad, and SONG2 consisting of 424 songs,\nlabeled “political” and “religion”. We ﬁrst extract the top\n5 words using our approach. We then perform a simple\nunsupervised clustering based on the semantic similarity\nbetween sets of keywords, using the same data-driven ap-\nproach in [1]. We then compare the error rate against the\nsemantic rule learner work in [4]. We observe that our\nperformance offers improvement in accuracy. Given that\nour method is unsupervised, while the previous method is\nsupervised, we consider this as a substantial improvement.\n2http://www.mudcat.org/download.cfmMethod Data Balance Error\nScott&Matwin SONG1 200/224 30.23%\nProposed SONG1 195/212 28.14%\nScott&Matwin SONG2 194/238 32.64%\nProposed SONG2 200/224 31.22%\nTable 1 . The comparison between two methods. The third\ncolumn shows the split between the two clusters.\nLabel Size Hit in Top 5 Hit in Any\nmarriage 195 47 64\nmurder 212 50 78\npoverty 94 25 35\nschool 29 9 11\nTable 2 . The result of topic keywords hits.\nAnother test is performed by directly counting the\nnumber of hits our keywords make in the keywords pro-\nvided in DigiTrad. We generate the top 5 keywords both\nfor the lyric and for each clusters of the lyric, and check\nwhether the given label is contained. For the same lyric,\nmultiple hits from different clusters are counted as one.\nConsidering that the labels given are general and we allow\nkeywords of different POS, we accept the different forms\nof the same word (such as “marriage” and “marry”), and\nwords with very similar meanings (such as “school” ver-\nsus “college”/“university”). For some lyrics, we succeed\nin separating the topics from noise after the sentence level\nclustering, but we fail in getting the right cluster.\n4 FUTURE WORK\nThe potential sources of error are the word sense disam-\nbiguation and words/links missing from WordNet. We\nmay be able to improve the performance by using semi-\nsupervised methods. Also, more sophisticated word sense\ndisambiguation could produce more accurate result. The\nuse of word co-occurrence in discussion forums and re-\nviews may help in ﬁnd more words.\n5 REFERENCES\n[1] A. Azran and Z. Ghahramani. In Proc. 23rd ICML ,\npages 57–64, 2006.\n[2] S. Patwardhan, S. Banerjee, and T. Pedersen. In Proc.\n4th CICLing , pages 241–257, 2003.\n[3] T. Pedersen, S. Patwardhan, and J. Michelizzi. In Proc.\n19th AAAI , pages 1024–1025, 2004.\n[4] S. Scott and S. Matwin. In COLING-ACL 98 Work-\nshop, pages 38–44, 1998.\n[5] H. G. Silber and K. F. McCoy. Comput. Ling. ,\n28(4):487–496, 2002."
    },
    {
        "title": "An Experiment on the Role of Pitch Intervals in Melodic Segmentation.",
        "author": [
            "Tillman Weyde",
            "Jens Wissmann",
            "Kerstin Neubarth"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417591",
        "url": "https://doi.org/10.5281/zenodo.1417591",
        "ee": "https://zenodo.org/records/1417591/files/WeydeWN07.pdf",
        "abstract": "This paper presents the results of an experiment to test the influence of IOI, dynamics, pitch change, and pitch direction change on melodic segmentation, extending an an earlier experiment [4]. The new results show little to no significant influence of pitch, when evaluated by a linear or log-linear statistical model with regression. This supports the earlier findings, which are in contrast to the commonly made assumption that greater pitch intervals lead to melodic segmentation. 1 INTRODUCTION The segmentation of melodies plays an important role in melody perception, cognition, and retrieval (e.g. [3], [5]) and applications in music e-learning, like automated generation of exercises from a given annotated music score. A common assumption is that the Gestalt Principles of proximity and similarity can be applied to the different musical dimensions of pitch, time, dynamics, and to derived quantities such as the change of direction in successive pitch intervals (see [2], [1]). This assumption has been tested empirically in [4] for pitch intervals up to 5 semitones, where neither pitch intervals nor changes of pitch interval direction were significant in linear or log-linear regression models. There was however the question whether there could be a significant effect for larger pitch intervals. 2 EXPERIMENTAL DESIGN The experiment uses mostly the same design as [4], which is therefore described here only briefly: The experiment uses a forced-choice design, where subjects listen to a melody and are asked whether the length of segments in the melody is 2 or 3 notes. Subjects were presented short melodic sequences, which were designed to be completely isochronous and uniform except for two conflicting segmentation cues, of which one indicated a segmentation into groups of two notes and the other into groups of three. The intensity of the cues was varied. This approach was chosen to approximate the situation of actual melodies, where there is normally more than one cue present. Pairs of cue types were used, as all combinations of values for three or more factors would have led to huge numbers of stimuli. c⃝2007 Austrian Computer Society (OCG). Four cues were tested: inter-onset-intervals, loudness accents, pitch intervals, and changes in pitch direction. Each of the cues was varied in several steps. The following values were used: • additional inter-onset-interval values of 30, 60, 90, 120, and 150 ms • loudness accents of 15, 30, 45, 60, and 75 MIDI velocity units • pitch intervals between notes of size 2, 4, 6, 7, 8, 9, 10, 11, and 12 semitones, alternating up and down • changing direction, with pitch intervals of 1, 2, 3, and 4 semitones between every pair of successive notes Each of these was used to cue segments of both two and three notes length. For the smaller pitch intervals not all values were used, as they had already been completely tested in [4]. In addition, a set of examples with only one factor used in segmentation cues was created for each factor and two melodies without segmentation cues, i.e. completely uniform and isochronous sequences. Some additional parameters were varied at random in this experiment: the initial pitch and loudness, the assignment of factors to group lengths, and the total length of the melody. They were also used as independent variables in the regression analyses, to find out if and how they influence the segmentation. The experiments were conducted in one session where each subject listened to all stimuli. The stimuli were presented via MIDI with a piano-like sound on a personal computer with a program that asked to choose of either ’2’ or ’3’ as preferred segment length. The stimuli were presented in random order with a short break of randomised length between the presentations. The subjects were ten music students between 20 and 23 years of age, five male and five female. 3 RESULTS In the following selected results and regression analyses of the experiments are presented.",
        "zenodo_id": 1417591,
        "dblp_key": "conf/ismir/WeydeWN07",
        "keywords": [
            "experiment",
            "melodic segmentation",
            "IOI",
            "dynamics",
            "pitch change",
            "pitch direction change",
            "linear regression",
            "log-linear regression",
            "Gestalt Principles",
            "proximity and similarity"
        ],
        "content": "AN EXPERIMENT ON THE ROLE OF PITCH INTERV ALS IN MELODIC\nSEGMENTATION\nTillman Weyde, Jens Wissmann, Kerstin Neubarth\nCity University London\nDepartment of Computing\nABSTRACT\nThis paper presents the results of an experiment to test\nthe inﬂuence of IOI, dynamics, pitch change, and pitch\ndirection change on melodic segmentation, extending an\nan earlier experiment [ 4]. The new results show little to\nno signiﬁcant inﬂuence of pitch, when evaluated by a lin-\near or log-linear statistical model with regression. This\nsupports the earlier ﬁndings, which are in contrast to the\ncommonly made assumption that greater pitch intervals\nlead to melodic segmentation.\n1 INTRODUCTION\nThe segmentation of melodies plays an important role in\nmelody perception, cognition, and retrieval (e.g. [ 3], [5])\nand applications in music e-learning, like automated gen-\neration of exercises from a given annotated music score. A\ncommon assumption is that the Gestalt Principles of prox-\nimity and similarity can be applied to the different musical\ndimensions of pitch, time, dynamics, and to derived quan-\ntities such as the change of direction in successive pitch\nintervals (see [ 2], [1]). This assumption has been tested\nempirically in [ 4] for pitch intervals up to 5 semitones,\nwhere neither pitch intervals nor changes of pitch interval\ndirection were signiﬁcant in linear or log-linear regression\nmodels. There was however the question whether there\ncould be a signiﬁcant effect for larger pitch intervals.\n2 EXPERIMENTAL DESIGN\nThe experiment uses mostly the same design as [ 4], which\nis therefore described here only brieﬂy: The experiment\nuses a forced-choice design, where subjects listen to a\nmelody and are asked whether the length of segments in\nthe melody is 2 or 3 notes. Subjects were presented short\nmelodic sequences, which were designed to be completely\nisochronous and uniform except for two conﬂicting seg-\nmentation cues, of which one indicated a segmentation\ninto groups of two notes and the other into groups of three.\nThe intensity of the cues was varied.\nThis approach was chosen to approximate the situation\nof actual melodies, where there is normally more than one\ncue present. Pairs of cue types were used, as all combina-\ntions of values for three or more factors would have led to\nhuge numbers of stimuli.\nc°2007 Austrian Computer Society (OCG).Four cues were tested: inter-onset-intervals, loudness\naccents, pitch intervals, and changes in pitch direction.\nEach of the cues was varied in several steps. The follow-\ning values were used:\n²additional inter-onset-interval values of 30, 60, 90,\n120, and 150 ms\n²loudness accents of 15, 30, 45, 60, and 75 MIDI\nvelocity units\n²pitch intervals between notes of size 2, 4, 6, 7, 8, 9,\n10, 11, and 12 semitones, alternating up and down\n²changing direction, with pitch intervals of 1, 2, 3,\nand 4 semitones between every pair of successive\nnotes\nEach of these was used to cue segments of both two and\nthree notes length. For the smaller pitch intervals not all\nvalues were used, as they had already been completely\ntested in [ 4].\nIn addition, a set of examples with only one factor used\nin segmentation cues was created for each factor and two\nmelodies without segmentation cues, i.e. completely uni-\nform and isochronous sequences. Some additional param-\neters were varied at random in this experiment: the ini-\ntial pitch and loudness, the assignment of factors to group\nlengths, and the total length of the melody. They were also\nused as independent variables in the regression analyses,\nto ﬁnd out if and how they inﬂuence the segmentation.\nThe experiments were conducted in one session where\neach subject listened to all stimuli. The stimuli were pre-\nsented via MIDI with a piano-like sound on a personal\ncomputer with a program that asked to choose of either ’2’\nor ’3’ as preferred segment length. The stimuli were pre-\nsented in random order with a short break of randomised\nlength between the presentations. The subjects were ten\nmusic students between 20 and 23 years of age, ﬁve male\nand ﬁve female.\n3 RESULTS\nIn the following selected results and regression analyses\nof the experiments are presented.\n3.1 Pairwise Experiments\nIn the IOI-Pitch stimuli we varied pitch and inter-onset-\nintervals. Table 1shows the logistic regression analysis\nof the results with segmentation conformance as the de-\npendent variable. It shows clearly that the effect of inter-\nonset intervals is signiﬁcant, while that of pitch intervalsTable 1 .Regression analysis of the IOI pitch factors.\nis not. Surprisingly, the other signiﬁcant variables are gen-\nder, age, and the length of the melody (in that order). All\nother factors are not signiﬁcant.\nIn the Pitch-Direction stimuli the regular pitch inter-\nvals change direction and additional pitch intervals. The\nregression analysis shows that only the assignment of the\ndynamics cue to the three note segmentation has a signiﬁ-\ncant effect.\nThe Pitch-Velocity stimuli combine the pitch changes\nwith velocity accents. In the regression analysis only the\nstrength of the loudness accents aresigniﬁcant, while the\npitch intervals were not. Interestingly gender and the as-\nsociation of dynamics with segments of size three were\nhighly signiﬁcant.\n3.2 Single-factor experiments\nIn addition to the pairwise experiments, we used several\nstimuli that contained only segmentation cues using a sin-\ngle factor. The hypothesis here was that with only one\nfactor being available, this would directly cause segmen-\ntation boundaries. In addition the question was whether\nthere would be a higher likelihood of user segmentation\nconforming to the cues, depending on the strength of the\ncue.\nThe actual results were dominated by saturation ef-\nfects, only IOI shows a signiﬁcant linear relation to the\nsegmentation.\n4 DISCUSSION\nThe results of the pairwise tests show signiﬁcant effects\non perceived segmentation for velocity and IOIs, but not\nfor pitch or change of pitch interval direction. This re-\nsult is consistent over all tested pairs. It is also consis-\ntent with the earlier experiment in [ 4] and extends this to\nlarger intervals up to an octave. This contrasts to most cur-rently accepted assumptions on melodic perception being\ndirectly related to pitch interval size ([ 2], [1]). The single\nfactor stimuli shows that pitch intervals do however have\nan inﬂuence on segmentation if they appear as the only\nsegmentation cue.\nIn addition there are some surprising results that gender\nand in one case age shows signiﬁcant effects. The effect\nof age seems likely to be not generalisable due to the small\nage range of the subjects. Also unexpectedly, there was an\ninteraction between the segment length and type of cue in\nsome cases, indicating that some factors are more effec-\ntive for certain segmentations. Other factors, like absolute\npitch, loudness, tempo, and melody length had little to no\ninﬂuence in the ranges tested.\nThe results are consistent throughout this and previous\ntests, showing signiﬁcant inﬂuence of pitch and velocity\non segmentation. It seems therefore justiﬁed to claim that\nrhythm and dynamics have a considerably stronger inﬂu-\nence on segmentation than pitch interval size and direction\nchanges, at least considering linear and log linear effects\nin the ranges of intervals up to an octave. It seems how-\never plausible that pitch does have a signiﬁcant effect on\nmelodic segmentation, as it plays such an important role\nin melody and music in general. The data of the pairwise\nexperiments indicate the ﬁfth and octave may have spe-\ncial effects. This seems to indicate that the so far assumed\nlinear relation has to be reconsidered and replaced with a\nmore complex model.\nAcknowledgment\nThis work is partially conducted within the i-Maestro\nproject supported by the European Community under the\nInformation Society Technologies (IST) priority of the 6th\nFramework Programme for R&D (IST-026883).\nReferences\n[1]Cambouropoulos, E. (2001). The local boundary de-\ntection model (lbdm) and its application in the study\nof expressive timing. In Proceedings of the Interna-\ntional Computer Music Conference 2001 , pages 290–\n293, Havanna, Cuba.\n[2]Lerdahl, F. and Jackendoff, R. (1983). A Generative\nTheory of Tonal Music . The MIT Press, Cambridge,\nMass.\n[3]Melucci, M. and Orio, N. (2004). Combining\nmelody processing and information retrieval tech-\nniques: methodology, evaluation, and system imple-\nmentation. J. Am. Soc. Inf. Sci. Technol. , 55(12):1058–\n1066.\n[4]Weyde, T. (2004). On the inﬂuence of pitch on\nmelodic segmentation. In Proceedings of the Fifth\nInternational Conference on Music Information Re-\ntrieval , Barclona. Universitat Pompeu Fabra.\n[5]Weyde, T. and Datzko, C. (2005). Efﬁcient melody\nretrieval with motif contour classes. In Proceedings\nof the 3rd International Conference on Music Informa-\ntion Retrieval , London, UK. Queen Mary University of\nLondon."
    },
    {
        "title": "Synthesized Polyphonic Music Database with Verifiable Ground Truth for Multiple F0 Estimation.",
        "author": [
            "Chunghsin Yeh",
            "Niels Bogaards",
            "Axel Röbel"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415732",
        "url": "https://doi.org/10.5281/zenodo.1415732",
        "ee": "https://zenodo.org/records/1415732/files/YehBR07.pdf",
        "abstract": "To study and to evaluate a multiple F0 estimation algorithm, a polyphonic database with verifiable ground truth is necessary. Real recordings with manual annotation as ground truth are often used for evaluation. However, ambiguities arise during manual annotation, which are often set up by subjective judgements. Therefore, in order to have access to verifiable ground truth, we propose a systematic method for creating a polyphonic music database. Multiple monophonic tracks are rendered from a given MIDI file, in which rendered samples are separated to prevent overlaps and to facilitate automatic annotation. F0s can then be reliably extracted as ground truth, which are stored using SDIF. 1 INTRODUCTION F0 (fundamental frequency) is an essential descriptor for periodic signals such as speech and music. Multiple F0 estimation aims at extracting the fundamental frequencies of concurrent sources. In the field of MIR (Music Information Retrieval), the multiple F0s serve as low-level acoustic features for building up high-level representation of music notes. In recent years, quite a few multiple F0 estimation algorithms have been developed for music signals but no common database (corpus + ground truth) for evaluation exists. In order to study and to evaluate a multiple F0 estimation algorithm, a polyphonic music database with reliable ground truth is necessary. There are mainly three types of polyphonic signals used as evaluation corpus: mixtures of monophonic samples, synthesized polyphonic music and real recordings. Mixtures of monophonic samples allow a diversity of combinations among different notes and instruments [1]. The ground truth is extracted by means of single F0 estimation, which can be verified more easily. The concern, however, is that the final mixtures may not have the same statistical properties as those found in real music. To increase the relevance of the test corpus for real world applications, the corpus should take into account musical structures. Synthesized polyphonic music can be rendered from MIDI [2] files by sequencers with sound modules c⃝2007 Austrian Computer Society (OCG). or samplers. Real recordings can be recordings of multitracks or stereo/mono mix-down tracks. Despite the wide availability of music corpora, establishing their ground truth remains an issue. We propose a systematic method to synthesize polyphonic music that allows to use existing single F0 estimation algorithms to establish the ground truth. In addition, the availability and the interchangeability of ground truth data together with the corpus is a main concern for evaluation. Therefore, we plan to distribute this polyphonic database with ground truth available in the SDIF (Sound Description Interchange Format) format. This paper is organized as follows. In section 2, we present innovative tools developed within IRCAM’s AudioSculpt application [3] for manual annotation or score alignment and discuss the issues. In section 3, we present a systematic method for creating a synthesized polyphonic music database. The method is reproducibleand the ground truth is verifiable. The format in which to store the ground truth is described in section 4. Finally, we discuss the evaluation concerns and the possibility of extending this method for different MIR evaluation tasks. 2 MANUAL ANNOTATION OF REAL RECORDINGS Nowadays, more and more evaluations use real recordings of mix-down tracks with manually annotated ground truth [4] [5]. The annotation process usually starts with a reference MIDI file and then aligns the note onsets and offsets to the observed spectrogram. Under the assumption that the notes in the reference MIDI file correspond exactly to what have been played in the real performance, the annotation process is in fact “score alignment” with audio signals. At IRCAM, innovative tools in AudioSculpt (see Figure 1) have been developed to facilitate verification and modification of signal analysis and manual annotation. Given a reference MIDI file and a real recording of the same musical piece, we first align the MIDI notes to the real recording automatically [6]. Then, details like note offs, slow attacks, etc., are manually corrected using AudioSculpt according to the following procedure:",
        "zenodo_id": 1415732,
        "dblp_key": "conf/ismir/YehBR07",
        "keywords": [
            "polyphonic database",
            "verifiable ground truth",
            "multiple F0 estimation",
            "real recordings",
            "manual annotation",
            "synthesized polyphonic music",
            "MIR evaluation",
            "SDIF format",
            "score alignment",
            "AudioSculpt application"
        ],
        "content": "SYNTHESIZEDPOLYPHONIC MUSICDATABASE WITHVERIFIABLE\nGROUND TRUTH FORMULTIPLEF0 ESTIMATION\nChunghsin Yeh\nIRCAM / CNRS-STMS\nParis, France\nChunghsin.Yeh@ircam.frNielsBogaards\nIRCAM\nParis,France\nNiels.Bogaards@ircam.frAxelRoebel\nIRCAM / CNRS-STMS\nParis, France\nAxel.Roebel@ircam.fr\nABSTRACT\nTo study and to evaluate a multiple F0 estimation algo-\nrithm, a polyphonicdatabase with veriﬁable groundtruth\nis necessary. Real recordings with manual annotation as\ngroundtruth are often used for evaluation. However, am-\nbiguities arise during manual annotation, which are often\nset up by subjective judgements. Therefore, in order to\nhave access to veriﬁable ground truth, we propose a sys-\ntematicmethodforcreatingapolyphonicmusicdatabase.\nMultiple monophonic tracks are rendered from a given\nMIDIﬁle,inwhichrenderedsamplesareseparatedtopre-\nvent overlaps and to facilitate automatic annotation. F0s\ncan then be reliably extracted as ground truth, which are\nstoredusingSDIF.\n1 INTRODUCTION\nF0 (fundamental frequency) is an essential descriptor for\nperiodic signals such as speech and music. Multiple F0\nestimationaimsatextractingthefundamentalfrequencies\nof concurrent sources. In the ﬁeld of MIR (Music In-\nformation Retrieval), the multiple F0s serve as low-level\nacousticfeaturesforbuildinguphigh-levelrepresentati on\nof music notes. In recent years, quite a few multiple F0\nestimationalgorithmshavebeendevelopedformusicsig-\nnals but no commondatabase (corpus+ groundtruth) for\nevaluationexists. Inordertostudyandtoevaluateamulti-\npleF0estimationalgorithm,apolyphonicmusicdatabase\nwithreliablegroundtruthisnecessary.\nTherearemainlythreetypesofpolyphonicsignalsused\nas evaluation corpus: mixtures of monophonic samples,\nsynthesized polyphonic music and real recordings. Mix-\ntures of monophonicsamples allow a diversity of combi-\nnations among different notes and instruments [1]. The\nground truth is extracted by means of single F0 estima-\ntion, which can be veriﬁed more easily. The concern,\nhowever,is that the ﬁnal mixturesmay not have the same\nstatistical properties as those found in real music. To in-\ncrease the relevance of the test corpus for real world ap-\nplications, the corpus should take into account musical\nstructures. Synthesizedpolyphonicmusiccanberendered\nfrom MIDI [2] ﬁles by sequencers with sound modules\nc/circlecopyrt2007AustrianComputerSociety(OCG).or samplers. Real recordings can be recordings of multi-\ntracksor stereo/monomix-downtracks. Despite the wide\navailability of music corpora, establishing their ground\ntruth remains an issue. We propose a systematic method\nto synthesize polyphonic music that allows to use exist-\ningsingleF0estimationalgorithmstoestablishtheground\ntruth. Inaddition,theavailabilityandtheinterchangeab il-\nityofgroundtruthdatatogetherwith thecorpusisa main\nconcern for evaluation. Therefore, we plan to distribute\nthispolyphonicdatabasewithgroundtruthavailableinthe\nSDIF (SoundDescriptionInterchangeFormat)format.\nThis paper is organized as follows. In section 2, we\npresent innovative tools developed within IRCAM’s Au-\ndioSculpt application [3] for manual annotation or score\nalignmentanddiscussthe issues. In section 3, we present\nasystematicmethodforcreatingasynthesizedpolyphonic\nmusicdatabase. Themethodisreproducibleandtheground\ntruthisveriﬁable. Theformatinwhichtostoretheground\ntruth is described in section 4. Finally, we discuss the\nevaluation concerns and the possibility of extending this\nmethodfordifferentMIRevaluationtasks.\n2 MANUALANNOTATIONOFREAL\nRECORDINGS\nNowadays,moreandmoreevaluationsuserealrecordings\nofmix-downtrackswithmanuallyannotatedgroundtruth\n[4][5]. Theannotationprocessusuallystartswitharefer-\nence MIDIﬁle andthenalignsthe noteonsetsandoffsets\nto the observed spectrogram. Under the assumption that\nthe notesin the referenceMIDI ﬁle correspondexactlyto\nwhat have been played in the real performance, the an-\nnotation process is in fact “score alignment” with audio\nsignals. At IRCAM, innovativetoolsin AudioSculpt(see\nFigure1)havebeendevelopedtofacilitateveriﬁcationand\nmodiﬁcationofsignalanalysisandmanualannotation.\nGivenareferenceMIDIﬁleandarealrecordingofthe\nsame musical piece, we ﬁrst align the MIDI notes to the\nreal recording automatically [6]. Then, details like note\noffs, slow attacks, etc., are manually corrected using Au-\ndioSculptaccordingto thefollowingprocedure:\n1. OverlayMIDInotesonthespectrogramasa piano-\nroll like representation. Adjust MIDI note grid by\ntuningforthe bestreferencefrequencyat noteA4.2. Generatetimemarkersbyautomaticonsetdetection\n[7]andadjusttheprobabilitythresholdaccordingto\nthe spectrogram.\n3. Verify and adjust note onsets detected around tran-\nsient markersvisuallyandauditively. Inadditionto\nthewaveformandspectrogram,the harmonicstool ,\ninstantaneous spectrum (synchronous to the navi-\ngation bar), etc., provide visual cues for the evolu-\ntionofharmoniccomponents. The diapason allows\naccurate measurementand sonic synthesisat a spe-\nciﬁctime-frequencypoint. Scrubprovidesinstanta-\nneoussynthesisofasingleFFTframe,whichallows\nusers to navigate auditivelyat any speed controlled\nby hand. Users can also listen to arbitrarily shaped\ntime-frequencyzones.\n4. Alignmarkersautomaticallytotheveriﬁedtransient\nmarkersusing magneticsnap .\n5. IfanyinconsistencyisfoundbetweentheMIDIﬁle\nandtherealperformance,missingnotescanbeadded\nandunwantednoteseliminated.\nFigure 1. Screenshot of AudioSculpt during annotation\nshowingMIDInotes,MIDInotegrids,onsetmarkers,the\ninstantaneousfrequencyspectrumandtheharmonicstool.\nDespite all the powerful tools for manual annotation,\ntiming ambiguities need to be resolved based on subjec-\ntive judgements. Above all, for reverberated recordings,\nreverberation extends the end of notes and overlaps with\nthefollowingnotesintimeandinfrequency.\nIf one aims to ﬁnd when a musician stops playing a\nnote,a scientiﬁc descriptionof reverberation[8] is neces -\nsary to identify the end of the playing. Due to reverber-\nation, real recordingsof monodicinstrumentsusually ap-\npeartobepolyphonic,whichrequiresamultipleF0track-\ning [9]. To our knowledge, the description of reverber-\nation is not yet available for polyphonic recordings in a\nreverberantenvironment.\nOn the other hand, if one deﬁnes the end of a note as\ntheendofitsreverberatedpart,theambiguityoccurswhen\n(1) certain partials are boosted by the room modes and\nextendlongerthan the others, and when (2) reverberationtails are overlappedby the followingnotesand the endof\nreverberationisnotobservable.\nIfmanualannotation/alignmentisreliablydonefornon-\nreverberatedrecording,itisstilldisputableatwhichacc u-\nracy one can extract multiple F0s as groundtruth. Due to\nall the issuesdiscussed above,evaluationbased onunver-\niﬁablereferencedataendangersthetrustworthinessofthe\nreported performance. Therefore, we believe that ground\ntruth should be derived by means of an automatic proce-\ndure from the isolated clean notes of the polyphonic mu-\nsic.\n3 METHODOLOGYFORCREATING\nPOLYPHONICMUSIC DATABASE\nTo be able to improvethe validityof the groundtruth, we\nproposetheuseofsynthesizedmusicrenderedfromMIDI\nﬁles. The biggest advantage of synthesized music is that\none can have access to every single note from which the\nground truth can be established. The argument against\nsynthesized music is often that it is “non-realistic”, but\nfew raise doubts about the ground truth. It seems that\nMIDInoteeventdataisconsideredasgroundtruth,which\nis not true. In fact, MIDI note off events are mes-\nsagesrequestingthesoundmodules/samplersto start ren-\ndering the end of notes, which usually extends the notes\nto sound longer after note off . Thus, creating the ref-\nerencedatafortherenderedaudiosignal fromitsoriginal\nMIDI ﬁle is not straightforward. The extended note du-\nration depends on the settings of sound modules or sam-\nplers, which is controllableand thuspredictable. In order\nto retain each sound source for reliable analysis as auto-\nmatic annotation,we present a systematic methodto syn-\nthesize polyphonic music from MIDI ﬁles together with\nveriﬁablegroundtruth.\nGiven a MIDI ﬁle, there are several ways to synthe-\nsizeamusicalpiece: mixingmonophonicsamplesaccord-\ning to MIDI note on events [10] [11], rendering MIDI\nﬁles using sequencers with either sound modules, soft-\nwareinstruments,orsamplers. WechoosetorenderMIDI\nﬁles with samplers for the following reasons. Firstly, se-\nquencers and samplers (or Sound Bank players) allow us\nto render MIDI ﬁles with real instrument sound samples\ninto more realistic music. Many efforts have been made\nto provide large collections of musical instrument sound\nsamples such as McGill University Master Samples1,\nIowa Musical Instrument Samples2, IRCAM Studio On\nLine3andRWC MusicalInstrumentSoundDatabase4.\nThese sample databases contain a variety of instruments\nwith different playing dynamicsand styles for every note\nin playable frequency ranges, and they are widely used\nfor research. Secondly, there exists an enormous amount\nof MIDI ﬁles available for personal use or research. This\nis a great potentialfor expandingthe database. Currently,\n1http://www.music.mcgill.ca/resources/mums/html/inde x.htm\n2http://theremin.music.uiowa.edu/MIS.html\n3http://forumnet.ircam.fr/402.html?&L=1\n4http://staff.aist.go.jp/m.goto/RWC-MDB/rwc-mdb-i.ht mlwe use the RWC Musical Instrument Sound Database to-\ngetherwith the StandardMIDI Files (SMF) ofRWC Mu-\nsic Database [12] [13]. There are a total of 3544 sam-\nplesof50instrumentsinRWC-MDB-I-2001and315high\nquality MIDI Files in RWC-MDB-C-2001-SMF, RWC-\nMDB-G-2001-SMF,RWC-MDB-J-2001-SMF,RWC-MDB-\nP-2001-SMF and RWC-MDB-R-2001-SMF. Finally, we\nare free to edit MIDI ﬁles for evaluation purposes and\nmake differentversions from the original MIDI ﬁles. For\nexample,limitingthemaximalconcurrentsourcesbysolo-\ning the designated tracks, changing instrument patches,\nmixingwithorwithoutdrumsandpercussiontracks,etc..\n3.1 Musical instrumentsound samples\nWhile continuous efforts are being undertaken to manu-\nally annotate music scene descriptors for RWC musical\npieces [14], no attention is paid to labeling RWC Musi-\ncalInstrumentSoundDatabaseRWC-MDB-I-2001. Each\nsoundﬁleinRWC-MDB-I-2001isacollectionofindivid-\nual notesacrosstheplayingrangeofthe instrumentanda\nmute gap was inserted between adjacent notes. The seg-\nmentation should not only separate individual notes but\nalso detect onsets for rendering precise timing of MIDI\nnote on eventsbecauseforcertaininstrumentsamples,\nharmonicsoundsareprecededbybreathyornoisyregions.\nIf the samples are segmented right after the silence gap,\nthey sometimes lead to noticeable delays when triggered\nby MIDI events to be played by samplers. These noisy\nparts in musical instrument soundscome from the fact of\nthe sound generation process. The sources of excitation\nfor musical instruments are mechanical or acoustical vi-\nbratorswhichcausetheresonator(instrumentbody)tovi-\nbratewithit. Theresultofthiscoupledvibratingsystemis\nthesetting-upofthe regimesofoscillation [15]. Aregime\nofoscillationisthestatethatthecoupledvibrationsyste m\nmaintainsa steady oscillation containingseveral harmon-\nically related frequency components. It is observed that\nwheninstrumentsareplayedwithlowerdynamics( pp),it\ntakes much more time to establish the regimesof oscilla-\ntion. In order to achieve precise onset rendering, we use\nAudioSculpt for segmenting individual notes. We share\nthe labeled onset markers in SDIF format to facilitate the\nreproductionoftheproposedmethoddescribedbelow.\n3.2 Creatinginstrument patches\nWhen receiving MIDI event messages, samplers can ren-\ndermusicalinstrumentsamplesaccordingtothe keymaps\ndeﬁned in an instrument patch . A sample can be as-\nsigned to a group of MIDI notes called a keyzone. A set\nof keyzones is called a keymap, which deﬁnes the map-\nping of individual samples to the MIDI notes at speciﬁed\nvelocities. For each MIDI note of a keymap, we assign\nthreesamplesofthesameMIDInotenumberbutdifferent\ndynamics (often labeled as ff,mfandpp). The mapping\nofthethreedynamicsto128velocitystepsis listedin Ta-\nble 1. In this way, an instrument patch includes all the\nsamples of a speciﬁc playing style, which results in more\nFigure 2. Comparison of MIDI notes with the spectro-\ngramoftherenderedaudiosignal\ndynamicsinthe renderedaudiosignals. Currently,we fo-\ncusonthe twoplayingstyles: normalandpizzicato.\ndynamics MIDIvelocityrange\nff 100-127\nmf 44-99\npp 0-43\nTable 1. Mapping the playing dynamics to the MIDI ve-\nlocityrange\n3.3 Rendering MIDI ﬁles into multiple monophonic\naudiotracks\nOnce instrument patches are created, MIDI ﬁles can be\nrendered into polyphonic music by a sequencer+sampler\nsystem. Direct rendering of all the tracks into one audio\nﬁlewouldpreventthepossibilityofestimatingtheground\ntruth using single-F0 estimation algorithm. One might\nthen suggest to render each MIDI track separately. How-\never,thisisnota propersolution,notonlyforpolyphonic\ninstrumenttracks(piano,guitar,etc.) butalsoformonodi c\ninstrumenttracks.\nTodiscusstheissues,oneexampleisillustratedinFig-\nure 2. The MIDI notes are extracted from the ﬂute track\nofLe Nozze di Figaro in RWC-MDB-C-2001-SMF. Af-\nter rendering them by a sequencer+sampler system using\nRWCﬂutesamples,thespectrogramoftherenderedaudio\nsignal is shown along with the MIDI notes. Each rectan-\ngle represents one MIDI note, with time boundaries de-\nﬁned by note on andnote off , and with frequency\nboundaries deﬁned by a quarter tone from its center fre-\nquency. It is observed that even if the MIDI note events\ndon’t overlap, the rendered signals may overlap in time\nand frequency, depending on the delta time between the\nnote events and the release time parameter of the instru-\nmentpatch.\nIn order to access individual sound sources for veri-\nﬁable analysis, it is necessary to prevent the overlaps of\nconcurrent notes as well as those of consecutive notes.Therefore,weproposetospliteachMIDItrackintotracks\nofseparatenotessuchthattherenderedsignalsdon’tover-\nlap. Giventhereleasetimesettingofaninstrumentpatch,\nconcurrent and consecutive notes in a MIDI track can be\nsplit intoseveraltrackswith thefollowingcondition:\nTnoteon (n)≥Tnote off (n−1) +Trelease (1)\nwhere Tnoteon (n)is thenote on time of the current\nnote,Tnote off (n−1)is thenote off time of the pre-\nvious note and Treleaseis the release time setting of the\ninstrumentpatch. Inthisway,therenderednotesareguar-\nanteedtonotoverlaponeanotherandwecanalwaysrefer\nto individual sound sources whenever necessary. When\nsplitting a MIDI ﬁle into several ones, the control mes-\nsages5areretainedinthesplittrackssuchthatindividual\nnotes are exactly the same as those rendered in the poly-\nphonicresult(seeFigure3).\nnote track Ntempo track\ncontrol messagesMIDI note events\ntempo track\ncontrol messages. . . \n. . . . . . MIDI note eventstempo track\ncontrol messagestempo track\ncontrol messages\ntempo track\ncontrol messagestempo track\ncontrol messagestempo track\ncontrol messagesMIDI note events MIDI note events\nMIDI note events MIDI note events MIDI note eventsinstrument track K−1 instrument track K instrument track 1\nnote track 1 note track N−1\nFigure 3 . Splitting MIDI ﬁles into several containing\ntracksofseparatenotes\n3.4 Groundtruth\nOnce notes are rendered into non-overlapping samples,\nwe are able to establish the ground truth from the anal-\nysis of each renderedsample. The groundtruth of funda-\nmentalfrequenciesshouldbeframedependant. Giventhe\nMIDI note number, the reference F0 can be calculated as\nfollows:\nFnote=FA4\n32·2(MIDInote number −9)/12(2)\nItisnotalwayscorrecttocalculate Fnotewitha ﬁxed FA4\n(for example, 440Hz) because the tuning frequency FA4\nmay differ and moreover, recorded samples may not be\nplayedintune. InFigure2,MIDInotesareplacedatcen-\nter frequencies calculated with FA4= 440. The D6 note\naround 1200 Hz either (1) has a higher tuning frequency,\nor(2)isnotplayedin tune.\n5Here include channel messages such as pitch bend.\nFigure4. GroundtruthofmultipleF0tracks\nIn order to obtain precise F0s as ground truth, F0 es-\ntimation is carried out twice for each sample: a coarse\nsearch followed by a ﬁne search. The coarse search uses\nFnotewithFA4= 440forafrequencyrange Fnote·[0.6 1.4].\nThen, limiting the search frequencyrange to a semi-tone,\ncentered at the energy-weightedaverage of coarsely esti-\nmated F0s. We use the YIN algorithm for F0 estimation\nbecause (1) it has been evaluated to be robust for mono-\nphonic signals [16] (2) it is available for research use. A\nwindowsizeof 46msisusedforanalyzingreferenceF0s.\nFor each F0 track of a sample, only the parts of good\nperiodicity serve as ground truth. The aperiodic parts at\nthe transients and near the end of notes are discarded by\nthresholding the aperiodicity measure in YIN. Figure 4\nshowsan exampleofestimatedF0 tracks.\n4 STORINGMULTIPLE F0SIN SOUND\nDESCRIPTION INTERCHANGEFORMAT\nForveriﬁableresearch,analysisdatashouldbestored,wel l\nstructured, made accessible and be of the highest attain-\nable quality. At the experiment stages, results are often\nsimply dumped into text or binary ﬁles using proprietary\nad-hoc formats to structure the data. However, this ap-\nproach has serious drawbacks in the difﬁculty to extend\nor to exchange. Ad-hoc formats need some kind of an-\nnotation to ensure the contents be interpreted correctly\nin the future or outside of the speciﬁc context in which\nthe ﬁle was created. When researchersneed to access the\ndata using various tools, as is the case with an evaluation\ndatabase,amoreﬂexibleformatisrequired.\n4.1 SDIF vs. XML\nA popular format for the storage of MIR data is XML.\nXML is simple and easily extensible, and well suited to\norganizingdatainahierarchicalway. However,XMLalso\nhassomeseriousdrawbacks,especiallyinthecaseoflow-\nlevelsoundanalysis. WhileXMLisarecognizedstandard\nformat,itcanbeseenasmainlyasyntacticstandard,leav-\ning the deﬁnition of semantics up to the developers for a\nspeciﬁc application. This means that in XML the samedata set can be described in an inﬁnite number of ways,\nwhichwillall belegal,butnotnecessarilycompatible.\nWe propose the use of the Sound Description Inter-\nchangeFormat(SDIF)[17],co-developedbyIRCAM,CN-\nMAT and MTG-UPF. SDIF addresses a number of spe-\nciﬁcissuesthatarisewhenusingXMLforthedescription\nof sound. One major difference between XML and SDIF\nis that SDIF stores its data in a binary format, which is\nmuchmoreefﬁcient,bothinsizeandspeed,andinamuch\nhigherprecisionthanatextorientedformat. Thishighde-\ngreeofefﬁciencyandaccuracyisespeciallyimportantfor\nlowleveldescriptors.\nIn addition, SDIF provides standard types for a large\nnumber of common aspects of sound, such as fundamen-\ntalfrequency,partialsandmarkers,whiletreatingsound’ s\never present time propertyin a special way. As it is ﬂexi-\nbletoextendSDIFbyaddingcustomtypesoraugmenting\nexisting ones, this will not compromise the compatibility\noftheﬁleswith otherprogramssupportingthestandard.\nContinuous development since its inception in 1997\nhas established SDIF as a mature and robust standard,\nsupporting a large range of platforms and programming\nenvironments. A recent move to Sourceforge6further\nhighlightsSDIFasanopensourceproject(LGPLlicense),\nreadyforintegrationintoopenapplicationsaswellascom-\nmercial ones. Binary and source distributions are avail-\nable for Windows, Linux, OSX , and bindings exists for\nC++, Matlab and for Java and scripting languages using\nSWIG.\n4.2 Storingmultiple F0sin SDIF\nAn example of multiple F0s stored as SDIF is shown in\nFigure 5. Multiple F0s ( 624.901Hz,840.123Hz, etc.)\nare to be stored together with the tracjectory IDs ( 1,2,\netc.). TheSDIFstructureisshownalongsidewiththetext\nrenditionofthestoreddata. Theoptionalname/valuetable\nis useful for storing metadata, for example the identity of\nan F0 estimation algorithm, analysis parameters, etc. In\nthe optional type declaration, new types can be declared\nand standard types extended. In this example, the type\n“XIND”isdeclaredforthetrajectoryIDs. Aftertheabove\nheaderpart,datamatricescanbestoredframebyframe.\n5 CONCLUSIONS AND DISCUSSIONS\nWe have presented a systematic method for the creation\nof a polyphonic music database. The synthesized music\ndatabase is reproducible, extensible and interchangeable .\nMost importantly, the ground truth is veriﬁable. We pro-\nposeto use SDIF forstoringlow-levelsignal descriptions\nsuch as the case for multiple F0s. Information about this\ndatabasecanbe foundat theauthor’spage7.\nTo evaluate an F0 estimation algorithm, the database\nshouldbegeneralizedforallpolyphoniesandforasmany\n6http://sdif.sourceforge.net\n7http://recherche.ircam.fr/equipes/analyse-\nsynthese/cyeh/database.htmlNAME / VALUE TABLE (optional)FILE HEADER\nTYPE DECLARATION (optional)\nFRAME\nSTREAM SIGNATURE SIZE\n40TIME MATRIX #\nSIGNATURESIGNATUREMATRIX\nROW # COLUMN #\nCOLUMN # ROW # DATA TYPEDATA TYPE\n1FQ0 0x0004 2 11FQ0 0 0.000000 21 MTD XIND{Id}\n1 FTD 1FQ0{XIND TrkID;}\n624.901\n840.123\n1\n2MATRIX\nXIND 2 1 0x0001\nFRAME\nMATRIX SIGNATURE SIZE STREAM\n0.011000TIME\n0 2 28 1FQ0\nSIGNATURESIGNATUREMATRIX\nMATRIXDATA TYPE ROW # COLUMN #\nCOLUMN # ROW # DATA TYPE1FQ0 0x0004 1 1\n840.123\nXIND 0x0001 1 1\n2\nFigure 5. SDIF structure with the text rendition of an\nexampleofmultipleF0s\ninstrumentsaspossible. Withthediverseinstrumentsound\nsamplesavailable forresearch, we are able to renderhigh\nquality MIDI ﬁles such as RWC SMFs. It is ﬂexible to\npre-edit the MIDI ﬁles for the evaluation purpose. By\nmeans of selecting and editing designated tracks, we can\nrendervariouspolyphoniesand instrumentcombinations.\nThedistributionofinstrumentsprogrammedinRWCSMFs\nare shown in Figure 6. Attention should be paid to the\nnon-uniformdistributionoftheinstrumentsinRWCSMFs.\nThedatabaseshouldbegeneralizedinawaythatnotasin-\ngleinstrumentis preferred.\nSince the extracted F0 tracks may overlap in time and\nin frequency,evaluation rules must be speciﬁed while re-\nportingthe evaluationresults. One mayspecify an allow-\nable frequency range within which multiple F0s can be\nconsidered as fewer, or even one. Flexible rules for tran-\nsient framesorweaker energysourcesmay also be speci-\nﬁed.\nThe proposed method can be extended to other MIR\nevaluation tasks, such as beat tracking, tempo extraction,\ndrum detection, chord detection, onset detection, score\nalignment,source separation,etc. Concerningthe ground\ntruth,beatsandtemposcanbeeasilyprogrammedinMIDI\nﬁles and chords can be extracted from MIDI ﬁles. For\nevaluation tasks requiring timing precision, the proposed\nmethodcanprovideveriﬁableanalysisasgroundtruth.050100150200250300350400450500\npiano\nchromatic perc.organguitarbassstrings\nensemblebrassreedpipe\nsynth leadsynth pad\nsynth effectsethnic\npercussion\nsound effectsnumber of tracks\n  \nclassical\ngenre\njazz\npolpular\nroyalty free\nFigure 6 . Instrument families used in RWC Standard\nMIDIFiles\n6 ACKNOWLEDGEMENT\nThis work is a part of the project MusicDiscover , sup-\nported by MRNT (le Minist` ere d´ el´ egu´ e ` a la Recherche\net aux Nouvelles Technologies) of France. The author\nC. Yeh would also like to thank Alain de Cheveign´ e and\nArshia Cont for discussing the issues of creating a poly-\nphonicmusicdatabasefortheevaluationofF0estimation.\n7 REFERENCES\n[1] Klapuri, A. “Multiple fundamental frequency estima-\ntion based on harmonicity and spectral smoothness”,\nIEEETrans.onSpeechandAudioProcessing ,Vol.11,\nNo.6,pp.804-816,November,2003.\n[2]CompleteMIDI1.0DetailedSpeciﬁcations(Japanese\nversion 98.1) , Association of Musical Electronics In-\ndustry,1998.\n[3] Bogaards, N., Roebel, A. and Rodet, X. “Soundanal-\nysis and processing with AudioSculpt 2”, Proc. of\nInt. Computer Music Conference (ICMC’04) Miami,\nFlorida,USA,2004.\n[4] Ryyn¨ anen, M. and Klapuri, A. “Polyphonic music\ntranscription using note event modeling”, Proc. IEEE\nWorkshoponApplicationsofSignalProcessingtoAu-\ndio and Acoustics (WASPAA’05) ,Mohonk,NY, USA,\n2005.\n[5] Kameoka, H., Nishimoto, T. and Sagayama, S. “A\nmultipitchanalyzerbasedonharmonictemporalstruc-\ntured clustering”, IEEE Trans. on Audio, Speech and\nLanguage Processing , pp. 982-994, Vol. 15, No. 3,\nMarch,2007.\n[6] Rodet, X., Escribe, J. and Durigon, S. “Improv-\ning score to audio alignment: percussion alignment\nand precise onset estimation”, Proc. of Int. ComputerMusic Conference (ICMC’04) , pp. 446-449, Miami,\nFlorida,USA, 2004.\n[7] Roebel, A. “Onset detection in polyphonicsignals by\nmeans of transient peak classiﬁcation”, International\nSymposium for Music Information Retrieval-MIREX\n(ISMIR/MIREX’06) ,Victoria,Canada,2006.\n[8] Baskind,A. Mod`elesetM´ethodesdeDescriptionSpa-\ntialedeSc `enesSonores ,Universit´ eParis6,December,\n2003.\n[9] Yeh,C.,Roebel,A.andRodet,X.“MultipleF0Track-\ning in Solo Recordings of Monodic Instruments”,\n120th AES Convention , Paris, France, May 20-23,\n2006.\n[10] Li, Y. and Wang D. L. “Pitch detection in polyphonic\nmusic using instrument tone models”, Proc. IEEE,\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP’07) , pp.II.481-484, Hon-\nolulu,HI,USA, April15-20,2007.\n[11] Kitahara, T., Goto, M., Komatani, K., Ogata, T. and\nOkuno,H.G.“InstrumentIdentiﬁcationinPolyphonic\nMusic: Feature Weighting to Minimize Inﬂuence of\nSound Overlaps”, EURASIP Journal on Advances in\nSignalProcessing ,Article ID51979,2007.\n[12] Goto, M., Hashiguchi, H., Nishimura, T. and Oka, R.\n“RWC Music Database: Popular, Classical, and Jazz\nMusicDatabases”, Proc.ofthe3rdInternationalCon-\nferenceonMusicInformationRetrieval(ISMIR2002) ,\npp.287-288,Paris,France,2002.\n[13] Goto, M. “RWC Music Database: Music Genre\nDatabase and Musical Instrument Sound Database”,\nProc.ofthe4thInternationalConferenceonMusicIn-\nformation Retrieval (ISMIR 2003) , pp. 229-230, Bal-\ntimore,Maryland,USA, 2003.\n[14] Goto, M. “AIST Annotation for the RWC Music\nDatabase”, Proc. of the 7th International Conference\non Music Information Retrieval (ISMIR 2006) , Victo-\nria,Canada,2006.\n[15] Benade, A. Fundamentals of Musical Acoustics .\nDoverPublications,Inc.,NewYork,1976.\n[16] de Cheveign´ e, A. and Kawahara, H. “YIN, a funda-\nmental frequency estimator for speech and music”,\nJournaloftheAcousticalSocietyofAmerica ,Vol.111,\nNo.4,pp.1917-1930,April,2002.\n[17] Schwarz, D. and Wright, M. “Extensions and Appli-\ncations of the SDIF Sound Description Interchange\nFormat”, Proc. of Int. Computer Music Conference\n(ICMC’00) ,Berlin,Germany,2000."
    },
    {
        "title": "Improving Efficiency and Scalability of Model-Based Music Recommender System Based on Incremental Training.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416880",
        "url": "https://doi.org/10.5281/zenodo.1416880",
        "ee": "https://zenodo.org/records/1416880/files/YoshiiGKOO07.pdf",
        "abstract": "We aimed at improving the efficiency and scalability of a hybrid music recommender system based on a probabilistic generative model that integrates both collaborative data (rating scores provided by users) and content-based data (acoustic features of musical pieces). Although the hybrid system was proved to make accurate recommendations, it lacks efficiency and scalability. In other words, the entire model needs to be re-trained from scratch whenever a new score, user, or piece is added. Furthermore, the system cannot deal with practical numbers of users and pieces on an enterprise scale. To improve efficiency, we propose an incremental method that partially updates the model at low computational cost. To enhance scalability, we propose a method that first constructs a small “core” model over fewer virtual representatives created from real users and pieces, and then adds the real users and pieces to the core model by using the incremental method. The experimental results revealed that the proposed system was not only efficient and scalable but also outperformed the original system in terms of accuracy. 1 INTRODUCTION Music recommender systems play important roles in current e-commerce to help users discover their favorites in huge databases [1]. For example, many e-commerce sites (e.g., Last.fm and Amazon.com [2]) use collaborative filtering techniques to recommend musical pieces to the user by examining how someone else has rated them. Although these techniques have been considered to be effective, they suffer from the famous “new item” problem. That is, nonrated pieces cannot be recommended. In addition, the variety of recommendations tends to be poor because most users mainly rate musical pieces by a small number of popular artists. This indicates that there still remains much room for enhancing the Long-Tail effect [3]. To overcome these limitations, content-based filtering techniques, which recommend musical pieces similar to user’s favorites in terms of musical content, are attracting attention of many researchers. However, a major approach based on automatic content analysis for musical c⃝2007 Austrian Computer Society (OCG). audio signals [4, 5] has not fully gained the popularity of end users. In contrast, Pandora, which depends on manual content annotation for commercial titles, is a well-known successful radio station on the Internet. This indicates that we should take into account important factors, such as cultural backgrounds and popularity on the market, that contribute to making reasonable recommendations but cannot be obtained from audio signals. However, this annotationbased approach lacks portability because it is not practical to manually annotate all compositions by amateurs in social networking services (e.g., MySpace.com). To make reasonable recommendations under any conditions, it is necessary to take a flexible hybrid approach that can integrate various types of available data. This improves robustness against inappropriate data in real world (e.g., malicious rating scores, erroneous automatic annotations, and inconsistent manual annotations). We therefore developed a hybrid recommender system using a probabilistic model that integrates both collaborative and content-based data in a theoretical way [6]. Although our system overcame the shortcomings of conventional techniques, critical problems in efficiency and scalability emerged when we tried to apply our system to ecommerce where several millions of users and pieces are managed. The system can neither promptly adapt recommendations to each user according to changes in his or her rating scores nor incrementally register new users and pieces. This is because the model should always be trained from scratch, where the time for training is proportional to both numbers of users and pieces. To improve efficiency, we propose an online method that updates only partial parameters of the model related to changes in the observed data. This enables the system to incrementally incorporate these changes into the model. To improve scalability, we propose a method that builds a small “core” model over fixed numbers of virtual representatives created from large numbers of real users and pieces. The core model is then updated while incrementally registering real users and pieces. The rest of this paper is organized as follows. Section 2 reviews our recommender system. Section 3 explains the proposed methods. Section 4 reports on the experiments. Section 5 summarizes the key findings of this study. 2 HYBRID MUSIC RECOMMENDER SYSTEM We will first define a recommendation task and then explain the original version of our recommender system [6].",
        "zenodo_id": 1416880,
        "dblp_key": "conf/ismir/YoshiiGKOO07",
        "keywords": [
            "recommendation task",
            "original version",
            "probabilistic generative model",
            "collaborative data",
            "content-based data",
            "accuracy",
            "efficiency",
            "scalability",
            "incremental method",
            "core model"
        ],
        "content": "IMPROVING EFFICIENCY AND SCALABILITY OF\nMODEL-BASED MUSIC RECOMMENDER SYSTEM\nBASED ON INCREMENTAL TRAINING\nKazuyoshi Yoshii †∗Masataka Goto ‡ Kazunori Komatani †\nTetsuya Ogata † Hiroshi G. Okuno †\n†Graduate School of Informatics, Kyoto University∗JSPS Research Fellow (DC1)\n‡National Institute of Advanced Industrial Science and Technology (AIST)\n{yoshii,komatani,ogata,okuno }@kuis.kyoto-u.ac.jp m.goto@aist.go.jp\nABSTRACT\nWe aimed at improving the efﬁciency and scalability of\na hybrid music recommender system based on a proba-\nbilistic generative model that integrates both collaborative\ndata (rating scores provided by users) and content-baseddata (acoustic features of musical pieces). Although thehybrid system was proved to make accurate recommen-\ndations, it lacks efﬁciency and scalability. In other words,\nthe entire model needs to be re-trained from scratch when-ever a new score, user, or piece is added. Furthermore, thesystem cannot deal with practical numbers of users and\npieces on an enterprise scale. To improve efﬁciency, we\npropose an incremental method that partially updates themodel at low computational cost. To enhance scalability,we propose a method that ﬁrst constructs a small “core”\nmodel over fewer virtual representatives created from real\nusers and pieces, and then adds the real users and pieces tothe core model by using the incremental method. The ex-\nperimental results revealed that the proposed system was\nnot only efﬁcient and scalable but also outperformed theoriginal system in terms of accuracy.\n1 INTRODUCTION\nMusic recommender systems play important roles in cur-\nrent e-commerce to help users discover their favorites inhuge databases [1]. For example, many e-commerce sites\n(e.g., Last.fm and Amazon.com [2]) use collaborative ﬁl-\ntering techniques to recommend musical pieces to the userby examining how someone else has rated them. Althoughthese techniques have been considered to be effective, they\nsuffer from the famous “new item” problem. That is, non-\nrated pieces cannot be recommended. In addition, the va-riety of recommendations tends to be poor because mostusers mainly rate musical pieces by a small number of\npopular artists. This indicates that there still remains much\nroom for enhancing the Long-Tail effect [3].\nTo overcome these limitations, content-based ﬁltering\ntechniques, which recommend musical pieces similar to\nuser’s favorites in terms of musical content, are attract-\ning attention of many researchers. However, a major ap-proach based on automatic content analysis for musical\nc/circlecopyrt2007 Austrian Computer Society (OCG).audio signals [4, 5] has not fully gained the popularity of\nend users. In contrast, Pandora, which depends on manual\ncontent annotation for commercial titles, is a well-known\nsuccessful radio station on the Internet. This indicates thatwe should take into account important factors, such as cul-\ntural backgrounds and popularity on the market, that con-\ntribute to making reasonable recommendations but cannotbe obtained from audio signals. However, this annotation-based approach lacks portability because it is not practical\nto manually annotate all compositions by amateurs in so-\ncial networking services (e.g., MySpace.com).\nTo make reasonable recommendations under any con-\nditions, it is necessary to take a ﬂexible hybrid approachthat can integrate various types of available data. This im-\nproves robustness against inappropriate data in real world\n(e.g., malicious rating scores, erroneous automatic anno-tations, and inconsistent manual annotations).\nWe therefore developed a hybrid recommender system\nusing a probabilistic model that integrates both collabora-\ntive and content-based data in a theoretical way [6]. Al-\nthough our system overcame the shortcomings of conven-tional techniques, critical problems in efﬁciency and scal-ability emerged when we tried to apply our system to e-\ncommerce where several millions of users and pieces are\nmanaged. The system can neither promptly adapt recom-mendations to each user according to changes in his orher rating scores nor incrementally register new users and\npieces. This is because the model should always be trained\nfrom scratch, where the time for training is proportional toboth numbers of users and pieces.\nTo improve efﬁciency, we propose an online method\nthat updates only partial parameters of the model related\nto changes in the observed data. This enables the system\nto incrementally incorporate these changes into the model.To improve scalability, we propose a method that builds asmall “core” model over ﬁxed numbers of virtual repre-\nsentatives created from large numbers of real users and\npieces. The core model is then updated while incremen-tally registering real users and pieces.\nThe rest of this paper is organized as follows. Section 2\nreviews our recommender system. Section 3 explains theproposed methods. Section 4 reports on the experiments.\nSection 5 summarizes the key ﬁndings of this study.2 HYBRID MUSIC RECOMMENDER SYSTEM\nWe will ﬁrst deﬁne a recommendation task and then ex-\nplain the original version of our recommender system [6].\n2.1 Task Statement\nThe objective of music recommendation is to rank musi-\ncal pieces that have not been rated by a target user. WeletU={u|1,···,N\nU}be the indices of users and M=\n{m|1,···,NM}be those of pieces, where NUis the num-\nb e ro fu s e r sa n d NMis that of pieces. We assumed that U\nandMwere registered in the system in advance.\nCollaborative data are rating scores, which are also reg-\nistered in the system. In this paper, we focus on scores on\na0-to-4scale as rating data. We let ru,mbe a rating score\ng i v e nt op i e c e mby user u,w h e r e ru,mis an integer be-\ntween 0and4(4being the best). By collecting all the\nrating scores, rating matrix Ris obtained by\nR={ru,m|1≤u≤NU,1≤m≤NM}.(1)\nWhen user uhas not rated piece m,φis substituted for\nru.mas a symbol, representing an “empty” score for con-\nvenience. Note that most scores in Rare empty in actual\ndata because all users have rated a few pieces in M.\nContent-based data are acoustic features automatically\nextracted from the polyphonic audio signals of all musical\npieces, M. We assumed that each piece would be repre-\nsented as a single vector of musical features. Let T=\n{t|1,···,NT}be the indices of these features, where NT\nis the total number (a dimension of the vector). Here, cm,t\nis deﬁned as the t-th element value of piece m. By collect-\ning all the feature vectors, content matrix Cis obtained by\nC={cm,t|1≤m≤NM,1≤t≤NT}.(2)\nThe method of extracting features we use is based on the\nbag-of-timbres model [6]. Note that we can incorporate\nmannual annotations into calculating maxtix C.\n2.2 Recommendation Method\nTo integrate the collaborative and content-based data, we\nused a probabilistic generative model, called a three-wayaspect model [7]. It explains the generative process forthe observed data by introducing a set of latent variables.\nThese variables correspond to conceptual genres ,w h i c h\nare not given in advance. As part of the generative pro-cess, the model directly represents user preferences (howmuch each genre is preferred by a target user), which are\nstatistically estimated with a theoretical proof.\nThe observed data are associated with latent variables,\nZ={z|1,···,N\nz},w h e r e Nzis the total number of\nthese, as outlined in Fig. 1. Each latent variable corre-\nsponds to a conceptual genre. Given user u, the set of con-\nditional probabilities {p(z|u)|z∈Z}reﬂects the musical\ntaste of user u. One possible interpretation is that user u\nstochastically selects genre zaccording to his or her pref-\nerence p(z|u), and genre zthen stochastically generates\npiecemand acoustic feature taccording to their proba-\nbilities, p(m|z)andp(t|z). We assumed the conditional\nindependence of users, pieces, and features through the\nlatent genres. This is the key point of our model.MZ\nT)|(ztp )| (zmpU\nAcoustic\nFeatureMusical \npieceUser\nLatent variable \n(conceptual genre))|(uzp)(up\n)| (umpSystem recommends\nwith highm piece\nFigure 1 . Asymmetric representation of aspect model.\n2.2.1 Formulation of Three-way Aspect Model\nWe will now explain the mathematical formulation for the\nthree-way aspect model. The assumption of conditional\nindependence over U,M,a n dTthrough Zleads to an\nasymmetric speciﬁcation for the joint probability distribu-tionp(u, m, t, z ), which is given by\np(u, m, t, z )=p(u)p(z|u)p(m|z)p(t|z), (3)\nwhere p(u)is the prior probability of user u.p(u, m, t, z )\nis the probability that user uwill select genre zand simul-\ntaneously listen to timbre tin piece m.\nMarginalizing out z, we obtain joint probability distri-\nbution p(u, m, t )overU,M,a n dT:\np(u, m, t )=/summationdisplay\nzp(u)p(z|u)p(m|z)p(t|z), (4)\nwhere the unknown model parameters are {p(z|u)|z∈\nZ,u∈U},{p(m|z)|m∈M,z ∈Z},a n d {p(t|z)|t∈\nT,z∈Z}, which are estimated by using rating matrix R\nand content matrix C. After these are estimated, musical\npieces are ranked for given user u/primeaccording to p(m|u/prime)∝/summationtext\ntp(u/prime,m ,t)∝/summationtext\nt,zp(z|u/prime)p(m|z)p(t|z).\n2.2.2 Estimation of Model Parameters\nWe will next explain how the model parameters are es-\ntimated. Let a tuple (u, m, t )be an event where user u\nlistens to timbre tin piece m. Here, we assumed that each\nevent would occur independently. The likelihood of the\nparameters for the observed data is given by\nL/prime=/productdisplay\nu,m,tp(u, m, t )n(u,m,t ), (5)\nwhere n(u, m, t )is the number of events (u, m, t ).I nt h i s\nstudy, we assumed that n(u, m, t )was proportional to the\nproduct of ru,m andcm,t.T h a t i s , n(u, m, t )∝ru,m×\ncm,t. This is based on the general observation that event\n(u, m, t )occurs more frequently if user uprefers piece m\nmore or the weight of timbre tin piece mis higher.\nGiven the observed data (rating matrix Rand content\nmatrix C), the log-likelihood, L, is obtained by\nL=/summationdisplay\nu,m,tn(u, m, t )l o gp(u, m, t ). (6)\nTo estimate the parameters that maximize Eq. (6), we use\nthe deterministic annealing EM (DAEM) algorithm [8],\nwhich can avoid the local maximum problem.3 IMPROVED EFFICIENCY AND SCALABILITY\nRecommender systems can be categorized into memory-\nbased and model-based groups in terms of methodology.The former always uses the entire data, RandC,t om a k e\nrecommendations. The latter, on the other hand, uses these\ndata to train the assumed models of estimating user pref-erences. These models are then used to make recommen-dations. The latter can generally achieve better responses\nin ranking musical pieces once the models are constructed.\nHowever, the computational cost involved in training thesemodels tends to be high. Our system, which belongs to thelatter group, also suffers from this disadvantage.\n3.1 Problems and Approach\nThe computational complexity of training the aspect model\nvia the EM algorithm is O(N\nUNMNTNZ)≈O(NUNM),\ntaking into account that NTandNZ, which are set to 64\nand10, remain constant. This causes two serious issues.\nOne concerns efﬁciency; this costly training is required\nwhenever the observed data changes. The other concerns\nscalability; both the computational time and memory loadrapidly increase according to O(N\nUNM). Although the\nefﬁciency and scalability are important factors for prac-\ntically managing recommender systems on a commercial\nscale, they have scarcely been taken into account.\nTo improve the efﬁciency, we propose an incremental\ntraining method for the three-way aspect model. Ours is\nan extended version of a method given by Zhang et al. [9]\nthat efﬁciently updates the basic (two-way) aspect modelused for collaborative ﬁltering. To improve the scalability,we integrate the incremental training method with a clus-\ntering method while improving the recommendation accu-\nracy. Note that the use of clustering methods degrades theaccuracy in general due to some approximations [10].\n3.2 Incremental Training Method\nWe will now explain the incremental training method that\nupdates the partial parameters of the aspect model. Afterthis, we will call the model that is initially obtained usingthe EM-based training method a base model . On the other\nhand, we will call the model that is obtained by incremen-\ntally training the base model an updated model .\nOur method individually addresses the following three\ncases to obtain the updated model:\n1. Updating the base model for a registered user ( ∈U)\nwho provides new rating scores.\n2. Extending the base model for a non-registered user\n(/∈U) who provides some rating scores.\n3. Extending the base model for a non-registered piece\n(/∈M) that has no rating scores.\nWhile the size of the model (the number of parameters) re-\nmains unchanged in the ﬁrst case, it increases in the othersbecause non-registered users or pieces are added.\n3.2.1 Updating Proﬁles of Registered Users\nGiven speciﬁc user u, conditional probabilistic distribu-\ntion{p(z|u)|z∈Z}, which is called a user proﬁle , cap-\ntures the user’s musical preference. Recall that p(z|u)rep-\nresents how likely user uis to select conceptual genre zaccording to the musical preference. The model assumes\nthat the proﬁles of all users are independent. Therefore,\nwhen a user newly provides (changes) some rating scores,we only need to update his or her proﬁle without affecting\nthe proﬁles of the others to keep the log-likelihood maxi-\nmized. This contributes to improved efﬁciency.\nWe aimed at updating the proﬁle of registered user u\n/prime:\n{p(z|u/prime)|z∈Z,u/prime∈U}, where user u/primehas provided\nsome new rating scores. We assumed that model param-\neters other than the proﬁle of user u/primewould be constant.\nTherefore, the maximization of log-likelihood Lis equiv-\nalent to that of the sum of terms concerning user u/primein\nL.W el e t Lu/primebe the log-likelihood for the observed data\nconcerning user u/prime, which is given by\nLu/prime=/summationdisplay\nm,tn(u/prime,m ,t)l o gp(m, t|u/prime) (7)\n=/summationdisplay\n<m,t |u/prime>log/summationdisplay\nzp(m|z)p(t|z)p(z|u/prime),(8)\nwhere we introduced a new operator,/summationtext\n<m,t |u/prime>,f o rX\n(Xis arbitrary), which represents/summationtext\nm,tn(u/prime,m ,t)X.U s -\ning Jensen’s inequality, we can rewrite Eq. (8) as\nLu/prime=/summationdisplay\n<m,t |u/prime>log/summationdisplay\nzp(m|z)p(t|z)\nδm,tp(z|u/prime)δm,t(9)\n≥/summationdisplay\n<m,t |u/prime>/summationdisplay\nzp(m|z)p(t|z)\nδm,tlogp(z|u/prime)\n+/summationdisplay\n<m,t |u/prime>logδm,t, (10)\nwhere δm,tis given by δm,t=/summationtext\nzp(m|z)p(t|z).\nBecause p(m|z)andp(t|z)are constant, the maximiza-\ntion of Lu/primeis equivalent to that of the ﬁrst term of Eq. (10).\nThis introduces the following maximization problem:\nMaximize/summationdisplay\n<m,t |u/prime>/summationdisplay\nzp(m|z)p(t|z)\nδm,tlogp(z|u/prime),(11)\ns.t./summationdisplay\nzp(z|u/prime)=1,(12)\nwhere Eq. (11) is an objective function and Eq. (12) is a\nconstraint function. {p(z|u/prime)|z∈Z}are the variables to\nbe optimized. Using the Lagrangian multiplier method,\nwe can ﬁnally obtain the user-proﬁle updating formula:\np(z|u/prime)=/summationdisplay\nm,tn(u/prime,m ,t)p(m|z)p(t|z)/summationtext\nz/primep(m|z/prime)p(t|z/prime)\n/summationdisplay\nm,tn(u/prime,m ,t).(13)\n3.2.2 Creating Proﬁles of Non-registered Users\nWe aimed at creating the proﬁle of non-registered user u/prime:\n{p(z|u/prime)|z∈Z,u/prime/∈U}, where user u/primehas some rating\nscores for registered pieces, {ru/prime,m|m∈M}. Note that\nthese scores were not used for training the base model. Inthis case, we can apply the updating formula (13) to createthe proﬁle by using p(m|z)andp(t|z)that were estimated\nby using the rating scores of registered users, U.…\n…\nRepresentative piecesZ T…Z T\n……\nMusical pieces MUsers U\n…Representative users\nMusical pieces M……Users U…Users U\nMusical pieces M…ClusteringClustering\nPartial \nupdatingZ T Z TPartial \nupdating\nCore \nmodel\nFigure 2 . Overview of scalability enhancement method.\nThe computational complexity of creating (updating)\nthe proﬁle of user u/primeisO(ΔNM),w h e r e ΔNMis the\nnumber of pieces that were rated by user u/prime. We only need\nto recalculate the ΔNMterms concerning these pieces in\neach summation of the updating formula (13).\n3.2.3 Registering Non-registered Musical Pieces\nWe aimed at estimating the probabilities that piece m/primewill\nbe generated from conceptual genres: {p(m/prime|z)|z∈Z},\nwhere m/primeis a non-registered piece. Here, we can ob-\ntain the probabilistic distribution {p(z|m/prime)|z∈Z}in the\nsame way as that described above. After the distributionis obtained, p(m\n/prime|z)is given by p(m/prime|z)∝p(z|m/prime)/p(z).\nNote that no piece m/primehas been rated by users. That is,\nonly content-based data {cm/prime,t|t∈T}are available. There-\nfore, the constant-time formula is obtained by\np(z|m/prime)=/summationdisplay\ntcm/prime,tp(t|z)/summationtext\nz/primep(t|z/prime)\n/summationdisplay\ntcm/prime,t. (14)\n3.3 Scalability Enhancement Method\nLet us now explain the scalability enhancement method,\nwhich enables the system to efﬁciently deal with large\nnumbers of users and musical pieces. Figure 2 shows theoverview of the method. The method is ﬁrst used to con-struct a “core” model for fewer virtual representative users\nand pieces by normally using EM-based ofﬂine training.\nThen, all users, U, and all pieces, M, which are virtually\nregarded as non-registered users and pieces, are added tothe core model by using incremental training. Note that\nthere are two orders in adding UandM. We determined\nthe best one in Section 4.4. Here, the problem is how tocreate representative users and pieces from UandM.\nTo solve this problem, we introduced clustering such as\nthe K-means method. All users, U, were classiﬁed into a\nsmall number of user groups. We used the Pearson corre-lation coefﬁcient to calculate the similarity two users hadin preferences. This is a typical measure in collaborative\nﬁltering (see [6]). On the other hand, all pieces, M,w e r e\nclassiﬁed into a small number of music groups accordingto the Euclidean distance between the feature vectors oftwo pieces. Finally, the representative users and pieces\nwere determined as the centroids of these groups.11φ3φ2φ44φ34φφ043φ01\n11φ3φ2φ44φ34φφ043φ01Group 1\nGroup 2\nUser1\n2\n3\n413 24 5 Musical pieceGroup 1 Group 2 Group 3\n4.00φ\n1.33 3.503.50 0.33\n4.00φ\n1.33 3.503.50 0.3313 2\n1\n2Representative pieceRepresentative userRating matrix\nFigure 3 . Calculation of new rating matrix for represen-\ntative users and musical pieces.\nOne problem remaining is how to create a rating ma-\ntrix and a content matrix for these representatives. These\nmatrices are used to train the core model. The former is\nobtained as outlined in Fig. 3. A score provided to a rep-resentative piece by a representative user is the average ofactual scores that were provided to musical pieces in the\ncorresponding music group by users in the corresponding\nuser group. The latter is obtained by calculating the aver-age of feature vectors in each music group.\n4 EXPERIMENTAL EVALUATION\nHere, we report on several experiments that were con-\nducted to evaluate our methods.\n4.1 Experimental Conditions\nThe collaborative and content-based data ( RandC)w e r e\nprepared in the same way as in our previous study [6].\nThe musical pieces we used were Japanese songs on singleCDs that were ranked in the weekly top-20 sales rankingsfrom 2000 to 2005. The corresponding rating scores were\ncollected from Amazon.co.jp. After unreliable users and\npieces had been removed that had less than four scores,N\nUwas316andNMwas358. The percentages for scores\n0,···,4in rating matrix Rcorrespond to 57.9%, 19.1%,\n8.57%, 4.85%, and 9.54%. The density of Rwas2.19%.\n4.2 Evaluation Measure\nThe experiments were conducted with 10-fold cross val-\nidation, i.e., training matrix Rtand evaluation matrix Re\nwere created from rating matrix Rby randomly masking\n10% of the actual scores in R, as outlined in Fig. 4.\nWe used an evaluation measure we had previously pro-\nposed [6] to calculate the accuracy of recommendations.This measure calculates the ratio of favorites to the num-ber of recommended pieces whose scores are masked over\nall users. We examined the entire top- xrankings of all\nusers ( x=1,3,10). Figure 5 shows an example in the\ncase of x=3. Note that we could not evaluate all the\nrecommended pieces (the total was xN\nU) because most\nof them had not actually been rated by users (the corre-\nsponding scores were φinRe). Here, we let Nrbe the\ntotal number of recommended pieces whose scores weremasked but were actually r(0≤r≤4), and let Nbe\nN=/summationtext\nrNr. Obviously, Nwas much less than xNU.\nWe let Arbe the ratio of NrtoN,T h a ti s , Ar=Nr/N.\nA higher value for A4indicates better performance. If\nrandom pieces are recommended, A4will become 57.9%,\nwhich is the same as the percentage for score 4inR.3φ 13 4 1φ 2φ4 3φ\n3φ 13 4 1φ 2φ4 3φ\nφφ 13 4 1φφφ4 3φ\nφφ 13 4 1φφφ4 3φ\n3φφφφφφ 2φφφφ\n3φφφφφφ 2φφφφ\nReRtRTraining matrix Evaluation matrix Rating matrix: Masked scoreUser1\n2\n3413 2Musical piece\nFigure 4 . Data preparation for 10-fold cross validation.\n13 24 5\n1φ0\n444\nφ34\nφ3φ\n311\n3 0 2φφ4 2 0φ4 4φ\n1φ0\n444\nφ34\nφ3φ\n311\n3 0 2φφ4 2 0φ4 4φ6\nEach box represents a pair:\npiece number (actual score).Musical pieceUser1\n2\n3\n: Masked score,78 9\nRating matrix\n4 (φ)2 ( 2 )4 (φ)\n3 ( 3 ) 7 (φ)8 ( 4 ) 6 (φ)6 (φ) 7 ( 4 )\n4 (φ)2 ( 2 )4 (φ)\n3 ( 3 ) 7 (φ)8 ( 4 ) 6 (φ)6 (φ) 7 ( 4 )\nTop-3 rankingsUser1\n2\n3which is regarded as φ\nin recommendationRank\n13 2\n50.04=A\nFigure 5 . Calculation of recommendation accuracy.\n4.3 Evaluation of Incremental Training Method\nWe evaluated our incremental training method for the three\ncases described in Section 3.\n4.3.1 Recommendations to Registered Users\nAn objective was to observe the decrease in the accuracy\nof recommendations according to the decrease in the per-\ncentage of rating scores that were used to construct thebase models. In addition, we examined the differences inaccuracy between base and updated models.\nLet us ﬁrst explain the experimental procedures. Using\nrating matrix R\nt, we prepared a base model and a total of\nten updated models. The former was constructed by usingR\ntas training data. The latter was obtained as follows:\n1. A temporary rating matrix, R/prime\nt, was prepared by\nrandomly masking the K%(K=0,10,20,···,90)\nof actual scores in training matrix Rt.\n2. A temporary base model was built by using R/prime\ntas\ntraining data.\n3. An updated model was obtained by incrementally\nadding the masked scores, i.e., by using Rt.\nEach model was used to rank the musical pieces. To cal-\nculate accuracies, we used evaluation matrix Rtin all the\nsettings. These procedures were iterated ten times whileswitching the ten rating matrices that were prepared for10-fold cross validation described in Section 4.2.\nFigure 6 plots the results, which shows that our method\ncan appropriately adapt recommendations according to theincrease in rating scores. We found that the accuracy hardlydeteriorated even when the amount of rating scores used to\nupdate the base model was increased to that for building it\n(K=5 0 ). Although the largest difference was about 5%\nin examining the top-1 rankings ( x=1), we can say that\na sufﬁciently high accuracy was maintained.50556065707580859095100\n0 1 02 03 04 05 06 07 08 09 0Ratio of rating scores for incremental training: K [%]Recommendation accuracy [%]Top-1 rankings (baseline): 93.5%\nTop-3 rankings (baseline): 86.4%\nTop-10 rankings (baseline): 80.7%\nTop-1 rankings (incremental)\nTop-3 rankings (incremental)\nTop-10 rankings (incremental)\nFigure 6 . Decrease in recommendation accuracy A4ac-\ncording to increase in scores for incremental training.\n4.3.2 Recommendations to Non-registered Users\nAn objective was to compare recommendations to regis-\ntered users with those to users who had not been regis-tered in terms of recommendation accuracy, A\n4. Smaller\ndifferences in accuracies indicate better performance.\nWe will now explain the experimental procedures:\n1. 10% of users, Unew, were randomly selected from\nU. They were regarded as non-registered users. We\nletUregbe the remaining users (registered users).\n2. A reduced training matrix, R/prime\nt, was obtained by re-\nmoving Unew from training matrix Rt.T h a ti s ,t h e\nsize of matrix R/prime\ntwas reduced to 90% of that of Rt.\n3. A temporary base model was constructed by using\nR/prime\ntas training data.\n4. To calculate the recommendation accuracy for Ureg,\nwe ﬁrst did the following procedures:\n(a) Proﬁles of Uregin the base model were up-\ndated by using R/prime\ntagain.\n(b) Recommendations based on the updated pro-\nﬁles were evaluated by using the rating scoresofU\nregin evaluation matrix Re.\nTo calculate the recommendation accuracy for Unew,\nwe then did the following procedures:\n(a) Proﬁles of Unewwere created by using the rat-\ning scores of Unew that were removed in step\n(2).\n(b) Recommendations based on the created pro-\nﬁles were evaluated by using the rating scores\nofUnewin evaluation matrix Re.\nThese procedures were iterated ten times while switch-\ning the ten rating matrices that were prepared for 10-fold\ncross validation. To evaluate the average and variance in\naccuracy, we repeated this experiment ten times.\nFigure 7 plots the results, which demonstrates that our\nincremental method can make accurate recommendationsto non-registered users as well as to registered users. We\nfound that the variances in accuracy tended to differ at a\nsigniﬁcant level of 5% through F-tests. However, t-testsrevealed that there were no differences in the average ac-curacies in the three types of rankings.6065707580859095100\nTop 1 Top 3 Top 10Recommendation accuracy [%]Registered users\nNon-registered users\nFigure 7 . Recommendation ac-\ncuracy A4for registered and non-\nregistered users.6065707580859095100\nTop 1 Top 3 Top 10Recommendation accuracy [%]Registered pieces\nNon-registered pieces\nFigure 8 . Recommendation ac-\ncuracy A4for registered and non-\nregistered musical pieces.6065707580859095100\nTop 1 Top 3 Top 10Recommendation accuracy [%]Baseline\nPieces-users\nUsers-pieces\nFigure 9 . Recommendation accu-\nracyA4by baseline (original) sys-\ntem and two scalable systems.\n4.3.3 Recommendations of Non-registered Musical Pieces\nAn objective was to compare recommendations of regis-\ntered musical pieces with those of non-registered pieces.in terms of recommendation accuracy, A\n4. Smaller differ-\nences in accuracies indicate better performance.\nThe experimental procedures were similar to those de-\nscribed in Section 4.3.2 except that “ Unew”a n d“ Ureg”\nwere replaced with “ Mnew”a n d“ Mreg,” where Mnew\nwere 10% randomly selected from M,a n dMregwere the\nremaining pieces (registered pieces). A base model was\ntrained by using partial data concerning Mreg. An up-\ndated model was obtained by extending the base model tothe entire data including M\nnew.\nFigure 8 plots the results, which shows that our incre-\nmental method can accurately recommend non-registeredpieces as well as registered pieces. We found no differ-ences in the average accuracies in the three types of rank-\nings through F-tests and t-tests.\n4.4 Evaluation of Scalability Enhancement Method\nAn objective is to compare the baseline system described\nin Section 2 with two scalable systems called a pieces-\nusers system and a users-pieces system in terms of recom-\nmendation accuracy, A\n4. The two scalable systems shared\nthe same core model for virtual representative users and\npieces, and updated it in different ways corresponding to\nthe lower and upper paths in Fig. 2, where the numbers ofthese representatives were set to 50each.\nFigure 9 plots the results, which shows that the pieces-\nusers system outperformed the others. This system gainedsigniﬁcant advantages over the baseline system in the av-erage accuracies in the top-3 and top-10 rankings although\nthere were no advantages in the top-1 rankings. This indi-\ncates that the DAEM algorithm used for training the coremodel worked better due to the reduction in the sparse-\nness of data by grouping UandM. The accuracies ob-\ntained with the users-pieces system, on the other hand,deteriorated. To create proﬁles of U, we should not use\nthe content-based data of virtual representative pieces but\nofreal pieces, M, because the sums in Eq. (13) operate\noverM. Equation (14), in contrast, does not need to oper-\nate over U. That is, all real pieces should be recovered in\nadvance of adding all real users.5 CONCLUSION\nWe presented an incremental training method and its ap-\nplication to scalability enhancement for our model-basedhybrid music recommender system that uses collaborative\nand content-based data. The incremental training method\nefﬁciently updates the partial parameters of the probabilis-tic model with theoretical proofs according to the growthof the observed data. The scalability enhancement method,\nwhich can speed up model training a hundred fold, has the\npotential to improve the accuracy of recommendations.That is, we found a breakthrough to overcome the trade-off, i.e., accuracy v.s. efﬁciency and scalability, which has\nbeen considered to be unavoidable. In the future, we plan\nto test our system with a larger database.\n6 REFERENCES\n[1] A. Uitdenbogerd and R. van Schyndel, “A Review of Fac-\ntors Affecting Music Recommender Success,” ISMIR , 2002,\npp. 204–208.\n[2] G. Linden and B. Smith, “Amazon.com Recommendations:\nItem-to-Item Collaborative Filtering,” IEEE Internet Com-\nputing , pp. 76-80, V ol. 7, No. 1, pp. 76–80, 2003.\n[3] C. Anderson, “The Long Tail: Why the Future of Business\nIs Selling Less of More,” Hyperion, 2006.\n[4] K. Hoashi, K. Matsumoto, and N. Inoue, “Personalization\nof User Proﬁles for Content-based Music Retrieval based onRelevance Feedback,” ACM Multimedia , 2003, pp. 110–119.\n[5] B. Logan, “Music Recommendation from Song Sets,” IS-\nMIR, 2004, pp. 425–428.\n[6] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and\nH. G. Okuno., “Hybrid Collaborative and Content-basedMusic Recommendation Using Probabilistic Model with La-tent User Preferences,” ISMIR , 2006, pp. 296–301.\n[7] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence, “Prob-\nabilistic Models for Uniﬁed Collaborative and Content-based Recommendation in Sparse-data Environments,” UAI,\n2001, pp. 437–444.\n[8] N. Ueda and R. Nakano, “Deterministic Annealing EM Al-\ngorithm,” Neural Net. , V ol. 11, No. 2, pp. 271–282, 1998.\n[9] L. Zhang, C. Li, Y . Xu, and B. Shi, “An Efﬁcient Solution\nto Factor Drifting Problem in the pLSA Model,” CIT, 2005,\npp. 175–181.\n[10] L. Ungar and D. Foster, “Clustering Methods for Collab-\norative Filtering,” Workshop on Recommendation Systems ,\n1998."
    },
    {
        "title": "Polyphonic Music Note Onset Detection Using Semi-Supervised Learning.",
        "author": [
            "Wei You",
            "Roger B. Dannenberg"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417385",
        "url": "https://doi.org/10.5281/zenodo.1417385",
        "ee": "https://zenodo.org/records/1417385/files/YouD07.pdf",
        "abstract": "Automatic note onset detection is particularly difficult in orchestral music (and polyphonic music in general). Machine learning offers one promising approach, but it is limited by the availability of labeled training data. Score-toaudio alignment, however, offers an economical way to locate onsets in recorded audio, and score data is freely available for many orchestral works in the form of standard MIDI files. Thus, large amounts of training data can be generated quickly, but it is limited by the accuracy of the alignment, which in turn is ultimately related to the problem of onset detection. Semi-supervised or bootstrapping techniques can be used to iteratively refine both onset detection functions and the data used to train the functions. We show that this approach can be used to improve and adapt a general purpose onset detection algorithm for use with orchestral music. 1 INTRODUCTION Finding the beginning of notes, or note onsets, in music audio is a problem that is widely studied. By finding note onsets, we can segment continuous music into discrete note events, benefiting tempo estimation, beat finding, automatic music transcription, and other analysis tasks. These in turn are often used as components in systems for music indexing and retrieval, music fingerprinting, and music similarity. Thus onset detection is a fundamental task for music information retrieval. However, because of the variability within and between musical instruments, finding note onsets is not trivial. In polyphonic pieces, note onsets may be difficult to separate from other notes, and in large ensembles such as an orchestra, masses of note onsets can be difficult to handle. Our focus is on massively polyphonic music, e.g. orchestra music. One reason previous work has focused on monophonic and polyphonic piano music is the availability of test data. Hand labeling music onsets is tedious work, and it would be very expensive to label all note onsets in large polyphonic works. For example, Beethoven’s Symphony no. 5 in C minor, first movement, has more than 10,000 notes and over 2,000 separate onset times over a duration of about 435s. If we assume labeling one note c⃝2007 Austrian Computer Society (OCG). takes 1 minute (quite optimistic in our experience), then more than 30 hours will be needed to label one movement. To solve this dilemma, audio-to-score alignment is used to estimate note onsets automatically. Typically, score alignment is performed with chroma features, which summarize 50 to 250ms windows of audio, but this does not provide the high time resolution one would like for onset labeling. For example, it is not unusual to see an average inter-onset time of 200ms in a polyphonic work. Therefore, we use a semi-supervised learning method to enhance the performance of the audio-to-score alignment, leading to improved onset detector training [6]. Thus, the task of acquiring training data is simply to locate corresponding audio and MIDI files and run a relatively fast alignment algorithm. Vastly increased numbers of training examples improve functions for onset detection. We compare the results of our trained onset detector to a high-quality, open-source onset detector, Aubio (http:// aubio.piem.org). To measure only improvements due to semi-supervised training, we used exactly the same features as Aubio, and we also used the Aubio peak-picking algorithm for our system. We also tried adding new features, hoping that machine learning would be able to take advantage of the additional information. After the related work section that follows, we explain the techniques used by our onset detection system. Section 4 presents an evaluation. Our findings are discussed in Section 5, which is followed by a concluding section. 2 RELATED WORK Current practice in note onset detection can be separated into two general approaches: 1. Apply a detection function, often based on change in spectrum and overall power; then use a temporal peak-picking algorithm to find local maxima in output from the detection function [2, 4, 1, 3].",
        "zenodo_id": 1417385,
        "dblp_key": "conf/ismir/YouD07",
        "keywords": [
            "Automatic note onset detection",
            "orchestral music",
            "polyphonic music",
            "machine learning",
            "labeled training data",
            "score-toaudio alignment",
            "score data",
            "standard MIDI files",
            "large amounts of training data",
            "accurate alignment"
        ],
        "content": "POLYPHONIC MUSIC NOTE ONSET DETECTION USING\nSEMI-SUPERVISED LEARNING\nWei You and Roger B. Dannenberg\nCarnegie Mellon University\nSchools of Computer Science and Music\nABSTRACT\nAutomatic note onset detection is particularly difﬁcult in\norchestral music (and polyphonic music in general). Ma-\nchine learning offers one promising approach, but it is lim-\nited by the availability of labeled training data. Score-to-\naudio alignment, however, offers an economical way to\nlocate onsets in recorded audio, and score data is freely\navailable for many orchestral works in the form of stan-\ndard MIDI ﬁles. Thus, large amounts of training data can\nbe generated quickly, but it is limited by the accuracy of\nthe alignment, which in turn is ultimately related to the\nproblem of onset detection. Semi-supervised or bootstrap-\nping techniques can be used to iteratively reﬁne both onset\ndetection functions and the data used to train the func-\ntions. We show that this approach can be used to improve\nand adapt a general purpose onset detection algorithm for\nuse with orchestral music.\n1 INTRODUCTION\nFinding the beginning of notes, or note onsets , in mu-\nsic audio is a problem that is widely studied. By ﬁnd-\ning note onsets, we can segment continuous music into\ndiscrete note events, beneﬁting tempo estimation, beat\nﬁnding, automatic music transcription, and other analysis\ntasks. These in turn are often used as components in sys-\ntems for music indexing and retrieval, music ﬁngerprint-\ning, and music similarity. Thus onset detection is a fun-\ndamental task for music information retrieval. However,\nbecause of the variability within and between musical in-\nstruments, ﬁnding note onsets is not trivial. In polyphonic\npieces, note onsets may be difﬁcult to separate from other\nnotes, and in large ensembles such as an orchestra, masses\nof note onsets can be difﬁcult to handle.\nOur focus is on massively polyphonic music, e.g. or-\nchestra music. One reason previous work has focused on\nmonophonic and polyphonic piano music is the availabil-\nity of test data. Hand labeling music onsets is tedious\nwork, and it would be very expensive to label all note on-\nsets in large polyphonic works. For example, Beethoven’s\nSymphony no. 5 in C minor, ﬁrst movement, has more\nthan 10,000 notes and over 2,000 separate onset times over\na duration of about 435s. If we assume labeling one note\nc\r2007 Austrian Computer Society (OCG).takes 1 minute (quite optimistic in our experience), then\nmore than 30 hours will be needed to label one movement.\nTo solve this dilemma, audio-to-score alignment is used\nto estimate note onsets automatically. Typically, score\nalignment is performed with chroma features, which sum-\nmarize 50 to 250ms windows of audio, but this does not\nprovide the high time resolution one would like for on-\nset labeling. For example, it is not unusual to see an av-\nerage inter-onset time of 200ms in a polyphonic work.\nTherefore, we use a semi-supervised learning method to\nenhance the performance of the audio-to-score alignment,\nleading to improved onset detector training [6]. Thus, the\ntask of acquiring training data is simply to locate corre-\nsponding audio and MIDI ﬁles and run a relatively fast\nalignment algorithm. Vastly increased numbers of train-\ning examples improve functions for onset detection.\nWe compare the results of our trained onset detector to\na high-quality, open-source onset detector, Aubio (http://\naubio.piem.org). To measure only improvements due to\nsemi-supervised training, we used exactly the same fea-\ntures as Aubio, and we also used the Aubio peak-picking\nalgorithm for our system. We also tried adding new fea-\ntures, hoping that machine learning would be able to take\nadvantage of the additional information.\nAfter the related work section that follows, we explain\nthe techniques used by our onset detection system. Sec-\ntion 4 presents an evaluation. Our ﬁndings are discussed\nin Section 5, which is followed by a concluding section.\n2 RELATED WORK\nCurrent practice in note onset detection can be separated\ninto two general approaches: 1. Apply a detection func-\ntion, often based on change in spectrum and overall power;\nthen use a temporal peak-picking algorithm to ﬁnd local\nmaxima in output from the detection function [2, 4, 1, 3].\n2. Using a variety of features, apply machine-learning\ntechniques to build a note onset classiﬁer.\nKapanci and Pfeffer [8] use a Hierarchical Model and\nsupport vector machine to estimate whether an onset is\nwithin a span of time. Marolt, et al., and Lacoste and Eck\n[10, 9] use Neural Network approaches for onset detec-\ntion. Dannenberg and Hu [6] used semi-supervised (or\nbootstrap) learning with Neural Networks for monophonic\nand piano music onset detection.\nMany of the research systems using machine-learningFigure 1 . Semi-supervised learning of an onset detector.\ntechniques are trained on the data of the Music Informa-\ntion Retrieval Evaluation eXchange (MIREX), which con-\nsists of 30 solo drum, 30 solo monophonic pitched instru-\nment, 10 solo polyphonic pitched instrument, and 15 com-\nplex (much less complex than orchestra works) pieces.\nThe total length of all sets is 14 minutes. This dataset\nis small for training a polyphonic music onset detector.\n3 TECHNIQUES\nOur approach acquires training data using score alignment\nto match symbolic (MIDI) data to recordings of acoustic\nmusic performances [11]. (See Figure 1.) The MIDI data\ncontains the onsets while the alignment tells us where to\nﬁnd these onsets in the audio recordings. Onset labels are\nused as training data for a Support Vector Machine, and\nthe output is used to further reﬁne the alignment data. This\nbootstrapping process is iterated until it converges. The\nﬁnal onset detector is treated with adaptive thresholding\nand peak picking to estimate note onset locations. These\nsteps are covered in more detail below.\n3.1 Audio-to-Score Alignment\nAudio-to-score alignment uses chroma vector features [12]\nand dynamic programming to align audio to note data from\na standard MIDI ﬁle [5]. The chroma vector captures in-\nformation about harmony and melody during a short time\ninterval, typically 50 to 250ms.\nAlignment is performed by constructing a distance ma-\ntrixSwhereSi;jis the Euclidean distance between audio\nchroma vector iand MIDI chroma vector j. Then, dy-\nnamic programming is used to ﬁnd a path from S0;0to\nSN;M that minimizes the sum of distances traversed by\nthe path. This path is then smoothed to form a continuous\nmapping between audio and MIDI. Each note onset time\nin the MIDI data can now be mapped to a corresponding\ntime in the audio.3.2 Acoustic Features\nFeature selection is important for note onset detection.\nThe main features used in our models are those of Aubio.\nThese are Energy of the Frame ,High Frequency Content ,\nSpectral Flux ,Phase Deviation ,Kullback Leibler Diver-\ngence ,Modiﬁed KL , and Complex Domain (see [2, 3] and\nhttp://aubio.piem.org for details.)\nFor some tests, we extended or modiﬁed the Aubio fea-\nture set with:\n\u000fHigher order differences : many features already in-\nclude some measure of change, such as spectral ﬂux.\nWe added ﬁrst- through fourth-order differences be-\ntween features, expanding 7 features to 35 features\nper frame.\n\u000fLarger frame : Aubio uses a default window size of\n1024 and a hop size of 512. We trained another on-\nset detector by adjusting the window size to 16384\nwith a hop size of 2048 in order to capture change\nover longer time scales.\n\u000fChroma Flux : We added a measure of change in the\nchroma vector, hoping to capture changes in melody\nand harmony, especially in string ensembles where\nslow onsets might not exhibit typical onset features.\n3.3 Semi-supervised learning\nFrom score alignment, we obtain a large set of labeled\ntraining data, but the labels are based on rather large win-\ndows, and the chroma vector features are chosen more for\ngross alignment than for precise onset detection. A boot-\nstrapping technique improves the labels while simultane-\nously learning a good onset detector [6].\nWe use a Support Vector Machine (SVM) classiﬁer with\nRadial Basis Function (RBF) kernels. Two parameters\nneed to be determined before using RBF kernels: Cand\r.\nIt is not known beforehand which Cand\rare best for the\nclassiﬁcation problem, but the difference in classiﬁcation\naccuracy between a good pair of ( C;\r) and a bad one can\nbe huge. Therefore, parameter searching should be done\nbefore training the whole model.\nWe used the LIBSVM library (http://www.csie.ntu.edu\n.tw/ cjlin/libsvm/) for the implementation of SVMs in our\nlearning. Before training on the whole dataset, we ran-\ndomly choose several independent subsets from the whole\ndataset, and then apply grid-search on Cand\rusing cross-\nvalidation. Then, we train classiﬁers on the whole dataset\nusing the best parameter pairs and choose the one with the\nbest performance.\nThe training set is initialized using features from mu-\nsic audio. The onset time of the kthonset of the aligned\nscore is denoted by Tk. A frame is labeled 1 (onset) if its\ntime matches some onset time Tk, and 0 (no onset) oth-\nerwise. The SVM is trained on this data, producing an\nonset detector whose per-frame output is interpreted as a\nprobability (from 0 to 1) of an onset in the frame.Music Title Onset\nframes\nBeethoven, Sym. #5 Op. 67, 1st mvt. 2377\nBach, Brandenburg #5, BWV 1050, 1st mvt. 4511\nChopin, Pn. Concerto #1, Op. 11, 2nd mvt. 3146\nHaydn, Sym. #94, 1st mvt. 4465\nMozart, Vn. Concerto #5, K. 219, 1st mvt. 3504\nMozart, Pn. Sonata K. 331, 1st mvt. 2349\nMozart, Sym. #40, K. 550, 1st mvt. 2076\nMozart, Cl. Quintet, K. 581, 1st mvt. 2579\nBach, Passacaglia & Fugue, BWV 582 3375\nTchaikovsky, Sym. #6, Op. 67, 1st mvt. 1638\nTable 1 . Training data.\nThe training data is then relabeled as follows: First,\na per-frame probability density P(i)is estimated by ini-\ntializing each P(i)to a small constant. Then, for each\nonset timeTk, as predicted by the score alignment, add a\nGaussian window with mean Tkand standard deviation of\nabout 100ms to P. Computef(i) =P(i)\u0002O(i)where\nO(i)is the output of the trained onset detector (so far),\nandf(i)represents onset probability after considering the\nprior knowledge Pand the probability based on features\nO(i). Finally, for each onset time Tk, ﬁnd the largest\nframe value f(i)within a window W1and (re)label it as\n1; all other frames are labeled 0. The W1window size is\ndeﬁned as follows:\nW1(i) = [max((Tk+Tk\u00001)=2;Tk\u0000W);\nmin((Tk+Tk+1)=2;Tk+W)](1)\nwhere W is 250ms. Retrain the SVM classiﬁer with the\nnew labels until the recall on a set of hand-labeled on-\nsets stops increasing. The whole learning process usually\ntakes 8 to 10 iterations.\nThe resulting onset detector is good, but returns many\nfalse positives. The detector can be further improved by\napplying a peak-picking algorithm to its output. We use\nthe same peak-picking algorithm as in Aubio to simplify\ncomparisons. It works by comparing the onset detection\nfunction output to an adaptive threshold, then searching\nfor local maxima when the threshold is crossed.\n4 EVALUATION AND RESULTS\nWe chose ten polyphonic music pieces as the training data,\nlisted in Table 1. All pieces come from the RWC Music\nDatabase [7]. The whole length of the dataset is more than\n90 minutes. There are 76,028 notes, with 30,020 distinct\nonset times separated by at least one frame period. In the\ntraining set, there are 30,020 instances labeled as positive;\nall the rest are negative.\nThe testing set consists of 18,521 notes, with 3,225\ndistinct onset times, taken from Johann II Strauss’s Blue\nDanube. As a reference, we compared our note-onset\ndetector performance to that of Aubio. Figure 2 plots\nFigure 2 . Performance comparison using Precision-\nRecall curves. Aubio : hand-tuned onset detector, SVM :\ntrained onset detection function using Aubio features,\nLong Frame : same as SVM except larger window size,\n+Chroma : same as SVM , but chroma ﬂux feature is added,\n+Higher Order : same as SVM , but higher-order differ-\nence features are added, MIDI : trained on audio synthe-\nsized from MIDI ﬁles, NoBootstrap : trained on 100 hand-\nlabeled onsets, no semi-supervised learning.\nPrecision-Recall curves for various conﬁgurations, which\ndiffer only in the features used. The best F-measures\n(F= 2PR=(P+R), wherePis precision and Ris recall)\nareAubio : 0.38, SVM : 0.47, Long Frame : 0.32, +Chroma :\n0.44, +Higher Order : 0.44, MIDI : 0.40, NoBootstrap :\n0.03 (these labels are deﬁned and also used in Figure 2).\nExcept for the Long Frame features, the onset detectors\ntrained with semi-supervised learning out-performed the\nAubio onset detector. Interestingly, the original Aubio\nfeature set worked better than any of our alternatives.\nTo give some idea of computation time, a typical run\nlabels 564s of audio containing 3225 onsets. The total\ncomputation time is 433s, of which 30s is spent calculat-\ning Aubio features, 385s for SVM classiﬁcation, and 18s\nfor peak picking. It takes about 70 hours to train the SVM\nclassiﬁer on a 2.4GHz Intel P4 system, including the 8 to\n10 bootstrapping iterations.\n5 DISCUSSION\nIn this study, machine learning outperformed a hand-tuned\ndetection system on our test data, indicating that our semi-\nsupervised learning approach is successful. At ﬁrst, we\nthought that Aubio would not perform particularly well\non orchestral music and that we would obtain signiﬁcant\nimprovements by introducing new features. We thought\nof machine learning as an efﬁcient way to explore various\nnew features. Our data suggests that ﬁnding new features\nor tuning them to orchestral music may not be so sim-\nple. None of the features we added offered signiﬁcant im-\nprovement to the original feature set we took from Aubio.\nThe improvement must be attributed to the improved clas-\nsiﬁer function.\nWhether improvements come from new features or re-\nﬁning the onset prediction function, the bottom line is thatour improvements are a direct result of machine learning,\nand any machine learning approach depends upon large\namounts of training data. Our semi-supervised learning\napproach offers a working solution to this problem. We\ndid not perform an extensive comparison of our onset de-\ntector to Aubio across a wide selection of music. Our only\nclaim is that semi-supervised learning can automatically\nadapt an onset detector to a class of music (in this case\nmassively polyphonic orchestral music) with good results.\nTo further study the contributions of semi-supervised\nlearning, we trained using SVM with a small set of 100\nhand-labeled onsets and with a large set of onsets from\nsynthesized MIDI ﬁles (see Figure 2). We also tried train-\ning on the score-alignment data without bootstrapping.\nThe results show it is important to have both a large data\nsetandactual acoustic data.\nOur test data contains thousands of points that seem\ntypical of orchestral music onset detection, but all of these\ncome from one performance that was held out from train-\ning. Further testing has begun to conﬁrm these initial re-\nsults, and a full cross-validation study is in progress.\nAnalysis of our audio-to-score alignment using a ran-\ndom sample of 100 hand-labeled onsets reveals that there\nare signiﬁcant alignment problems in some cases. Further\nwork is needed to identify the sources of these problems,\nwhich could include errors in MIDI ﬁles and limitations\nof our alignment algorithm. When alignment errors are\nmore than about 100ms, the onset labels might as well\nbe random because the average interval between onsets\nin our entire dataset is about 200ms. That we are able\nto show good performance in spite of the fact that many\nof our training data points are effectively random is actu-\nally a strong endorsement of our approach. It seems likely\nthat the bootstrapping process is “searching” for real on-\nsets in the neighborhoods of erroneously labeled frames,\nminimizing the damage of the bad data. Our test data is\nbased solely upon score alignment without further reﬁne-\nment using bootstrapping techniques. We are working to\ncharacterize and improve the quality of the test data.\n6 SUMMARY AND CONCLUSIONS\nOnset detection in polyphonic music and particularly or-\nchestral music is difﬁcult. We explored the use of machine\nlearning to improve onset detection functions. To solve\nthe problem of training data, we use a semi-supervised\nlearning technique combined with score alignment. The\nresult of alignment is an estimate of the onset time of ev-\nery note in the MIDI ﬁle, and these estimates are improved\nby iteratively applying our onset detector and then retrain-\ning on the new data.\nOur resulting onset detection function shows a signiﬁ-\ncant improvement over a hand-tuned onset detector using\nthe same features. Our onset detector is trained on poly-\nphonic music that is mostly from orchestra performances.\nWhile this is an interesting problem in itself, future work\nmight explore whether it is better to specialize onset de-\ntectors for different types of music or to pool all availabletraining data and create one general-purpose onset detec-\ntor. Either way, semi-supervised learning is a promising\napproach to gathering large amounts of training data, lead-\ning to signiﬁcant improvements in onset detection.\n7 REFERENCES\n[1] Alonso, M., Richard, G., and David, B. “Extracting\nnote onsets from musical recordings,” IEEE Interna-\ntional Conference on Multimedia & Expo , Amster-\ndam, 2005.\n[2] Bello, J., Duxbury, C., Davies, M., and Sandler, M.\n“On the use of phase and energy for musical onset de-\ntection in the complex domain,” IEEE Signal Process-\ning Letters , vol. 11, no. 6 June 2004.\n[3] Brossier, P., Bello, J., and Plumbley, M. “Real-time\ntemporal segmentation of note objects in music sig-\nnals,” in Proceedings of the ICMC , Miami, Florida,\n2004, pp. 458-461.\n[4] Collins, N. “A Change Discrimination Onset Detec-\ntor with Peak Scoring Peak Picker and Time Domain\nCorrection,” Music Information Retrieval Exchange ,\nMIREX 2005.\n[5] Dannenberg, R., and Hu, N. “Polyphonic Audio\nMatching for Score Following and Intelligent Audio\nEditors,” in Proceedings of the 2003 Inter. Computer\nMusic Conference , Singapore, 2003, pp. 27-34.\n[6] Dannenberg, R., and Hu, N. “Bootstrap learning for\naccurate onset detection.” Machine Learning 65 (2-3),\npp. 457-471.\n[7] Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R.\n“RWC Music Database: Popular, Classical, and Jazz\nMusic Databases,” in Proceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR 2002) , (October) 2002, pp. 287-288.\n[8] Kapanci, E., and Pfeffer, A. “A hierarchical approach\nto onset detection,” in Proceedings of the ICMC , Mi-\nami, Florida, 2004, pp. 438-441.\n[9] Lacoste, A., and Eck, D. “Onset Detection with Artiﬁ-\ncial Neural Networks for MIREX 2005,” Music Infor-\nmation Retrieval Exchange , MIREX 2005.\n[10] Marolt, M., Kavcic, A., and Privosnik, M. “Neural\nnetworks for note onset detection in piano music,” in\nProceeding of the 2002 ICMC , Miami, Florida, 2002.\n[11] Turetsky, R., and Ellis, D. “Ground-Truth Transcrip-\ntions of Real Music from Force-Aligned MIDI Syn-\ntheses,” 4th Inter. Symposium on Music Information\nRetrieval ISMIR-03 Baltimore, 2003, pp. 135-141.\n[12] Wakeﬁeld, G. “Mathematical representation of joint\ntime-chroma distributions,” in International Sympo-\nsium on Optical Science, Engineering, and Instrumen-\ntation, SPIE’99 , Denver, 1999."
    },
    {
        "title": "Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007, Vienna, Austria, September 23-27, 2007",
        "author": [
            "Simon Dixon",
            "David Bainbridge 0001",
            "Rainer Typke"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": null,
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\nŞentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmanoğlu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., Şentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/2007"
    }
]