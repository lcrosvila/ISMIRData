[
    {
        "title": "Computational Modeling of Induced Emotion Using GEMS.",
        "author": [
            "Anna Aljanaki",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415192",
        "url": "https://doi.org/10.5281/zenodo.1415192",
        "ee": "https://zenodo.org/records/1415192/files/AljanakiWV14.pdf",
        "abstract": "Most researchers in the automatic music emotion recogni- tion field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. More- over, in many cases it might be important to model in- duced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which ad- dresses these two issues. We collected the data for our study using a game with a purpose. We exploit a compre- hensive set of features from several state-of-the-art tool- boxes and propose a new set of harmonically motivated features. The performance of these feature sets is com- pared. Additionally, we use expert human annotations to explore the dependency between musicologically mean- ingful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can bet- ter approximate human perception.",
        "zenodo_id": 1415192,
        "dblp_key": "conf/ismir/AljanakiWV14",
        "keywords": [
            "automatic music emotion recognition",
            "two-dimensional valence and arousal model",
            "diversity of emotions",
            "induced emotion",
            "multidimensional emotional space",
            "Geneva Emotional Music Scales (GEMS)",
            "complicated features",
            "expert human annotations",
            "musicologically meaningful characteristics",
            "approximate human perception"
        ]
    },
    {
        "title": "The VIS Framework: Analyzing Counterpoint in Large Datasets.",
        "author": [
            "Christopher Antila",
            "Julie Cumming"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417767",
        "url": "https://doi.org/10.5281/zenodo.1417767",
        "ee": "https://zenodo.org/records/1417767/files/AntilaC14.pdf",
        "abstract": "The VIS Framework for Music Analysis is a modular Python library designed for \u201cbig data\u201d queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strat- egy in VIS, and the flexibility of this approach for future researchers.",
        "zenodo_id": 1417767,
        "dblp_key": "conf/ismir/AntilaC14",
        "keywords": [
            "VIS Framework",
            "modular Python library",
            "big data queries",
            "symbolic musical data",
            "musicological needs",
            "counterpoint study",
            "music21 library",
            "pandas library",
            "style change",
            "contrapuntal patterns"
        ]
    },
    {
        "title": "An Association-based Approach to Genre Classification in Music.",
        "author": [
            "Tom Arjannikov",
            "John Z. Zhang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415786",
        "url": "https://doi.org/10.5281/zenodo.1415786",
        "ee": "https://zenodo.org/records/1415786/files/ArjannikovZ14.pdf",
        "abstract": "Music Information Retrieval (MIR) is a multi-disciplin- ary research area that aims to automate the access to large- volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a non- trivial problem in the field, namely music genre classifi- cation, which is one of the core tasks in MIR. In our pro- posed approach, we make use of association analysis to study and predict music genres based on the acoustic fea- tures extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various re- lated issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi- label situation in genre classification.",
        "zenodo_id": 1415786,
        "dblp_key": "conf/ismir/ArjannikovZ14",
        "keywords": [
            "Music Information Retrieval",
            "Automate access to large-volume music data",
            "Music genre classification",
            "Association analysis",
            "Acoustic features",
            "Content-based features",
            "Genre prediction",
            "Feasibility experiments",
            "Publicly available datasets",
            "Multi-label situation"
        ]
    },
    {
        "title": "Tempo- and Transposition-invariant Identification of Piece and Score Position.",
        "author": [
            "Andreas Arzt",
            "Gerhard Widmer",
            "Reinhard Sonnleitner"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415850",
        "url": "https://doi.org/10.5281/zenodo.1415850",
        "ee": "https://zenodo.org/records/1415850/files/ArztWS14.pdf",
        "abstract": "We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo- invariant symbolic fingerprinting method, replacing the ab- solute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big de- crease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verifi- cation step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the re- trieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications.",
        "zenodo_id": 1415850,
        "dblp_key": "conf/ismir/ArztWS14",
        "keywords": [
            "algorithm",
            "audio performance",
            "database of musical scores",
            "tempo-invariant",
            "transposition-invariant",
            "symbolic fingerprinting",
            "relative representation",
            "discriminative power",
            "noise",
            "tracking algorithm"
        ]
    },
    {
        "title": "Cognition-inspired Descriptors for Scalable Cover Song Retrieval.",
        "author": [
            "Jan Van Balen",
            "Dimitrios Bountouridis",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417795",
        "url": "https://doi.org/10.5281/zenodo.1417795",
        "ee": "https://zenodo.org/records/1417795/files/BalenBWV14.pdf",
        "abstract": "Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the de- velopment of descriptors that effectively capture musical structures at the song level. The performance of the pro- posed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance.",
        "zenodo_id": 1417795,
        "dblp_key": "conf/ismir/BalenBWV14",
        "keywords": [
            "music cognition studies",
            "computational musicology",
            "mid- to high-level computational analysis",
            "content-based retrieval",
            "descriptors",
            "musical structures",
            "cover song retrieval",
            "scalable",
            "stability and variation",
            "model"
        ]
    },
    {
        "title": "Estimation of the Direction of Strokes and Arpeggios.",
        "author": [
            "Isabel Barbancho",
            "George Tzanetakis",
            "Lorenzo J. Tard\u00f3n",
            "Peter F. Driessen",
            "Ana M. Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418053",
        "url": "https://doi.org/10.5281/zenodo.1418053",
        "ee": "https://zenodo.org/records/1418053/files/BarbanchoTTDB14.pdf",
        "abstract": "Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actu- ally, in some instruments, it is impossible to trigger mul- tiple notes simultaneously. In others, the player can con- sciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically es- timate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analy- sis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direc- tion, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ.",
        "zenodo_id": 1418053,
        "dblp_key": "conf/ismir/BarbanchoTTDB14",
        "keywords": [
            "chord",
            "notes",
            "instrument",
            "simultaneously",
            "player",
            "sequence",
            "notes",
            "pitch",
            "system",
            "estimation"
        ]
    },
    {
        "title": "Exploiting Instrument-wise Playing/Non-Playing Labels for Score Synchronization of Symphonic Music.",
        "author": [
            "Alessio Bazzica",
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415772",
        "url": "https://doi.org/10.5281/zenodo.1415772",
        "ee": "https://zenodo.org/records/1415772/files/BazzicaLH14.pdf",
        "abstract": "Synchronization of a score to an audio-visual music per- formance recording is usually done by solving an audio- to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise \u201cplaying\u201d (P) and \u201cnon-playing\u201d (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based repre- sentation of the music piece, both at the score and perfor- mance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches.",
        "zenodo_id": 1415772,
        "dblp_key": "conf/ismir/BazzicaLH14",
        "keywords": [
            "score",
            "audio-visual music performance recording",
            "synchronization",
            "audio-to-MIDI alignment problem",
            "instrument-wise playing (P) and non-playing (NP) labels",
            "efficient way of computing the distance",
            "dynamic time warping",
            "P/NP-based representation",
            "performance-to-score synchronization",
            "robustness"
        ]
    },
    {
        "title": "Template Adaptation for Improving Automatic Music Transcription.",
        "author": [
            "Emmanouil Benetos",
            "Roland Badeau",
            "Tillman Weyde",
            "Ga\u00ebl Richard"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418059",
        "url": "https://doi.org/10.5281/zenodo.1418059",
        "ee": "https://zenodo.org/records/1418059/files/BenetosBWR14.pdf",
        "abstract": "In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based auto- matic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By per- forming a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We pro- pose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch de- tection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed.",
        "zenodo_id": 1418059,
        "dblp_key": "conf/ismir/BenetosBWR14",
        "keywords": [
            "automatic music transcription",
            "dictionary templates",
            "spectral shape",
            "instrument sources",
            "conservative transcription pre-processing",
            "adaptive transcription",
            "single-instrument transcription",
            "multiple-instrument transcription",
            "MAPS database",
            "Bach10 database"
        ]
    },
    {
        "title": "MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.",
        "author": [
            "Rachel M. Bittner",
            "Justin Salamon",
            "Mike Tierney",
            "Matthias Mauch",
            "Chris Cannam",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.2620624",
        "url": "https://doi.org/10.5281/zenodo.2620624",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T028_322_Paper.pdf",
        "abstract": "Subset of MedleyDB: 103 solomonophonic stem audio filesand correspondingmanually annotated pitch (f0) annotations.\n\nFor further details, refer to theMedleyDB website.\n\nFurther Annotation and Metadata files are version controlled and are available in theMedleyDB githubrepository:Metadatacan be foundhere,Annotationscan be foundhere.\n\nFor detailed information about the dataset, please visit MedleyDB&#39;swebsite.\n\n\n\nIf you make use of MedleyDB for academic purposes, please cite the following publication:\n\nR. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.",
        "zenodo_id": 2620624,
        "dblp_key": "conf/ismir/BittnerSTMCB14",
        "keywords": [
            "Subset",
            "Solomonophonic",
            "Audio",
            "Files",
            "Pitch",
            "Annotations",
            "MedleyDB",
            "Website",
            "Version",
            "Controlled"
        ]
    },
    {
        "title": "A Multi-model Approach to Beat Tracking Considering Heterogeneous Music Styles.",
        "author": [
            "Sebastian B\u00f6ck",
            "Florian Krebs",
            "Gerhard Widmer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415240",
        "url": "https://doi.org/10.5281/zenodo.1415240",
        "ee": "https://zenodo.org/records/1415240/files/BockKW14.pdf",
        "abstract": "In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possi- ble beat positions. It chooses the model with the most ap- propriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state- of-the-art methods. Under certain conditions the system is able to match even human tapping performance.",
        "zenodo_id": 1415240,
        "dblp_key": "conf/ismir/BockKW14",
        "keywords": [
            "beat tracking algorithm",
            "multi-model approach",
            "recurrent neural networks",
            "tempo and phase modeling",
            "dynamic Bayesian network",
            "performance gains",
            "human tapping performance",
            "various music styles",
            "state-of-the-art methods",
            "experimental evaluation"
        ]
    },
    {
        "title": "Information-Theoretic Measures of Music Listening Behaviour.",
        "author": [
            "Daniel Boland",
            "Roderick Murray-Smith"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416192",
        "url": "https://doi.org/10.5281/zenodo.1416192",
        "ee": "https://zenodo.org/records/1416192/files/BolandM14.pdf",
        "abstract": "We present an information-theoretic approach to the mea- surement of users\u2019 music listening behaviour and selection of music features. Existing ethnographic studies of mu- sic use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10, 000 hand- made playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users\u2019 criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels.",
        "zenodo_id": 1416192,
        "dblp_key": "conf/ismir/BolandM14",
        "keywords": [
            "entropy",
            "SPUD dataset",
            "music listening behaviour",
            "music retrieval systems",
            "user and audio stream metadata",
            "music features",
            "playlist curation",
            "objective ground truth labels",
            "quantitative approach",
            "user-centred evaluation"
        ]
    },
    {
        "title": "On Comparative Statistics for Labelling Tasks: What can We Learn from MIREX ACE 2013?",
        "author": [
            "John Ashley Burgoyne",
            "W. Bas de Haas",
            "Johan Pauwels"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417091",
        "url": "https://doi.org/10.5281/zenodo.1417091",
        "ee": "https://zenodo.org/records/1417091/files/BurgoyneHP14.pdf",
        "abstract": "For , the evaluation of audio chord estimation () followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the  evaluations from previous years. With this new informa- tion, however, comes new interpretive challenges. What are the correlations among different songs and, more im- portantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in propor- tion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song- algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires mov- ing beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for , supported with gen- eralised estimating equations () to correct for repeated measures. We use the results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone.",
        "zenodo_id": 1417091,
        "dblp_key": "conf/ismir/BurgoyneHP14",
        "keywords": [
            "evaluation",
            "audio chord estimation",
            "new scheme",
            "chord vocabularies",
            "segmentation measures",
            "informa- tion",
            "interpretive challenges",
            "correlations",
            "songs",
            "submissions"
        ]
    },
    {
        "title": "Theoretical Framework of A Computational Model of Auditory Memory for Music Emotion Recognition.",
        "author": [
            "Marcelo F. Caetano",
            "Frans Wiering"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417211",
        "url": "https://doi.org/10.5281/zenodo.1417211",
        "ee": "https://zenodo.org/records/1417211/files/CaetanoW14.pdf",
        "abstract": "The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. How- ever, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incor- porates temporal information into MER systems. We ad- vocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER re- searchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions.",
        "zenodo_id": 1417211,
        "dblp_key": "conf/ismir/CaetanoW14",
        "keywords": [
            "semantic gap",
            "glass ceiling",
            "alternative proposals",
            "auditory memory",
            "temporal information",
            "MER systems",
            "musical meaning",
            "musical emotions",
            "improved systems",
            "complexity"
        ]
    },
    {
        "title": "Developing Tonal Perception through Unsupervised Learning.",
        "author": [
            "Carlos Eduardo Cancino Chac\u00f3n",
            "Stefan Lattner",
            "Maarten Grachten"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416058",
        "url": "https://doi.org/10.5281/zenodo.1416058",
        "ee": "https://zenodo.org/records/1416058/files/ChaconLG14.pdf",
        "abstract": "The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in en- culturation, the latter accounting for cross-cultural differ- ences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how mu- sical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochas- tic neural network. We show that even based on a lim- ited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organiza- tion of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analy- sis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the ex- traction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone exper- iments by Krumhansl and Shepard, measuring the organi- zation of tones within a key in human perception. We find that the responses of the RBM share qualitative character- istics with those of both trained and untrained listeners.",
        "zenodo_id": 1416058,
        "dblp_key": "conf/ismir/ChaconLG14",
        "keywords": [
            "tonal structure",
            "perception",
            "low-level perceptual mechanisms",
            "cultural differences",
            "unsupervised machine learning",
            "Restricted Boltzmann Machine",
            "generative stochastic neural network",
            "musical concepts",
            "learning from musical data",
            "circle of fifths"
        ]
    },
    {
        "title": "Improving Query by Tapping via Tempo Alignment.",
        "author": [
            "Chun-Ta Chen",
            "Jyh-Shing Roger Jang",
            "Chun-Hung Lu"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416910",
        "url": "https://doi.org/10.5281/zenodo.1416910",
        "ee": "https://zenodo.org/records/1416910/files/ChenJL14.pdf",
        "abstract": "Query by tapping (QBT) is a content trieval method that can retrieve a song by taking the u er\u2019s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (i ter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Exper ments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy 10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT pr vides an interesting and innovative way for music retrie al by shaking or tapping the devices, which is also di cussed in the paper.",
        "zenodo_id": 1416910,
        "dblp_key": "conf/ismir/ChenJL14",
        "keywords": [
            "query-by-tapping",
            "content retrieval",
            "song matching",
            "note onset intervals",
            "dynamic programming",
            "IOI vector",
            "dataset alignment",
            "accuracy 10",
            "MRR",
            "personal handheld devices"
        ]
    },
    {
        "title": "Multiple Viewpiont Melodic Prediction with Fixed-Context Neural Networks.",
        "author": [
            "Srikanth Cherla",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416944",
        "url": "https://doi.org/10.5281/zenodo.1416944",
        "ee": "https://zenodo.org/records/1416944/files/CherlaWG14.pdf",
        "abstract": "The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previ- ous work using this representation has predominantly re- lied on n-gram and variable order Markov models for mu- sic sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann ma- chines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is com- parable to that of models previously evaluated on the same task. We then combine the predictions of individual mod- els using an entropy-weighted combination scheme to im- prove the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the indi- vidual models in the combination.",
        "zenodo_id": 1416944,
        "dblp_key": "conf/ismir/CherlaWG14",
        "keywords": [
            "multiple viewpoints representation",
            "symbolic music data",
            "analysis and generation",
            "n-gram and variable order Markov models",
            "restricted Boltzmann machines",
            "neural network models",
            "fixed-length sequences",
            "pitch prediction",
            "entropy-weighted combination",
            "overall prediction performance"
        ]
    },
    {
        "title": "Improving Rhythmic Transcriptions via Probability Models Applied Post-OMR.",
        "author": [
            "Maura Church",
            "Michael Scott Cuthbert"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416752",
        "url": "https://doi.org/10.5281/zenodo.1416752",
        "ee": "https://zenodo.org/records/1416752/files/ChurchC14.pdf",
        "abstract": "Despite many improvements in the recognition of graph- ical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self- similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and move- ments (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future re- finements and applications to pitch correction that could further improve the accuracy of OMR systems.",
        "zenodo_id": 1416752,
        "dblp_key": "conf/ismir/ChurchC14",
        "keywords": [
            "Optical Music Recognition",
            "Inaccuracies in resultant score",
            "Rhythmic errors",
            "Time-consuming to fix",
            "Rhythmic self-similarity",
            "Prior probabilities",
            "Commercial OMR system",
            "Correct rhythmic errors",
            "Contradiction between notes",
            "Underlying time signature"
        ]
    },
    {
        "title": "Social Music in Cars.",
        "author": [
            "Sally Jo Cunningham",
            "David M. Nichols",
            "David Bainbridge 0001",
            "Hassan Ali"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415676",
        "url": "https://doi.org/10.5281/zenodo.1415676",
        "ee": "https://zenodo.org/records/1415676/files/CunninghamNBA14.pdf",
        "abstract": "This paper builds an understanding of how music is cur- rently experienced by a social group travelling together in a car\u2014how songs are chosen for playing, how music both reflects and influences the group\u2019s mood and social interac- tion, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social expe- rience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data.",
        "zenodo_id": 1415676,
        "dblp_key": "conf/ismir/CunninghamNBA14",
        "keywords": [
            "music",
            "social group",
            "car travel",
            "song selection",
            "group mood",
            "social interaction",
            "hardware/software",
            "ethnographic data",
            "in-car music",
            "user interface"
        ]
    },
    {
        "title": "Evaluating the Evaluation Measures for Beat Tracking.",
        "author": [
            "Matthew E. P. Davies",
            "Sebastian B\u00f6ck"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415128",
        "url": "https://doi.org/10.5281/zenodo.1415128",
        "ee": "https://zenodo.org/records/1415128/files/DaviesB14.pdf",
        "abstract": "The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evalu- ation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple ac- curacy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the re- lationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow al- ternative metrical levels appear more correlated than those which do not. Second, we explore the effect of param- eterisation of objective evaluation measures, and demon- strate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being over- estimated via objective evaluation.",
        "zenodo_id": 1415128,
        "dblp_key": "conf/ismir/DaviesB14",
        "keywords": [
            "human listeners",
            "judgment performance",
            "alternative metrical levels",
            "objective evalu- ation measures",
            "correlation between objective scores and subjective ratings",
            "effect of parameterisation",
            "correlation maximised",
            "smaller tolerance windows",
            "true beat tracking performance",
            "over-estimated via objective evaluation"
        ]
    },
    {
        "title": "Ten Years of MIREX (Music Information Retrieval Evaluation eXchange): Reflections, Challenges and Opportunities.",
        "author": [
            "J. Stephen Downie",
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Kahyun Choi",
            "Sally Jo Cunningham",
            "Yun Hao"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417181",
        "url": "https://doi.org/10.5281/zenodo.1417181",
        "ee": "https://zenodo.org/records/1417181/files/DownieHLCCH14.pdf",
        "abstract": "The Music Information Retrieval Evaluation eXchange (MIREX) has been run annually since 2005, with the Oc- tober 2014 plenary marking its tenth iteration. By 2013, MIREX has evaluated approximately 2000 individual music information retrieval (MIR) algorithms for a wide range of tasks over 37 different test collections. MIREX has involved researchers from over 29 different countries with a median of 109 individual participants per year. This paper summarizes the history of MIREX from its earliest planning meeting in 2001 to the present. It re- flects upon the administrative, financial, and technologi- cal challenges MIREX has faced and describes how those challenges have been surmounted. We propose new fund- ing models, a distributed evaluation framework, and more holistic user experience evaluation tasks-some evolu- tionary, some revolutionary for the continued success of MIREX. We hope that this paper will inspire MIR com- munity members to contribute their ideas so MIREX can have many more successful years to come.",
        "zenodo_id": 1417181,
        "dblp_key": "conf/ismir/DownieHLCCH14"
    },
    {
        "title": "Extending Harmonic-Percussive Separation of Audio Signals.",
        "author": [
            "Jonathan Driedger",
            "Meinard M\u00fcller",
            "Sascha Disch"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415226",
        "url": "https://doi.org/10.5281/zenodo.1415226",
        "ee": "https://zenodo.org/records/1415226/files/DriedgerMD14.pdf",
        "abstract": "In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the com- puted components are often not of purely harmonic or per- cussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, de- pending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive compo- nent and vice versa. In this paper we present two exten- sions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition pro- cess that allows for tightening separation results and for enforcing the components to be clearly harmonic or per- cussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel con- cept is exploited to add a third residual component to the decomposition which captures the sounds that lie in be- tween the clearly harmonic and percussive sounds of the audio signal.",
        "zenodo_id": 1415226,
        "dblp_key": "conf/ismir/DriedgerMD14",
        "keywords": [
            "harmonic",
            "percussive",
            "noise-like",
            "leakage",
            "harmonic sounds",
            "percussive component",
            "state-of-the-art",
            "separation factor",
            "STN audio model",
            "residual component"
        ]
    },
    {
        "title": "Note-level Music Transcription by Maximum Likelihood Sampling.",
        "author": [
            "Zhiyao Duan",
            "David Temperley"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416534",
        "url": "https://doi.org/10.5281/zenodo.1416534",
        "ee": "https://zenodo.org/records/1416534/files/DuanT14.pdf",
        "abstract": "Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estima- tion (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note- level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experi- ments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions.",
        "zenodo_id": 1416534,
        "dblp_key": "conf/ismir/DuanT14",
        "keywords": [
            "note-level music transcription",
            "pitch estimation",
            "spurious notes",
            "random sampling",
            "MPE model",
            "likelihood evaluation",
            "transcription performance",
            "polyphony",
            "state-of-the-art systems",
            "J.S. Bach chorales"
        ]
    },
    {
        "title": "Discovering Typical Motifs of a Raga from One-Liners of Songs in Carnatic Music.",
        "author": [
            "Shrey Dutta",
            "Hema A. Murthy"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418157",
        "url": "https://doi.org/10.5281/zenodo.1418157",
        "ee": "https://zenodo.org/records/1418157/files/DuttaM14.pdf",
        "abstract": "Typical motifs of a r\u00afaga can be found in the various songs that are composed in the same r\u00afaga by different composers. The compositions in Carnatic music have a definite struc- ture, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic pro- gramming based algorithm. The density of match between the one-liners and normalized cost along-with a new mea- sure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and lo- cate the matched pattern. The typical motifs of a r\u00afaga are then filtered using compositions of various r\u00afagas. Motifs are considered typical if they are present in the composi- tions of the given r\u00afaga and are not found in compositions of other r\u00afagas.",
        "zenodo_id": 1418157,
        "dblp_key": "conf/ismir/DuttaM14",
        "keywords": [
            "r\u00afaga",
            "songs",
            "composers",
            "structure",
            "pallavi",
            "anupallavi",
            "charanam",
            "tala",
            "one-liners",
            "matched pattern"
        ]
    },
    {
        "title": "Impact of Listening Behavior on Music Recommendation.",
        "author": [
            "Katayoun Farrahi",
            "Markus Schedl",
            "Andreu Vall",
            "David Hauger",
            "Marko Tkalcic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417505",
        "url": "https://doi.org/10.5281/zenodo.1417505",
        "ee": "https://zenodo.org/records/1417505/files/FarrahiSVHT14.pdf",
        "abstract": "The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommen- dations with features extracted from a user\u2019s history of lis- tening events. We investigate the impact of listener\u2019s be- havior by considering features such as play counts, \u201cmain- streaminess\u201d, and diversity in music taste on the perfor- mance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user\u2019s listening behavior is characterized into a three dimensional feature space consisting of play count, \u201cmainstreaminess\u201d (i.e. the degree to which the observed user listens to cur- rently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influ- ence figures of merit of various music recommendation ap- proaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country.",
        "zenodo_id": 1417505,
        "dblp_key": "conf/ismir/FarrahiSVHT14",
        "keywords": [
            "music recommendation systems",
            "increasingly intelligent",
            "user behavior",
            "personalized recommendations",
            "features extracted",
            "users listening events",
            "three dimensional feature space",
            "play counts",
            "mainstreaminess",
            "diversity"
        ]
    },
    {
        "title": "On Inter-rater Agreement in Audio Music Similarity.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416970",
        "url": "https://doi.org/10.5281/zenodo.1416970",
        "ee": "https://zenodo.org/records/1416970/files/Flexer14.pdf",
        "abstract": "One of the central tasks in the annual MIREX evaluation campaign is the \u201dAudio Music Similarity and Retrieval (AMS)\u201d task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judg- ment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of per- formance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algo- rithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity.",
        "zenodo_id": 1416970,
        "dblp_key": "conf/ismir/Flexer14",
        "keywords": [
            "Audio Music Similarity and Retrieval (AMS)",
            "MIREX evaluation campaign",
            "Human graders",
            "Subjective judg- ment",
            "Inter-rater agreement",
            "Performance upper bound",
            "Participating algorithms",
            "2009",
            "Upper bound achievement",
            "Future evaluations"
        ]
    },
    {
        "title": "Automatic Instrument Classification of Ethnomusicological Audio Recordings.",
        "author": [
            "Dominique Fourer",
            "Jean-Luc Rouas",
            "Pierre Hanna",
            "Matthias Robine"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417655",
        "url": "https://doi.org/10.5281/zenodo.1417655",
        "ee": "https://zenodo.org/records/1417655/files/FourerRHR14.pdf",
        "abstract": "Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of in- terest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approa- ches which require qualitative and diversified training data- bases to obtain accurate results. In this paper, we intro- duce a diversified database composed of worldwide non- western instruments audio recordings on which is evalu- ated an effective timbre classification method. A compar- ative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicologi- cal indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis.",
        "zenodo_id": 1417655,
        "dblp_key": "conf/ismir/FourerRHR14",
        "keywords": [
            "Automatic timbre characterization",
            "audio signals",
            "measuring similarities",
            "machine learning approaches",
            "training data-bases",
            "effective timbre classification",
            "worldwide non-western instruments",
            "evaluation",
            "Iowa musical instruments database",
            "practical solution"
        ]
    },
    {
        "title": "A Proximity Grid Optimization Method to Improve Audio Search for Sound Design.",
        "author": [
            "Christian Frisson",
            "St\u00e9phane Dupont",
            "Willy Yvart",
            "Nicolas Riche",
            "Xavier Siebert",
            "Thierry Dutoit"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417245",
        "url": "https://doi.org/10.5281/zenodo.1417245",
        "ee": "https://zenodo.org/records/1417245/files/FrissonDYRSD14.pdf",
        "abstract": "Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research ap- plications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student- t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a stan- dard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This op- timization method can serve as a human-readable metric for the comparison of dimension reduction techniques.",
        "zenodo_id": 1417245,
        "dblp_key": "conf/ismir/FrissonDYRSD14",
        "keywords": [
            "sound designers",
            "dedicated applications",
            "default file browsers",
            "content-based research applications",
            "cloud-like similarity layouts",
            "feature extraction",
            "dimension reduction",
            "Student- t Stochastic Neighbor Embedding",
            "proximity grid",
            "proximity grid optimization"
        ]
    },
    {
        "title": "Towards Modeling Texture in Symbolic Data.",
        "author": [
            "Mathieu Giraud",
            "Florence Lev\u00e9",
            "Florent Mercier",
            "Marc Rigaudi\u00e8re",
            "Donatien Thorez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415030",
        "url": "https://doi.org/10.5281/zenodo.1415030",
        "ee": "https://zenodo.org/records/1415030/files/GiraudLMRT14.pdf",
        "abstract": "Studying texture is a part of many musicological analy- ses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature com- monly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to for- malize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an al- gorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evalua- tion of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn.",
        "zenodo_id": 1415030,
        "dblp_key": "conf/ismir/GiraudLMRT14",
        "keywords": [
            "texture",
            "musicological analyses",
            "cognition of musical structures",
            "melody",
            "accompaniment layers",
            "polyphonic data",
            "homorhythmic layers",
            "polyphonic data",
            "parallel motions",
            "ground truth analysis"
        ]
    },
    {
        "title": "Towards Automatic Content-Based Separation of DJ Mixes into Single Tracks.",
        "author": [
            "Nikolay Glazyrin"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415566",
        "url": "https://doi.org/10.5281/zenodo.1415566",
        "ee": "https://zenodo.org/records/1415566/files/Glazyrin14.pdf",
        "abstract": "DJ mixes and radio show recordings constitute an impor- tant and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural seg- mentation, this problem has a number of distinctive fea- tures that make difficulties for structural segmentation al- gorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usu- ally comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available times- tamped DJ mix recordings of various styles.",
        "zenodo_id": 1415566,
        "dblp_key": "conf/ismir/Glazyrin14",
        "keywords": [
            "DJ mixes",
            "radio show recordings",
            "continuous DJ mix separation",
            "single tracks",
            "timestamping a mix",
            "music and data source",
            "structural segmentation",
            "distinctive features",
            "algorithm",
            "content-based data"
        ]
    },
    {
        "title": "Estimating Musical Time Information from Performed MIDI Files.",
        "author": [
            "Harald Grohganz",
            "Michael Clausen",
            "Meinard M\u00fcller"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417925",
        "url": "https://doi.org/10.5281/zenodo.1417925",
        "ee": "https://zenodo.org/records/1417925/files/GrohganzCM14.pdf",
        "abstract": "Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score- related parameters. MIDI allows for representing musi- cal time information as specified by sheet music as well as physical time information that reflects performance as- pects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this pa- per, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert per- formed MIDI files into semantically enriched score-like MIDI files.",
        "zenodo_id": 1417925,
        "dblp_key": "conf/ismir/GrohganzCM14",
        "keywords": [
            "MIDI",
            "musical time information",
            "sheet music",
            "physical time information",
            "score-related parameters",
            "musical beat",
            "tempo information",
            "performed MIDI file",
            "global estimate of the time signature",
            "local errors in the pulse grid estimation"
        ]
    },
    {
        "title": "Musical Structural Analysis Database Based on GTTM.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415658",
        "url": "https://doi.org/10.5281/zenodo.1415658",
        "ee": "https://zenodo.org/records/1415658/files/HamanakaHT14.pdf",
        "abstract": "This paper, we present the publication of our analysis da- ta and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical piec- es with standard MIDI files and annotated data are key to advancements in the field of music information technolo- gy. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical struc- ture analysis, we are now publicizing 300 pieces of anal- ysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations.",
        "zenodo_id": 1415658,
        "dblp_key": "conf/ismir/HamanakaHT14",
        "keywords": [
            "generative theory of tonal music",
            "musical databases",
            "music information technology",
            "analysis data",
            "analyzing tool",
            "step-by-step collection",
            "research advancements",
            "musical structure analysis",
            "new musicologist results",
            "original GTTM database"
        ]
    },
    {
        "title": "Hierarchical Approach to Detect Common Mistakes of Beginner Flute Players.",
        "author": [
            "Yoonchang Han",
            "Kyogu Lee"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415878",
        "url": "https://doi.org/10.5281/zenodo.1415878",
        "ee": "https://zenodo.org/records/1415878/files/HanL14.pdf",
        "abstract": "Music lessons are a repetitive process of giving feedback on a student\u2019s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instru- ment. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical ap- proach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incor- rect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The sys- tem is structured hierarchically, as mis-fingering detec- tion requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Exper- imental results show that it is possible to identify differ- ent mistakes in flute performance using our proposed al- gorithms.",
        "zenodo_id": 1415878,
        "dblp_key": "conf/ismir/HanL14",
        "keywords": [
            "repetitive process",
            "feedback on performance",
            "improvement of performance skills",
            "unique characteristics of instruments",
            "common mistakes of beginner flute players",
            "hierarchical approach",
            "detection of common mistakes",
            "structured algorithms",
            "quantifying quality of flute sound",
            "structured hierarchy"
        ]
    },
    {
        "title": "Predicting Expressive Dynamics in Piano Performances using Neural Networks.",
        "author": [
            "Sam van Herwaarden",
            "Maarten Grachten",
            "W. Bas de Haas"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416678",
        "url": "https://doi.org/10.5281/zenodo.1416678",
        "ee": "https://zenodo.org/records/1416678/files/HerwaardenGH14.pdf",
        "abstract": "This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these fea- tures are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a B\u00a8osendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by differ- ent pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach dif- fers from earlier work in a number of ways: (1) an ad- ditional input representation based on a local history of velocity values is used, (2) the RBMs are trained to re- sult in a network with sparse activations, (3) network con- nectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance.",
        "zenodo_id": 1416678,
        "dblp_key": "conf/ismir/HerwaardenGH14",
        "keywords": [
            "predicting",
            "expressive",
            "accentuation",
            "piano",
            "performances",
            "neural",
            "networks",
            "Restricted",
            "Bosendorfer",
            "piano"
        ]
    },
    {
        "title": "Tracking the &quot;Odd&quot;: Meter Inference in a Culturally Diverse Music Corpus.",
        "author": [
            "Andre Holzapfel",
            "Florian Krebs",
            "Ajay Srinivasamurthy"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415000",
        "url": "https://doi.org/10.5281/zenodo.1415000",
        "ee": "https://zenodo.org/records/1415000/files/HolzapfelKS14.pdf",
        "abstract": "In this paper, we approach the tasks of beat tracking, down- beat recognition and rhythmic style classification in non- Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evalua- tion, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and down- beats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with com- parable results, providing a novel method for inferring the metrical structure in culturally diverse datasets.",
        "zenodo_id": 1415000,
        "dblp_key": "conf/ismir/HolzapfelKS14",
        "keywords": [
            "Bayesian model",
            "tempo inference",
            "downbeats recognition",
            "rhythmic style classification",
            "non-Western music",
            "adaptation to styles",
            "time signatures",
            "music corpus",
            "odd meter types",
            "state of the art"
        ]
    },
    {
        "title": "Detection of Motor Changes in Violin Playing by EMG Signals.",
        "author": [
            "Ling-Chi Hsu",
            "Yu-Lin Wang",
            "Yi-Ju Lin",
            "Cheryl D. Metcalf",
            "Alvin W. Y. Su"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416452",
        "url": "https://doi.org/10.5281/zenodo.1416452",
        "ee": "https://zenodo.org/records/1416452/files/HsuWLMS14.pdf",
        "abstract": "Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the infor- mation from audio signal is not enough to understand the sensorimotor programming in players. Recently, the in- vestigation of muscular activities of players during per- formance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultane- ously and also develop algorithms to analyze the music performance and discover its relation to player\u2019s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of vio- lin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular ac- tivities during violin playing. With the additional infor- mation of the audio signal, the proposed work could effi- ciently extract the period and detect the direction of mo- tor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance.",
        "zenodo_id": 1416452,
        "dblp_key": "conf/ismir/HsuWLMS14",
        "keywords": [
            "harmonious body movements",
            "motor sequences",
            "muscular activities",
            "EMG signal",
            "music performance",
            "sensorimotor programming",
            "violin bowing",
            "audio sounds",
            "features",
            "muscular activities"
        ]
    },
    {
        "title": "A Cross-Cultural Study on the Mood of K-POP Songs.",
        "author": [
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Kahyun Choi",
            "J. Stephen Downie"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417993",
        "url": "https://doi.org/10.5281/zenodo.1417993",
        "ee": "https://zenodo.org/records/1417993/files/HuLCD14.pdf",
        "abstract": "Prior research suggests that music mood is one of the most important criteria when people look for music-but the perception of mood may be subjective and can be in- fluenced by many factors including the listeners' cultural background. In recent years, the number of studies of mu- sic mood perceptions by various cultural groups and of automated mood classification of music from different cultures has been increasing. However, there has yet to be a well-established testbed for evaluating cross-cultural tasks in Music Information Retrieval (MIR). Moreover, most existing datasets in MIR consist mainly of Western music and the cultural backgrounds of the annotators were mostly not taken into consideration or were limited to one cultural group. In this study, we built a collection of 1,892 K-pop (Korean Pop) songs with mood annota- tions collected from both Korean and American listeners, based on three different mood models. We analyze the differences and similarities between the mood judgments of the two listener groups, and propose potential MIR tasks that can be evaluated on this dataset.",
        "zenodo_id": 1417993,
        "dblp_key": "conf/ismir/HuLCD14",
        "keywords": [
            "music mood",
            "subjective perception",
            "cultural background",
            "increasing studies",
            "MIR tasks",
            "cross-cultural evaluations",
            "K-pop songs",
            "mood annotations",
            "listener groups",
            "MIR tasks"
        ]
    },
    {
        "title": "Music Information Behaviors and System Preferences of University Students in Hong Kong.",
        "author": [
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Leanne Ka Yan Wong"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414778",
        "url": "https://doi.org/10.5281/zenodo.1414778",
        "ee": "https://zenodo.org/records/1414778/files/HuLW14.pdf",
        "abstract": "This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supple- mental interviews were conducted in order to investigate users\u2019 music information related activities. We found that university students in Hong Kong listened to music fre- quently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple",
        "zenodo_id": 1414778,
        "dblp_key": "conf/ismir/HuLW14",
        "keywords": [
            "quantitative",
            "qualitative",
            "user study",
            "music information needs",
            "university students",
            "Hong Kong",
            "music information related activities",
            "listening to music",
            "entertainment",
            "stress reduction"
        ]
    },
    {
        "title": "Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks.",
        "author": [
            "Po-Sen Huang",
            "Minje Kim",
            "Mark Hasegawa-Johnson",
            "Paris Smaragdis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415678",
        "url": "https://doi.org/10.5281/zenodo.1415678",
        "ee": "https://zenodo.org/records/1415678/files/HuangKHS14.pdf",
        "abstract": "Monaural source separation is important for many real world applications. It is challenging since only single chan- nel information is available. In this paper, we explore us- ing deep recurrent neural networks for singing voice sep- aration from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal con- nections are explored. We propose jointly optimizing the networks for multiple source signals by including the sepa- ration step as a nonlinear operation in the last layer. Differ- ent discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30\u223c2.48 dB GNSDR gain and 4.32\u223c5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset.",
        "zenodo_id": 1415678,
        "dblp_key": "conf/ismir/HuangKHS14",
        "keywords": [
            "monaural source separation",
            "real world applications",
            "challenging",
            "deep recurrent neural networks",
            "supervised setting",
            "singing voice separation",
            "single channel information",
            "joint optimization",
            "source to interference ratio",
            "state-of-the-art performance"
        ]
    },
    {
        "title": "JAMS: A JSON Annotated Music Specification for Reproducible MIR Research.",
        "author": [
            "Eric J. Humphrey",
            "Justin Salamon",
            "Oriol Nieto",
            "Jon Forsyth",
            "Rachel M. Bittner",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415924",
        "url": "https://doi.org/10.5281/zenodo.1415924",
        "ee": "https://zenodo.org/records/1415924/files/HumphreySNFBB14.pdf",
        "abstract": "The continued growth of MIR is motivating more com- plex annotation data, consisting of richer information, mul- tiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of address- ing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, com- prehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we pro- vide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and dis- cuss how now is a crucial time to make a concerted effort toward sustainable annotation standards.",
        "zenodo_id": 1415924,
        "dblp_key": "conf/ismir/HumphreySNFBB14",
        "keywords": [
            "MIR",
            "annotation data",
            "richer information",
            "multiple annotations",
            "music signal",
            "JSON-based format",
            "simpler",
            "structure",
            "sustainability",
            "future research"
        ]
    },
    {
        "title": "Geographical Region Mapping Scheme Based on Musical Preferences.",
        "author": [
            "Sanghoon Jun",
            "Seungmin Rho",
            "Eenjun Hwang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418011",
        "url": "https://doi.org/10.5281/zenodo.1418011",
        "ee": "https://zenodo.org/records/1418011/files/JunRH14.pdf",
        "abstract": "Many countries and cities in the world tend to have dif- ferent types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize ge- ographical proximity for evaluating the similarity of mu- sic preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as cul- ture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their popula- tions. That is, countries or cities in a traditional map are rearranged in the music map such that regions with simi- lar musical preferences are close to one another. To do this, we collect users\u2019 music play history and extract pop- ular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results.",
        "zenodo_id": 1418011,
        "dblp_key": "conf/ismir/JunRH14",
        "keywords": [
            "countries",
            "cities",
            "different types of preferred music",
            "geographical proximity",
            "music preferences",
            "musical preferences",
            "popularity",
            "musical preferences",
            "iterative algorithm",
            "regions"
        ]
    },
    {
        "title": "A Data Set for Computational Studies of Schenkerian Analysis.",
        "author": [
            "Phillip B. Kirlin"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417833",
        "url": "https://doi.org/10.5281/zenodo.1417833",
        "ee": "https://zenodo.org/records/1417833/files/Kirlin14.pdf",
        "abstract": "Schenkerian analysis, a kind of hierarchical music anal- ysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computa- tional studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those composi- tions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously eval- uate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first pub- licly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical se- lections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software.",
        "zenodo_id": 1417833,
        "dblp_key": "conf/ismir/Kirlin14",
        "keywords": [
            "Schenkerian analysis",
            "music analysis",
            "computational studies",
            "data sets",
            "ground-truth analyses",
            "publicly available",
            "large-scale data",
            "musical selections",
            "encoding",
            "software"
        ]
    },
    {
        "title": "Automated Detection of Single- and Multi-Note Ornaments in Irish Traditional Flute Playing.",
        "author": [
            "M\u00fcnevver K\u00f6k\u00fcer",
            "Peter Jancovic",
            "Islah Ali-MacLachlan",
            "Cham Athwal"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415266",
        "url": "https://doi.org/10.5281/zenodo.1415266",
        "ee": "https://zenodo.org/records/1415266/files/KokuerJAA14.pdf",
        "abstract": "This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented orna- ment detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamenta- tion. We employed onset detection methods based on sig- nal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by ad- jacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evalua- tions are performed on monophonic flute recordings from Grey Larsen\u2019s CD, which was manually annotated by an experienced flute player. The onset and single- and multi- note ornament detection performance is presented in terms of the precision, recall and F-measure.",
        "zenodo_id": 1415266,
        "dblp_key": "conf/ismir/KokuerJAA14",
        "keywords": [
            "automatic system",
            "single- and multi-note ornaments",
            "Irish traditional flute playing",
            "onset detection",
            "fundamental frequency",
            "musical ornamentation",
            "duration",
            "pitch",
            "experimental evaluations",
            "monophonic flute recordings"
        ]
    },
    {
        "title": "Probabilistic Extraction of Beat Positions from a Beat Activation Function.",
        "author": [
            "Filip Korzeniowski",
            "Sebastian B\u00f6ck",
            "Gerhard Widmer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415118",
        "url": "https://doi.org/10.5281/zenodo.1415118",
        "ee": "https://zenodo.org/records/1415118/files/KorzeniowskiBW14.pdf",
        "abstract": "We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker cur- rently uses for this purpose. Our experiments show im- provement upon the current method for a variety of data sets and quality measures, as well as better results com- pared to other state-of-the-art algorithms.",
        "zenodo_id": 1415118,
        "dblp_key": "conf/ismir/KorzeniowskiBW14",
        "keywords": [
            "probabilistic",
            "beat positions",
            "neural network",
            "greedy search",
            "replacement",
            "data sets",
            "quality measures",
            "improvement",
            "state-of-the-art",
            "algorithms"
        ]
    },
    {
        "title": "Cadence Detection in Western Traditional Stanzaic Songs using Melodic and Textual Features.",
        "author": [
            "Peter van Kranenburg",
            "Folgert Karsdorp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416172",
        "url": "https://doi.org/10.5281/zenodo.1416172",
        "ee": "https://zenodo.org/records/1416172/files/KranenburgK14.pdf",
        "abstract": "Many Western songs are hierarchically structured in stan- zas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdi- vided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour [11]. We employ a random forest classification algorithm. The pre- cision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An abla- tion study shows that none of the kinds of features is suffi- cient to account for good classification, while some of the groups perform moderately well on their own.",
        "zenodo_id": 1416172,
        "dblp_key": "conf/ismir/KranenburgK14",
        "keywords": [
            "cadential",
            "melodic",
            "lyrics",
            "stanza",
            "phrase",
            "symbolically",
            "folk",
            "trigram",
            "classification",
            "random forest"
        ]
    },
    {
        "title": "Computational Models for Perceived Melodic Similarity in A Cappella Flamenco Singing.",
        "author": [
            "Nadine Kroher",
            "Emilia G\u00f3mez",
            "Catherine Guastavino",
            "Francisco G\u00f3mez 0001",
            "Jordi Bonada"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416306",
        "url": "https://doi.org/10.5281/zenodo.1416306",
        "ee": "https://zenodo.org/records/1416306/files/KroherGGGB14.pdf",
        "abstract": "The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a com- mon melodic skeleton, which is subject to spontaneous im- provisation containing strong prolongations and ornamen- tations. For our research we collected human similarity judgements from na\u00a8\u0131ve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We further- more calculated distances from manually extracted high- level descriptors defined by flamenco experts. The suitabi- lity of a set of computational melodic similarity measures was evaluated by analyzing the correlation between com- puted similarity and human ratings. We observed signifi- cant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between hu- man ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and orna- mentation are better suited to model perceived similarity for this material than models based on chroma distance.",
        "zenodo_id": 1416306,
        "dblp_key": "conf/ismir/KroherGGGB14",
        "keywords": [
            "melodic similarity",
            "cappella flamenco singing",
            "flamenco songs",
            "spontaneous improvisation",
            "high-level descriptors",
            "computational melodic similarity measures",
            "correlation between human ratings",
            "flamenco experts",
            "temporal deviation",
            "dynamics"
        ]
    },
    {
        "title": "Keyword Spotting in A-capella Singing.",
        "author": [
            "Anna M. Kruspe"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416870",
        "url": "https://doi.org/10.5281/zenodo.1416870",
        "ee": "https://zenodo.org/records/1416870/files/Kruspe14.pdf",
        "abstract": "Keyword spotting (or spoken term detection) is an inter- esting task in Music Information Retrieval that can be ap- plied to a number of problems. Its purposes include topi- cal search and improvements for genre classification. Key- word spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher vari- ations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel- Frequency Cepstral Coefficents (MFCCs), Perceptual Lin- ear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we aver- age the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classi- fier.",
        "zenodo_id": 1416870,
        "dblp_key": "conf/ismir/Kruspe14"
    },
    {
        "title": "Automatic Melody Transcription based on Chord Transcription.",
        "author": [
            "Antti Laaksonen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415218",
        "url": "https://doi.org/10.5281/zenodo.1415218",
        "ee": "https://zenodo.org/records/1415218/files/Laaksonen14.pdf",
        "abstract": "This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord tran- scription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To eval- uate the algorithm, we present a new ground truth dataset that consists of 1,5 hours of audio excerpts together with hand-made melody and chord transcriptions.",
        "zenodo_id": 1415218,
        "dblp_key": "conf/ismir/Laaksonen14",
        "keywords": [
            "automatic",
            "melody",
            "transcription",
            "chord",
            "transcription",
            "audio",
            "segments",
            "melody",
            "patterns",
            "harmony"
        ]
    },
    {
        "title": "Automatic Key Partition Based on Tonal Organization Information of Classical Music.",
        "author": [
            "Wang-Kong Lam",
            "Tan Lee 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414748",
        "url": "https://doi.org/10.5281/zenodo.1414748",
        "ee": "https://zenodo.org/records/1414748/files/LamL14.pdf",
        "abstract": "Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we de- scribe a generative account of chord progression consist- ing of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is com- pared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental re- sult shows that our system is better in terms of key detec- tion accuracy.",
        "zenodo_id": 1414748,
        "dblp_key": "conf/ismir/LamL14",
        "keywords": [
            "key information",
            "tonal music analysis",
            "chord progressions",
            "specific structures and rules",
            "phrase-structure grammar rules",
            "modulation",
            "musically sensible key partition",
            "classical music excerpts",
            "Kostka-Payne corpus",
            "key detection accuracy"
        ]
    },
    {
        "title": "Improving Music Recommender Systems: What Can We Learn from Research on Music Tastes?",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417797",
        "url": "https://doi.org/10.5281/zenodo.1417797",
        "ee": "https://zenodo.org/records/1417797/files/Laplante14.pdf",
        "abstract": "The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such pre- dictions are difficult to make accurately due to the com- plex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and so- ciology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations be- tween music preferences and a wide variety of sociodem- ographic and individual characteristics, including person- ality traits, values, ethnicity, gender, social class, and po- litical orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review.",
        "zenodo_id": 1417797,
        "dblp_key": "conf/ismir/Laplante14",
        "keywords": [
            "music tastes",
            "pre- dictions",
            "complex nature",
            "correlates",
            "music preferences",
            "sociodemographic",
            "individual characteristics",
            "social influences",
            "personal and social identities",
            "music recommender systems"
        ]
    },
    {
        "title": "In-depth Motivic Analysis based on Multiparametric Closed Pattern and Cyclic Sequence Mining.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418345",
        "url": "https://doi.org/10.5281/zenodo.1418345",
        "ee": "https://zenodo.org/records/1418345/files/Lartillot14.pdf",
        "abstract": "The paper describes a computational system for exhaus- tive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclic- ity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for auto- mated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground- truth motives are detected, while additional relevant infor- mation completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source frame- work for audio and music analysis.",
        "zenodo_id": 1418345,
        "dblp_key": "conf/ismir/Lartillot14",
        "keywords": [
            "computational system",
            "exhaustive but compact description",
            "motivic patterns",
            "symbolic representations",
            "closed heterogeneous pattern mining",
            "multiparametric space",
            "control of pattern cyclic-ity",
            "new method for auto-automated bundling",
            "ground-truth motives",
            "additional relevant information"
        ]
    },
    {
        "title": "Codebook-based Scalable Music Tagging with Poisson Matrix Factorization.",
        "author": [
            "Dawen Liang",
            "John W. Paisley",
            "Dan Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416120",
        "url": "https://doi.org/10.5281/zenodo.1416120",
        "ee": "https://zenodo.org/records/1416120/files/LiangPE14.pdf",
        "abstract": "Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a \u201cbag-of-tags\u201d representation. This ap- proach exploits the shared latent structure between seman- tic tags and acoustic codewords. Leveraging the recently- developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used.",
        "zenodo_id": 1416120,
        "dblp_key": "conf/ismir/LiangPE14",
        "keywords": [
            "Automatic",
            "music",
            "tagging",
            "MIR",
            "matrix",
            "completion",
            "Poisson",
            "factorization",
            "latent",
            "structure"
        ]
    },
    {
        "title": "Modeling Temporal Structure in Music for Emotion Prediction using Pairwise Comparisons.",
        "author": [
            "Jens Madsen",
            "Bj\u00f8rn Sand Jensen",
            "Jan Larsen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416384",
        "url": "https://doi.org/10.5281/zenodo.1416384",
        "ee": "https://zenodo.org/records/1416384/files/MadsenJL14.pdf",
        "abstract": "The temporal structure of music is essential for the cogni- tive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of mu- sic such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal infor- mation is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel pro- cessing pipeline: 1) Extracting audio features for each track resulting in a multivariate \u201dfeature time series\u201d. 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autore- gressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis.",
        "zenodo_id": 1416384,
        "dblp_key": "conf/ismir/MadsenJL14",
        "keywords": [
            "temporal",
            "structure",
            "music",
            "cognitive",
            "processes",
            "emotions",
            "expressed",
            "music",
            "predicting",
            "higher-level"
        ]
    },
    {
        "title": "Bayesian Audio Alignment based on a Unified Model of Music Composition and Performance.",
        "author": [
            "Akira Maezawa",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii",
            "Hiroshi G. Okuno"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415594",
        "url": "https://doi.org/10.5281/zenodo.1415594",
        "ee": "https://zenodo.org/records/1415594/files/MaezawaIYO14.pdf",
        "abstract": "This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Con- ventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic sim- ilarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each per- formance. To solve this problem, our model explicitly rep- resents two state sequences: a top-level sequence that de- termines the common structure inherent in the music it- self and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two se- quences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given perfor- mances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different perfor- mances of the same piece by analyzing the bottom-level se- quences. Experimental evaluation showed that our method outperformed the conventional methods.",
        "zenodo_id": 1415594,
        "dblp_key": "conf/ismir/MaezawaIYO14",
        "keywords": [
            "probabilistic",
            "model",
            "aligns",
            "multiple",
            "performances",
            "music",
            "dynamic",
            "time",
            "warping",
            "left-to-right"
        ]
    },
    {
        "title": "An Analysis and Evaluation of Audio Features for Multitrack Music Mixtures.",
        "author": [
            "Brecht De Man",
            "Brett Leonard",
            "Richard L. King",
            "Joshua D. Reiss"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416832",
        "url": "https://doi.org/10.5281/zenodo.1416832",
        "ee": "https://zenodo.org/records/1416832/files/ManLKR14.pdf",
        "abstract": "Mixing multitrack music is an expert task where charac- teristics of the individual elements and their sum are ma- nipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight dif- ferent engineers. We consider a range of features describ- ing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automa- tion of various mixing processes are explored. Part of the data used in this work is published in a new online mul- titrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared.",
        "zenodo_id": 1416832,
        "dblp_key": "conf/ismir/ManLKR14",
        "keywords": [
            "mixing multitrack music",
            "expert task",
            "characteristics manipulation",
            "technical issues resolution",
            "creative vision",
            "engineers contribution",
            "dynamic characteristics",
            "spatial characteristics",
            "spectral characteristics",
            "multidimensional analysis"
        ]
    },
    {
        "title": "Systematic Multi-scale Set-class Analysis.",
        "author": [
            "Agust\u00edn Martorell",
            "Emilia G\u00f3mez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417595",
        "url": "https://doi.org/10.5281/zenodo.1417595",
        "ee": "https://zenodo.org/records/1417595/files/MartorellG14.pdf",
        "abstract": "This work reviews and elaborates a methodology for hi- erarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and rep- resentation of Sapp\u2019s \u2018keyscapes\u2019 to the description stage, by introducing a set-class level of description. This pro- vides a systematic, mid-level, and standard analytical lex- icon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time se- ries encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informative- ness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal de- scriptors, and music information retrieval applications.",
        "zenodo_id": 1417595,
        "dblp_key": "conf/ismir/MartorellG14",
        "keywords": [
            "hierarchical",
            "multi-scale",
            "set-class",
            "analysis",
            "music",
            "pieces",
            "keyscapes",
            "representation",
            "description",
            "standard"
        ]
    },
    {
        "title": "Spotting a Query Phrase from Polyphonic Music Audio Signals Based on Semi-supervised Nonnegative Matrix Factorization.",
        "author": [
            "Taro Masuda",
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Shigeo Morishima"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415780",
        "url": "https://doi.org/10.5281/zenodo.1415780",
        "ee": "https://zenodo.org/records/1415780/files/MasudaYGM14.pdf",
        "abstract": "This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The \u201cphrase\u201d in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals con- sisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the dis- tance between a query and partial components of a musi- cal piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The ex- perimental results showed that our method outperformed conventional matching methods.",
        "zenodo_id": 1415780,
        "dblp_key": "conf/ismir/MasudaYGM14",
        "keywords": [
            "query-by-audio",
            "detect temporal locations",
            "musical phrase",
            "query",
            "mixture signals",
            "gamma process nonnegative matrix factorization",
            "spectrogram",
            "basis spectra",
            "activation patterns",
            "distance calculation"
        ]
    },
    {
        "title": "Analyzing Song Structure with Spectral Clustering.",
        "author": [
            "Brian McFee",
            "Dan Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415778",
        "url": "https://doi.org/10.5281/zenodo.1415778",
        "ee": "https://zenodo.org/records/1415778/files/McFeeE14.pdf",
        "abstract": "Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self- similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encod- ing of repetition structure, and exposes the hierarchical re- lationships among structural components at differing lev- els of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation.",
        "zenodo_id": 1415778,
        "dblp_key": "conf/ismir/McFeeE14",
        "keywords": [
            "self-similarity matrix",
            "time-series features",
            "spectral graph theory",
            "repeated patterns",
            "building blocks",
            "large-scale structure",
            "low-dimensional encoding",
            "structural components",
            "hierarchical relationships",
            "music segmentation"
        ]
    },
    {
        "title": "Audio-to-score Alignment at the Note Level for Orchestral Recordings.",
        "author": [
            "Marius Miron",
            "Julio Jos\u00e9 Carabias-Orti",
            "Jordi Janer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416150",
        "url": "https://doi.org/10.5281/zenodo.1416150",
        "ee": "https://zenodo.org/records/1416150/files/MironCJ14.pdf",
        "abstract": "In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise align- ment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and off- sets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best com- bination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets.",
        "zenodo_id": 1416150,
        "dblp_key": "conf/ismir/MironCJ14"
    },
    {
        "title": "Evaluation Framework for Automatic Singing Transcription.",
        "author": [
            "Emilio Molina",
            "Ana M. Barbancho",
            "Lorenzo J. Tard\u00f3n",
            "Isabel Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417729",
        "url": "https://doi.org/10.5281/zenodo.1417729",
        "ee": "https://zenodo.org/records/1417729/files/MolinaBTB14.pdf",
        "abstract": "In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors ma- de by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers.",
        "zenodo_id": 1417729,
        "dblp_key": "conf/ismir/MolinaBTB14",
        "keywords": [
            "evaluation strategies",
            "automatic singing transcription",
            "novel evaluation framework",
            "cross-annotated dataset",
            "extended evaluation measures",
            "Matlab toolbox",
            "standard MIREX note-tracking measures",
            "extra information about errors",
            "comparison of singing transcribers",
            "practical case of use"
        ]
    },
    {
        "title": "The Importance of F0 Tracking in Query-by-singing-humming.",
        "author": [
            "Emilio Molina",
            "Lorenzo J. Tard\u00f3n",
            "Isabel Barbancho",
            "Ana M. Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416818",
        "url": "https://doi.org/10.5281/zenodo.1416818",
        "ee": "https://zenodo.org/records/1416818/files/MolinaTBB14.pdf",
        "abstract": "In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query- by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evalu- ation, we measured the QBSH performance for 189 differ- ent combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In ad- dition, we also found clear differences in robustness to F0 transcription errors between different matchers.",
        "zenodo_id": 1416818,
        "dblp_key": "conf/ismir/MolinaTBB14",
        "keywords": [
            "F0 trackers",
            "query-by-singing-humming",
            "MIR-QBSH dataset",
            "pub-style noise",
            "smartphone-style distortion",
            "audio-to-MIDI melodic matching",
            "state-of-the-art systems",
            "simple baseline method",
            "QBSH performance",
            "robustness to F0 transcription errors"
        ]
    },
    {
        "title": "Taste Space Versus the World: an Embedding Analysis of Listening Habits and Geography.",
        "author": [
            "Joshua L. Moore",
            "Thorsten Joachims",
            "Douglas R. Turnbull"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415178",
        "url": "https://doi.org/10.5281/zenodo.1415178",
        "ee": "https://zenodo.org/records/1415178/files/MooreJT14.pdf",
        "abstract": "Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the \u201ctaste space\u201d in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In par- ticular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city- artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location.",
        "zenodo_id": 1415178,
        "dblp_key": "conf/ismir/MooreJT14",
        "keywords": [
            "Probabilistic embedding methods",
            "discrete objects",
            "human interaction data",
            "compact and accurate predictive model",
            "compact and flexible representation",
            "taste space",
            "Million Musical Tweets Dataset",
            "cultural and geographical differences",
            "listening patterns",
            "spatial representations"
        ]
    },
    {
        "title": "A Combined Thematic and Acoustic Approach for a Music Recommendation Service in TV Commercials.",
        "author": [
            "Mohamed Morchid",
            "Richard Dufour",
            "Georges Linar\u00e8s"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415098",
        "url": "https://doi.org/10.5281/zenodo.1415098",
        "ee": "https://zenodo.org/records/1415098/files/MorchidDL14.pdf",
        "abstract": "We hypothesize that different genres of writing use dif- ferent adjectives for the same concept. We test our hy- pothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four lead- ing newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popu- lar English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of ad- jectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these proba- bility distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more like- ly to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry.",
        "zenodo_id": 1415098,
        "dblp_key": "conf/ismir/MorchidDL14",
        "keywords": [
            "hypothesis",
            "genres",
            "different adjectives",
            "English Wikipedia",
            "news articles",
            "articles",
            "poetry",
            "probability distribution",
            "adjectives",
            "document classification"
        ]
    },
    {
        "title": "Merged-Output HMM for Piano Fingering of Both Hands.",
        "author": [
            "Eita Nakamura",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415152",
        "url": "https://doi.org/10.5281/zenodo.1415152",
        "ee": "https://zenodo.org/records/1415152/files/NakamuraOS14.pdf",
        "abstract": "This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of finger- ing for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the finger- ing model is also proposed and yields reasonable results.",
        "zenodo_id": 1415152,
        "dblp_key": "conf/ismir/NakamuraOS14",
        "keywords": [
            "piano fingering model",
            "ensemble scores",
            "quantifying difficulty",
            "automating piano reduction",
            "voice part separation",
            "merged-output hidden Markov model",
            "difficulty of performance",
            "hand application",
            "automation of decision",
            "previous difficulty"
        ]
    },
    {
        "title": "Harmonic-Temporal Factor Decomposition Incorporating Music Prior Information for Informed Monaural Source Separation.",
        "author": [
            "Tomohiko Nakamura",
            "Kotaro Shikata",
            "Norihiro Takamune",
            "Hirokazu Kameoka"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417463",
        "url": "https://doi.org/10.5281/zenodo.1417463",
        "ee": "https://zenodo.org/records/1417463/files/NakamuraSTK14.pdf",
        "abstract": "For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative ma- trix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA- based approach called the \u201charmonic-temporal clustering (HTC)\u201d aims to cluster the time-frequency components of an observed signal based on a constraint designed ac- cording to the local time-frequency structure common in many sound sources (such as harmonicity and the conti- nuity of frequency and amplitude modulations). This pa- per proposes a new approach for monaural source sepa- ration called the \u201cHarmonic-Temporal Factor Decompo- sition (HTFD)\u201d by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separa- tion scheme.",
        "zenodo_id": 1417463,
        "dblp_key": "conf/ismir/NakamuraSTK14",
        "keywords": [
            "non-negative matrix factorization (NMF)",
            "computational auditory scene analysis (CASA)",
            "harmonic-temporal clustering (HTC)",
            "harmonic-temporal factor decomposition (HTFD)",
            "spectrogram model",
            "musically relevant information",
            "separation scheme",
            "local time-frequency structure",
            "modulations",
            "source separation"
        ]
    },
    {
        "title": "Identifying Polyphonic Musical Patterns From Audio Recordings Using Music Segmentation Techniques.",
        "author": [
            "Oriol Nieto",
            "Morwaread Mary Farbood"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417259",
        "url": "https://doi.org/10.5281/zenodo.1417259",
        "ee": "https://zenodo.org/records/1417259/files/NietoF14.pdf",
        "abstract": "This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may con- tain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmenta- tion, which exploits repetitive features in order to auto- matically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the re- cently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics.",
        "zenodo_id": 1417259,
        "dblp_key": "conf/ismir/NietoF14",
        "keywords": [
            "patterns",
            "note collections",
            "repeatedly occur",
            "musical work",
            "harmony",
            "timbre",
            "rhythm",
            "algorithm",
            "music segmentation",
            "audio recordings"
        ]
    },
    {
        "title": "Perceptual Analysis of the F-Measure to Evaluate Section Boundaries in Music.",
        "author": [
            "Oriol Nieto",
            "Morwaread Mary Farbood",
            "Tristan Jehan",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414958",
        "url": "https://doi.org/10.5281/zenodo.1414958",
        "ee": "https://zenodo.org/records/1414958/files/NietoFJB14.pdf",
        "abstract": "In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the bound- aries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F- measure rather than the recall component, therefore mak- ing the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experi- ments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over re- call, making the section boundary evaluation more expres- sive and reliable.",
        "zenodo_id": 1414958,
        "dblp_key": "conf/ismir/NietoFJB14",
        "keywords": [
            "limitations",
            "F-measure",
            "evaluation",
            "quality",
            "boundaries",
            "automatic segmentation",
            "music",
            "subjects",
            "rating",
            "perceptually informative"
        ]
    },
    {
        "title": "Transfer Learning by Supervised Pre-training for Audio-based Music Classification.",
        "author": [
            "A\u00e4ron van den Oord",
            "Sander Dieleman",
            "Benjamin Schrauwen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415890",
        "url": "https://doi.org/10.5281/zenodo.1415890",
        "ee": "https://zenodo.org/records/1415890/files/OordDS14.pdf",
        "abstract": "Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, be- cause the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classifica- tion tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning ap- proach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We eval- uate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets.",
        "zenodo_id": 1415890,
        "dblp_key": "conf/ismir/OordDS14",
        "keywords": [
            "publicly available",
            "increasing need",
            "digital distribution",
            "efficient cataloging",
            "easily browsable",
            "deep learning",
            "feature learning",
            "music information retrieval",
            "large amounts of training data",
            "supervised pre-training"
        ]
    },
    {
        "title": "Modeling Rhythm Similarity for Electronic Dance Music.",
        "author": [
            "Maria Panteli",
            "Niels Bogaards",
            "Aline K. Honingh"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416664",
        "url": "https://doi.org/10.5281/zenodo.1416664",
        "ee": "https://zenodo.org/records/1416664/files/PanteliBH14.pdf",
        "abstract": "A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a \u2018loop\u2019, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between seg- ments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most no- tably: attack phase of onsets, periodicity of rhythmic el- ements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, af- ter which the similarity between segments can be calcu- lated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the over- all performance of the model with perceptual ratings of rhythm similarity.",
        "zenodo_id": 1416664,
        "dblp_key": "conf/ismir/PanteliBH14",
        "keywords": [
            "model",
            "rhythm similarity",
            "electronic dance music",
            "loop",
            "perceptual rhythmic streams",
            "onset detection",
            "downbeat detection",
            "feature vector",
            "listening experiment",
            "perceptual ratings"
        ]
    },
    {
        "title": "Improving Music Structure Segmentation using lag-priors.",
        "author": [
            "Geoffroy Peeters",
            "Victor Bisot"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414764",
        "url": "https://doi.org/10.5281/zenodo.1414764",
        "ee": "https://zenodo.org/records/1414764/files/PeetersB14.pdf",
        "abstract": "Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the sig- nal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. pro- posed in 2012 to cumulate these simultaneous values (us- ing a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test- sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large im- provement of the segmentation results.",
        "zenodo_id": 1414764,
        "dblp_key": "conf/ismir/PeetersB14",
        "keywords": [
            "repetitive content",
            "Self-Similarity-Matrix (SSM)",
            "sub-diagonals",
            "cumulate values",
            "constant-lag",
            "structure feature",
            "novelty curve",
            "MIREX segmentation evaluation measure",
            "large improvement",
            "combination approach"
        ]
    },
    {
        "title": "Introducing a Dataset of Emotional and Color Responses to Music.",
        "author": [
            "Matevz Pesek",
            "Primoz Godec",
            "Mojca Poredos",
            "Gregor Strle",
            "Joze Guna",
            "Emilija Stojmenova",
            "Matevz Pogacnik",
            "Matija Marolt"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415926",
        "url": "https://doi.org/10.5281/zenodo.1415926",
        "ee": "https://zenodo.org/records/1415926/files/PesekGPSGSPM14.pdf",
        "abstract": "The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gath-ering user responses is described along with two new inter-faces for capturing emotional states: the MoodGraph and Mood- Stripe. An evaluation study showed both inter-faces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants\u2019 emotional states and color associations, as well as relationships be- tween musically perceived and induced emotions. We be- lieve the size of the dataset, in-terfaces and multi-modal approach (connecting emo-tional, visual and auditory as- pects of human perception) give a valuable contribution to current research.",
        "zenodo_id": 1415926,
        "dblp_key": "conf/ismir/PesekGPSGSPM14",
        "keywords": [
            "new",
            "dataset",
            "mood-dependent",
            "color",
            "responses",
            "music",
            "methodology",
            "user",
            "responses",
            "interfaces"
        ]
    },
    {
        "title": "A Compositional Hierarchical Model for Music Information Retrieval.",
        "author": [
            "Matevz Pesek",
            "Ales Leonardis",
            "Matija Marolt"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416084",
        "url": "https://doi.org/10.5281/zenodo.1416084",
        "ee": "https://zenodo.org/records/1416084/files/PesekLM14.pdf",
        "abstract": "This paper presents a biologically-inspired composit- ional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear in- sight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower- level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by- one from music signals. Parts in each layer are compo- sitions of parts from previous layers based on statistical co-occurrences as the driving force of the learning pro- cess. We present the model\u2019s structure and compare it to other deep architectures. A preliminary evaluation of the model\u2019s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is pro- vided. Additionally, we show how the model can be ex- tended to event-based music processing, which is our final goal.",
        "zenodo_id": 1416084,
        "dblp_key": "conf/ismir/PesekLM14",
        "keywords": [
            "biologically-inspired",
            "compositional hierarchical",
            "MIR model",
            "deep learning",
            "alternative to neural networks",
            "generativeness",
            "transparency",
            "music signals",
            "multiple layers",
            "statistical co-occurrences"
        ]
    },
    {
        "title": "Frame-Level Audio Segmentation for Abridged Musical Works.",
        "author": [
            "Thomas Pr\u00e4tzlich",
            "Meinard M\u00fcller"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418039",
        "url": "https://doi.org/10.5281/zenodo.1418039",
        "ee": "https://zenodo.org/records/1418039/files/PratzlichM14.pdf",
        "abstract": "Large-scale musical works such as operas may last sev- eral hours and typically involve a huge number of mu- sicians. For such compositions, one often finds differ- ent arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller en- sembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automati- cally segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the un- known version. Considering the opera \u201cDer Freisch\u00a8utz\u201d as an example scenario, we discuss how to balance out flex- ibility and robustness properties of our proposed frame- level segmentation procedure.",
        "zenodo_id": 1418039,
        "dblp_key": "conf/ismir/PratzlichM14",
        "keywords": [
            "audio recording",
            "semantically meaningful parts",
            "automatically segmenting",
            "reference segmentation",
            "abridged versions",
            "strongly abridged",
            "frame-level matching",
            "opera Der Freisch\u00fctz",
            "flexibility",
            "robustness"
        ]
    },
    {
        "title": "MuSe: A Music Recommendation Management System.",
        "author": [
            "Martin Przyjaciel-Zablocki",
            "Thomas Hornung 0001",
            "Alexander Sch\u00e4tzle",
            "Sven Gau\u00df",
            "Io Taxidou",
            "Georg Lausen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418195",
        "url": "https://doi.org/10.5281/zenodo.1418195",
        "ee": "https://zenodo.org/records/1418195/files/Przyjaciel-ZablockiHSGTL14.pdf",
        "abstract": "Evaluating music recommender systems is a highly repet- itive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE \u2013 a music recommen- dation management system \u2013 for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off- the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating on- line streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender al- gorithms can be plugged in comfortably and evaluations can be configured and managed online.",
        "zenodo_id": 1418195,
        "dblp_key": "conf/ismir/Przyjaciel-ZablockiHSGTL14",
        "keywords": [
            "Evaluating music recommender systems",
            "repetitive yet non-trivial task",
            "immediate evaluation by listening",
            "MUSE system",
            "off-the-shelf evaluation algorithms",
            "online evaluation system",
            "automatic reporting",
            "legal evaluation in real time",
            "built-in user management system",
            "privacy standards"
        ]
    },
    {
        "title": "Verovio: A library for Engraving MEI Music Notation into SVG.",
        "author": [
            "Laurent Pugin",
            "Rodolfo Zitellini",
            "Perry Roland"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417589",
        "url": "https://doi.org/10.5281/zenodo.1417589",
        "ee": "https://zenodo.org/records/1417589/files/PuginZR14.pdf",
        "abstract": "Rendering symbolic music notation is a common compo- nent of many MIR applications, and many tools are avail- able for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a li- brary and toolkit for rendering MEI. A significant ad- vantage of Verovio is that it implements MEI\u2019s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last op- tion is particularly interesting because it provides a com- plete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily address- able, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license.",
        "zenodo_id": 1417589,
        "dblp_key": "conf/ismir/PuginZR14",
        "keywords": [
            "MEI notation",
            "Music Encoding Initiative",
            "Verovio",
            "native rendering",
            "Music research projects",
            "internal structure",
            "command-line tool",
            "library",
            "lightweight tool",
            "Emscripten LLVM-to-JavaScript"
        ]
    },
    {
        "title": "MIR_EVAL: A Transparent Implementation of Common MIR Metrics.",
        "author": [
            "Colin Raffel",
            "Brian McFee",
            "Eric J. Humphrey",
            "Justin Salamon",
            "Oriol Nieto",
            "Dawen Liang",
            "Daniel P. W. Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416528",
        "url": "https://doi.org/10.5281/zenodo.1416528",
        "ee": "https://zenodo.org/records/1416528/files/RaffelMHSNLE14.pdf",
        "abstract": "Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval\u2019s architecture, design, and intended use.",
        "zenodo_id": 1416528,
        "dblp_key": "conf/ismir/RaffelMHSNLE14",
        "keywords": [
            "evaluation",
            "algorithms",
            "music",
            "data",
            "MIR",
            "software",
            "library",
            "metrics",
            "performance",
            "MIR algorithms"
        ]
    },
    {
        "title": "Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis.",
        "author": [
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416030",
        "url": "https://doi.org/10.5281/zenodo.1416030",
        "ee": "https://zenodo.org/records/1416030/files/RepettoS14.pdf",
        "abstract": "Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are in- timately combined. As an oral tradition, its musical di- mension is the result of the application of a series of pre- defined conventions and it offers unique concepts for mu- sicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, edito- rial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the con- tent of the corpus, and evaluate its suitability for compu- tational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, espe- cially for melodic analysis, and suggest approaches for future work.",
        "zenodo_id": 1416030,
        "dblp_key": "conf/ismir/RepettoS14",
        "keywords": [
            "Jingju",
            "Chinese traditional performing art",
            "Oral tradition",
            "Musical dimension",
            "Pre-defined conventions",
            "Musicological research",
            "Computational analyses",
            "CompMusic project",
            "Corpus of jingju music",
            "Acquisition criteria"
        ]
    },
    {
        "title": "Multi-Strategy Segmentation of Melodies.",
        "author": [
            "Marcelo Enrique Rodr\u00edguez-L\u00f3pez",
            "Anja Volk",
            "Dimitrios Bountouridis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418013",
        "url": "https://doi.org/10.5281/zenodo.1418013",
        "ee": "https://zenodo.org/records/1418013/files/Rodriguez-LopezVB14.pdf",
        "abstract": "Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a \u2018single strategy\u2019 (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and eval- uate a \u2018multi-strategy\u2019 system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a gi- ven boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We com- pare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the in- strumental set.",
        "zenodo_id": 1418013,
        "dblp_key": "conf/ismir/Rodriguez-LopezVB14",
        "keywords": [
            "melodic segmentation",
            "automatic music processing",
            "single strategy",
            "multiple cues",
            "symbolically encoded melodies",
            "multi-strategy system",
            "perceptual relevance",
            "boundary detection models",
            "plausible segmentation",
            "corpus of instrumental and vocal folk melodies"
        ]
    },
    {
        "title": "LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics.",
        "author": [
            "Shoto Sasaki",
            "Kazuyoshi Yoshii",
            "Tomoyasu Nakano",
            "Masataka Goto",
            "Shigeo Morishima"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418075",
        "url": "https://doi.org/10.5281/zenodo.1418075",
        "ee": "https://zenodo.org/records/1418075/files/SasakiYNGM14.pdf",
        "abstract": "This paper presents a lyrics retrieval system called Lyric- sRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user\u2019s intention behind a query when a word given as a query can be used in different con- texts. For example, the word\u201ctears\u201dcan appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we pro- pose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent top- ics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are ar- ranged close to each other. The subjective experiments us- ing 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests.",
        "zenodo_id": 1418075,
        "dblp_key": "conf/ismir/SasakiYNGM14",
        "keywords": [
            "lyrics retrieval system",
            "interactive browsing",
            "visualizing topics",
            "latent Dirichlet allocation",
            "topic radar chart",
            "topic visualization",
            "two-dimensional space",
            "song lyrics",
            "similar topics",
            "user intention"
        ]
    },
    {
        "title": "On The Changing Regulations of Privacy and Personal Information in MIR.",
        "author": [
            "Pierre Saurel",
            "Francis Rousseaux",
            "Marc Danger"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416638",
        "url": "https://doi.org/10.5281/zenodo.1416638",
        "ee": "https://zenodo.org/records/1416638/files/SaurelRD14.pdf",
        "abstract": "In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regula- tions to take effect in the coming years regarding how such information can be collected, stored and manipulat- ed, with equally strict penalties for being found in viola- tion of the law. Here, we provide a summary of these changes, consid- er how they relate to our data sources and research prac- tices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such chang- es might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our per- sonal legal interpretations are strictly mentioned for illus- tration purpose, and reader should seek proper legal counsel.",
        "zenodo_id": 1416638,
        "dblp_key": "conf/ismir/SaurelRD14",
        "keywords": [
            "user feedback",
            "human subjects data",
            "personal information",
            "European Union regulations",
            "new strict regulations",
            "data collection",
            "data storage",
            "data manipulation",
            "legal penalties",
            "subject-friendly research"
        ]
    },
    {
        "title": "Music Analysis as a Smallest Grammar Problem.",
        "author": [
            "Kirill A. Sidorov",
            "Andrew Jones",
            "A. David Marshall"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417653",
        "url": "https://doi.org/10.5281/zenodo.1417653",
        "ee": "https://zenodo.org/records/1417653/files/SidorovJM14.pdf",
        "abstract": "In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explain- ing a musical work\u2019s structure. The proposed method is predicated on the hypothesis that the shortest possible gram- mar provides a model of the musical structure which is a good representation of the composer\u2019s intent. The effec- tiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our ap- proach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection.",
        "zenodo_id": 1417653,
        "dblp_key": "conf/ismir/SidorovJM14",
        "keywords": [
            "automatically generated grammar",
            "explaining musical works structure",
            "composers intent",
            "effectiveness comparison",
            "error detection",
            "intuitive editing",
            "creative transformation",
            "automatic summarization",
            "musical complexity estimation",
            "plagiarism detection"
        ]
    },
    {
        "title": "An RNN-based Music Language Model for Improving Automatic Music Transcription.",
        "author": [
            "Siddharth Sigtia",
            "Emmanouil Benetos",
            "Srikanth Cherla",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez",
            "Simon Dixon"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416792",
        "url": "https://doi.org/10.5281/zenodo.1416792",
        "ee": "https://zenodo.org/records/1416792/files/SigtiaBCWGD14.pdf",
        "abstract": "In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcrip- tion performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal struc- ture present in symbolic music data. Similar to the func- tion of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the oc- currence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior infor- mation from the MLM is incorporated into the transcrip- tion framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic mu- sic and report a significant 3% improvement in terms of F- measure, when compared to using an acoustic-only model.",
        "zenodo_id": 1416792,
        "dblp_key": "conf/ismir/SigtiaBCWGD14",
        "keywords": [
            "Music Language Models",
            "Automatic Music Transcription",
            "Nottingham dataset",
            "Recurrent Neural Network",
            "probabilistic latent component analysis",
            "Dirichlet priors",
            "F-measure",
            "symbolic polyphonic music",
            "acoustic-only model",
            "3% improvement"
        ]
    },
    {
        "title": "Music Classification by Transductive Learning Using Bipartite Heterogeneous Networks.",
        "author": [
            "Diego Furtado Silva",
            "Rafael Geraldeli Rossi",
            "Solange Oliveira Rezende",
            "Gustavo Enrique De Almeida Prado Alves Batista"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418265",
        "url": "https://doi.org/10.5281/zenodo.1418265",
        "ee": "https://zenodo.org/records/1418265/files/SilvaRRB14.pdf",
        "abstract": "The popularization of music distribution in electronic for- mat has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled in- stances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous net- works have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they oc- cur. Thus, there is no parameter or additional costs to gen- erate such networks. In this paper, we propose the use of the bipartite network representation to perform trans- ductive classification of music, using a bag-of-frames ap- proach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available.",
        "zenodo_id": 1418265,
        "dblp_key": "conf/ismir/SilvaRRB14",
        "keywords": [
            "music distribution",
            "incomplete metadata",
            "music recommendation",
            "transductive classification",
            "label propagation",
            "networks",
            "data representation",
            "parameters",
            "classification accuracy",
            "bipartite networks"
        ]
    },
    {
        "title": "On Cultural, Textual and Experiential Aspects of Music Mood.",
        "author": [
            "Abhishek Singhi",
            "Daniel G. Brown 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417391",
        "url": "https://doi.org/10.5281/zenodo.1417391",
        "ee": "https://zenodo.org/records/1417391/files/SinghiB14.pdf",
        "abstract": "We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years.  While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hear- ing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music.  We conclude that people as- sign different moods to the same song in these three sce- narios. People tend to assign a song to the mood cluster that includes \u201cmelancholy\u201d more often when they read the lyrics without listening to it, and having access to the lyr- ics does not help reduce the difference in music mood perception between Canadian and Chinese listeners sig- nificantly. Our results cause us to question the idea that songs have \u201cinherent mood\u201d. Rather, we suggest that the mood depends on both cultural and experiential context.",
        "zenodo_id": 1417391,
        "dblp_key": "conf/ismir/SinghiB14",
        "keywords": [
            "music mood perception",
            "Canadian listeners",
            "Chinese-Canadians",
            "Chinese people",
            "user study",
            "lyrics presence",
            "mood assignments",
            "stable mood assignments",
            "recent popular English songs",
            "access to lyrics"
        ]
    },
    {
        "title": "Are Poetry and Lyrics All That Different?",
        "author": [
            "Abhishek Singhi",
            "Daniel G. Brown 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417004",
        "url": "https://doi.org/10.5281/zenodo.1417004",
        "ee": "https://zenodo.org/records/1417004/files/SinghiB14a.pdf",
        "abstract": "Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made man- ually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes in- creasingly critical. This paper describes the LIA music recommendation system for advertisements using both tex- tual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable sound- track from a list of candidate songs, given a TV commer- cial. The organizers provide a development dataset includ- ing multimedia features. The initial assumption of the pro- posed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this sub- set, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm.",
        "zenodo_id": 1417004,
        "dblp_key": "conf/ismir/SinghiB14a",
        "keywords": [
            "commercial advertisements",
            "song illustration",
            "economic impact",
            "commercial video",
            "acoustic features",
            "MediaEval 2013 Soundtrack task",
            "textual features",
            "automatic system",
            "predict the most suitable soundtrack",
            "TV commercial"
        ]
    },
    {
        "title": "Panako - A Scalable Acoustic Fingerprinting System Handling Time-Scale and Pitch Modification.",
        "author": [
            "Joren Six",
            "Marc Leman"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.7185611",
        "url": "https://doi.org/10.5281/zenodo.7185611",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T048_122_Paper.pdf",
        "abstract": "A snapshot of the Panako acoustic fingerprinting system as described in the paper titled &#39;Panako: a scalable audio search system&#39; in the Journal of Open Source Software.\n\nPanako solves the problem of finding short audio fragments in large digital audio archives.The content based audio search algorithm implemented in Panako is able to identify a short audio query in a large database of thousands of hours of audio using an acoustic fingerprinting technique.",
        "zenodo_id": 7185611,
        "dblp_key": "conf/ismir/SixL14",
        "keywords": [
            "Panako",
            "audio search system",
            "audio fragments",
            "digital audio archives",
            "content based",
            "audio search algorithm",
            "acoustic fingerprinting",
            "large database",
            "thousands of hours",
            "audio query"
        ]
    },
    {
        "title": "Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera.",
        "author": [
            "Ajay Srinivasamurthy",
            "Rafael Caro Repetto",
            "Sundar Harshavardhan",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1285593",
        "url": "https://doi.org/10.5281/zenodo.1285593",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T078_134_Paper.pdf",
        "abstract": "The Beijing Opera Percussion Pattern (BOPP) dataset is a collection of audio examples of percussion patterns played by the percussion ensemble in Beijing Opera (Jingju, \u4eac\u5267). The percussion ensemble in Jingju plays a set of pre-defined and labeled percussion patterns, which serve many functions.The percussion patterns can be defined as sequences of strokes played by different combinations of the percussion instruments, and the resulting variety of timbres are transmitted using oral syllables as mnemonics. More information on the percussion instruments used in Beijing Opera can be found at http://compmusic.upf.edu/examples-percussion-bo.\n\nThe dataset presented here was used as the training dataset in the referenced paper. A detailed description of percussion patterns inJingjucan also be found in it.\n\nDATASET\n\nThe dataset is a collection of 133 audio percussion patterns spanning five different pattern classes as described below. The scores for the patterns and additional details about the patterns are at:http://compmusic.upf.edu/bo-perc-patterns\n\nAudio Content\n\nThe audio files are short segments containing one of the above mentioned patterns. The audio is stereo, sampled at 44.1 kHz, and stored as wav files. The segments were chosen from the introductory parts of arias. The recordings of arias are from commercially available releases spanning various artists. The audio and segments were chosen carefully by a musicologist to be representative of the percussion patterns that occur inJingju. The audio segments contain diverse instrument timbres of percussion instruments (though the same set of instruments are played, there can be slight variations in the individual instruments across different ensembles), recording quality and period of the recording. Though these recordings were chosen from introductions of arias where only percussion ensemble is playing, there are some examples in the dataset where the melodic accompaniment starts before the percussion pattern ends.\n\nAnnotations\n\nEach of the audio patterns has an associated syllable level transcription of the audio pattern. The transcription is obtained from the score for the pattern and is not time aligned to the audio. The transcription is done using a reduced set of five syllables and is sufficient to computationally model the timbres of all the syllables. The annotations are stored as Hidden Markov Model Toolkit (HTK) label files. There is also a single master label file provided for batch processing using HTK (http://htk.eng.cam.ac.uk/).\n\nDataset organization\n\nThe dataset has wav files and label files. The files are named as\n\npIDInstID.extension\n\nThe pID is as in Table 1, instID is a three digit identifier for the specific instance of the pattern, and extension can be .wav for the audio file or .lab for the label file. pID \u03f5{10, 11, 12, 13, 14}, InstID \u03f5{1, 2, ..., NpID}. e.g. The audio file and the label file for the fifth instance of the pattern duotuo is named 12005.wav and 12005.lab, respectively. The master label file is called masterLabels.lab\n\nUsing this dataset\n\nIf you use the dataset in your work, please cite the following publication:\n\n\nAjay Srinivasamurthy, Rafael Caro Repetto, Harshavardhan Sundar, Xavier Serra, Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera, in Proceedings of the 15th International Society for Music Information Retrieval (ISMIR) Conference, Taipei, Taiwan, Oct 2014.\n\n\nhttp://hdl.handle.net/10230/25677\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nCONTACT\n\nIf you have any questions or comments about the dataset, please feel free to write to us.\n\nAjay Srinivasamurthy (ajays.murthy@upf.edu)\n\nRafael Caro Repetto (rafael.caro@upf.edu)\n\n\n\nhttp://compmusic.upf.edu/bopp-dataset",
        "zenodo_id": 1285593,
        "dblp_key": "conf/ismir/SrinivasamurthyRHS14"
    },
    {
        "title": "Classifying EEG Recordings of Rhythm Perception.",
        "author": [
            "Sebastian Stober",
            "Daniel J. Cameron",
            "Jessica A. Grahn"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415734",
        "url": "https://doi.org/10.5281/zenodo.1415734",
        "ee": "https://zenodo.org/records/1415734/files/StoberCG14.pdf",
        "abstract": "Electroencephalography (EEG) recordings of rhythm percep- tion might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms \u2013 half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme \u2013 were each repeated for 32 sec- onds. During presentation, the participants\u2019 brain waves were recorded via 14 EEG channels. We applied stacked denois- ing autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we in- vestigated how far these techniques can be used to recognize the individual rhythms.",
        "zenodo_id": 1415734,
        "dblp_key": "conf/ismir/StoberCG14",
        "keywords": [
            "Electroencephalography",
            "EEG recordings",
            "rhythm perception",
            "distinguish different rhythm types",
            "identify rhythms",
            "deep learning techniques",
            "adults",
            "rhythmic tone sequences",
            "East African music",
            "Western music"
        ]
    },
    {
        "title": "Formalizing the Problem of Music Description.",
        "author": [
            "Bob L. Sturm",
            "Rolf Bardeli",
            "Thibault Langlois",
            "Valentin Emiya"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414914",
        "url": "https://doi.org/10.5281/zenodo.1414914",
        "ee": "https://zenodo.org/records/1414914/files/SturmBLE14.pdf",
        "abstract": "The lack of a formalism for \u201cthe problem of music descrip- tion\u201d results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the \u201cground truth\u201d of a music dataset without attending to the music it contains. To address these issues, we formal- ize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formal- ism applied to three examples drawn from the literature.",
        "zenodo_id": 1414914,
        "dblp_key": "conf/ismir/SturmBLE14",
        "keywords": [
            "formalism",
            "ambiguity",
            "evaluation",
            "criteria",
            "success",
            "paradox",
            "system",
            "system",
            "evaluation",
            "illustrations"
        ]
    },
    {
        "title": "The Kiki-Bouba Challenge: Algorithmic Composition for Content-based MIR Research and Development.",
        "author": [
            "Bob L. Sturm",
            "Nick Collins"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416364",
        "url": "https://doi.org/10.5281/zenodo.1416364",
        "ee": "https://zenodo.org/records/1416364/files/SturmC14.pdf",
        "abstract": "We propose the \u201cKiki-Bouba Challenge\u201d (KBC) for the re- search and development of content-based music informa- tion retrieval (MIR) systems. This challenge is unencum- bered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imper- fect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irrepro- ducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition \u2014 a hitherto under-exploited resource for MIR.",
        "zenodo_id": 1416364,
        "dblp_key": "conf/ismir/SturmC14",
        "keywords": [
            "Kiki-Bouba Challenge",
            "content-based music information retrieval",
            "unencumbered by problems",
            "free data",
            "perfect ground truth",
            "well-specifiable characteristics",
            "open source algorithmic composition",
            "under-exploited resource",
            "imperfect copyrights",
            "irreproducibility"
        ]
    },
    {
        "title": "Sparse Cepstral, Phase Codes for Guitar Playing Technique Classification.",
        "author": [
            "Li Su 0002",
            "Li-Fan Yu",
            "Yi-Hsuan Yang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417215",
        "url": "https://doi.org/10.5281/zenodo.1417215",
        "ee": "https://zenodo.org/records/1417215/files/SuYY14.pdf",
        "abstract": "Automatic recognition of guitar playing techniques is chal- lenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our eval- uation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse represen- tations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos.",
        "zenodo_id": 1417215,
        "dblp_key": "conf/ismir/SuYY14",
        "keywords": [
            "Automatic recognition",
            "guitar playing techniques",
            "subtle nuances",
            "comparative study",
            "features extracted",
            "magnitude spectrum",
            "cepstrum",
            "phase derivatives",
            "group-delay function",
            "instantaneous frequency deviation"
        ]
    },
    {
        "title": "Melody Extraction from Polyphonic Audio of Western Opera: A Method based on Detection of the Singer&apos;s Formant.",
        "author": [
            "Zheng Tang",
            "Dawn A. A. Black"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416714",
        "url": "https://doi.org/10.5281/zenodo.1416714",
        "ee": "https://zenodo.org/records/1416714/files/TangB14.pdf",
        "abstract": "Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer\u2019s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera sing- ers [3]. In this paper we introduce a novel melody extrac- tion algorithm based on this feature for opera signals. At the front end, it automatically detects the singer\u2019s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch sa- lience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic atten- uation is adopted to handle octave errors which are com- mon in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of- the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer\u2019s formant detec- tion. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%).",
        "zenodo_id": 1416714,
        "dblp_key": "conf/ismir/TangB14",
        "keywords": [
            "opera",
            "singers formant",
            "Long-Term Average Spectrum (LTAS)",
            "Fan Chirp Transform (FChT)",
            "pitch salience",
            "octave errors",
            "subharmonic attenuation",
            "outliers in pitch detection",
            "voicing detection",
            "voicing false alarm"
        ]
    },
    {
        "title": "Drum Transcription via Classification of Bar-Level Rhythmic Patterns.",
        "author": [
            "Lucas Thompson",
            "Simon Dixon",
            "Matthias Mauch"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417341",
        "url": "https://doi.org/10.5281/zenodo.1417341",
        "ee": "https://zenodo.org/records/1417341/files/ThompsonDM14.pdf",
        "abstract": "We propose a novel method for automatic drum transcrip- tion from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognis- ing individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhyth- mic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six dif- ferent drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly be- low that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment.",
        "zenodo_id": 1417341,
        "dblp_key": "conf/ismir/ThompsonDM14",
        "keywords": [
            "drum transcription",
            "automatic method",
            "individual drums",
            "bar-level drum patterns",
            "support vector machines",
            "synthesised audio",
            "audio similarity",
            "measures of precision and recall",
            "real-world recordings",
            "polyphonic accompaniment"
        ]
    },
    {
        "title": "Design And Evaluation of Onset Detectors using Different Fusion Policies.",
        "author": [
            "Mi Tian 0001",
            "Gy\u00f6rgy Fazekas",
            "Dawn A. A. Black",
            "Mark B. Sandler"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415002",
        "url": "https://doi.org/10.5281/zenodo.1415002",
        "ee": "https://zenodo.org/records/1415002/files/TianFBS14.pdf",
        "abstract": "Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detec- tion methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achiev- ing better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the pa- rameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset de- tectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter set- tings.",
        "zenodo_id": 1415002,
        "dblp_key": "conf/ismir/TianFBS14",
        "keywords": [
            "Music Information Retrieval",
            "Note onset detection",
            "Fusion policies",
            "Existing algorithms",
            "Combining different algorithms",
            "Linear combination",
            "Late decision fusion",
            "Large scale evaluation",
            "Published datasets",
            "Chinese traditional instrument samples"
        ]
    },
    {
        "title": "Boundary Detection in Music Structure Analysis using Convolutional Neural Networks.",
        "author": [
            "Karen Ullrich",
            "Jan Schl\u00fcter",
            "Thomas Grill"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415886",
        "url": "https://doi.org/10.5281/zenodo.1415886",
        "ee": "https://zenodo.org/records/1415886/files/UllrichSG14.pdf",
        "abstract": "The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectro- grams. On a representative subset of the SALAMI struc- tural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at dif- ferent temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of \u00b10.5 seconds, and from",
        "zenodo_id": 1415886,
        "dblp_key": "conf/ismir/UllrichSG14",
        "keywords": [
            "Convolutional Neural Networks",
            "mel-scaled magnitude spectrograms",
            "boundary retrieval F-measure",
            "SALAMI structural annotation dataset",
            "audio signals",
            "music structure analysis",
            "temporal tolerances",
            "state-of-the-art",
            "\u00b10.5 seconds",
            "\u00b10.5 seconds"
        ]
    },
    {
        "title": "What is the Effect of Audio Quality on the Robustness of MFCCs and Chroma Features?",
        "author": [
            "Juli\u00e1n Urbano",
            "Dmitry Bogdanov",
            "Perfecto Herrera",
            "Emilia G\u00f3mez",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416276",
        "url": "https://doi.org/10.5281/zenodo.1416276",
        "ee": "https://zenodo.org/records/1416276/files/UrbanoBHGS14.pdf",
        "abstract": "Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical appli- cations they are to be computed on music corpora contain- ing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may af- fect the computation of descriptors. This raises the ques- tion of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute sev- eral statistics to quantify the robustness of the resulting de- scriptors, and then estimate the practical effects for a sam- ple task like genre classification.",
        "zenodo_id": 1416276,
        "dblp_key": "conf/ismir/UrbanoBHGS14"
    },
    {
        "title": "Vocal Separation using Singer-Vowel Priors Obtained from Polyphonic Audio.",
        "author": [
            "Shrikant Venkataramani",
            "Nagesh Nayak",
            "Preeti Rao",
            "Rajbabu Velmurugan"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414974",
        "url": "https://doi.org/10.5281/zenodo.1414974",
        "ee": "https://zenodo.org/records/1414974/files/VenkataramaniNRV14.pdf",
        "abstract": "Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic- sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior sep- aration of the lead vocal from the instrumental background. Singer specific dictionaries learned from available poly- phonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF op- timization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral en- velope.",
        "zenodo_id": 1414974,
        "dblp_key": "conf/ismir/VenkataramaniNRV14",
        "keywords": [
            "harmonic-sinusoidal modeling",
            "matrix decomposition methods",
            "singer identity",
            "soft mask",
            "melodic instruments",
            "polyphonic recordings",
            "dictionary learning",
            "NMF optimization",
            "vocal signal bases",
            "sparseness constraints"
        ]
    },
    {
        "title": "Towards Seamless Network Music Performance: Predicting an Ensemble&apos;s Expressive Decisions for Distributed Performance.",
        "author": [
            "Bogdan Vera",
            "Elaine Chew"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416186",
        "url": "https://doi.org/10.5281/zenodo.1416186",
        "ee": "https://zenodo.org/records/1416186/files/VeraC14.pdf",
        "abstract": "Internet performance faces the challenge of network la- tency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of dis- tributed or co-located ensembles. We evaluate the model by comparing its prediction ac- curacy to two others: one employing only linear condi- tional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent dis- tributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promis- ing, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predic- tive timing cues using our system.",
        "zenodo_id": 1416186,
        "dblp_key": "conf/ismir/VeraC14",
        "keywords": [
            "network latency",
            "music prediction",
            "expressive timing",
            "Bayesian network",
            "stylistic model selection",
            "linear conditional gaussian distributions",
            "rehearsals",
            "distributed ensembles",
            "custom-composed piece",
            "predictive timing cues"
        ]
    },
    {
        "title": "Robust Joint Alignment of Multiple Versions of a Piece of Music.",
        "author": [
            "Siying Wang 0001",
            "Sebastian Ewert",
            "Simon Dixon"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416382",
        "url": "https://doi.org/10.5281/zenodo.1416382",
        "ee": "https://zenodo.org/records/1416382/files/WangED14.pdf",
        "abstract": "Large music content libraries often comprise multiple ver- sions of a piece of music. To establish a link between dif- ferent versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be signif- icant such that even state-of-the-art methods fail to iden- tify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state- of-the-art method, with significantly less outliers (standard deviation 53% lower).",
        "zenodo_id": 1416382,
        "dblp_key": "conf/ismir/WangED14",
        "keywords": [
            "automatic music alignment",
            "multiple versions",
            "correct alignment",
            "robustness",
            "joint alignment",
            "comparison",
            "significantly less outliers",
            "standard deviation",
            "state-of-the-art method",
            "Mazurka Project"
        ]
    },
    {
        "title": "Automatic Set List Identification and Song Segmentation for Full-Length Concert Videos.",
        "author": [
            "Ju-Chiang Wang",
            "Ming-Chi Yen",
            "Yi-Hsuan Yang",
            "Hsin-Min Wang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417361",
        "url": "https://doi.org/10.5281/zenodo.1417361",
        "ee": "https://zenodo.org/records/1417361/files/WangYYW14.pdf",
        "abstract": "Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural ques- tions that arise include \u201cwhat is the set list?\u201d and \u201cwhen does each song begin and end?\u201d Indeed, many full con- cert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typ- ically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research prob- lem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identifica- tion, musical event detection, music alignment, and struc- tural segmentation. Moreover, we propose a greedy ap- proach that sequentially identifies a song from a database of studio versions and simultaneously estimates its prob- able boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm.",
        "zenodo_id": 1417361,
        "dblp_key": "conf/ismir/WangYYW14",
        "keywords": [
            "full concert videos",
            "set list",
            "timecodes",
            "uploaders",
            "viewers",
            "metadata",
            "automated solution",
            "research problem",
            "music information retrieval",
            "audio fingerprinting"
        ]
    },
    {
        "title": "Emotional Predisposition of Musical Instrument Timbres with Static Spectra.",
        "author": [
            "Bin Wu 0013",
            "Andrew Horner",
            "Chung Lee"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417153",
        "url": "https://doi.org/10.5281/zenodo.1417153",
        "ee": "https://zenodo.org/records/1417153/files/WuHL14.pdf",
        "abstract": "Music is one of the strongest triggers of emotions. Re- cent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown sig- nificant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted follow- up listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition.",
        "zenodo_id": 1417153,
        "dblp_key": "conf/ismir/WuHL14",
        "keywords": [
            "emotions",
            "music",
            "timbres",
            "spectral centroid",
            "harmonic ratio",
            "brightness",
            "static tones",
            "emotional predisposition",
            "even/odd harmonic ratio",
            "spectral shape"
        ]
    },
    {
        "title": "Gender Identification and Age Estimation of Users Based on Music Metadata.",
        "author": [
            "Ming-Ju Wu",
            "Jyh-Shing Roger Jang",
            "Chun-Hung Lu"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417605",
        "url": "https://doi.org/10.5281/zenodo.1417605",
        "ee": "https://zenodo.org/records/1417605/files/WuJL14.pdf",
        "abstract": "Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users\u2019 real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information.",
        "zenodo_id": 1417605,
        "dblp_key": "conf/ismir/WuJL14",
        "keywords": [
            "Music recommendation",
            "users real-world identity",
            "music metadata",
            "gender identification",
            "age estimation",
            "experimental results",
            "accuracy",
            "mean absolute error",
            "personal information",
            "improving music recommendation"
        ]
    },
    {
        "title": "Enhancing Collaborative Filtering Music Recommendation by Balancing Exploration and Exploitation.",
        "author": [
            "Zhe Xing",
            "Xinxi Wang",
            "Ye Wang 0007"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416776",
        "url": "https://doi.org/10.5281/zenodo.1416776",
        "ee": "https://zenodo.org/records/1416776/files/XingWW14.pdf",
        "abstract": "Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation al- gorithms work in a greedy way, invariably recommend- ing songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal per- formance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users\u2019 musical tastes, we use a Bayesian graphical model that takes account of both CF latent fac- tors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance.",
        "zenodo_id": 1416776,
        "dblp_key": "conf/ismir/XingWW14",
        "keywords": [
            "Collaborative filtering",
            "greedy approach",
            "exploration",
            "Bayesian graphical model",
            "Bayesian inference",
            "recommendation performance",
            "user study",
            "simulation experiments",
            "novel reinforcement learning",
            "latent factors"
        ]
    },
    {
        "title": "Detecting Drops in Electronic Dance Music: Content based approaches to a socially significant music event.",
        "author": [
            "Karthik Yadati",
            "Martha A. Larson",
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417081",
        "url": "https://doi.org/10.5281/zenodo.1417081",
        "ee": "https://zenodo.org/records/1417081/files/YadatiLLH14.pdf",
        "abstract": "Electronic dance music (EDM) is a popular genre of mu- sic. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the num- ber of users who leave comments in the general neighbor- hood of drop events in music on online audio distribution platforms like SoundCloud. The variability that character- izes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage ap- proach to drop detection that first models the sound char- acteristics during drop events and then incorporates tem- poral structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the pro- posed method for combining content analysis and social activity to detect events in music recordings.",
        "zenodo_id": 1417081,
        "dblp_key": "conf/ismir/YadatiLLH14",
        "keywords": [
            "Electronic dance music",
            "automatic detection",
            "drop event",
            "SoundCloud",
            "two-stage approach",
            "sound characteristics",
            "temporal structure",
            "watershed moment",
            "social comments",
            "weak reference labels"
        ]
    },
    {
        "title": "Bayesian Singing-Voice Separation.",
        "author": [
            "Po-Kai Yang",
            "Chung-Chien Hsu",
            "Jen-Tzung Chien"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417373",
        "url": "https://doi.org/10.5281/zenodo.1417373",
        "ee": "https://zenodo.org/records/1417373/files/YangHC14.pdf",
        "abstract": "This paper presents a Bayesian nonnegative matrix fac- torization (NMF) approach to extract singing voice from",
        "zenodo_id": 1417373,
        "dblp_key": "conf/ismir/YangHC14",
        "keywords": [
            "Bayesian",
            "nonnegative",
            "matrix",
            "factorization",
            "Singing",
            "voice",
            "extract",
            "fac- torization",
            "NMF",
            "approach"
        ]
    },
    {
        "title": "Singing Voice Separation Using Spectro-Temporal Modulation Features.",
        "author": [
            "Frederick Z. Yen",
            "Yin-Jyun Luo",
            "Tai-Shih Chi"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417695",
        "url": "https://doi.org/10.5281/zenodo.1417695",
        "ee": "https://zenodo.org/records/1417695/files/YenLC14.pdf",
        "abstract": "An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational audito- ry scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time- frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three catego- ries in music audio signals: vocal, harmonic, and percus- sive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algo- rithm was tested using the MIR-1K dataset and demon- strated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures.",
        "zenodo_id": 1417695,
        "dblp_key": "conf/ismir/YenLC14",
        "keywords": [
            "auditory-perception",
            "singing voice separation",
            "monaural music recordings",
            "computational auditory scene analysis",
            "spectrograms",
            "spectral-temporal modulation",
            "music audio signals",
            "vocal category",
            "time-frequency masking",
            "MIR-1K dataset"
        ]
    },
    {
        "title": "Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing.",
        "author": [
            "Shuo Zhang",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T062_292_Paper.pdf",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/ZhangRS14"
    },
    {
        "title": "Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014, Taipei, Taiwan, October 27-31, 2014",
        "author": [
            "Hsin-Min Wang",
            "Yi-Hsuan Yang",
            "Jin Ha Lee 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2014"
    }
]