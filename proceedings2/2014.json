[
    {
        "title": "Computational Modeling of Induced Emotion Using GEMS.",
        "author": [
            "Anna Aljanaki",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415192",
        "url": "https://doi.org/10.5281/zenodo.1415192",
        "ee": "https://zenodo.org/records/1415192/files/AljanakiWV14.pdf",
        "abstract": "Most researchers in the automatic music emotion recogni- tion field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. More- over, in many cases it might be important to model in- duced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which ad- dresses these two issues. We collected the data for our study using a game with a purpose. We exploit a compre- hensive set of features from several state-of-the-art tool- boxes and propose a new set of harmonically motivated features. The performance of these feature sets is com- pared. Additionally, we use expert human annotations to explore the dependency between musicologically mean- ingful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can bet- ter approximate human perception.",
        "zenodo_id": 1415192,
        "dblp_key": "conf/ismir/AljanakiWV14",
        "keywords": [
            "automatic music emotion recognition",
            "two-dimensional valence and arousal model",
            "diversity of emotions",
            "induced emotion",
            "multidimensional emotional space",
            "Geneva Emotional Music Scales (GEMS)",
            "complicated features",
            "expert human annotations",
            "musicologically meaningful characteristics",
            "approximate human perception"
        ],
        "content": "COMPUTATIONAL MODELING OF INDUCED EMOTION USING GEMS\nAnna Aljanaki\nUtrecht University\nA.Aljanaki@uu.nlFrans Wiering\nUtrecht University\nF.Wiering@uu.nlRemco C. Veltkamp\nUtrecht University\nR.C.Veltkamp@uu.nl\nABSTRACT\nMost researchers in the automatic music emotion recogni-\ntion ﬁeld focus on the two-dimensional valence and arousal\nmodel. This model though does not account for the whole\ndiversity of emotions expressible through music. More-\nover, in many cases it might be important to model in-\nduced (felt) emotion, rather than perceived emotion. In\nthis paper we explore a multidimensional emotional space,\nthe Geneva Emotional Music Scales (GEMS), which ad-\ndresses these two issues. We collected the data for our\nstudy using a game with a purpose. We exploit a compre-\nhensive set of features from several state-of-the-art tool-\nboxes and propose a new set of harmonically motivated\nfeatures. The performance of these feature sets is com-\npared. Additionally, we use expert human annotations to\nexplore the dependency between musicologically mean-\ningful characteristics of music and emotional categories of\nGEMS, demonstrating the need for algorithms that can bet-\nter approximate human perception.\n1. INTRODUCTION\nMost of the e\u000bort in automatic music emotion recognition\n(MER) is invested into modeling two dimensions of mu-\nsical emotion: valence (positive vs. negative) and arousal\n(quiet vs. energetic) (V-A) [16]. Regardless of the popular-\nity of V-A, the question of which model of musical emo-\ntion is best has not yet been solved. The di\u000eculty is, on\none hand, in creating a model that reﬂects the complex-\nity and subtlety of the emotions that music can demon-\nstrate, while on the other hand providing a linguistically\nunambiguous framework that is convenient to use to re-\nfer to such a complex non-verbal concept as musical emo-\ntion. Categorical models, possessing few (usually 4–6, but\nsometimes as many as 18) [16] classes are oversimplifying\nthe problem, while V-A has been criticized for a lack of\ndiscerning capability, for instance in the case of fear and\nanger. Other pitfalls of V-A model are that it was not cre-\nated speciﬁcally for music, and is especially unsuited to\ndescribe induced (felt) emotion, which might be important\nfor some MER tasks, e.g. composing a playlist using emo-\nc\rAnna Aljanaki, Frans Wiering, Remco C. Veltkamp.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Anna Aljanaki, Frans Wiering, Remco\nC. Veltkamp. “COMPUTATIONAL MODELING OF INDUCED EMO-\nTION USING GEMS”, 15th International Society for Music Information\nRetrieval Conference, 2014.tional query and in any other cases when the music should\ncreate a certain emotion in listener. The relationship be-\ntween induced and perceived emotion is not yet fully un-\nderstood, but they are surely not equivalent — one may lis-\nten to angry music without feeling angry, but instead feel\nenergetic and happy. It was demonstrated that some types\nof emotions (especially negative ones) are less likely to be\ninduced by music, though music can express them [17].\nIn this paper we address the problem of modeling in-\nduced emotion by using GEMS. GEMS is a domain-spe-\nciﬁc categorical emotional model, developed by Zentner\net al. [17] speciﬁcally for music. The model was derived\nvia a three-stage collection and ﬁltering of terms which are\nrelevant to musical emotion, after which the model was\nveriﬁed in a music listening-context. Being based on emo-\ntional ontology which comes from listeners, it must be a\nmore convenient tool to retrieve music than, for instance,\npoints on a V-A plane. The full GEMS scale consists of 45\nterms, with shorter versions of 25 and 9 terms. We used\nthe 9-term version of GEMS (see Table 1) to collect data\nusing a game with a purpose.\nEmotion induced by music depends on many factors,\nsome of which are external to music itself, such as cul-\ntural and personal associations, social listening context, the\nmood of the listener. Naturally, induced emotion is also\nhighly subjective and varies a lot across listeners, depend-\ning on their musical taste and personality. In this paper we\ndo not consider all these factors and will only deal with\nthe question to which extent induced emotion can be mod-\neled using acoustic features only. Such a scenario, when no\ninput from the end-user (except for, maybe, genre prefer-\nences) is available, is plausible for a real-world application\nof a MER task. We employ four di\u000berent feature sets: low-\nlevel features related to timbre and energy, extracted using\nOpenSmile,1and a more musically motivated feature set,\ncontaining high-level features, related to mode, rhythm,\nand harmony, from the MIRToolbox,2PsySound3and\nSonicAnnotator.4We also enhance the performance of the\nlatter by designing new features that describe the harmonic\ncontent of music. As induced emotion is a highly subjec-\ntive phenomenon, the performance of the model will be\nconfounded by the amount of agreement between listen-\ners which provide the ground-truth. As far as audio-based\nfeatures are not perfect yet, we try to estimate this upper\nbound for our data by employing human experts, who an-\n1opensmile.sourceforge.net\n2jyu.ﬁ/hum/laitokset/musiikki/en/research/coe/materials/mirtoolbox\n3psysound.wikidot.com\n4isophonics.net/SonicAnnotator\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n373notate a subset of the data with ten musicological features.\nContribution. This paper explores computational ap-\nproaches to modeling induced musical emotion and esti-\nmates the upper boundary for such a task, in case when no\npersonal or contextual factors can be taken into account. It\nis also suggested that more than two dimensions are nec-\nessary to represent musical emotion adequately. New fea-\ntures for harmonic description of music are proposed.\n2. RELATED WORK\nMusic emotion recognition is a young, but fast-developing\nﬁeld. Reviewing it in its entirety is out of scope of this pa-\nper. For such a review we are referring to [16]. In this sec-\ntion we will brieﬂy summarize the commonly used meth-\nods and approaches that are relevant to this paper.\nAutomatic MER can be formulated both as a regression\nand classiﬁcation problem, depending on the underlying\nemotional model. As such, the whole entirety of machine\nlearning algorithms can be used for MER. In this paper\nwe are employing Support Vector Regression (SVR), as it\ndemonstrated good performance [7,15] and can learn com-\nplex non-linear dependencies from the feature space. Be-\nlow we describe several MER systems.\nIn [15], V-A is modeled with acoustic features (spec-\ntral contrast, DWCH and other low-level features from\nMarsyas and PsySound) using SVR, achieving perfor-\nmance of 0.76 for arousal and 0.53 for valence (in terms of\nPearson’s rhere and further). In [7], ﬁve dimensions (basic\nemotions) were modeled with a set of timbral, rhythmic\nand tonal features, using SVR. The performance varied\nfrom 0.59 to 0.69. In [5], pleasure, arousal and dominance\nwere modeled with AdaBoost.RM using features extracted\nfrom audio, MIDI and lyrics. An approach based on audio\nfeatures only performed worse than multimodal features\napproach (0.4 for valence, 0.72 for arousal and 0.62 for\ndominance).\nVarious chord-based statistical measures have already\nbeen employed for di\u000berent MIR tasks, such as music\nsimilarity or genre detection. In [3], chordal features\n(longest common chord sequence and histogram statistics\non chords) were used to ﬁnd similar songs and to estimate\ntheir emotion (in terms of valence) based on chord simi-\nlarity. In [9], chordal statistics is used for MER, but the\nduration of chords is not taken into account, which we ac-\ncount for in this paper. Interval-based features, described\nhere, to our knowledge have not been used before.\nA computational approach to modeling musical emo-\ntion using GEMS has not been adopted before. In [11],\nGEMS was used to collect data dynamically on 36 mu-\nsical excerpts. Listener agreement was very good (Cron-\nbach’s alpha ranging from 0.84 to 0.98). In [12], GEMS is\ncompared to a three-dimensional (valence-arousal-tension)\nand categorical (anger, fear, happiness, sadness, tender-\nness) models. The consistency of responses is compared,\nand it is found that GEMS categories have both some of\nthe highest (joyful activation, tension) and some of the\nlowest (wonder, transcendence) agreement. It was also\nfound that GEMS categories are redundant, and valenceand arousal dimensions account for 89% of variance. That\nexperiment, though, was performed on 16 musical excerpts\nonly, and the excerpts were selected using criteria based on\nV-A model, which might have resulted in bias.\n3. DATA DESCRIPTION\nThe dataset that we analyze consists of 400 musical ex-\ncerpts (44100 Hz, 128 kbps). Each excerpt is 1 minute\nlong (except for 4 classical pieces which were shorter than\n1 minute). It is evenly split (100 pieces per genre) by four\ngenres (classical, rock, pop and electronic music). In many\nstudies, musical excerpts are specially selected for their\nstrong emotional content that best ﬁts the chosen emotional\nmodel, and only the excerpts that all the annotators agree\nupon, are left. In our dataset we maintain a good ecolog-\nical validity by selecting music randomly from a Creative\nCommons recording label Magnatune, only making sure\nthat the recordings are of good quality.\nBased on conclusions from [11, 12], we renamed two\nGEMS categories by replacing them with one of their sub-\ncategories (wonder was replaced with amazement, and\ntranscendence with solemnity). Participants were asked\nto select no more than three emotional terms from a list of\nnine. They were instructed to describe how music made\nthem feel, and not what it expressed, and were encour-\naged to do so in a game context [1]. All the songs were\nannotated by at least 10 players (mean =20.8, SD =14).\nThe game with a purpose was launched and advertised\nthrough social networks. The game,5as well as annota-\ntions and audio,6are accessible online. More than 1700\nplayers have contributed. The game was streaming music\nfor 138 hours in total. A detailed description and analysis\nof the data can be found in [1] or in a technical report. [2]\nWe are not interested in modeling irritation from non-\npreferred music, but rather di\u000berences in emotional per-\nception across listeners that come from other factors. We\nintroduce a question to report disliking the music and dis-\ncard such answers. We also clean the data by computing\nFleiss’s kappa on all the annotations for every musical ex-\ncerpt, and discarding the songs with negative kappa (this\nindicates that the answers are extremely inconsistent (33\nsongs)). Fleiss’s kappa is designed to estimate agreement,\nwhen the answers are binary or categorical. We use this\nvery loose criteria, as it is expected to ﬁnd a lot of disagree-\nment. We retain the remaining 367 songs for analysis.\nThe game participants were asked to choose several cat-\negories from a list, but for the purposes of modeling we\ntranslate the annotations into a continuous space by using\nthe following equation:\nscore1\ni j=1\nnnX\nk=1ak; (1)\nwhere score1\ni jis an estimated value of emotion ifor song\nj,akis the answer of the k-th participant on a question\nwhether emotion iis present in song jor not (answer is\n5www.emotify.org\n6www.projects.science.uu.nl/memotion/emotifydata/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n374C1 C2 C3\nAmazement 0:01 \u00000:73\u00000:07\nSolemnity \u00000:07 0:12 0:89\nTenderness 0:75 0:19 \u00000:22\nNostalgia 0:57 0:46 \u00000:41\nCalmness 0:80 0:22 0:28\nPower \u00000:80\u00000:17\u00000:06\nJoyful activation \u00000:37\u00000:74\u00000:32\nTension \u00000:72 0:20 0:30\nSadness 0:13 0:80\u00000:05\nTable 1. PCA on the GEMS categories.\nFigure 1. Intervals and their inversions.\neither 0 or 1), and nis the total number of participants,\nwho listened to song j.\nThe dimensions that we obtain are not orthogonal: most\nof them are somewhat correlated. To determine the under-\nlying structure, we perform Principal Components Anal-\nysis. According to a Scree test, three underlying dimen-\nsions were found in the data, which together explain 69%\nof variance. Table 1 shows the three-component solution\nrotated with varimax. The ﬁrst component, which accounts\nfor 32% of variance, is mostly correlated with calmness vs.\npower, the second (accounts for 23% of variance) with joy-\nful activation vs. sadness, and the third (accounts for 14%\nof variance) with solemnity vs. nostalgia. This suggests\nthat the underlying dimensional space of GEMS is three-\ndimensional. We might suggest that it resembles valence-\narousal-triviality model [13].\n4. HARMONIC FEATURES\nIt has been repeatedly shown that valence is more di\u000e-\ncult to model than arousal. In this section we describe fea-\ntures, that we added to our dataset to improve prediction of\nmodality in music.\nMusical chords, as well as intervals are known to be\nimportant for a\u000bective perception of music [10], as well as\nother MIR tasks. Chord and melody based features have\nbeen successfully applied to genre recognition of symbol-\nically represented music [8]. We compute statistics on the\nintervals and chords occurring in the piece.\n4.1 Interval Features\nWe segment audio, using local peaks in the harmonic\nchange detection function (HCDF) [6]. HCDF describes\ntonal centroid ﬂuctuations. The segments that we obtain\nare mostly smaller than 1 second and reﬂect single notes,\nchords or intervals. Based on the wrapped chromagrams\nFigure 2. Distribution of chords (Chordino and HPA).\ncomputed from the spectrum of this segments, we select\ntwo highest (energy-wise) peaks and compute the interval\nbetween them. For each interval, we compute its combined\nduration, weighted by its loudness (expressed by energy of\nthe bins). Then, we sum up this statistics for intervals\nand their inversions. Figure 1 illustrates the concept (each\nbar corresponds to the musical representation of a feature\nthat we obtain). As there are 6 distinct intervals with in-\nversions, we obtain 6 features. We expect that augmented\nfourths and ﬁfths (tritone) could reﬂect tension, contrary to\nperfect fourths and ﬁfths. The proportion of minor thirds\nand major sixths, as opposed to proportion of major thirds\nand minor sixths, could reﬂect the modality. The interval-\ninversion pairs containing seconds are rather unrestful.\n4.2 Chord Features\nTo extract chord statistics, we used 2 chord extraction\ntools, HPA7(Harmonic Progression Analyzer) and Chor-\ndino8plugins for Sonic Annotator. The ﬁrst plugin pro-\nvides 8 types of chords: major, minor, seventh, major and\nminor seventh, diminished, sixth and augmented. The sec-\nond plugin, in addition to these eight types, also provides\nminor sixth and slash chords (chords for which bass note\nis di\u000berent from the tonic, and might as well not belong\nto the chord). The chords are annotated with their onsets\nand o\u000bsets. After experimentation, only the chords from\nChordino were left, because those demonstrated more cor-\nrelation with the data. We computed the proportion of each\ntype of chord in the dataset, obtaining nine new features.\nThe slash chords were discarded by merging them with\ntheir base chord (e.g., Am/F chord is counted as a minor\nchord). The distribution of chords was uneven, with major\nchords being in majority (for details see Figure 2). Exam-\nining the accuracy of these chord extraction tools was not\nour goal, but the amount of disagreement between the two\ntools could give an idea about that (see Figure 2). From\nour experiments we concluded that weighting the chords\nby their duration is an important step, which improves the\nperformance of chord histograms.\n7patterns.enm.bris.ac.uk/hpa-software-package\n8isophonics.net/nnls-chroma\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n375TempoArticu-\nlationRhythmic\ncomplexity Mode Intensity Tonalness Pitch MelodyRhythmic\nclarity\nAmazement 0:50 \u00000:37 *0:27 **0:24 *0:27\nSolemnity\u00000:44 0:39 \u00000:45 \u00000:34\nTenderness\u00000:48 0:56 0:30\u00000:48 *0:29 0:44 0:54\nNostalgia\u00000:47\u00000:57 \u00000:30 *0:28 *0:27 0:50\nCalmness\u00000:64 0:48 \u00000:50 0:36\nPower 0:39 \u00000:35 *\u00000:27 0:51 \u00000:47\u00000:43\nJoyful\nactivation0:76\u00000:70 *0:27 **0:24 0:41 0:31\nTension \u00000:36 \u00000:36 \u00000:47\u00000:44\u00000:66\nSadness\u00000:45 0:51 \u00000:38 **\u00000:23 **\u00000:24 *0:27\nTable 2. Correlations between manually assessed factors and emotional categories.\n5. MANUALLY ASSESSED FEATURES\nIn this section we describe an additional feature set that\nwe composed using human experts, and explain the prop-\nerties of GEMS categories through perceptual musically\nmotivated factors. Because of huge time load that manual\nannotation creates we only could annotate part of the data\n(60 pieces out of 367).\n5.1 Procedure\nThree musicians (26–61 years, over 10 years of formal\nmusical training) annotated 60 pieces (15 pieces from each\ngenre) from the dataset with 10 factors, on a scale from 1\nto 10. The meaning of points on the scale was di\u000berent for\neach factor (for instance, for tempo 1 would mean ‘very\nslow’ and 10 would mean ‘very fast ’). The list of factors\nwas taken from the study of Wedin [13]: tempo (slow—\nfast), articulation (staccato—legato), mode (minor—ma-\njor), intensity (pp—\u000b), tonalness (atonal—tonal), pitch\n(bass—treble), melody (unmelodious—melodious), rhyth-\nmic clarity (vague—ﬁrm). We added rhythmic complexity\n(simple—complex) to this list, and eliminated style (date\nof composition) and type (serious—popular) from it.\n5.2 Analysis\nAfter examining correlations with the data, one of the fac-\ntors was discarded as non-informative (simple or complex\nharmony). This factor lacked consistency between annota-\ntors as well. Table 2 shows the correlations (Spearman’s\n\u001a) between manually assessed factors and emotional cat-\negories. We used a non-parametric test, because distribu-\ntion of emotional categories is not normal, skewed towards\nsmaller values (emotion was more often not present than\npresent). All the correlations are signiﬁcant with p-value <\n0.01, except for the ones marked with asterisk, which are\nsigniﬁcant with p-value <0.05. The values that are absent\nor marked with double asterisks failed to reach statistical\nsigniﬁcance, but some of them are still listed, because they\nillustrate important trends which are very probable to reach\nsigniﬁcance should we have more data.\nMany GEMS categories were quite correlated (tender-\nness andnostalgia: r=0:5, tenderness andcalmness:r=0:52, power andjoyful activation: r=0:4). All of\nthese have, however, musical characteristics that allow lis-\nteners to di\u000berentiate them, as we will see below.\nBoth nostalgia andtenderness correlate with slow tempo\nand legato articulation, but tenderness is also correlated\nwith higher pitch, major mode, and legato articulation (as\nopposed to staccato for nostalgia). Calmness is charac-\nterized by slow tempo, legato articulation and smaller in-\ntensity, similarly to tenderness. But tenderness features a\ncorrelation with melodiousness and major mode as well.\nBoth power andjoyful activation are correlated with fast\ntempo, and intensity, but power is correlated with minor\nmode and joyful activation with major mode.\nAs we would expect, tension is strongly correlated with\nnon-melodiousness and atonality, lower pitch and minor\nmode. Sadness, strangely, is much less correlated with\nmode, but it more characterized by legato articulation, slow\ntempo and smaller rhythmic complexity.\n6. EVALUATION\n6.1 Features\nWe use four toolboxes for MIR to extract features from au-\ndio: MIRToolbox, OpenSmile, PsySound and two V AMP\nplugins for SonicAnnotator. We also extract harmonic fea-\ntures, described in Section 4. These particular tools are\nchosen because the features they provide were specially\ndesigned for MER. MIRToolbox was conceived as a tool\nfor investigating a relationship between emotion and fea-\ntures in music. OpenSmile combines features from Speech\nProcessing and MIR and demonstrated good performance\non cross-domain emotion recognition [14]. We evaluate\nthree following computational and one human-assessed\nfeature sets:\n1.MIRToolbox +PsySound: 40 features from MIR-\nToolbox (spectral features, HCDF, mode, inharmonicity\netc.) and 4 features related to loudness from PsySound\n(using the loudness model of Chalupper and Fastl).\n2.OpenSmile: 6552 low-level supra-segmental features\n(chroma features, MFCCs or energy, and statistical\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n376Feature set MIRToolbox +PsySound OpenSmile MP +Harm Musicological\nr RMSE r RMSE r RMSE r RMSE\nAmazement :07\u0006:18:99\u0006:16:19\u0006:15:95\u0006:13:16\u0006:15 1:05\u0006:11:35\u0006:30:85\u0006:24\nSolemnity :35\u0006:14:80\u0006:09:42\u0006:16:95\u0006:13:43\u0006:08:89\u0006:15:60\u0006:24:84\u0006:22\nTenderness :50\u0006:10:84\u0006:10:52\u0006:12:95\u0006:07:57\u0006:12:85\u0006:18:87\u0006:09:50\u0006:19\nNostalgia :53\u0006:16:82\u0006:12:53\u0006:18:89\u0006:07:45\u0006:12:88\u0006:10:69\u0006:24:69\u0006:16\nCalmness :55\u0006:14:83\u0006:09:55\u0006:16:89\u0006:07:60\u0006:11:78\u0006:09:71\u0006:17:70\u0006:16\nPower :48\u0006:18:82\u0006:13:56\u0006:09:84\u0006:09:56\u0006:11:80\u0006:16:65\u0006:13:78\u0006:26\nJoyful\nactivation:63\u0006:08:77\u0006:11:68\u0006:08:80\u0006:08:66\u0006:12:75\u0006:11:74\u0006:28:58\u0006:15\nTension :38\u0006:14:87\u0006:20:41\u0006:19:94\u0006:19:46\u0006:11:85\u0006:13:58\u0006:35:71\u0006:36\nSadness :41\u0006:13:87\u0006:11:40\u0006:18:96\u0006:18:42\u0006:13:88\u0006:12:39\u0006:28:93\u0006:20\nTable 3. Evaluation of 4 feature sets on the data. Pearson’s rand RMSE with their standard deviations (across cross-\nvalidation rounds) are shown.\nfunctionals applied to them (such as mean, standard\ndeviation, inter-quartile range, skewness, kurtosis etc.).\n3.MP+Harm: to evaluate performance of harmonic fea-\ntures, we add them to the ﬁrst feature set. It doesn’t\nmake sense to evaluate them alone, because they only\ncover one aspect of music.\n4.Musicological feature set: these are 9 factors of music\ndescribed in section 5.\n6.2 Learning Algorithm\nAfter trying SVR, Gaussian Processes Regression and lin-\near regression, we chose SVR (the LIBSVM implementa-\ntion9) as a learning algorithm. The best performance was\nachieved using the RBF kernel, which is deﬁned as fol-\nlows:\nk(xi;xj)=exp\u0010\n\u0000\rkxi\u0000xjk2\u0011\n; (2)\nwhere \ris a parameter given to SVR. All the parame-\nters, C (error cost), epsilon (slack of the loss function) and\n\r, are optimized with grid-search for each feature set (but\nnot for each emotion). To select an optimal set of features,\nwe use recursive feature elimination (RFE). RFE assigns\nweights to features based on output from a model, and re-\nmoves attributes until performance is no longer improved.\n6.3 Evaluation\nWe evaluate the performances of the four systems us-\ning 10-fold cross-validation, splitting the dataset by artist\n(there are 140 distinct artists per 400 songs). If a song\nfrom artist A appears in the training set, there will be no\nsongs from this artist in the test set. Table 3 shows evalua-\ntion results. The accuracy of the models di\u000bers greatly per\ncategory, while all the feature sets demonstrate the same\npattern of success and failure (for instance, perform badly\nonamazement and well on joyful activation). This reﬂects\nthe fact that these two categories are very di\u000berent in their\nsubjectiveness. Figure 3 illustrates the performance of the\n9www.csie.ntu.edu.tw/ cjlin/libsvm/systems (r ) for each of the categories and Cronbach’s al-\npha (which measures agreement) computed on listener’s\nanswers (see [1] for more details), and shows that they are\nhighly correlated. The low agreement between listeners re-\nsults in conﬂicting cues, which limit model performance.\nIn general, the accuracy is comparable to accuracy\nachieved for perceived emotion by others [5,7,15], though\nit is somewhat lower. This might be explained by the fact\nthat all the categories contain both arousal and valence\ncomponents, and induced emotion annotations are less\nconsistent. In [7], tenderness was predicted with R=0:67,\nas compared to R=0:57 for MP+Harm system in our\ncase. For power andjoyful activation, the predictions from\nthe best systems (MP+Harm andOpenSmile) demon-\nstrated 0.56 and 0.68 correlation with the ground truth,\nwhile in [5, 15] it was 0.72 and 0.76 for arousal.\nThe performance of all the three computational mod-\nels is comparable, though MP+Harm model performs\nslightly better in general. Adding harmonic features im-\nproves average performance from 0:43 to 0:47, and perfor-\nmance of the best system (MP+Harm) decreases to 0:35\nwhen answers from people who disliked the music are not\ndiscarded. As we were interested in evaluating the new\nfeatures, we checked which features were considered im-\nportant by RFE. For power, the tritone proportion was im-\nportant (positively correlated with power), for sadness, the\nproportion of minor chords, for tenderness, the proportion\nof seventh chords (negatively correlates), for tension, the\nproportion of tritones, for joyful activation, the proportion\nof seconds and inversions (positive correlation).\nThemusicological feature set demonstrates the best\nperformance as compared to all the features derived from\nsignal-processing, demonstrating that our ability to model\nhuman perception is not yet perfect.\n7. CONCLUSION\nWe analyze the performance of audio features on predic-\ntion of induced musical emotion. The performance of the\nbest system is somewhat lower than can be achieved for\nperceived emotion recognition. We conduct PCA and ﬁnd\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n377Figure 3. Comparison of systems’ performance with\nCronbach’s alpha per category.\nthree dimensions in the GEMS model, which are best ex-\nplained by axes spanning calmness—power, joyful activa-\ntion—sadness andsolemnity—nostalgia). This ﬁnding is\nsupported by other studies in the ﬁeld [4, 13].\nWe conclude that it is possible to predict induced musi-\ncal emotion for some emotional categories, such as tender-\nness andjoyful activation, but for many others it might not\nbe possible without contextual information. We also show\nthat despite this limitation, there is still room for improve-\nment by developing features that can better approximate\nhuman perception of music, which can be pursued in fu-\nture work on emotion recognition.10\n8. REFERENCES\n[1] A. Aljanaki, D. Bountouridis, J.A. Burgoyne, J. van\nBalen, F. Wiering, H. Honing, and R. C. Veltkamp:\n“Designing Games with a Purpose for Data Collection\nin Music Research. Emotify and Hooked: Two Case\nStudies”, Proceedings of Games and Learning Alliance\nConference, 2013.\n[2] A. Aljanaki, F. Wiering, and R. C. Veltkamp:\n“Collecting annotations for induced musical emo-\ntion via online game with a purpose Emotify”,\nwww.cs.uu.nl/research/techreps/UU-CS-2014-\n015.html, 2014.\n[3] H.-T. Cheng, Y .-H. Yang, Y .-C. Lin, I.-B. Liao, and H.\nH. Chen: “Automatic chord recognition for music clas-\nsiﬁcation and retrieval”, IEEE International Confer-\nence on Multimedia and Expo, pp. 1505–1508, 2008.\n[4] J. R. J. Fontaine, K. R. Scherer, E. B. Roesch, and P.\nC. Ellsworth: “The World of Emotions is not Two-\nDimensional”, Psychological Science, V ol. 18, No. 12,\npp. 1050–1057, 2007.\n10This research was supported by COMMIT/.[5] D. Guan, X. Chen, and D. Yang: “Music Emotion\nRegression Based on Multi-modal Features”, CMMR,\np. 70–77, 2012.\n[6] C. A. Harte, and M. B. Sandler: “Detecting harmonic\nchange in musical audio”, Proceedings of Audio and\nMusic Computing for Multimedia Workshop, 2006.\n[7] C. Laurier, O. Lartillot, T. Eerola, and P. Toiviainen:\n“Exploring Relationships between Audio Features and\nEmotion in Music”, Conference of European Society\nfor the Cognitive Sciences of Music, 2009.\n[8] C. Mckay, and I. Fujinaga: “Automatic genre classiﬁ-\ncation using large high-level musical feature sets”, In\nInt. Conf. on Music Information Retrieval, pp. 525–\n530, 2004.\n[9] B. Schuller, J. Dorfner, and G. Rigoll: “Determination\nof Nonprototypical Valence and Arousal in Popular\nMusic: Features and Performances”, EURASIP Jour-\nnal on Audio, Speech, and Music Processing, Special\nIssue on Scalable Audio-Content Analysis pp. 735–\n854, 2010.\n[10] B. Sollberge, R. Rebe, and D. Eckstein: “Musi-\ncal Chords as A\u000bective Priming Context in a Word-\nEvaluation Task”, Music Perception: An Interdisci-\nplinary Journal, V ol. 20, No. 3, pp. 263–282, 2003.\n[11] K. Torres-Eliard, C. Labbe, and D. Grandjean: “To-\nwards a dynamic approach to the study of emotions\nexpressed by music”, Proceedings of the 4th Interna-\ntional ICST Conference on Intelligent Technologies for\nInteractive Entertainment, pp. 252–259, 2011.\n[12] J. K. Vuoskoski, and T. Eerola: “Domain-speciﬁc or\nnot? The applicability of di\u000berent emotion models in\nthe assessment of music-induced emotions”, Proceed-\nings of the 10th International Conference on Music\nPerception and Cognition, pp. 196–199, 2010.\n[13] L. Wedin: “A Multidimensional Study of Perceptual-\nEmotional Qualities in Music”, Scandinavian Journal\nof Psychology, V ol. 13, pp. 241–257, 1972.\n[14] F. Weninger, F. Eyben, B. W. Schuller, M. Mortillaro,\nand K. R. Scherer: “On the Acoustics of Emotion in\nAudio: What Speech, Music, and Sound have in Com-\nmon”, Front Psychol, V ol. 4, p. 292, 2013.\n[15] Y .-H. Yang, Y .-C. Lin, Y .-F. Su, and H. H. Chen: “A\nRegression Approach to Music Emotion Recognition”,\nIEEE Transactions on Audio, Speech, and Language\nProcessing, V ol. 16, No. 2, pp. 448–457, 2008.\n[16] Y .-H. Yang, and H. H. Chen: “Machine Recognition of\nMusic Emotion: A Review”, ACM Trans. Intell. Syst.\nTechnol., V ol. 3, No. 3, pp. 1–30, 2012.\n[17] M. Zentner, D. Grandjean, and K. R. Scherer: “Emo-\ntions evoked by the sound of music: characterization,\nclassiﬁcation, and measurement”, Emotion, V ol. 8,\nNo. 4, pp. 494–521, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n378"
    },
    {
        "title": "The VIS Framework: Analyzing Counterpoint in Large Datasets.",
        "author": [
            "Christopher Antila",
            "Julie Cumming"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417767",
        "url": "https://doi.org/10.5281/zenodo.1417767",
        "ee": "https://zenodo.org/records/1417767/files/AntilaC14.pdf",
        "abstract": "The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strat- egy in VIS, and the flexibility of this approach for future researchers.",
        "zenodo_id": 1417767,
        "dblp_key": "conf/ismir/AntilaC14",
        "keywords": [
            "VIS Framework",
            "modular Python library",
            "big data queries",
            "symbolic musical data",
            "musicological needs",
            "counterpoint study",
            "music21 library",
            "pandas library",
            "style change",
            "contrapuntal patterns"
        ],
        "content": "THE VIS FRAMEWORK: ANALYZING COUNTERPOINT IN LARGE\nDATASETS\nChristopher Antila and Julie Cumming\nMcGill University\nchristopher@antila.ca; julie.cumming@mcgill.ca\nABSTRACT\nTheVIS Framework for Music Analysis is a modular Python\nlibrary designed for “big data” queries in symbolic musical\ndata. Initially created as a tool for studying musical style\nchange in counterpoint, we have built on the music21 and\npandas libraries to provide the foundation for much more.\nWe describe the musicological needs that inspired the\ncreation and growth of the VIS Framework, along with a\nsurvey of similar previous research. To demonstrate the\neffectiveness of our analytic approach and software, we\npresent a sample query showing that the most commonly\nrepeated contrapuntal patterns vary between three related\nstyle periods. We also emphasize our adaptation of typical\nn-gram-based research in music, our implementation strat-\negy in VIS, and the ﬂexibility of this approach for future\nresearchers.\n1. INTRODUCTION\n1.1 Counterpoint\n“The evolution of Western music can be characterized in\nterms of a dialectic between acceptable vertical sonori-\nties on the one hand. . . and acceptable melodic motions on\nthe other.” [12] A full understanding of polyphonic mu-\nsic (with more than one voice or part) requires descrip-\ntion in terms of this dialectic, which is called counterpoint.\nWhereas music information retrieval research (such as [6])\ntypically describes polyphonic music only in terms of ver-\ntical (simultaneous or harmonic) intervals, musicologists\ninterested in contrapuntal patterns also want to know the\nhorizontal (sequential or melodic) intervals in each voice\nthat connect the vertical intervals. Since counterpoint de-\nscribes how pitches in independent voices are combined\nin polyphonic music, a computerized approach to counter-\npoint analysis of symbolic music can provide a wealth of\ninformation to musicologists, who have previously relied\nprimarily on prose descriptions of musical style.1\n1We\nwish to thank the following people for their contributions:\nNatasha Dillabough, Ichiro Fujinaga, Jane Hatter, Jamie Klassen, Alexan-\nder Morgan, Catherine Motuz, Peter Schubert. The ELVIS Project was\nc⃝Christopher Antila\nand Julie Cumming.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Christopher Antila and Julie Cumming.\n“The VIS Framework: Analyzing Counterpoint in Large Datasets”, 15th\nInternational Society for Music Information Retrieval Conference, 2014.\n-2(6\n15)6 8\n+23 7 vert.:\n3\nhoriz.:5\n+2 -3 -36\nFigure\n1. Symbolic score annotated with vertical and hor-\nizontal intervals. A common contrapuntal module appears\nin the box.\nFigure 1 shows the counterpoint between two voices in\na fragment of music. We annotated the vertical intervals\nabove the score, and the lower voice’s horizontal intervals\nbelow. Note that we show intervals by diatonic step size,\ncounting number of lines and spaces between two notes,\nrather than semitones. We describe this contrapuntal mod-\nule further in Section 2.1. By using intervals rather than\nnote names, we can generalize patterns across pitch levels,\nso the same pattern may start on any pitch. For this article,\nwe ignore interval quality (e.g., major or minor third) by\nusing diatonic intervals (e.g., third), allowing generaliza-\ntion across mode and key. We do use interval quality for\nother queries—this is a choice available in VIS at runtime.\nTo allow computerized processing of contrapuntal pat-\nterns, we encode the counterpoint between two voices with\nalternating vertical and horizontal intervals. In Figure 1,\nthe ﬁrst two beats are “3 +2 3.” We call these patterns in-\nterval n-grams, where nis the number of vertical intervals.\nOurn-gram notation system is easily intelligible to music\ntheorists and musicologists, and allows us to stay close to\nmusicology.\n1.2 Research Questions\nUntil recently, musicologists’ ability to accurately describe\npolyphonic textures was severely limited: any one person\ncan learn only a limited amount of music in a lifetime, and\nthe computer-based tools for describing or analyzing poly-\nphonic music in detail are insufﬁciently precise for many\nrepertoires. Descriptions of musical style and style change\nare often vague, derived from intuitive impressions and\npersonal knowledge of repertoire rather than quantiﬁable\nsupported by\nthe Digging into Data Challenge; the Canadian team re-\nsponsible for the work described in this paper was additionally funded by\nthe Social Sciences and Humanities Research Council of Canada.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n71evidence.\nOur project attempts the opposite by quantita-\ntively describing musical style change using counterpoint.\nWe chose counterpoint not only because musicologists\nare already aware of its importance, but because it allows\nus to consider structure in all polyphonic music, which in-\ncludes the majority of surviving Western music created\nafter 1300. Our project’s initial goal is to ﬁnd the most\nfrequently-repeated contrapuntal patterns for different pe-\nriods, genres, and composers, to help form detailed, evid-\nence-based descriptions of style periods and style change\nby knowing which features change over time and when. In\naddition, statistical models will allow a fresh approach to\nattribution problems (determining the composer of a piece\nwhere it is not otherwise known), by enabling us to de-\nscribe some of the factors that distinguish a composer’s\nstyle.\n1.3 The VIS Framework\nOur project’s most important accomplishment is the VIS\nFramework—the software we developed to answer the re-\nsearch questions described above. (VIS stands for “verti-\ncal interval successions,” which is a way to describe coun-\nterpoint). Currently VIS’s primary function is to ﬁnd con-\ntrapuntal patterns in symbolic music, recording them with\nthe notation described above in Figure 1 so they may be\ncounted. However, we designed the framework to allow a\nmuch broader set of queries, and we intend to add sup-\nport for additional musical dimensions (like meter and har-\nmony) as well as more complicated statistical experiments\n(like Markov-chain modeling).\nWe used the Counterpoint Web App, a Web-based user\ninterface for VIS’s counterpoint functionality, to run the\nanalyses presented in this article.2Such Web-based soft-\nware encourages musicologists to participate in data-driven\nanalysis even if they are otherwise unable to program. The\nWeb App’s visual design, the use of musicologist-friendly\nterms and user workﬂows, and the ability to output analy-\nsis results on musical scores are signiﬁcant advantages. At\nthe same time, programmers are encouraged to download\nand extend the VIS Framework using its well-documented\nPython API. While our Framework provides a guide for\nstructuring analysis workﬂows, each analytic step beneﬁts\nfrom our integration of the music21 andpandas libraries.\nTogether, these allow analytic approaches more amenable\nto musicians and statisticians, respectively.3\n2. BACKGROUND\n2.1 Contrapuntal Modules\nAcontrapuntal module is a repeated contrapuntal pattern\nmade from a series of vertical (harmonic) and horizontal\n(melodic) intervals—a repeated interval n-gram. [11] We\nare primarily interested in the frequency and nature of two-\nvoice contrapuntal modules. VIS allows us to computerize\ntedious score analysis previously done by hand, as when\nPeter Schubert identiﬁed modules in Palestrina. [13] While\n2Visitcounterpoint.elvisproject.ca .\n3Refer\ntopandas.pydata.org andmit.edu/music21 .\n84.0\n-262.0 6.0 offset: 0.0\nhoriz.:vert.: 71.5\n11.0\nFigure\n2. “Cadence” contrapuntal module from Figure 1,\nwithmusic21 offset values.\ntwo-voice contrapuntal modules are the primary structural\nelement of much Renaissance music, we can ﬁnd contra-\npuntal modules in nearly all polyphonic music, so our soft-\nware and research strategies will be useful for a wide range\nof music. [3]\nFigure 2 shows a representation of the “7 1 6 -2 8” in-\nterval 3-gram (a 3-gram because there are three vertical\nintervals). Using a half-note rhythmic offset, the ﬁrst verti-\ncal interval is a seventh, the horizontal motion of the lower\npart is a unison (1), there is a vertical sixth, the lower part\nmoves down by a second (-2), and the ﬁnal vertical interval\nis an octave. In modal counterpoint, this is a highly con-\nventionalized ﬁgure used to signal a cadence—a closing or\nconcluding gesture for a phrase or piece. This is the same\n3-gram as in the box in Figure 1.\nImportantly, our analysis method requires that voicing\ninformation is encoded in our ﬁles. MIDI ﬁles where all\nparts are given in the same channel cannot be analyzed use-\nfully with our software.\n2.2 Previous Uses of n-Grams in MIR\nWe have chosen to map musical patterns with n-grams\npartly because of their previous use in natural language\nprocessing.4Some previous uses of n-grams in music anal-\nysis, and computerized counterpoint analysis, are described\nbelow.\nJ. Stephen Downie’s dissertation presents a method for\nindexing melodic n-grams in a large set of folk melodies\nthat will be searched using “Query by Humming” (QBH).\n[7] Downie’s system is optimized for what he calls “lookup,”\nrather than “analysis,” and he admits that it lacks the de-\ntail required by musicologists. Importantly, Downie only\nindexes horizontal intervals: melody rather than counter-\npoint.\nAnother QBH lookup system, proposed by Shyamala\nDoraisamy, adapts n-grams for polyphonic music. [5, 6]\nWhile this system does account for polyphony, it does not\nrecord horizontal intervals so it lacks the detailed contra-\npuntal information we seek. Furthermore, Doraisamy’s in-\ntervals are based on MIDI note numbers rather than the di-\natonic steps preferred by musicologists. Finally, the largest\ninterval allowed by Doraisamy’s tokenization strategy is 26\n4As in\nthe Google Ngram Viewer; refer to\nbooks.google.com/ngrams .\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n72semitones—just o\nver two octaves, and therefore narrower\nthan the normal distance between outer voices even in Re-\nnaissance polyphony. Considering the gradual expansion\nof range in polyphonic music, to the extremes of the late\n19th-century orchestra, the gradual appearance of large in-\ntervals may be an important style indicator.\nMeredith has proposed geometric transformation sys-\ntems for encoding multi-dimensional musical information\nin a computer-friendly way. [9, 10] We especially appreci-\nate the multi-dimensional emphasis and the mathematical\nproperties of these systems, and the software’s ability to\nwork even when voicing information is not available in the\nsymbolic ﬁle.\nFinally, J ¨urgensen has studied accidentals in a large ﬁf-\nteenth-century manuscript of organ intabulations with an\napproach very similar to ours, but carried out using the\nHumdrum Toolkit. [8] She locates cadences by identifying\na contrapuntal model and then records the use of acciden-\ntals at the cadence. While she searches only for speciﬁc\ncontrapuntal modules, we identify all of the n-grams in a\ntest set in order to determine the most frequently recurring\ncontrapuntal patterns.\n2.3 Multi-Dimensional n-Grams in VIS\nConsidering these previous uses of n-grams and counter-\npoint in MIR, we designed our software with the ﬂexi-\nbility to accommodate our requirements, as well as those\nof future analysis strategies. By tokenizing n-grams with\nstrings that minimally transform the input, musicologists\ncan readily understand the information presented in an n-\ngram. This strategy offers a further beneﬁt to programmers,\nwho can easily create n-grams that include different musi-\ncal dimensions without necessarily developing a new token\ntransformation system. Users may choose to implement\nany tokenization strategy on top of our existing n-gram-\nindexing module.\nThe example 3-gram shown in Figure 2 is tokenized in-\nternally as “7 1,” “6 -2,” “8 END.” Although there appear\nto be 2n\u00001tokens, we consider a vertical interval and\nits following horizontal interval as a combined unit—as\nthough it were a letter in an n-gram as used in computa-\ntional linguistics. The simplicity afforded by using strings\nas tokens, each of which may contain an arbitrary array of\nmusical information, has been advantageous.\nIndeed, the difﬁculty of determining a musical analogue\nto the letter, word, and phrase divisions used in computa-\ntional linguistics may be one of the reasons that computer-\ndriven research has yet to gain much traction in mainstream\nmusicology. That music lacks an equivalent for space char-\nacters poses an even greater problem in this regard: while\nsome music does use clear breaks between phrases, their\nexact placement can often be disputed among experts. Mu-\nsicologists also wish to account for the multiple simultane-\nous melody lines of polyphonic music, which has no equiv-\nalent in natural language. These are the primary motivating\nfactors behind our multi-dimensional interval n-gram to-\nkens that encode both vertical and horizontal intervals. As\nour research continues, context models and multiple view-point systems, in the style of Conklin and Witten, will par-\ntially obviate the questions of which nvalue to use, and of\nhow best to incorporate varied musical elements. [1]\nThe popularity of Python within scientiﬁc computing\ncommunities allows us to beneﬁt from any software that\naccepts pandas data objects. The easy-to-learn, object-or-\niented API of music21, along with the relatively high\nnumber of supported ﬁle formats, are also signiﬁcant ad-\nvantages. In the 1980s, a music analysis toolkit consisting\nof a collection of awkscripts was sensible, but Humdrum’s\nlimitation to UNIX systems and a single symbolic ﬁle for-\nmat pose undesirable limitations for a big data project.\n3. EXPERIMENT\n3.1 Data Sets\nWe present an experiment to quantitatively describe style\nchange in the Renaissance period, providing a partial an-\nswer for our primary research question.5We assembled\ntest sets for three similar style periods, named after a rep-\nresentative composer from the period: Ockeghem (1440–\n85), Josquin (1485–1521), and Palestrina (1540–85). The\npieces in the test set were chosen to represent the style pe-\nriod as accurately as our project’s database allowed.6The\ntwenty-year gap between the later periods is a result of less\nsymbolic music being available from those decades. Each\nset consists of a mixture of sacred and secular vocal music,\nmost with four parts, in a variety of genres, from a variety\nof composers. Though we analyzed n-grams between two\nand twenty-eight vertical intervals long, we report our re-\nsults only for 3-grams because they are the shortest contra-\npuntal unit that holds meaning. Note that we include results\nfrom all possible two-part combinations, reﬂecting Renais-\nsance contrapuntal thinking, where many-part textures are\ncomposed from a series of two-part structures. [3,13]\nThe Ockeghem test set consists of 50 ﬁles: 28 in the\nMIDI format and 22 in **kern. For the composers, 8 pieces\nwere written by Busnoys, 32 by Ockeghem, and 10 are late\nworks by Dufay. The longest repeated n-gram was a 25-\ngram.\nThe Josquin test set consists of 56 ﬁles: 18 MIDI, 23\n**kern, 9 MusicXML, and 6 NoteWorthy Composer. For\nthe composers, 3 pieces were written by Agricola, 7 by\nBrumel, 6 by Compre, 2 by Fvin, 12 by Isaac, 19 by Josquin,\n3 by Mouton, 2 by Obrecht, and 2 by la Rue. The longest\nrepeated n-gram was a 28-gram.\nFinally, the Palestrina test set consists of 53 ﬁles: 30\nMIDI, 15 **kern, 6 MusicXML, and 2 NoteWorthy Com-\nposer. For the composers, 15 pieces were written by Pales-\ntrina, 9 by Rore, 28 by Victoria, and 1 by Wert. The longest\nrepeated n-gram was a 26-gram.\n3.2 Methodology\nTheVIS Framework uses a modular approach to query de-\nsign, dividing analysis tasks into a series of well-deﬁned\n5You\nmay download our test sets from\nelvisproject.ca/ismir2014.\n6Visitdatabase.elvisproject.ca.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n73steps.7We\nintend the module break-down to be helpful for\nmusicologists who wish to reason about and design their\nown queries. Thus, musicological concerns drove the cre-\nation of many of the analysis steps, such as the ﬁltering\nmodules described below. The interval n-gram frequency\nexperiment in this article uses the following modules: Note-\nRestIndexer ,IntervalIndexer ,HorizontalIntervalIndexer ,\nFilterByOffsetIndexer ,FilterByRepeatIndexer ,NGramInd-\nexer,ColumnAggregator , and ﬁnally the FrequencyExper-\nimenter.8\nTheNoteRestIndexer ﬁnds note names and rests from\namusic21 Score . The IntervalIndexer andHorizontal-\nIntervalIndexer calculate vertical and horizontal intervals,\nrespectively.\nTheFilterByOffsetIndexer uses a basic algorithm to ﬁl-\nter weak-beat embellishing tones that otherwise obscure\nstructural counterpoint. We regularize observations to a giv-\nen rhythmic offset time interval using the music21 offset,\nmeasured in quarter lengths. Refer to Figure 2 as an exam-\nple, where vertical intervals are ﬁltered with a 2.0 offset.\nEvents beginning on a multiple of that duration will be re-\ntained (like the notes at 0.0, 2.0, and 4.0). Events lasting for\nmultiples of that duration will appear to be repeated (like\nthe note at 4.0, which is also recorded at 6.0). Events not\nbeginning on a multiple of the duration will be removed\n(like the notes at 1.0 and 1.5) or shifted to the following\noffset, if no new event occurs. For this study, we chose a\nhalf-note (2.0) offset interval in accordance with Renais-\nsance notation practices, but this can be changed in VIS at\nruntime.\nTheFilterByRepeatIndexer removes events that are iden-\ntical to the immediately preceding event. Because of its\nplacement in our workﬂow for this experiment, subsequent\nvertical intervals will not be counted if they use the same\npitches. Our interval n-grams therefore necessarily involve\ncontrapuntal motion, which is required for proper pattern\nrecognition. Such repeated events arise in musical scores,\nfor example, when singers recite many words on the same\npitch. The FilterByOffsetIndexer may also create repeated\nevents, as at offset 6.0 in Figure 2. Users may choose not\nto run this module.\nIn this article, our NGramIndexer includes results from\nall pairs of part combination. Users may exclude some com-\nbinations at runtime, choosing to limit their query to the\nhighest and lowest parts, for example. On receiving inter-\nvals from the FilterByRepeatIndexer , the NGramIndexer\nuses the gliding window technique to capture all possible\noverlapping interval n-grams. The indexer also accepts a\nlist of tokens that prevent an n-gram from being counted.\nWe use this feature to avoid counting contrapuntal pat-\nterns that include rests. Finally, the NGramIndexer may\nadd grouping characters, surrounding “vertical” events in\nbrackets and “horizontal” events in parentheses to enhance\nlegibility of long n-grams. The 3-grams in this article are\nshort enough that grouping characters are unnecessary; on\n7This section\nrefers to the 2. xrelease series.\n8For more information about the VIS Framework’s analysis mod-\nules and overall architecture, please refer to our Python API at\nvis.elvisproject.ca.the other hand, the legibility of the “[10] (+2) [9] (1) [8]\n(+2) [7] (1) [6] (-2) [8]” 6-gram found 27 times in the Pal-\nestrina test set greatly beneﬁts from grouping characters.\nTheFrequencyExperimenter counts the number of oc-\ncurrences of each n-gram. These results, still speciﬁc to\npart combinations within pieces, are then combined with\ntheColumnAggregator .\nOn receiving a spreadsheet of results from VIS, we cal-\nculated the number of n-grams as the percentage total of\nalln-grams in each of the test sets. For each set, we also\ncounted the total number of 3-grams observed (including\nall repetitions of all 3-grams), the number of distinct 3-\ngram types (whether repeated or not), and the number of\n3-gram types that occur more than once; these are shown\nbelow in Table 1.\n3.3 Results\nDue to the limited time span represented in this study, we\nwish to suggest avenues for future exploration, rather than\noffer conclusive ﬁndings. We present a visualization of the\nexperimental results in Figure 3, a hybrid between a Venn\ndiagram, word cloud (i.e., a 3-gram cloud), and a time-\nline. The diagram includes interval 3-grams that consti-\ntute greater than 0.2% of the 3-grams in at least one of\nthe test sets. When a 3-gram appears in an intersection of\nstyle periods, that 3-gram constitutes greater than 0.2% of\nthe 3-grams in those sets. As in a world cloud, the font\nsize is scaled proportionately to a 3-gram’s frequency in\nthe test sets in which it is common. Most visually striking\nis the conﬁrmation of musicologists’ existing experiential\nknowledge: certain contrapuntal patterns are common to\nall three style periods, including the cadence module (“7\n1 6 -2 8”) and two other 3-grams that end with the “7 1\n6” cadential suspension. These results make sense because\ncadences are an essential feature of musical syntax.\nTest\nSet Total Types Repeated Types\nOcke\nghem 30,640 10,644 4,509 (42%)\nJosquin 31,233 9,268 4,323 (47%)\nPalestrina 33,339 10,773 5,023 (47%)\nTable\n1. Summary of 3-gram repetitions in our query.\nIn addition to the common cadential patterns noted above,\nboth Figure 3 and Table 1 show evidence of stylistic change\nover time. Most notably, the Josquin and Palestrina test\nsets show a higher level of repetition than the Ockeghem\nset. The number of 3-grams included in Figure 3 is higher\nin the Josquin test set (with seventeen 3-grams) than ei-\nther the Ockeghem or Palestrina sets (both with eleven 3-\ngrams). Yet Table 1 indicates the Josquin and Palestrina\nsets both have a higher percentage of 3-gram types that\nare repeated at least once (47% in both sets, compared to\n42% in the Ockeghem set). These data suggest an increase\nin repetition of contrapuntal modules from the Ockeghem\nto the Josquin generations, which was retained in the Pa-\nlestrina generation. Figure 3 only partially reinforces this\nsuggestion: while ﬁve 3-grams are unique to the Ockeghem\nset, six are unique to the Josquin set, but only one is unique\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n74Figure\n3. Frequency of contrapuntal modules is different\nbetween temporally-adjacent style periods.\nto the Palestrina set. Moreover, the “5 -2 6 -2 6” module,\nunique to the Palestrina set, is the least common 3-gram in\nFigure 3—how did contrapuntal repetition both decrease\nin Palestrina’s generation andremain the same?\nPrevious research by Cumming and Schubert may help\nus explain the data. In 2008, Cumming noted that exact\nrepetition became much more common in Josquin’s life-\ntime than in Ockeghem’s. [2] Schubert showed that com-\nposers tended to repeat contrapuntal patterns in inversion\nduring Palestrina’s lifetime, so that the lower voice is moved\nabove the original upper voice. [13] Inversion changes a\ncontrapuntal pattern’s vertical intervals in a consistent way\nthat preserves, but switches, the horizontal intervals of the\ntwo parts. For example, “7 1 6 -2 8” inverts at the octave\nto “2 -2 3 +2 1.” While humans can recognize both forms\nas two versions of the same pattern, VIS currently shows\nonly exact repetition; future enhancements will permit us\nto equate the original and the inversion. This decision may\nexplain why our data show lower rates of repetition for the\nPalestrina test set.\nWe ﬁnd further evidence of stylistic change in Figure 3:\ncertain patterns that musicologists consider to be common\nacross all Renaissance music are in fact not equally com-\nmon in our three test sets. For example, motion by parallel\nthirds and tenths appears to be more common in certain\nstyle periods than others, and in a way that does not yet\nmake sense. The Palestrina set shares ascending parallel\nthirds (“3 +2 3 +2 3”) with the Ockeghem and descending\nparallel thirds (“3 -2 3 -2 3”) with the Josquin set. Ascend-\ning parallel tenths (“10 +2 10 +2 10”) are more common\nin the Ockeghem set, and descending parallel tenths (“10\n-2 10 -2 10”) in the Josquin set. In particular, descending\nparallel thirds are an order of magnitude less common in\nthe Ockeghem test set than the Josquin or Palestrina (con-\nstituting 0.013%, 0.272%, and 0.225% of 3-grams in their\ntest set, respectively). Conventional musicological wisdom\nsuggests these 3-grams will be equally common in all three\ntest sets, and that parallel tenths will be more common than\nparallel thirds in later style periods, as the range between\nvoices expands. Since the reasons for such a deviation are\nnot yet known, we require further investigation to study the\nchanging nature of contrapuntal repetition during the Re-\nnaissance period. Yet even with these preliminary ﬁndingsit is clear that evidence-based research has much to offer\nmusicology.\n4. FUTURE WORK\nOur research will continue by extending VIS to add the\noption of equivalence classes that can group, for example,\ninversionally-related interval n-grams. We will also build\non previous work with melody- and harmony-focussed mul-\ntiple viewpoint systems to create an all-voice contrapuntal\nprediction model. [1,14]\nOur experiments will continue with larger test sets for\nincreased conﬁdence in our ﬁndings, also adding style peri-\nods earlier than the Ockeghem and later than the Palestrina\nsets, and subdividing our current style periods. This will\nhelp us reassess boundaries between style periods, and ex-\nactly what such a boundary entails. We will also compare\nresults of single pieces with test sets of various sizes.\nFinally, we will implement additional multi-dimension-\naln-gram tokens, for example by adding the note name\nof the lowest voice. This approach would encode Figure 2\nas “7 F 1 6 F -2 8 E.” In Renaissance music, this type of\nn-gram will clarify the relationships between contrapuntal\nmodules and a piece’s mode.\n5. CONCLUSION\nTheVIS Framework for Music Analysis is a musicologist-\nfriendly Python library designed to analyze large amounts\nof symbolic musical data. Thus far, our work has concen-\ntrated on counterpoint—successions of vertical intervals\nand the horizontal intervals connecting them—which some\nscholars view as composers’ primary concern throughout\nthe development of Western music. Our software uses multi-\ndimensional n-grams to ﬁnd and count the frequency of\nrepeated contrapuntal patterns, or modules. In particular,\nby retaining all inputted dimensions and using strings as\ntokens (rather than integers or characters), we simultane-\nously allow musicologists to quickly understand the con-\ntent of an n-gram while also avoiding the challenge of\ndeveloping a new tokenization strategy for every musical\ndimension added to the n-gram. We hope this ﬂexibility\nand ease-of-use encourages musicologists and non-expert\nprogrammers, who would otherwise be discouraged from\ncomputer-based music analysis, to experiment more freely.\nThe results of our query presented in this article, which\ncompares the most commonly-repeated contrapuntal mod-\nules in three Renaissance style periods, show the type of\ninsight possible from computerized music research. The\ntime-consuming effort required for previous work on con-\ntrapuntal modules is greatly reduced when analysts have\naccess to specialized computer software. We analyzed more\nthan 150 polyphonic compositions for interval n-grams be-\ntween two and twenty-eight vertical intervals in length,\nwhich would have taken months or years for a human.\nEven with simple mathematical strategies like counting the\nfrequency of interval n-grams to know which are most com-\nmon, we can conﬁrm existing intuitive knowledge about\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n75the foundations\nof counterpoint while also suggesting av-\nenues for future research on the nature of musical repeti-\ntion.\n6. REFERENCES\n[1] D. Conklin and I. Witten. Multiple viewpoint systems\nfor music prediction. Journal of New Music Research ,\n24(1):51–73, 1995.\n[2] J. Cumming. From variety to repetition: The birth\nof imitative polyphony. In Bruno Bouckaert, Eugeen\nSchreurs, and Ivan Asselman, editors, Yearbook of the\nAlamire Foundation, number 6, pages 21–44. Alamire,\n2008.\n[3] J. Cumming. From two-part framework to movable\nmodule. In Judith Peraino, editor, Medieval music in\npractice: Studies in honor of Richard Crocker , pages\n177–215. American Institute of Musicology, 2013.\n[4] M. S. Cuthbert and C. Ariza. music21: A toolkit for\ncomputer-aided musicology and symbolic music data.\nInProceedings of the International Symposium on Mu-\nsic Information Retrieval , pages 637–42, 2010.\n[5] S. Doraisamy. Polyphonic Music Retrieval: The n-\ngram approach . PhD thesis, University of London,\n2004.\n[6] S. Doraisamy and S. Rger. Robust polyphonic music\nretrieval with n-grams. Journal of Intelligent Informa-\ntion Systems, 21(1):53–70, 2003.\n[7] J. S. Downie. Evaluating a Simple Approach to Music\nInformation Retrieval: Conceiving melodic n-grams as\ntext. PhD thesis, University of Western Ontario, 1999.\n[8] F. J ¨urgensen. Cadential accidentals in the Buxheim or-\ngan book and its concordances: A midﬁfteenth-century\ncontext for musica ﬁcta practice. Acta Musicologica ,\n83(1):39–68, 2011.\n[9] D. Meredith. A geometric language for representing\nstructure in polyphonic music. In Proceedings of the\nInternational Society for Music Information Retrieval ,\npages 133–8, 2012.\n[10] D. Meredith, K. Lemstr ¨om, and G. Wiggins. Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music. Journal of\nNew Music Research , 41(4):321–45.\n[11] J. A. Owens. Composers at Work: The Craft of Musi-\ncal Composition 1450–1600. Oxford University Press,\n1997.\n[12] P. Schubert. Counterpoint pedagogy in the renaissance.\nIn T. Christensen, editor, The Cambridge History of\nWestern Music Theory, pages 503–33. Cambridge Uni-\nversity Press, 2002.[13] P. Schubert. Hidden forms in Palestrina’s ﬁrst book of\nfour-voice motets. Journal of the American Musicolog-\nical Society, 60(3):483–556, 2007.\n[14] R. Whorley, G. Wiggins, C. Rhodes, and M. Pearce.\nMultiple viewpoint systems: Time complexity and the\nconstruction of domains for complex musical view-\npoints in the harmonisation problem. Journal of New\nMusic Research, 42(3):237–66, 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n76"
    },
    {
        "title": "An Association-based Approach to Genre Classification in Music.",
        "author": [
            "Tom Arjannikov",
            "John Z. Zhang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415786",
        "url": "https://doi.org/10.5281/zenodo.1415786",
        "ee": "https://zenodo.org/records/1415786/files/ArjannikovZ14.pdf",
        "abstract": "Music Information Retrieval (MIR) is a multi-disciplin- ary research area that aims to automate the access to large- volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a non- trivial problem in the field, namely music genre classifi- cation, which is one of the core tasks in MIR. In our pro- posed approach, we make use of association analysis to study and predict music genres based on the acoustic fea- tures extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various re- lated issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi- label situation in genre classification.",
        "zenodo_id": 1415786,
        "dblp_key": "conf/ismir/ArjannikovZ14",
        "keywords": [
            "Music Information Retrieval",
            "Automate access to large-volume music data",
            "Music genre classification",
            "Association analysis",
            "Acoustic features",
            "Content-based features",
            "Genre prediction",
            "Feasibility experiments",
            "Publicly available datasets",
            "Multi-label situation"
        ],
        "content": "AN ASSOCIATION-BASED APPROACH\nTO GENRE CLASSIFICATION IN MUSIC\nTom Arjannikov\nUniversity of Lethbridge\ntom.arjannikov@uleth.caJohn Z. Zhang\nUniversity of Lethbridge\nzhang@cs.uleth.ca\nABSTRACT\nMusic Information Retrieval (MIR) is a multi-disciplin-\nary research area that aims to automate the access to large-\nvolume music data, including browsing, retrieval, storage,\netc. The work that we present in this paper tackles a non-\ntrivial problem in the ﬁeld, namely music genre classiﬁ-\ncation, which is one of the core tasks in MIR. In our pro-\nposed approach, we make use of association analysis to\nstudy and predict music genres based on the acoustic fea-\ntures extracted directly from music. In essence, we build\nan associative classiﬁer, which ﬁnds inherent associations\nbetween content-based features and individual genres and\nthen uses them to predict the genre(s) of a new music piece.\nWe demonstrate the feasibility of our approach through a\nseries of experiments using two publicly available music\ndatasets. One of them is the largest available in MIR and\ncontains real world data, while the other has been widely\nused and provides a good benchmarking basis. We show\nthe effectiveness of our approach and discuss various re-\nlated issues. In addition, due to its associative nature, our\nclassiﬁer can assign multiple genres to a single music piece;\nhopefully this would offer insights into the prevalent multi-\nlabel situation in genre classiﬁcation.\n1. INTRODUCTION\nThe recent advances in technology, such as data storage\nand compression, data processing, information retrieval,\nand artiﬁcial intelligence, facilitate music recognition, mu-\nsic composition, music archiving, etc. The Internet is fur-\nther promoting the enormous growth of digital music col-\nlections. Millions of songs previously in physical formats\nare now readily available through instant access, stimulat-\ning and motivating research efforts in meeting new chal-\nlenges. Among them is Music Information Retrieval (MIR),\nan interdisciplinary area that attracts practitioners from in-\nformation retrieval, computer science, musicology, psy-\nchology, etc. One of the main tasks in MIR is the design\nand implementation of algorithmic approaches to manag-\ning large collections of digital music, including automatic\nc\rTom Arjannikov, John Z. Zhang.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Tom Arjannikov, John Z. Zhang. “AN\nASSOCIATION-BASED APPROACH\nTO GENRE CLASSIFICATION IN MUSIC”, 15th International Society\nfor Music Information Retrieval Conference, 2014.tag annotation, recommendation, playlist generation, etc.\nThe work to be presented in this paper explores the\nfeasibility of applying association analysis to music genre\nclassiﬁcation. Through our experience with music data,\nwe have found that there are some inherent associations\nbetween audio characteristics and human assigned music\ngenre labels. Accordingly, it would be desirable to see\nwhether these associations, if found, can provide insight\ninto genre classiﬁcation of music. Our work in this paper\nis geared toward this target.\nIn a nutshell, our proposed approach uses music data it-\nself by extracting useful information from it and conduct-\ning association analysis to make genre prediction. When\nwe talk about the actual sound data of music, we refer\nto whatever is stored on various media, such as magnetic\ntapes and now in the digital format. We can extract useful\ninformation from this data via signal processing. This in-\nformation represents the different characteristics of the ac-\ntual sound stored on media [10]. We refer to it as content-\nbased features and use it with our approach. To our knowl-\nedge, we are among the ﬁrst to propose using association\nanalysis for music genre classiﬁcation in the MIR commu-\nnity.\n2. PREVIOUS WORK\n2.1 Classiﬁcation in MIR\nClassiﬁcation is the process of organizing objects into pre-\ndeﬁned classes. It is a supervised type of learning, where\nwe are given some labeled objects from which we form a\ncomputational model that can be used to classify new, pre-\nviously unseen objects [15].\nClassiﬁcation is one of the core tasks in MIR, since it is\nusually the ﬁrst step in many applications, such as on-line\nmusic retrieval, playlist recommendation, etc. In our work,\nwe focus on genre classiﬁcation, which is concerned with\ncategorizing music audio into different genres. Tzanetakis\nand Cook [18] are among the ﬁrst to work on this problem,\nwhere the task is to label an unknown piece of music with\na correct genre name. They show that this is a difﬁcult\nproblem even for humans and report that college students\nachieve no more than 70% accuracy.\nPrevious works in MIR along this direction include the\nfollowing. DeCoro et al. [5] use Bayesian Model to aid in\nhierarchical classiﬁcation of music by aggregating the re-\nsults of multiple independent classiﬁers and, thus, perform\nerror correction and improve overall classiﬁcation accu-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n95racy. Recent examples of using Support Vector Machines\n(SVM) for music genre classiﬁcation include an investiga-\ntion of Meng and Shawe-Taylor [13], where they explore\ndifferent kernels used in a support vector classiﬁer. Li and\nSleep [9] extend normalized information distance into ker-\nnel distance for SVM and demonstrate classiﬁcation accu-\nracy comparable to others. In addition, recently, Anglade\net al. [3] use Decision Tree for music genre classiﬁcation\nby utilizing frequent chord sequences to induce context\nfree deﬁnite clause grammars of music genres.\n2.2 Association Analysis in MIR\nAssociation analysis attempts to discover the inherent rela-\ntions among data objects in an application domain. These\nrelations are represented as association rules. An example\nof such application domain is the shopping basket analy-\nsis in supermarkets, where one tries to discover relations\namong the items purchased by customers. For example,\nthe association rule fmilk ,eggsg!fbreadg implies that,\nifmilk andeggs are bought together by a customer, then\nbread is likely to be bought as well, i.e., they have some\ninherent statistical relationships [7].\nWe consider the so-called itemsets, such as fmilk ,eggs ,\nbreadg in the above example, to be frequent if they appear\nin many transactions. The support of an itemset represents\nthe percentage of transactions that contain the itemset and\nminimum support is the threshold that separates the fre-\nquent itemsets from the infrequent ones. A frequent item-\nset can produce an association rule of the form A!B,\nwhereAandBare non-empty itemsets and ATB=\u001e.\nAn association rule holds for a dataset with some mini-\nmum support and conﬁdence, which is the percentage of\ntransactions containing Athat also contain B[7].\nA formal treatment of applying association analysis in\nMIR is in Section 3. Within the context of MIR, each track\nor music piece is represented using a set of content-based\nfeatures derived from its digitized data. Together, a set of\nthese features place the given track in a discrete location in\nthe feature space. Intuitively, the tracks that are very sim-\nilar to each other may share the same neighborhood. This\ncould help with organizing music collections for effective\ndata retrieval. When grouped together, the features contain\nsome patterns. We would like to look for these patterns and\nuse them for music genre classiﬁcation.\nKuo et al. [8] propose a way to recommend music based\non the emotion that it conveys and look for associations in\ndata that contains information perceived only by humans.\nSimilarly, Xiao et al. [19] use a parameterized statistical\nmodel to look for associations between timbre and per-\nceived tempo. Liao et al. [12] use a dual-wing harmo-\nnium model to discover association patterns between MTV\nvideo clips and the music that accompanies those clips.\nNeubarth et al. [14] present a method of association rule\nmining with constraints and discover rules in the form of\nA!B, telling that either region implies genre or genre\nimplies region. Arjannikov et al. [4] use association analy-\nsis to verify tag annotation in music, though their approach\nis based on textual music tags and is not content-based.Our work to be presented below is different from the above\nand is among the initial efforts to apply association analy-\nsis to content-based music genre classiﬁcation.\n3. CLASSIFYING MUSIC INTO GENRES VIA\nASSOCIATION ANALYSIS\nOur work in this paper is focused on the music genre tags.\nAs stated in [6, 10, 10], any discrete set of tags that are not\ncorrelated can be used as categories, or classes, into which\nwe could split a collection of music pieces. Arjannikov\net al. [4] show that association analysis reveals patterns in\nmusic textual tags. This motivates our investigation of as-\nsociation analysis in content-based music features.\n3.1 Notation\nAssociation analysis requires discrete items, however, most\ncontent-based music features are not. Thus, when given a\nset of features F=ff1,f2,f3,\u0001\u0001\u0001,fkg, we discretize\neach feature into a predetermined number of bins b, where\nb > 1, and derive a new feature set F0=ff0\n11,f0\n12,\u0001\u0001\u0001,\nf0\n1b,f0\n21,f0\n22,\u0001\u0001\u0001,f0\n2b,\u0001\u0001\u0001,f0\nk1,f0\nk2,\u0001\u0001\u0001,f0\nkbg. Then, from\nthe set of music pieces M, we derive a transactional style\ndatasetD=fd1,d2,\u0001\u0001\u0001,drg, wherer=jMj. Each\ntransactiondi=fa1,a2,\u0001\u0001\u0001,akgcorresponds to a music\npiece and each ajindiis a feature item in the literal form\nFpBq, wherepcorresponds to the feature number in F0\nandqcorresponds to the bin number, into which the fea-\nture for the particular music piece falls. For example, if\nthe ﬁrst content-based feature is a number between 0and\n1, and it is discretized into 10equidistant bins, then, given\na particular music piece, whose ﬁrst feature value is 0:125,\nits corresponding diwould contain the label F1B2.\nWhen we formulate our problem as described above,\nthe music set M, becomes a transactional set Dsuitable\nfor association mining.\n3.2 Proposed Approach\nWe call our proposed approach association-based music\ngenre classiﬁer (AMGC). Figure 1 depicts the whole pro-\ncess of using AMGC, which is detailed below.\n3.2.1 AMGC\nWe start by preparing our data during the pre-processing\nstage. First, we acquire content-based features from mu-\nsic; in this paper, we use the features that have already\nbeen extracted and published for the purpose of compar-\ning different classiﬁers on even ground [16, 17]. Then, we\ndiscretize any continuous features. It is worth noting that\nobtaining optimal discretization is an open problem in ma-\nchine learning. In our work, we use feature discretization\nbased on equal width of bins, for its simplicity, to avoid any\npossible bias based on class labels. Then we form transac-\ntional style datasets, as described in Section 3.1, and split\nthe training dataset into subsets, one for each genre. Fi-\nnally, we remove any items that appear in all transactions\nwith a certain frequency threshold (FRQ), which is the per-\ncent of transactions containing the item.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n96Figure 1. The three stages of our proposed association-based approach to classify music into genres.\nDuring the training stage, we invoke the Apriori algo-\nrithm [1, 2] and mine frequent itemsets from each genre’s\nsets of items at some minimum support. From these we\ngenerate classiﬁcation rules of the form A!B, whereA\nis the frequent itemset and Bis the genre associated with\nthat itemset. Then, we remove any itemsets that appear in\ntwo or more genres. The resulting rules uniquely represent\ntheir respective genres and we use them for classiﬁcation\nduring the last stage.\n3.2.2 Scoring Method\nTo obtain a classiﬁcation score for each genre, we use the\nfollowing four components. Itemset Percentage (IP) is the\npercent of itemsets that a given music piece matches for\na given genre out of all itemsets matched from that genre.\nSupport Sum (SS) is the sum of the matched itemsets’ min-\nimum support divided by the sum of all itemsets’ mini-\nmum support for the given genre. Conﬁdence Sum (CS)\nis the current genre’s conﬁdence sum of the matched item-\nsets divided by the sum of all itemsets’ conﬁdence. Finally,\nLength Sum (LS), the sum of cardinalities of the matched\nitemsets divided by the sum of cardinalities of all itemsets\nfor the given genre.\nWe score each music piece against each genre’s set of\nrules as following. First, we create a voting vector, whose\ncardinality is equal to the number of genres, and com-\npute the corresponding component’s value for each genre.\nThen, the genre with the highest value is voted as a candi-\ndate of that component, and its element in the voting vector\nis incremented by 1. Thus, the four components result in\nfour votes and the genre with the highest number of votes\nis declared as winner and becomes the predicted genre of\nthe given music piece.\n3.2.3 Accuracy Evaluations\nIn our work, we use the following classiﬁcation measures.\nRecall, also known as sensitivity, represents the percentage\nof correctly classiﬁed instances for that genre [7]. Preci-\nsionreﬂects the percentage of correctly classiﬁed instances\nfrom all instances that are perceived as belonging to that\ngenre by the classiﬁer [7]. Finally, accuracy is calculated\nby dividing the number of all correctly classiﬁed instances\nfor all genres by the total number of predictions made [7].Because AMGC can assign multiple genre labels to a\nsingle music piece, we compute the Multi-Labeling Rate\n(MLR) by dividing the total number of predicted labels by\nthe number of all test instances of a genre. MLR falls into\nthe range between 1and the total number of genres with\nfrequent itemsets. The closer it is to 1, the fewer multi-\nlabel assignments were made, which indicates that AMGC\nis performing more like a single-label classiﬁer. If MLR\nis equal to the total number of genres, then the results of\nclassiﬁcation are least useful. Furthermore, if MLR is be-\nlow1, then there are music pieces, whose genres could not\nbe predicted.\n3.3 Goals\nOur aim is to test whether the classiﬁcation rules obtained\nfrom music content-based features by AMGC can be used\nto categorize music into genres. For this, we designate\nthree goals: (G 1) AMGC achieves a classiﬁcation accuracy\nthat is better than choosing genres at random; (G 2) AMGC\nis stable - when given similar datasets, it should achieve\nsimilar classiﬁcation accuracy; (G 3) AMGC attains higher\naccuracy with better quality data and fewer genres.\n4. EXPERIMENT RESULTS AND DISCUSSIONS\n4.1 Data Preparation\nThe classiﬁcation task at hand requires content-based fea-\ntures paired with genre tags and we ﬁnd two datasets that\nﬁt this requirement.\nThe Latin Music Database [17], denoted as DLMD, is\npopular in the music genre classiﬁcation task despite its\nsmall size. There are many classiﬁcation results available\nin the literature, which are based on a set of features that\nhas already been extracted and circulated as part of DLMD.\nThus, we can test the feasibility of our approach without\nintroducing variance based on difference in feature extrac-\ntion techniques. Moreover, DLMD usually results in high\nclassiﬁcation accuracy for many methods [17]. We use one\nof the three sets of features included with it, which is ex-\ntracted from the beginning 30 seconds of each music piece.\nTheMillion Song Dataset Benchmarking [16], denoted\nasDMSDB , is much larger than DLMD and boasts several\nsets of content-based features. We use ﬁve of these sets\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n97and the genre labels, which were originally obtained from\nAllmusic [16]. Additionally, we restrict the number of\ntracks to 1000 per genre, in order to balance the number\nof training and testing examples among genres.\nDataset Number of Number of Number of Type of\nname songs genres features Features\nDLMD 3000 10 30 MFCC\nDMSDB-1 1500 15 10 MM\nDMSDB-2 1500 15 16 Spectral\nDMSDB-3 1500 15 20 LPC\nDMSDB-4 1500 15 20 AM\nDMSDB-5 1500 15 26 MFCC\nTable 1. Music genre datasets and their statistics.\nWe include some statistical information about the data-\nsets in Table 1 and label them accordingly. We split each\none into two equal-sized partitions at random, while main-\ntaining the genres balanced; each genre is represented by\nequal number of tracks in both partitions. One of the par-\ntitions becomes the training set and the other becomes the\ntesting set. If there are too many music pieces belonging\nto one genre as compared to others, we remove the extra\ntracks at random. If a genre is represented by fewer pieces\nthan 300forDLMD and1000 forDMSDB , then we do not\nuse that genre in our experiments. This reduces the origi-\nnalDMSDB dataset to 17genres from 25. Moreover, during\nStage 2 of our proposed approach, when we mine frequent\nitemsets, two of the genres produce none; therefore, only\n15genres persist, as reported in Table 1. DLMD remains at\n10genres because it was originally balanced at 300music\npieces per genre.\nIn the following section, we demonstrate through our\nexperiment results how we achieve the three goals formu-\nlated in Section 3.3.\n4.2 Results and Discussions\n 0.25 0.3 0.35 0.4 0.45 0.5\n23456789101520253040Accuracy\nNumber of Bins  FRQ=95%\n  FRQ=90%\n  FRQ=85%\n  FRQ=80%\n  FRQ=75%\n  FRQ=70%\n  FRQ=65%\n  FRQ=60%\nFigure 2.DLMD at minimum support = 20%.\nDuring our experiments, we observe that our proposed\nparameters affect the classiﬁcation accuracy, and thus, they\nare effective. It is evident from Figures 2 and 3 that the\nnumber of discretization bins affects the classiﬁcation ac-\ncuracy for both DLMD andDMSDB . Figure 4 demonstrates\nhow the classiﬁcation accuracy is affected by the minimum\nsupport parameter. We also note that AMGC performs\n 0.15 0.16 0.17 0.18 0.19 0.2 0.21 0.22 0.23 0.24\n678910152025304050607090120150Accuracy\nNumber of Bins  FRQ=95%\n  FRQ=90%\n  FRQ=85%\n  FRQ=80%\n  FRQ=75%\n  FRQ=70%\n  FRQ=65%\n  FRQ=60%Figure 3.DMSDB-2 at minimum support = 2%.\nmuch better than if we were to choose genres at random.\nThus, we conﬁrm that AMGC works for some parameter\nsettings and conclude our work towards G1.\nAs demonstrated in the literature, the classiﬁcation ac-\ncuracy usually increases when the number of classes is\nreduced [11]. Thus, we reduce the number of genres for\nbothDLMD andDMSDB to5and observe that AMGC per-\nforms better. Therefore, we report only the results for the\nsmaller set of genres in Figures 4 through 9. We also ob-\nserve thatDLMD achieves higher accuracy than DMSDB as\ncan be seen in Figures 2 and 3. This concludes our work\ntowardsG3, as AMGC performs better with a better qual-\nity dataset, moreover, it performs better on a reduced set of\ngenres.\n 0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58\n30252019181716151413121110987654321Accuracy\nMinimum Support  FRQ=95%\n  FRQ=90%\n  FRQ=85%\n  FRQ=80%\n  FRQ=75%\n  FRQ=70%\n  FRQ=65%\n  FRQ=60%\nFigure 4.DMSDB-2 withnumberof bins = 20.\n 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65\n2468101214161820Accuracy\nMinimum Support  MSD-MFCC\n  MSD-SPEC\n  MSD-MM\n  MSD-AM\n  MSD-LPC\n  MSD-MFCC-11\n  MSD-AM-19\nFigure 5. All ﬁve DMSDB datasets compared, with\nnumberof bins = 13, unless otherwise speciﬁed.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n98 0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58\n234567891011121314151618202530354045Evaluation Measure\nMinimum Support  MSD-SPEC-15b-AC\n  MSD-SPEC-15b-RE\n  MSD-SPEC-15b-PR\n  MSD-SPEC-20b-AC\n  MSD-SPEC-20b-RE\n  MSD-SPEC-20b-PRFigure 6.DMSDB-2 across different minimum support.\n 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6\n45678910152025304050607090120150Evaluation Measure\nNumber of Bins  MSD-SPEC-2s-AC\n  MSD-SPEC-2s-RE\n  MSD-SPEC-2s-PR\n  MSD-SPEC-10s-AC\n  MSD-SPEC-10s-RE\n  MSD-SPEC-10s-PR\nFigure 7.DMSDB-2 across different number of bins.\n 0 1 2 3 4 5 6\n345678910152025304050607090120150Genres Present\nNumber of Bins  LMD-MFCC-90s-GP\n  MSD-MFCC-90s-GP\n  LMD-MFCC-20s-GP\n  MSD-MFCC-20s-GP\nFigure 8. 5 genres of DLMD andDMSDB-5 atFRQ = 95.\nIt is clear from Figures 2, 3 and 4, that the FRQ parame-\nter does not signiﬁcantly affect the classiﬁcation accuracy,\nalthough, it produces highest accuracy overall when set to\n95%. We use this setting in all of the experiment results in\nFigures 5 through 9.\nDuring our experiments, we observe that DMSDB data-\nsets perform best at lower minimum support and number of\nbins settings. We set the number of bins to 13and perform\na sweep across minimum support values between 2and\n20. As can be seen in Figure 5, among all ﬁve, DMSDB-2\nperforms the best and DMSDB-4 the worst. Three of the ﬁve\ndatasets achieve their highest accuracy when the number\nof bins is set to 13; however, DMSDB-4 performs better at\n19bins, andDMSDB-5 at11bins. Thus, we include the\ncorresponding results in Figure 5.\n 0 1 2 3 4 5\n345678910152025304050607090120150Multi-label Rate\nNumber of Bins  LMD-MFCC-90s-MLR\n  MSD-MFCC-90s-MLR\n  LMD-MFCC-20s-MLR\n  MSD-MFCC-20s-MLRFigure 9. 5 genres of DLMD andDMSDB-5 atFRQ = 95.\nWe observe that all three evaluation measures, recall ,\nprecision, and accuracy , obtain very similar values to\neach other in our experiments, as can be seen in Figures\n6 and 7. It can also be seen in Figures 2 through 7, that\nAMGC does not behave arbitrarily, when given different\ndatasets or different parameter settings. This conﬁrms that\nour approach is stable and concludes our work towards G2.\nDuring our experiments, we notice that for some val-\nues of minimum support and for some numbers of bins,\nAMGC performs much better than choosing genre assign-\nment at random. However, with other values of these pa-\nrameters, AMGC predicts majority of music to be of one\ngenre. Moreover, sometimes it votes for all genres equally,\nwhere MLR becomes equal to the number of genres. Fur-\nthermore, we encountered certain parameter settings, when\nsome or all genres were not represented by any classiﬁ-\ncation rules. We investigate the behaviour of MLR and\nthe number of genres present in both DLMD andDMSDB\nthrough further experiments and report our ﬁndings in Fig-\nures 8 and 9. Here, we set the minimum support to 20and\nthen to 90for both datasets. As can be seen in Figure 8, at\nthe higher minimum support, some genres are discarded,\ndue to removal of intersections during Stage 2 of our ap-\nproach. Meanwhile, Figure 9 illustrates that AMGC be-\nhaves as a single label classiﬁer, because we remove rules\nthat are found among any genre-pair, thus, the remaining\nrules are representative of a single genre.\nWhen experimenting with our approach on music genre\nclassiﬁcation using different features in DMSDB , we use\nthe same genre assignment and alternate the features. This\nhelps us conﬁrm that difference in content-based features\nresult in different classiﬁcation performance. Hence, dif-\nferent features are more or less useful for the genre clas-\nsiﬁcation task, which is reﬂected by the feature selection\ntask in MIR.\nIn our experiments, we notice that it may take a long\ntime to pre-process the data and train the classiﬁer. How-\never, the resulting classiﬁcation model is very fast, where\nits speed can be expressed as the number of classiﬁcation\nrules multiplied by the number of music pieces to be clas-\nsiﬁed.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n995. CONCLUSION\nIn this paper, we introduce a novel approach to MIR, name-\nly, using association analysis to help music genre classi-\nﬁcation. Association analysis looks for frequent patterns\nin music data, which represent the similarity of all music\npieces in a given genre.\nThrough experiments, we demonstrate the effectiveness\nof our approach and conﬁrm that association analysis can\nbe applied to music data. However, there is still room\nfor improvement, which includes feature extraction, fea-\nture selection and discretization. We believe that as they\nimprove, our method will also improve. We can also take\nsome immediate steps to improve our classiﬁer by tuning\nthe two parameters, minimum support for mining frequent\nitems and the number of discretization bins. Our experi-\nments demonstrate that these two parameters are directly\nrelated to the performance of our classiﬁer, and they vary\ndepending on the data. Hence, tuning these parameters to\neach speciﬁc dataset will improve the classiﬁcation accu-\nracy. We leave these to our future work.\n6. REFERENCES\n[1] R. Agrawal, T. Imieli ´nski, and A. Swami. Mining asso-\nciation rules between sets of items in large databases.\nInProceedings of the ACM SIGMOD International\nConference on Management of Data, volume 22, pages\n207–216. ACM, 1993.\n[2] R. Agrawal and R. Srikant. Fast algorithms for mining\nassociation rules in large databases. In Proceedings of\nthe 20th International Conference on Very Large Data\nBases, volume 1215, pages 487–499. Morgan Kauf-\nmann Publishers Inc., 1994.\n[3] A. Anglade, R. Ramirez, and S. Dixon. Genre classi-\nﬁcation using harmony rules induced from automatic\nchord transcriptions. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 669–674. ISMIR, 2009.\n[4] T. Arjannikov, C. Sanden, and J. Z. Zhang. Verifying\ntag annotations through association analysis. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference, pages 195–200. ISMIR,\n2013.\n[5] C. DeCoro, Z. Barutcuoglu, and R. Fiebrink. Bayesian\naggregation for hierarchical genre classiﬁcation. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference, pages 77–80. ISMIR,\n2007.\n[6] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of\naudio-based music classiﬁcation and annotation. IEEE\nTransactions on Multimedia, 13(2):303–319, 2011.\n[7] J. Han and M. Kamber. Data Mining: Concepts and\nTechniques. Morgan Kaufmann Publishers Inc., the\nsecond edition, 2006.[8] F.-F. Kuo, M.-F. Chiang, M.-K. Shan, and S.-Y . Lee.\nEmotion-based music recommendation by association\ndiscovery from ﬁlm music. In Proceedings of the 13th\nAnnual ACM International Conference on Multimedia,\npages 507–510. ACM, 2005.\n[9] M. Li and R. Sleep. Genre classiﬁcation via an lz78-\nbased string kernel. In Proceedings of the International\nSociety for Music Information Retrieval Conference,\npages 252–259. ISMIR, 2005.\n[10] T. Li, O. Mitsunori, and G. Tzanetakis, editors. Music\nData Mining. CRC Press, 2012.\n[11] T. Li, M. Ogihara, and Q. Li. A comparative study on\ncontent-based music genre classiﬁcation. In Proceed-\nings of the 26th Annual International ACM SIGIR Con-\nference on Research and Development in Informaion\nRetrieval, pages 282–289. ACM, 2003.\n[12] C. Liao, P. Wang, and Y . Zhang. Mining association\npatterns between music and video clips in professional\nmtv. In B. Huet, A. Smeaton, K. Mayer-Patel, and\nY . Avrithis, editors, Advances in Multimedia Mod-\nelling, volume 5371 of Lecture Notes in Computer\nScience, pages 401–412. Springer Berlin Heidelberg,\n2009.\n[13] A. Meng and J. Shawe-Taylor. An investigation of fea-\nture models for music genre classiﬁcation using the\nsupport vector classiﬁer. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference, pages 604–609. ISMIR, 2005.\n[14] K. Neubarth, I. Goienetxea, C. Johnson, and D. Con-\nklin. Association mining of folk music genres and to-\nponyms. In Proceedings of the International Society\nfor Music Information Retrieval Conference, pages 7–\n12. ISMIR, 2012.\n[15] S. Russel and P. Norvig. Artiﬁcial Intelligence: A Mod-\nern Approach. Pearson education Inc., the third edition,\n2010.\n[16] A. Schindler, R. Mayer, and A. Rauber. Facilitating\ncomprehensive benchmarking experiments on the mil-\nlion song dataset. In Proceedings of the International\nSociety for Music Information Retrieval Conference,\npages 469–474. ISMIR, 2012.\n[17] C. N. J. Silla, C. A. A. Kaestner, and A. L. Koerich.\nThe latin music database. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference, pages 451–456. ISMIR, 2008.\n[18] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and\nAudio Processing, 10(5):293–302, July 2002.\n[19] L. Xiao, A. Tian, W. Li, and J. Zhou. Using a statistic\nmodel to capture the association between timber and\nperceived tempo. In Proceedings of the International\nSociety for Music Information Retrieval, pages 659–\n662. ISMIR, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n100"
    },
    {
        "title": "Tempo- and Transposition-invariant Identification of Piece and Score Position.",
        "author": [
            "Andreas Arzt",
            "Gerhard Widmer",
            "Reinhard Sonnleitner"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415850",
        "url": "https://doi.org/10.5281/zenodo.1415850",
        "ee": "https://zenodo.org/records/1415850/files/ArztWS14.pdf",
        "abstract": "We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo- invariant symbolic fingerprinting method, replacing the ab- solute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big de- crease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verifi- cation step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the re- trieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications.",
        "zenodo_id": 1415850,
        "dblp_key": "conf/ismir/ArztWS14",
        "keywords": [
            "algorithm",
            "audio performance",
            "database of musical scores",
            "tempo-invariant",
            "transposition-invariant",
            "symbolic fingerprinting",
            "relative representation",
            "discriminative power",
            "noise",
            "tracking algorithm"
        ],
        "content": "TEMPO- AND TRANSPOSITION-INV ARIANT IDENTIFICATION OF\nPIECE AND SCORE POSITION\nAndreas Arzt1, Gerhard Widmer1;2, Reinhard Sonnleitner1\n1Department of Computational Perception, Johannes Kepler University, Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\nandreas.arzt@jku.at\nABSTRACT\nWe present an algorithm that, given a very small snippet\nof an audio performance and a database of musical scores,\nquickly identiﬁes the piece and the position in the score.\nThe algorithm is both tempo- and transposition-invariant.\nWe approach the problem by extending an existing tempo-\ninvariant symbolic ﬁngerprinting method, replacing the ab-\nsolute pitch information in the ﬁngerprints with a relative\nrepresentation. Not surprisingly, this leads to a big de-\ncrease in the discriminative power of the ﬁngerprints. To\novercome this problem, we propose an additional veriﬁ-\ncation step to ﬁlter out the introduced noise. Finally, we\npresent a simple tracking algorithm that increases the re-\ntrieval precision for longer queries. Experiments show that\nboth modiﬁcations improve the results, and make the new\nalgorithm usable for a wide range of applications.\n1. INTRODUCTION\nEfﬁcient algorithms for content-based retrieval play an im-\nportant role in many areas of music retrieval. A well known\nexample are audio ﬁngerprinting algorithms, which permit\nthe retrieval of all audio ﬁles from the database that are\n(almost) exact replicas of a given example query (a short\naudio excerpt). For this task there exist efﬁcient algorithms\nthat are in everyday commercial use (see e.g. [4], [13]).\nA related task, relevant especially in the world of classi-\ncal music, is the following: given a short audio excerpt of\na performance of a piece, identify both the piece (i.e. the\nmusical score the performance is based on), and the posi-\ntion within the piece. For example, when presented with an\naudio excerpt of Vladimir Horowitz playing Chopin’s Noc-\nturne Op. 55 No. 1, the goal is to return the name and data\nof the piece (Nocturne Op. 55 No. 1 by Chopin) rather than\nidentifying the exact audio recording. Hence, the database\nfor this task does not contain audio recordings, but sym-\nbolic representations of musical scores. This is related to\nversion identiﬁcation (see [11] for an overview), where the\nc\rAndreas Arzt1, Gerhard Widmer1;2, Reinhard\nSonnleitner1.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Andreas Arzt1, Gerhard Widmer1;2,\nReinhard Sonnleitner1. “Tempo- and Transposition-invariant Identiﬁca-\ntion of Piece and Score Position”, 15th International Society for Music\nInformation Retrieval Conference, 2014.goal is to identify different versions of one and the same\nsong, mostly in order to detect cover versions in popular\nmusic.\nA common way to solve this task, especially for clas-\nsical music, is to use an audio matching algorithm (see\ne.g. [10]). Here, all the scores are ﬁrst transformed into\naudio ﬁles (or a suitable in-between representation), and\nthen aligned to the query in question, most commonly with\nalgorithms based on dynamic programming techniques. A\nlimitation of this approach is that relatively large queries\nare needed (e.g. 20 seconds), to achieve good retrieval re-\nsults. Another problem is computational cost. To cope\nwith this, in [8] clever indexing strategies were presented\nthat greatly reduce the computation time.\nIn [2] an approach is presented that tries to solve the\ntask in the symbolic domain instead. First, the query is\ntransformed into a symbolic list of note events via an audio\ntranscription algorithm. Then, a globally tempo-invariant\nﬁngerprinting method is used to query the database and\nidentify matching positions. In this way even for queries\nwith lengths of only a few seconds very robust retrieval\nresults can be achieved. A downside is that this method\ndepends on automatic music transcription, which in gen-\neral is an unsolved problem. In [2] a state of the art tran-\nscription system for piano music is used, thus limiting the\napproach to piano music only, at least for the time being.\nIn addition, we identiﬁed two other limitations of this\nalgorithm, which we tackle in this paper. First, the ap-\nproach depends on the performer playing the piece in the\ncorrect key and the correct octave (i.e. in the same key\nand octave as it is stored in the database). In music it\nis quite common to transpose a piece of music accord-\ning to speciﬁc circumstances, e.g. a singer preferring to\nsing in a speciﬁc range. Secondly, while this algorithm\nworks very well for small queries, larger queries with local\ntempo changes within the query tend to be problematic. Of\ncourse these limitations were already discussed in the lit-\nerature for other approaches, see e.g. [10] for tempo- and\ntransposition-invariant audio matching.\nIn this paper we present solutions to both problems by\nproposing (1) a transposition-invariant ﬁngerprinting meth-\nod for symbolic music representations which uses an ad-\nditional veriﬁcation step that largely compensates for the\ngeneral loss in discriminative power, and (2) a simple but\neffective tracking method that essentially achieves not only\nglobal, but also local invariance to tempo changes.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5492. TEMPO-INV ARIANT FINGERPRINTING\nThe basis of our algorithm is a ﬁngerprinting method pre-\nsented in [2] (which in turn is based on [13]) that is invari-\nant to the global tempo of both the query and the entries\nin the database. In this section we will give a brief sum-\nmary of this algorithm. Then we will show how to make\nittransposition-invariant (Section 3) and how to make it\ninvariant to local tempo changes (Section 4).\n2.1 Building the Score Database\nIn [2] a ﬁngerprinting algorithm was introduced that is in-\nvariant to global tempo differences between the query and\nthe scores in the database. Each score is represented as an\nordered list of [ontime, pitch] pairs, which in turn are ex-\ntracted from MIDI ﬁles with a suitable but constant tempo\nfor the whole piece.\nFor each score, ﬁngerprint tokens are generated and stor-\ned in a database. Tokens are created from triplets of note-\non events according to some constraints to make them tem-\npo invariant. A ﬁxed event eis paired with the ﬁrst n1\nevents with a distance of at least dseconds “in the fu-\nture” of e. This results in n1event pairs. For each of\nthese pairs this step is repeated with the n2future events\nwith a distance of at least dseconds. This ﬁnally results\ninn1\u0003n2event triplets. In our experiments we used the\nvalues d= 0:05seconds and n1=n2= 5 (i.e. for each\nevent 25 tokens are created). The pair creation steps are\nconstrained to notes which are at most 2 octaves apart.\nGiven such a triplet consisting of the events e1,e2and\ne3, the time difference td1;2between e1ande2and the\ntime difference td2;3between e2ande3are computed. To\nget a tempo independent ﬁngerprint token, the ratio of the\ntime differences is computed: tdr=td2;3\ntd1;2. This ﬁnally\nleads to a ﬁngerprint token dbtoken = [pitch 1:pitch 2:\npitch 3:tdr] :pieceID :time :td1;2, with the hash\nkey being [pitch 1:pitch 2:pitch 3:tdr],pieceID the\nidentiﬁer of the piece, and time the onset time of e1. The\ntokens in our database are unique, i.e. we only insert the\ngenerated token if an equivalent one does not exist yet.\n2.2 Querying the Database\nBefore querying the database, the query (an audio snippet\nof a performance) has to be transformed into a symbolic\nrepresentation. The algorithm we use to transcribe musical\nnote onsets from an audio signal is based on the system\ndescribed in [3]. The result of this step is a possibly very\nnoisy list of [ontime, pitch] pairs.\nThis list is processed in exactly the same fashion as\nabove, resulting in a list of tokens of the form qtoken =\n[qpitch 1:qpitch 2:qpitch 3:qtdr] :qtime :qtd1;2.\nThen, all the tokens which match hash keys of the query\ntokens are extracted from the database (we allow a maxi-\nmal deviation of the ratio of the time differences of 15%).\nFor querying, the general idea is to ﬁnd regions in the\ndatabase of scores which share a continuous sequence of\ntokens with the query. To quickly identify these regions\nwe use the histogram approach presented in [2] and [13].This is a computationally inexpensive way of ﬁnding these\nsequences by sorting the matched tokens into a histogram\nwith a bin width of 1 second such that peaks appear at the\nstart points of these regions (i.e. the start point where the\nquery matches a database position). We also included the\nrestriction that each query token can only be sorted at most\nonce into each bin of the histogram, effectively preventing\nexcessively high scores for sequences of repeated patterns\nin a brief period of time.\nThe matching score for each score position is computed\nas the number of tokens in the respective histogram bin. In\naddition, we can also compute a tempo estimate, i.e. the\ntempo of the performance compared to the tempo in the\nscore, by taking the mean of the ratios of td1;2andqtd1;2\nof the respective matching query and database tokens that\nwere sorted in the bin in question. We will use this infor-\nmation for the tracking approach presented in Section 4.\n3. TRANSPOSITION-INV ARIANT\nFINGERPRINTS\n3.1 General Approach\nIn the algorithm described above, the pitches in the hash\nkeys are represented as absolute values. Thus, if a per-\nformer decides to transpose a piece by an arbitrary number\nof semi-tones, any identiﬁcation attempt by the algorithm\nmust fail.\nTo overcome this problem, we suggest a simple, relative\nrepresentation of the pitch values, which makes the algo-\nrithm invariant to linear transpositions. Instead of using 3\nabsolute pitch values, we replace them by 2 differences,\npd1=pitch 2\u0000pitch 1andpd2=pitch 3\u0000pitch 2, re-\nsulting in a hash key [pd1:pd2:tdr]. For use in Section\n3.2 below we additionally store pitch 1, the absolute pitch\nof the ﬁrst note, in the token value.\nIn every other aspect the algorithm works in the same\nway as the purely tempo-invariant version described above.\nOf course this kind of transposition invariance cannot come\nfor free as the resulting ﬁngerprints will not be as discrim-\ninative as before. This has two important direct conse-\nquences: (1) the retrieval accuracy will suffer, and (2) for\nevery query a lot more matching tokens are found in the\ndatabase, thus the runtime for each query increases (see\nSection 5).\n3.2 De-noising the Results: Token Veriﬁcation\nTo compensate for the loss in discriminative power we pro-\npose an additional step before accepting a database token\nas a match to the query. The general idea is taken from [9]\nand was ﬁrst used in a music context by [12]. It is based\non a veriﬁcation step for each returned token that looks at\nthe context within the query and the context at the returned\nposition the database.\nEach token dbtoken that was returned in response to\naqtoken can be used to project the query (i.e. the notes\nidentiﬁed from the query audio snippet by the transcrip-\ntion algorithm) to the possibly matching position in the\nscore indicated by the dbtoken. The intuition then is that at\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n550true matching positions we will ﬁnd a majority of the notes\nfrom the query at their expected positions in the score. This\nwill permit us to more reliably decide if the match of hash\nkeys is a false positive or an actual match.\nTo do this, we need to compute the pitch shift and the\ntempo difference between the query and the potential po-\nsition in the database. The pitch shift is computed as the\ndifference of the pitch 1ofqtoken anddbtoken . The dif-\nference in tempo is computed as the ratio of td1;2of the\ntwo tokens. This information can now in turn be used to\ncompute the expected time and pitch for each query note\nat the current score position hypothesis. We actually do\nnot do this for the whole query, but only for a window of\nw= 10 notes, centred at the event e1of the query, and we\nexclude the notes e1,e2ande3from this list (as they were\nalready used to come up with the match in the ﬁrst place).\nWe now take these wnotes and check if they appear in\nthe database as would be expected. In this search we are\nstrict on the pitch value, but allow for a window of \u0006100\nms with regards to the actual time in the database. If we can\nconﬁrm that a certain percentage of notes from the query\nappears in the database as expected (in the experiments we\nused0:8), we ﬁnally accept the query token as an actual\nmatch.\nAs this approach is computationally expensive, we actu-\nally compute the results in two steps: we ﬁrst do ‘normal’\nﬁngerprinting without the veriﬁcation step and only keep\nthe top 5% of the results. We then perform the veriﬁcation\nstep on these results only and recompute the scores. On\nour dataset this effectively more than halves the computa-\ntion time.\n4. PROCESSING LONGER QUERIES:\nMULTI-AGENT TRACKING\nThe ﬁngerprinting method in [2] was mainly concerned\nwith invariance regarding the global tempo. When apply-\ning this algorithm to our database with longer queries, lo-\ncal tempo changes (i.e. tempo changes within the query)\nprove to be problematic, because they break the ‘cheap’\nhistogram approach that is used to determine continuous\nregions of matching tokens.\nInstead of using computationally much more expensive\nmethods for determining these regions, we propose to split\nlonger queries into shorter ones and track the results of\nthese sub-queries over time. This is based on the assump-\ntion that in short queries the tempo is (quasi) stationary,\nand that a few exceptions will not break the tracking algo-\nrithm we use. In our implementation, we split each query\ninto sub-queries with a window size of w= 15 notes and\na hop size of h= 5notes and then feed each sub-query to\nthe ﬁngerprinter individually.\nEach result of a sub-query (but at most the top 100 po-\nsitions that are returned) is in turn fed to an on-line posi-\ntion hypothesis tracking algorithm. In our current proof-\nof-concept implementation we use a simple on-line rule-\nbased multi-agent approach, inspired by the beat-tracking\nalgorithm described in [6]. For a purely off-line retrieval\ntask a non-causal algorithm will lead to even better results.The basic idea is to create virtual ‘agents’ for positions\nin the result sets. Each agent has a current hypothesis of\nthe piece, the position within the piece and the tempo, and\na score based on the results of the sub-queries. The agents\nare updated, if possible, with newly arriving data. In do-\ning so, agents that represent positions that successively oc-\ncur in result sets will accumulate higher scores than agents\nthat represent positions that only occurred once or twice by\nchance, and are most probably false positives.\nMore precisely, we iterate over all sub-queries and per-\nform the following steps in each iteration:\n\u000fNormalise Scores: First the scores of the positions\nin the result set of the sub-query are normalised by\ndividing them by their median. This makes sure that\neach iteration has approximately the same inﬂuence\non the tracking process.\n\u000fUpdate Agents: For every agent, we look for a match-\ning position in the result set of the sub-query (i.e. a\nposition that approximately ﬁts the extrapolated po-\nsition of the agent, given the old position, the tempo,\nand the elapsed time). The position, the tempo and\nthe score of the agent are updated with the new data\nfrom the matching result of the sub-query. If we do\nnot ﬁnd a matching position in the result set, we up-\ndate the agent with a score of 0, and the extrapo-\nlated position is taken as the new hypothesis. If a\nmatching position is found, the accumulated score\nis updated in a fashion such that scores from further\nin the past have a smaller impact than more recent\nones. Each agent has a ring buffer sof size 50, in\nwhich the scores of the individual sub-queries are\nbeing stored. The accumulated score of the agent is\nthen calculated as score acc=50P\ni=1si\n1+log i, where s1\nis the most recent score.\n\u000fCreate Agents: Each sub-query result that was not\nused to update an existing agent is used to initialise\na new agent at the respective score position (i.e. in\nthe ﬁrst iteration up to 100 agents are created).\n\u000fRemove obsolete Agents: Finally, agents with low\nscores are removed. In our implementation we sim-\nply remove agents that are older then 10 iterations\nand are not part of the current top 25 agents.\nAt each point in time the agents are ordered by score acc\nand can be seen as hypotheses about the current position\nin the database of pieces. Thus, in the case of a single\nlong query, the agents with the highest accumulated scores\nare returned in the end. In an on-line scenario, where an\naudio stream is constantly being monitored by the ﬁnger-\nprinting system, the current top hypotheses can be returned\nafter each performed update (i.e. after each processed sub-\nquery).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5515. EV ALUATION\n5.1 Dataset Description\nFor the evaluation of the proposed algorithms a ground\ntruth is needed. We need exact alignments of performances\n(recordings) of classical music to their respective scores\nsuch that we know exactly when each note given in the\nscore is actually played in the performance. This data can\neither be generated by a computer program or by extensive\nmanual annotation but both ways are prone to errors.\nLuckily, we have access to two unique datasets where\nprofessional pianists played performances on a computer-\ncontrolled piano1and thus every action (e.g. key presses,\npedal movements) was recorded. The ﬁrst dataset (see\n[14]) consists of performances of the ﬁrst movements of\n13 Mozart piano sonatas by Roland Batik. The second,\nmuch larger, dataset consists of nearly the complete solo\npiano works by Chopin performed by Nikita Magaloff [7].\nFor the latter set we do not have the original audio ﬁles and\nthus replayed the symbolic performance data on a Yamaha\nN2 hybrid piano and recorded the resulting performances.\nAs we have both symbolic and audio information about\nthe performances, we know the exact timing of each played\nnote in the audio ﬁles. To build the score database we con-\nverted the sheet music to MIDI ﬁles with a constant tempo\nsuch that the overall duration of the ﬁle is similar to a ‘nor-\nmal’ performance of the piece.\nIn addition to these two datasets the score database in-\ncludes the complete Beethoven piano sonatas, two sym-\nphonies by Beethoven, and various other piano pieces. To\nthis data we have no ground truth, but this is irrelevant\nsince we do not actively query for them with performance\ndata in our evaluation runs. See Table 1 for an overview of\nthe complete dataset.\n5.2 Results\nFor the evaluation we follow the procedure from [2]. A\nscore position Xis considered correct if it marks the be-\nginning (+/- 1.5 seconds) of a score section that is identi-\ncal in note content, over a time span the length of the query\n(but at least 20 notes), to the note content of the ‘real’ score\nsituation corresponding to the audio segment that the sys-\ntem was just listening to. We can establish this as we have\nthe correct alignment between performance time and score\npositions — our ground truth). This complex deﬁnition\nis necessary because musical pieces may contain repeated\nsections or phrases, and it is impossible for the system (or\nanyone else, for that matter) to guess the ‘true’ one out of a\nset of identical passages matching the current performance\nsnippet, given just that performance snippet as input. We\nacknowledge that a measurement of musical time in a score\nin terms of seconds is rather unusual. But as the MIDI\ntempos in our database generally are set in a meaningful\nway, this seemed the best decision to make errors compa-\nrable over different pieces, with different time signatures –\nit would not be very meaningful to, e.g. compare errors in\nbars or beats over different pieces.\n1B¨osendorfer SE 290We tested the algorithms with different query lengths:\n10, 15, 20 and 25 notes (automatically transcribed from\nthe audio query). For each of the query lengths, we gener-\nated 2500 queries by picking random points in the perfor-\nmances of our test database, and used them as input for the\nproposed algorithms. Duplicate retrieval results (i.e. posi-\ntions that have the exact same note content; also, duplicate\npiece IDs for the experiments on piece-level) are removed\nfrom the result set.\nTable 2 shows the results of the original tempo-invariant\n(but not pitch-invariant) algorithm on our dataset. Here,\nwe present results for two categories: correctly identiﬁed\npieces, and correctly identiﬁed piece and position in the\nscore. For both categories we give the percentage of cor-\nrect results at rank 1, and the mean reciprocal rank. This\nexperiment basically conﬁrms the results that were reported\nin [2] on a larger database (more than twice as large), for\nwhich a slight drop in performance is expected.\nIn addition, for the experiments with the transposition-\ninvariant ﬁngerprinting method, we transposed each score\nrandomly by between -11 and +11 semitones – although\nstrictly speaking this was not necessary, as the transposition-\ninvariant algorithm returns exactly the same (large) set of\ntokens for un-transposed and transposed queries or scores.\nTable 3 gives the results of the transposition-invariant\nmethod on these queries, both without (left) and with the\nveriﬁcation step (right). As expected, the use of pitch-\ninvariant ﬁngerprints without additional veriﬁcation causes\na big decrease in retrieval precision (compare left half of\nTable 3 with Table 2). Furthermore, the loss in discrimi-\nnative power of the ﬁngerprint tokens also results in an in-\ncreased number of tokens returned for every query, which\nhas a direct inﬂuence on the runtime of the algorithm (last\nrow in Table 3). The proposed veriﬁcation step solves the\nprecision problem, at least to some extent, and in our opin-\nion makes the approach usable. Of course this does not\ncome for free, as the runtime increases slightly.\nWe also tried to use the veriﬁcation step with the origi-\nnal tempo-invariant algorithm but were not able to improve\non the retrieval results. At least on our test data the tempo-\ninvariant ﬁngerprints are discriminative enough to mostly\navoid false positives.\nFinally, Table 4 gives the results on slightly longer quer-\nies for both the original tempo-invariant and the new tempo-\nand transposition-invariant algorithm. As can be seen, for\nthe detection of the exact position in the score, using no\ntracking, the results based on queries with length 100 notes\nare worse than those for queries with only 50 notes, i.e.\nmore information leads to worse results. This is caused\nby local tempo changes within the query, which break the\nhistogram approach for ﬁnding sequences of matching to-\nkens.\nAs shown on the right hand side for both ﬁngerprinting\ntypes in Table 4, the approach of splitting longer queries\ninto shorter ones and tracking the results takes care of this\nproblem. Please note that for the tracking approach we\ncheck if the position hypotheses after the last tracking step\nmatch the correct position in the score. Thus, as this is an\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n552Score Database Testset\nData Description Number of Pieces Notes in Score Notes in Performance Performance Duration\nChopin Corpus 154 325,263 326,501 9:38:36\nMozart Corpus 13 42,049 42,095 1:23:56\nAdditional Pieces 159 574,926 – –\nTotal 326 942,238\nTable 1 . Database and Testset Overview. In the database, all the pieces are included. As we only have performances\naligned to the scores for the Chopin and the Mozart corpus, only these are included in the test set to query the database.\nQuery Length in Notes 10 15 20 25\nCorrect Piece as Top Match 0.6 0.82 0.88 0.91\nCorrect Piece Mean Reciprocal Rank (MRR) 0.68 0.86 0.91 0.93\nCorrect Position as Top Match 0.53 0.72 0.77 0.79\nCorrect Position Mean Reciprocal Rank (MRR) 0.60 0.79 0.83 0.85\nMean Query Length in Seconds 1.47 2.26 3.16 3.82\nMean Query Execution Time in Seconds 0.02 0.06 0.11 0.16\nTable 2. Results for different query sizes of the original tempo-invariant piece and score position identiﬁcation algorithm\non the test database at the piece level (upper half) and on the score position level (lower half). Each estimate is based on\n2500 random audio queries. For both categories the percentage of correct detections at rank 1 and the mean reciprocal rank\n(MRR) are given. Additionally, the mean length of the query in seconds and the mean execution time for a query is shown.\nWithout Veriﬁcation With Veriﬁcation\nQuery Length in Notes 10 15 20 25 10 15 20 25\nCorrect Piece as Top Match 0.30 0.40 0.41 0.40 0.43 0.63 0.71 0.75\nCorrect Piece MRR 0.36 0.47 0.50 0.49 0.49 0.69 0.76 0.79\nCorrect Position as Top Match 0.23 0.33 0.32 0.32 0.33 0.51 0.57 0.60\nCorrect Position MRR 0.29 0.40 0.41 0.40 0.41 0.59 0.66 0.69\nMean Query Length in Seconds 1.47 2.26 3.16 3.82 1.47 2.26 3.16 3.82\nMean Query Execution Time in Seconds 0.10 0.32 0.62 0.91 0.12 0.38 0.72 1.09\nTable 3 . Results for different query sizes of the proposed tempo- and transposition-invariant piece and score position\nidentiﬁcation algorithm on the test database with (right) and without (left) the proposed veriﬁcation step. Each estimate is\nbased on 2500 random audio queries. The upper half shows recognition results on the piece level, the lower half on the\nscore position level. For both categories the percentage of correct detections at rank 1 and the mean reciprocal rank (MRR)\nare given. Additionally, the mean length of the query in seconds and the mean execution time for a query is shown.\nTempo-invariant Tempo- and Pitch-invariant\nNo Tracking Tracking No Tracking Tracking\nQuery Length in Notes 50 100 50 100 50 100 50 100\nCorrect Piece as Top Match 0.95 0.96 0.98 1 0.81 0.79 0.92 0.98\nCorrect Piece MRR 0.97 0.98 0.99 1 0.85 0.82 0.94 0.99\nCorrect Position as Top Match 0.78 0.73 0.87 0.88 0.64 0.59 0.77 0.83\nCorrect Position MRR 0.85 0.81 0.89 0.90 0.72 0.66 0.82 0.86\nMean Query Length in Seconds 7.62 15.03 7.62 15.03 7.62 15.03 7.62 15.03\nMean Query Execution Time in Seconds 0.42 0.92 0.49 1.08 2.71 6.11 3.21 7.09\nTable 4. Results of the proposed tracking algorithm on the test database for both the original tempo-invariant algorithm\n(left) and the new tempo- and transposition-invariant approach (right), including the veriﬁcation step. For the category ‘No\nTracking’, the query was fed directly to the ﬁngerprinting algorithm. For ‘Tracking’, the queries were split into sub-queries\nwith a window size of 15 notes and a hop size of 5 notes, and the individual results were tracked by our proof-of-concept\nmulti-agent approach. Evaluation of the tracking approach is based on the ﬁnding the endpoint of a query (see text). Each\nestimate is based on 2500 random audio queries. The upper half shows recognition results on the piece level, the lower half\non the score position level. For both categories the percentage of correct detections at rank 1 and the mean reciprocal rank\n(MRR) are given. Additionally, the mean length of the query in seconds and the mean execution time for a query is shown.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n553on-line algorithm, we are not interested in the start posi-\ntion of the query in the score, but in the endpoint, i.e. if the\nquery was tracked successfully, and the correct current po-\nsition is returned. Even the causal approach leads to a high\npercentage of correct results with both the original and the\ntempo- and pitch-invariant ﬁngerprinting algorithm. Most\nof the remaining mistakes happen because (very) similar\nparts within one and the same piece are confused.\n6. CONCLUSIONS\n6.1 Applications\nThe proposed algorithm is useful in a wide range of ap-\nplications. As a retrieval algorithm it enables fast and ro-\nbust (inter- and intra-document) searching and browsing in\nlarge collections of musical scores and corresponding per-\nformances. Furthermore, we believe that the algorithm is\nnot limited to retrieval tasks in classical music, but may be\nof use for cover version identiﬁcation in general, and pos-\nsibly many other tasks. For example, it was already suc-\ncessfully applied in the ﬁeld of symbolic music processing\nto ﬁnd repeating motifs and sections in complex musical\nscores [5].\nCurrently, the algorithm is mainly used in an on-line\nscenario (see [1]). In connection with a score following\nalgorithm it can act as a ‘piano music companion’. The\nsystem is able to recognise arbitrary pieces of classical pi-\nano music, identify the position in the score and track the\nprogress of the performer. This enables a wide range of\napplications for musicians and for consumers of classical\nmusic.\n6.2 Future Work\nIn its current state the algorithm is able to recognise the\ncorrect piece and the score position even for very short\nqueries of piano music. It is invariant to both tempo dif-\nferences and transpositions and can be used in on-line con-\ntexts (i.e. to monitor audio streams and at any time report\nwhat it is listening to) and as an off-line retrieval algorithm.\nThe main direction for future work is to lift the restriction\nto piano music and make it applicable to all kinds of classi-\ncal music, even orchestral music. The limiting component\nat the moment is the transcription algorithm, which is only\ntrained on piano sounds.\n7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Fund\n(FWF) under project number Z159 and the EU FP7 Project\nPHENICX (grant no. 601166).\n8. REFERENCES\n[1] A. Arzt, S. B ¨ock, S. Flossmann, H. Frostel, M. Gasser,\nand G. Widmer. The complete classical music compan-\nion v0. 9. In Proceedings of the 53rd AES Conference\non Semantic Audio, 2014.[2] A. Arzt, S. B ¨ock, and G. Widmer. Fast identiﬁcation of\npiece and score position via symbolic ﬁngerprinting. In\nProceedings of the International Conference on Music\nInformation Retrieval (ISMIR), 2012.\n[3] S. B ¨ock and M. Schedl. Polyphonic piano note tran-\nscription with recurrent neural networks. In Proceed-\nings of the International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2012.\n[4] P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A re-\nview of algorithms for audio ﬁngerprinting. In Pro-\nceedings of the IEEE International Workshop on Mul-\ntimedia Signal Processing (MMSP), 2002.\n[5] T. Collins, A. Arzt, S. Flossmann, and G. Widmer.\nSiarct-cfp: Improving precision and the discovery of\ninexact musical patterns in point-set representations. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR), 2013.\n[6] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30(1):39–58, 2001.\n[7] S. Flossmann, W. Goebl, M. Grachten, B. Nie-\ndermayer, and G. Widmer. The Magaloff project:\nAn interim report. Journal of New Music Research,\n39(4):363–377, 2010.\n[8] F. Kurth and M. M ¨uller. Efﬁcient index-based audio\nmatching. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 16(2):382–395, 2008.\n[9] D. Lang, D. W. Hogg, K. Mierle, M. Blanton, and\nS. Roweis. Astrometry. net: Blind astrometric calibra-\ntion of arbitrary astronomical images. The Astronomi-\ncal Journal, 139(5):1782, 2010.\n[10] M. M ¨uller, F. Kurth, and M. Clausen. Audio matching\nvia chroma-based statistical features. In Proceedings\nof the International Conference on Music Information\nRetrieval (ISMIR), 2005.\n[11] J. Serra, E. G ´omez, and P. Herrera. Audio cover song\nidentiﬁcation and similarity: background, approaches,\nevaluation, and beyond. In Z. W. Ras and A. A. Wiec-\nzorkowska, editors, Advances in Music Information\nRetrieval, pages 307–332. Springer, 2010.\n[12] R. Sonnleitner and G. Widmer. Quad-based audio ﬁn-\ngerprinting robust to time and frequency scaling. In\nProceedings of the International Conference on Dig-\nital Audio Effects, 2014.\n[13] A. Wang. An industrial strength audio search algo-\nrithm. In Proceedings of the International Conference\non Music Information Retrieval (ISMIR), 2003.\n[14] G. Widmer. Discovering simple rules in complex data:\nA meta-learning algorithm and some surprising mu-\nsical discoveries. Artiﬁcial Intelligence, 146(2):129–\n148, 2003.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n554"
    },
    {
        "title": "Cognition-inspired Descriptors for Scalable Cover Song Retrieval.",
        "author": [
            "Jan Van Balen",
            "Dimitrios Bountouridis",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417795",
        "url": "https://doi.org/10.5281/zenodo.1417795",
        "ee": "https://zenodo.org/records/1417795/files/BalenBWV14.pdf",
        "abstract": "Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the de- velopment of descriptors that effectively capture musical structures at the song level. The performance of the pro- posed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance.",
        "zenodo_id": 1417795,
        "dblp_key": "conf/ismir/BalenBWV14",
        "keywords": [
            "music cognition studies",
            "computational musicology",
            "mid- to high-level computational analysis",
            "content-based retrieval",
            "descriptors",
            "musical structures",
            "cover song retrieval",
            "scalable",
            "stability and variation",
            "model"
        ],
        "content": "COGNITION-INSPIRED DESCRIPTORS FOR\nSCALABLE COVER SONG RETRIEV AL\nJan Van Balen, Dimitrios Bountouridis, Frans Wiering, Remco Veltkamp\nUtrecht University, Department of Information and Computing Sciences\nj.m.h.vanbalen@uu.nl, d.bountouridis@uu.nl\nABSTRACT\nInspired by representations used in music cognition studies\nand computational musicology, we propose three simple\nand interpretable descriptors for use in mid- to high-level\ncomputational analysis of musical audio and applications\nin content-based retrieval. We also argue that the task of\nscalable cover song retrieval is very suitable for the de-\nvelopment of descriptors that effectively capture musical\nstructures at the song level. The performance of the pro-\nposed descriptions in a cover song problem is presented.\nWe further demonstrate that, due to the musically-informed\nnature of the descriptors, an independently established model\nof stability and variation in covers songs can be integrated\nto improve performance.\n1. INTRODUCTION\nThe aim of this paper is to demonstrate the use of three\nnew, cognition-inspired music descriptors for content-based\nmusic retrieval.\n1.1 Audio Descriptors\nThere is a growing consensus that some of the most widely\nused features in Music Information Research, while very\neffective for engineering applications, do not serve the dia-\nlog with other branches of music research [1]. As a classic\nexample, MFCC features can be shown to predict human\nratings of various perceptual qualities of a sound. Yet, from\nthe perspective of neuropsychology, claims that they math-\nematically approximate parts of auditory perception have\nbecome difﬁcult to justify as more parts of the auditory\npathway are understood.\nMeanwhile, a recent analysis of evaluation practices by\nSturm [18] suggests that MIR systems designed to clas-\nsify songs into high-level attributes like genre, mood or\ninstrumentation may rely on confounded factors unrelated\nto any high-level property of the music, even if their per-\nformance numbers approach 100%. Researchers have fo-\nc\rJan Van Balen, Dimitrios Bountouridis, Frans Wiering,\nRemco Veltkamp.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Jan Van Balen, Dimitrios Boun-\ntouridis, Frans Wiering, Remco Veltkamp. “Cognition-inspired descrip-\ntors for\nScalable Cover Song Retrieval”, 15th International Society for Music In-\nformation Retrieval Conference, 2014.cused too much on the same evaluation measures and the\nsame datasets and as a result, today, top performing genre\nand mood recognition systems rely on the same low-level\nfeatures that are used to classify bird sounds.1\nWe also observe that, despite the increasing availabil-\nity of truly big audio data and the promising achievements\nof MIR over the last decade, studies that turn big audio\ndata into ﬁndings about music itself seem hard to ﬁnd. No-\ntable exceptions include studies on scales and intonation,\nand [16]. In the latter, pitch, timbre and loudness data were\nanalyzed for the Million Song Dataset, focusing on the dis-\ntribution and transitions of discretized code words. Yet, we\nhave also observed that this analysis sparks debate among\nmusic researchers outside the MIR ﬁeld, in part because of\nthe descriptors used. The study uses the Echo Nest audio\nfeatures provided with the dataset, which are computed us-\ning undisclosed, proprietary methods and therefore objec-\ntively difﬁcult in interpretation.\n1.2 Towards Cognitive Audio Descriptors\nIn a longer-term effort towards modeling cognition level\nqualities of music such as its complexity, expectedness and\nrepetitiveness from raw audio data, we aim to design and\nevaluate features that describe harmony, melody and rhythm\non a level that has not gained the attention it deserves in\nMIR’s audio community, perhaps due to the ‘success’ of\nlow-level features discussed above. In the long run, we be-\nlieve, this will provide insights into the building blocks of\nmusic: riffs, motives, choruses, and so on.\n1.3 Cover Song Detection\nIn this section, we argue that the task of scalable cover\nsong retrieval is very suitable for developing descriptors\nthat effectively capture mid- to high-level musical struc-\ntures, such as chords, riffs and hooks.\nCover detection systems take query song and a database\nand aim to ﬁnd other versions of the query song. Since\nmany real-world cover versions drastically modulate mul-\ntiple aspects of the original: systems must allow for devi-\nations in key, tempo, structure, lyrics, harmonisation and\nphrasing, to name just a few. Most successful cover detec-\ntion algorithms are built around a two-stage architecture.\nIn the ﬁrst stage, the system computes a time series repre-\nsentation of the harmony or pitch for each of the songs in a\ndatabase. In the second stage, the time series representing\n1largely MFCC and spectral moments, see [6, 18] for examples\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n379the query is compared to each of these representations, typ-\nically by means of some kind of alignment, i.e. computing\nthe locations of maximum local correspondence between\nthe two documents being compared. See [15] for more on\nthis task and an overview of cover detection strategies.\n2. SCALABLE COVER SONG RETRIEV AL\nGenerally, alignment methods are computationally expen-\nsive but effective. Results achieved this way have reached\nmean average precision (MAP) ﬁgures of around 0:75 at\nthe MIREX evaluation exchange.2\nWhen it comes to large-scale cover detection (hundreds\nof queries and thousands of songs), however, alignment-\nbased methods can become impractical. Imagine a musi-\ncologist whose aim is not to retrieve matches to a single\nquery, but to study all the relations in a large, representa-\ntive corpus. Alignment-based techniques are no longer an\noption: a full pair-wise comparison of 10;000documents\nwould take weeks, if not months.3.\nThis is why some researchers have recently focused on\nscalable techniques for cover song detection. Scalable strate-\ngies are often inspired by audio ﬁngerprinting and involve\nthe computation of an indexable digest of (a set of) po-\ntentially stable landmarks in the time series, which can be\nstored and matched through a single inexpensive look-up.\nExamples include the ‘jumpcodes’ approach by [2], the\nﬁrst system to be tested using the Million Song Dataset.\nThis study reports a recall of 9:6% on the top 1 percent\nof retrieved candidates. Another relevant example is the\ninterval-gram approach by Walters [19], which computes\nﬁngerprinting-inspired histograms of local pitch intervals,\ndesigned for hashing using wavelet decomposition.\nReality shows that stable landmarks are relatively easy\nto ﬁnd when looking for exact matches (as in ﬁngerprint-\ning), but hard to ﬁnd in real-world cover songs. A more\npromising approach was presented by Bertin-Mahieux in\n[3]], where the 2D Fourier transform of beat-synchronized\nchroma features is used as the primary representation. The\naccuracy reported is several times better than for the sys-\ntem based on jumpcodes. Unfortunately, what exactly these\nfeatures capture is difﬁcult to explain.\nThe challenges laid out in the above paragraph make\ncover song detection an ideal test case to evaluate a special\nclass of descriptors: harmony, melody and rhythm descrip-\ntors, global or local, which have a ﬁxed dimensionality\nand some tolerance to deviations in key, tempo and global\nstructure. If a collection of descriptors can be designed that\naccurately describes a song’s melody, harmony and rhythm\nin a way that is robust to the song’s precise structure, tempo\nand key, we should have a way to determine similarity be-\ntween the ‘musical material’ of two songs and assess if the\nunderlying composition is likely to be the same.\n2http://www.music-ir.org/mirex/wiki/2009:\nAudio_Cover_Song_Identification_Results\n3MIRex 2008 (the last to report runtimes) saw times of around 1:4\u0000\n3:7\u0002105s for a task that involves 115; 000comparisons. The fastest of\nthese algorithms would take 1.8 years to compute the1\n2108comparisons\nrequired in the above scenario. The best performing algorithm would take\n6 years.3. PITCH AND HARMONY DESCRIPTORS\nThere is an increasing amount of evidence that the pri-\nmary mechanism governing musical expectations is statis-\ntical learning [7, 12]. On a general level, this implies that\nthe conditional probabilities of musical events play a large\nrole in their cognitive processing. Regarding features and\ndescripors, it justiﬁes opportunities of analyzing songs and\ncorpora in terms of probabily distributions. Expectations\nresulting from the exposure to statistical patterns have in\nturn been shown to affect the perception of melodic com-\nplexity and familiarity. See [7] for more on the role of\nexpectation in preference, familiarity and recall.\nWe propose three new descriptors: the pitch bihistogram,\nthe chroma correlation coefﬁcients and the harmonization\nfeature. The pitch bihistogram describes melody and ap-\nproximates a histogram of pitch bigrams. The chroma cor-\nrelation coefﬁcients relate to harmony. They approximate\nthe co-occurrence of chord notes in a song. The third rep-\nresentation, the harmonization feature, combines harmony\nand melody information. These three descriptors will now\nbe presented in more detail.\n3.1 The Pitch Bihistogram\nPitch bigrams are ordered pairs of pitches, similar to word\nor letter bigrams used in computational linguistics. Several\nauthors have proposed music descriptions based on pitch\nbigrams, most of them from the domain of cognitive sci-\nence [10, 11, 13]. Distributions of bigrams effectively en-\ncode ﬁrst-degree expectations. More precisely: if the dis-\ntribution of bigrams in a piece is conditioned on the ﬁrst\npitch in the bigram, we obtain the conditional frequency of\na pitch given the one preceding it.\nThe ﬁrst new feature we introduce will follow the bi-\ngram paradigm. Essentially, it captures how often two\npitches p1andp2occur less than a distance dapart.\nAssume that a melody time series P(t), quantized to\nsemitones and folded to one octave, can be obtained. If a\npitch histogram is deﬁned as:\nh(p) =X\nP(t)=p1\nn; (1)\nwithnthe length of the time series and p2f1; 2; : : : 12g,\nthe proposed feature is then deﬁned:\nB(p1; p2) =X\nP(t1)=p 1\nP(t2)=p 2w(t2\u0000t1) (2)\nwhere\nw(x) =(\n1\nd;if0< x < d:\n0; otherwise:(3)\nThis will be reffered to as the pitch bihistogram, a bi-\ngram representation that can be computed from continu-\nous melodic pitch. Note that the use of pitch classes rather\nthan pitch creates an inherent robustness to octave errors\nin the melody estimation step, making the feature insensi-\ntive to one of the most common errors encountered in pitch\nextraction.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n380Alternatively, scale degrees can be used instead of ab-\nsolute pitch class. In this scenario, the pitch contour P(t)\nmust ﬁrst be aligned to an estimate of the piece’s overall\ntonal center. As a tonal center, the tonic can be used. How-\never, for extra robustness to misestimating the tonic, we\nsuggest to use the tonic for major keys and the minor third\nfor minor keys.\n3.2 Chroma Correlation Coefﬁcients\nThe second feature representation we propose focuses on\nvertical rather than horizontal pitch relation. It encodes\nwhich pitches appear simultaneously in a signal.\nC(p1; p2) =corr(c(t; p 1); c(t; p 2)); (4)\nwhere c(t; p) is a 12-dimensional chroma time series (also\nknown as pitch class proﬁle) computed from the song au-\ndio. From this chroma representation of the song c(t; p) we\ncompute the correlation coefﬁcients between each pair of\nchroma dimensions to obtain a 12\u000212matrix of chroma\ncorrelation coefﬁcients C(p1; p2). Like the pitch bihis-\ntogram, the chroma features can be transposed to the same\ntonal center (tonic or third) based on an estimate of the\noverall or local key.\n3.3 Harmonisation Feature\nFinally, the harmonisation feature is a set of histograms\nof the harmonic pitches ph2f1; : : : ; 12gas they accom-\npany each melodic pitch pm2 f1; : : : ; 12g. It is com-\nputed from the pitch contour P(t)and a chroma time se-\nriesc(t; p h), which should be adjusted to have the same\nsampling rate and aligned to a common tonal center.\nH(pm; ph) =X\nP(t)=pmc(t; p h): (5)\nFrom a memory and statistical learning perspective, the\nchroma correlation coefﬁencients and harmonisation fea-\nture may be used to approximate expectations that include:\nthe expected consonant pitches given a chord note, the ex-\npected harmony given a melodic pitch, and the expected\nmelodic pitch given a chord note. Apart from [8], where\na feature resembling the chroma correlation coefﬁcients is\nproposed, information of this kind has yet to be exploited\nin a functioning (audio) MIR system. Like the pitch bi-\nhistogram and the chroma correlation coefﬁcients, the har-\nmonisation feature has a dimensionality of 12\u000212.\n4. EXPERIMENTS\nTo evaluate the performance of the above features for cover\nsong retrieval, we set up a number of experiments around\nthecovers80 dataset by Ellis [5]. This dataset is a collec-\ntion of 80 cover song pairs, divided into a ﬁxed list of 80\nqueries and 80 candidates. Though covers80 is not actu-\nally ‘large-scale’, it is often used for benchmarking4and\nits associated audio data are freely available. In contrast,\nthe much larger Second Hand Songs dataset is distributed\n4results for this dataset have been reported by at least four authors [15]only in the form of standard Echo Nest features. These fea-\ntures do not include any melody description, which is the\nbasis for the descriptors proposed in this study.\nRegarding scalability, we chose to follow the approach\ntaken in [19], in which the scalability of the algorithm fol-\nlows from the simplicity of the matching step. The pro-\nposed procedure is computationally scalable in the sense\nthat, with the appropriate hashing strategy, matching can\nbe performed in constant time with respect to the size of\nthe database. Nevertheless, we acknowledge that the dis-\ntinguishing power of the algorithm must be assessed in the\ncontext of much more data. A large scale evaluation of our\nalgorithm, adapted to an appopriate dataset and extended\nto include hashing solutions and indexing, is planned as\nfuture work.\n4.1 Experiment 1: Global Fingerprints\nIn the ﬁrst experiment, the three descriptors from section\n3 were extracted for all 160 complete songs. Pitch con-\ntours were computed using Melodia and chroma features\nusing HPCP, using default settings [14].5For efﬁciency\nin computing the pitch bihistogram, the pitch contour was\nmedian-ﬁltered and downsampled to1\n4of the default frame\nrate. The bihistogram was also slightly compressed by tak-\ning its square root.\nThe resulting reprentations (B ,CandH) were then\nscaled to the same range by whitening them for each song\nindividually (subtracting the mean of their ndimensions,\nand dividing by the standard deviation; n= 144). To avoid\nrelying on key estimation, features in this experiment were\nnot aligned to any tonal center, but transposed to all 12\npossible keys. In a last step of the extraction stage, the\nfeatures were scaled with a set of dedicated weights w=\n(w1; w2; w3)and concatenated to 12 432-dimensional vec-\ntors, one for each key. We refer to these vectors as the\nglobal ﬁngerprints.\nIn the matching stage of the experiment, the distances\nbetween all queries and candidates were computed using a\ncosine distance. For each query, all candidates were ranked\nby distance. Two evaluation metrics were computed: recall\nat 1 (the proportion of covers retrieved among the top 1\nresult for each query; R1) and recall at 5 (proportion of\ncover retrieved ‘top 5’; R5).\n4.2 Experiment 2: Thumbnail Fingerprints\nIn a second experiment, the songs in the database were\nﬁrst segmented into structural sections using structure fea-\ntures as described by Serra [17]. This algorithm performed\nbest at the 2012 MIREX evaluation exchange in the task of\n‘music structure segmentation’, both for boundary recov-\nery and for frame pair clustering. (A slight simpliﬁcation\nwas made in the stage where sections are compared: no\ndynamic time warping was applied in our model.) From\nthis segmentation, two non-overlapping thumbnails are se-\nlected as follows:\n5mtg.upf.edu/technologies\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3811. Simplify the sequence of section labels (e.g. abab-\nCabCC): merge groups of section labels that consis-\ntently appear together (e.g. AACACC for the exam-\nple above).\n2. Compute the total number of seconds covered by\neach of the labels A, B, C... and ﬁnd the two sec-\ntion labels covering most of the song.\n3. Return the boundaries of the ﬁrst appearance of the\nselected labels.\nThe ﬁngerprint as described above was computed for\nthe full song as well as for the resulting thumbnails, yield-\ning three different ﬁngerprints: one global and two thumb-\nnail ﬁngerprints, stored separately. As in experiment 1, we\ntransposed these thumbnails to all keys, resulting in a total\nof 36 ﬁngerprints extracted per song: 12 for the full song,\n12 for the ﬁrst thumbnail and 12 for the second thumbnail.\n4.3 Experiment 3: Stability Model\nIn the last experiment, we introduced a model of stability\nin cover song melodies. This model was derived indepen-\ndently, through analysis of a dataset of annotated melodies\nof cover songs variations. Given the melody contour for\na song section, the model estimates the stability at each\npoint in the melody. Here, stability is deﬁned as the prob-\nability of the same pitch appearing in the same place in a\nperformed variation of that melody.\nThe stability estimates produced by the model are based\non three components that are found to correlate with sta-\nbility: the duration of notes, the position of a note inside a\nsection, and the pitch interval. The details of the model and\nits implementation are described in the following section.\n5. STABILITY MODEL\nThe model we apply is a quantitative model of melody sta-\nbility in cover songs. As it has been established for applica-\ntions broader than the current study, it is based on a unique,\nmanually assembled collection of annotated cover songs\nmelodies. The dataset contains four transcribed melodic\nvariations for 45 so-called ‘cliques’ of cover songs, a sub-\nset of the Second Hand Songs dataset.6. Some songs have\none section transcribed, some have more, resulting in a to-\ntal of 240 transcriptions.\nFor the case study presented here, the transcriptions were\nanalysed using multiple sequence alignment (MSA) and a\nprobabilistic deﬁnition of stability.\n5.1 Multiple Sequence Alignment\nMSA is a bioinformatics method that extends pairwise align-\nment of symbolic arrays to a higher number of sequences\n[4]. There are many approaches to MSA, some employing\nhidden markov models or genetic algorithms. The most\npopular approach is progressive alignment. This technique\ncreates an MSA by combining pairwise alignments (PWA)\n6http://labrosa.ee.columbia.edu/millionsong/\nsecondhand\nLAAALAAALAAAJ HHHHHHFECALALAAALALAAALAAAJ J J HHHHHHFHH J\nLAALALAALALAAAAJ HHHHHCEECAL ALAAALAAAALAAJ J J HHHHHHHE HEJ\nLAALALAAALAAAHHJ HHJ J J HLAALALAAALEAALEAEAJ J HJ HHEJ\n51015202530354045501\n2\n3\nLAALALAALALAAAAJ HHHHHCEECALALAAALA_AAALAAJ J J HHHHHHH EHEJ\nLAA_ALAA_ALAAA_J HHHHHHFECALALAAALALAAALAAAJ J J HHHHHH FHHJ\nLAALALAA_ALAAAHHJ HHJ J J HLAALALAAAL_EAALEAEAJ J ___HJ HH E__J\n5101520253035404550551\n2\n3\n051015202530354045505500.51\n5101520253035404550551\n2\n3Figure 1. A clique of melodies before (top) and after (bot-\ntom) multiple sequence alignment.\nstarting from the most similar sequences, constructing a\ntree usually denoted as the ‘guide tree’. Unlike MSA, pair-\nwise alignment has been researched extensively in the (sym-\nbolic) MIR community, see [9] for an overview.\nWhenever two sequences are aligned, a consensus can\nbe computed, which can be used for the alignment connect-\ning the two sequences to the rest of the three. The consen-\nsus is a new compromise sequence formed using heuristics\nto resolve the ambiguity at non-matching elements. These\nheuristics govern how gaps propagate through the tree, or\nwhether ‘leaf’ or ‘branch’ elements are favored. The cur-\nrent model favors gaps and branch elements.\nWhen the root consensus of the tree is reached, a last\niteration of PWA’s aligns each sequence to the root con-\nsensus to obtain the ﬁnal MSA. Figure 1 shows two sets\nof melodic sequences (mapped to a one-octave alphabet\nfA: : :Lg) before and after MSA. Note that the MSA is\nbased on a PWA strategy which maximizes an optimality\ncriterion based on not just pitch but also duration and onset\ntimes.\n5.2 Stability\nThestability of a note in a melody is now deﬁned as the\nprobability of the same note being found in the same posi-\ntion in an optimally aligned variation of that melody.\nEmpirically, given a set of Naligned sequences\nfsk(i)gi= 1: : : n; k = 1: : : N (6)\nwe compute the stability of event sk(i)as:\nstab(s k(i)) =1\nN\u00001j=NX\nj 6=k\nj=1sj(i) == sk(i) (7)\nAs an example, in a position iwith events s1(i) = A,\ns2(i) =A,s3(i) =A and s4(i) =B, the stability of A is\n0:66. The stability of B is 0.\n5.3 Findings\nAs described in the previous section, we drew a random\nsample of notes from the dataset in order to observe how\nstability behaves as a function of the event’s pitch, duration\nand position inside the song section.\nThe ﬁrst relationship has ‘position’ as the independent\nvariable and describes the stability as it evolves throughout\nthe section. Figure 2 shows how stability changes with\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n382Figure 2. Stability of an event vs. position in the melody.\nFigure 3. Stability of an event vs. duration.\nposition. The mean and 95% CI for the mean are shown\nfor two different binnings of the position variable. The 4-\nbin curve illustrates how stability generally decreases with\nposition. The more detailed 64-bin curve shows how the\nﬁrst two thirds of a melody are more stable than the last,\nthough an increased stability can be seen at the end of the\nsection.\nFigure 3 shows the stability of notes as a function of\ntheir duration. The distribution of note durations is cen-\ntered around 1%of the segment length. Below and above\nthis value, the stability goes slightly up. This suggests that\nnotes with less common durations are more stable. How-\never, the trend is weak compared with the effect of posi-\ntion. Note duration information will therefore not be used\nin the experiments in this study.\nFigure 4 shows the stability (mean and 95% CI for the\nmean) of a note given the pitch interval that follows. Note\nhow the relative stability of one-semitone jumps stands\nout compared to repetitions and two-semitone jumps, even\nthough two-semitone jumps are far more frequent. This\nsuggests again that less-frequent events are more stable.\nMore analysis as to this hypothesis will be performed in a\nlater study.\n6. DISCUSSION\nTable 1 summarizes the results of the cover song retrieval\nexperiments.\nIn the experiments where each descriptor was tested\nindividually, the harmony descriptors (chroma correlation\ncoefﬁcients) performed best: we obtained an accuracy of\nover30%. When looking at the top 5, there was a re-\ncall of 53:8%. The recall at 5 evaluation measure is in-\ncluded to give an impression of the performance that could\nFigure 4. Stability of an event vs. the interval that follows.\nbe gained if the current system were complemented with\nan alignment-based approach to sort the top-ranking can-\ndidates, as proposed by [19].\nThe next results show that, for the three features to-\ngether, the global ﬁngerprints outperform the thumbnail\nﬁngerprints (42:5% vs.37:5%), and combining both types\ndoes not increase performance further. In other conﬁgura-\ntions, thumbnail ﬁngerprints were observed to outperform\nthe global ﬁngerprints. This is possibly the result of seg-\nmentation choices: short segments produce sparse ﬁnger-\nprints, which are in turn farther apart in the feature space\nthan ‘dense’ ﬁngerprints.\nIn experiment 3, two components of the stability model\nwere integrated in the cover detection system. The 4-bin\nstability vs. position curve (scaled to the [0;1]range) was\nused as a weighting to emphasize parts of the melody be-\nfore computing the thumbnails’ pitch bihistogram. The\nstability per interval (compressed by taking its square root)\nwas used to weigh the pitch bihistogram directly.\nWith the stability information added to the model, the\ntop 1 precision reaches 45:0%. The top 5 recall is 56:3%.\nThis result is situated between the accuracy of the ﬁrst\nalignment-based strategies (42:5%), and the accuracy of a\nrecent scalable system (53:8%; [19]). We conclude that the\ndescriptors capture enough information to discriminate be-\ntween individual compositions, which we set out to show.\n7. CONCLUSIONS\nIn this study, three new audio descriptors are presented.\nTheir interpretation is discussed, and results are presented\nfor an application in cover song retrieval. To illustrate the\nbeneﬁt of feature interpretability, an independent model of\ncover song stability is integrated into the system.\nWe conclude that current performance ﬁgures, though\nnot state-of-the-art, are a strong indication that scalable\ncover detection can indeed be achieved using interpretable,\ncognition-inspired features. Second, we observe that the\npitch bihistogram feature, the chroma correlation coefﬁ-\ncients and the harmonisation feature capture enough infor-\nmation to discriminate between individual compositions,\nproving that they are at the same time meaningful and highly\ninformative, a scarse resource in the MIR feature toolkit.\nFinally, it has been demonstrated that the problems of cognition-\nlevel audio description and scalable cover detection can be\nsuccesfully addressed together.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n383Descriptor R1 R5\nGlobal ﬁngerprints B 0.288 0.438\nC 0.313 0.538\nH 0.200 0.375\nw= (2;3;1) 0.425 0.575\nThumbnail ﬁngerprints w= (2;3;1) 0.388 0.513\nGlobal + thumbnail ﬁngerprints w= (2;3;1) 0.425 0.538\nBoth ﬁngerprints + stability model w= (2;3;1) 0.450 0.563\nTable 1. Summary of experiment results. ware the feature weights. Performance measures are recall at 1 (proportion of\ncovers retrieved ‘top 1’; R1) and recall at 5 (proportion of cover retrieved among ‘top 5’; R5).\nAs future work, tests will be carried out to assess the\ndiscriminatory power of the features when applied to a\nlarger cover song problem.\n8. ACKNOWLEDGEMENTS\nThis research is supported by the NWO CATCH project\nCOGITCH (640.005.004), and the FES project COMMIT/.\n9. REFERENCES\n[1] Jean-Julien Aucouturier and Emmanuel Bigand. Seven\nproblems that keep MIR from attracting the interest of\ncognition and neuroscience. Journal of Intelligent In-\nformation Systems, 41(3):483–497, July 2013.\n[2] T Bertin-Mahieux and Daniel P W Ellis. Large-scale\ncover song recognition using hashed chroma land-\nmarks. IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics, pages 10–13, 2011.\n[3] T Bertin-Mahieux and Daniel P W Ellis. Large-Scale\nCover Song Recognition Using The 2d Fourier Trans-\nform Magnitude. In Proc Int Soc for Music Information\nRetrieval Conference, pages 2–7, 2012.\n[4] H Carrillo and D Lipman. The Multiple Sequence\nAlignment Problem in Biology. SIAM Journal on Ap-\nplied Mathematics, 1988.\n[5] Daniel P. W. Ellis and C.V . Cotton. The ”covers80”\ncover song data set, 2007.\n[6] M. Graciarena, M. Delplanche, E. Shriberg, A Stol-\ncke, and L. Ferrer. Acoustic front-end optimization for\nbird species recognition. In IEEE int conf on Acoustics\nSpeech and Signal Processing (ICASSP), pages 293–\n296, March 2010.\n[7] David Huron. Musical Expectation. In The 1999 Ernest\nBloch Lectures. 1999.\n[8] Samuel Kim and Shrikanth Narayanan. Dynamic\nchroma feature vectors with applications to cover song\nidentiﬁcation. 2008 IEEE 10th Workshop on Multime-\ndia Signal Processing, pages 984–987, October 2008.\n[9] Peter van Kranenburg. A Computational Approach to\nContent-Based Retrieval of Folk Song Melodies. PhD\nthesis, Utrecht University, 2010.[10] Y . Li and D. Huron. Melodic modeling: A comparison\nof scale degree and interval. In Proc. of the Int. Com-\nputer Music Congerence, 2006.\n[11] Daniel M ¨ullensiefen and Klaus Frieler. Evaluating\ndifferent approaches to measuring the similarity of\nmelodies. Data Science and Classiﬁcation, 2006.\n[12] Marcus T. Pearce, Mara Herrojo Ruiz, Selina Kapasi,\nGeraint A. Wiggins, and Joydeep Bhattacharya. Unsu-\npervised statistical learning underpins computational,\nbehavioural, and neural manifestations of musical ex-\npectation. NeuroImage, 50(1):302 – 313, 2010.\n[13] Pablo H Rodriguez Zivic, Favio Shifres, and\nGuillermo a Cecchi. Perceptual basis of evolving\nWestern musical styles. Proceedings of the National\nAcademy of Sciences of the United States of America,\n110(24):10034–8, June 2013.\n[14] J. Salamon and E. Gomez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. In IEEE Trans. on Audio, Speech and Lan-\nguage Processing, 2010.\n[15] Joan Serr `a.Identiﬁcation of versions of the same musi-\ncal composition by processing audio descriptions. PhD\nthesis, Universitat Pompeu Fabra, 2011.\n[16] Joan Serr `a, Alvaro Corral, Mari ´an Bogu ˜n´a, Mart ´ın\nHaro, and Josep Ll Arcos. Measuring the evolution\nof contemporary western popular music. Scientiﬁc re-\nports, 2:521, January 2012.\n[17] Joan Serra, M Meinard, Peter Grosche, and Josep Ll\nArcos. Unsupervised Detection of Music Boundaries\nby Time Series Structure Features. Proc of the AAAI\nConf on Artiﬁcial Intelligence, pages 1613–1619,\n2012.\n[18] Bob L. Sturm. Classiﬁcation accuracy is not enough.\nJournal of Intelligent Information Systems, 41(3):371–\n406, July 2013.\n[19] Thomas C Walters, David A Ross, and Richard F Lyon.\nThe Intervalgram : An Audio Feature for Large-scale\nMelody Recognition. In Int. Symp. on Computer Music\nModeling and Retrieval (CMMR), pages 19–22, 2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n384"
    },
    {
        "title": "Estimation of the Direction of Strokes and Arpeggios.",
        "author": [
            "Isabel Barbancho",
            "George Tzanetakis",
            "Lorenzo J. Tardón",
            "Peter F. Driessen",
            "Ana M. Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418053",
        "url": "https://doi.org/10.5281/zenodo.1418053",
        "ee": "https://zenodo.org/records/1418053/files/BarbanchoTTDB14.pdf",
        "abstract": "Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actu- ally, in some instruments, it is impossible to trigger mul- tiple notes simultaneously. In others, the player can con- sciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically es- timate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analy- sis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direc- tion, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ.",
        "zenodo_id": 1418053,
        "dblp_key": "conf/ismir/BarbanchoTTDB14",
        "keywords": [
            "chord",
            "notes",
            "instrument",
            "simultaneously",
            "player",
            "sequence",
            "notes",
            "pitch",
            "system",
            "estimation"
        ],
        "content": "ESTIMATION OF THE DIRECTION OF STROKES AND ARPEGGIOS\nIs\nabel Barbancho1, George Tzanetakis2, Lorenzo J. Tard ´on1, Peter F. Driessen2, Ana M. Barbancho1\n1Universidad de M´ alaga, ATIC Research Group, ETSI Telecomunicaci´ on,\nDpt. Ingenier´ ıa de Comunicaciones, 29071 M´ alaga, Spain\n2University of Victoria, Department of Computer Science, Victoria, Canada\nibp@ic.uma.es, gtzan@cs.uvic.ca, lorenzo@ic.uma.es,\npeter@ece.uvic.ca, abp@ic.uma.es\nABSTRACT\nWhenever a chord is played in a musical instrument, the\nnotes are not commonly played at the same time. Actu-\nally, in some instruments, it is impossible to trigger mul-\ntiple notes simultaneously. In others, the player can con-\nsciously select the order of the sequence of notes to play to\ncreate a chord. In either case, the notes in the chord can be\nplayed very fast, and they can be played from the lowest to\nthe highest pitch note (upstroke) or from the highest to the\nlowest pitch note (downstroke).\nIn this paper, we describe a system to automatically es-\ntimate the direction of strokes and arpeggios from audio\nrecordings. The proposed system is based on the analy-\nsis of the spectrogram to identify meaningful changes. In\naddition to the estimation of the up or down stroke direc-\ntion, the proposed method provides information about the\nnumber of notes that constitute the chord, as well as the\nchord playing speed. The system has been tested with four\ndifferent instruments: guitar, piano, autoharp and organ.\n1. INTRODUCTION\nThe design and development of music transcription sys-\ntems has been an open research topic since the ﬁrst at-\ntempts made by Moorer in 1977 [15]. Since then, many\nauthors have worked in different aspects of the transcrip-\ntion problem [12], [17]. A common task in this context is\nautomatic chord transcription [13], [1], [3], [7], [14], but\nalso, other aspects beyond the mere detection of the notes\nplayed are nowadays considered, shifting the focus of the\nresearch to different pieces of information related to the\nway in which these notes are played, i.e. musical expres-\nsiveness [18], [4], [7], [11].\nA chord can be deﬁned as a speciﬁc set of notes that\nsound at the same time. Often, when a chord is played, not\nall the notes in the chord start at the same time. Because\nc/circlecopyrtIsabel Barbancho1, George Tzanetakis2, Lorenzo J.\nTard´ on1, Peter F. Driessen2, Ana M. Barbancho1.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Isabel Barbancho1, George\nTzanetakis2, Lorenzo J. Tard´ on1, Peter F. Driessen2, Ana M.\nBarbancho1. “ESTIMATION OF THE DIRECTION OF STROKES\nAND ARPEGGIOS”, 15th International Society for Music Information\nRetrieval Conference, 2014.of the mechanics of actuation of some instruments like the\nguitar, the mandolin, and the autoharp [20], it is hard to\nexcite different strings at the same time. Instead the per-\nformer typically actuates them sequentially in a stroke. A\nstroke is a single motion across the strings of the instru-\nment. The stroke can have two different directions: UP,\nwhen the hand moves from the lowest to the highest note,\nand DOWN, when the hand moves from the highest to the\nlowest note. A strum is made up of several strokes com-\nbined in a rhythmic pattern. In other instruments like the\npiano or the organ, all the notes that belong to a certain\nchord can be played at the same time. However, the mu-\nsician can still choose to play the chord in arpeggio mode,\ni.e., one note after another. Again, the arpeggio direction\ncan be up or down.\nIn this paper, we propose a new chord related analysis\ntask focused on the identiﬁcation of the stroke or arpeggio\ndirection (up or down) in chords. Because the movement\ncan be fast it is not feasible to look for onsets [6] to detect\neach note individually. Therefore, a different approach will\nbe proposed. In addition to the detection of the stroke di-\nrection, our proposed method also detects the speed with\nwhich the chord has been played as well as the number\nof notes. The estimation of the number of notes played\nin a chord is a problem that has not been typically ad-\ndressed, although some references can be found related to\nthe estimation of the number of instruments in polyphonic\nmusic [16], which constitutes a related but different prob-\nlem. Regarding the chord playing speed, to the best our\nknowledge there are no published works to identify this\nparameter except when speciﬁc hardware is used for the\ntask [19], [9]. The paper is organized as follows: in Sec-\ntion 2, the proposed system model is explained. Section\n3 presents some experimental results and Section 4 draws\nsome conclusions.\n2. STROKE AND ARPEGGIO ANALYSIS\nThe main goal of this work is the analysis of audio ex-\ncerpts to detect if a chord has been played from lower to\nhigher notes (UP) or vice versa (DOWN). The movement\nto play a chord may be quite fast and all the information\nabout the movement is contained at the very beginning of\nthe chord waveform. After all the strings of the chord have\nbeen played, it is no longer possible to know whether the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n41movement was up or down because the resulting sound\nco\nntains all the component pitches. This means that any\nfeature that may provide information about how the spec-\ntrum varies when the chord is being played has to be calcu-\nlated at the very beginning of the chord. We will consider\nthat the time needed to complete a stroke varies from 1s\n(relatively slow) to less than 0.2s, when the chord is played\nfast.\nLetxdenote the samples of the played chord under\nstudy. In order to calculate a spectrogram, the samples\nxare divided into segments xm= [xm[1],...,x m[M]]T,\nwhere M is the selected window size for the spectrogram\ncalculation. Let PSDmdenote the Power Spectral Density\nof each segment xmandLmthe logarithm of the PSDm\ni.eLm= 10log10(PSDm). In Fig. 1, the log spectrogram\nof an ‘F Major’ guitar chord played from the lowest to the\nhighest string is shown (up stroke). The exact fret posi-\ntion employed to play this chord is frets= [2,2,3,4,4,2]\nwhere the vector frets represents the frets pressed to play\nthe chord from string 1(highest string) to string 6(low-\nest string). This chord has been generated synthetically to\ncontrol the exact delay between each note in the chord (in\nthis case the delay is τ= 4ms). The guitar samples have\nbeen extracted from the RWC database [10]. As it can be\nobserved in Fig. 1, the information about the stroke direc-\ntion is not directly visible in the spectrogram. Therefore, in\norder to detect the stroke direction, the spectrogram needs\nto be further analysed.\nFigure 1 . Spectrogram of an F Major chord UP stroke in a\nc\nlassic guitar (upper ﬁgure) and an E Major chord DOWN\nstroke. Audio ﬁle is sampled with fs= 44100 Hz. For\nthe spectrogram, the selected parameters are window size\nM= 1024 ,overlapp = 512 with a Hamming window.\nThe DFT size is K= 4096 . For convenience, the MIDI\nnumbers are shown in the y-axis instead of the frequency\nbins: MIDI = 69+12log2(f/440) .2.1 Detection of new spectral components\nWhenever a new note is played, it is expected that new\nspectral components corresponding to the new note will\nbe added to the existing components of the previous note\n(if any). In auditory scene analysis [8] this is termed the\n‘old+new heuristic’. The main idea is to take advantage\nof this heuristic by detecting whether the current spectrum\ncontains new components or, conversely, whether it simply\nretains the components from the previous spectrum. As we\nare frequently dealing with sounds that decay quickly our\nmodel of sustained notes will also contain a decay compo-\nnent. In order to detect ‘old+new’ changes we minimize\nthe following equation:\nǫ[m] = min\nα[m]/bracketleftBiggK/summationdisplay\nk=1|Lm[k]−α[m]Lm−1[k]|/bracketrightBigg\n(1)\nThe goal is to ﬁnd a local α[m](decay factor) that mini-\nmizesǫ[m]for two consecutive windows mandm−1. The\nminimization is carried out by means of the unconstrained\nnonlinear minimization Nelder-Mead method [21]. The\nidea is to remove from window mall the spectral compo-\nnents that were also present in window m−1with a gain\nadjustment so that any new spectral component becomes\nmore clearly visible. Thus, if there are no new played notes\nin window mwith respect to window m−1,ǫ[m]will be\nsmall, otherwise ǫ[m]will become larger because of the\npresence of the new note.\nIn Fig. 2 (a) and (b), the normalized evolutions of α[m]\nandǫ[m]respectively are displayed for the F Major UP\nchord shown in Fig.1 (a). The vertical lines represent the\ninstants when new notes appear in the chord. When a new\nnote is played in the chord, α[m]attains a minimum and\nǫ[m]a maximum. In order to automatically detect the in-\nstants when the new notes appear, the following variables\nare deﬁned:\nǫ′[m] =/braceleftbiggǫ[m]−ǫ[m−1]ifǫ[m]−ǫ[m−1]>0.5\n0otherwise(2)\nα′[m] =α[m]−α[m−1] (3)\nfα,ǫ[m] =ǫ′[m]\nmax(ǫ′)·/vextendsingle/vextendsingle/vextendsingle/vextendsingleα′[m]\nmax(α′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle(4)\nFig. 2 (c) shows the behaviour of fα,ǫ, where if becomes\neasy to identify the presence of new notes. In addition, it\nis also possible to estimate the number of notes played in\nthe chord (in this case 6), as well as the stroke speed.\n2.2 Estimation of number of notes and stroke speed\nAfter a measure that highlights the presence of new notes\nhas been deﬁned, the next step is to ﬁnd the peaks of fα,ǫ.\nEach sample of the function fα,ǫ(m)is compared against\nfα,ǫ(m−1)andfα,ǫ(m+ 1) . Iffα,ǫ(m)is larger than\nboth neighbors (local maximum) and fα,ǫ(m)>0.1, then\na candidate local peak is detected. Finally, if there are two\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n420 10 20 30 40 50 60 70 800.850.90.951a)α(m)\nm\n0 10 20 30 40 50 60 70 8000.51b)ε(m)\nm\n0 10 20 30 40 50 60 70 8000.51c) fα,ε(m)\nm\nFigure 2 . F Major chord UP stroke in classic guitar: (a) Evolution of α[ m]that minimizes equation (1) , (b) Evolution\nof the error ǫ[m]as deﬁned in equation (1) and (c) Evolution of fα,ǫ[m](equation (4)) where the presence of new notes\nbecomes apparent.\npeaks less than two points apart, the smallest one is not\nconsidered. Once these selected peaks have been localized,\nthe ﬁnal step is to determine which ones belong to played\nnotes so that the number of played notes can be estimated\ntogether with the speed of the stroke. The key observation\nis that the time difference between the note onsets that be-\nlong to the same stroke or arpeggio will be approximately\nconstant. The reason is that, because of human physiology,\nin most cases the stroke is performed with ﬁxed speed.\nLetflocsstand for a function that contains the positions\nwhere the selected peaks of fα,ǫare located. The objective\nis to detect sets of approximately equispaced peaks which\nwill correspond to the played notes in a chord or arpeggio.\nThen, the number of played notes NPN ewill be estimated\nas follows:\nNPNe=nneq+2 (5)\nwherenneqrepresents the minimum value of nsuch that\nthe distance between the positions of peaks contained in\nflocsis no longer kept approximately constant. nneqis\ndeﬁned as:\nnneq= argmin\nn/parenleftbigg\n|f′′\nlocs(n)|>3/parenrightbigg\n(6)\nwheref′′\nlocs(n)stands for the second order difference of\nflocs(n).\nFinally, the stroke speed estimate in notes per second is\ngiven by:\nV=flocs(NPNe−3)·(windowsize −overlapp )\nfs·NPN(7)Once the location of every new note is estimated using\nthe method described, the feature to detected the stroke di-\nrection is computed.\n2.3 Feature to detect stroke direction\nIn Fig. 3, the details of the windows in which the spectral\nchanges occur are depicted for the two guitar chords that\nare being analysed. The stroke direction can be guessed\nfrom those ﬁgures, but we still need to derive a meaning-\nful computational feature that can be used for automatic\nclassiﬁcation.\nIn order to reduce the amount of information to be pro-\ncessed by the classiﬁer that will decide the stroke direction,\na meaningful feature must be considered. Thus, the spec-\ntral centroid in each of the windows in which the changes\nhave been detected is calculated.\nThe spectral centroid is the centre of gravity of the spec-\ntrum itself [22], [24] and, in our case, it is estimated in each\nof the windows xmwhere the change has been detected.\nThis feature will be denoted SPCm(Spectral Centroid of\nwindowm) and it is calculated as follows:\nSPCm=/parenleftBiggK/summationdisplay\nk=1fm(k)PSDm(k)/slashBiggK/summationdisplay\nk=1PSDm(k)/parenrightBigg\n(8)\nwherePSDmis the power spectral density of the window\nxmandfmis the corresponding frequency vector.\nNote that we will use SPCs-KD when the SPCs are es-\ntimated with the delays known beforehand and SPCs-ED\nwhen the delays are estimated according to the procedure\ndescribed in section 2.1.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n43Window numbersMIDI numbersF Major chord played from the lowest to the highest frequency\n  \n4 8 12 16 2090\n80\n70\n60\n50\n40\n00.20.40.60.81\nWindow numbersMIDI numbersE Major chord played from the highest to the lowest frequency\n  \n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5110\n100\n90\n80\n70\n60\n00.20.40.60.81\nFigure 3 . Windows of the spectrogram of the UP F Major\nc\nhord and the DOWN E Major chord in which new notes\nappear.\nFig. 4 illustrates the behaviour of the SPC in the se-\nlected windows in which a change of the spectral content\nis detected for the UP F Major chord and the DOWN E\nMajor chord shown in the previous illustrations.\n1 2 3 4 5200250300350400Low to High\nWindow numbersSpectral centroid\n1 2 3 4 5360380400420440460High to Low\nWindow numbersSpectral centroid\nFigure 4 . Spectral centroid evolution for the UP F Major\nc\nhord and the DOWN E Major chord in the windows of the\nspectrogram in which the changes happen.\n3. CLASSIFICATION RESULTS OF UP AND\nDOWN STROKES\nThe proposed scheme has been tested with four different\ninstruments: guitar, piano, organ and autoharp. The gui-\ntar and organ samples have been extracted from the RWC\ndatabase [10], the piano recordings have been extracted\nfrom [2] and the autoharp recordings have been speciﬁ-\ncally made by the research team.\nA subset of the chords used in the experiment contains\nchords artiﬁcially assembled so that all the information re-\ngarding the number of notes played and the delay is avail-\nable for assessing the proposed system performance. Allaudio ﬁle are sampled with fs= 44100 Hz. The delay\nbetween consecutive notes in a chord ranges between 1000\nsamples ( 11ms) and5000 samples ( 55ms).\nWith the guitar and the autoharp, the natural way of\nplaying a chord is by adding the sound of one string af-\nter another. The guitar is a well known instrument [5], but\nthe autoharp is not. The autoharp is an American instru-\nment invented in 1881 by Charles F. Zimmerman. It was\nvery popular in Canada and the USA for teaching music\nfundamentals because it is easy to play and introduces in\na very intuitive way harmony concepts. Brieﬂy, the instru-\nment has 36strings and the musician can select which ones\ncan vibrate by pressing buttons corresponding to different\nchords. The buttons in the autoharp mute the strings corre-\nsponding to the notes that do not belong to the chord to be\nplayed. Then, the musician actuates the strings by strum-\nming with the other hand. In the guitar, the decay of each\nstring is exponential and very fast. In the case of the auto-\nharp, due to the resonance box, the decay of the sound is\nslower. In the piano and in the organ, the musician can play\nthe chords arpeggiated. In the piano the decay is also ex-\nponential but in the organ the sound of a note is sustained\nand decays slowly.\nIn Tables 1 and 1, the UP and DOWN classiﬁcation re-\nsults are summarized for the artiﬁcially assembled chords.\nIn all the cases, 100chords have been used for training ( 50\nUP and50DOWN) and a total of 500chords equally dis-\ntributed among UP and DOWN have been used to evaluate\nthe classiﬁcation performance. The chord recordings used\nfor training and testing purposes are separate and different.\nThe performance of the proposed feature is compared\nagainst a baseline that makes use of MFCCs (Mel Fre-\nquency Cepstral Coefﬁcients) calculated as explained in\n[22]. More speciﬁcally, 15coefﬁcients are considered with\nthe ﬁrst one, corresponding to the DC component, removed.\nA Fisher Linear Discriminant and a linear Support Vec-\ntor Machine (SVM) classiﬁer [23] have been evaluated.\nLooking at Tables 1 and 2, we observe that the results\nof the proposed method and feature are satisfactory. In al-\nmost all the cases, the performance of the proposed scheme\nis better than the one achieved by the baseline based on\nMFCCs.\nThe error in the determination of the number of played\nnotes is estimated as follows:\nError NPN=A/parenleftBigg\n|NPN e−NPNr|\nNPNr/parenrightBigg\n·100(9)\nwhereA()is the averaging operator, NPNestands for the\nestimated Number of Played Notes in (5) and NPN rrepre-\nsents the the actual number of notes.\nThe error in the estimated delay between consecutive\nnotes is evaluated as follows:\nError W=A/parenleftBigg\n|We−Wr|\nNPNe·Wd/parenrightBigg\n·100 (10)\nwhereWerepresents the windows in which a signiﬁcant\nspectral change has been found, Wrstands for the windows\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n44Instrument strokeFisher\nSPCs-KD SPCs-ED MFCCs\nGuitarup 93.88 78.88 72.22\ndown 96.11 97.22 60.55\noverall 95.00 88.05 66.38\nPianoup 91.95 79.31 77.85\ndown 97.81 84.36 81.42\noverall 94.88 81.83 79.64\nOrganup 90.00 89.16 78.33\ndown 90.00 86.66 56.66\noverall 90.00 87.91 67.50\nAutoharpup 100 94.44 97.91\ndown 100 86.80 79.86\noverall 100 90.62 88.88\nTable 1 . Success Rate (%) of UP and DOWN stroke clas-\ns\niﬁcation using a Fisher linear classiﬁer [23]. The features\nused by the classiﬁer are: SPCs-KD (Spectral Centroid of\nselected Windows with known-delay), SPCs-ED (Spectral\nCentroid of selected Windows with estimated delay) and\nMFCCs ( 15Mel Frequency Cepstral Coefﬁcients).\nInstrument strokeSVM\nSPCs-KD SPCs-ED MFCCs\nGuitarup 91.57 87.77 58.44\ndown 95.01 95.55 98.78\noverall 93.29 91.66 78.61\nPianoup 90.12 81.22 77.25\ndown 96.45 82.84 83.63\noverall 93.28 82.03 80.44\nOrganup 89.16 90.52 90.83\ndown 88.66 87.98 51.66\noverall 88.91 89.25 71.25\nAutoharpup 99.30 90.97 91.27\ndown 97.91 95.14 90.89\noverall 98.61 93.05 91.08\nTable 2 . Success Rate (%) of UP and DOWN stroke clas-\ns\niﬁcation using a linear SVM classiﬁer [23]. The features\nused by the classiﬁer are: SPCs-KD (Spectral Centroid of\nselected Windows with known-delay), SPCs-ED (Spectral\nCentroid of selected Windows with estimated delay) and\nMFCCs ( 15Mel Frequency Cepstral Coefﬁcients).\nwhere the changes actually happen and Wdis number of\nwindows between two consecutive Wrwindows. Table 3\nshows the obtained results.\nThe proposed method for the estimation of the number\nof notes and delays can be improved. This is a ﬁrst ap-\nproach to solve this problem. Our main goal has been to\ndetect the up or down stroke direction which is useful to\ncomplete the transcription of the performance of certain\ninstruments, speciﬁcally the autoharp. The performance\nattained in the detection of the stroke direction is satisfac-\ntory according to the results shown.\nIt is important to note, that even though Error Wseems\nto be quite high, this error is in most of cases positive, i.e.,\nthe change is detected one or two windows after the ﬁrstInstrument stroke Error NP NError W\nGuitarup 37.65 10.49\ndown 33.33 15.92\noverall 35.49 13.20\nPianoup 30.72 28.38\ndown 33.65 18.10\noverall 32.18 23.24\nOrganup 24.54 29.72\ndown 36.52 26.12\noverall 30.53 27.92\nAutoharpup 53.06 10.46\ndown 42.88 13.96\noverall 47.97 12.21\nTable 3 . Error (%) in the estimation of the number of notes\np\nlayed and in the estimation of the delay between consec-\nutive played notes in chords.\nInstrument strokeFisher\nSPCs-ED MFCCs\nAutoharpup 65.21 43.47\ndown 86.44 94.91\noverall 75.10 69.19\nSVM\nSPCs-ED MFCCs\nup 73.77 62.84\ndown 89.83 81.52\noverall 77.52 72.18\nTable 4 . Success Rate (%) of UP and DOWN stroke clas-\ns\niﬁcation for real autoharp chords.\nwindow that actually contains the change. This issue is not\ncritical for the feature employed by the classiﬁer because\nit is possible to observe the difference in the estimation of\ntheSPCmin (8).\nFinally, Table 4 presents the results obtained for real\nchords played in an autoharp. We have used 100chords\nfor training and 230chords for testing. The 330autoharp\nchords recorded are equaly distributed between UP and\nDOWN chords and in different tessituras. It can be ob-\nserved that the proposed feature outperforms the baseline\nproposed based on the usage of MFCCs.\n4. CONCLUSIONS\nIn this paper, a new feature to detect the up or down direc-\ntion of strokes and arpeggios has been presented. The de-\nveloped method also provides information about the num-\nber of played notes and the stroke speed.\nThe system have been tested with four different instru-\nments: classic guitar, piano, autoharp and organ and it has\nbeen shown how the new proposed feature outperforms the\nbaseline deﬁned for this task. The baseline makes use of\nthe well known MFCCs as classiﬁcation features so that\nthe baseline scheme can be easily replicated. The Matlab\nﬁles used to generate the data-set for piano, guitar and or-\ngan, the audio ﬁles of the autoharp and the ground truth are\navailable upon request for reproducible research.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n455. ACKNOWLEDGMENTS\nTh\nis work has been funded by the Ministerio de Econom´ ıa\ny Competitividad of the Spanish Government under Project\nNo. TIN2013-47276-C6-2-R, by the Junta de Andaluc´ ıa\nunder Project No. P11-TIC-7154 and by the Ministerio de\nEducaci´ on, Cultura y Deporte through the Programa Na-\ncional de Movilidad de Recursos Humanos del Plan Na-\ncional de I-D+i 2008- 2011. Universidad de M´ alaga. Cam-\npus de Excelencia Internacional Andaluc´ ıa Tech.\n6. REFERENCES\n[1] A. M. Barbancho, I. Barbancho, B. Soto, and L.J.\nTard´ on. Transcription of piano recordings. 2011 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 377–380, 2011.\n[2] A. M. Barbancho, I. Barbancho, L. J. Tard´ on, and\nE. Molina. Database of Piano Chords. An Engineer-\ning View of Harmony . SpringerBriefs in Electrical and\nComputer Engineering, 2013.\n[3] A. M. Barbancho, A. Klapuri, L.J. Tard´ on, and I. Bar-\nbancho. Automatic transcription of guitar chords and\nﬁngering from audio. IEEE Transactions on Audio,\nSpeech, and Language Processing , 20:915–921, March\n2012.\n[4] I. Barbancho, C. de la Bandera, A. M. Barbancho, and\nL. J. Tard´ on. Transcription and expressiveness detec-\ntion system for violin music. Proceedings of the IEEE\nconference on Acoustics, Speech, and Signal Proc.\n(ICASSP) , pages 189–192, March 2009.\n[5] I. Barbancho, L.J Tardon, S. Sammartino, and A.M.\nBarbancho. Inharmonicity-based method for the au-\ntomatic generation of guitar tablature. Audio, Speech,\nand Language Processing, IEEE Transactions on ,\n20(6):1857–1868, 2012.\n[6] J.P. Bello, L. Daudet, and M.B. Sandler. A tutorial on\nonset detection in music signals. IEEE Trans. on Au-\ndio, Speech and Language Processing , 14:1035–1047,\nSeptember 2005.\n[7] E. Benetos, S. Dixon, D. Giannoulis, and H. Kirchhoff.\nAutomatic music transcription: Breaking the glass\nceiling. ISMIR , 2012.\n[8] Albert S Bregman. Auditory scene analysis: The per-\nceptual organization of sound . MIT press, 1994.\n[9] D. Chadefaux, J.-L. Le Carrou, B. Fabre, and\nL. Daudet. Experimentally based description of harp\nplucking. The Journal of the Acoustical Society of\nAmerica , 131:844, 2012.\n[10] M. Goto. Development of the RWC music database. In\n18th Int. Con. on Acoustics , volume I, pages 553–556,\n2004.[11] A. Kirke and E. R. Miranda. An overview of computer\nsystems for expressive music performance. In Guide to\nComputing for Expressive Music Performance , pages\n1–47. Springer, 2013.\n[12] A. Klapuri and T. Virtanen. Automatic music transcrip-\ntion. In Handbook of Signal Processing in Acoustics ,\npages 277–303. Springer, 2009.\n[13] A.P. Klapuri. Multiple fundamental frequency estima-\ntion based on harmonicity and spectral smoothness.\nIEEE Trans. on Speech and Audio Processing , 11:804–\n816, Nov. 2003.\n[14] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent hmms\ntrained on synthesized audio. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 16(2):291–\n301, 2008.\n[15] J. A. Moorer. On the transcription of musical sound\nby computer. Computer Music Journal , pages 32–38,\n1977.\n[16] M.Schoefﬂer, F.R. Stoter, H.Bayerlein, B.Edler, and\nJ.Herre. An experiment about estimating the number\nof instruments in polyphonic music: a comparison be-\ntween internet and laboratory results. ISMIR , 2013.\n[17] M. M¨ uller, D. P. W. Ellis, A. Klapuri, and G. Richard.\nSignal processing for music analysis. IEEE Journal\nof Selected Topics in Signal Processing , 5:1088–1110,\nOctober 2011.\n[18] T. H. Ozaslan, E Guaus, E. Palacios, and J. L. Ar-\ncos. Identifying attack articulations in classical guitar.\nInComputer Music Modeling and Retrieval. Exploring\nMusic Contents. Lecture Notes in Computer Science,\npages 219–241. Springer-Verlag, 2011.\n[19] J.A. Paradiso, L.S Pardue, K.-Y . Hsiao, and A.Y . Ben-\nbasat. Electromagnetic tagging for electronic music in-\nterfaces. Journal of New Music Research , 32(4):395–\n409, 2003.\n[20] M. Peterson. Mel Bays Complete Method for Autoharp\nor Chromaharp . Mel Bay Publications, 1979.\n[21] W.H. Press, S. A. Teukolsky, W.T. Vetterling, and\nB.P. Flannery. Numerical Recipes: The Art of Scien-\ntiﬁc Computing. Third Edition . Cambridge University\nPress, 2007.\n[22] L.J. Tard´ on, S.Sammartino, and I.Barbancho. Design\nof an efﬁcient music-speech discriminator. Journal of\nthe Acoustical Society of America , 1:271–279, January\n2010.\n[23] S. Theodoridis and K. Koutroumbas. Pattern Recogni-\ntion, 4th Edition . Academic Press, 2008.\n[24] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. on Audio, Speech\nand Language Processing , 10:293–302, 2002.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n46"
    },
    {
        "title": "Exploiting Instrument-wise Playing/Non-Playing Labels for Score Synchronization of Symphonic Music.",
        "author": [
            "Alessio Bazzica",
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415772",
        "url": "https://doi.org/10.5281/zenodo.1415772",
        "ee": "https://zenodo.org/records/1415772/files/BazzicaLH14.pdf",
        "abstract": "Synchronization of a score to an audio-visual music per- formance recording is usually done by solving an audio- to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise “playing” (P) and “non-playing” (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based repre- sentation of the music piece, both at the score and perfor- mance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches.",
        "zenodo_id": 1415772,
        "dblp_key": "conf/ismir/BazzicaLH14",
        "keywords": [
            "score",
            "audio-visual music performance recording",
            "synchronization",
            "audio-to-MIDI alignment problem",
            "instrument-wise playing (P) and non-playing (NP) labels",
            "efficient way of computing the distance",
            "dynamic time warping",
            "P/NP-based representation",
            "performance-to-score synchronization",
            "robustness"
        ],
        "content": "EXPLOITING INSTRUMENT-WISE PLAYING/NON-PLAYING LABELS\nFOR SCORE SYNCHRONIZATION OF SYMPHONIC MUSIC\nAlessio Bazzica\nDelft University of Technology\na.bazzica@tudelft.nlCynthia C. S. Liem\nDelft University of Technology\nc.c.s.liem@tudelft.nlAlan Hanjalic\nDelft University of Technology\na.hanjalic@tudelft.nl\nABSTRACT\nSynchronization of a score to an audio-visual music per-\nformance recording is usually done by solving an audio-\nto-MIDI alignment problem. In this paper, we focus on the\npossibility to represent both the score and the performance\nusing information about which instrument is active at a\ngiven time stamp. More speciﬁcally, we investigate to what\nextent instrument-wise “playing” (P) and “non-playing”\n(NP) labels are informative in the synchronization process\nand what role the visual channel can have for the extraction\nof P/NP labels. After introducing the P/NP-based repre-\nsentation of the music piece, both at the score and perfor-\nmance level, we deﬁne an efﬁcient way of computing the\ndistance between the two representations, which serves as\ninput for the synchronization step based on dynamic time\nwarping. In parallel with assessing the effectiveness of the\nproposed representation, we also study its robustness when\nmissing and/or erroneous labels occur. Our experimental\nresults show that P/NP-based music piece representation is\ninformative for performance-to-score synchronization and\nmay beneﬁt the existing audio-only approaches.\n1. INTRODUCTION AND RELATED WORK\nSynchronizing an audio recording to a symbolic repre-\nsentation of the performed musical score is beneﬁcial to\nmany tasks and applications in the domains of music anal-\nysis, indexing and retrieval, like audio source separation\n[4, 9], automatic accompaniment [2], sheet music-audio\nidentiﬁcation [6] and music transcription [13]. As stated\nin [7], “sheet music and audio recordings represent and\ndescribe music on different semantic levels” thus making\nthem complementary for the functionalities they serve.\nThe need for effective and efﬁcient solutions for audio-\nscore synchronization is especially present for genres like\nsymphonic classical music, for which the task remains\nchallenging due to the typically long duration of the pieces\nand a high number of instruments involved [1]. The ex-\nisting solutions usually turn this synchronization problem\nc\rAlessio Bazzica, Cynthia C. S. Liem, Alan Hanjalic.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Alessio Bazzica, Cynthia C. S. Liem,\nAlan Hanjalic. “Exploiting Instrument-wise Playing/Non-Playing Labels\nfor Score Synchronization of Symphonic Music”, 15th International So-\nciety for Music Information Retrieval Conference, 2014.\nFigure 1: An illustration of the representation of a symphonic\nmusic piece using the matrix of playing/non-playing labels.\ninto an audio-to-audio alignment one [11], where the score\nis rendered in audio form using its MIDI representation.\nIn this paper, we investigate whether sequences of play-\ning (P) and non-playing (NP) labels, extracted per instru-\nment continuously over time, can alternatively be used to\nsynchronize a recording of a music performance to a MIDI\nﬁle. At a given time stamp, the P (NP) label is assigned to\nan instrument if it is (not) being played. If such labels are\navailable, a representation of the music piece as illustrated\nin Figure 1 can be obtained: a matrix encoding the P/NP\n“state” for different instruments occurring in the piece at\nsubsequent time stamps. Investigating the potential of this\nrepresentation for synchronization purposes, we will ad-\ndress the following research questions:\n\u000fRQ1: How robust is P/NP-based synchronization in\ncase of erroneous or missing labels?\n\u000fRQ2: How does synchronizing P/NP labels behave\nat different time resolutions?\nWe are particularly interested in this representation, as\nP/NP information for orchestra musicians will also be\npresent in the signal information of a recording. While\nsuch information will be hard to obtain from the au-\ndio channel, it can be obtained from the visual channel.\nThus, in case an audio-visual performance is available,\nusing P/NP information opens up possibilities for video-\nto-score synchronization as a means to solve a score-to-\nperformance synchronization problem.\nThe rest of the paper is structured as follows. In\nSection 2, we formulate the performance-to-score syn-\nchronization problem in terms of features based on P/NP\nlabels. Then, we explain how the P/NP matrix is con-\nstructed to represent the score (Section 3) and we elaborate\non the possibilities for extracting the P/NP matrix to rep-\nresent the analyzed performance (Section 4). In Section 5\nwe propose an efﬁcient method for solving the synchro-\nnization problem. The experimental setup is described\nin Section 6 and in Section 7 we report the results of our\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n201Figure 2: Example of a MPNP matrix with missing labels.\nexperimental assessment of the proposed synchronization\nmethodology and provide answers to our research ques-\ntions. The discussion in Section 8 concludes the paper.\n2. PROBLEM DEFINITION\nGiven an audio-visual recording of a performance and a\nsymbolic representation of the performed scores, we ad-\ndress the problem of synchronizing these two resources by\nexploiting information about the instruments which are ac-\ntive over time.\nLetL=f\u00001; 0;1gbe a set encoding the three la-\nbels non-playing (NP), missing (X) and playing (P). Let\nMPNP =fmijgbe a matrix of NI\u0002NTelements where\nNIis the number of instruments and NTis the number\nof time points at which the P/NP state is observed. The\nvalue of mij2Lrepresents the state of the i-th instru-\nment observed at the j-th time point (1\u0014i\u0014NIand\n1\u0014j\u0014NT). An example of MPNP is given in Figure 2.\nWe now assume that the matrices MAV\nPNP andMS\nPNP are\ngiven and represent the P/NP information respectively ex-\ntracted by the audio-visual recording and the sheet music.\nThe two matrices have the same number of rows and each\nrow is associated to each instrumental part. The number of\ncolumns, i.e. observations over time, is in general different.\nThe synchronization problem can be then formulated as\nthe problem of ﬁnding a time map fsync:f1: : : NAV\nTg!\nf1: : : NS\nTglinking the observation time points of the two\nresources.\n3. SCORE P/NP REPRESENTATION\nFor a given piece, we generate one P/NP matrix MS\nPNP\nfor the score relying on the corresponding MIDI ﬁle as the\ninformation source.\nWe start generating the representation of the score by\nparsing the data of each available track in the given MIDI\nﬁle. Typically, one track per instrument is added and is\nused as a symbolic representation of the instrumental part’s\nscore. More precisely, when there is more than one track\nfor the same instrument (e.g. Violin 1, Violin 2 - which\nare two different instrumental parts), we keep both tracks\nas separate. In the second step, we use a sliding window\nthat moves along the MIDI ﬁle and derive a P/NP label per\ntrack and window position. A track receives a P label if\nthere is at least one note played within the window. We\nwork with the window in order to comply with the fact\nthat a played note has a beginning and end and therefore\nlasts for an interval of time. In this sense, a played note\nis registered when there is an overlap between the sliding\nwindow and the play interval of that note.The length of the window is selected such that short\nrests within a musical phrase do not lead to misleading\nP-NP-P switches. We namely consider a musician in the\n“play” mode if she is within the “active” sequence of the\npiece with respect to her instrumental part’s score, in-\ndependently whether at some time stamps no notes are\nplayed. In our experiments, we use a window length of\n4 seconds which has been determined by empirical evalu-\nation, and a step-size of 1 second. This process generates\none label per track every second.\nIn order to generalize the parameter setting for window\nlength and offset, we also related them to the internal MIDI\nﬁle time unit. For this purpose, we set a reference value for\nthe tempo. Once the value is assigned, the sliding window\nparameters are converted from seconds to beats. The eas-\niest choice is adopting a ﬁxed value of tempo for every\nperformance. Alternatively, when an audio-visual record-\ning is available, the reference tempo can be estimated as\nthe number of beats in the MIDI ﬁle divided by the length\nof the recording expressed in minutes. A detailed investi-\ngation of different choices of the tempo is reported in [6].\n4. PERFORMANCE P/NP REPRESENTATION\nWhile an automated method could be thought of to extract\nthe P/NP matrix MAV\nPNP from a given audio-visual record-\ning, developing such a method is beyond the scope of this\npaper. Instead, our core focus is assessing the potential of\nsuch a matrix for synchronization purposes, taking into ac-\ncount the fact that labels obtained from real-world data can\nbe noisy or even missing. We therefore deploy two strate-\ngies which mimic the automated extraction of the MAV\nPNP\nmatrices. We generate them: (i) artiﬁcially, by producing\n(noisy) variations of the P/NP matrices derived from MIDI\nﬁles (Section 4.1), and (ii) more realistically, by deriving\nthe labels directly from the visual channel of a recording\nin a semi-automatic way (Section 4.2).\n4.1 Generating synthetic P/NP matrices\nThe ﬁrst strategy produces synthetic P/NP matrices by an-\nalyzing MIDI ﬁles as follows. Similarly to the process of\ngenerating a P/NP matrix for the score, we apply a slid-\ning window to the MIDI ﬁle and extract labels per instru-\nmental track at each window position. This time, however,\ntime is randomly warped, i.e. the sliding window moves\nover time with non-constant velocity. More speciﬁcally,\nwe generate random time-warping functions by randomly\nchanging slope every 3 minutes and by adding a certain\namount of random noise in order to avoid perfect piecewise\nlinear functions. In a real audio-visual recording analysis\npipeline, we expect that erroneous and missing P/NP labels\nwill occur. Missing labels may occur if musicians cannot\nbe detected, e.g. because of occlusion or leaving the cam-\nera’s angle of view in case of camera movement. In order\nto simulate such sources of noise, we modify the gener-\nated P/NP tracks by randomly ﬂipping and/or deleting pre-\ndetermined amounts of labels at random positions of the\nP/NP matrices.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n202Figure 3: Example of P/NP labels extracted from the visual channel (red dots) and compared to labels extracted by the score (blue line).\n4.2 Obtaining P/NP matrices from a video recording\nThe second strategy more closely mimics the actual video\nanalysis process and involves a simple, but effective\nmethod that we introduce for this purpose. In this method,\nwe build on the fact that video recordings of a symphonic\nmusic piece are typically characterized by regular close-up\nshots of different musicians. From the key frames rep-\nresenting these shots, as illustrated by the examples in\nFigure 4, it can be inferred whether they are using their\ninstrument at that time stamp or not, for instance by inves-\ntigating their body pose [14].\nFigure 4: Examples of body poses indicating playing/non-\nplaying state of a musician.\nIn the ﬁrst step, a key frame is extracted every second\nin order to produce one label per second, as in the case of\nthe scores. Faces are detected via off-the-shelf face detec-\ntors and upper-body images are extracted by extending the\nbounding box’s areas of face detector outputs. We clus-\nter the obtained images using low-level global features en-\ncoding color, shape and texture information. Clustering\nis done using k-means with the goal to isolate images of\ndifferent musicians. In order to obtain high precision, we\nchoose a large value for k. As a result, we obtain clus-\nters mostly containing images of the same musician, but\nalso multiple clusters for the same musician. Noisy clus-\nters (those not dominated by a single musician) are dis-\ncarded, while the remaining are labeled by linking them to\nthe correspondent track of the MIDI ﬁle (according to the\nmusician’s instrument and position in the orchestra, i.e. the\ninstrumental part). In order to label the upper-body images\nas P/NP, we generate sub-clusters using the same features\nas those extracted in the previous (clustering) step. Us-\ning once again k-means, but now with kequal to 3 (one\ncluster meant for P labels, one for NP and one extra label\nfor possible outliers), we build sub-clusters which we label\nas either playing (P), non-playing (NP) or undeﬁned (X).\nOnce the labels for every musician are obtained, they are\naggregated by instrumental part (e.g. the labels from all the\nViolin 2 players are combined by majority voting). An ex-\nample of a P/NP subsequence extracted by visual analysis\nis given in Figure 3.5. SYNCHRONIZATION METHODOLOGY\nIn this section, we describe the synchronization strat-\negy used in our experiments. The general idea is to\ncompare conﬁgurations of P/NP labels for every pair of\nperformance-score time points and produce a distance ma-\ntrix. The latter can then serve as input into a synchroniza-\ntion algorithm, for which we adopt the well-known dy-\nnamic time warping (DTW) principle. This implies we will\nnot be able to handle undeﬁned amounts of repeats of parts\nof the score. However, this is a general issue for DTW also\nholding for existing synchronization approaches, which we\nconsider out of the scope of this paper.\nIn order to ﬁnd the time map between performance and\nscore, we need to solve the problem of ﬁnding time links\nbetween the given MAV\nPNP andMS\nPNP matrices. To this end,\nwe use a state-of-the-art DTW algorithm [12].\n5.1 Computing the distance matrix\nTen Holt et. al. [12] compute the distance matrix through\nthe following steps: (i) both dimensions of the matrices\nare normalized to have zero mean and unit variance, (ii)\noptionally a Gaussian ﬁlter is applied, and (iii) pairs of\nvectors are compared using the city block distance. In our\ncase, we take advantage of the fact that our matrices con-\ntain values belonging to the ﬁnite set of 3 different integers,\nnamely the set Lintroduced in Section 2. This enables us\nto propose an alternative, just as effective, but more efﬁ-\ncient method to compute the distance matrix.\nLetmAV\njandmS\nkbe two column vectors respec-\ntively belonging to MAV\nPNP andMS\nPNP . To measure how\n(dis-)similar those two vectors are, we deﬁne a correlation\nscore sjkas follows:\nsjk= corr(mAV\nj;mS\nk) =NIX\ni=1mAV\nij\u0001mS\nik\nFrom such deﬁnition, it follows that a pair of observed\nmatching labels add a positive unitary contribution. If the\nobserved labels do not match, the added contribution is\nunitary and negative. Finally, if one or both labels are not\nobserved (i.e. at least one of them is X), the contribution is\n0. Hence, it also holds \u0000NI\u0014sjk\u0014+NI. The maximum\nis reached only if the two vectors are equal. Correlation\nscores can be efﬁciently computed as dot-product of the\ngiven P/NP matrices, namely as (MAV\nPNP)>MS\nPNP.\nThe distance matrix D=fdjkg, whose values are zero\nwhen the compared vectors are equal, can now be com-\nputed as djk=NI\u0000sjk. As a result, Dwill have NAV\nT\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n203noisy MPNP very noisy MPNP\nTen Holt et. al. our method Ten Holt et. al. our method\nTable 1: Comparing our distance matrix deﬁnition to Ten Holt et. al. [12]. By visual inspection, we observe comparable alignment\nperformances. However, the computation of our distance matrix is much faster.\nrows and NS\nTcolumns. When the correlation is the highest,\nnamely equal to NI, the distance will be zero.\nOur approach has two properties that make the com-\nputation of Dfast: Dis computed via the dot product\nand it contains integer values only (as opposed to stan-\ndard methods based on real-valued distances). As shown\nin Table 1, both the distance matrix proposed in [12] and\nusing our deﬁnition produce comparable results. Since our\nmethod allows signiﬁcantly faster computation (up to 40\ntimes faster), we adopt it in our experiments.\n5.2 Dynamic Time Warping\nOnce the distance matrix Dis computed, the time map\nbetween MAV\nPNP andMS\nPNP is determined by solving the\noptimization problem: P?= arg minPcost(D; P )where\nP=f(p`;p`+1)gis a path through the items of D\nhaving a cost deﬁned by the function cost(D; P ). More\nspeciﬁcally, p`= (iAV\n`; iS\n`)is a coordinate of an ele-\nment in D. The cost function is deﬁned as cost(D; P ) =PjPj\n`=1diAV\n`;iS\n`The aforementioned problem is efﬁciently\nsolved via dynamic programing using the well-known dy-\nnamic time warping (DTW) algorithm. Examples of P?\npaths computed via DTW are shown in the ﬁgures of\nTable 1.\nOnce P?is found, the time map fsync is computed\nthrough the linear interpolation of the correspondences in\nP?, i.e. the set of coordinates fp?\n`= (iAV\n`; iS\n`)g. This map\nallows to deﬁne correspondences between the two matri-\nces, as shown in the example of Figure 5.\nFigure 5: Example of produced alignment between two fully-\nobserved MPNP matrices.6. EXPERIMENTAL SETUP\nIn this section, we describe our experimental setup in-\ncluding details about the dataset. In order to ensure the\nreproducibility of the experiments, we release the code\nand share the URLs of the analyzed freely available MIDI\nﬁles1.\nWe evaluate the performances of our method on a set\nof 29 symphonic pieces composed by Beethoven, Mahler,\nMozart and Schubert. The dataset consists of 114 MIDI\nﬁles. Each MIDI ﬁle contains a number of tracks cor-\nresponding to different parts performed in a symphonic\npiece. For instance, ﬁrst and second violins are typically\nencoded in two different parts (e.g. “Violin 1” and “Violin\n2”). In such a case, we keep both tracks separate since mu-\nsicians in the visual channel can be labeled according to\nthe score which they perform (and not just by their instru-\nment). We ensured that the MIDI ﬁles contain tracks which\nare mutually synchronized (i.e. MIDI ﬁles of type 1). The\nnumber of instrumental parts, or MIDI tracks, ranges be-\ntween 7 and 31 and is distributed as shown in Figure 7.\nFigure 7: Distribution of the number of instrumental parts across\nperformances in the data set.\nFor each MIDI ﬁle, we perform the following steps.\nFirst, we generate one MS\nPNP matrix using a ﬁxed ref-\nerence tempo of 100 BPM. The reason why we use the\nsame value for every piece is that we evaluate our method\non artiﬁcial warping paths, hence we do not need to\nadapt the sliding window parameters to any actual perfor-\nmance. Then we generate one random time-warping func-\ntion which has two functions: (i) it is used as ground-truth\nwhen evaluating the alignment performance, and (ii) it is\nused to make one time-warped P/NP matrix MAV\nPNP. The\nlatter is used as template to build noisy copies of MAV\nPNP\nand evaluate the robustness of our method. Each tem-\nplate P/NP matrix is used to generate a set of noisy P/NP\nmatrices which are affected by different pre-determined\namounts of noise. We consider two sources of noise: mis-\ntaken and missing labels. For both sources, we generate\n1http://homepage.tudelft.nl/f8j6a/ISMIR2014baz.zip\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n204(a)Tolerance: 1 second.\n (b)Tolerance: 2 seconds.\n (c)Tolerance: 5 seconds.\nFigure 6: Average matching rates as a function of the percentage of mistaken and/or missing labels at different tolerance thresholds.\nthe following percentages of noisy labels: 0% (noiseless),\n2%, 5%, 10%, 20%, 30%, 40% and 50%. For every pair\nof noise percentages, e.g. 5% mistaken + 10% missing,\nwe create 5 different noisy versions of the original P/NP\nmatrix2. Therefore, for each MIDI ﬁle, the ﬁnal set of\nmatrices has the size 1 + (8\u00028\u00001)\u00025 = 316. Overall,\nwe evaluate the temporal alignment of 316\u0002114 = 36024\nP/NP sequences.\nFor each pair of MPNP matrices to be aligned, we com-\npute the matching rate by sampling fsync and measuring\nthe distance from the true alignment. A match occurs when\nthe distance between linked time points is below a thresh-\nold. In our experiments, we evaluate the matching rate us-\ning three different threshold values: 1, 2 and 5 seconds.\nFinally, we apply the video-based P/NP label extrac-\ntion strategy described in Section 4.2 to a multiple cam-\nera video recording of the 4th movement of Symphony\nno. 3 op. 55 of Beethoven performed by the Royal Con-\ncertgebouw Orchestra (The Netherlands). For this perfor-\nmance, in which 54 musicians play 19 instrumental parts,\nwe use the MIDI ﬁle and the correspondent performance-\nscore temporal alignment ﬁle which are shared by the au-\nthors of [8]. The latter is used as ground truth when evalu-\nating the synchronization performance.\n7. RESULTS\nIn this section, we present the obtained results and pro-\nvide answers to the research questions posed in Section 1.\nWe start by presenting in Figure 6 the computed matching\nrates in 3 distinct matrices, one for each threshold value.\nGiven a threshold, the overall matching rates are reported\nin an 8\u00028matrix since we separately compute the aver-\nage matching rate for each pair of mistaken-missing noise\nrates. Overall, we see two expected effects: (i) the average\nmatching rate decreases for larger amounts of noise, and\n(ii) the performance increases with the increasing thresh-\nold. What was not expected is the fact that the best perfor-\nmance is not obtained in the noiseless case. For instance,\nwhen the threshold is 5 seconds, we obtained an average\nmatching rate of 81.7% in the noiseless case and 85.0%\nin the case of 0% mistaken and 10% missing labels. One\npossible explanation is that 10% missing labels could give\nmore “freedom” to the DTW algorithm than the noiseless\n2We do not add extra copies for the pair (0%,0%), i.e. the template\nmatrix.case. Such freedom may lead to a better global optimiza-\ntion. In order to fully understand the reported outcome,\nhowever, further investigation is needed, which we leave\nfor future work.\nAs for our ﬁrst research question, we conclude that the\nalignment through P/NP sequences is more robust to miss-\ning labels than to mistaken ones. We show this by the fact\nthat the performance for 0% mistaken and 50% missing la-\nbels are higher than in the opposite case, namely for 50%\nmistaken and 0% missing labels. In general the best perfor-\nmance is obtained for up to 10% mistaken and 30% miss-\ning labels.\nIn the second research question we address the behav-\nior at different time resolutions. Since labels are sampled\nevery second, it is clear why acceptable matching rates are\nonly obtained at coarse resolution (namely for a threshold\nof 5 seconds).\nFinally, we comment on the results obtained when syn-\nchronizing through the P/NP labels assigned via visual\nanalysis. The P/NP matrix, shown in Figure 8a, is affected\nby noise as follows: there are 53.95% missing and 8.65%\nmistaken labels.\n(a)MAV\nPNP andMS\nPNP\n (b)DTW\nFigure 8: Real data example: P/NP labels by analysis of video\nWe immediately notice the large amount of missing la-\nbels. This is mainly caused by the inability to infer a P/NP\nlabel at those time points when all the musicians of a cer-\ntain instrumental part are not recorded. Additionally, some\nof the image clusters generated as described in Section 4.2\nare not pure and hence labeled as X.\nThe obtained synchronization performance at 1, 2 and 5\nseconds of tolerance are respectively 18.74%, 34.49% and\n60.70%. This is in line with the results obtained with syn-\nthetic data whose performance at 10% of mistaken labels\nand 50% of missing for the three different tolerances are\n24.3%, 44.2% and 65.9%. Carrying out the second exper-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n205iment was also useful to get insight about the distribution\nof missing labels. By inspecting Figure 8a, we notice that\nsuch a type of noise is not randomly distributed. Some\nmusicians are sparsely observed over time hence leading\nto missing labels patterns which differ from uniform dis-\ntributed random noise.\n8. DISCUSSION\nIn this paper, we presented a novel method to synchro-\nnize score information of a symphonic piece to a perfor-\nmance of this piece. In doing this, we used a simple feature\n(the act of playing or not) which trivially is encoded in the\nscore, and feasibly can be obtained from the visual channel\nof an audio-visual recording of the performance. Unique\nabout our approach is that both for the score and the perfor-\nmance, we start from measuring individual musician con-\ntributions, and only then aggregate up to the full ensemble\nlevel to perform synchronization. This makes a case for us-\ning the visual channel of an audio-visual recording. In the\naudio channel, which so far has predominantly been con-\nsidered for score-to-performance synchronization, even if\nseparate microphones are used per instrument, different in-\nstruments will never be fully isolated from each other in a\nrealistic playing setting. Furthermore, audio source sep-\naration for polyphonic orchestral music is far from being\nsolved. However, in the visual channel, different players\nare separated by default, up to the point that a ﬁrst clarinet\nplayer can be distinguished from a second clarinet player,\nand individual contributions can be measured for both.\nOur method still works at a rough time resolution, and\nlacks the temporal sub-second precision of typical audio-\nscore synchronization methods. However, it is compu-\ntationally inexpensive, and thus can quickly provide a\nrough synchronization, in which individual instrumental\npart contributions are automatically marked over time.\nConsequently, interesting follow-up approaches could be\ndevised, in which cross- or multi- modal approaches might\nlead to stronger solutions, as already argued in [3, 10].\nFor the problem of score synchronization, a logical next\nstep is to combine our analysis with typical audio-score\nsynchronization approaches, or approaches generally re-\nlying on multiple synchronization methods, such as [5],\nto investigate whether a combination of methods improves\nthe precision and efﬁciency of the synchronization proce-\ndure. Our added visual information layer can further be\nuseful for e.g. devising structural performance characteris-\ntics, e.g. the occurrence of repeats. Our general synchro-\nnization results will also be useful for source separation\nprocedures, since the obtained P/NP annotations indicate\nactive sound-producing sources over time. Furthermore,\nresults of our method can serve applications focusing on\nstudying and learning about musical performances. We can\neasily output an activity map or multidimensional time-\nscrolling bar, visualizing which orchestra parts are active\nover time in a performance. Information about expected\nmusical activity across sections can also help directing the\nfocus of an audience member towards dedicated players or\nthe full ensemble.Finally, it will be interesting to investigate points where\nP/NP information in the visual and score channel clearly\ndisagree. For example, in Figure 3, some time after the\nﬂutist starts playing, there is a moment where the score\nindicates a non-playing interval, while the ﬂutist keeps a\nplaying pose. We hypothesize that this indicates that, while\na (long) rest is notated, the musical discourse actually still\ncontinues. While this also will need further investigation,\nthis opens up new possibilities for research in performance\nanalysis and musical phrasing, broadening the potential\nimpact of this work even further.\nAcknowledgements The research leading to these results has received\nfunding from the European Union Seventh Framework Programme FP7\n/ 2007–2013 through the PHENICX project under Grant Agreement no.\n601166.\n9. REFERENCES\n[1] A. D’Aguanno and G. Vercellesi. Automatic Music Synchronization\nUsing Partial Score Representation Based on IEEE 1599. Journal of\nMultimedia, 4(1), 2009.\n[2] R.B. Dannenberg and C. Raphael. Music Score Alignment and Com-\nputer Accompaniment. Communications of the ACM, 49(8):38–43,\n2006.\n[3] S. Essid and G. Richard. Fusion of Multimodal Information in Music\nContent Analysis. Multimodal Music Processing, 3:37–52, 2012.\n[4] S. Ewert and M. Muller. Using Score-informed Constraints for NMF-\nbased Source Separation. In Acoustics, Speech and Signal Processing\n(ICASSP), 2012 IEEE International Conference on, pages 129–132.\nIEEE, 2012.\n[5] S. Ewert, M. M ¨uller, and R.B. Dannenberg. Towards Reliable Par-\ntial Music Alignments Using Multiple Synchronization Strategies. In\nAdaptive Multimedia Retrieval. Understanding Media and Adapting\nto the User, pages 35–48. Springer, 2011.\n[6] C. Fremerey, M. Clausen, S. Ewert, and M. M ¨uller. Sheet Music-\nAudio Identiﬁcation. In ISMIR, pages 645–650, 2009.\n[7] C. Fremerey, M. M ¨uller, and M. Clausen. Towards Bridging the Gap\nbetween Sheet Music and Audio. Knowledge Representation for In-\ntelligent Music Processing, (09051), 2009.\n[8] M. Grachten, M. Gasser, A. Arzt, and G. Widmer. Automatic Align-\nment of Music Performances with Structural Differences. In ISMIR,\npages 607–612, 2013.\n[9] Y . Han and C. Raphael. Informed Source Separation of Orchestra and\nSoloist Using Masking and Unmasking. In ISCA-SAPA Tutorial and\nResearch Workshop, Makuhari, Japan, 2010.\n[10] C.C.S. Liem, M. M ¨uller, D. Eck, G. Tzanetakis, and A. Hanjalic.\nThe Need for Music Information Retrieval with User-centered and\nMultimodal Strategies. In Proceedings of the 1st international ACM\nworkshop MIRUM, pages 1–6. ACM, 2011.\n[11] M. M ¨uller, F. Kurth, and T. R ¨oder. Towards an Efﬁcient Algorithm\nfor Automatic Score-to-Audio Synchronization. In ISMIR, 2004.\n[12] G.A. Ten Holt, M.J.T. Reinders, and E.A. Hendriks. Multi-\ndimensional Dynamic Time Warping for Gesture Recognition. In\n13th annual conference of the Advanced School for Computing and\nImaging, volume 119, 2007.\n[13] R.J. Turetsky and D.P.W. Ellis. Ground-truth Transcriptions of Real\nMusic from Force-aligned MIDI Syntheses. ISMIR 2003, pages 135–\n141, 2003.\n[14] B. Yao, J. Ma, and L. Fei-Fei. Discovering Object Functionality. In\nICCV, pages 2512–2519, 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n206"
    },
    {
        "title": "Template Adaptation for Improving Automatic Music Transcription.",
        "author": [
            "Emmanouil Benetos",
            "Roland Badeau",
            "Tillman Weyde",
            "Gaël Richard"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418059",
        "url": "https://doi.org/10.5281/zenodo.1418059",
        "ee": "https://zenodo.org/records/1418059/files/BenetosBWR14.pdf",
        "abstract": "In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based auto- matic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By per- forming a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We pro- pose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch de- tection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed.",
        "zenodo_id": 1418059,
        "dblp_key": "conf/ismir/BenetosBWR14",
        "keywords": [
            "automatic music transcription",
            "dictionary templates",
            "spectral shape",
            "instrument sources",
            "conservative transcription pre-processing",
            "adaptive transcription",
            "single-instrument transcription",
            "multiple-instrument transcription",
            "MAPS database",
            "Bach10 database"
        ],
        "content": "TEMPLATE ADAPTATION FOR IMPROVING\nAUT\nOMATIC MUSIC TRANSCRIPTION\nEmmanouilBenetos†, Roland Badeau‡, Tillman Weyde†and Ga¨el Richard‡\n†Department of Computer Science,City University London, UK\n{emmanouil.benetos.1, t.e.weyde}@city.ac.uk\n‡Institut Mines-T ´el´ecom, T´el´ecomParisTech, CNRS LTCI, France\n{roland.badeau, gael.richard}@telecom-paristech.fr\nABSTRACT\nIn this work, we propose a system for automatic music\ntranscriptionwhichadaptsdictionarytemplatessothatthey\nclosely match the spectral shape of the instrument sources\npresent in each recording. Current dictionary-based auto-\nmatictranscriptionsystemskeeptheinputdictionaryﬁxed,\nthusthespectralshapeofthedictionarycomponentsmight\nnotmatchtheshapeofthetestinstrumentsources. Byper-\nforming a conservative transcription pre-processing step,\nthe spectral shape of detected notes can be extracted and\nutilized in order to adapt the template dictionary. We pro-\npose two variants for adaptive transcription, namely for\nsingle-instrumenttranscriptionandformultiple-instrument\ntranscription. ExperimentsarecarriedoutusingtheMAPS\nand Bach10 databases. Results in terms of multi-pitch de-\ntectionandinstrumentassignmentshowthatthereisaclear\nand consistent improvement when adapting the dictionary\nin contrast withkeeping the dictionary ﬁxed.\n1. INTRODUCTION\nAutomaticmusictranscription(AMT)isdeﬁnedasthepro-\ncessofconvertinganacousticmusicsignalintosomeform\nof music notation [3]. Subtasks of AMT include multi-\npitchdetection,onset/offsetdetection,andinstrumentiden-\ntiﬁcation. Recently, the vast majority of transcription ap-\nproaches use spectrogram factorization methods such as\nnon-negative matrix factorization (NMF) and probabilis-\ntic latent component analysis (PLCA), which attempt to\ndecompose an input non-negative spectrogram into spec-\ntral templates and note activations (e.g. [2,10,17]). The\nspectral templates can either be pre-extracted and stored\nin a template dictionary [2,17] or can be estimated using\nparametric spectral models [10]. An open problem with\ndictionary-based methods is that the templates might not\nmatch the spectral shape of the input instrument sources.\nEB is supported by a City University London Research Fellowship.\nThis\nwork was supported by a T ´el´ecom ParisTech sabbatical grant.\nc/circlecop†rtE.Benetos, R. Badeau, T. Weyde,and G. Richard.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Emmanouil Benetos, Roland Badeau,\nTillman Weyde, and Ga ¨el Richard. “Template adaptation for improving\nautomatic music transcription”, 15th International Society for Music In-\nformation Retrieval Conference,2014.Also, unconstrained methods such as NMF and standard\nPLCA that jointly update the spectral templates and pitch\nactivationscanleadtothecreationofnon-informativebases,\nand thus, to poor transcription results. It has been shown\n(e.g. [3]) that the use of templates from the same instru-\nment model or recording conditions can dramatically im-\nprove transcription performance.\nRelated work on automatically estimating or adapting\ntemplatesfortranscriptionincludes[12],wheretheauthors\nproposed a system for user-assisted (i.e. semi-automatic)\nmusictranscriptioninanNMFsetting. Theusercanlabela\nfewnotesintherecording;knowledgeofthelabellednotes\ncan be used in order to create a dictionary that matches\nthe input source. In addition, in [18], the authors propose\na dictionary adaptation step within a sparse model that is\nsuitable for single-instrument multi-pitch detection.\nIn this paper, we propose a method for template adap-\ntation suitable for multiple-instrument polyphonic music\ntranscription(supportingbothmulti-pitchdetectionandin-\nstrument assignment). The proposed method is based on a\nmultiple-instrumenttranscriptionsystemusingPLCA,and\nsupportingtuningchangesandfrequencymodulations. By\nperformingaconservativetranscriptioninapre-processing\nstep, notes are detected with a high degree of conﬁdence\nand are used in order to expand the current template dic-\ntionary. An additional PLCA-based dictionary adaptation\nstep can further reﬁne the dictionary, so that it matches\nclosely the input source(s). Two system variants are pro-\nposed, for single- and multiple-instrument transcription.\nExperimentsusingtheMAPS[8]andBach10[7]databases\nshow a consistent improvement in multi-pitch detection\nandinstrumentassignmentperformancewhentheproposed\ntemplate adaptation method is used.\nThe outline of the paper is as follows. In Section 2,\ntheproposedsingle-instrumenttranscriptionsystemispre-\nsented, with the multiple-instrument version presented in\nSection 3. The employed datasets, evaluation metrics, and\nresults are detailed in Section 4. Finally, conclusions are\ndrawn and future directions are indicated in Section 5.\n2. SINGLE-INSTRUMENT SYSTEM\nInthefollowing,wedescribeamethodforsingle-instrument\npolyphonic music transcription based on a dictionary of\npre-extracted note templates, which is adapted in order to\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n175match the input instrument source. The proposed system\ncontains\na“conservative”transcriptionpre-processingstep\nin order to detect notes with a high degree of conﬁdence,\na dictionary adaptation step, and a ﬁnal transcription step.\nThediagramoftheproposedsystemcanbeseeninFig. 1.\n2.1 Pre-processing\nAs a pre-processing step, we perform an initial transcrip-\ntion which uses a ﬁxed template dictionary (in which the\ntemplatesmightnotbeextractedfromthesameinstrument\nsource, model, or recording conditions). The main goal is\nto only detect notes for which we have a high degree of\nconﬁdence;inordertoachievethis,weperforma“conser-\nvative” transcription, as in [16], where the employed tran-\nscription system detects notes with high precision and low\nrecall. In other words, the system returns few false alarms\nbut might miss several notes present in the recording.\nIn order to perform the conservative transcription pre-\nprocessingstep,weusethespectrogramfactorization-based\nmodelof[2],whichisbasedonprobabilisticlatentcompo-\nnent analysis (PLCA) [14] and supports the use of a ﬁxed\ntemplate dictionary. It should be noted that the system\nin [2] ranked ﬁrst in the MIREX transcription tasks [1].\nThemodelof[2]takesasinputanormalizedlog-frequency\nspectrogram Vω,t∈RΩ×T(ωdenotes frequency and t\ntime) and approximates it as a bivariate probability distri-\nbutionP(ω,t).P(ω,t)is in turn decomposed as:\nP(ω,t) =P(t)/summationdisplay\np,f,sP(ω|s,p,f)Pt(f|p)Pt(s|p)Pt(p)\n(1)\nwherep,f,sdenote pitch, log-frequency shifting, and in-\nstrument source (in the single-instrument case, srefers to\ninstrument model), respectively. P(t)is the spectrogram\nenergy(knownquantity)and P(ω|s,p,f)arepre-extracted\nspectral templates for pitch p, source/model s, which are\nalso pre-shifted across log-frequency according to param-\neterf.Pt(f|p)is the time-varying log-frequency shifting\nfor pitch p,Pt(s|p)is the source contribution, and Pt(p)\nis the pitch activation. As a log-frequency representation\nwe use the constant-Q transform [13] with 60 bins/octave,\nresultingin f∈[1,...,5],where f= 3istheidealtuning\nposition for the template (using equal temperament).\nUsing a ﬁxed template dictionary, the parameters that\nneedtobeestimatedare Pt(f|p),Pt(s|p),andPt(p). This\ncan be achieved using the expectation-maximization (EM)\nalgorithm [5], with 15-20 iterations being typically sufﬁ-\ncient. Theresultingmulti-pitchoutputisgivenby P(p,t) =\nP(t)Pt(p).\nInordertoextractnoteeventsinspectrogramfactorization-\nbasedAMTalgorithms,typicallythresholdingisperformed\non the pitch activations (P (p,t)in this case). The value\nof the threshold θcontrols the levels of precision/recall. A\nlowthresholdhasahighrecallandlowprecision;theoppo-\nsiteoccurswithahighthreshold. Byselectingahighvalue\nofθ, in essence we perform a conservative transcription.\nThe ﬁnal output of the pre-processing step is a collection\nof pitches and time frames {p1,t1},{p2,t2},...,{pN,tN}\nwhichcanbeusedinordertoadaptthetemplatedictionary.\nnorm. magnitude\nω(log-frequencybin)norm. magnitude\nω(log-frequencybin)i(frameindex)ω\n0 100 200 300 400 500 6000 100 200 300 400 500 600100 200 300 400 500 600 700\n00.050.10.150.200.10.20.30.4100200300400500\nFigure 2. Top: a collection of spectra ˆV(42)(note D4)\nfrom piano recording ‘alb se2’ taken from the MAPS\ndatabase\n(piano model: ENSTDkCl). Middle: extracted\nnormalisedtemplate P(ω|p= 42). Bottom: aD4template\nfrompiano model AkPnBcht from the MAPS database.\n2.2 Template Adaptation\nGiven a collection of detected pitches, the ﬁrst step re-\ngarding template adaptation is to collect the spectra that\ncorrespondtotheaforementionedpitchesintherecording.\nThus, for each pitch pall time frames tipthat contain that\npitcharecollected(where i= 1,...,N pandNpisthenum-\nber of frames containing p).\nSubsequently, for each pitch pwe create a collection of\nspectra where that pitch is observed:\nˆV(p)=Vω,t∈tip⊗hp (2)\nwherehpis a harmonic comb that serves as an indicator\nfunction (setting to zero all frequency bins not belonging\nto pitchp), and⊗denotes elementwise multiplication. In\nother words, ˆV(p)∈RΩ×Npis a collection of the spectra\ncorresponding to detected pitch pin the input recording.\nUsing information from ˆV(p), new spectral templates\narecreatedforeach pthatwasdetectedintheconservative\ntranscription step. In order to create the new templates,\nthe standard PLCA algorithm is used with one component\n[14], with the input in each case being ˆV(p). The output\nforeachpisaspectraltemplate w(p)whichcanbeusedin\norder to expand the present dictionary.\nGiven thattheconservative transcriptionstepmight not\nhave detected all possible pitches present in the recording,\ninformationfromtheextractedtemplatescanbeusedinor-\nder to estimate missing templates. As in the user-assisted\ncase of [12], we can derive templates at missing pitches\nbysimplyshiftingexistingtemplatesacrosslog-frequency.\nGiven a missing pitch template, we consider a neighbor-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n176AUDIO\nREPRESENT\nATIONCONSERVATIVE DICTIONARY\nDICTIONARYTRANSCRIPTIONTRANSCRIPTION OUTPUT\nDICTIONARYUPDATED ORIGINALT/F\nEXP\nANSION(+ OPTIONAL\nPLCA ADAPTATION)\nFigure 1. Proposed system diagram.\nhoodofupto4semitones;ifatemplateexistsintheneigh-\nborhood, it is shifted accordingly in order to estimate the\nmissingtemplate. Finally,theresultingtemplatedictionary\nis pre-shifted across log-frequency over a semitone range\nin order to account for tuning deviations and frequency\nmodulations. The output of the template adaptation step\nis normalized and denoted as P(ω|s=snew,p,f), where\nsnewrefers to the new instrument source that is added to\nthe existing dictionary.\nThe template adaptation step is illustrated in Fig. 2,\nwhereacollectionofextractedspectrafornoteD4ofapi-\nano recording can be seen, along with the computed tem-\nplate, as well as with a template for the same note taken\nfrom a different piano model. By comparing the two pi-\nano spectra, the importance of adapting templates to the\nspeciﬁc instrument source can be seen.\n2.3 Transcription\nHaving created an expanded dictionary with a set of note\ntemplates taken from the instrument source present in the\nrecording, the recording is re-transcribed using the new\ndictionary and the model of (1). In order to further adapt\ntheextractedtemplatestotheinputsource,anoptionalstep\nis also applied on updating the new template set during\nthe PLCA iterations. The modiﬁed iterative update rule is\nbased on the work of [15] (which incorporated prior infor-\nmation on PLCA update rules) and is applied only for the\nnew set of templates. It is formulated as:\nˆP(ω|snew,p,f) =/summationtext\ntαPt(p,f,s new|ω)Vω,t+(1−α)P(ω|snew,p,f)/summationtext\nω,tαPt(p,f,s new|ω)Vω,t+(1−α)P(ω|snew,p,f)\n(3)\nwherePt(p,f,s|ω)is the posterior of the model (deﬁned\nin [2]), and αis a parameter which controls the weight of\nthe PLCA adaptation, with (1−α)giving weight to the\nset of extracted templates from the procedure of Section\n2.2. In this work, αis set to 0.05, thus the PLCA tem-\nplate adaptation is only slightly changing the shape of the\ntemplates (given that the model is unconstrained, giving a\nlarge weight to the PLCA adaptation step would result in\nnon-meaningful templates).\nFinally, the output of the transcription step is given by\nP(p,t) =P(t)Pt(p). For converting the non-binary pitch\nactivation into a binary piano-roll representation, as in [6]\nwe perform thresholding on P(p,t)followed by a process\nremoving note events with a duration less than 80ms.3. MULTIPLE-INSTRUMENT SYSTEM\nIn dictionary-based multiple-instrument transcription, the\ndictionary typically consists of one or more sets of tem-\nplates per instrument. Thus, in order to update dictionary\ntemplates for multiple instruments, modiﬁcations need to\nbe made fromthe system presented in Section 2.\nRegardingthepre-processingstep,westillusethemodel\nof (1), which supports multiple-instrument transcription.\nIn this case, sdenotes instrument source. An advantage of\nthe model of (1) is that it can produce an instrument as-\nsignment output (i.e. each detected note is assigned to a\nspeciﬁc instrument). Thus, having estimated the unknown\nmodel parameters, the instrument assignment output for\ninstrument sinsis given by the following time-pitch rep-\nresentation:\nP(s=sins,p,t) =Pt(s=sins|p)Pt(p)P(t)(4)\nTherepresentation P(s,p,t)canbethresholdedinthesame\nwayasthepitchactivationinordertoderiveabinarypiano-\nroll representation of the notes produced by a speciﬁc in-\nstrument. Here, we perform “conservative” thresholding\n(i.e. use a high θvalue) for every instrument in P(s,p,t)\nin order to create a collection of detected pitches and time\nframes per instrument:\n{s1,p1,t1},{s2,p2,t2},...,{sN,pN,tN}(5)\nwheres∈1,...,S,p∈1,...,88, andt∈1,...,T.\nFor performing multi-instrument template adaptation,\nwe collect all time frames tipsthat contain pitch pand in-\nstrument s. We create a collection of spectra ˆV(p,s)where\na pitch is observed for a speciﬁc instrument, in the same\nway as in (2). Using information from ˆV(p,s), new spec-\ntral templates are created for speciﬁc cases of sandpus-\ning the single-component PLCA algorithm. As in Section\n2.2, templates at missing pitches are derived by translat-\ning existing templates across log-frequency. The output\nof the template adaptation step is denoted as P(ω|s=\n{snew1,snew2,...},p,f )wheresnew1,snew2,...denotethe\nnew sets of templates for the existing instruments.\nFinally, the input recording is re-transcribed using the\nmodelof(1),byutilizingtheexpandeddictionary. Wealso\napplythesameoptionalPLCA-baseddictionaryadaptation\nstep shown in Section 2.3. The multiple-instrument tran-\nscription system has two sets of outputs: the pitch activa-\ntionP(p,t)(whichisusedformulti-pitchdetectionevalu-\nation) and the instrument contribution P(s,p,t)(which is\nused for instrument assignment evaluation).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n177(c)\nt(sec)MIDI pitch(b)MIDI pitch(a)MIDI pitch\n5 10 15 20 25 30 355 10 15 20 25 30 355 10 15 20 25 30 35\n405060708090100405060708090100405060708090100\nFigure3. (a)Thepitchgroundtruthforthebassoon-violin\nduet\n‘Nun bitten’ from the Bach10 database. (b) The tran-\nscription piano-roll without template adaptation. (c) The\ntranscription piano-roll with template adaptation.\nAn example of how template adaptation can improve\ntranscriptionperformanceforamultiple-instrumentrecord-\ning (bassoon and violin) is given in Fig. 3, where the tran-\nscription output with template adaptation has signiﬁcantly\nfewerfalsealarmscomparedwithtranscriptionwithouttem-\nplate adaptation (in which many extra detected notes can\nbe seen in higher pitches).\n4. EVALUATION\n4.1 Datasets\nFor training the single-instrument system of Section 2, we\nusedisolatednoterecordingsfromthe‘AkPnBcht’and‘Sp-\ntkBGCl’ piano models of the MAPS database [8]. We\nused the standard PLCA algorithm with one component\n[14] in order to extract a single template per note, cover-\ning the complete piano note range. For testing the single-\ninstrument system, we used thirty piano segments of 30s\nduration from the MAPS database from the ‘ENSTDkCl’\npiano model; the test dataset has in the past been used for\nmulti-pitchevaluation(e.g.[2,4,19]). Forcomparativepur-\nposes, we also extracted training templates from the same\ntest source (‘ENSTDkCl’).\nFor training the multiple-instrument system of Section\n3, we used isolated note samples of bassoon and violin\nfrom the RWC database [11], covering the complete note\nrangeoftheinstruments. Fortestingthemultiple-instrument\nsystem,wecreatedtenduetsofbassoon-violin,mixedfrom\nsingleinstrumenttracksfromthemulti-trackBach10dataset\n[7]. The duration of the recordings varies from 25-41sec.\nForcomparativepurposes,wealsoextracteddictionarytem-System PrenRecnFn\nC1 66.41% 48.41% 55.33%\nC2 68.07% 48.80% 56.26%\nC3 67.84% 49.38% 56.56%\nC4 (oracle) 70.43% 50.35% 58.17%\nTable 1. Multi-pitch detection results for the single-\ninstrument\nsystem using the MAPS database.\nplates for bassoon and violin from the single instrument\ntracks of the Bach10 database, in order to demonstrate the\nupper performance limit of the transcription system.\n4.2 Metrics\nFor evaluating the performance of the proposed systems\nformulti-pitchdetection,weemployonset-onlynote-based\ntranscription metrics, which are used in the MIREX note\ntracking task [1]. A detected note is considered correct if\nitspitchmatchesagroundtruthpitchanditsonsetiswithin\na 50ms tolerance of a ground-truth onset. The resulting\nnote-basedprecision,recall,andF-measurearedeﬁnedas:\nPren=Ntp\nNsysRecn=Ntp\nNrefFn=2RecnPren\nRecn+Pren(6)\nwhereNtpis\nthenumberofcorrectlydetectedpitches, Nsys\nis the number of pitches detected by the system, and Nref\nis the number of reference pitches.\nFor the instrument assignment evaluations we use the\npitchground-truthofeachinstrumentseparately(compared\nwith the instrument-speciﬁc piano-roll output of the sys-\ntem),andcomputetheF-measuremetricsforbassoon(F b)\nand violin (F v).\n4.3 Results - Single Instrument Evaluation\nForsingle-instrumenttranscriptionevaluationusingthe30\nMAPS recordings, results are shown in Table 1 using four\ndifferent system conﬁgurations. Conﬁguration C1 corre-\nsponds to the system without template adaptation; C2 to\nthesystemwithtemplateadaptation;C3tothesystemwith\ntemplateadaptationusingboththecreationofthenewdic-\ntionary plus the PLCA update of the dictionary, as shown\nin Section 2.2. Finally, C4 refers to comparative exper-\niments without template adaptation, but using templates\nfrom the same instrument source (‘ENSTDkCl’ model in\nthesingle-instrumentcase),whichismeanttodemonstrate\nthe upper performance limitof the transcription system.\nFromthesingle-instrumentmulti-pitchdetectionresults,\nit can be seen that an improvement of +0.9% in terms of\nFnis reported when using the template adaptation pro-\ncedure; the improvement rises to +1.2% when also using\nthePLCAdictionaryadaptationupdates. Theperformance\ndifferencebetweentheoriginalC1system(withoutknowl-\nedge of the source templates) and the ‘optimal’ system\n(C4) which contains templates from the same test source\nis 2.8%; thus, the proposed template adaptation steps can\nhelp bridge the gap, without requiring any knowledge of\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n178Fn(%)\nθ0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.04055.55656.55757.558\nFigure 4. Multi-pitch detection results on the MAPS-\nENSTDkCl\nset using different values of θ.\nthe test instrument source. Regarding precision and recall,\nin all cases it can be seen that the transcription system has\nfewer false alarms than missed note detections. The pro-\nposed template adaptation steps help in equally improving\nprecision and recall.\nInordertodeterminethevalueoftheconservativetran-\nscriptionthreshold θ,weusedatrainingsubsetof10record-\nings from the MAPS ‘SptkBGCl’ models; the value of\nθ= 0.028 was selected by maximising Pren. In Fig-\nure4,transcriptionperformanceontheMAPS-ENSTDkCl\nset is reported by selecting various values for θ. It can\nbe seen that the transcription performance can reach up to\nFn=57.4% with θ= 0.015, which enforces the argument\nthattheproposedtemplateadaptationmethodcansuccess-\nfully adapt dictionary templates so that they match the in-\nput instrument source.\nAnothercomparisonforthesingle-instrumentsystemis\nmade, where the dictionary derived from Section 2.2 re-\nplaces the dictionary of instrument ‘SptkBGCl’ (instead\nof expanding the original dictionary). The resulting Fn\nis 55.88%, indicating that expanding the dictionary leads\nto better results compared with replacing the dictionary. It\nshould also be noted that the achieved transcription per-\nformance outperforms the system in [19] which reports\na frame-based F-measure of 52.4%, whereas the template\nadaptation system reports a frame-based Fof 59.73%. Fi-\nnally, no rigorous ﬁgures for statistical signiﬁcance of the\nresults can be given since all signal frames cannot be con-\nsidered as independent samples. However, the reported\ntestsarerunonseveralthousandsofframeswhichleads,if\nthe samples were independent, to a statistically signiﬁcant\ndifference of the order of 0.6% (with95% conﬁdence).\n4.4 Results - Multiple Instrument Evaluation\nFor multiple-instrument evaluation, we also use the four\ndifferent system conﬁgurations that were used for single-\ninstrument transcription. For system conﬁguration C3, we\nperform the PLCA dictionary update using 3 variants: by\nupdating the bassoon only, by updating the violin only, or\nbyupdatingbothdictionaries. Transcriptionresultsforthe\nmultiple-instrument case are shown in Table 2.\nItcanbeseenthatwithoutanytemplateadaptation(C1),\nFn= 67.72%;byperformingtheproposedtemplateadap-\ntation step (C2), the multi-pitch detection F-measure im-System PrenRecnFnFbFv\nC1 64.79% 71.20% 67.72% 70.19% 42.10%\nC2 69.71% 75.72% 72.51% 70.81% 45.98%\nC3 (violin) 70.02% 75.41% 72.50% 70.54% 44.51%\nC3 (bassoon) 72.49% 77.67% 74.90% 68.77% 45.87%\nC3 (both) 71.30% 77.37% 74.11% 67.57% 44.08%\nC4 (oracle) 74.90% 82.94% 78.64% 81.25% 62.05%\nTable 2. Multi-pitch detection and instrument assign-\nment\nresults for the multiple-instrument system using the\nBach10 dataset.\nproves by +4.8%.\nBy performing template adaptation with C3 which also\nincludes the PLCA update rule of (3), although no per-\nformance gain is obtained over the C2 conﬁguration for\nthe violin updates, there is a +2.4% improvement over C2\nwhen updating the bassoon dictionary only. Finally, when\nupdatingbothdictionaries,thereisaperformancedropfor\nFbandFvover the C2 conﬁguration (but the system still\noutperforms the original C1 system). The performance of\nthe PLCA-based dictionary updates can be explained by\nthe fact that the update rule of (3) might combine the ob-\nserved spectra from both instruments and produce dictio-\nnaries that might represent a combination of the two in-\nstruments. Finally,theC4systemrepresentstheupperper-\nformance limit, which is +11.7% higher than when using\na dictionary from a different instrument models or record-\ning conditions. It can be seen that the proposed template\nadaptationmethodshelpinbridgingthatperformancegap,\nresulting in a dictionary that closely matches the test in-\nstrument sources.\nRegarding instrument assignment performance, in all\ncases the bassoon note identiﬁcation reports better results\ncompared to violin note identiﬁcation. It can be seen that\nwith the proposed template adaptation, the bassoon identi-\nﬁcation remains relatively constant (a small improvement\nof+0.6%isreportedwhencomparingC1withC2). Onthe\nother hand, violin identiﬁcation improves by +3.9%; this\nindicatesthattheRWCbassoontemplatescloselymatched\ntheBach10bassoonmodels,whereastheviolinRWCtem-\nplates could greatly beneﬁt from template adaptation.\nBy comparing the MAPS and Bach10 evaluations, an\nobservation can be made that the performance improve-\nment using the proposed template adaptation method de-\npendsonthemismatchbetweentheoriginaldictionaryand\nthespectralshapeoftheinstrumentspresentintherecord-\nings. Thus, the 11.7% performance gap for the Bach10\ndatasetledtoagreaterimprovementforthetemplateadap-\ntation method compared to the 2.8% performance gap re-\nported for the MAPS dataset (which led to a smaller, yet\nconsistentimprovementwhenusingtheproposedtemplate\nadaptation method).\n5. CONCLUSIONS\nIn this paper, we proposed a novel method for template\nadaptation for automatic music transcription, that can be\nused in dictionary-based systems. We utilized a multiple-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n179instrument transcription system based on probabilistic la-\ntent\ncomponentanalysis,andperformedaconservativetran-\nscription pre-processing step in order to detect notes with\na high conﬁdence. Based on the initial transcription, the\nspectra of the detected notes are collected, processed, and\nare used in order to create a new dictionary that closely\nmatchesthespectralcharacteristicsoftheinputinstrument\nsource(s). Bothsingle-instrumentandmulti-instrumentvari-\nants of the proposed method are presented and evaluated,\nin terms of multi-pitch detection and instrument assign-\nment. Experimental results using the MAPS and Bach10\ndatasets show that there is a clear and consistent perfor-\nmanceimprovementwhenusingtheproposedtemplateadap-\ntationmethod,especiallywhenthereisalargediscrepancy\nbetween the original dictionary and the spectral character-\nistics of the test instrument sources.\nIn the future, we will evaluate the proposed system us-\ningmultiple-instrumentrecordingswithmorethantwoin-\nstruments. Parametric models (such as source-ﬁlter mod-\nels) will also be investigated for updating the note tem-\nplates, along with adaptive methods for deriving the con-\nservative transcription threshold. We also plan to evaluate\nthe proposed system in the next MIREX evaluations [1].\nFinally, the proposed template adaptation steps will also\nbe evaluated in the context of score-informed source sepa-\nration using spectrogram factorization models [9].\n6. REFERENCES\n[1] Music Information Retrieval Evaluation eX-\nchange (MIREX). http://music-ir.org/\nmirexwiki/.\n[2] E.Benetos,S.Cherla,andT.Weyde.Anefﬁcientshift-\ninvariant model for polyphonic music transcription. In\n6th Int. Workshop on Machine Learning and Music,\nPrague, Czech Republic, September 2013.\n[3] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff,\nand A. Klapuri. Automatic music transcription: chal-\nlenges and future directions. J. Intelligent Information\nSystems, 41(3):407–434, December 2013.\n[4] J. J. Carabias-Orti, T. Virtanen, P. Vera-Candeas,\nN. Ruiz-Reyes, and F. J. Ca ˜nadas-Quesada. Musi-\ncal instrument sound multi-excitation model for non-\nnegative spectrogram factorization. IEEE J. Selected\nTopics in Signal Processing, 5(6):1144–1158, 2011.\n[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-\nimum likelihood from incomplete data via the EM al-\ngorithm.J.RoyalStatisticalSociety,39(1):1–38,1977.\n[6] A.Dessein,A.Cont,andG.Lemaitre. Real-timepoly-\nphonic music transcription with non-negative matrix\nfactorization and beta-divergence. In 11th Int. Society\nforMusicInformationRetrievalConf.,pages489–494,\n2010.\n[7] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamen-\ntal frequency estimation by modeling spectral peaksandnon-peakregions. IEEETrans.Audio,Speech,and\nLanguage Processing, 18(8):2121–2133, 2010.\n[8] V.Emiya,R.Badeau,andB.David.Multipitchestima-\ntion of piano sounds using a new probabilistic spectral\nsmoothnessprinciple. IEEETrans.Audio, Speech, and\nLanguage Processing, 18(6):1643–1654, 2010.\n[9] S. Ewert, B.Pardo, M. M ¨uller, and M. D. Plumb-\nley. Score-informed source separation for musical au-\ndio recordings. IEEE Signal Processing Magazine,\n31(3):116–124, May 2014.\n[10] B. Fuentes, R. Badeau, and G. Richard. Harmonic\nadaptive latent component analysis of audio and ap-\nplication to music transcription. IEEE Trans. Audio,\nSpeech, and Language Processing, 21(9):1854–1866,\n2013.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: music genre database and mu-\nsical instrument sound database. In International Con-\nferenceonMusicInformationRetrieval,October2003.\n[12] H. Kirchhoff, S. Dixon, and Anssi Klapuri. Missing\ntemplate estimation for user-assisted music transcrip-\ntion. InIEEEInt.Conf.Audio,SpeechandSignalPro-\ncessing, pages 26–30, 2013.\n[13] C. Sch ¨orkhuber and A. Klapuri. Constant-Q transform\ntoolbox for music processing. In 7th Sound and Music\nComputing Conf., Barcelona, Spain, July 2010.\n[14] M. Shashanka, B. Raj, and P. Smaragdis. Probabilis-\nticlatentvariablemodelsasnonnegativefactorizations.\nComputational Intelligence and Neuroscience, 2008.\nArticle ID947438.\n[15] P. Smaragdis and G. Mysore. Separation by “hum-\nming”: user-guided sound extraction from mono-\nphonicmixtures.In IEEEWorkshoponApplicationsof\nSignal Processing to Audio and Acoustics, pages 69–\n72, New Paltz, USA, October 2009.\n[16] D. Tidhar, M. Mauch, and S. Dixon. High precision\nfrequency estimationforharpsichordtuningclassiﬁca-\ntion. InIEEEInt.Conf.Audio,SpeechandSignalPro-\ncessing, pages 61–64, Dallas, USA, March 2010.\n[17] F. Weninger, C. Kirst, B. Schuller, and H.-J. Bungartz.\nA discriminative approach to polyphonic piano note\ntranscriptionusingsupervisednon-negativematrixfac-\ntorization.In IEEEInt.Conf.Audio,SpeechandSignal\nProcessing, pages 6–10, May 2013.\n[18] T. B. Yakar, P. Sprechmann, R. Litman, A. M. Bron-\nstein, and G. Sapiro. Bilevel sparse models for poly-\nphonicmusictranscription.In 14thInt.SocietyforMu-\nsic Information Retrieval Conf., pages 65–70, 2013.\n[19] K. Yoshii and M. Goto. Inﬁnite composite autoregres-\nsive models for music signal analysis. In 13th Int. So-\ncietyforMusicInformationRetrievalConf.,pages79–\n84, October 2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n180"
    },
    {
        "title": "MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.",
        "author": [
            "Rachel M. Bittner",
            "Justin Salamon",
            "Mike Tierney",
            "Matthias Mauch",
            "Chris Cannam",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.2620624",
        "url": "https://doi.org/10.5281/zenodo.2620624",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T028_322_Paper.pdf",
        "abstract": "Subset of MedleyDB: 103 solomonophonic stem audio filesand correspondingmanually annotated pitch (f0) annotations.\n\nFor further details, refer to theMedleyDB website.\n\nFurther Annotation and Metadata files are version controlled and are available in theMedleyDB githubrepository:Metadatacan be foundhere,Annotationscan be foundhere.\n\nFor detailed information about the dataset, please visit MedleyDB&#39;swebsite.\n\n\n\nIf you make use of MedleyDB for academic purposes, please cite the following publication:\n\nR. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.",
        "zenodo_id": 2620624,
        "dblp_key": "conf/ismir/BittnerSTMCB14",
        "keywords": [
            "Subset",
            "Solomonophonic",
            "Audio",
            "Files",
            "Pitch",
            "Annotations",
            "MedleyDB",
            "Website",
            "Version",
            "Controlled"
        ]
    },
    {
        "title": "A Multi-model Approach to Beat Tracking Considering Heterogeneous Music Styles.",
        "author": [
            "Sebastian Böck",
            "Florian Krebs",
            "Gerhard Widmer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415240",
        "url": "https://doi.org/10.5281/zenodo.1415240",
        "ee": "https://zenodo.org/records/1415240/files/BockKW14.pdf",
        "abstract": "In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possi- ble beat positions. It chooses the model with the most ap- propriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state- of-the-art methods. Under certain conditions the system is able to match even human tapping performance.",
        "zenodo_id": 1415240,
        "dblp_key": "conf/ismir/BockKW14",
        "keywords": [
            "beat tracking algorithm",
            "multi-model approach",
            "recurrent neural networks",
            "tempo and phase modeling",
            "dynamic Bayesian network",
            "performance gains",
            "human tapping performance",
            "various music styles",
            "state-of-the-art methods",
            "experimental evaluation"
        ],
        "content": "A MULTI-MODEL APPROACH TO BEAT TRACKING CONSIDERING\nHETEROGENEOUS MUSIC STYLES\nSebastian B ¨ock, Florian Krebs and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nsebastian.boeck@jku.at\nABSTRACT\nIn this paper we present a new beat tracking algorithm\nwhich extends an existing state-of-the-art system with a\nmulti-model approach to represent different music styles.\nThe system uses multiple recurrent neural networks, which\nare specialised on certain musical styles, to estimate possi-\nble beat positions. It chooses the model with the most ap-\npropriate beat activation function for the input signal and\njointly models the tempo and phase of the beats from this\nactivation function with a dynamic Bayesian network. We\ntest our system on three big datasets of various styles and\nreport performance gains of up to 27% over existing state-\nof-the-art methods. Under certain conditions the system is\nable to match even human tapping performance.\n1. INTRODUCTION AND RELATED WORK\nThe automatic inference of the metrical structure in mu-\nsic is a fundamental problem in the music information re-\ntrieval ﬁeld. In this line, beat tracking deals with ﬁnding\nthe most salient level of this metrical grid, the beat. The\nbeat consists of a sequence of regular time instants which\nusually invokes human reactions like foot tapping. During\nthe last years, beat tracking algorithms have considerably\nimproved in performance. But still they are far from being\nconsidered on par with human beat tracking abilities – es-\npecially for music styles which do not have simple metrical\nand rhythmic structures.\nMost methods for beat tracking extract some features\nfrom the audio signal as a ﬁrst step. As features, com-\nmonly low-level features such as amplitude envelopes [20]\nor spectral features [2], mid-level features like onsets ei-\nther in discretised [8,12] or continuous form [6,10,16,18],\nchord changes [12,18] or combinations thereof with higher\nlevel features such as rhythmic patterns [17] or metrical\nrelations [11] are used. The feature extraction is usually\nfollowed by a stage that determines periodicities within\nthe extracted features sequences. Autocorrelation [2,9,12]\nand comb ﬁlters [6, 20] are commonly used techniques for\nc\rSebastian B ¨ock, Florian Krebs and Gerhard Widmer.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Sebastian B ¨ock, Florian Krebs and\nGerhard Widmer. “A Multi-Model Approach to Beat Tracking Consid-\nering Heterogeneous Music Styles”, 15th International Society for Music\nInformation Retrieval Conference, 2014.this task. Most systems then determine the most predom-\ninant tempo from these periodicities and subsequently de-\ntermine the beat times using multiple agents approaches\n[8,12], dynamic programming [6,10], hidden Markov mod-\nels (HMM) [7,16,18], or recurrent neural networks (RNN)\n[2]. Other systems operate directly on the input features\nand jointly determine the tempo and phase of the beats us-\ningdynamic Bayesian networks (DBN) [3, 14, 17, 21].\nOne of the most common problems of beat tracking\nsystems are “octave errors”, meaning that a system de-\ntects beats at double or half the rate of the ground truth\ntempo. For human tappers this generally does not consti-\ntute a problem, as can be seen when comparing beat track-\ning results at different metrical levels [6]. Hainsworth and\nMacleod stated that beat tracking systems will have to be\nstyle speciﬁc in the future in order to improve the state-of-\nthe-art [14]. This is consistent with the ﬁnding of Krebs et\nal. [17] who showed on a dataset of Ballroom music that\nthe beat tracking performance can be improved by incor-\nporating style-speciﬁc knowledge, especially by resolving\nthe octave error. While approaches have been proposed\nwhich combined multiple existing features for beat track-\ning [22], no one has so far combined several models spe-\ncialised on different musical styles to improve the overall\nperformance.\nIn this paper, we propose a multi-model approach to\nfuse information of different models that have been spe-\ncialised on heterogeneous music styles. The model is based\non the recurrent neural network (RNN) beat tracking sys-\ntem proposed in [2] and can be easily adapted to any mu-\nsic style without further parameter tweaking, only by pro-\nviding a corresponding beat-annotated dataset. Further,\nwe propose an additional dynamic Bayesian network stage\nbased on the work of Whiteley et al. [21] which jointly in-\nfers the tempo and the beat phase from the beat activations\nof the RNN stage.\n2. PROPOSED METHOD\nThe new beat tracking algorithm is based on the state-of-\nthe-art approach presented by B ¨ock and Schedl in [2]. We\nextend their system to be able to better deal with heteroge-\nneous music styles and combine it with a dynamic Bayesian\nnetwork similar to the ones presented in [21] and [17].\nThe basic structure is depicted in Figure 1 and consists\nof the following elements: ﬁrst the audio signal is pre-\nprocessed and fed into multiple neural network beat track-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n603ing modules. Each of the modules is trained on different\naudio material and outputs a different beat activation func-\ntion when activated with a musical signal. These functions\nare then fed into a module which chooses the most appro-\npriate model and passes its activation function to a dynamic\nBayesian network to infer the actual beat positions.\nModel 2Model NModel SwitcherSignalBeatsModel 1Pre-processing•••Reference NetworkDynamic Bayesian Network\nFigure 1. Overview of the new multi-model beat tracking\nsystem.\nTheoretically, a single network large enough should be\nable to model all the different music styles simultaneously,\nbut unfortunately this optimal solution is hardly achiev-\nable. The main reason for this is the difﬁculty to choose an\nabsolutely balanced training set with an evenly distributed\nset of beats over all the different dimensions relevant for\ndetecting beats. These include rhythmic patterns [17, 20],\nharmonic aspects and many other features. To overcome\nthis limitation, we split the available training data into mul-\ntiple parts. Each part should represent a more homoge-\nneous subset than the whole set so that the networks are\nable to specialise on the dominant aspects of this subset.\nIt seems reasonable to assume that humans do some-\nthing similar when tracking beats [4]. Depending on the\nstyle of the music, the rhythmic patterns present, the in-\nstrumentation, the timbre, they apply their musical knowl-\nedge to chose one of their “learned” models and then de-\ncide which musical events are beats or not. Our approach\nmimics this behaviour by learning multiple distinct mod-\nels.\n2.1 Signal pre-processing\nAll neural networks share the same signal pre-processing\nstep, which is very similar to the work in [2]. As inputs\nto the different neural networks, the logarithmically ﬁl-\ntered and scaled spectrograms of three parallel Short Time\nFourier Transforms (STFT) obtained for different window\nlengths and their positive ﬁrst order differences are used.\nThe system works with a constant frame rate frof 100\nframes per second. Window lengths of 23:2 ms,46:4 ms\nand92:9 ms are used and the resulting spectrogram bins\nof the discrete Fourier transforms are ﬁltered with over-\nlapping triangular ﬁlters to have a frequency resolution of\nthree bands per octave. To put all resulting magnitude val-\nues into a positive range we add 1 before taking the loga-\nrithm.2.2 Multiple parallel neural networks\nAt the core of the new approach, multiple neural networks\nare used to determine possible beat locations in the audio\nsignal. As outlined previously, these networks are trained\non material with different music styles to be able to better\ndetect the beats in heterogeneous music styles.\nAs networks we chose the same recurrent neural net-\nwork (RNN) topology as in [2] with three bidirectional hid-\nden layers with 25 long short-term memory (LSTM) units\nper layer. For training of the networks, standard gradient\ndescent with error backpropagation and a learning rate of\n1e\u00004is used. We initialise the network weights with a\nGaussian distribution with mean 0and standard deviation\nof0:1. We use early stopping with a disjoint validation set\nto stop training if no improvement over 20 epochs can be\nobserved.\nOne reference network is trained on the complete dataset\nuntil the stopping criterion is reached for the ﬁrst time. We\nuse this point during the training phase to diverge the spe-\ncialised models from the reference network.\nAfterwards, all networks are ﬁne-tuned with a reduced\nlearning rate of 1e\u00005on either the complete set or the indi-\nvidual subsets (cf. Section 3.1) with the above mentioned\nstopping criterion. Given Nsubsets,N+ 1 models are\ngenerated.\nThe output functions of the network models represent\nthe beat probability at each time frame. Instead of tracking\nthe beats with an autocorrelation function as described in\nthe original work, the beat activation functions of the dif-\nferent models are fed into the next model-selection stage.\n2.3 Model selection\nThe purpose of this stage is to select a model which outputs\na better beat activation function than the reference model\nwhen activated with a signal. Compared to the reference\nmodel, the specialised models produce better predictions\non input data which is similar to that used for ﬁne-tuning,\nbut worse predictions on signals dissimilar to the training\ndata. This behaviour can be seen in Figure 2, where the\nspecialised model produces higher beat activation values\nat the beat locations and lower values elsewhere.\nTable 1 illustrates the impact on the Ballroom subset,\nwhere the relative gain of the best specialised model com-\npared to the reference model (+1:7%) is lower than the\npenalties of the other models (\u00002:3% to\u00006:3%). The\nfact that the performance degradation of the unsuitable spe-\ncialised models is greater than the gain of the most suitable\nmodel allows us to use a very simple but effective method\nto choose the best model.\nTo select the best performing model, all network out-\nputs of the ﬁne-tuned networks are compared with the out-\nput of the reference network (which was trained on the\nwhole training set) and the one yielding the lowest mean\nsquared difference is selected as the ﬁnal one and its out-\nput is fed into the ﬁnal beat tracking stage.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6040 50 100 150 200 250 300 350 400\ntime [frames]0.00.10.20.30.40.50.6beat activationFigure 2. Example beat activations for a 4 seconds ball-\nroom snippet. Red is the reference network’s activations,\nblack the selected model and blue a discarded one. Green\ndashed vertical lines denote the annotated beat positions.\nF-measure Cemgil AMLc AMLt\nSMC * 0.834 0.807 0.664 0.767\nHainsworth * 0.867 0.839 0.694 0.793\nBallroom * 0.904 0.872 0.777 0.853\nReference 0.887 0.855 0.748 0.831\nMulti-model 0.897 0.866 0.759 0.841\nTable 1. Performance of differently specialised mod-\nels (marked with asterisks, ﬁne-tuned on the SMC,\nHainsworth andBallroom subsets) on the Ballroom subset\ncompared to the reference model and the network selected\nby the multi-model selection stage.\n2.4 Dynamic Bayesian network\nIndependent of whether only one or multiple neural net-\nworks are used, the approach of B ¨ock and Schedl [2] has\na fundamental shortcoming: the ﬁnal peak-picking stage\ndoes not try to ﬁnd a global optimum when selecting the\nﬁnal locations of the beats. It rather determines the dom-\ninant tempo of the piece (or a segment of certain length)\nand then aligns the beat positions according to this tempo\nby simply choosing the best start position and then pro-\ngressively locating the beats at positions with the highest\nactivation function values in a certain region around the\npre-determined position. To allow a greater responsiveness\nto tempo changes, this chosen region must not be too small.\nHowever, this also introduces a weakness to the algorithm,\nbecause the tracking stage can easily get distracted by a\nfew misaligned beats and needs some time to recover from\nthis fault. The activation function depicted in Figure 2 has\ntwo of these spurious detections around frames 100 and\n200.\nTo circumvent this problem, we feed the output of the\nchosen neural network model into a dynamic Bayesian net-\nwork (DBN) which jointly infers tempo and phase of a beat\nsequence. Another advantage of this new method is that\nwe are able to model both beat and non-beat states, which\nwas shown to perform superior to the case where only beat\nstates are modelled [7].The DBN we use is closely related to the one proposed\nin [21], adapted to our speciﬁc needs. Instead of mod-\nelling whole bars, we only model one beat period which re-\nduces the size of the search space. Additionally we do not\nmodel rhythmic patterns explicitly and leave this higher\nlevel analysis to the neural networks. This ﬁnally leads to\na DBN which consists of two hidden variables, the tempo\n!and the position \u001einside a beat period. In order to in-\nfer the hidden variables from an audio signal, we have to\nspecify three entities: A transition model which describes\nthe transitions between the hidden variables, an observa-\ntion model which takes the beat activations from the neural\nnetwork and transforms them into probabilities suitable for\nthe DBN, and the initial distribution which encodes prior\nknowledge about the hidden variables. For computational\nease we discretise the tempo-beat space to be able to use\nstandard hidden Markov model (HMM) [19] algorithms\nfor inference.\n2.4.1 Transition model\nThe beat period is discretised into \b = 640 equidistant\ncells and\u001e2f1;:::; \bg. We refer to the unit of the variable\n\u001e(position inside a beat period) as pib.\u001ekat audio frame\nkis then computed by\n\u001ek= (\u001ek\u00001+!k\u00001\u00001)mod\b + 1: (1)\nThe tempo space is discretised into \n = 23 equidistant\ncells, which cover the tempo range up to 215 beats per\nminute (BPM). The unit of the tempo variable !ispib per\naudio frame. As we want to restrict !to integer values (to\nstay within the \u001egrid at transitions), we need a high reso-\nlution of\u001ein order to get a high resolution of !. Based on\nexperiments with the training set, we set the tempo space\nto!2f6;:::; \ng, where!= 6is equivalent to a minimum\ntempo of 6\u000260\u0002fr=\b\u001956BPM. As in [21] we only\nallow for three tempo transitions at time frame k: It stays\nconstant, it accelerates, or it decelerates.\n!k=8\n<\n:!k\u00001; P (!kj!k\u00001) = 1\u0000p!\n!k\u00001+ 1; P (!kj!k\u00001) =p!\n2\n!k\u00001\u00001; P (!kj!k\u00001) =p!\n2(2)\nTransitions to tempi outside of the allowed range are not\nallowed by setting the corresponding transition probabili-\nties to zero. The probability of a tempo change p!was set\nto0:002.\n2.4.2 Observation model\nSince the beat activation function aproduced by the neural\nnetwork is limited to the range [0;1]and shows high val-\nues at beat positions and low values at non-beat positions,\nwe use the activation function directly as state-conditional\nobservation distributions (similar to [7]). We deﬁne the ob-\nservation likelihood as\nP(akj\u001ek) =\u001aak; 1\u0014\u001ek\u0014\b\n\u00151\u0000ak\n\u0015\u00001; otherwise:(3)\n\u00152[\b\n\b\u00001;\b]is a parameter that controls the proportion of\nthe beat interval which is considered as beat and non-beat\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n605location. Smaller values of \u0015(a higher proportion of beat\nlocations and a smaller proportion of non-beat locations)\nare especially important for higher tempi, as the DBN vis-\nits only a few position states of a beat interval and could\npossibly miss the beginning of a beat. On the other hand,\nhigher values of \u0015(a smaller proportion of beat locations)\nlead to less accurate beat tracking, as the activations are\nblurred in the state domain of the DBN. On our training\nset we achieved the best results with the value \u0015= 16.\n2.4.3 Initial state distribution\nThe initial state distribution is normally used to incorporate\nany prior knowledge about the hidden states, such as tempo\ndistributions. In this paper, we use a uniform distribution\nover all states, for simplicity and ease of generalisation.\n2.4.4 Inference\nWe are interested in the sequence of hidden variables \u001e1:K\nand!1:K, that maximise the posterior probability of the\nhidden variables given the observations (activations a1:K).\nCombining the discrete states of \u001eand!into one state\nvector xk= [\u001ek;!k], we can compute the maximum a-\nposteriori state sequence x\u0003\n1:Kby\nx\u0003\n1:K= arg max\nx1:Kp(x1:Kja1:K): (4)\nEquation 4 can be computed efﬁciently using the well-\nknown Viterbi algorithm [19]. Finally the set of beat times\nBare determined by the set of time frames kwhich were\nassigned to a beat position (B =fk:\u001ek<\u001ek\u00001g). In our\nexperiments we found that the beat detection becomes less\naccurate if the part of the beat interval which is considered\nas beat-state is too large (i.e. smaller values of \u0015). There-\nfore we determine the ﬁnal beat times by looking for the\nhighest beat activation value inside the beat-state window\nW=fk:\u001ek\u0014\b\n\u0015g.\n3. EV ALUATION\nFor the development and evaluation of the algorithm we\nused some well-known datasets. This allows for highest\ncomparability with previously published results of state-\nof-the-art algorithms.\n3.1 Datasets\nAs training material for our system, the datasets introduced\nin [13–15] are used. They are called Ballroom, Hainsworth\nandSMC respectively. To show the ability of our new al-\ngorithm to adapt to various music styles, a very simple ap-\nproach of splitting the complete dataset into multiple sub-\nsets according to the original source was chosen. Although\nfar from optimal – both the SMC andHainsworth datasets\ncontain heterogeneous music styles – we still consider this\na valid choice, since any “better” splitting would allow\nthe system to adapt even further to heterogeneous styles\nand in turn lead to better results. At least the three sets\nhave a somehow different focus regarding the music styles\npresent.3.2 Performance measures\nIn line with almost all other publications on the topic of\nbeat tracking, we report the following scores:\nF-measure : counts the number of true positive (correctly\nlocated beats within a tolerance window of \u000670 ms), false\npositive and negative detections;\nP-score : measures the tracking accuracy by the correla-\ntion of the detections and the annotations, considering\ndeviations within 20% of the annotated beat interval as\ncorrect;\nCemgil : places a Gaussian function with a standard de-\nviation of 40 ms around the annotations and then mea-\nsures the tracking accuracy by summing up the scores of\nthe detected beats on this function normalising it by the\noverall length of the annotations or detections, whichever\nis greater;\nCMLc & CMLt : measure the longest continuously seg-\nment (CMLc) or all correctly tracked beats (CMLt) at the\ncorrect metrical level. A beat is considered correct if it is\nreported within a 17.5% tempo and phase tolerance, and\nthe same applies for the previously detected beat;\nAMLc & AMLt : like CMLc & CMLt, but additionally\nallow offbeat and double/half as well as triple/third tempo\nvariations of the annotated beats;\nD & D g: the information gain (D) and global information\ngain (D g) are phase agnostic measures comparing the an-\nnotations with the detections (and vice-versa) building a\nerror histogram and then calculating the Kullback-Leibler\ndivergence w.r.t. a uniform histogram.\nA more detailed description of the evaluation methods can\nbe found in [5]. However, since we only investigate of-\nﬂine algorithms, we do not skip the ﬁrst ﬁve seconds for\nevaluation.\n3.3 Results & Discussion\nTable 2 lists the performance results of the reference imple-\nmentation, B ¨ock’s BeatTracker.2013, and the various ex-\ntensions proposed in this paper for all datasets. All results\nare obtained with 8-fold cross validation with previously\ndeﬁned splittings, ensuring that no pieces are used both for\ntraining or parameter tuning and testing purposes. Addi-\ntionally, we compare our new approach to published stat-\nof-the-art results on the Hainsworth andBallroom datasets.\n3.3.1 Multi-model extension\nAs can be seen, the use of the multi-model extension al-\nmost always improves the results over the implementation\nit is based on, especially on the SMC set. The gain in per-\nformance on the Ballroom set was expected, since Krebs et\nal. already showed that modelling rhythmic patterns helps\nto increase the overall detection accuracy [17]. Although\nwe did not split the set according to the individual rhythmic\npatterns, the overall style of ballroom music can be con-\nsidered unique enough to be distinct from the other music\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n606F-measure P-score Cemgil CMLc CMLt AMLc AMLt D D g\nBallroom\nBeatTracker.2013 [1, 2] 0.887 0.863 0.855 0.719 0.795 0.748 0.831 3.404 2.596\n— Multi-Model 0.897 0.875 0.866 0.740 0.814 0.759 0.841 3.480 2.674\n— DBN 0.903 0.876 0.838 0.792 0.825 0.873 0.915 3.427 2.275\n— Multi-Model + DBN 0.910 0.881 0.845 0.800 0.830 0.885 0.924 3.469 2.352\nKrebs et al. [17] 0.855 0.839 0.772 0.745 0.786 0.818 0.865 2.499 1.681\nZapata et al. [22]y 0.767 0.735 0.672 0.586 0.607 0.824 0.860 2.750 1.187\nHainsworth\nBeatTracker.2013 [1, 2] 0.832 0.843 0.712 0.618 0.756 0.655 0.807 2.167 1.468\n— Multi-Model 0.832 0.847 0.716 0.617 0.761 0.652 0.809 2.171 1.490\n— DBN 0.843 0.867 0.711 0.696 0.808 0.759 0.883 2.251 1.481\n— Multi-Model + DBN 0.840 0.865 0.707 0.696 0.803 0.760 0.881 2.268 1.466\nZapata et al. [22]y 0.710 0.732 0.589 0.569 0.642 0.709 0.824 2.057 0.880\nDavies et al. [6] - - - 0.548 0.612 0.681 0.789 - -\nPeeters & Papadopoulos [18] - - - 0.547 0.628 0.703 0.831 - -\nDegara et al. [7] - - - 0.561 0.629 0.719 0.815 - -\nHuman tapper [6]z - - - 0.528 0.812 0.575 0.874 - -\nSMC\nBeatTracker.2013 [1, 2] 0.497 0.598 0.402 0.238 0.360 0.279 0.436 1.263 0.416\n— Multi-Model 0.514 0.617 0.415 0.257 0.389 0.296 0.467 1.324 0.467\n— DBN 0.516 0.622 0.404 0.294 0.415 0.378 0.550 1.426 0.504\n— Multi-Model + DBN 0.529 0.630 0.415 0.296 0.428 0.383 0.567 1.460 0.531\nZapata et al. [22]y 0.369 0.460 0.285 0.115 0.158 0.239 0.397 0.879 0.126\nTable 2. Performance of the proposed algorithm on the Ballroom [13], Hainsworth [14] and SMC [15] datasets. Beat-\nTracker is the reference implementation our Multi-Model anddynamic Bayesian network (DBN) extensions are built on.\nThe results marked with yare obtained with Essentia’s implementation of the multi-feature beat tracker.1zdenotes causal\n(i.e. online) processing, all listed algorithms use non-causal analysis (i.e. ofﬂine processing) with the best results in bold.\nstyles present in the other sets and the salient features can\nbe exploited successfully by the multi-model approach.\n3.3.2 Dynamic Bayesian network extension\nAs already indicated in the original paper [2] (and described\nearlier in Section 2.4), the original BeatTracker can be eas-\nily distracted by some misaligned beats and then needs\nsome time to recover from any failure. The newly adapted\ndynamic Bayesian network beat tracking stage does not\nsuffer from this shortcoming by searching for the glob-\nally best beat locations. The use of the DBN boosts the\nperformance on all datasets for almost all evaluation mea-\nsures. Interestingly, the Cemgil accuracy is degraded by\nusing the DBN stage. This might be explained by the fact\nthat the discretisation grid of the beat period beat posi-\ntions becomes too coarse for low tempi (cf. Section 2.4.4)\nand therefore yields inaccurate beat detections, which es-\npecially affect the Cemgil accuracy. This is one of the is-\nsues that needs to be resolved in the future, especially for\nlower tempi where the penalty is the highest.\n3.3.3 Comparison with other methods\nOur new system set side by side with other state-of-the-art\nalgorithms draws a clear picture. It outperforms all of them\nconsiderably – independently of the dataset and evaluation\nmeasure chosen. Especially the high performance boosts\nof the CMLc and CMLt scores on the Hainworth dataset\nhighlight the ability to track the beats at the correct metri-\ncal level signiﬁcantly more often than any other method.Davies et al. [6] also list performance results of a hu-\nman tapper on the same dataset. However it must be noted\nthat these were obtained by online real-time tapping, hence\nthey cannot be compared directly to the system presented.\nHowever, the system of Davies et al. can also be switched\nto causal mode (and thus being comparable to a human\ntapper). In this mode it achieved performance reduced by\napproximately 10% [6]. Adding the same amount to the\nreported tapping results of 0.528 CMLc and 0.575 AMlc\nsuggests that our system is capable of performing as good\nas humans when continuous tapping is required.\nOn the Ballroom set we achieve higher results than the\nparticularly specialised system of Krebs et al. [17]. Since\nour DBN approach is a simpliﬁed variant of their model, it\ncan be assumed that the relatively low scores of the Cemgil\naccuracy and the information gain are due to the same rea-\nson – the coarse discretisation of the beat or bar states.\nNonetheless, comparing the continuity scores (which have\nhigher tolerance thresholds) we can still report an average\nincrease in performance of more than 5%.\n4. CONCLUSIONS & OUTLOOK\nIn this paper we have presented a new beat tracking system\nwhich is able to improve over existing algorithms by incor-\nporating multiple models which were trained on different\nmusic styles and combining it with a dynamic Bayesian\n1http://essentia.upf.edu, v2.0.1\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n607network for the ﬁnal inference of the beats. The combina-\ntion of these two extensions yields a performance boost –\ndepending on the dataset and evaluation measures chosen\n– of up to 27% relative, matching human tapping results\nunder certain conditions. It outperforms other state-of-the-\nart algorithms in tracking the beats at the correct metrical\nlevel by 20%.\nWe showed that the specialisation on a certain musical\nstyle helps to improve the overall performance, although\nthe method for splitting the available data into sets of dif-\nferent styles and then selecting the most appropriate model\nis rather simple. For the future we will investigate more ad-\nvanced techniques for the selection of suitable data for the\ncreation of the specialised models, e.g. splitting the datasets\naccording to dance styles as performed by Krebs et al. [17]\nor applying unsupervised clustering techniques. We also\nexpect better results from more advanced model selection\nmethods. One possible approach could be to feed the indi-\nvidual model activations to the dynamic Bayesian network\nand let it choose among them.\nFinally, the Bayesian network could be tuned towards\nusing a ﬁner beat positions grid and thus reporting the beats\nat more appropriate times than just selecting the position\nof the highest activation reported by the neural network\nmodel.\n5. ACKNOWLEDGMENTS\nThis work is supported by the European Union Seventh\nFramework Programme FP7 / 2007-2013 through the\nGiantSteps project (grant agreement no. 610591) and the\nAustrian Science Fund (FWF) project Z159.\n6. REFERENCES\n[1] MIREX 2013 beat tracking results. http://nema.lis.\nillinois.edu/nema_out/mirex2013/results/\nabt/, 2013.\n[2] S. B ¨ock and M. Schedl. Enhanced Beat Tracking with\nContext-Aware Neural Networks. In Proceedings of the 14th\nInternational Conference on Digital Audio Effects (DAFx-\n11), pages 135–139, Paris, France, September 2011.\n[3] A. T. Cemgil, H. Kappen, P. Desain, and H. Honing. On\ntempo tracking: Tempogram Representation and Kalman ﬁl-\ntering. Journal of New Music Research, 28:4:259–273, 2001.\n[4] N. Collins. Towards a style-speciﬁc basis for computational\nbeat tracking. In Proceedings of the 9th International Confer-\nence on Music Perception and Cognition (ICMPC9), pages\n461–467, Bologna, Italy, 2006.\n[5] M. E. P. Davies, N. Degara, and M. D. Plumbley. Evalu-\nation methods for musical audio beat tracking algorithms.\nTechnical Report C4DM-TR-09-06, Centre for Digital Mu-\nsic, Queen Mary University of London, 2009.\n[6] M. E. P. Davies and M. D. Plumbley. Context-dependent\nbeat tracking of musical audio. IEEE Transactions on Audio,\nSpeech, and Language Processing, 15(3):1009–1020, March\n2007.\n[7] N. Degara, E. Argones-R ´ua, A. Pena, S. Torres-Guijarro,\nM. E. P. Davies, and M. D. Plumbley. Reliability-informed\nbeat tracking of musical signals. IEEE Transactions on Au-\ndio, Speech and Language Processing, 20(1):290–301, Jan-\nuary 2012.[8] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Research,\n30:39–58, 2001.\n[9] D. Eck. Beat tracking using an autocorrelation phase ma-\ntrix. In Proceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP 2007),\nvolume 4, pages 1313–1316, Honolulu, Hawaii, USA, April\n2007.\n[10] D. P. W. Ellis. Beat tracking by dynamic programming. Jour-\nnal of New Music Research, 2007:51–60, 2007.\n[11] A. Gkiokas, V . Katsouros, G. Carayannis, and T. Stafylakis.\nMusic tempo estimation and beat tracking by applying source\nseparation and metrical relations. In Proceedings of the 37th\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP 2012), pages 421–424, Kyoto, Japan,\nMarch 2012.\n[12] M. Goto and Y . Muraoka. Beat tracking based on multiple-\nagent architecture a real-time beat tracking system for audio\nsignals. In Proceedings of the International Conference on\nMultiagent Systems, pages 103–110, 1996.\n[13] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis,\nC. Uhle, and P. Cano. An experimental comparison of au-\ndio tempo induction algorithms. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 14(5):1832–1844,\nSeptember 2006.\n[14] S. Hainsworth and M. Macleod. Particle ﬁltering applied to\nmusical tempo tracking. EURASIP J. Appl. Signal Process.,\n15:2385–2395, January 2004.\n[15] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. Oliveira, and\nF. Gouyon. Selective sampling for beat tracking evaluation.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 20(9):2539–2548, November 2012.\n[16] A. Klapuri, A. Eronen, and J. Astola. Analysis of the me-\nter of acoustic musical signals. IEEE Transactions on Audio,\nSpeech, and Language Processing, 14(1):342–355, January\n2006.\n[17] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern model-\ning for beat and downbeat tracking in musical audio. In Pro-\nceedings of the 14th International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2013), pages 227–232,\nCuritiba, Brazil, November 2013.\n[18] G. Peeters and H. Papadopoulos. Simultaneous beat and\ndownbeat-tracking using a probabilistic framework: The-\nory and large-scale evaluation. IEEE Transactions on Audio,\nSpeech, and Language Processing, 19(6):1754–1769, 2011.\n[19] L. Rabiner. A tutorial on hidden Markov models and se-\nlected applications in speech recognition. In Proceedings of\nthe IEEE, pages 257–286, 1989.\n[20] E. D. Scheirer. Tempo and beat analysis of acoustic musi-\ncal signals. The Journal of the Acoustical Society of America,\n103(1):588–601, 1998.\n[21] N. Whiteley, A. Cemgil, and S. Godsill. Bayesian modelling\nof temporal structure in musical audio. In Proceedings of the\n7th International Conference on Music Information Retrieval\n(ISMIR 2006), pages 29–34, Victoria, BC, Canada, October\n2006.\n[22] J. R. Zapata, M. E. P. Davies, and E. G ´omez. Multi-feature\nbeat tracking. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 22(4):816–825, April 2014.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n608"
    },
    {
        "title": "Information-Theoretic Measures of Music Listening Behaviour.",
        "author": [
            "Daniel Boland",
            "Roderick Murray-Smith"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416192",
        "url": "https://doi.org/10.5281/zenodo.1416192",
        "ee": "https://zenodo.org/records/1416192/files/BolandM14.pdf",
        "abstract": "We present an information-theoretic approach to the mea- surement of users’ music listening behaviour and selection of music features. Existing ethnographic studies of mu- sic use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10, 000 hand- made playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users’ criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels.",
        "zenodo_id": 1416192,
        "dblp_key": "conf/ismir/BolandM14",
        "keywords": [
            "entropy",
            "SPUD dataset",
            "music listening behaviour",
            "music retrieval systems",
            "user and audio stream metadata",
            "music features",
            "playlist curation",
            "objective ground truth labels",
            "quantitative approach",
            "user-centred evaluation"
        ],
        "content": "INFORMATION-THEORETIC MEASURES\nOF MUSIC LISTENING BEHA VIOUR\nDaniel Boland, Roderick Murray-Smith\nSchool of Computing Science, University of Glasgow, United Kingdom\ndaniel@dcs.gla.ac.uk; roderick.murray-smith@glasgow.ac.uk\nABSTRACT\nWe present an information-theoretic approach to the mea-\nsurement of users’ music listening behaviour and selection\nof music features. Existing ethnographic studies of mu-\nsic use have guided the design of music retrieval systems\nhowever are typically qualitative and exploratory in nature.\nWe introduce the SPUD dataset, comprising 10;000hand-\nmade playlists, with user and audio stream metadata. With\nthis, we illustrate the use of entropy for analysing music\nlistening behaviour, e.g. identifying when a user changed\nmusic retrieval system. We then develop an approach to\nidentifying music features that reﬂect users’ criteria for\nplaylist curation, rejecting features that are independent of\nuser behaviour. The dataset and the code used to produce\nit are made available. The techniques described support\na quantitative yet user-centred approach to the evaluation\nof music features and retrieval systems, without assuming\nobjective ground truth labels.\n1. INTRODUCTION\nUnderstanding how users interact with music retrieval sys-\ntems is of fundamental importance to the ﬁeld of Music\nInformation Retrieval (MIR). The design and evaluation of\nsuch systems is conditioned upon assumptions about users,\ntheir listening behaviours and their interpretation of mu-\nsic. While user studies have offered guidance to the ﬁeld\nthus far, they are mostly exploratory and qualitative [20].\nThe availability of quantitative metrics would support the\nrapid evaluation and optimisation of music retrieval. In\nthis work, we develop an information-theoretic approach\nto measuring users’ music listening behaviour, with a view\nto informing the development of music retrieval systems.\nTo demonstrate the use of these measures, we compiled\n‘Streamable Playlists with User Data’ (SPUD) – a dataset\ncomprising 10;000playlists from Last.fm1produced by\n3351 users, with track metadata including audio streams\nfrom Spotify.2We combine the dataset with the mood and\ngenre classiﬁcation of Syntonetic’s Moodagent,3yielding\na range of intuitive music features to serve as examples.\nc\rDaniel Boland, Roderick Murray-Smith.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Daniel Boland, Roderick Murray-\nSmith. “Information-Theoretic Measures of Music Listening Behaviour”,\n15th International Society for Music Information Retrieval Conference,\n2014.We identify the entropy of music features as a metric\nfor characterising music listening behaviour. This mea-\nsure can be used to produce time-series analyses of user\nbehaviour, allowing for the identiﬁcation of events where\nthis behaviour changed. In a case study, the date when a\nuser adopted a different music retrieval system is detected.\nThese detailed analyses of listening behaviour can support\nuser studies or provide implicit relevance feedback to mu-\nsic retrieval. More broad analyses are performed across\nthe10;000playlists. A Mutual Information based feature\nselection algorithm is employed to identify music features\nrelevant to how users create playlists. This user-centred\nfeature selection can sanity-check the choice of features in\nMIR. The information-theoretic approach introduced here\nis applicable to any discretisable feature set and distinct in\nbeing based solely upon actual user behaviour rather than\nassumed ground-truth. With the techniques described here,\nMIR researchers can perform quantitative yet user-centred\nevaluations of their music features and retrieval systems.\n1.1 Understanding Users\nUser studies have provided insights about user behaviour\nin retrieving and listening to music and highlighted the\nlack of consideration in MIR about actual user needs. In\n2003, Cunningham et al. bemoaned that development of\nmusic retrieval systems relied on “anecdotal evidence of\nuser needs, intuitive feelings for user information seeking\nbehavior, and a priori assumptions of typical usage scenar-\nios” [5]. While the number of user studies has grown, the\nsituation has been slow to improve. A review conducted\na decade later noted that approaches to system evaluation\nstill ignore the ﬁndings of user studies [12]. This issue\nis stated more strongly by Schedl and Flexer, describing\nsystems-centric evaluations that “completely ignore user\ncontext and user properties, even though they clearly in-\nﬂuence the result” [15]. Even systems-centric work, such\nas the development of music classiﬁers, must consider the\nuser-speciﬁc nature of MIR. Downie termed this the multi-\nexperiential challenge, and noted that “Music ultimately\nexists in the mind of its perceiver” [6]. Despite all of\nthis, the assumption of an objective ground truth for music\ngenre, mood etc. is common [4], with evaluations focusing\non these rather than considering users. It is clear that much\nwork remains in placing the user at the centre of MIR.\n1 .http://www.last.fm\n2 .http://www.spotify.com\n3 .http://www.moodagent.com Last accessed: 30/04/14\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5611.2 Evaluation in MIR\nThe lack of robust evaluations in the ﬁeld of MIR was iden-\ntiﬁed by Futrelle and Downie as early as 2003 [8]. They\nnoted the lack of any standardised evaluations and in par-\nticular that MIR research commonly had an “emphasis on\nbasic research over application to, and involvement with,\nusers.” In an effort to address these failings, the Music\nInformation Retrieval Evaluation Exchange (MIREX) was\nestablished [7]. MIREX provides a standardised frame-\nwork of evaluation for a range of MIR problems using\ncommon metrics and datasets, and acts as the benchmark\nfor the ﬁeld. While the focus on this benchmark has done\na great deal towards the standardisation of evaluations, it\nhas distracted research from evaluations with real users.\nA large amount of evaluative work in MIR focuses on\nthe performance of classiﬁers, typically of mood or genre\nclasses. A thorough treatment of the typical approaches to\nevaluation and their shortcomings is given by Sturm [17].\nWe note that virtually all such evaluations seek to circum-\nvent involving users, instead relying on a ‘ground truth’\nwhich is assumed to be objective. An example of a widely\nused ground truth dataset is GTZAN, a small collection\nof music with the author’s genre annotations. Even were\nthe objectivity of such annotations to be assumed, such\ndatasets can be subject to confounding factors and misla-\nbellings as shown by Sturm [16]. Schedl et al. also observe\nthat MIREX evaluations involve assessors’ own subjective\nannotations as ground truth [15].\n1.3 User-Centred Approaches\nThere remains a need for robust, standardised evaluations\nfeaturing actual users of MIR systems, with growing calls\nfor a more user-centric approach. Schedl and Flexer made\nthe broad case for “putting the user in the center of music\ninformation retrieval”, concerning not only user-centred\ndevelopment but also the need for evaluative experiments\nwhich control independent variables that may affect depen-\ndent variables [14]. We note that there is, in particular, a\nneed for quantitative dependent variables for user-centred\nevaluations. For limited tasks such as audio similarity or\ngenre classiﬁcation, existing dependent variables may be\nsufﬁcient. If the ﬁeld of MIR is to concern itself with the\ndevelopment of complete music retrieval systems, their in-\nterfaces, interaction techniques, and the needs of a variety\nof users, then additional metrics are required. Within the\nﬁeld of HCI it is typical to use qualitative methods such as\nthe think-aloud protocol [9] or Likert-scale questionnaires\nsuch as the NASA Task Load Index (TLX) [10].\nGiven that the purpose of a Music Retrieval system is to\nsupport the user’s retrieval of music, a dependent variable\nto measure this ability is desirable. Such a measure cannot\nbe acquired independently of users – the deﬁnition of mu-\nsical relevance is itself subjective. Users now have access\nto ‘Big Music’ – online collections with millions of songs,\nyet it is unclear how to evaluate their ability to retrieve this\nmusic. The information-theoretic methodology introduced\nin this work aims to quantify the exploration, diversity and\nunderlying mental models of users’ music retrieval.\nFigure 1. Distribution of playlist lengths within the SPUD\ndataset. The distribution peaks around a playlist length of\n12 songs. There is a long tail of lengthy playlists.\n2. THE SPUD DATASET\nThe SPUD dataset of 10;000 playlists was produced by\nscraping from Last.fm users who were active throughout\nMarch and April, 2014. The tracks for each playlist are\nalso associated with a Spotify stream, with scraped meta-\ndata, such as artist, popularity, duration etc. The number\nof unique tracks in the dataset is 271;389from 3351 users.\nThe distribution of playlist lengths is shown in Figure 1.\nWe augment the dataset with proprietary mood and genre\nfeatures produced by Syntonetic’s Moodagent. We do this\nto provide high-level and intuitive features which can be\nused as examples to illustrate the techniques being dis-\ncussed. It is clear that many issues remain with genre and\nmood classiﬁcation [18] and the results in this work should\nbe interpreted with this in mind. Our aim in this work is\nnot to identify which features are best for music classiﬁca-\ntion but to contribute an approach for gaining an additional\nperspective on music features. Another dataset of playlists\nAOTM-2011 is published [13] however the authors only\ngive fragments of playlists where songs are also present\nin the Million Song Dataset (MSD) [1]. The MSD provides\nmusic features for a million songs but only a small frac-\ntion of songs in AOTM-2011 were matched in MSD. Our\nSPUD dataset is distinct in maintaining complete playlists\nand having time-series data of songs listened to.\n3. MEASURING MUSIC LISTENING BEHA VIOUR\nWhen evaluating a music retrieval system, or performing\na user study, it would be useful to quantify the music-\nlistening behaviour of users. Studying this behaviour over\ntime would enable the identiﬁcation of how different mu-\nsic retrieval systems inﬂuence user behaviour. Quantifying\nlistening behaviour would also provide a dependent vari-\nable for use in MIR evaluations. We introduce entropy\nas one such quantitative measure, capturing how a user’s\nmusic-listening relates to the music features of their songs.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5623.1 Entropy\nFor each song being played by a user, the value of a given\nmusic feature can be taken as a random variable X. The\nentropy H(X)of this variable indicates the uncertainty\nabout the value of that feature over multiple songs in a lis-\ntening session. This entropy measure gives a scale from\na feature’s value never changing, through to every level of\nthe feature being equally likely. The more a user constrains\ntheir music selection by a particular feature, e.g. mood or\nalbum, then the lower the entropy is over those features.\nThe entropy for a feature is deﬁned as:\nH(X) =\u0000X\nx2Xp(x) log2[p(x)] ; (1)\nwhere xis every possible level of the feature Xand the dis-\ntribution p(x)is estimated from the songs in the listening\nsession. The resulting entropy value is measured in bits,\nthough can be normalised by dividing by the maximum\nentropy log2[jXj]. Estimating entropy in this way can be\ndone for any set of features, though requires that they are\ndiscretised to an appropriate number of levels.\nFor example, if a music listening session is dominated\nby songs of a particular tempo, the distribution over values\nof a TEMPO feature would be very biased. The entropy\nH(TEMPO )would thus be very low. Conversely, if users\nused shufﬂe or listened to music irrespective of tempo, then\nthe entropy H(TEMPO )would tend towards the average\nentropy of the whole collection.\n3.2 Applying a Window Function\nMany research questions regarding a user’s music listening\nbehaviour concern the change in that behaviour over time.\nAn evaluation of a music retrieval interface might hypothe-\nsise that users will be empowered to explore a more diverse\nrange of music. Musicologists may be interested to study\nhow listening behaviour has changed over time and which\nevents precede such changes. It is thus of interest to ex-\ntend Eqn (1) to deﬁne a measure of entropy which is also a\nfunction of time:\nH(X; t) =H(w(X; t)); (2)\nwhere w(X; t)is a window function taking nsamples of X\naround time t. In this paper we use a rectangular window\nfunction with n= 20, assuming that most albums will\nhave fewer tracks than this. The entropy at any given point\nis limited to the maximum possible H(X; t) = log2[n]i.e.\nwhere each of the npoints has a unique value.\nAn example of the change in entropy for a music feature\nover time is shown in Figure 2. In this case H(ALBUM )is\nshown as this will be 0for album-based listening and at\nmaximum for exploratory or radio-like listening. It is im-\nportant to note that while trends in mean entropy can be\nidentiﬁed, the entropy of music listening is itself quite a\nnoisy signal – it is unlikely that a user will maintain a sin-\ngle music-listening behaviour over a large period of time.\nPeriods of album listening (low or zero entropy) can be\nseen through the time-series, even after the overall trend is\ntowards shufﬂe or radio-like music listening.\nFigure 2. Windowed entropy over albums shows a user’s\nalbum-based music listening over time. Each point repre-\nsents 20 track plays. The black line depicts mean entropy,\ncalculated using locally weighted regression [3] with 95%\nCI of the mean shaded. A changepoint is detected around\nFeb. 2010, as the user began using online radio (light blue)\n3.3 Changepoints in Music Retrieval\nHaving produced a time-series analysis of music-listening\nbehaviour, we are now able to identify events which caused\nchanges in this behaviour. In order to identify change-\npoints in the listening history, we apply the ‘Pruned Exact\nLinear Time’ (PELT) algorithm [11]. The time-series is\npartitioned in a way that reduces a cost function of changes\nin the mean and variance of the entropy. Changepoints can\nbe of use in user studies, for example in Figure 2, the user\nexplained in an interview that the detected changepoint oc-\ncurred when they switched to using online radio. There\nis a brief return to album-based listening after the change-\npoint – users’ music retrieval behaviour can be a mixture of\ndifferent retrieval models. Changepoint detection can also\nbe a user-centred dependent variable in evaluating music\nretrieval interfaces i.e. does listening behaviour change as\nthe interface changes? Further examples of user studies are\navailable with the SPUD dataset.\n3.4 Identifying Listening Style\nThe style of music retrieval that the user is engaging in\ncan be inferred using the entropy measures. Where the\nentropy for a given music feature is low, the user’s listening\nbehaviour can be characterised by that feature i.e. we can\nbe certain about that feature’s level. Alternately, where a\nfeature has high entropy, then the user is not ‘using’ that\nfeature in their retrieval. When a user opts to use shufﬂe-\nbased playback i.e. the random selection of tracks, there\nis the unique case that entropy across all features will tend\ntowards the maximum. In many cases, feature entropies\nhave high covariance, e.g. songs on an album will have the\nsame artist and similar features. We did not include other\nfeatures in Figure 2 as the same pattern was apparent.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5634. SELECTING FEATURES FROM PLAYLISTS\nIdentifying which music features best describe a range of\nplaylists is not only useful for playlist recommendation,\nbut also provides an insight into how users organise and\nthink about music. Music recommendation and playlist\ngeneration typically work on the basis of genre, mood and\npopularity, and we investigate which of these features is\nsupported by actual user behaviour. As existing retrieval\nsystems are based upon these features, there is a poten-\ntial ‘chicken-and-egg’ effect where the features which best\ndescribe user playlists are those which users are currently\nexposed to in existing retrieval interfaces.\n4.1 Mutual Information\nInformation-theoretic measures can be used to identify to\nwhat degree a feature shares information with class labels.\nFor a feature Xand a class label Y, the mutual information\nI(X;Y)between these two can be given as:\nI(X;Y) =H(X)\u0000H(XjY); (3)\nthat is, the entropy of the feature H(X)minus the entropy\nof that feature if the class is known H(XjY). By tak-\ning membership of playlists as a class label, we can deter-\nmine how much we can know about a song’s features if we\nknow what playlist it is in. When using mutual information\nto compare clusterings in this way, care must be taken to\naccount for random chance mutual information [19]. We\nadapt this approach to focus on how much the feature en-\ntropy is reduced, and normalise accordingly:\nAMI (X;Y) =I(X;Y)\u0000E[I(X;Y)]\nH(X)\u0000E[I(X;Y)]; (4)\nwhere AMI (X;Y)is the adjusted mutual information and\nE[I(X;Y)]is the expectation of the mutual information\ni.e. due to random chance. The AMI gives a normalised\nmeasure of how much of the feature’s entropy is explained\nby the playlist. When AMI = 1, the feature level is known\nexactly if the playlist is known, when AMI = 0, nothing\nabout the feature is known if the playlist is known.\n4.2 Linking Features to Playlists\nWe analysed the AMI between the 10;000playlists in the\nSPUD dataset and a variety of high level music features.\nThe ranking of some of these features is given in Figure 3.\nOur aim is only to illustrate this approach, as any results\nare only as reliable as the underlying features. With this in\nmind, the features ROCK and ANGRY had the most uncer-\ntainty explained by playlist membership. While the values\nmay seem small, they are calculated over many playlists,\nwhich may combine moods, genres and other criteria. As\nthese features change most between playlists (rather than\nwithin them), they are the most useful for characterising\nthe differences between playlists. The DURATION feature\nranked higher than expected, further investigation revealed\nplaylists that combined lengthy DJ mixes. It is perhaps\nunsurprising that playlists were not well characterised by\nwhether they included WORLD music.\nFigure 3. Features are ranked by their Adjusted Mutual\nInformation with playlist membership. Playlists are dis-\ntinguished more by whether they contain ROCK orANGRY\nmusic than by whether they contain POPULAR orWORLD .\nIt is of interest that TEMPO was not one of the highest\nranked features, illustrating the style of insights available\nwhen using this approach. Further investigation is required\nto determine whether playlists are not based on tempo as\nmuch as is often asumed or if this result is due to the pecu-\nliarities of the proprietary perceptual tempo detection.\n4.3 Feature Selection\nFeatures can be selected using information-theoretic mea-\nsures, with a rigorous treatment of the ﬁeld given by Brown\net al. [2]. They deﬁne a unifying framework within which\nto discuss methods for selecting a subset of features using\nmutual information. This is done by deﬁning a J criterion\nfor a feature:\nJ(fn) =I(fn;CjS): (5)\nThis gives a measure of how much information the fea-\nture shares with playlists given some previously selected\nfeatures, and can be used as a greedy feature selection al-\ngorithm. Intuitively, features should be selected that are\nrelevant to the classes but that are also not redundant with\nregard to previously selected features. A range of estima-\ntors for I(fn;CjS)are discussed in [2].\nAs a demonstration of the feature selection approach\nwe have described, we apply it to the features depicted in\nFigure 3, selecting features to minimise redundancy. The\nselected subset of features in rank order is: ROCK ,DURA -\nTION ,POPULARITY ,TENDER and JOY. It is notable that\nANGRY had an AMI that was almost the same as ROCK ,\nbut it is redundant if ROCK is included. Unsurprisingly, the\nsecond feature selected is from a different source than the\nﬁrst – the duration information from Spotify adds to that\nused to produce the Syntonetic mood and genre features.\nReducing redundancy in the selected features in this way\nyields a very different ordering, though one that may give a\nclearer insight into the factors behind playlist construction.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5645. DISCUSSION\nWhile we reiterate that this work only uses a speciﬁc set of\nmusic features and user base, we consider our results to be\nencouraging. It is clear that the use of entropy can provide\na detailed time-series analysis of user behaviour and could\nprove a valuable tool for MIR evaluation. Similarly, the use\nof adjusted mutual information allows MIR researchers to\ndirectly link work on acquiring music features to the ways\nin which users interact with music. In this section we con-\nsider how the information-theoretic techniques described\nin this work can inform the ﬁeld of MIR.\n5.1 User-Centred Feature Selection\nThe feature selection shown in this paper is done directly\nfrom the user data. In contrast, feature selection is usu-\nally performed using classiﬁer wrappers with ground truth\nclass labels such as genre. The use of genre is based on the\nassumption that it would support the way users currently\norganise music and features are selected based on these\nlabels. This has lead to issues including classiﬁers being\ntrained on factors that are confounded with these labels\nand that are not of relevance to genre or users [18]. Our\napproach selects features independently of the choice of\nclassiﬁer, in what is termed a ‘ﬁlter’ approach. The beneﬁt\nof doing this is that a wide range of features can be quickly\nﬁltered at relatively little computational expense. While\nthe classiﬁer ‘wrapper’ approach may achieve greater per-\nformance, it is more computationally expensive and more\nlikely to suffer from overﬁtting.\nThe key beneﬁt of ﬁltering features based on user be-\nhaviour is that it provides a perspective on music features\nthat is free from assumptions about users and music ground\ntruth. This user-centred perspective provides a sanity-check\nfor music features and classiﬁcation – if a feature does not\nreﬂect the ways in which users organise their music, then\nhow useful is it for music retrieval?\n5.2 When To Learn\nThe information-theoretic measures presented offer an im-\nplicit relevance feedback for music retrieval. While we\nhave considered the entropy of features as reﬂecting user\nbehaviour, this behaviour is conditioned upon the existing\nmusic retrieval interfaces being used. For example, after\nissuing a query and receiving results, the user selects rel-\nevant songs from those results. If the entropy of a feature\nfor those selected songs is small relative to the result set,\nthen this feature is implicitly relevant to the retrieval.\nThe identiﬁcation of shufﬂe and explorative behaviour\nprovides some context for this implicit relevance feedback.\nMusic which is listened to in a seemingly random fashion\nmay represent an absent or disengaged user, adding noise\nto attempts to weight recommender systems or build a user\nproﬁle. At the very least, where entropy is high across all\nfeatures, then those features do not reﬂect the user’s mental\nmodel for their music retrieval. The detection of shufﬂe\nor high-entropy listening states thus provides a useful data\nhygiene measure when interpreting listening data.5.3 Engagement\nThe entropy measures capture how much each feature is\nbeing ‘controlled’ by the user when selecting their music.\nWe have shown that it spans a scale from a user choosing to\nlisten to something speciﬁc to the user yielding control to\nradio or shufﬂe. Considering entropy over many features in\nthis way gives a high-dimensional vector representing the\nuser’s engagement with music. Different styles of music\nretrieval occupy different points in this space, commonly\nthe two extremes of listening to a speciﬁc album or just\nshufﬂing. There is an opportunity for music retrieval that\nhas the ﬂexibility to support users engaging and applying\ncontrol over music features only insofar as they desire to.\nAn example of this would be a shufﬂe mode that allowed\nusers to bias it to varying degrees, or to some extent, the\nfeedback mechanism in recommender systems.\n5.4 Open Source\nThe SPUD dataset is made available for download at:\nhttp://www.dcs.gla.ac.uk/ ˜daniel/spud/\nExample R scripts for importing data from SPUD and pro-\nducing the analyses and plots in this paper are included.\nThe code used to scrape this dataset is available under the\nMIT open source license, and can be accessed at:\nhttp://www.github.com/dcboland/\nThe MoodAgent features are commercially sensitive,\nthus not included in the SPUD dataset. At present, indus-\ntry is far better placed to provide such large scale analyses\nof music data than academia. Even with user data and the\nrequired computational power, large-scale music analyses\nrequire licensing arrangements with content providers, pre-\nsenting a serious challenge to academic MIR research. Our\nadoption of commercially provided features has allowed us\nto demonstrate our information-theoretic approach, and we\ndistribute the audio stream links, however it is unlikely that\nmany MIR researchers will have the resources to replicate\nall of these large scale analyses. The CoSound4project\nis an example of industry collaborating with academic re-\nsearch and state bodies to navigate the complex issues of\nmusic licensing and large-scale analysis.\n6. CONCLUSION\nThis work introduces an information-theoretic approach to\nthe study of users’ music listening behaviour. The case is\nmade for a more user-focused yet quantitative approach to\nevaluation in MIR. We described the use of entropy to pro-\nduce time-series analyses of user behaviour, and showed\nhow changes in music-listening style can be detected. An\nexample is given where a user started using online radio,\nhaving higher entropy in their listening. We introduced\nthe use of adjusted mutual information to establish which\nmusic features are linked to playlist organisation. These\ntechniques provide a quantitative approach to user studies\nand ground feature selection in user behaviour, contribut-\ning tools to support the user-centred future of MIR.\n4 .http://www.cosound.dk/ Last accessed: 30/04/14\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n565ACKNOWLEDGEMENTS\nThis work was supported in part by Bang & Olufsen and\nthe Danish Council for Strategic Research of the Danish\nAgency for Science Technology and Innovation under the\nCoSound project, case number 11-115328. This publica-\ntion only reﬂects the authors’ views.\n7. REFERENCES\n[1] T Bertin-Mahieux, D. P Ellis, B Whitman, and\nP Lamere. The Million Song Dataset. In Proceedings\nof the 12th International Conference on Music Infor-\nmation Retrieval, Miami, Florida, 2011.\n[2] G Brown, A Pocock, M.-J Zhao, and M Luj ´an. Condi-\ntional likelihood maximisation: a unifying framework\nfor information theoretic feature selection. The Journal\nof Machine Learning Research, 13:27–66, 2012.\n[3] W. S Cleveland and S. J Devlin. Locally weighted re-\ngression: an approach to regression analysis by local\nﬁtting. Journal of the American Statistical Association,\n83(403):596–610, 1988.\n[4] A Craft and G Wiggins. How many beans make ﬁve?\nthe consensus problem in music-genre classiﬁcation\nand a new evaluation method for single-genre categori-\nsation systems. In Proceedings of the 8th International\nConference on Music Information Retrieval, Vienna,\nAustria, 2007.\n[5] S. J Cunningham, N Reeves, and M Britland. An ethno-\ngraphic study of music information seeking: implica-\ntions for the design of a music digital library. In Pro-\nceedings of the 3rd ACM/IEEE-CS joint conference on\nDigital libraries, Houston, Texas, 2003.\n[6] J. S Downie. Music Information Retrieval. An-\nnual Review of Information Science and Technology,\n37(1):295–340, January 2003.\n[7] J. S Downie. The Music Information Retrieval\nEvaluation eXchange (MIREX). D-Lib Magazine,\n12(12):795–825, 2006.\n[8] J Futrelle and J. S Downie. Interdisciplinary Research\nIssues in Music Information Retrieval: ISMIR 2000 -\n2002. Journal of New Music Research, 32(2):121–131,\n2003.\n[9] J. D Gould and C Lewis. Designing for usability: key\nprinciples and what designers think. Communications\nof the ACM, 28(3):300–311, 1985.[10] S. G Hart. NASA-task load index (NASA-TLX); 20\nyears later. In Proceedings of the Human Factors and\nErgonomics Society 50th Annual Meeting, San Fran-\ncisco, California, 2006.\n[11] R Killick, P Fearnhead, and I. A Eckley. Optimal De-\ntection of Changepoints With a Linear Computational\nCost. Journal of the American Statistical Association,\n107(500):1590–1598, 2012.\n[12] J. H Lee and S. J Cunningham. The Impact (or Non-\nimpact) of User Studies in Music Information Re-\ntrieval. In Proceedings of the 13th International Con-\nference for Music Information Retrieval, Porto, Portu-\ngal, 2012.\n[13] B McFee and G Lanckriet. Hypergraph models of\nplaylist dialects. In Proceedings of the 13th Interna-\ntional Conference for Music Information Retrieval,\nPorto, Portugal, 2012.\n[14] M Schedl and A Flexer. Putting the User in the Center\nof Music Information Retrieval. In Proceedings of the\n13th International Conference on Music Information\nRetrieval, Porto, Portugal, 2012.\n[15] M Schedl, A Flexer, and J Urbano. The neglected user\nin music information retrieval research. Journal of In-\ntelligent Information Systems, 41(3):523–539, 2013.\n[16] B. L Sturm. An Analysis of the GTZAN Music Genre\nDataset. In Proceedings of the 2nd International ACM\nWorkshop on Music Information Retrieval with User-\ncentered and Multimodal Strategies, MIRUM ’12,\nNew York, USA, 2012.\n[17] B. L Sturm. Classiﬁcation accuracy is not enough.\nJournal of Intelligent Information Systems, 41(3):371–\n406, 2013.\n[18] B. L Sturm. A simple method to determine if a music\ninformation retrieval system is a horse. IEEE Transac-\ntions on Multimedia, 2014.\n[19] N. X Vinh, J Epps, and J Bailey. Information theoretic\nmeasures for clusterings comparison: Variants, proper-\nties, normalization and correction for chance. Journal\nof Machine Learning Research, 11:2837–2854, 2010.\n[20] D. M Weigl and C Guastavino. User studies in the mu-\nsic information retrieval literature. In Proceedings of\nthe 12th International Conference for Music Informa-\ntion Retrieval, Miami, Florida, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n566"
    },
    {
        "title": "On Comparative Statistics for Labelling Tasks: What can We Learn from MIREX ACE 2013?",
        "author": [
            "John Ashley Burgoyne",
            "W. Bas de Haas",
            "Johan Pauwels"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417091",
        "url": "https://doi.org/10.5281/zenodo.1417091",
        "ee": "https://zenodo.org/records/1417091/files/BurgoyneHP14.pdf",
        "abstract": "For , the evaluation of audio chord estimation () followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the  evaluations from previous years. With this new informa- tion, however, comes new interpretive challenges. What are the correlations among different songs and, more im- portantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in propor- tion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song- algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires mov- ing beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for , supported with gen- eralised estimating equations () to correct for repeated measures. We use the results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone.",
        "zenodo_id": 1417091,
        "dblp_key": "conf/ismir/BurgoyneHP14",
        "keywords": [
            "evaluation",
            "audio chord estimation",
            "new scheme",
            "chord vocabularies",
            "segmentation measures",
            "informa- tion",
            "interpretive challenges",
            "correlations",
            "songs",
            "submissions"
        ],
        "content": "ON COMPARATIVE STATISTICS FOR LABELLING TASKS:\nWHAT CAN WE LEARN FROM MIREX ACE 2013?\nJohn Ashley Burgoyne\nUniversiteit van Amsterdam\nj.a.burgoyne@uva.nlW. Bas de Haas\nUniversiteit Utrecht\nw.b.dehaas@uu.nlJohan Pauwels\n￿￿￿￿ ￿￿￿￿￿–￿￿￿￿–￿￿￿￿\njohan.pauwels@gmail.com\nABSTRACT\nFor￿￿￿￿￿ ￿￿￿￿, the evaluation of audio chord estimation\n(￿￿￿) followed a new scheme. Using chord vocabularies\nof differing complexity as well as segmentation measures,\nthe new scheme provides more information than the ￿￿￿\nevaluations from previous years. With this new informa-\ntion, however, comes new interpretive challenges. Whatare the correlations among different songs and, more im-\nportantly, different submissions across the new measures?\nPerformance falls off for all submissions as the vocabularies\nincrease in complexity, but does it do so directly in propor-\ntion to the number of more complex chords, or are certain\nalgorithms indeed more robust? What are the outliers, song-\nalgorithm pairs where the performance was substantially\nhigher or lower than would be predicted, and how can they\nbe explained? Answering these questions requires mov-ing beyond the Friedman tests that have most often beenused to compare algorithms to a richer underlying model.\nWe propose a logistic-regression approach for generating\ncomparative statistics for ￿￿￿￿￿ ￿￿￿ , supported with gen-\neralised estimating equations (￿￿￿￿) to correct for repeated\nmeasures. We use the ￿￿￿￿￿ ￿￿￿￿ ￿￿￿ results as a case\nstudy to illustrate our proposed method, including some of\ninteresting aspects of the evaluation that might not apparent\nfrom the headline results alone.\n1. INTRODUCTION\nAutomatic chord estimation (￿￿￿) has a long tradition\nwithin the music information retrieval (￿￿￿) community,\nand chord transcriptions are generally recognised as a useful\nmid-level representation in academia as well as in industry.\nFor instance, in an academic context it has been shown that\nchords are interesting for addressing musicological hypo-\ntheses [￿,￿￿ ], and that they can be used as a mid-level feature\nto aid in retrieval tasks like cover-song detection [￿,￿￿ ]. In\nJohan Pauwels is no longer af￿ liated with ￿￿￿￿ . Data and source code\nto reproduce this paper, including all statistics and￿ gures, are available\nfromhttp://bitbucket.org/jaburgoyne/ismir-2014.\n© John Ashley Burgoyne, W. Bas de Haas, Johan Pauwels.\nLicensed under a Creative Commons Attribution￿. ￿International License\n(￿￿ ￿￿ ￿.￿).Attribution: John Ashley Burgoyne, W. Bas de Haas, Johan\nPauwels. “On comparative statistics for labelling tasks: What can we learn\nfrom ￿￿￿￿￿ ￿￿￿ ￿￿￿￿?”,￿￿ th International Society for Music Information\nRetrieval Conference,￿￿￿￿.an industrial setting, music start-ups like Riffstation￿and\nChordify￿use￿￿￿ in their music teaching tools, and at\nthe time of writing, Chordify attracts more than ￿million\nunique visitors every month [￿].\nIn order to compare different algorithmic approaches in\nan impartial setting, the Music Information Retrieval Evalu-\nation eXchange (￿￿￿￿￿) introducted an annual ￿￿￿task in\n￿￿￿￿. Since then, between ￿￿and￿￿algorithms have been\nsubmitted each year by between ￿and￿￿teams. Despite\nthe fact that ￿￿￿ algorithms are used outside of academic\nenvironments, and even though the number of ￿￿￿￿￿ par-\nticipants has decreased slightly over the last three years,\nthe problem of automatic chord estimation is nowhere near\nsolved. Automatically extracted chord sequences have clas-\nsically been evaluated by calculating the chord symbol recall\n(￿￿￿) , which re￿ects the proportion of correctly labelled\nchords in a single song, and a weighted chord symbol recall\n(￿￿￿￿) , which weights the average ￿￿￿of a set of songs by\ntheir length. On fresh validation data, the best-performing\nalgorithms in ￿￿￿￿ achieved ￿￿￿￿ of only ￿￿percent, and\nthat only when the range of possible chords was restricted\nexclusively to the ￿￿major, minor and “no-chord” labels;\nthe￿ gure drops to ￿￿percent when the evaluation is exten-\nded to include seventh chords (see Table￿).\n￿￿￿￿￿ is a terri￿c platform for evaluating the perform-\nance of ￿￿￿ algorithms, but by ￿￿￿￿ it was already being\nrecognised that the metrics could be improved. At that time,\nthey included only ￿￿￿and￿￿￿￿ using a vocabulary of￿￿\nmajor chords, ￿￿minor chords and a “no-chord” label. At\n￿￿￿￿￿ ￿￿￿￿, a group of ten researchers met to discuss their\ndissatisfaction. In the resulting ‘Utrecht Agreement’,￿it\nwas proposed that future evaluations should include more\ndiverse chord vocabularies, such as seventh chords and in-\nversions, as the￿￿ -chord vocabulary was considered a rather\ncoarse representation of tonal harmony. Furthermore, the\ngroup agreed that it was important to include a measure of\nsegmentation quality in addition to ￿￿￿and￿￿￿￿ .\nAt approximately the same time, Christopher Harte pro-\nposed a formalisation of measures that implemented the\naspirations indicated in the Utrecht agreement [￿]. Recently,\nPauwels and Peeters reformulated and extended Harte’s\nwork with the precise aim of handling differences in chord\nvocabulary between annotated ground truth and algorithmic\n￿http://www.riffstation.com/\n￿http://chordify.net\n￿http://www.music-ir.org/mirex/wiki/The_\nUtrecht_Agreement_on_Chord_Evaluation\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n525Algorithm # Types Inversions? Training? I II III IV V VI VII VIII\n￿￿￿ 7 • 76 74 72 60 58 84 79 89\n￿￿￿￿￿ 10 75 71 69 59 57 82 79 86\n￿￿￿ 13 • 76 72 70 59 57 85 80 90\n￿￿￿￿￿ 10 74 71 69 58 56 83 79 86\n￿￿￿ 13 76 72 70 58 56 85 81 89\n￿￿￿ 7 75 71 69 54 52 83 80 88\n￿￿￿ 5 69 66 64 51 49 83 78 87\n￿￿￿ 2 70 68 65 50 48 83 82 84\n￿￿￿ 10 • 71 67 65 49 47 83 83 83\n￿￿￿ 2 71 67 65 49 46 82 79 86\n￿￿￿ 5 67 63 61 44 43 82 81 83\n￿￿￿ 2 9 7 6 5 5 51 92 35\nTable￿ . Number of supported chord types, inversion support, training support, and ￿￿￿￿￿ results on the Billboard ￿￿￿￿test\nset for all￿￿￿￿ ￿￿￿ submissions. I: root only; II: major-minor vocabulary; III: major-minor vocabulary with inversions; IV:\nmajor-minor vocabulary with sevenths; V: major-minor vocabulary with sevenths and inversions; VI: mean segmentation\nscore; VII: under-segmentation; VIII: over-segmentation. Adapted from the ￿￿￿￿￿ Wiki.\noutput on one hand, and among the output of different al-\ngorithms on the other hand [￿￿]. They also performed a\nrigorous re-evaluation of all ￿￿￿￿￿ ￿￿￿ submissions from\n￿￿￿￿ to￿￿￿￿ . As of ￿￿￿￿￿ ￿￿￿￿, these revised evalu-\nation procedures, including the chord-sequence segment-ation evaluation suggested by Harte [￿] and Mauch [￿￿],\nhave been adopted in the context of the ￿￿￿￿￿ ￿￿￿ task.\n￿￿￿￿￿ ￿￿￿ evaluation has also typically included com-\nparative statistics to help determine whether the differencesin performance between pairs of algorithms are statistically\nsigni￿cant. Traditionally, Friedman’s ￿￿￿￿￿ has been used\nfor this purpose, accompanied by Tukey’s Honest Signi￿c-\nant Difference tests for each pair of algorithms. Friedman’s\n￿￿￿￿￿ is equivalent to a standard two-way ￿￿￿￿￿ with\nthe actual measurements (in our case ￿￿￿￿ or directional\nHamming distance [ ￿￿￿], the new segmentation measure)\nreplaced by the rank of each treatment (in our case, each al-\ngorithm) on that measure within each block (in our case, foreach song) [￿￿]. The rank transformation makes Friedman’s\n￿￿￿￿￿ an excellent ‘one size￿ ts all’ approach that can be\napplied with minimal regard to the underlying distribution\nof the data, but these bene￿ts come with costs. Like any non-\nparametric test, Friedman’s ￿￿￿￿￿ can be less powerful\nthan parametric alternatives where the distribution is known,\nand the rank transformation can obscure information in-herent to the underlying measurement, magnifying trivial\ndifferences and neutralising signi￿cant inter-correlations.\nBut there is no need to pay the costs of Friedman’s ￿￿-\n￿￿￿ for evaluating chord estimation. Fundamentally, ￿￿￿￿\nis a proportion, speci￿cally the expected proportion of au-\ndio frames that an estimation algorithm will label correctly,\nand as such, it￿ ts naturally into logistic regression (i.e., a\nlogit model ). Likewise, ￿￿￿ is constrained to fall between\n￿and￿￿￿percent, and thus it is also suitable for the same\ntype of analysis. The remainder of this paper describes howlogistic regression can be used to compare chord estimationalgorithms, using ￿￿￿￿￿ results from ￿￿￿￿ to illustrate four\nkey bene￿ts: easier interpretation, greater statistical power,\nbuilt-in correlation estimates for identifying relationships\namong algorithms, and better detection of outliers.2. LOGISTIC REGRESSION WITH GEES\nProportions cannot be distributed normally because they are\nsupported exclusively on [￿,￿ ], and thus they present chal-\nlenges for traditional techniques of statistical analysis. Logitmodels are designed to handle these challenges without sac-\nri￿cing the simplicity of the usual linear function relating\nparameters and covariates [￿, ch.￿]:\n⇡(x;\u0000)=ex0\u0000\n1+ex0\u0000, (￿)\nor equivalently\nlog⇡(x;\u0000)\n1\u0000⇡(x;\u0000)=x0\u0000, (￿)\nwhere ⇡represents the relative frequency of ‘success’ given\nthe values of covariates in xand parameters \u0000. In the case\nof a basic model for ￿￿￿￿￿ ￿￿￿ ,xwould identify the al-\ngorithm and ⇡would be the relative frequency of correct\nchord labels for that algorithm (i.e., ￿￿￿￿ ). In the case\nof data like ￿￿￿ results, where there are proportions piof\ncorrect labels over nianalysis frames rather than binary suc-\ncesses or failures, iindexing all combinations of individual\nsongs and algorithms, logistic regression assumes that each\npirepresents the observed proportion of successes among\nniconditionally-independent binary observations, or more\nformally, that the piare distributed binomially:\nfP|N,X(p|n,x;\u0000)= n\npn!\n⇡pn(1\u0000⇡)(1\u0000p)n.(￿)\nThe expected value for each piis naturally ⇡i=⇡(xi;\u0000),\nthe overall relative frequency of success given xi:\nE[P |N,X]=⇡(x;\u0000). (￿)\nLogistic regression models are most often￿ t by the\nmaximum-likelihood technique, i.e., one is seeking a vector\nˆ\u0000to maximise the log-likelihood given the data:\n`P|N,X(\u0000;p,n,X)=X\ni\"\nlog ni\npini!\n+\npinilog⇡i+(1\u0000pi)nilog (1\u0000⇡i)#\n.(￿)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n526One thus solves the system of likelihood equations for \u0000,\nwhereby the gradient of Equation ￿is set to zero:\nr\u0000`P|N,X(\u0000;p,n,X)=X\ni(pi\u0000⇡i)nixi=0 (￿)\nand so X\nipinixi=X\ni⇡inixi. (￿)\nIn the case of ￿￿￿￿￿ ￿￿￿ evaluation, each xiis simply\nan indicator vector to partition the data by algorithm, and\nthusˆ\u0000is the parameter vector for which ⇡iequals the song-\nlength–weighted mean over all pifor that algorithm.\n2.1 Quasi-Binomial Models\nUnder a strict logit model, the variance of each piis inversely\nproportional to ni:\nvar[P |N,X]= 1\nn!\n⇡(1\u0000⇡). (￿)\nEquation ￿only holds, however, if the estimates of chord\nlabels for each audio frame are independent. For ￿￿￿, this is\nunrealistic: only the most naïve algorithms treat every frame\nindependently. Some kind of time-dependence structure is\nstandard, most frequently a hidden Markov model or some\nclose derivative thereof. Hence one would expect that the\nvariance of ￿￿￿￿ estimates should be rather larger than the\nbasic logit model would suggest.\nThis type of problem is extremely common across dis-\nciplines, so much so that is has been given a name, over-\ndispersion, and some authors go so far as to state that ‘unless\nthere are good external reasons for relying on the binomial\nassumption [of independence], it seems wise to be cautious\nand to assume that over-dispersion is present to some ex-\ntent unless and until it is shown to be absent’ [￿￿, p.￿￿￿].One standard approach to handling over-dispersion is touse a so-called quasi-likelihood [￿, §￿.￿ ]. In case of lo-\ngistic regression, this typically entails a modi￿cation to the\nassumption on the distribution of the pithat includes an\nadditional dispersion parameter \u0000. The expected values are\nthe same as a standard binomial model, but\nvar[P |N,X]=✓\u0000\nn◆\n⇡(1\u0000⇡). (￿)\nThese models are known as quasi-likelihood models\nbecause one loses a closed-form solution for the actualprobability distribution\nfP|N,X; one knows only that the\npibehave something like binomially-distributed variables,\nwith identical means but proportionally more variance. The\nparameter estimates ˆ\u0000and predictions ⇡(·;ˆ\u0000)for a quasi-\nbinomial model are the same as ordinary logistic regression,\nbut the estimated variance-covariance matrices are scaled\nby the estimated dispersion parameter ˆ\u0000(and likewise the\nstandard errors are scaled by its square root). The disper-\nsion parameter is estimated so that the theoretical variance\nmatches the empirical variance in the data, and because of\nthe form of Equation￿ , it renders any scaling considerations\nfor the nimoot.\nOther approaches to handling over-dispersion include\nbeta-binomial models [￿, §￿￿.￿ ] and beta regression [￿],\nbut we prefer the simplicity of the quasi-likelihood model.2.2 Generalised Estimating Equations ( ￿￿￿s)\nThe quasi-binomial model achieves most of what one would\nbe looking for when evaluating ￿￿￿ for￿￿￿￿￿ : it handles\nproportions naturally, is consistent with the weighted aver-\naging used to compute ￿￿￿￿ , and adjusts for over-dispersion\nin a way that also eliminates any worries about scaling. Non-\netheless, it is slightly over-conservative for evaluating ￿￿￿.\nAs discussed earlier, quasi-binomial models are necessary\nto account for over-dispersion, and one important source\nof over-dispersion in these data is the lack of independence\nof chord estimates from most algorithms within the same\nsong. ￿￿￿￿￿ exhibits another important violation of the in-\ndependence assumption, however: all algorithms are tested\non the same sets of songs, and some songs are clearly more\ndif￿cult than others. Put differently, one does not expect\nthe algorithms to perform completely independently of one\nanother on the same song but rather expects a certain cor-\nrelation in performance across the set of songs. By taking\nthat correlation into account, one can improve the preci-sion of estimates, particularly the precision of pair-wise\ncomparisons [￿, §￿￿.￿].\nA relatively straightforward variant of quasi-likelihood\nknown as generalised estimating equations (￿￿￿s) incor-\nporates this type of correlation [￿, ch.￿￿ ]. With the ￿￿￿\napproach, rather than predicting each piindividually, one\npredicts complete vectors of proportions pifor each relev-\nant group, much as Friedman’s test seeks to estimate ranks\nwithin each group. For ￿￿￿, the groups are songs, and thus\none considers the observations to be vectors pi, one for each\nsong, where pijrepresents the ￿￿￿or segmentation score\nfor algorithm jon song i. Analogous to the case of ordinary\nquasi-binomial or logistic regression,\nEf\nPj|N,Xjg\n=⇡(xj;\u0000). (￿￿)\nLikewise, analogous to the quasi-binomial variance,\nvarf\nPj|N,Xjg\n=✓\u0000\nn◆\n⇡j(1\u0000⇡j). (￿￿)\nBecause the ￿￿￿ approach is concerned with vector-\nvalued estimates rather than point estimates, it also involves\nestimating a full variance-covariance matrix. In addition to\n\u0000and\u0000, the approach requires a further vector of parameters\n↵and an a priori assumption on the correlation structure of\nthePjin the form of a function R(↵)that yields a correlation\nmatrix. (One might, for example, assume that that the Pj\nareexchangeable, i.e., that every pair shares a common\ncorrelation coef￿ cient.) Then if Bis a diagonal matrix such\nthatBjj=var [Pj|N,Xj],\ncov[P |N,X]=B1/2R(↵)B1/2. (￿￿)\nIf all of the Pjare uncorrelated with each other, then this\nformula reduces to the basic quasi-binomial model, which\nassumes a diagonal covariance matrix. The￿ nal step of\n￿￿￿ estimation adjusts Equation ￿￿according to the actual\ncorrelations observed in the data, and as such, ￿￿￿s are\nquite robust in practice even when the a priori assumptions\nabout the correlation structure are incorrect [￿, §￿￿.￿.￿].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n527● ● ●\n●● ●\n●● ● ●\n●●\n●●\n●●\n●●\n● ●●\n●●\n●\n●KO2\nNMSD2\nCB4\nNMSD1\nCB3\nKO1\nPP4\nPP3\nCF2\nNG1NG2\nSB824681012\nAlgorithmRank per Song (1 low; 12 high)  \n        f \ng           f \ng             \ng         \ne \n  \ng           f \ng         \ne \nf       \nc \nd \n            \nd e \n        \nc \n            \nc \nd \n        \nb \n          a \n            \n● ●●\n●●\n●\n●●\n●●\n●\n●\n(a) Friedman’s ￿￿￿￿￿●●●●●●●●●●●●●●\n●●\n●●●\n●KO2\nNMSD2\nCB4\nNMSD1\nCB3\nKO1\nPP4\nPP3\nCF2\nNG1NG2\nSB80.00.20.40.60.81.0\nAlgorithmChord−Symbol Recall (CSR)  \n        f           f         \ne \nf         \ne \nf         \ne \nf       \nd e \n      \nc \nd \n      \nb \nc \n          \nc \nd \n      \nb \nc \n        \nb \n        a \n          \n● ● ● ● ●\n●● ● ● ●●\n●\n(b) Logistic Regression\nFigure￿ . Boxplots and compact letter displays for the ￿￿￿￿￿ ￿￿￿ ￿￿￿￿ results on the Billboard ￿￿￿￿ test set with vocabulary\nV (seventh chords and inversions), weighted by song length. Bold lines represent medians and￿ lled dots means. N=￿￿￿\nsongs per algorithm. Given the respective models, there are insuf￿ cient data to distinguish among algorithms sharing a letter,\ncorrecting to hold the ￿￿￿at↵=.￿￿￿. Although Friedman’s ￿￿￿￿￿ detects ￿more signi￿cant pairwise differences than\nlogistic regression (￿￿ vs.￿￿ ), it operates on a different scale than ￿￿￿and misorders algorithms relative to ￿￿￿￿ .\n3. ILLUSTRATIVE RESULTS\n￿￿￿￿￿ ￿￿￿ ￿￿￿￿ evaluated ￿￿algorithms according to a\nbattery of eight rubrics ( ￿￿￿￿ on￿ve harmonic vocabu-\nlaries and three segmentation measures) on each of three\ndifferent data sets (the Isophonics set, including music from\nthe Beatles, Queen, and Zweieck [￿￿] and two versions of\nthe McGill Billboard set, including music from the Amer-\nican pop charts [￿]). There is insuf￿ cient space to present\nthe results of logistic regression on all combinations, and so\nwe will focus on a single one of the data sets, the Billboard\n￿￿￿￿ test set. In some cases, logistic regression allows us to\nspeak to all measures (￿￿ ￿￿￿ observations), but in general,\nwe will also restrict ourselves to discussing the newest and\nmost challenging of the harmonic vocabularies for ￿￿￿￿ :\nVocabulary V (￿￿￿￿ observations), which includes major\nchords, minor chords, major sevenths, minor sevenths, dom-\ninant sevenths, and the complete set of inversions of all of\nthe above. We are interested in four key questions.\n￿.How do pairwise comparisons under logistic regres-\nsion compare to pairwise comparisons with Fried-\nman’s ￿￿￿￿￿ ? Is logistic regression more powerful?\n￿.Are there differences among algorithms as the har-\nmonic vocabularies get more dif￿ cult, or is the drop\nperformance uniform? In other words, is there a be-\nne￿t to continuing with so many vocabularies?\n￿.Are all ￿￿￿ algorithms making similar mistakes, or\ndo they vary in their strengths and weaknesses?\n￿.Which algorithm-song pairs exhibited unexpectedly\ngood or bad performance, and is there anything to be\nlearned from these observations?\n3.1 Pairwise Comparisons\nThe boxplots in Figure ￿give a more detailed view of the\nperformance of each algorithm than Table￿ . The￿gureis restricted to Vocabulary V, with the algorithms in des-\ncending order by ￿￿￿￿ . Figure￿ a comes from Friedman’s\n￿￿￿￿￿ weighted by song length, and thus its y-axis re￿ects\nnot￿￿￿directly but the per-song ranks with respect to ￿￿￿.\nFigure￿ b comes from quasi-binomial regression estimated\nwith ￿￿￿s, as described in Section￿ . Its y-axis does re￿ect\nper-song ￿￿￿. Above the boxplots, all signi￿cant pairwise\ndifferences are recorded as a compact letter display. In the\ninterest of reproducible research, we used a stricter ↵=\n.￿￿￿ threshold for reporting pairwise comparisons with the\nmore contemporary false-discovery-rate (￿￿￿) approach of\nBenjamini and Hochberg, as opposed to more traditional\nTukey tests at ↵=.￿￿[￿ ,￿]. Within either of the sub￿gures,\nthe difference in performance between two algorithms that\nshare any letter in the compact letter display is notstatistic-\nally signi￿cant. Overall, Friedman’s ￿￿￿￿￿ found ￿more\nsigni￿cant pairwise differences than logistic regression.\n3.2 Effect of Vocabulary\nTo test the utility of the new evaluation vocabularies, we\nran both Friedman ￿￿￿￿￿ s (ranked separately for each\nvocabulary) and logistic regressions and looked for signi-\n￿cant interactions among the algorithm, inversions (present\nor absent from the vocabulary) and the complexity of the\nvocabulary (root only, major-minor, or major-minor with\n￿ths). Under Friedman’s ￿￿￿￿￿ , there was a signi￿cant\nAlgorithm ⇥Complexity interaction, F(￿￿,￿￿￿￿ )=￿.￿￿,\np<.￿￿￿. The logistic regression model identi￿ed a sig-\nni￿cant three-way Algorithm ⇥Complexity ⇥Inversions\ninteraction, \u00002(￿￿)=￿￿.￿￿, p<.￿￿￿, but the additional\ninteraction with inversions should be interpreted with care:\nonly one algorithm ( ￿￿￿) attempts to recognise inversions.\n3.3 Correlation Matrices\nTable ￿presents the inter-correlations of ￿￿￿￿ between\nalgorithms, rank-transformed (Spearman’s correlations, ana-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n528Algorithm ￿￿￿ ￿￿￿￿￿ ￿￿￿ ￿￿￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿ ￿￿￿\n￿￿￿ – .07 .11 \u0000.05 .10 .03\u0000.41⇤\u0000.44⇤\u0000.03\u0000.35⇤.05\u0000.01\n￿￿￿￿￿ .25⇤– \u0000.01 .49⇤\u0000.25⇤\u0000.20\u0000.19\u0000.36⇤.00\u0000.33⇤.02\u0000.06\n￿￿￿ .41⇤.39⇤– .12 .47⇤\u0000.46⇤\u0000.30⇤\u0000.48⇤.09\u0000.38⇤.08\u0000.09\n￿￿￿￿￿ .30⇤.60⇤.53⇤– \u0000.17\u0000.45⇤\u0000.08\u0000.45⇤.27⇤\u0000.44⇤.17\u0000.10\n￿￿￿ .34⇤.10 .76⇤.42⇤–\u0000.19\u0000.26⇤\u0000.14\u0000.08\u0000.17\u0000.16\u0000.08\n￿￿￿ \u0000.04 \u0000.42⇤\u0000.51⇤\u0000.51⇤\u0000.29⇤–\u0000.10 .42⇤\u0000.41⇤.50⇤\u0000.52⇤.05\n￿￿￿ \u0000.22 .08 \u0000.16 .06 \u0000.07\u0000.05 – .37⇤\u0000.03 .00 .05\u0000.03\n￿￿￿ \u0000.49⇤\u0000.46⇤\u0000.61⇤\u0000.53⇤\u0000.37⇤.68⇤.22 –\u0000.48⇤.66⇤\u0000.48⇤.04\n￿￿￿ .09 .19 .24⇤.42⇤.17\u0000.49⇤.06\u0000.51⇤–\u0000.48⇤.48⇤\u0000.14\n￿￿￿ \u0000.54⇤\u0000.42⇤\u0000.60⇤\u0000.56⇤\u0000.41⇤.68⇤.04 .85⇤\u0000.47⇤–\u0000.40⇤\u0000.10\n￿￿￿ .09 .17 .17 .16 \u0000.03\u0000.50⇤\u0000.09\u0000.54⇤.50⇤\u0000.40⇤–\u0000.11\n￿￿￿ \u0000.32⇤\u0000.44⇤\u0000.44⇤\u0000.52⇤\u0000.46⇤.00\u0000.32⇤.08\u0000.33⇤.08\u0000.16 –\nTable￿ . Pearson’s correlations on the coef￿ cients from logistic regression (￿￿￿￿) for the Billboard ￿￿￿￿ test set with\nvocabulary V (lower triangle); Spearman’s correlations for the same data (upper triangle). N=￿￿￿songs per cell. Starred\ncorrelations are signi￿cant at ↵=.￿￿￿, controlling for the ￿￿￿. A set of algorithms (viz., ￿￿￿,￿￿￿,￿￿￿, and ￿￿￿) stands\nout for negative correlations with the top performers; in general, these algorithms did not attempt to recognise seventh chords.\nlogous to Friedman’s ￿￿￿￿￿ ) in the upper triangle, and in\nthe lower triangle, as estimated from logistic regression with\n￿￿￿s. Signi￿cant correlations are marked, again controlling\nthe￿￿￿at↵=.￿￿￿. Positive correlations do not necessarily\nimply that the algorithms perform similarly; rather it im-\nplies that they￿ nd the same songs relatively easy or dif￿cult.\nNegative correlations imply that songs that one algorithm\n￿nds dif￿ cult are relatively easy for the other algorithm.\n3.4 Outliers\nTo identify outliers, we considered all evaluations on the\nBillboard ￿￿￿￿ test set and examined the distribution of\nresiduals. Chauvenet’s criterion for outliers in a sample of\nthis size is to lie more than￿. ￿￿standard deviations from the\nmean [￿￿, §￿.￿ ]. Under Friedman’s ￿￿￿￿￿ , Chauvenet’s\ncriterion identi￿ed ￿extreme data points. These are all for\nalgorithm ￿￿￿, a submission with a programming bug that\nerroneously returned alternating C- and B-major chords re-\ngardless of the song, on songs that were so dif￿ cult for most\nother algorithms that the essentially random approach of\nthe bug did better. Under the logistic regression model, the\ncriterion identi￿ed ￿￿extreme points. Here, the unexpected\nbehaviour was primarily for songs that are tuned a quarter-\ntone off from standard tuning (A￿ =￿￿￿Hz). The ground\ntruth necessarily is ‘rounded off’ to standard tuning in one\ndirection or the other, but in cases where an otherwise high-\nperforming algorithm happened to round off in the opposite\ndirection, the performance is markedly low.\n4. DISCUSSION\nWe were surprised to￿ nd that in terms of distinguishing\nbetween algorithms, Friedman’s ￿￿￿￿￿ was in fact more\npowerful than logistic regression, detecting a few extra sig-\nni￿cant pairs. Nonetheless, the two approaches yield sub-\nstantially equivalent broad conclusions: that a group of top\nperformers – ￿￿￿,￿￿￿,￿￿￿,￿￿￿￿￿ , and ￿￿￿￿￿ – are\nstatistically indistinguishable from each other, with ￿￿￿\nalso indistinguishable from the lower end of this group.\nMoreover, having now bene￿ted from years of study, ￿￿￿￿is a reasonably intuitive and well-motivated measure of ￿￿￿\nperformance, and it is awkward to have to work on the Fried-\nman’s rank scale instead, especially since it ultimately ranks\nthe algorithms’ overall performance in a slightly different\norder than the headline ￿￿￿￿ -based results.\nFriedman’s ￿￿￿￿￿ did exhibit less power for our ques-\ntion about interactions between algorithms and differing\nchord vocabularies. Again, ￿￿￿￿ as a unit and as a concept\nis highly meaningful for chord estimation, and there is aconceptual loss from rank transformation. Given the rank\ntransformation, Friedman’s ￿￿￿￿￿ can only be sensitive to\nrecon￿gurations of relative performance as the vocabularies\nbecome more dif￿ cult; logistic regression can also be sens-\nitive to different effect sizes across algorithms even when\ntheir relative ordering remains the same.\nIt was encouraging to see that under either statistical\nmodel, there was a bene￿t to evaluating with multiple vocab-ularies. That encouraged us to examine the inter-correlationsfor the performance of the algorithms. Figure ￿summarises\nthe original correlation matrix in Table ￿more visually by\nusing the correlations from logistic regression as the basis\nof a hierarchical clustering. Two clear groups emerge, both\nfrom the clustering and from minding negative correlations\nin the original matrix: one relatively low-performing group\nincluding ￿￿￿,￿￿￿,￿￿￿, and ￿￿￿, and one relatively high-\nperforming group including all others but for perhaps ￿￿￿,\nwhich does not seem to correlate strongly with any other\nalgorithm. The shape of the equivalent tree based on Spear-\nman’s correlations is similar but for joining ￿￿￿with ￿￿￿\ninstead of the high-performing group. Table ￿uncovers\nthe secret behind the low performers: ￿￿￿ excepted, none\nof the low-performing algorithms attempt to recognise sev-\nenth chords, which comprise ￿￿percent of all chords under\nVocabulary V. Furthermore, we performed an additional\nevaluation of seventh chords only, in the style of [￿￿] and\nusing their software available online.￿From the resulting\nlow score of ￿￿￿, we can deduce that this algorithm is\nable to recognise seventh chords in theory, but that it was\nmost likely trained on the relatively seventh-poor Isophon-\n￿https://github.com/jpauwels/MusOOEvaluator\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n529SB8\nKO1\nPP3\nNG1\nPP4\nKO2\nCB4\nCB3\nNMSD2NMSD1\nCF2\nNG20.0 0.2 0.4 0.6 0.8Pearson's Distance\nFigure￿ . Hierarchical clustering of algorithms based on\n￿￿￿￿ for for the Billboard ￿￿￿￿ test set with vocabulary V ,\nPearson’s distance as derived from the estimated correlation\nmatrix under logistic regression, and complete linkage. The\ngroup of algorithms that is negatively correlated with thetop performers appears at the left. ￿￿￿stands out as the\nmost idiosyncratic performer.\nics corpus (only ￿￿percent of all chords). ￿￿￿ is the same\nalgorithm trained directly on the ￿￿￿￿￿ Billboard training\ncorpus, and with that training, it becomes a top performer.\nOur analysis of outliers again showed Friedman’s ￿￿￿￿￿\nto be less powerful than logistic regression, as one would\nexpect given the range restrictions on rank transformation.\nBut here also the more important advantage of logistic re-\ngression is the ability to work on the ￿￿￿￿ scale. Outliers\nunder the logistic regression model are also points that have\nan unusually strong effect on the reported results. In ouranalysis, they highlight the practical consequences of the\nwell-known problem of atypically-tuned commercial record-\nings. Although we would not propose deleting outliers, it is\nsobering to know that tuning problems may be having an\noutsized effect on our headline evaluation￿ gures. It might\nbe worth considering allowing algorithms their best score\nin keys up to a semitone above or below the ground truth.\nOverall, we have shown that as ￿￿￿ becomes more es-\ntablished and its evaluation more thorough, it is useful to\nuse a subtler statistical model for comparative analysis. We\nrecommend that future ￿￿￿￿￿ ￿￿￿ evaluations use logistic\nregression in preference to Friedman’s ￿￿￿￿￿ . It preserves\nthe natural units and scales of ￿￿￿￿ and segementation\nanalysis, is more powerful for many (although not all) stat-\nistical tests, and when augmented with ￿￿￿s, it allows for\na detailed correlational analysis of which algorithms tend\nto have problems with the same songs as others and which\nhave perhaps genuinely broken innovative ground. This is\nby no means to suggest that Friedman’s test is a bad test in\ngeneral – its near-universal applicability makes it an excel-\nlent choice in many circumstances, including many other￿￿￿￿￿ evaluations – but for ￿￿￿, we believe that the ex-\ntra understanding logistic regression can offer may helpresearchers predict which techniques are most promising\nfor breaking the current performance plateau.\n5. REFERENCES\n[￿]A. Agresti. Categorical Data Analysis. Wiley, New York,\n￿nd edition,￿￿￿￿.[￿]Y. Benjamini and Y. Hochberg. Controlling the falsediscovery rate: A practical and powerful approach tomultiple testing. J. Roy. Stat. Soc. B,￿(￿￿):￿￿￿–￿￿￿,\n￿￿￿￿.\n[￿]J. A. Burgoyne. Stochastic Processes and Database-\nDriven Musicology. PhD thesis, McGill U., Montréal,\nQC,￿￿￿￿.\n[￿]J. A. Burgoyne, J. Wild, and I. Fujinaga. An expert\nground-truth set for audio chord recognition and music\nanalysis. In Proc. Int. Soc. Music Inf. Retr., pages￿￿￿–\n￿￿, Miami, FL,￿￿￿￿.\n[￿]S. Ferrari and F. Cribari-Neto. Beta regression for model-ling rates and proportions. J. Appl. Stat. ,￿￿(￿):￿￿￿–￿￿￿,\n￿￿￿￿.\n[￿]W. B. de Haas, J. P. Magalhães, D. ten Heggeler, G. Bek-\nenkamp, and T. Ruizendaal. Chordify: Chord transcrip-\ntion for the masses. Demo at the Int. Soc. Music Inf.\nRetr. Conf., Curitiba, Brazil,￿￿￿￿.\n[￿]W. B. de Haas, J. P. Magalhães, R. C. Veltkamp, and\nF. Wiering. Harmtrace: Improving harmonic similarity\nestimation using functional harmony analysis. In Proc.\nInt. Soc. Music Inf. Retr., pages￿￿–￿￿ , Miami, FL,￿￿￿￿.\n[￿]C. Harte. Towards Automatic Extraction of Harmony\nInformation from Music Signals. PhD thesis, Queen\nMary, U. London,￿￿￿￿.\n[￿]V . E. Johnson. Revised standards for statistical evidence.\nP . Nat’l Acad. Sci. USA,￿￿￿(￿￿):￿￿￿￿￿–￿￿ ,￿￿￿￿.\n[￿￿] M. Khadkevich and M. Omologo. Large-scale cover\nsong identi￿cation using chord pro￿les. In Proc. Int. Soc.\nMusic Inf. Retr. Conf., pages￿￿￿–￿￿ , Curitiba, Brazil,\n￿￿￿￿.\n[￿￿] M. H. Kutner, C. J. Nachtsheim, J. Neter, and W. Li. Ap-\nplied Linear Statistical Models. McGraw-Hill, Boston,\nMA,￿ th edition,￿￿￿￿.\n[￿￿] M. Mauch. Automatic Chord Transcription from Audio\nUsing Computational Models of Musical Context. PhD\nthesis, Queen Mary, U. London,￿￿￿￿.\n[￿￿] M. Mauch, S. Dixon, C. Harte, M. Casey, and B. Fields.\nDiscovering chord idioms through Beatles and RealBook songs. In Proc. Int. Soc. Music Inf. Retr. Conf.,\npages￿￿￿–￿￿ , Vienna, Austria,￿￿￿￿.\n[￿￿] P. McCullagh and J. A. Nelder. Generalized Linear Mod-\nels. Chapman & Hall/CRC, Boca Raton, FL,￿ nd edition,\n￿￿￿￿.\n[￿￿] J. Pauwels and G. Peeters. Evaluating automaticallyestimated chord sequences. In Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process., pages￿￿￿–￿￿ , Van-\ncouver, British Columbia,￿￿￿￿.\n[￿￿] J. R. Taylor. An Introduction to Error Analysis: The\nStudy of Uncertainties in Physical Measurements. Uni-\nversity Science Books, Sausalito, CA,￿ nd edition,￿￿￿￿.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n530"
    },
    {
        "title": "Theoretical Framework of A Computational Model of Auditory Memory for Music Emotion Recognition.",
        "author": [
            "Marcelo F. Caetano",
            "Frans Wiering"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417211",
        "url": "https://doi.org/10.5281/zenodo.1417211",
        "ee": "https://zenodo.org/records/1417211/files/CaetanoW14.pdf",
        "abstract": "The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. How- ever, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incor- porates temporal information into MER systems. We ad- vocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER re- searchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions.",
        "zenodo_id": 1417211,
        "dblp_key": "conf/ismir/CaetanoW14",
        "keywords": [
            "semantic gap",
            "glass ceiling",
            "alternative proposals",
            "auditory memory",
            "temporal information",
            "MER systems",
            "musical meaning",
            "musical emotions",
            "improved systems",
            "complexity"
        ],
        "content": "THEORETICAL FRAMEWORK OF A COMPUTATIONAL MODEL OF\nAUDITORY MEMORY FOR MUSIC EMOTION RECOGNITION\nMarcelo Caetano\nSound and Music Computing Group\nINESC TEC, Porto, Portugal\nmcaetano@inesctec.ptFrans Wiering\nDep. Information and Computing Sciences\nUtrecht University, The Netherlands\nf.wiering@uu.nl\nABSTRACT\nThe bag of frames (BOF) approach commonly used in\nmusic emotion recognition (MER) has several limitations.\nThesemantic gap is believed to be responsible for the glass\nceiling on the performance of BOF MER systems. How-\never, there are hardly any alternative proposals to address\nit. In this article, we introduce the theoretical framework\nof a computational model of auditory memory that incor-\nporates temporal information into MER systems. We ad-\nvocate that the organization of auditory memory places\ntime at the core of the link between musical meaning and\nmusical emotions. The main goal is to motivate MER re-\nsearchers to develop an improved class of systems capable\nof overcoming the limitations of the BOF approach and\ncoping with the inherent complexity of musical emotions.\n1. INTRODUCTION\nIn the literature, the aim of music emotion recognition\n(MER) is commonly said to be the development of systems\nto automatically estimate listeners’ emotional response to\nmusic [2, 7, 8, 11, 18, 19, 33] or simply to organize or clas-\nsify music in terms of emotional content [14, 17]. Appli-\ncations of MER range from managing music libraries and\nmusic recommendation systems to movies, musicals, ad-\nvertising, games, and even music therapy, music educa-\ntion, and music composition [11]. Possibly inspired by au-\ntomatic music genre classiﬁcation [28, 29], a typical ap-\nproach to MER categorizes emotions into a number of\nclasses and applies machine learning techniques to train\na classiﬁer and compare the results against human annota-\ntions, considered the “ground truth” [14, 19, 28, 32]. Kim\net. al [14] presented a thorough state-of-the-art review, ex-\nploring a wide range of research in MER systems, focusing\nparticularly on methods that use textual information (e.g.,\nwebsites, tags, and lyrics) and content-based approaches,\nas well as systems combining multiple feature domains\n(e.g., features plus text). Commonly, music features are\nc\rMarcelo Caetano, Frans Wiering.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Marcelo Caetano, Frans Wiering.\n“Theoretical Framework of a Computational Model of Auditory Memory\nfor Music Emotion Recognition”, 15th International Society for Music\nInformation Retrieval Conference, 2014.estimated from the audio and used to represent the mu-\nsic. These features are calculated independently from each\nother and from their temporal progression, resulting in the\nbag of frames (BOF) [11, 14] paradigm.\nThe ‘Audio Mood Classiﬁcation’ (AMC) task in\nMIREX [5, 10] epitomizes the BOF approach to MER,\npresenting systems whose performance range from 25 %\nto 70 % (see Table 1). Present efforts in MER typically\nconcentrate on the machine learning algorithm that per-\nforms the map in an attempt to break the ‘glass ceiling’ [1]\nthought to limit system performance. The perceived mu-\nsical information that does not seem to be contained in\nthe audio even though listeners agree about its existence,\ncalled ‘semantic gap’ [3, 31], is considered to be the cause\nof the ‘glass ceiling.’ However, the current approach to\nMER has been the subject of criticism [2, 11, 28, 31].\nKnowledge about music cognition, music psychology,\nand musicology is seldom explored in MER. It is widely\nknown that musical experience involves more than mere\nprocessing of music features. Music happens essentially in\nthe brain [31], so we need to take the cognitive mechanisms\ninvolved in processing musical information into account if\nwe want to be able to model people’s emotional response\nto music. Among the cognitive processes involved in lis-\ntening to music, memory plays a major role [27]. Music\nis intrinsically temporal, and time is experienced through\nmemory. Studies [12,16,25] suggest that the temporal evo-\nlution of the musical features is intrinsically linked to lis-\nteners’ emotional response to music.\nIn this article, we speculate that the so called ‘semantic\ngap’ [3] is a mere reﬂection of how the BOF approach mis-\nrepresents both the listener and musical experience. Our\ngoal is not to review MER, but rather emphasize the lim-\nitations of the BOF approach and propose an alternative\nmodel that relies on the organization of auditory memory\nto exploit temporal information from the succession of mu-\nsical sounds. For example, BOF MER systems typically\nencode temporal information in delta and delta-delta co-\nefﬁcients [1], capturing only local instantaneous temporal\nvariations of the feature values. In a previous work [2],\nwe discussed different MER systems that exploit tempo-\nral information differently. Here, we take a step further\nand propose the theoretical framework of a computational\nmodel of auditory memory for MER . Our aim is to moti-\nvate MER research to bridge the ‘semantic gap’ and break\nthe so called ‘glass ceiling’ [1, 3, 31].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n331The next section discusses the complexity of musical\nemotions and how this relates to the glass ceiling prevent-\ning BOF MER systems to improve their performance as\na motivation for proposing a paradigm change. Then, we\nbrieﬂy introduce the model of auditory memory adopted,\nfollowed by the proposed framework and considerations\nabout its implementation. Finally, we present the conclu-\nsions and discuss future directions of this theoretical work.\n2. MACHINE LEARNING AND MER\nIt is generally agreed that music conveys and evokes emo-\ntions [9, 13]. In other words, listeners might feel happy\nlistening to a piece or simply perceive it as happy [9].\nResearch on music and emotions usually investigates the\nmusical factors involved in the process as well as listen-\ners’ response to music. There are many unanswered ques-\ntions [13, 21], such as “which emotions does music ex-\npress?”, “in what context do musical emotions occur?”,\n“how does music express emotions?”, and “which factors\nin music are expressive of emotions?” Researchers need to\naddress controversial issues to investigate these questions.\nOn the one hand, the relevant musical factors, and on the\nother hand, the deﬁnition andmeasurement of emotion.\nThere is evidence [13] of emotional reactions to mu-\nsic in terms of various subcomponents, such as subjective\nfeeling, psychophysiology, brain activation, emotional ex-\npression, action tendency, emotion regulation and these,\nin turn, feature different psychological mechanisms like\nbrain stem reﬂexes, evaluative conditioning, emotional\ncontagion, visual imagery, episodic memory, rhythmic en-\ntrainment, andmusical expectancy. Each mechanism is re-\nsponsive to its own combination of information in the mu-\nsic, the listener, and the situation. Among the causal fac-\ntors that potentially affect listeners’ emotional response to\nmusic are personal, situational, andmusical [21]. Personal\nfactors include age, gender, personality, musical training,\nmusic preference, and current mood; situational factors can\nbe physical such as acoustic and visual conditions, time\nand place, or social such as type of audience, and occa-\nsion. Musical factors include genre, style, key, tuning, or-\nchestration, among many others.\nMost modern emotion theorists suggest that an emotion\nepisode consists of coordinated changes in three major re-\naction components: physiological arousal, motor expres-\nsion, and subjective feeling (the emotion triad). Accord-\ning to this componential approach to emotion, we would\nneed to measure physiological changes, facial and vocal\nexpression, as well as gestures and posture along with self\nreported feelings using a rich emotion vocabulary to esti-\nmate the listeners’ emotional response. In MER, the emo-\ntional response to music is commonly collected as self-\nreported annotations for each music track, capturing “sub-\njective feelings” associated or experienced by the listener.\nSome researchers [9] speculate that musical sounds can ef-\nfectively cause emotional reactions (via brain stem reﬂex,\nfor example), suggesting that certain music dimensions\nand qualities communicate similar affective experiences to\nmany listeners. The literature on the emotional effects ofmusic [9,13] has accumulated evidence that listeners often\nagree about the emotions expressed (or elicited) by a par-\nticular piece, suggesting that there are aspects in music that\ncan be associated with similar emotional responses across\ncultures, personal bias or preferences.\nIt is probably impractical to hope to develop a MER sys-\ntem that could account for all facets of this complex prob-\nlem. There is no universally accepted model or explanation\nfor the relationship between music and emotions. How-\never, we point out that it is widely known and accepted that\nMER systems oversimplify the problem when adopting the\nBOF approach [11]. In this context, we propose a theoreti-\ncal framework that uses the organization of auditory mem-\nory to incorporate temporal information into MER. We ar-\ngue that time lies at the core of the complex relationship\nbetween music and emotions and that auditory memory\nmediates the processes involved. In what follows, we fo-\ncus on the link between musical sounds and self-reported\nsubjective feelings associated to them through music lis-\ntening. In other words, the association between the audio\nfeatures and perceived emotions.\n2.1 The Glass Ceiling on System Performance\nThe performance of music information retrieval (MIR) sys-\ntems hasn’t improved satisfactorily over the years [1, 10]\ndue to several shortcomings. Aucouturier and Pachet [1]\nused the term ‘glass ceiling’ to suggest that there is a lim-\nitation on system performance at about 65% R-precision\nwhen using BOF and machine learning in music similar-\nity. Similarly, Huq et. al [11] examined the limitations of\nthe BOF approach to MER. They present the results of a\nsystematic study trying to maximize the prediction perfor-\nmance of an automated MER system using machine learn-\ning. They report that none of the variations they considered\nleads to a substantial improvement in performance, which\nthey interpret as a limit on what is achievable with machine\nlearning and BOF.\nMIREX [10] started in 2005 with the goal of systemati-\ncally evaluating state-of-the-art MIR algorithms, promot-\ning the development of the ﬁeld, and increasing system\nperformance by competition and (possibly) cooperation.\nMIREX included an “Audio Mood Classiﬁcation” (AMC)\ntask for the ﬁrst time in 2007 ‘inspired by the growing\ninterest in classifying music by moods, and the difﬁculty\nin the evaluation of music mood classiﬁcation caused by\nthe subjective nature of mood’ [10]. MIREX’s AMC task\nuses a categorical representation of emotions divided in\nﬁve classes. These ﬁve ‘mood clusters’ were obtained by\nanalyzing ‘mood labels’ (user tags) for popular music from\nthe All Music Guide1.\nThe MIREX wiki2presents the “Raw Classiﬁcation\nAccuracy Averaged Over Three Train/Test Folds” per sys-\ntem. Table 1 summarizes system performance over the\nyears for the MIREX task AMC, showing the minimum,\nmaximum, average, and standard deviation of these val-\nues across systems. Minimum performance has steadily\n1All Music Guide http://www.allmusic.com/\n2http://www.music-ir.org/mirex/wiki/MIREX_HOME\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n332Table 1: MIREX AMC performance from 2007 to 2013.\nMinimum Maximum Average STD\n2007 25.67% 61.50% 52.65% 11.19%\n2008 30.33% 63.67% 52.39% 7.72%\n2009 34.67% 65.67% 57.67% 6.17%\n2010 36.12% 63.78% 56.48% 6.36%\n2011 39.81% 69.48% 57.98% 9.33%\n2012 46.14% 67.80% 62.67% 6.17%\n2013 28.83% 67.83% 59.81% 10.29%\nimproved, but maximum performance presents a less sig-\nniﬁcant improvement. The standard deviation of perfor-\nmance across systems has a general trend towards decreas-\ning (suggesting more homogeneity over the years). Most\nalgorithms are also tested in different classiﬁcation tasks\n(musical genre, for example), and the best in one task\nare often also very good at other tasks, maybe indicating\nthere is more machine learning than musical knowledge\ninvolved. Sturm [28] discusses the validity of the current\nevaluation in MER. He argues that the current paradigm of\nclassifying music according to emotions only allows us to\nconclude how well an MER system can reproduce “ground\ntruth” labels of the test data, irrespective of whether these\nMER systems use factors irrelevant to emotion in music.\n2.2 Bridging the Semantic Gap\nIn MIR, audio processing manipulates signals generated\nby musical performance, whereas music is an abstract and\nintangible cultural construct. The sounds per se do not\ncontain the essence of music because music exists in the\nmind of the listener. The very notion of a ‘semantic gap’\nis misleading [31]. The current BOF approach to MER\nviews music simply as data (audio signals) and therefore\nmisrepresents musical experience. Machine learning per-\nforms a rigid map from “music features” to “emotional la-\nbels”, as illustrated in part a) of Fig. 1, treating music as a\nstimulus that causes a speciﬁc emotional response irrespec-\ntive of personal and contextual factors which are known to\naffect listeners’ emotional response [12, 16, 25] such as\nlisteners’ previous exposure and the impact of the unfold-\ning musical process. Memory is particularly important in\nthe recognition of patterns that are either stored in long-\nterm memory (LTM) from previous pieces or in short-term\nmemory (STM) from the present piece. Music seems to be\none of the most powerful cues to bring emotional experi-\nences from memory back into awareness.\nWiggins [31] suggests to look at the literature from mu-\nsicology and psychology to study the cognitive mecha-\nnisms involved in human music perception as the starting\npoint of MIR research, particularly musical memory, for\nthey deﬁne music. He argues that music is not just pro-\ncessed by the listeners, it is deﬁned by them. Wiggins\nstates that “music is a cognitive model”, therefore, only\ncognitive models are likely to succeed in processing music\nin a human-like way. He writes that “to treat music in a\nway which is not human-like is meaningless, because mu-\nsic is deﬁned by humans. Finally, he concludes that the\nFigure 1: Approaches to MER. Part a) illustrates the BOF\napproach, which uses machine learning to map music fea-\ntures to a region of a model of emotion. Part b) illustrates\nthe proposed approach, which relies on the organization of\nauditory memory to estimate musical emotions as a form\nof musical meaning emerging from musical structure.\nhuman response to memory is key to understanding the\npsychophysiological effect of musical stimuli, and that this\ndomain is often missing altogether from MIR research. In\nthis work, we view perceived musical emotions as a par-\nticular form of musical meaning [12, 16, 25], which is in-\ntimately related to musical structure by the organization of\nauditory memory [27], as represented in part b) of Fig. 1.\n3. AUDITORY MEMORY AND MER\nConceptually, memory can be divided into three processes\n[27]: sensory memory (echoic memory and early process-\ning); short-term memory (or working memory); and long-\nterm memory. Each of these memory processes functions\non a different time scale, which can be loosely related to\nlevels of musical experience, the “level of event fusion”,\nthe “melodic and rhythmic level”, and the “formal level”,\nrespectively. Echoic memory corresponds to early process-\ning, when the inner ear converts sounds into trains of nerve\nimpulses that represent the frequency and amplitude of in-\ndividual acoustic vibrations. During feature extraction, in-\ndividual acoustic features (e.g., pitch, overtone structure)\nare extracted and then bound together into auditory events.\nThe events then trigger those parts of long-term memory\n(LTM) activated by similar events in the past, establishing\na context that takes the form of expectations, or memory of\nthe recent past. Long-term memories that are a part of this\nongoing context can persist as current “short-term mem-\nory” (STM). Short-term memories disappear from con-\nsciousness unless they are brought back into the focus of\nawareness repeatedly (e.g. by means of the rehearsal loop).\nWhen the information is particularly striking or novel, it\nmay be passed back to LTM and cause modiﬁcations of\nsimilar memories already established, otherwise it is lost.\nThe three types of processing deﬁne three basic time\nscales on which musical events and patterns take place,\nwhich, in turn, affect our emotional response to music.\nThe event fusion level of experience (echoic memory) is\nassociated with pitch perception. The main characteristic\nof the melodic and rhythmic level is that separate events\non this time scale are grouped together in the present as\nmelodic grouping and rhythmic grouping, associated with\nSTM. Units on the formal level of musical experience con-\nsist of entire sections of music and are associated with\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n333System Input System OutputFigure 2: The proposed framework for MER. The blocks\nare system components and the arrows indicate the ﬂow of\ninformation. In the shaded area is pattern recognition, and\noutside are the proposed processes, namely, the “unfolding\nmusical process” and the listener’s “previous exposure”.\nThe ﬁgure also illustrates how the organization of auditory\nmemory is related to system blocks.\nLTM and our previous musical exposure. Echoic mem-\nory and early processing provide our immediate experi-\nence of the present moment of music in the focus of con-\nscious awareness, and help to segment it into manageable\nunits; STM establishes the continuity and discontinuity of\nthat movement with the immediate past; and LTM provides\nthe context that gives it meaning, by relating the moment\nto a larger framework of ongoing experience and previous\nknowledge. The organization of memory and the limits of\nour ability to remember have a profound effect on how we\nperceive patterns of events and boundaries in time. Time is\na key element in memory processes and should be brought\nto the foreground of MER [2].\n4. THE PROPOSED FRAMEWORK\nFig. 2 shows the framework we propose to incorporate\nmemory processes in MER systems to illustrate how au-\nditory memory affects musical experience. The blocks as-\nsociated with the system have a white background, while\nmemory processes have a dark background. The arrows\nrepresent the ﬂow of information, while the dashed line\nrepresents the relationship between memory processes and\nsystem blocks. The proposed framework can be interpreted\nas an extension of the traditional approach (shaded area) by\nincluding two blocks, previous exposure andunfolding mu-\nsic process. In the BOF approach, the music features are\nassociated with echoic memory, related to very short tem-\nporal scales and uncorrelated with the past or predictions\nof future events. The framework we propose includes the\n“Unfolding Musical Process” and “Previous Exposure” to\naccount for LTM and STM. The “Unfolding Musical Pro-\ncess” represents the listeners’ perception of time (related\nto musical context and expectations), while “Previous Ex-\nposure” represents the personal and cultural factors that\nmakes listeners unique.4.1 Unfolding Musical Process\nThe unfolding music process uses temporal information\nfrom the current music stream to account for repeti-\ntions and expectations. As Fig. 2 suggests, the unfold-\ning musical process acts as feedback loop that affects\nthe map between the music features and the listener re-\nsponse. The dynamic aspect of musical emotion relates\nto the cognition of musical structure [12, 16, 25]. Mu-\nsical emotions change over time in intensity and qual-\nity, and these emotional changes covary with changes in\npsycho-physiological measures [16, 25]. The human cog-\nnitive system regulates our expectations to make predic-\ntions [12]. Music (among other stimuli) inﬂuences this\nprinciple, modulating our emotions. As the music unfolds,\nthe model is used to generate expectations, which are im-\nplicated in the experience of listening to music. Musical\nmeaning and emotion depend on how the actual events in\nthe music play against this background of expectations.\n4.2 Previous Exposure\nThe framework in Fig. 2 illustrates that previous exposure\naccounts for musical events stored in LTM that affect the\nlisteners’ emotional response to music. Musical emotions\nmay change according to musical genre [6], cultural back-\nground, musical training and exposure, mood, physiolog-\nical state, personal disposition and taste [9, 12]. This in-\nformation is user speciﬁc and depends on context thus it\ncannot be retrieved from the current music stream, rather,\nit has to be supplied by the listener.\n5. IMPLEMENTATION ISSUES\nHere we address how to treat individual components of the\nmodel, which parts need human input and which are auto-\nmatic, and how the different system components communi-\ncate and what information they share. The proposed frame-\nwork urges for a paradigm change in MER research rather\nthan simply a different kind of MER systems, including\nrepresenting the music stream, collecting time-stamped an-\nnotations, and system validation and evaluation [28]. Thus\nwe propose a class of dynamic MER systems that contin-\nuously estimate how the listener’s perceived emotions un-\nfold in time from a time-varying input stream of audio fea-\ntures calculated from different musically related temporal\nlevels.\n5.1 Music Stream as System Input\nThe proposed system input is a music stream unfolding\nin time rather than a static (BOF) representation. To in-\ncorporate time into MER, the system should monitor the\ntemporal evolution of the music features [25] at different\ntime scales, the “level of event fusion”, the “melodic and\nrhythmic level”, and the “formal level”. The feature vec-\ntor should be calculated for every frame of the audio sig-\nnal and kept as a time series (i.e., a time-varying vector of\nfeatures). Time-series analysis techniques such as linear\nprediction and correlations (among many others) might be\nused to extract trends and model information at later stages.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3345.2 Music Features\nEerola [6, 7] proposes to select musically relevant fea-\ntures that have been shown to relate to musical emotions.\nHe presents a list of candidate features for a computa-\ntional model of emotions that can be automatically esti-\nmated from the audio and that would allow meaningful\nannotations of the music, dividing the features into musi-\ncally relevant levels related to three temporal scales. Sny-\nder [27] describes three different temporal scales for mu-\nsical events based on the limits of human perception and\nauditory memory. Coutinho et. al [4] sustain that the\nstructure of affect elicited by music is largely dependent\non dynamic temporal patterns in low-level music structural\nparameters. In their experiments, a signiﬁcant part of the\nlisteners’ reported emotions can be predicted from a set of\nsix psychoacoustic features, namely, loudness, pitch level,\npitch contour, tempo, texture, and sharpness. Schubert [26]\nused loudness, tempo, melodic contour, texture, and spec-\ntral centroid as predictors in linear regression models of\nvalence and arousal.\nFig. 1 suggests that MER systems should use the mu-\nsical structure to estimate musical meaning such as emo-\ntions. Musical structure emerges from temporal patterns\nof music features. In other words, MER systems should\ninclude information about the rate of temporal change of\nmusic features, such as how changes in loudness correlate\nwith the expression of emotions rather than loudness val-\nues only. These loudness variations, in turn, form patterns\nof repetition on a larger temporal scale related to the struc-\nture of the piece that should also be exploited. Thus the\nfeatures should be hierarchically organized in a musically\nmeaningful way according to auditory memory [27].\n5.3 Listener Response and System Output\nRecently, some authors started investigating how the emo-\ntional response evolves in time as the music unfolds.\nKrumhansl [16] proposes to collect listener’s responses\ncontinuously while the music is played, recognizing that\nretrospective judgements are not sensitive to unfolding\nprocesses. Recording listener’s emotional ratings over\ntime as time-stamped annotations requires listeners to\nwrite down the emotional label and a time stamp as the mu-\nsic unfolds, a task that has received attention [20]. Emo-\ntions are dynamic and have distinctive temporal proﬁles\nthat are not captured by traditional models (boredom is\nvery different from astonishment in this respect, for exam-\nple). In this case, the temporal proﬁles would be matched\nagainst prototypes stored in memory. Some musical web-\nsites allow listeners to ‘tag’ speciﬁc points of the waveform\n(for instance, SoundCloud3), a valuable source of tempo-\nral annotations for popular music.\n5.4 Unfolding Musical Process\nTheunfolding musical process acts as feedback loop that\nexploits the temporal evolution of music features at the\nthree different time scales. The temporal correlation of\n3http://soundcloud.com/each feature must be exploited and fed back to the map-\nping mechanism (see ‘unfolding musical process’) to esti-\nmate listeners’ response to the repetitions and the degree\nof “surprise” that certain elements might have [26]. Schu-\nbert [25] studied the relationship between music features\nand perceived emotion using continuous response method-\nology and time-series analysis. Recently, MER systems\nstarted tracking temporal changes [4,22–24,30]. However,\nmodeling the unfolding musical process describes how the\ntime-varying emotional trajectory varies as a function of\nmusic features. Korhonen et al. [15] use auto-regressive\nmodels to predict current musical emotions from present\nand past feature values, including information about the\nrate of change or dynamics of the features.\n5.5 Previous Exposure\nPrevious exposure is responsible for system customization\nand could use reinforcement learning to alter system re-\nsponse to the unfolding musical process. Here, the user\ninput tunes the long-term system behavior according to ex-\nternal factors (independent from temporal evolution of fea-\ntures) such as context, mood, genre, cultural background,\netc. Eerola [6] investigated the inﬂuence of musical genre\non emotional expression and reported that there is a set\nof music features that seem to be independent of musical\ngenre. Yang et al. [33] studied the role of individuality in\nMER by evaluating the prediction accuracy of group-wise\nandpersonalized MER systems by simply using annota-\ntions from a single user as “ground truth” to train the MER\nsystem.\n6. CONCLUSIONS\nResearch on music emotion recognition (MER) commonly\nrelies on the bag of frames (BOF) approach, which uses\nmachine learning to train a system to map music features to\na region of the emotion space. In this article, we discussed\nwhy the BOF approach misrepresents musical experience,\nunderplays the role of memory in listeners’ emotional re-\nsponse to music, and neglects the temporal nature of mu-\nsic. The organization of auditory memory plays a major\nrole in the experience of listening to music. We proposed\na framework that uses the organization of auditory mem-\nory to bring time to the foreground of MER. We prompted\nMER researchers to represent music as a time-varying vec-\ntor of features and to investigate how the emotions evolve\nin time as the music develops, representing the listener’s\nemotional response as an emotional trajectory. Finally, we\ndiscussed how to exploit the unfolding music process and\nprevious exposure to incorporate the current musical con-\ntext and personal factors into MER systems.\nThe incorporation of time might not be enough to ac-\ncount for the subjective nature of musical emotions. Cul-\nture, individual differences and the present state of the lis-\ntener are factors in understanding aesthetic responses to\nmusic. Thus a probabilistic or fuzzy approach could also\nrepresent a signiﬁcant step forward in understanding aes-\nthetic responses to music. We prompt MER researchers to\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n335adopt a paradigm change to cope with the complexity of\nhuman emotions in one of its canonical means of expres-\nsion, music.\n7. ACKNOWLEDGEMENTS\nThis work was partially supported by the Media Arts and\nTechnologies project (MAT), NORTE-07-0124-FEDER-\n000061, which is ﬁnanced by the North Portugal Regional\nOperational Programme (ON.2 O Novo Norte), under the\nNational Strategic Reference Framework (NSRF), through\nthe European Regional Development Fund (ERDF), and\nby national funds, through the Portuguese funding agency,\nFundao para a Ci ˆencia e a Tecnologia (FCT). Frans Wier-\ning is supported by the FES project COMMIT/.\n8. REFERENCES\n[1] J.J. Aucouturier, F. Pachet: “Improving timbre similarity:\nHow high is the sky?” Journ. Neg. Res. Speech Audio Sci.,\nV ol. 1, No. 1, 2004.\n[2] M. Caetano, A. Mouchtaris, F. Wiering: The role of time in\nmusic emotion recognition: Modeling musical emotions from\ntime-varying music features, LNCS, Springer-Verlag, 2013.\n[3] O. Celma, X. Serra: “FOAFing the music: Bridging the se-\nmantic gap in music recommendation,” Journ. Web Seman-\nticsV ol. 6, No. 4, 2008.\n[4] E. Coutinho, A. Cangelosi: “Musical emotions: Predicting\nsecond-by-second subjective feelings of emotion from low-\nlevel psychoacoustic features and physiological measure-\nments,” Emotion V ol. 11, No. 4, pp. 921–937, 2011. 2004.\n[5] S. Cunningham, D. Bainbridge, J. Downie: “The impact of\nMIREX on scholarly research (2005-2010),” Proc. ISMIR,\n2012.\n[6] T. Eerola: “Are the emotions expressed in music genre-\nspeciﬁc? An audio-based evaluation of datasets spanning\nclassical, ﬁlm, pop and mixed genres,” Journ. New Mus. Res.,\nV ol. 40, No. 4, pp. 349–366, 2011.\n[7] T. Eerola: “Modeling listeners’ emotional response to mu-\nsic,” Topics Cog. Sci., V ol. 4, No. 4, pp. 1–18, 2012.\n[8] A. Friberg: “Digital audio emotions - An overview of com-\nputer analysis and synthesis of emotional expression in mu-\nsic,” Proc. DAFx, 2008.\n[9] A. Gabrielsson, E. Lindstr ¨om. The role of structure in the mu-\nsical expression of emotions. Handbook of Music and Emo-\ntion, Oxford University Press, 2011.\n[10] X. Hu, J. Downie, C. Laurier, M. Bay, and A. Ehmann:\n“The 2007 MIREX audio mood classiﬁcation task: Lessons\nlearned,” Proc. ISMIR, 2008.\n[11] A. Huq, J. Bello, R. Rowe: “Automated music emotion\nrecognition: A systematic evaluation,” Journ. New Mus. Res.,\nV ol. 39, No. 3, pp. 227–244, 2010.\n[12] D. Huron: Sweet Anticipation: Music and the Psychology of\nExpectation, Bradford Books, MIT Press, 2008.\n[13] P. Juslin, S. Liljestr ¨om, D. V ¨astfj¨all, L. Lundqvist: How does\nmusic evoke emotions? Exploring the underlying mecha-\nnisms. In: Handbook of Music and Emotion, Oxford Uni-\nversity Press, 2011.[14] Y . Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson,\nJ. Scott, J. Speck, D. Turnbull: “Music emotion recognition:\nA state of the art review,” Proc. ISMIR, 2010.\n[15] M. Korhonen, D. Clausi, M. Jernigan: “Modeling Emotional\nContent of Music Using System Identiﬁcation,” IEEE Trans.\nSyst., Man, Cybern., V ol. 36, No. 3, pp. 588–599, 2005.\n[16] C. Krumhansl: “Music: A Link Between Cognition and\nEmotion,” Current Direct. Psychol. Sci., V ol. 11, No. 2,\npp. 45–50, 2002.\n[17] C. Laurier, M. Sordo, J. Serr `a, P. Herrera: “Music mood rep-\nresentations from social tags,” Proc. ISMIR, 2009.\n[18] L. Lu, D. Liu, H. Zhang: “Automatic mood detection and\ntracking of music audio signals,” IEEE Trans. Audio, Speech,\nLang. Proc., V ol. 14, No. 1, pp. 5–18, 2006.\n[19] K. MacDorman, S. Ough, H. Chang: “Automatic emotion\nprediction of song excerpts: Index construction, algorithm\ndesign, and empirical comparison,” Journ. New Mus. Res.,\nV ol. 36, No. 4, pp. 281–299, 2007.\n[20] F. Nagel, R. Kopiez, O. Grewe, E. Altenm ¨uller: “EMuJoy:\nSoftware for continuous measurement of perceived emotions\nin music,” Behavior Res. Meth., V ol. 39, No. 2, pp. 283–290,\n2007.\n[21] K. Scherer: “Which emotions can be induced by music? what\nare the underlying mechanisms? and how can we measure\nthem?” Journ. New Mus. Res., V ol. 33, No. 3, pp. 239–251,\n2004.\n[22] E. Schmidt, Y . Kim: “Modeling musical emotion dynamics\nwith conditional random ﬁelds,” Proc. ISMIR, 2011.\n[23] E. Schmidt, Y . Kim: “Prediction of time-varying musical\nmood distributions from audio,” Proc. ISMIR, 2010.\n[24] E. Schmidt, Y . Kim: “Prediction of time-varying musical\nmood distributions using kalman ﬁltering,” Proc. ICMLA,\n2010.\n[25] E. Schubert: “Modeling perceived emotion with continuous\nmusical features,” Music Percep.: An Interdiscipl. Journ.,\nV ol. 21, No. 4, pp. 561–585, 2004.\n[26] E. Schubert: “Analysis of emotional dimensions in music\nusing time series techniques,” Context: Journ. Mus. Res.,\nV ol. 31, pp. 65–80, 2006.\n[27] B. Snyder: Music and Memory: An Introduction., MIT Press,\n2001.\n[28] B. Sturm: “Evaluating music emotion recognition: Lessons\nfrom music genre recognition?,” Proc. IEEE ICMEW, 2013.\n[29] G. Tzanetakis, P. Cook: “Musical Genre Classiﬁcation of\nAudio Signals,” IEEE Trans. Speech, Audio Proc., V ol. 10,\nNo. 5, pp. 293–302, 2002.\n[30] Y . Vaizman, R. Granot, G. Lanckriet: “Modeling dynamic\npatterns for emotional content in music,” Proc. ISMIR, 2011.\n[31] G. Wiggins: “Semantic gap?? Schemantic schmap!!\nMethodological considerations in the scientiﬁc study of mu-\nsic,” Proc. Int. Symp. Mult., 2009.\n[32] Y . Yang, H. Chen: “Ranking-based emotion recognition\nfor music organization and retrieval,” IEEE Trans. Audio,\nSpeech, Lang. Proc., V ol. 19, No. 4, pp. 762–774, 2011.\n[33] Y . Yang, Y . Su, Y . Lin, H. Chen: “Music emotion recognition:\nthe role of individuality,” Proc. HCM, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n336"
    },
    {
        "title": "Developing Tonal Perception through Unsupervised Learning.",
        "author": [
            "Carlos Eduardo Cancino Chacón",
            "Stefan Lattner",
            "Maarten Grachten"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416058",
        "url": "https://doi.org/10.5281/zenodo.1416058",
        "ee": "https://zenodo.org/records/1416058/files/ChaconLG14.pdf",
        "abstract": "The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in en- culturation, the latter accounting for cross-cultural differ- ences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how mu- sical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochas- tic neural network. We show that even based on a lim- ited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organiza- tion of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analy- sis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the ex- traction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone exper- iments by Krumhansl and Shepard, measuring the organi- zation of tones within a key in human perception. We find that the responses of the RBM share qualitative character- istics with those of both trained and untrained listeners.",
        "zenodo_id": 1416058,
        "dblp_key": "conf/ismir/ChaconLG14",
        "keywords": [
            "tonal structure",
            "perception",
            "low-level perceptual mechanisms",
            "cultural differences",
            "unsupervised machine learning",
            "Restricted Boltzmann Machine",
            "generative stochastic neural network",
            "musical concepts",
            "learning from musical data",
            "circle of fifths"
        ],
        "content": "DEVELOPING TONAL PERCEPTION THROUGH UNSUPERVISED\nLEARNING\nCarlos Eduardo Cancino Chac ´on, Stefan Lattner, Maarten Grachten\nAustrian Research Institute for Artiﬁcial Intelligence\nfcarlos.cancino,stefan.lattner,maarten.grachteng@ofai.at\nABSTRACT\nThe perception of tonal structure in music seems to be\nrooted both in low-level perceptual mechanisms and in en-\nculturation, the latter accounting for cross-cultural differ-\nences in perceived tonal structure. Unsupervised machine\nlearning methods are a powerful tool for studying how mu-\nsical concepts may emerge from exposure to music. In\nthis paper, we investigate to what degree tonal structure\ncan be learned from musical data by unsupervised training\nof a Restricted Boltzmann Machine, a generative stochas-\ntic neural network. We show that even based on a lim-\nited set of musical data, the model learns several aspects\nof tonal structure. Firstly, the model learns an organiza-\ntion of musical material from different keys that conveys\nthe topology of the circle of ﬁfths (CoF). Although such a\ntopology can be learned using principal component analy-\nsis (PCA) when using pitch-only representations, we found\nthat using a pitch-duration representation impedes the ex-\ntraction of the CoF topology much more for PCA than\nfor the RBM. Furthermore, we replicate probe-tone exper-\niments by Krumhansl and Shepard, measuring the organi-\nzation of tones within a key in human perception. We ﬁnd\nthat the responses of the RBM share qualitative character-\nistics with those of both trained and untrained listeners.\n1. INTRODUCTION\nModern approaches in music theory recognize that tonal-\nity can be broadly described as the organization of pitch\nclasses into a hierarchical structure of tensions-relaxations\naround a tonal axis [10,15,16]. This conception of tonality\nis not limited to western tonal classical music, but can also\nbe applied to modal music, popular music (e.g. jazz, rock)\nand non-western folk music [3]. This notion of tonality is\nnot only a music theoretic construct: perceptual processing\nof musical stimuli in human listeners has been found to ex-\nhibit this type of organization as well [10]. Speciﬁc types\nof hierarchical organization of pitch classes are partly ex-\nplained by acoustic attributes of pitch, especially the con-\nsonance between pairs of pitches [10], suggesting that low-\nc\rCarlos Eduardo Cancino Chac ´on, Stefan Lattner,\nMaarten Grachten.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Carlos Eduardo Cancino Chac ´on,\nStefan Lattner, Maarten Grachten. “Developing tonal perception through\nunsupervised learning”, 15th International Society for Music Information\nRetrieval Conference, 2014.level processing of acoustic stimuli may be relevant for the\nperception of tonal structure.\nHowever, tonal structure is not only reﬂected in the phys-\nical attributes of pitch, it is also manifest in the statistical\nproperties of music, such as the duration and frequency of\noccurrence of pitches [17], as illustrated in Figure 1. As\nSaffran et al. have shown [14], human listeners (including\ninfants) are sensitive to such statistical regularities, and this\nleads to the view that tonal perception may be shaped by\n(long time) exposure to music exhibiting statistical regular-\nities regarding frequency of occurrence of pitches, rhyth-\nmic emphasis, the position of occurrence within musical\nphrases, and possibly other aspects [9].\nIt is this process, the formation of tonal structure through\nexposure to musical stimuli, that we focus on in this paper.\nWe choose a particularly straightforward approach, using\na Restricted Boltzmann Machine (RBM) [6] to learn the\nprobability distribution of melodic sequences, represented\nas n-grams of notes. In a ﬁrst explorative experiment, we\nexamine to what degree the feature space learned by the\nRBM is musically meaningful. Using the resemblance of\nthe feature space to the circle of ﬁfths as a quantitative cri-\nterion, we investigate the impact of the n-gram length, and\ncompare pitch-only input representations to input repre-\nsentations that include both pitch and duration. In a second\nexperiment, we use the RBM to simulate listener ratings in\na probe tone test, and compare the results to ratings from\nhuman listeners of different skill levels.\nThe structure of the paper is as follows: In Section 2, we\ndiscuss prior work on the induction of tonal structure us-\ning computational models. Section 3 relates the different\naspects of the unsupervised learning task to various per-\nceptual mechanisms that are assumed to be at play in the\nperception of tonal structure. Section 4 brieﬂy describes\nthe RBM model, the data used for training the model, and\nrepresentation of the data. The experiments on tonal or-\nganization and the organization of pitches are described in\nSections 5 and 6, respectively. Conclusions and future di-\nrections are presented in Section 7.\n2. RELATED WORK\nThe idea of studying the perception of tonal structure by\nusing computational models to simulate the enculturation\nprocess is not new. For example, Tillmann et al. [18] use\na hierarchical self-organizing map (SOM) [8] to learn rep-\nresentations of tonal structure from pitch-class representa-\ntions of chord sequences. They ﬁnd that their model is able\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n195to develop an organization comparable to that of empirical\ndata gathered from various studies on human perception\nof tonality. Leman [12] presents an alternative approach\nto modeling the perception of tonality. He employs a psy-\nchoacoustic model in combination with a SOM to learn\ntonal representations starting from acoustic data. Further-\nmore, Toiviainen & Krumhansl examined the perception\nof musical scales by projecting human ratings to the fea-\nture space of a SOM, which was trained on scale proﬁles\nof Krumhansl [19].\nA commonality among the mentioned works is the choice\nof the self-organizing map as a model for accommodating\nthe learning process. The reason for this preference may\nbe that both the spatial mapping of the data, and the com-\npetitive learning algorithm employed by the SOM, are bi-\nologically plausible characteristics of the human sensory\ncortex [7]. The RBM model used in the work presented\nhere, is not explicitly presented as (nor was it designed to\nbe) a biologically plausible model of learning in the brain.\nNevertheless RBMs and deep belief nets based on RBMs,\nin combination with sparseness constraints on the activa-\ntion of hidden units, are able to learn features from vi-\nsual data that strongly resemble receptive ﬁelds of neurons\nin the visual cortex [11]. As such, RBMs prove to be a\nvalid computational modeling approach for learning bio-\nlogically plausible representations from musical data.\nA fundamental difference between SOMs and RBMs\nis that in the former, the hidden units represent points in\nan explicitly deﬁned low-dimensional feature space. In\nRBMs, the feature space is deﬁned by the set of all possi-\nble combinations of hidden unit activations, such that each\nhidden unit represents a dimension of the feature space.\nThis allows for representations of data instances as a (non-\nlinear) combination of features. The topology of this high-\ndimensional feature space can be visualized in a 2-D space\nusing PCA.\n3. PERCEPTUAL MECHANISMS\nAs argued by Smith and Schmuckler [17], perceptual pro-\ncesses like discrimination, differentiation andorganization\nplay an important role in the perception of musical tonality.\nIn this Section, we will brieﬂy describe these processes,\nand show how they can be related to formal aspects of the\ncomputational modeling methods, such as the choice of\ninput data representation, and the topology of the feature\nspace being learned.\nPerceptual discrimination refers to the sensitivity of a\nsystem to differences along some perceivable stimulus di-\nmension. In computational learning models, this relates\nto the form of input data representation. In general, the\ntype of relevant input features depends heavily on the re-\nspective learning task [1]. Musical data comprises much\ncontext-dependent information that can not be trivially in-\nferred from low-level representations. To decide on an ap-\npropriate representation is thus not always an easy task.\nFor instance, pitch content can be represented in several\nways, such as frequency spectra, MIDI note numbers, or\npitch classes. In our current experiments, we use MIDInote numbers as well as pitch class representations. Dura-\ntion is encoded separately from pitch. An advantage of this\nover combined pitch-duration representations (e.g. piano-\nroll notation) is that the n-gram size is speciﬁed in the num-\nber of notes, rather than an absolute time interval. This al-\nlows for comparing pitch-only to pitch-duration represen-\ntations. The input data will be referred to as Input Space\n(IS), and will be described in more detail in Section 4.3.\nDifferentiation is a higher order ability that refers to the\nsegregation of the perceived stimuli into elements on the\nbasis of its discriminable differences [17]. In an unsuper-\nvised model we can identify this ability as the capacity of\nthe system to segregate the data in the IS into clusters in the\nFeature Space (FS). In the context of tonality, an example\nof differentiation would be the capacity of an unsupervised\nmodel to cluster the data in the FS in such a way that each\ncluster represents a musical key. A measure of quality of\nthis clustering would then be the variance of each cluster,\nas smaller variances imply a better differentiation of the\ndata with respect to each class.\nOrganization builds on the concept of differentiation, as\nit establishes relations between the differentiated elements,\nas well as the nature of the relations themselves. In an\nunsupervised model, this can be understood as the topol-\nogy of the FS. In this way, geometric features such as the\ndistance between clusters, as well as the relative position\nbetween them can express similarity.\nBharucha [10, cited by Krumhansl] recognizes two types\nof hierarchies regarding musical tonality. Event hierar-\nchies refer to the functional signiﬁcance of single note events\nin a speciﬁc musical context, while tonal hierarchies ac-\ncount for the abstract musical structure in a particular cul-\nture or genre, e.g. the functional signiﬁcance of all ele-\nments of a pitch class relative to all other pitch classes.\nIn our case, we compare the organization of the data in\nthe FS to the circle of ﬁfths, a well known music theoretical\nconstruct that explains the relations and the neighborhood\nof keys [15]. As a measure of quality we use the Procrustes\nDistance (PD) [4] of the centroids of the clustered data in\nthe feature space with respect to the CoF.\n4. METHODS\n4.1 Restricted Boltzmann Machine\nA Restricted Boltzmann Machine is a stochastic Neural\nNetwork (NN) with two layers, a visible layer with units\nv2 f0; 1grand a hidden layer with units h2 f0; 1gq\n[6]. The units of both layers are fully interconnected with\nweights W2Rr\u0002q, while there are no connections be-\ntween the units within a layer. Given a visible vector v,\nthe free energy of the model can be calculated as:\nF(v) =\u0000a|v\u0000X\nilog\u0010\n1 +e(bi+W iv)\u0011\n;(1)\nwhere a2Rrandb2Rqare bias vectors, and Wiis the\ni-th row of the weight matrix.\nGiven v, a sample of hcan be obtained from its condi-\ntional activation probability, given by:\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n196Major Major\nMinor Minorrelative frequency relative frequency\nrelative duration\nScale degree Scale degree\nScale degree Scale degreerelative durationFigure 1. Occurrence and duration distributions of the\nfugues from Bach’s Well Tempered Clavier.\np(h=1jv) =\u001b(b+ (v|W)|); (2)\nwhere\u001b(x) = 1=(1+ e\u0000x)is the logistic sigmoid function.\nIn experiment 1, we consider the conditional activation\nprobability of vector has the result of the projection of\nvinto the FS. In the second experiment, we calculate the\nenergy using Eq. (1).\n4.1.1 Training\nWe train the model with 200 hidden units for 1000 epochs\nwith Contrastive Divergence (CD) [6], using 3Gibbs sam-\npling steps and a mini-batch size of 500for the weight up-\ndates. The learning rate is set to 0:01 and the momentum\nto0:3. These parameters were empirically selected accord-\ning to the rules of thumb suggested by Hinton in [5]. In\naddition, we use the well-known L2 weight-decay regular-\nization which penalizes large weight coefﬁcients.\nBased on properties of neural coding, sparsity and se-\nlectivity can be used as constraints for the optimization of\nthe training algorithm [2]. Sparsity encourages competi-\ntion between hidden units, and selectivity prevents over-\ndominance by any individual unit. These constraints are\nused in our training, with a linear falloff of its inﬂuence\nover the ﬁrst 200 epochs from 50% to 30%.\n4.2 Training Corpus\nJ. S. Bach’s Well Tempered Clavier (WTC), composed be-\ntween 1722 and 1742, is widely recognized as one of the\nmost inﬂuential works in music history [15]. It is also\none of the most important works that systematically spans\nthe whole range of major and minor keys, and is therefore\nwell-suited for experiments on tonality. In this paper, we\nuse MIDI versions of the 48 fugues of the WTC as corpus,\nencoded by David Huron and taken from the KernScores\nwebsite (http://kern.ccarh.org). Each fugue is decomposed\ninto its voices (two to ﬁve), and we consider each voice\nas a single monophonic melody in its respective key. In\nFigure 1, the distributions of the occurrence and duration\nof the notes of the WTC are shown. These distributions are\nsimilar to the key proﬁles by Krumhansl & Kessler [19].\nFigure 2. Twelve random pitch-duration training instances\nof the WTC corpus as 20-grams before linearization. Notes\nare ordered horizontally, the vertical dimension accounts\nfor pitch and duration values, respectively. The left part\nof each instance shows the one-out-of-m pitch representa-\ntion of 20consecutive notes, the right part shows the cor-\nresponding duration representation.\n::\n: h\nW\n::\n: ::\n: \u0001 \u0001\n\u0001 ::\n: v\n::\n: stst\u0000n+1 st\u0000n+2\nFigure 3. The RBM architecture used. An input vector v\nis constituted by a linearized n-gram, where sjis a binary\nrepresentation of note j.\n4.3 Input Representation\nFrom the monophonic melodies, we construct a set of n-\ngrams by using a sliding window of size nand a step size\nof1. Depending on the experiment, we either use only\npitch information, or we use both the pitch and duration of\nthe notes. In the ﬁrst case, an n-gram is a concatenation\nofnbit vectors of size m, where the i-th bit vector is a\none-out-of-m representation of the pitch of note i.\nIn the second case, nadditional vectors are added to\nthe n-gram, where the i-th vector now represents the du-\nration of the i-th note (see the right half of the instances\nshown in Figure 2). Such a duration vector is constructed\nby quantizing all durations of a melody into 12bins and\nby relating each of those to one of 12 units. A duration\nthat falls into bin kis represented by activating units 1 to\nk. After linearization, the resulting n-gram constitutes the\nvisible vector v, as illustrated in Figure 3.\n5. TONAL ORGANIZATION\nIn this experiment, we examine the ability of an RBM to\nlearn tonal relationships between n-grams. To that end, we\nproject the FS learned by the RBM into a two-dimensional\nspace using Randomized Principal Component Analysis\n(rPCA) [13]. As the CoF is the underlying music the-\noretical construct for the relationships between keys, we\nare interested to what degree we can approximate the CoF\ntopology. As a baseline, we compare this projection to a\ndirect projection of the IS, again using rPCA.\n5.1 Training\nWe encode the WTC corpus as described in 4.3. As keys\nare characterized by distributions of pitch classes, the pitch\nrange is set to m= 12. In order to examine the organiza-\ntion ability of the RBM under different settings, we use\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n197Randomized PCA on Feature Space\nPC 1PC 2Figure 4. 2-D visualization of n-grams in the FS using\nrPCA. N-grams belonging to a key have the same color,\neach centroid is marked with the corresponding cluster’s\nkey label. (Best viewed in color)\nProcrustes distance to Circle of Fifth\nN-gram lengthInput Space, pitchInput Space, pitch-durationFeature Space, pitchFeature Space, pitch-duration\nFigure 5. The average Procrustes Distances from major\nkey centroids to the major CoF of 5 runs for different n-\ngram lengths after rPCA on the IS and on the FS. pitch and\npitch-duration representations are used as input.\nn-grams of various lengths, and also compare pitch and\npitch-duration representations.\n5.2 Evaluation\nWe use rPCA to project all n-grams in both the IS and the\nFS into a two-dimensional space. In this space, for each\nkey we determine the mean of all n-grams created from\npieces in that key. The organization of those centroids is\nthen compared to the organization of keys in the CoF by\ncomputing the PD of both shapes, separately for major and\nminor keys. To make different expansions of data points in\nspace comparable, the PD is ﬁnally divided by the perime-\nter of the target CoF.\n5.3 Results and Discussion\nFigure 4 shows the organization of n-grams in the FS. Clus-\nter centers are organized similarly to how keys are orga-nized in the CoF, which is consistent with the representa-\ntions of the probe tone ratings obtained by Krumhansl and\nKessler [9,10]. Note that relative minors tend to be shifted\ncounterclockwise with respect to their major counterparts.\nThis occurs in Krumhansl’s results as well [10, pp. 43], and\ncan be explained by two factors, namely the alteration of\nthe sixth degree in the melodic minor scale, which is iden-\ntical to the seventh degree of the dominant of the relative\nmajor counterpart (e.g. the melodic Am scale shares the F#\nwith the G major scale, the dominant of C major), and due\nto the tonal modulations concerning the form of the piece\n(e.g. fugues in minor keys tend to have certain passages in\nthe relative major, while fugues in major keys tend to have\npassages in both the relative minor and the dominant).\nFigure 5 shows, that the Procrustes Distance to the CoF\ntends to stabilize at a minimum with an n-gram length of\nabout nine. This can be explained by the fact that n-grams\nof that length contain enough information to obtain the re-\nspective distribution of a key well enough. Adding dura-\ntion information clearly impedes the organization of clus-\nters in a CoF topology. As the occurrence of notes in the\nWTC is strongly correlated to their absolute duration (see\nFigure 1), and rhythmic information is not directly linked\nto the CoF organization, this is not unexpected. Interest-\ningly, for larger n-gram sizes the FS of the RBM is not dis-\nrupted as much by the inclusion of distractive information\nas the rPCA on the IS.\n6. ORGANIZATION OF PITCHES\nA probe-tone test, proposed by Krumhansl et al. [9, 10],\nconsists of a set of musical stimuli (such as scales, chord\ncadences, or musical pieces) that unambiguously instanti-\nate a speciﬁc key, and a set of probe tones, typically the set\nof 12 pitch classes. Listeners are then required to rate on\na numerical scale, from 1 (“very bad”) to 7 (“very good”),\nhow well the probe tones ﬁt the musical stimulus. In order\nto explore the hierarchical event organization of pitches in-\nduced by the RBM, we compare our model with a partic-\nular probe tone test conducted by Krumhansl and Shep-\nard [10, cited by Krumhansl]. In this speciﬁc experiment,\nthe musical stimulus consisted of an incomplete C major\nscale (in both ascending and descending contexts), and lis-\nteners were asked to give a numerical rating of the degree\nto which each probe tone ﬁts the scale. The stimuli of this\nparticular setup are illustrated in part Figure 6 a), while the\nprobe tones are shown in Figure 6 c). The participants of\nthe experiment were divided in three groups according to\ntheir number of years of formal musical training.\n6.1 Training\nAs we are only interested in the ability of the model to\nlearn tonal hierarchies in major and minor mode, we trans-\npose all melodies to C major and C minor, respectively. In\norder to remain consistent with the aforementioned exper-\niment of Krumhansl & Shepard, rather than using pitch-\nclasses, we allow the training data to be in a range of three\noctaves, ranging from MIDI pitch numbers 48 to 74 (such\nthat both the stimuli and the probe tones can be represented\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n198Figure 6. Stimuli/probe tones used in the probe-tone test.\nwithout wrapping). Most of the n-grams of the transposed\nWTC data fall in that range, or can be transposed octave-\nwise to fall in that range. N-grams for which this is not\npossible are ignored.\nSince the fugues from the WTC contain certain tonal\nmodulations, in order to train the RBM with prototypi-\ncal examples of major and minor scales, all n-grams are\nclassiﬁed using the Krumhansl & Kessler key-ﬁnding algo-\nrithm [10, cited by Krumhansl] and those whose annotated\nkey is not the same as that identiﬁed by the classiﬁer (ca.\n53% of the corpus) are removed. The training is executed\nas described in 4.1.1.\n6.2 Evaluation\nTwo different probe tone tests are conducted. The ﬁrst test\naims to reproduce the setup by Krumhansl and Shepard,\nand thus, the stimuli consist of the major ascending (start-\ning from C3) and descending scales (starting from C5)\nshown in Figure 6 a). For the second experiment, the stim-\nuli consist of ascending and descending major and melodic\nminor scales, but this time both are generated in the middle\nC octave, as shown in Figure 6 b). For both tests, the set\nof probe tones consist of all notes of the chromatic scale\n(starting from C4) as shown in Figure 6 c). We construct\nn-grams of length 8, consisting of the 7 notes of the target\nstimulus and a probe tone as the last note. This results in\nvisible vectors vptof length 36\u00028. The free energy corre-\nsponding to each combination of stimulus and probe tone\nis calculated using Eq. (1). In order to compare our results\nto those of human listeners, these energies are scaled using\nan afﬁne transformation as follows:\nJudgment(v pt) =\u000bF(vpt)\u0000\f; (3)\nwhere the constants \u000b;\f are selected such that the mean\nand the variance of the scaled energy are equal to those of\nthe judgments reported in [10].\n6.3 Results and Discussion\nFigure 7 shows the results of the probe tone test, and in\nTable 1 the correlations of the RBM judgments with re-\nspect to those of expert and untrained listeners are pre-\nsented. These results suggest that the model can learn\nsome event hierarchy structures, such as the prevalence of\ndiatonic over chromatic notes, similar to the judgment ofGroup r p-value\nExpert ascending 0.7213 0.0054\nUntrained ascending 0.7942 0.0012\nExpert descending 0.7985 0.0011\nUntrained descending 0.8344 0.0004\nTable 1. Pearson correlations and p-values for the judg-\nments of the probe tone tests.\ntrained listeners. In addition, the model develops a sense\nfor melodic direction, preferring probe tones close to the\nﬁnal notes of the stimulus, which is consistent with the\nratings of untrained listeners. Stimulated in the middle\noctave, the model is able to distinguish major and minor\nmodes, especially the major and minor thirds reﬂect the\ncharacteristics of the respective diatonic triads. The model\nresponses do not show explicit octave equivalence, since C\nand C’ are not equally emphasized. Still it is interesting to\nnote that a stimulus in the lower octave has implications on\nthe pitch expectations in the middle octave, and that these\nimplications are in correspondence with the tonal hierar-\nchy of the key implied by the stimulus.\n7. CONCLUSION\nIn this paper we show that tonal structure can be learned\nfrom musical data with an RBM using unsupervised train-\ning with a limited set of monophonic melodies. The model\nis able to reproduce the topology of the CoF using pitch n-\ngram representations of the input data. We found that for\nsuccessful inference of the CoF, a minimal n-gram length\nof nine notes is needed, and that longer n-grams do not lead\nto better representations. Furthermore, although duration\ninformation profoundly disturbs the learning of tonal struc-\nture through the baseline rPCA method, the RBM model is\nless affected by distracting duration information.\nBy way of a probe tone test, we explored the organiza-\ntion of pitches in the context of major and minor modes.\nOur results show the model was able to learn several as-\npects of tonal structure, in particular the hierarchical preva-\nlence of diatonic over chromatic tones. Comparing results\nwith Krumhansl’s probe tone experiments on human sub-\njects with different levels of musical training do not yield a\nconclusive classiﬁcation of the model: the model displays\naspects of both untrained and trained subjects.\nAn important feature of tonal perception in trained sub-\njects is octave equivalence. This feature was not well-\nreproduced by the model. It is possible that a pre-condition\nfor octave-equivalence is the harmonic overlap of octaves.\nIn our current setup, the overtone structure of tones is not\nrepresented. To test this hypothesis, we intend to investi-\ngate whether using harmonic tone representations leads to\nstronger octave-equivalence in the the model.\nFurthermore we wish to investigate which factors in-\nduce more expert-like perception of tonal structure. Pos-\nsible factors include the size of the training data, and the\ndepth of the model (in terms of hidden layers).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n199  C C# D D# E F F# G G# A A# B C’012345678Judgment\nProbe tonesAscending\n  \nRBM\nexperts\nuntrained\n  C C# D D# E F F# G G# A A# B C’012345678Judgment\nProbe tonesDescending\n  \nRBM\nexperts\nuntrained\n  C C# D D# E F F# G G# A A# B C’012345678Judgment\nProbe tonesAscending\n  \nMajor\nMinor\n  C C# D D# E F F# G G# A A# B C’012345678Judgment\nProbe tonesDescending\n  \nMajor\nMinorFigure 7. (Top) Comparison of the judgments for the probe tones between the RBM and human listeners for both ascending\n(left) and descending (right) major stimulus in the lower and upper octave, respectively. (Bottom) Comparison of the\njudgments for the probe tones of the RBM for both major and melodic minor stimulus in the middle octave. In all cases,\nresponses are measured in the middle octave.\n8. ACKNOWLEDGMENTS\nThis work is supported by the European Union Seventh\nFramework Programme, through the Lrn2Cre8 project (FET\ngrant agreement no. 610859). We thank Geraint Wiggins,\nKat R. Agres and Jamie Forth for valuable suggestions and\ncommentaries on this work.\n9. REFERENCES\n[1] C. M. Bishop. Pattern Recognition and Machine\nLearning. Springer Verlag, 2009.\n[2] H. Goh, N. Thome, and M. Cord. Biasing restricted\nBoltzmann machines to manipulate latent selectivity\nand sparsity. NIPS Workshop on Deep Learning and\nUnsupervised Feature Learning, pages 1–8, 2010.\n[3] E. G ´omez. Tonal description of music audio signals.\nPhD thesis, Universitat Pompeu Fabra, Barcelona,\n2006.\n[4] C. Goodall. Procrustes Methods in the Statistical Anal-\nysis of Shape. Journal of the Royal Statistical Society.\nSeries B (Methodological), 53(2):285–339, 1991.\n[5] G. E. Hinton. A practical guide to training restricted\nBoltzmann machines. Tech. Report UTML TR 2010-\n003, Department of Computer Science, University of\nToronto, 2010.\n[6] G. E. Hinton. Training products of experts by min-\nimizing contrastive divergence. Neural Computation,\n14(8):1771–1800, 2002.\n[7] T. Kohonen. The self-organizing map. Proceedings of\nthe IEEE, 78(9):1464–1480, 1990.\n[8] T. Kohonen. Self-organized formation of topologi-\ncally correct feature maps. Biol. Cybernetics, 43:59–\n69, 1982.[9] C. L. Krumhansl and L. L.Cuddy. A theory of tonal hi-\nerarchies in music. In Handbook of Auditory Research.\nSpringer, New York, 2010.\n[10] C. L. Krumhansl. Cognitive foundations of musical\npitch. Cognitive foundations of musical pitch. Oxford\nUniversity Press, New York, 1990.\n[11] H. Lee, C. Ekanadham, and A. Y . Ng. Sparse deep\nbelief net model for visual area V2. In Advances in\nNeural Information Processing Systems 20, pages 873–\n880. 2008.\n[12] M. Leman. A model of retroactive tone center percep-\ntion. Music Perception, 12(4):439–471, 1995.\n[13] V . Rokhlin, A. Szlam, and M. Tygert. A randomized\nalgorithm for principal component analysis. arXiv.org,\npage 2274, 2008.\n[14] J. R. Saffran, E. K. Johnson, R. N. Aslin, and E. L.\nNewport. Statistical learning of tone sequences by hu-\nman infants and adults. Cognition, 70(1):27 – 52, 1999.\n[15] F. Salzer. Structural hearing; tonal coherence in music.\nNew York, Dover Publications, 1962.\n[16] H. Schenker. Harmony. University of Chicago Press,\n1980.\n[17] N. A. Smith and M. A. Schmuckler. The Perception\nof Tonal Structure Through the Differentiation and Or-\nganization of Pitches. Journal of Exp. Psych.: Human\nPerception and Performance, 30(2):268–286, 2004.\n[18] B. Tillmann, J. J. Bharucha, and E. Bigand. Implicit\nlearning of tonality: a self-organizing approach. Psy-\nchological review, 107(4):885–913, 2000.\n[19] P. Toiviainen and C. L. Krumhansl. Measuring and\nmodeling real-time responses to music: The dynam-\nics of tonality induction. Perception, 32(6):741–766,\n2003.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n200"
    },
    {
        "title": "Improving Query by Tapping via Tempo Alignment.",
        "author": [
            "Chun-Ta Chen",
            "Jyh-Shing Roger Jang",
            "Chun-Hung Lu"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416910",
        "url": "https://doi.org/10.5281/zenodo.1416910",
        "ee": "https://zenodo.org/records/1416910/files/ChenJL14.pdf",
        "abstract": "Query by tapping (QBT) is a content trieval method that can retrieve a song by taking the u er’s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (i ter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Exper ments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy 10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT pr vides an interesting and innovative way for music retrie al by shaking or tapping the devices, which is also di cussed in the paper.",
        "zenodo_id": 1416910,
        "dblp_key": "conf/ismir/ChenJL14",
        "keywords": [
            "query-by-tapping",
            "content retrieval",
            "song matching",
            "note onset intervals",
            "dynamic programming",
            "IOI vector",
            "dataset alignment",
            "accuracy 10",
            "MRR",
            "personal handheld devices"
        ],
        "content": "IMPROVED QUERY\nChun-Ta Chen \nDepartment of Computer Science \nNational Tsing Hua University \nHsinchu, Taiwan \nchun-ta.chen@mirlab.org  \nABSTRACT \nQuery by tapping (QBT) is a content\ntrieval method that can retrieve a song by taking the u\ner’s tapping or clapping at the note onsets of the intended \nsong in the database for comparison. This paper proposes \na new query-by- tapping algorithm that aligns the IOI (i\nter-onset interval) vector of the query sequence with \nsongs in the dataset by build ing an IOI ratio matrix, and \nthen applies a dynamic programming (DP) method to \ncompute the optimum path with minimum cost. Exper\nments on different datasets indicate that our algorithm \noutperforms other previous approaches in accuracy\n10 and MRR), wit h a speedup factor of 3 in computation. \nWith the advent of personal handheld devices, QBT pr\nvides an interesting and innovative way for music retrie\nal by shaking or tapping the devices, which is also di\ncussed in the paper. \n1. INTRODUCTION\nQBT is a mechanism for content- based music retrieval \nwhich extracts the note onset time from recordings of u\ners' input tapping or symbolic signals, which it then co\npares against a song database to retrieve the correct song. \nUnlike query-by- singing/humming (QBSH) [1, 2], \ntakes the user's melody pitch for comparison, QBT only \nuses the note duration for comparison, with no pitch i\nformation. This makes QBT more difficul\nthan QBSH, be cause the note onset in QBT contains less \ninformation than the musical pi tch in QBSH, raising the \nlikelihood of collision. For example, musical pieces with \ndifferent melodies but similar rhythmic patterns may be \ncharacterized by the same onset sequence.\nOne may argue that QBT is not a popular way of m\nsic retrieval. Some people may even think it is not useful. \nHowever, with the advent of personal handheld devices, \nwe can think QBT as a novel way of human\ninterface. For instance, with the use of QBT, one may \nshake or click his/her mobile phones in order to retrieve a \nsong. Moreover, one can even use a personal style of \nshaking or clicking as the password to unlock a phone. \nThese innovative ways of human- machine interface ind\ncate that QBT, though not the most popul\n © Chun-Ta Chen, Jyh- Shing Roger Jang\nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Chun-Ta Chen\nJang, Chun-Hung Lu. “Improved Query-By- Tapping via Tempo \nAlignment ”, 15th International Society for Music Information Retrieval \nConference, 2014.  IMPROVED QUERY -BY-TAPPING VIA TEMPO ALIGNMENT\nJyh-Shing Roger Jang  Chun\nDepartment of Computer Science \nNational Taiwan University \nTaipei, Taiwan \njang@mirlab.org Innovative Digitech\ntions & Services Institute (IDEAS), I\nstitute for Information Industry, Taiwan\nenricoghlu@iii.org.tw\nQuery by tapping (QBT) is a content -based music re-\ntrieval method that can retrieve a song by taking the u s-\ner’s tapping or clapping at the note onsets of the intended \nsong in the database for comparison. This paper proposes \ntapping algorithm that aligns the IOI (i n-\nonset interval) vector of the query sequence with \ning an IOI ratio matrix, and \nthen applies a dynamic programming (DP) method to \ncompute the optimum path with minimum cost. Exper i-\nments on different datasets indicate that our algorithm \noutperforms other previous approaches in accuracy  (top-\nh a speedup factor of 3 in computation. \nWith the advent of personal handheld devices, QBT pr o-\nvides an interesting and innovative way for music retrie v-\nal by shaking or tapping the devices, which is also di s-\nINTRODUCTION  \nbased music retrieval \nwhich extracts the note onset time from recordings of u s-\ners' input tapping or symbolic signals, which it then co m-\npares against a song database to retrieve the correct song. \nsinging/humming (QBSH) [1, 2], which \ntakes the user's melody pitch for comparison, QBT only \nuses the note duration for comparison, with no pitch i n-\nformation. This makes QBT more difficul t to implement \ncause the note onset in QBT contains less \ntch in QBSH, raising the \nlikelihood of collision. For example, musical pieces with \ndifferent melodies but similar rhythmic patterns may be \ncharacterized by the same onset sequence.  \nOne may argue that QBT is not a popular way of m u-\nmay even think it is not useful. \nHowever, with the advent of personal handheld devices, \nwe can think QBT as a novel way of human -computer \ninterface. For instance, with the use of QBT, one may \nshake or click his/her mobile phones in order to retrieve a \ng. Moreover, one can even use a personal style of \nshaking or clicking as the password to unlock a phone. \nmachine interface ind i-\npopular way of music retrieval, is itself interesting and could\nother innovative applications [10].\nQBT system algorithms are based on the estimation of \nthe similarity between two onset sequences. For example, \nG. Eisenberg proposed a simple algorithm called \"Direct \nMeasure\" to accomplish such comparis\nTypke presented a variant of the earth mover's distance \nappropriate for searching rhythmic patterns [5]. Among \nthese algorithms, the techniques of dynamic progra\nming (DP) have been widely used, such as R. Jang's D\nnamic Time Warping (DTW) [\nalgorithm [7, 8], and P. Hanna's adaptation of local \nalignment algorithm [9]. \nIn this paper, we propose and test a new QBT alg\nrithm. In Section 2, we discuss the general frameworks of \nQBT and existing QBT metho\nproposed method. Experiments with different QBT tec\nniques are described in Section 4. Finally, Section 5 co\ncludes this paper. \n2. THE QBT SYSTEM\nFig. 1 illustrates the flowchart of our query\nsys-tem. In general, there are 2 kinds of inputs to a QBT \nsys-tem: \n\u0001 Symbolic input: The onset time of the tapping event is \nprovided symbolically with little or no amb\ninstance, the user may tap on a PC\niPad’s touch panel to give the onset time.\n\u0001 Acoustic input: The onset time is extracted from \nacoustic input of the user’ s tapping on a microphone. \nThis input method requires additional onset \nto extract the onset time of the acoustic input. For e\nample, we can estimate the onset time by l\nmaximum- picking of the input audio’s intensity as in \n[5], or by detecting the transients of kurtosis variation \nas in [7]. \nThe input onset sequen ce can be obtained as the inter \nonset interval (IOI) vecto r whose elements are the diffe\nence between two successive onset times. The note onset \nsequences extracted from the monophonic MIDIs (or the \nmelody track of polyphonic MIDIs) in the song database \nare also converted into IOIs in advance. We can then a\nply a QBT algorithm to compare the query IOI vector to \nthose in the database in order to retrieve the most similar \nsong from the database. A QBT algorithm usually needs \nto perform IOI vector normalizatio\ncomparison. Normalization can take care of tempo deviShing Roger Jang , Chun-Hung Lu. \nLicensed under a Creative Commons Attribution 4.0 International \na Chen, Jyh-Shing Roger \nTapping via Tempo \n”, 15th International Society for Music Information Retrieval  \nTAPPING VIA TEMPO ALIGNMENT  \nChun-Hung Lu  \nInnovative Digitech -Enabled Applica-\ntions & Services Institute (IDEAS), I n-\nstitute for Information Industry, Taiwan  \nenricoghlu@iii.org.tw \nretrieval, is itself interesting and could  pave the way for \nother innovative applications [10].  \nQBT system algorithms are based on the estimation of \nthe similarity between two onset sequences. For example, \nG. Eisenberg proposed a simple algorithm called \"Direct \nMeasure\" to accomplish such comparis ons [3, 4]. R. \nTypke presented a variant of the earth mover's distance \nappropriate for searching rhythmic patterns [5]. Among \nthese algorithms, the techniques of dynamic progra m-\nming (DP) have been widely used, such as R. Jang's D y-\nnamic Time Warping (DTW) [ 6], G. Peters's edit distance \nalgorithm [7, 8], and P. Hanna's adaptation of local \nIn this paper, we propose and test a new QBT alg o-\nrithm. In Section 2, we discuss the general frameworks of \nQBT and existing QBT metho ds. Section 3 describes the \nposed method. Experiments with different QBT tec h-\nniques are described in Section 4. Finally, Section 5 co n-\nTHE QBT SYSTEM  \nFig. 1 illustrates the flowchart of our query -by-tapping \ntem. In general, there are 2 kinds of inputs to a QBT \nSymbolic input: The onset time of the tapping event is \nprovided symbolically with little or no amb iguity. For \ninstance, the user may tap on a PC ’s keyboard or an \ns touch panel to give the onset time.  \nAcoustic input: The onset time is extracted from \ns tapping on a microphone. \nThis input method requires additional onset detection \nto extract the onset time of the acoustic input. For e x-\nample, we can estimate the onset time by l ocal-\npicking of the input audio’s intensity as in \n[5], or by detecting the transients of kurtosis variation \nce can be obtained as the inter \nr whose elements are the diffe r-\nence between two successive onset times. The note onset \nsequences extracted from the monophonic MIDIs (or the \nmelody track of polyphonic MIDIs) in the song database \ne also converted into IOIs in advance. We can then a p-\nply a QBT algorithm to compare the query IOI vector to \nthose in the database in order to retrieve the most similar \nsong from the database. A QBT algorithm usually needs \nto perform IOI vector normalizatio n before similarity \ncomparison. Normalization can take care of tempo devi a-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n289  \n \nt\nion, which similarity comparison can handle possible \ninsertion/deletion errors. Once normalization is per-\nformed, we can apply similarity comparison to find the \nsimilarity between the IOI query vector and that of each \ndatabase song. The system can then return a ranked list of \nall database songs according to their similarity to the \nquery input. \nNormalization and the similarity comparison are de-\ntailed in the following sections. \n2.1 Normalization of IOI Vectors \nIn most cases, the tempo of the user's query input is dif-\nferent from those of the candidate songs in the database. \nTo deal with this problem, we need to normalize the IOI \nvectors of the input query and the candidate songs. There \nare 2 common methods for normalization. The first one is \nto convert the summation of all IOI to a constant value \n[5]. \n (1)\nwhere {qi, i=1~m}  is the input query IOI vector, and { rj, \nj\n=1~\u0001 \u0002} is a reference IOI vector from the song database. \nNote that the reference IOI vector of a song is truncated to \na variety of lengths in order to match the query IOI. For \ninstance, \u0001 \u0002 may be set to a value from m-2 to m+2 in or-\nder to deal with possible insertions and deletions in the \nquery input. Thus all these variant normalized versions of \nthe IOI vectors for a song must be compared for similarity \nwith the query IOI vector. The second method is to \nrepresent the normalized IOI vector as the ratio of the cur-\nrent IOI element to its preceding element [7]. That is: \n (2)\nwhere {si} is the original input query or reference IOI \nv\nector, and {\u0003̃\u0005}  is its normalized version. The advantage \no\nf this method is that computation-wise it is much simp-\nler than the first one.  However, this method is suscepti-\nble to the problem of magnified insertion and deletion errors of the original IOI vectors, if any. For example, an \nIOI vector is [1, 2, 1], then its normalized vector is [1, 2, \n0.5]. If this IOI vector is wrongly tapped as [1, 1, 1, 1] \n(i.e., with one insertion in the second IOI), the normalized \nwill become [1, 1, 1, 1], which has a larger degree of dif-\nference from the groundtruth after normalization. This \nkind of amplified difference is harder to recover in the \nstep of similarity comparison. \n2.2 Similarity Comparison \nA robust QBT system should be able to handle insertion \nand deletion errors since most of the common users are \nnot likely to tap the correct note sequence of the intended \nsong precisely. In particular, a common user is likely to \nlose one or several notes when the song has a fast tempo, \nwhich leads to deletion errors. On the other hand, though \nless likely, a user may have a wrong impression of the \nintended song and taps more notes instead, which lead to \ninsertion errors. Several methods have been proposed to \ncompare IOI vectors for QBT, including the earth mov-\ner’s distance [4] and several DP-based methods [5], [6], \n[7] which can deal with two input vectors of different \nlengths. In general, the earth mover’s distance is faster \nthan DP-based methods, but its retrieval accuracy is not \nas good [11]. Our goal is to obtain a good accuracy with a \nreasonable amount of computing time. Therefore, the \nproposed method is based on a highly efficient DP-based \nmethod for better accuracy. \n3. THE SHIFTED ALIGNMENT ALGORITHM \nThis section presents the proposed method to QBT. The \nmethod can also be divided into two stages of IOI norma-\nlization and similarity comparison. We shall describe \nthese two steps and explain the advantages over the state-\nof-art QBT methods. \nNormalization : In QBT, though the query IOI vector \nand its target song IOI vector are not necessarily of the \nsame size, the ratio of their tempos should be close to a \nconstant. In other words, the ratios of an IOI element of a \nquery to the corresponding one of the target song should \nbe close to a constant. To take advantage of this fact, we \ncan shift the query IOI vector (relatively to the target \nsong IOI vector) to construct an IOI ratio matrix in order \nto find the optimum mapping between IOI elements of \nthese two sequences. An example is shown in Fig. 2(a), \nwhere the input query IOI vector is represented by { qi, \ni\n=1~m}, and the reference IOI vector from the song data-\nbase by { rj, j=1~n} . As displayed in the figure, the refer-\nence IOI vector is shown at the top and the shifted query \nIOI vectors are shown below. Each element of a shifted \nquery IOI vector is mapped to that of the reference IOI \nvector in the same column. Take the first shifted query \nIOI vector as an example, its second element q2 is \nm\napped to r1 of the reference IOI vector, q3 is mapped to \nr2, etc. For each matched element pair, we divide the ∑∑\n==\n==\nn\nkk j jm\nkk i i\nr r rq q q\n~\n11\n~~\n\n≥ ==\n−2  if  , /~1~\n11\ni s s ss\ni i i \n \nFig. 1. QBT system flowchart \n \n MIDI songs\nOnset \nextractionAcoustic \ninputOff-line processing\nNote duration\nextractionOn-line processing\nSimilarity\ncomparisonNormalization\nQuery\nresultsSymbolic \ninput\nor\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n290  \n \nq\nuery IOI by its mapping reference IOI to construct an \nIOI ratio matrix M according to the following formula: \n \n (3) \nwhere the size of the matrix M is min( m,n)*( is + ie +1). \nis and ie are the left- and right-shift amount of the query \nI\nOI vector, respectively. Fig. 2(b) is the IOI ratio matrix \nof fig. 2(a). In this example, is and ie are 1 and 2, respec-\nt\nively. Since the length of the query is usually shorter, m \nis generally much less than n. Besides, in practice, if the \nanchor position is the beginning of a song, then we can \nimprove the computation efficiency by truncating a refer-\nence IOI vector to a length slightly longer (e.g., 5-\nelement longer) than the length of query IOI vector. \nUnlike the equation (1) which requires many different \nversions of normalized reference IOI vectors for similari-\nty comparison, the proposed approach requires only one-\ntime normalization to generate a single IOI ratio table for \ncomputing the similarity. So the proposed approach is \nguaranteed to more efficient. \nSimilarity comparison: In order to handle insertions \nand deletions in a flexible yet robust manner, we propose \na dynamic programming method to compute the similari-\nty between the query and the reference IOI vectors. The \nbasic principle is to identify a path over the IOI ratio ma-\ntrix M where the elemental values along the path should \nbe as close as possible to one another. In other words, the \naccumulated IOI ratio variation should be minimal along \nthe optimal path. Fig. 3 illustrates two typical numeric \nexamples that involve insertion and deletion in the optim-\nal path. In fig. 3(a), query IOI vector and reference IOI \nvector have the same tempo, so their elements are pretty \nmuch the same except that there is an insertion in the \nquery. That is, the fourth element of the reference IOI vector is equally split into 2 elements in the query. Fig. \n3(b) is the IOI ratio matrix derived from the fig. 3(a), \nwith the optimal path surrounded by dashed lines. The \nhorizontal direction within the optimal path represent \none-to-one sequential mapping between the two vectors \nwithout insertion or deletion. The vertical direction with-\nin the path indicates an insertion, where the 4th and 5th \nquery IOI elements should be mapped to the 4th reference \nIOI element. On the other hand, Fig. 3(c) demonstrates an \nexample of deletion where the query misses the 4th onset \nof the reference vector. Fig. 3(d) shows the corresponding \nIOI ratio matrix with the optimal path surrounded by \ndashed lines. The vertical shift of the path indicates a de-\nletion where the 4th query IOI element should be mapped \nto the 4th and 5th reference IOI elements. \nIf there is no insertion or deletion in the query, each \nelement along the optimal path should have a value close \nto its preceding element. With insertion or deletion, then \nthe optimal path exhibits some specific behavior. There-\nfore our goal is to find the optimal path with minimal var-\niations between neighboring elements in the path, with \nspecial consideration for specific path behavior to ac-\ncommodate insertion and deletion. The variation between \nneighboring IOI ratio elements can be represented as the \ndeviation between 1 and the ratio of one IOI ratio element \nto the preceding modified IOI ratio element, which takes \ninto consideration the specific path behavior for accom-\nmodating insertion and deletion. The resulting recurrent \nequation for the optimum-value function j iD,for DP is \ns\nhown next:  ≤\n+ + − ≤=+ + −\notherwise  ,                      0) , min( 1 1   if  ,       /1\n,n m j i i r qMs j j i i\nj is \n \n(a) \n \n \n(\nb) \nFig. 2. Example of the normalization step of the shifted \nalignment algorithm: (a) R eference IOI vector and the \nshifted query IOI vectors. (b) IOI ratio matrix. \n \n ‧‧‧\n‧‧‧‧‧‧‧‧‧‧‧‧r1 r2 r3 r5\nq1 q2 q3 q4r6\nq5reference\nquery\nq1 q2 q3 q4 q5 query\nq1 q2 q3 q4 q5 query\nq1 q2 q3 q4 q5 queryr4 r7\nq2 /r1 q3 /r2 q4 /r3 q5 /r4 … … … …\nq1 /r1 q2 /r2 q3 /r3 q4 /r4 q5 /r5 … … …\n0 q1 /r2 q2/r3 q3 /r4 q4 /r5 q5 /r6 … …\n0\n0 q1 /r3 q2 /r4 q3 /r5 q4 /r6 q5 /r7 … \n(a)  \n \n(c) \n \n \n(b)  \n(d) \n \nFig. 3. Typical examples of the shifted alignment algo-\nr\nithm: (a) is an example where the query IOI vector has \nan insertion; (b) is the corresponding IOI ratio matrix; (c) \nis another example where the query IOI vector has a dele-\ntion; and (d) is the corresponding IOI ratio matrix. The \npath enclosed by dashed line in (b) and (d) represents the \noptimal DP path. \n \n 1 2 3 4 2\n1 2 3 21\n2reference\nqu\nery\n1 2 3 2 2 query\n1 2 3 2 2 query\n1 2 3 2 2 query2\n2\n2\n21 2 3 2 2\n1 2 3 41\n1reference\nqu\nery\n1 2 3 4 1 query\n1 2 3 4 1 query\n1 2 3 4 1 query\n2 1.5 0.67 0.5 1 0\n1 1 1 0.5 1 2\n0 0.5 0.67 0.75 1 2\n0 0 0.33 0.5 1.5 22 1.5 1.33 0.5 0 0\n1 1 1 2 0.5 0\n0 0.5 0.67 1.5 2 1\n0 0 0.33 1 1.5 4\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n291  \n \n (4) \nwhere Hi,j is the modified IOI ratio element with values in \nt\nhe set {Mi,j,  Mi+1,j+Mi,j,  Mi-1,j-1Mi,j/(Mi-1,j-1+Mi,j)}. The \na\nctual value of Hi,j depends on the backtrack index in the \na\nbove formula. Specifically, Hi,j will respectively be set \nt\no the first, second or third element in the set if Di,j takes \ni\nts value from the first, second or third row of the above \nrecurrent formula. The first row in the formula indicates \none-by-one sequential mapping of the query and the ref-\nerence IOI. The second row considers the case when the \nuser commits an insertion error by taking one note as two, \nwith the addition of a constant η1 as its penalty. The third \nr\now considers the case when the user commits a deletion \nerror by taking two notes as one, with the addition of a \nconstant η2 as its penalty. Fig. 4 illustrates these three \nc\nonditions with three allowable local paths in the DP ma-\ntrix D. Note that equation (4) does not consider some \nspecial cases of n-to-1 insertion or 1-to-n deletion when n \nis greater than 2. We can easily modify the equation in \norder to take such considerations, but we choose not to do \nso since these special cases rarely occur. Moreover, we \nwant to keep the formula simple for straightforward im-\nplementation and better efficiency. \nThe degree of similarity between two IOI vectors can \nthus be determined from the matrix D. The strategy com-\npares the elements in the corresponding positions of the \nlast non-zeros element in each row of the matrix M. For \nexample, if the DP matrix D is derived from the IOI ratio \nmatrix M in Fig. 2(b), we need to compare the next-to-\nlast element of the first row with the last element of the \nother rows in D.  The optimal cost is the minimal value of \nthese elements. The size of the DP matrix is \nmin(m,n)*(is+ie+1) , which is less than the size ( m*n) of \nthe DP algorithms in [6], [7], [9]. In addition, our algo-rithm can be easily extended to the QBT system with \n“anywhere” anchor positions by setting the ie to the \nl\nength of the reference IOI vector. \n4. PERFORMANCE EVALUATION \nTo evaluate the proposed method, we design 3 experi-\nments and compare the performance with that of the \nstate-of-art algorithm. The first experiment compares the \nrecognition rate with algorithms in MIREX QBT task. \nThe second experiment compares their computing speeds. \nThe third experiment demonstrates the robustness of the \nproposed method using a larger dataset. These experi-\nments are described in the following sub-sections. \n4.1 MIREX QBT Evaluation Task \nWe have submitted our algorithm to the 2012 MIREX \nQBT task [12], which involves two subtasks for symbolic \nand acoustic inputs, respectively. Because the onset de-\ntection of acoustic input is not the focus of this paper, the \nfollowing experiments only consider the case of queries \nwith symbolic input. There are 2 datasets of symbolic in-\nput, including Jang's dataset of 890 queries (with \ngroundtruth onsets to be used as the symbolic input) and \n136 monophonic MIDIs, and Hsiao's dataset of 410 sym-\nbolic queries and 143 monophonic MIDIs. The queries of \nboth datasets are all tapped from the beginning of the tar-\nget song. These datasets are published in 2009 and can be \ndownloaded from the MIREX QBT webpage. The top-10 \nhit rate and the mean reciprocal rank (MRR) are used as \nthe performance indices of a submitted QBT method. Fig. \n5 shows the performance of 5 submitted algorithms, with \n(a) and (b) are respectively the results of Jang's and \nHsiao’s datasets. Out of these five submissions, “HL” and \n“ML” do not have clear descriptions about their algo-\nrithms in the MIREX abstracts. Therefore, these 2 algo-\nrithms are not included in the experiments in section 4.2 \nand 4.3. “HAFR” is the implementation of [9], which \nclaimed that its results outperformed other submissions, \nincluding the methods of [5] and [6], in MIREX 2008. \nThe algorithm “CJ” is an improved version of [6]. The \nsubmission of \"SA\" is the proposed algorithm in this pa-\nper. \nAs shown in fig. 5(a), our algorithm outperforms al-\nmost all the other submissions except for the MRR in \nJang's dataset where our submission is ranked the second. \nIn fact, the MRR of our algorithm is only 0.3% lower \nthan that of \"CJ\". On the other hand, the top-10 hit rate of \nour submission is 0.3% higher than that of \"CJ\". So the \nperformances of “CJ” and “SA” are very close in this da-\ntaset. From fig. 5(b), it is obvious that our algorithm \nsimply outperforms all the other submission in both MRR \nand top-10 hit rate. As a whole, the proposed method ob-\ntains good results in MIREX QBT contest. \n \n\n\n+ − ⋅+⋅\n++ −++− +\n=\n− − − −− −\n− −− ++\n− +−−\n2\n2 , 1 , 1 , 1, 1 , 1\n2 11\n1 , 1, , 1\n1 11 ,,\n1\n1111\nm\nin\nηη\nj i j i j ij i j i\n,j ij ij i j i\n,j ij ij i\ni,j\ni,j\nH M MM\nM\nDHM MDHMD\nD \n \nFig. 4. Constrained DP paths \n \n Di,jDi,j-1\nDi+1,j-1Di-1,j-2 Deletion path\nI\nnsertion pathNormal path\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n292  \n \n \n4\n.2 Evaluation of Computation Efficiency \nIn this experiment, we want to compare the efficiency of \nseveral QBT algorithms. We implemented three submis-\nsions (including ours) to 2012 MIREX QBT tasks in C \nlanguage. The “ML” and “HL” algorithms were not in-\ncluded in this experiment due to the lack of clear descrip-\ntions about their algorithms in the MIREX abstracts. The \nexperiment was conducted on a PC with an AMD Athlon \n2.4GHz CPU and 1G RAM. Each algorithm was repeated \n10 times over Jang’s dataset to obtain the average compu-\nting time of a single query. The results are shown in Ta-\nble 1 which indicates that our algorithm is at least 3 times \nfaster than the other two algorithms. This is due to the \nfact that our algorithm has an efficient way of normaliza-\ntion for IOI vectors (as described in section 3), leading to \na smaller table for DP optimal path finding. \n \nAlgorithm  A vg. time (ms)  \nHAFR 421 \nCJ 213 \nSA 65 \nTable 1. Speed comparison of QBT algorithms \n \nFrom these two experiments, we can claim that our al-\ngorithm strike a good balance between the recognition \nrate and computation efficiency. \n4.3 Experiment with Larger Databases \nThe MIREX QBT datasets are well organized for QBT \nresearch. However, both datasets contain song databases \nof slightly more than 100 songs. These small database \nsizes lead to high accuracy for all submissions in MIREX \nQBT task. Therefore, we designed another experiment to \ndemonstrate how the performance varies with the dataset sizes. We collected 1000 MIDIs which are different from \nthe MIDIs in the MIREX QBT datasets. And we enlarge \nthe original databases by adding 250 noise MIDIs each \ntime, and evaluate the performance in both MRR and top-\n10 hit rate. \nFig. 6 shows the experimental results. As the number \nof noise MIDIs increases, the recognition rate of each al-\ngorithm gradually decreases. In Jang’s dataset of the fig. \n6(a), the top-10 hit rate of “SA” is the best among all al-\ngorithms (left subplot). However, the MRR of “SA” and \n“CJ” are very close and the value of one is slightly higher \nthan the other in different number of noise MIDIs (right \nsubplot). In fig. 6(b), our algorithm notably outperforms \nthe others in both top-10 hit rate (left subplot) and MRR \n(right subplot). It is interesting to note that the decay of \nthe top-10 hit rate of “SA” is slower than the others in \nboth datasets, especially in Jang’s dataset. This indicates \nthat our algorithm has better resistance to these noise \nMIDIs in top-10 hit rate. In both datasets, “SA” still had \n>85% top-10 rate and >60% MRR. Therefore we can \nconclude that the proposed method is more robust in deal-\ning with a large song database. \n5. CONCLUSION \nIn this paper, we have proposed a shifted-alignment algo-\nrithm for QBT by constructing an IOI ratio matrix, in \nwhich each element is the ratio of relative IOI elements \nof the query and a reference song. The similarity compar-\nison is based on DP to deal with possible insertions and \ndeletions of query IOI vectors. We evaluated the perfor-\nmance of the proposed method with two datasets. The \nexperimental results showed that our algorithm exhibited Algorithm Top 10 MRR \nHL 0.888 0.784 \nML 0.876 0.797 \nHAFR 0.876 0.770 \nCJ 0.908 0.840 \nSA 0.911 0.837 \n \n \n(a) Result 1: Jang’s dataset \n \nAlgorithm Top 10 MRR \nHL 0.900 0.687 \nML 0.851 0.711 \nHAFR 0.883 0.702 \nCJ 0.893 0.690 \nSA 0.929 0.745 \n \n \n(b) Result 2: Hsiao’s dataset \n \nFig. 5. Results of MIREX QBT evaluation task \n \n HLMLHAFRCJSA0.60.70.80.91\n  \nTop10\nMRR\nHLMLHAFRCJSA0.60.70.80.91\n  \nTop10\nMRR  \n(a) Result 1: Jang’s dataset \n \n  \n(b) Result 2: Hsiao’s dataset \n \nFig. 6. Results of the performance versus database sizes. \n(a) is the performance of top-10 hit rate (left subplot) and \nMRR (right subplot) using Jang’s dataset. (b) is the per-\nformance of top-10 hit rate (left subplot) and MRR (right \nsubplot) using Hsiao’s dataset. \n \n 0 250 500 750 10000.70.750.80.850.90.951\nNum. of noise MIDIsTop 10 (%)\n  \nHAFR\nCJ\nSA\n0 250 500 750 10000.50.60.70.80.91\nNum. of noise MIDIsMRR (%)\n  \nHAFR\nCJ\nSA\n0 250 500 750 10000.70.750.80.850.90.951\nNum. of noise MIDIsTop 10 (%)\n  \nHAFR\nCJ\nSA\n0 250 500 750 10000.50.60.70.80.91\nNum. of noise MIDIsMRR (%)\n  \nHAFR\nCJ\nSA\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n293  \n \na\nn overall better accuracy than other submissions to 2012 \nMIREX query-by-taping task. Moreover, the computation \ntime is at least 3 times faster than others. We also con-\nducted an experiment to demonstrate that our algorithm \nperforms better and more robustly than other existing \nQBT algorithms in the case of large databases. In particu-\nlar, our algorithm has a top-10 hit rate larger than 85% \nand MRR larger than 60% in both databases when the \nnumber of noise MIDIs is as high as 1000.  \nAlthough the proposed method performs well in the \nexperiments, the recognition rate still has room for further \nimprovement, especially in the case of “anywhere” anc-\nhor position, that is, the user is allowed to start tapping \nfrom anywhere in the middle of a song. From the experi-\nmental results, we can observe that each algorithm has its \nstrength and weakness in dealing with different queries \nand database songs. Therefore, one direction of our im-\nmediate future work is to find an optimal way to combine \nthese methods for better accuracy. \n6. ACKNOWLEDGEMENT \nThis study is conducted under the \"NSC 102-3114-Y-\n307-026 A Research on Social Influence and Decision \nSupport Analytics\" of the Institute for Information Indus-\ntry which is subsidized by the National Science Council. \n7. REFERENCES \n[1] R. B. Dannenberg, W. P. Birmingham, B. Pardo, N. \nHu, C. Meek, and G. Tzanetakis:  “A Comparative \nEvaluation of Search Techniques for Query-by-\nHumming Using the MUSART Testbed,” Journal of \nthe American Society for  Information Science and \nTechnology  (JASIST), vol. 58, no. 5, pp. 687–701, \n2007. \n[2] J.-S. Roger Jang, H. R. Lee, and M. Y. Kao:  \n“Content-based Music Retrieval Using Linear \nScaling and Branch-and-bound Tree Search,” IEEE \nInternational Conference on Multimedia and Expo , \npp. 289-292, 2001. \n[3] G. Eisenberg, J. Batke, and T. Sikora: “Beatbank - \nan MPEG-7 Compliant Query by Tapping System,” \n116th Convention of the Audio Engineering Society , \nBerlin, Germany, pp.189-192, May 2004. \n[4] G. Eisenberg, J. M. Batke, and T. Sikora: \n“Efficiently Computable Similarity Measures for \nQuery by Tapping System,” Proc. of the 7th Int. \nConference on Digital Audio Effects  (DAFx'04), \nOctober, 2004. \n[5] R. Typke, and A. W. Typke: “A Tunneling-Vantage \nIndexing Method for Non-Metrics,” 9th \nInternational Conference on Music Information \nRetrieval, Philadelphia, USA, pp683-688, \nSeptember 14-18, 2008 [6] J.-S. Roger Jang, H. R. Lee, C. H. Yeh: “Query by \nTapping A New Paradigm for Content-Based Music \nRetrieval from Acoustic input,” Second IEEE \nPacific-Rim Conference on Multimedia , pp590-597, \nOctober, 2001 \n[7] G. Peters, C. Anthony, and M. Schwartz: “Song \nSearch And Retrieval by Tapping,” Proceedings of \nAAAI'05 Proceedings of the 20th national \nconference on Artificial intelligence , pp. 1696-1697, \n2005 \n[8] G. Peters, D. Cukierman, C. Anthony, and M. \nSchwartz: “Online Music Search by Tapping,” \nAmbient Intelligence in Everyday Life , pages 178–\n197. Springer, 2006. \n[9] P. Hanna and M. Robine: “Query By Tapping \nSystem Based On Alignment Algorithm,” \nProceedings of the IEEE International Conference \non Acoustics, Speech, and Signal Processing  \n(ICASSP), pp. 1881-1884, 2009. \n[10] B. Kaneshiro, H. S. Kim, J. Herrera, J. Oh, J. Berger \nand M. Slaney. “QBT-Extended: An Annotated \nDataset of Melodically Contoured Tapped Queries,” \nProceedings of the 14th International Society for \nMusic Information Retrieval Conference , Curitiba, \nBrazil, November, 2013. \n[11] L. Wang: “MIREX 2012 QBSH Task: YINLONG’s \nSolution,”  Music Information Retrieval Evaluation \neXchange 2012. \n[12] The Music Information Retrieval Evaluation \neXchange evaluation task of query by tapping: \nhttp://www.music-ir.org/mirex/wiki/2012: \nQuery_by_Tapping \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n294"
    },
    {
        "title": "Multiple Viewpiont Melodic Prediction with Fixed-Context Neural Networks.",
        "author": [
            "Srikanth Cherla",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416944",
        "url": "https://doi.org/10.5281/zenodo.1416944",
        "ee": "https://zenodo.org/records/1416944/files/CherlaWG14.pdf",
        "abstract": "The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previ- ous work using this representation has predominantly re- lied on n-gram and variable order Markov models for mu- sic sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann ma- chines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is com- parable to that of models previously evaluated on the same task. We then combine the predictions of individual mod- els using an entropy-weighted combination scheme to im- prove the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the indi- vidual models in the combination.",
        "zenodo_id": 1416944,
        "dblp_key": "conf/ismir/CherlaWG14",
        "keywords": [
            "multiple viewpoints representation",
            "symbolic music data",
            "analysis and generation",
            "n-gram and variable order Markov models",
            "restricted Boltzmann machines",
            "neural network models",
            "fixed-length sequences",
            "pitch prediction",
            "entropy-weighted combination",
            "overall prediction performance"
        ],
        "content": "MULTIPLE VIEWPOINT MELODIC PREDICTION WITH\nFIXED-CONTEXT NEURAL NETWORKS\nSrikanth Cherla1;2, Tillman Weyde1;2and Artur d’Avila Garcez2\n1Music Informatics Research Group, Department of Computer Science, City University London\n2Machine Learning Group, Department of Computer Science, City University London\nfsrikanth.cherla.1, t.e.weyde, a.garcezg@city.ac.uk\nABSTRACT\nThe multiple viewpoints representation is an event-based\nrepresentation of symbolic music data which offers a means\nfor the analysis and generation of notated music. Previ-\nous work using this representation has predominantly re-\nlied onn-gram and variable order Markov models for mu-\nsic sequence modelling. Recently the efﬁcacy of a class\nof distributed models, namely restricted Boltzmann ma-\nchines, was demonstrated for this purpose. In this paper,\nwe demonstrate the use of two neural network models which\nuse ﬁxed-length sequences of various viewpoint types as\ninput to predict the pitch of the next note in the sequence.\nThe predictive performance of each of these models is com-\nparable to that of models previously evaluated on the same\ntask. We then combine the predictions of individual mod-\nels using an entropy-weighted combination scheme to im-\nprove the overall prediction performance, and compare this\nwith the predictions of a single equivalent model which\ntakes as input all the viewpoint types of each of the indi-\nvidual models in the combination.\n1. INTRODUCTION\nWe are interested in the computational modelling of melo-\ndies available in symbolic music data formats such as MIDI\nand KERN. For this purpose, we chose to work with a rep-\nresentation of symbolic music ﬁrst proposed in [9] in rela-\ntion to multiple viewpoints for music prediction (which we\nrefer to here as the “multiple viewpoints representation”).\nThe multiple viewpoints representation is an event-based\nrepresentation extracted from symbolic music data where\na given piece of music is decomposed into parallel streams\nof features, known as viewpoint types. Each viewpoint type\nis either a directly observable musical dimension such as\npitch andnote duration, or an abstract one derived from\nthem such as inter-onset interval orpitch contour. In or-\nder to analyse musical structure using this representation,\none can train a machine learning model on sequences of\nc\rSrikanth Cherla1;2, Tillman Weyde1;2and Artur\nd’Avila Garcez2.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Srikanth Cherla1;2, Tillman Weyde1;2\nand Artur d’Avila Garcez2. “Multiple Viewpoint Melodic Prediction with\nFixed-Context Neural Networks”, 15th International Society for Music\nInformation Retrieval Conference, 2014.viewpoint types and apply it to tasks such as music gener-\nation [6] and classiﬁcation [3, 7]. This representation has\nalso been the focus of more recent work related to music\ncognition [14,17]. The novelty of this approach is in its ex-\ntension of previous work in language modelling to music\nwith an information theoretic backing which facilitates an\nobjective evaluation of models for music prediction. Ap-\nproaches based on information theory have been of interest\nin musicology to understand structure and meaning in mu-\nsic in terms of its predictability [10, 11, 13].\nIn the original work on multiple viewpoints [9] and that\nwhich followed [15, 21], Markov models were exclusively\nemployed for music modelling using this framework. While\nthis is a reasonable choice, Markov models are often faced\nwith a problem related to data sparsity known as the curse\nof dimensionality [2]. This refers to the exponential rise in\nthe number of model parameters to be estimated with the\nlength of the modelled sequences. Models which employ\ndistributed architectures such as neural networks tend to\navoid this problem, as they do not require enumerating all\nstate transition probabilities, but rather the weights of the\nnetwork encode only those dependencies necessary to min-\nimize prediction error. It was demonstrated more recently\nin [4] how a distributed model — the restricted Boltzmann\nmachine, is a suitable alternative in this context. It was also\nsuggested in [8] that neural networks might be suitable al-\nternatives to n-gram models for music modelling with mul-\ntiple viewpoints but no actual research in this direction has\nensued.\nIn this paper, we ﬁrst present two neural networks for\nmodelling sequences of musical pitch. The ﬁrst is a sim-\nple feed-forward neural network [20], and the second is\nthe musical extension of the Neural Probabilistic Language\nModel [2] — a deeper feed-forward network with an added\nweight-sharing layer between the input and hidden lay-\ners. The latter was originally proposed for learning dis-\ntributed representations of words in language modelling.\nBoth models predict a probability distribution over the pos-\nsible values of the next pitch given a ﬁxed-length context\nas input. Their predictive performance is comparable to or\nbetter than previously evaluated melody prediction mod-\nels in [4, 16]. The second network is further extended to\nmake use of additional viewpoint types extracted from the\ncontext, as inputs for the same task of predicting musi-\ncal pitch. We then combine the predictions of individual\nmodels with different viewpoint types as their respective\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n101inputs using an entropy-weighted combination scheme to\nimprove the overall prediction performance, and compare\nthis with the predictions of a single model which takes as\ninput all the viewpoint types of each of the individual mod-\nels in the combination.\nWe begin with an overview of the multiple viewpoints\nrepresentation in Section 2. This is followed by a descrip-\ntion of the two neural networks which are used with this\nrepresentation, in Section 3. Section 4 presents an evalua-\ntion of the predictive performance of the two models along\nwith a comparison to previous work. Finally, directions for\nfuture research are outlined in Section 5.\n2. MULTIPLE VIEWPOINT SYSTEMS\nIn order to explain music prediction with multiple view-\npoints, the analogy to natural language is used here. In\nstatistical language modelling, the goal is to build a model\nthat can estimate the joint probability distribution of subse-\nquences of words occurring in a language L. A statistical\nlanguage model (SLM) can be represented by the condi-\ntional probability of the next word wTgiven all the previ-\nous ones [w1;:::;w (T\u00001)](written here as w(T\u00001)\n1 ), as\nP(wT\n1) =TY\nt=1P(wtjw(t\u00001)\n1): (1)\nThe most commonly used SLMs are n-gram models,\nwhich rely on the simplifying assumption that the proba-\nbility of a word in a sequence depends only on the imme-\ndiately preceding (n\u00001)words [12]. This is known as the\nMarkov assumption, and reduces (1) to\nP(wT\n1) =TY\nt=1P(wtjw(t\u00001)\n(t\u0000n+1)): (2)\nFollowing this approach, musical styles can be inter-\npreted as vast and complex languages [9]. In predicting\nmusic, one is interested in learning the joint distribution\nofmusical event sequencessT\n1in a musical language S.\nMuch in the same way as an SLM, a system for music pre-\ndiction models the conditional distribution p(stjs(t\u00001)\n1), or\nunder the Markov assumption p(stjs(t\u00001)\n(t\u0000n+1)). For each\nprediction, context information is obtained from the events\ns(t\u00001)\n(t\u0000n+1)immediately preceding st. Musical events have a\nrich internal structure and can be expressed in terms of di-\nrectly observable or derived musical features such as pitch,\nnote duration, inter-onset interval, or a combination of two\nor more such features. The framework of multiple view-\npoint systems for music prediction [9] was proposed in or-\nder to efﬁciently handle this rich internal structure of music\nby exploiting information contained in these different mu-\nsical feature sequences, while at the same time limiting the\ndimensionality of the models using these features. In the\ninterest of brevity, we limit ourselves to an informal discus-\nsion of multiple viewpoint systems for monophonic music\nprediction and refer the reader to [9] for a more detailed\nexplanation.A musical event srefers to the occurrence of a note in a\nmelody. A viewpoint type (or simply type)\u001crefers to any\nof a set of musical features that describe an event. The do-\nmain of a type, denoted by [\u001c]is the set of possible values\nof that type. A basic type is a directly observable or given\nfeature such as pitch, note duration, key-signature ortime-\nsignature. A derived type can be derived from any of the\nbasic types or other derived types. Two or more types can\nbe “linked” by taking the Cartesian product of their respec-\ntive domains, thus creating a linked viewpoint type. A mul-\ntiple viewpoints system (MVS) is a set of models, each of\nwhich is trained on subsequences of one type, whose indi-\nvidual predictions are combined in some way to inﬂuence\nthe prediction of the next event in a given event sequence.\nGiven a context s(t\u00001)\n(t\u0000n+1)and an event st, each viewpoint \u001c\nin an MVS must compute the probability p\u001c(stjs(t\u00001)\n(t\u0000n+1)).\nIn order to input the viewpoint type sequences to the\nneural network models, we ﬁrst convert each input type\nvalue into a binary one-hot encoding. When a context\nevent is missing or undeﬁned, each element of the vector\nis initialized to 1=jSj. When there is more than one input\ntype, one-hot vectors corresponding to all the input types\nfor a musical event are concatenated to obtain an input vec-\ntor for that event. As we are dealing with models of ﬁxed\ncontext-length l, the ﬁnal input feature vector input to the\nmodel is a concatenation of lsuch vectors. In doing so, we\nare effectively bypassing the need to compute a Cartesian\nproduct to link viewpoint types before using them as input\nto a single model which has been the practice when using\nn-gram and variable order Markov models.\nEach model in an MVS relies on a different source of in-\nformation (its respective input types) to make a prediction\nabout the target viewpoint type. The accuracy of the pre-\ndiction depends on how informative these input types are\nof the target type. It is possible to combine the information\nprovided by different input types for possibly better pre-\ndictive performance. Here, we consider two ways of doing\nthis - implicitly in a single model which is trained using a\nset of input types, and explicitly by combining the prob-\nability distributions of multiple models, each of which is\ntrained separately on a mutually exclusive subset of these\ninput types. While the former is only a special case of what\nhas been described so far, we provide an explanation of the\nlatter below in Section 2.1.\n2.1 Combining Multiple Models\nIt was demonstrated in [9, 15] that an entropy-weighted\ncombination of the predictions of two or more n-gram or\nvariable order Markov models typically results in ensem-\nbles with better predictive performance than any of the in-\ndividual models. As it is the predicted distributions which\nare combined, this approach is independent of the types of\nmodels involved. Here, we brieﬂy describe two approaches\nfor creating such ensembles. Let Mbe a set of models and\npm(s)be the probability assigned to symbol s2[\u001ctgt]by\nmodelm, where [\u001ctgt]is the domain of the target type.\nThe ﬁrst approach involves taking a weighted arithmetic\nmean of their respective predictions. This is the mixture-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n102of-experts combination, and is deﬁned as\np(s) =P\nm2Mwmpm(s)P\nm2Mwm\nwhere each of the weights wmdepends on the entropy of\nthe distribution generated by the corresponding model m\nin the combination such that greater entropy (and hence\nuncertainty) is associated with a lower weight [5]. The\nweights are given by the expression wm=Hrel(pm)\u0000b,\nwhere the relative entropy Hrel(pm)is\nHrel(pm) =(\nH(pm)=Hmax(pm);ifHmax([\u001ctgt])>0\n1; otherwise\nThe best value of the bias bis determined through cross-\nvalidation. The quantities HandHmax are respectively\nthe entropy of the prediction and the maximum entropy of\npredictions over the symbol space [\u001ctgt], and are deﬁned as\nH(p) =\u0000X\ns2[\u001ctgt]p(s) log2p(s): (3)\nHmax(p) = log2jSj:\nwherep(s2[\u001ctgt]) =p(\u001f=s)is the probability mass\nfunction of a random variable \u001fdistributed over the dis-\ncrete alphabet [\u001ctgt]such that the individual probabilities\nare independent and sum to 1.\nThe second combination method — product-of-experts,\nis computed similarly as the weighted geometric mean of\nthe probability distributions. This is given by\np(s) =1\nR Y\nm2Mpm(s)wm! 1P\nm2Mwm\nwhereRis a normalisation constant which ensures that the\nresulting distribution over Ssums to unity. The weights\nwmin this case are obtained in the same manner as for\nthe mixture-of-experts case. It was observed in a previous\napplication of these two combination methods to melody\nmodelling [15], that product-of-experts resulted in a greater\nimprovement in predictive performance.\n3. FIXED-CONTEXT NEURAL NETWORKS\nIn this section, we provide a brief overview of the two\nﬁxed-context neural network models which we employed\nfor the task of predicting the pitch of the next note in a\nmelody, given a viewpoint type context which leads up to\nit. These are (1) a feed-forward neural network, and (2)\na neural probabilistic melody model. The key difference\nbetween the two is the presence of an additional weight-\nsharing layer in the latter which transforms the binary rep-\nresentation of the viewpoint types into lower-dimensional\nreal-valued vectors before passing these on as inputs to a\nfeed-forward network (much like the former).: :\n: y\n: :\n:h\n: :\n: : :\n: : :\n: xW(0)W(1)\n(a) Feed-forward Neural Network\n: :\n: y\n: :\n:h\n: :\n: : :\n: : :\n: v\n: :\n: : :\n: : :\n: xW(c)W(c)W(0)W(1)\n(b) Neural Probabilistic Melody Model\nFigure 1: The two models employed for multiple view-\npoint melodic prediction in this paper (biases ignored in\nthe illustration). A concatenation of the ﬁxed-length input\ntype context is presented to each model in its visible layer\nand the predictions are made in the output layer.\n3.1 Feed-forward Neural Network\nIn its simplest form, a feed-forward neural network (Fig-\nure 1) consists of an input layer x2Rn, a hidden layer\nh2Rmand an output layer y2Rl. The input layer\nis connected to the hidden layer by a weight-matrix W(0)\nand likewise, the hidden layer to the output layer by a ma-\ntrixW(1). Each unit in the hidden layer typically applies\na non-linear function to the input it receives from the layer\nbelow it. Similarly, each unit of the output layer applies a\nfunction to the input it receives from the hidden layer im-\nmediately preceding it. In a network with a single hidden\nlayer, this happens according to the following equations\nu(0)=b(0)+W(0)x (4)\nh=f(0)(u) (5)\nu(1)=b(1)+W(1)h (6)\ny=f(1)(v) (7)\nwhere b(0)andb(1)are the hidden and output layer biases,\nf(0)andf(1)are functions applied to the input received\nby each node in the hidden and output layers respectively.\nThus, for a given input x, the output yis calculated as\ny=f(1)(b(1)+W(1)\u0001f(0)(b(0)+W(0)x)) (8)\nIn the present case, f(0)is the logistic sigmoid func-\ntion andf(1)is the softmax function. The network can\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n103be trained in a supervised manner using the backpropaga-\ntion algorithm [20]. This algorithm applies the chain rule\nof differentiation to propagate the error between the target\noutput and the output of the model backwards into the net-\nwork, and use these derivatives to appropriately update the\nmodel parameters (the network weights and biases).\n3.2 Neural Probabilistic Melody Model\nNext we consider the neural probabilistic melody model\n(NPMM), which was originally introduced in [2] as a lan-\nguage model for word sequences. It consists of a feed-\nforward network such as the one described in Section 3.1,\nwith an additional embedding layer below it (Figure 1).\nThis model takes as input a concatenation of binary view-\npoint type vectors (cf. Section 3) which represent a ﬁxed-\nlength context. The ﬁrst layer of the network maps each\nof these sparse binary vectors to lower-dimensional dense\nreal-valued vectors which make up the input layer of what\nis essentially a feed-forward network above it. This map-\nping is determined by a shared weight matrix W(c)which\nis learned from data, and is given by\nv=W(c)x: (9)\nThe hidden layer in the case of the NPMM consists of\nhyperbolic-tangent activation units. The output layer con-\ntains softmax units. The model is trained with backprop-\nagation using gradient descent as in the case of a standard\nfeed-forward neural network.\n4. EVALUATION\nThe ﬁrst goal of this paper is to demonstrate the suitabil-\nity of ﬁxed-context neural networks for multiple viewpoint\nmelodic prediction. To this end, we compare the two mod-\nels described in Section 3 with variable-order Markov mod-\nels (VOMMs) and restricted Boltzmann machines (RBMs).\nIt was observed that the predictive performance of each of\nthe neural network models is either comparable to or bet-\nter than that of the best VOMMs of both bounded and un-\nbounded order [16], while slightly worse than the RBM\nof [4] (Figure 2). Second, we wish to compare the predic-\ntions of a single neural network which uses multiple input\ntypes with that of an ensemble of networks with smaller\ninput dimensions, each of which uses a subset of the input\ntypes of the former, and combined with the entropy-based\nweighting scheme described in 2.1. We found that, while\nthe addition of viewpoint types does improve the predic-\ntive performance in both cases, that of the single network\nis slightly worse than the ensemble (Figure 3). Moreover,\nthe extent of this improvement diminishes with an increase\nin context length.\n4.1 Dataset\nEvaluation was carried out on a corpus of monophonic\nMIDI melodies that cover a range of musical styles. It\nconsists of 4datasets - Bach chorale melodies, and folk\nmelodies from Canada, China and Germany, with a total0 2 4 6 82:62:72:82:9\nContext lengthCross Entrop\nyV OMM (b)\nV OMM (u)\nRBM\nFNN\nNPMM\nFigure 2: Comparison between the predictive perfor-\nmances of the best bounded and unbounded variable-order\nMarkov models (VOMM(b) and VOMM(u) respectively),\nthe best restricted Boltzmann machine (RBM), the feed-\nforward neural network (FNN) and the neural probabilistic\nmelody model (NPMM) averaged over the datasets.\nof37;229musical events. These were also used to eval-\nuate RBMs and variable order Markov models for music\nprediction in [4, 16]. To facilitate a direct comparison, the\nmelodies are not transposed to a default key.\n4.2 Evaluation Measure\nIn order to evaluate the proposed prediction models, we\nturn to a previous study of Markov models for music pre-\ndiction in [16]. There, cross entropy was used to measure\nthe information content of the models. This is a quantity\nrelated to entropy (3). The value of entropy, with reference\nto a prediction model, is a measure of the uncertainty of its\npredictions. A higher value reﬂects greater uncertainty. In\npractice, one rarely knows the true probability distribution\nof the stochastic process and uses a model to approximate\nthe probabilities in (3). An estimate of the goodness of\nthis approximation can be measured using cross entropy\n(Hc) which represents the divergence between the entropy\ncalculated from the estimated probabilities and the source\nmodel. This quantity can be computed over all the subse-\nquences of length nin the test dataDtest, as\nHc(pmod;Dtest) =\u0000P\nsn\n12Dtestlog2pmod(snjs(n\u00001)\n1 )\njDtestj\n(10)\nwherepmodis the probability assigned by the model to the\nlast pitch in the subsequence given its preceding context.\nCross-entropy approaches the true entropy as the number\nof test samples (jD testj) increases.\n4.3 Model Selection\nDifferent neural network conﬁgurations were evaluated by\na grid search over the learning rate \u0011=f0:05; 0:1g, the\nnumber of hidden units nhid=f25;50;100;200;400g,\nnumber of embedding units nemb=f10;20g (only for\nthe NPMM), and weight decay wdecay =f0:0000; 0:0001;\n0:0005g. Each model was trained using mini-batch gradi-\nent descent up to a maximum of 1000 epochs with a batch\nsize of 100samples. Early-stopping [19] and weight-decay\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n104were also incorporated to counter overﬁtting. The momen-\ntum parameter \u0016, was set to 0:5during the ﬁrst ﬁve epochs\nand then increased to 0:9for the rest of the training. Each\nmodel was evaluated with 10-fold cross-validation, with\nfolds identical to those used in [4, 16] for the sake of com-\nparison.\n4.4 Model Comparison\nWe carried out a comparison between the predictive perfor-\nmance of the two neural network models presented here,\nand models previously evaluated on the same datasets [4,\n16]. It is to be noted that, since neither of our models is\nupdated online during prediction, the comparison with the\nvariable order Markov models of [16] is limited to their\nbest performing Long-Term Models. These are of order\nbound 2 and unbounded order (labelled there as C*I). It is\nevident from Figure 2 that both the neural network models\nare able to take advantage of information in longer contexts\nthan the bounded order n-gram models. This is also a fea-\nture of the RBM, whose best case of context-length 5out-\nperforms the rest of the models in the plot. The slight de-\nterioration in the performance of the feed-forward network\nfor longer contexts is possibly due to poor optimization\nof its parameters. This is considering the fact that weight-\ndecay and early-stopping were implemented in the training\nalgorithm to prevent overﬁtting. While it was not possible\nto incorporate further steps for better parameter optimiza-\ntion in this paper, the results are still illustrative of the net-\nworks’ suitability at the given task and the improvement in\nperformance with context consistent with each other and\nwith that of the RBMs. Possible optimizations have been\nleft as future work, and will be discussed in Section 5.\n4.5 Model Combination\nIn order to evaluate the combination of viewpoint types, we\nselected one type which is related to the “what” in music\n— scale-degree (intfref ), and another which is related to\nthe “when” — inter-onset interval (ioi), from the several\npossible choices that exist. Furthermore, this experiment\nwas performed using the NPMM and only on the Chinese\nfolk melody dataset for the purpose of illustration, with\nthe assumption that a similar trend would be observed with\nthe other model and datasets. As our target viewpoint type\ni.e. the one being predicted, is musical pitch (seqpitch ),\nthe ﬁrst model has the input types seqpitch andintfref\nand the second one seqpitch andioi. The additional view-\npoints are incorporated as explained in Section 2. The pre-\ndictions of these two models are combined explicitly using\nthe mixture- and product-of-experts schemes. On the other\nhand, the implicit combination of these two is a single\nmodel whose input types are seqpitch ,intfref andioi.\nFigure 3 compares the predictions of the pitch-only version\nof the NPMM and the three models using the additional in-\nput types. It can be seen that each of these three models has\na better predictive performance than its pitch-only coun-\nterpart, thus conﬁrming the relevance of the added view-\npoint types to musical pitch prediction. Both the mixture-\nand product-of-experts combination schemes (seqpitch \u00020 2 4 6 82:83\nContext lengthCross Entrop\nyseqpitch\nseqpitch \u0002intfref \u0002ioi\nseqpitch \u0002(intfref +mioi)\nseqpitch \u0002(intfref +pioi)\nFigure 3: Comparison between the predictive perfor-\nmances, on the Chinese folk melody dataset, of the pitch-\nonly NPMM, its extension which uses the intfref andioi\ntypes as additional input, and ensembles each of which\ncombines two models of input types (a) seqpitch andintfref\n(b)seqpitch andioiusing the mixture (+ m) and product\n(+p) combination schemes.\n(intfref +mioi) andseqpitch\u0002(intfref +pioi) re-\nspectively in the plot) result in very similar predictive per-\nformance, with the latter working only slightly better for\nshorter context-lengths of 1,2and3. Moreover, both these\nexplicit combinations of viewpoint types perform better\nthan the single implicit combination of types (seqpitch \u0002\nintfref\u0002ioiin the plot). One will, however, notice\nthat the cross entropy of the predictions slightly worsens at\nlonger context-lengths, and that the discrepancy between\nthe implicit and explicit combinations gradually increases\nin these cases. As mentioned earlier, we attribute this to\nthe optimization of the network parameters, which is to be\ndealt with in future work.\n5. CONCLUSIONS & FUTURE WORK\nThe two neural network models for melodic prediction pre-\nsented here have been found to have a predictive perfor-\nmance comparable to or better than previously evaluated\nVOMMs, but slightly worse than that of RBMs. Predic-\ntive performance can be further improved by the addition\nof viewpoint types to the same model, or by combining\nmultiple models using an entropy-weighted combination\nscheme. In our experiments, the latter tended to be better.\nOne open issue that remains is the parameter optimiza-\ntion in the two networks presented here. It was observed\nthat, particularly when the input layer of a network is large\nand the dataset relatively small, the predictive performance\ndoes not improve as expected with context-length and the\naddition of viewpoint types. We note here that the re-\nsults presented have been generated with models imple-\nmented in-house1for use with the Python machine learn-\ning library scikit-learn [18], and were thus limited in the\nvarious initialization and optimization strategies used in\ntheir learning algorithms. We also suspect this to be the\nreason for the limited success of the NPMM which ex-\nhibited relatively more promising results in its language\n1Code available upon request.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n105modelling application in [2]. Many more measures to im-\nprove generalization and overall prediction accuracy (such\nas dropout, different weights initialization strategies and\nlayer-wise pre-training) have been suggested in [1]. Incor-\nporating these measures (or using an existing neural net-\nwork library which does) can further improve the results.\nApart from this, there are three other aspects which are\nof immediate interest to us. The ﬁrst is the incorporation\nof a short-term element in the prediction model which up-\ndates its parameters as data is presented to it, and has been\nshown to result in improved prediction performance and\nhuman-like predictions [15]. Secondly, while the num-\nber of parameters of the ﬁxed-context models presented\nhere increases linearly with the context-length (assuming\na ﬁxed number of hidden units), we are at present experi-\nmenting with recurrent networks where this problem does\nnot arise due to their recurrent connections. And ﬁnally,\nthe extension of the said models to polyphonic multiple\nviewpoints representations is also an open issue at the mo-\nment which we hope to address in the future.\n6. ACKNOWLEDGEMENTS\nSrikanth Cherla is supported by a PhD studentship from\nCity University London. The authors would like to thank\nMarcus Pearce for his valuable advice, and the anonymous\nreviewers for their feedback on the submission.\n7. REFERENCES\n[1] Yoshua Bengio. Practical Recommendations for\nGradient-Based Training of Deep Architectures. In\nNeural Networks: Tricks of the Trade, pages 437–478.\n2012.\n[2] Yoshua Bengio, Rejean Ducharme, Pascal Vincent,\nand Christian Jauvin. A Neural Probabilistic Lan-\nguage Model. Journal of Machine Learning Research,\n3:1137–1155, 2003.\n[3] Srikanth Cherla, Artur d’Avila Garcez, and Tillman\nWeyde. A neural probabilistic model for predicting\nmelodic sequences. In International Workshop on Ma-\nchine Learning and Music, 2013.\n[4] Srikanth Cherla, Tillman Weyde, Artur d’Avila Garcez,\nand Marcus Pearce. A distributed model for multiple\nviewpoint melodic prediction. In International Society\nfor Music Information Retrieval Conference, pages 15–\n20, 2013.\n[5] Darrell Conklin. Prediction and entropy of music.\n1990.\n[6] Darrell Conklin. Music generation from statistical\nmodels. In AISB Symposium on Artiﬁcial Intelligence\nand Creativity in the Arts and Sciences, pages 30–35,\n2003.\n[7] Darrell Conklin. Multiple viewpoint systems for mu-\nsic classiﬁcation. Journal of New Music Research,\n42(1):19–26, 2013.[8] Darrell Conklin and John G Cleary. Modelling and\ngenerating music using multiple viewpoints. 1988.\n[9] Darrell Conklin and Ian H Witten. Multiple viewpoint\nsystems for music prediction. Journal of New Music\nResearch, 24(1):51–73, 1995.\n[10] Greg Cox. On the relationship between entropy and\nmeaning in music: An exploration with recurrent neu-\nral networks. Proceedings of the 32nd Annual Cogni-\ntive Science Society. Austin TX: CSS, 2010.\n[11] David Brian Huron. Sweet anticipation: Music and the\npsychology of expectation. MIT press, 2006.\n[12] Christopher D Manning and Hinrich Sch ¨utze. Founda-\ntions of statistical natural language processing. MIT\npress, 1999.\n[13] Leonard B Meyer. Meaning in music and information\ntheory. The Journal of Aesthetics and Art Criticism,\n15(4):412–424, 1957.\n[14] Diana Omigie, Marcus T Pearce, Victoria J\nWilliamson, and Lauren Stewart. Electrophysio-\nlogical correlates of melodic processing in congenital\namusia. Neuropsychologia, 2013.\n[15] Marcus T Pearce. The construction and evaluation of\nstatistical models of melodic structure in music per-\nception and composition. PhD thesis, City University\nLondon, 2005.\n[16] Marcus T Pearce and Geraint A Wiggins. Improved\nmethods for statistical modelling of monophonic mu-\nsic.Journal of New Music Research, 33(4):367–385,\n2004.\n[17] Marcus T Pearce and Geraint A Wiggins. Expectation\nin melody: The inﬂuence of context and learning. Mu-\nsic Perception, 23(5):377–405, June 2006.\n[18] Fabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. Scikit-learn: Machine learning in\npython. The Journal of Machine Learning Research ,\n12:2825–2830, 2011.\n[19] Lutz Prechelt. Early Stopping But When? In Neural\nNetworks: Tricks of the Trade, pages 55–69. 2012.\n[20] David E Rumelhart, Geoffrey E Hinton, and\nRonald J Williams. Learning representations by back-\npropagating errors. Cognitive modeling, 1988.\n[21] Raymond P Whorley. The Construction and Evalua-\ntion of Statistical Models of Melody and Harmony.\nPhD thesis, 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n106"
    },
    {
        "title": "Improving Rhythmic Transcriptions via Probability Models Applied Post-OMR.",
        "author": [
            "Maura Church",
            "Michael Scott Cuthbert"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416752",
        "url": "https://doi.org/10.5281/zenodo.1416752",
        "ee": "https://zenodo.org/records/1416752/files/ChurchC14.pdf",
        "abstract": "Despite many improvements in the recognition of graph- ical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self- similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and move- ments (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future re- finements and applications to pitch correction that could further improve the accuracy of OMR systems.",
        "zenodo_id": 1416752,
        "dblp_key": "conf/ismir/ChurchC14",
        "keywords": [
            "Optical Music Recognition",
            "Inaccuracies in resultant score",
            "Rhythmic errors",
            "Time-consuming to fix",
            "Rhythmic self-similarity",
            "Prior probabilities",
            "Commercial OMR system",
            "Correct rhythmic errors",
            "Contradiction between notes",
            "Underlying time signature"
        ],
        "content": "IMPROVING RHYTHMIC TRANSCRIPTIONS  \nVIA PROBABILITY MODELS APPLIED POST -OMR \nMaura Church  Michael Scott Cuthbert \nApplied Math, Harvard University \nand Google Inc. \nmaura.church@gmail.com   Music and Theater Arts \nM.I.T. \ncuthbert@mit.edu  \nABSTRACT  \nDespite many improvements in the recognition of grap h-\nical elements, even the best implementations of Optical \nMusic Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositi ons \nrepeat rhythms between parts and at various places throughout the score. Information about rhythmic self -\nsimilarity, however, has not previously been used in \nOMR systems.  \nThis paper describes and implements methods for using \nthe prior probabilities for rhythmic similarities in scores \nproduced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post -correction results to \nhand- encod ed scores of 37 polyphonic pieces and mov e-\nments (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19 % \n(min: 2%, max: 36%).  \nThe paper includes a public release of an implementation \nof the model  in \nmusic21  and also suggests future re-\nfinements and applications to pitch correction that could \nfurther improve the accuracy of OMR systems.  \n1. INTRODUCTION  \nMillions of paper copies of musical scores are found in \nlibraries and archival collections and hundreds of tho u-\nsands of scores have already been scanned  as PDFs  in \nrepositories such as IMSLP  [5]. A scan of a score cannot, \nhowever, be searched or manipulate d musically, so Opt i-\ncal Music Recognition (OMR) software is necessary to transform an image  of a score  into symbol ic formats (see \n[7] for a recent synthesis of relevant work and extensive \nbibliography; only the most relevant citations from this work are included here). Pro jects such as Peachnote [10] \nshow both the feasibility of recognizing large bodies of scores and also the limitations that errors introduce, par-ticularly in searches such as chord progressions that rely on accurate recognition of multiple musical staves.  \nUnderstandably, the bulk of OMR research has focused on improving the algorithms for recognizing graphical  \nprimitives and converting them to musical objects based \non their relationships on the staves. Improving score a c-\ncuracy using musical knowledge (models of tonality, m e-\nter, form) has largely been relegated to “future work” se c-\ntions and when discus sed has focused on localized stru c-\ntures such as beams and measures and requires access to \nthe “guts” of a recognition engine (see Section 6.2.2 in [9]). Improvements to score accuracy based on the output \nof OMR systems using multiple OMR engines have been suggested [ 2] and when implemented  yielded  results that \nwere more accurate  than individual OMR engines, though  \nthe results were  not statistically significant compared to \nthe best commercial systems [1 ]. Improving the accuracy \nof an OMR score using musical knowledge and a single engine’s output alone remains an open field.  \nThis paper proposes using rhythmic repetition and sim i-\nlarity  within a score to create a model where measure-\nlevel metrical errors can be fixed using correctly reco g-\nnized (or at least metrically consistent) measures found in other places in the same score, creating a self -healing \nmethod for post- OMR processing conditioned on proba-\nbilities based on rhythmic similarity and statistics of symbolic misidentification.  \n2. PRIOR PROBABILITIES OF DISTA NCE  \nMost Western musical scores, excepting those in certain \npost-common practice styles (e.g., Boulez, Cage), use \nand gain cohesion through a limited rhythmic vocabulary \nacross measures . Rhythms are often repeated immediat e-\nly or after a fixed distance (e.g ., after a 2, 4, or 8 measure \ndistance ). In a multipart score, different instruments o f-\nten employ the same rhythms in a measure or throughout a pas sage. From a parsed musical score, it is not difficult \nto construct a hash of the sequence of durations in ea ch \nmeasure of each part (hereafter simply called “measure”; “measure s tack” will refer to measures sounding  together  \nacross all parts ); if grace notes are ha ndled sepa rately, \nand i nterior voi ces are fla ttened (e.g., using  the m usic21 \nchordify method)  then has h-key co llisions will only \noccur in the rare cases where two graphically di stinct \nsymbols equate to the same length in quarter notes (such \nas a do tted-triplet eighth note and a normal eighth).  \n © Maura Church,  Mi chael Scott Cuthbert.  \nLicensed under a Creative Commons Attribution 4.0 International L i-\ncen\nse (CC BY 4.0). Attribution:  Maura Church and Michael Scott \nCuthbert. “ Improving Rhythmic Tran scrip tions via Probability Models \nApplied Post -OMR”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n643  \n \nWithin each part, the prior probability that a measure m0 \nwill have the same rhythm as the measure n bars late r (or \nearlier) can be computed (the prior -based -on-distance, or \nPrD). Similarly, the prior probability that , within a \nmeasure stack , part p will have the same rhythm as part q \ncan also be computed ( the prior -based -on-part, or PrP).  \nFigure 1 show s these two priors for the violin I and viola \nparts of the first movement of Mozart K525 ( Eine kleine \nNachtmusik ). Individual parts have their own characteri s-\ntic shapes; for instance, the melodic violin I ( top left ), \nshows less rhythmic similarity overall than the viola  \n(bot. left ). This difference results from the greater rhyt h-\nmic variety of the violin I part compared to the viola \npart. M oments of large- scale repetition such as b etween \nthe exposition and recapitulation , however,  are easily \nvisible as spikes in the PrD graph  for violin I. (Possible \nrefinements to the mo del taking into account localized \nsimilarities are given at the end of this paper.) The PrP graphs (right ) show that both parts are more similar to \nthe vio loncello part than to any other part. However, the \nviola is more similar to the cello (and to violin II) that \nviolin I is to any other part.  \n  \n  \nFigure 1. Priors based on distance (l. in measure separa-\ntion) and part (r. ) for the violin I ( top) and viola (bot.) \nparts in Mozart, K525.  \n3. PRIOR PROBABILITIES OF CHANGE  \n3.1 Individual Change Probabilities  \nThe probability that any given musical glyph will be read \ncorrectly or incorrectly is dependent on the quality of \nscan, the quality of original print, the OMR en gine used, \nand the type of repertory. One possible generalization \nused in the literature [ 8] is to classify errors as class co n-\nfusion (e.g., rest for note, with probability of occurring c), \nomissions (e.g., of whole symbols or of dots, tuplet marks: probab ility o), additions (a ), and general value \nconfusion (e.g., quarter for eighth : v). Other errors, such \nas sharp for natural  or tie for slur, do not affect rhythmic \naccuracy. Although accuracy would be improved by computing  these values independently for each OMR sy s-\ntem and quality of scan, such work is beyond the scope of the current paper. Therefore, we use  Rossant and Bloch’s \nrecognition rates, adjusting them  for the differences b e-\ntween working with individual symbols (such as dots and note stems) and symbolic objects (such as dotted -eighth \nand quarter notes). The values used in this model  are \nthus: c = .003, o = .009, a = .004, v = .016.\n1 As will b e-\ncome clear, more accurate measures would only improve the results given below. Subtracting these probabiliti es \nfrom 1.0, the rate of equality, e, is .968.  \n3.2 Aggregate Change Distances  \nThe similarity of two measures can be calculated  in a \nnumber of different ways, including the earth mover di s-\ntance, the Hamming distance, and the minimum L e-\nvenshtein or edit distance.  The nature of the change pro b-\nabilities o btained from Rossant and Bloch along with the \ninherent difficu lties of finding the one -to-one corr e-\nspondence of input and output objects required for other methods, made L evenshtein distance the most feasible \nmethod. The proba bility that certain changes would occur \nin a given orig inally scanned measure (source, S) to tran s-\nform it into the OMR output measure (destination, D) is \ndetermined  by finding, through an implementation of edit \ndistance, va lues for i, j, k, l, and m (for number of class \nchanges, omissions, additions, value changes, and u n-\nchanged el ements) that maximize : \n p\nS, D = c i · o j · a k · v l · e m   (1) \nEquation (1), the prior -based -on-changes or PrC,  can be \nused to derive a probability of rhythmic change due  to \nOMR errors between any two arbitrary measures, but the \nmodel employed here concerns itself with measures with \nincorrect rhythms, or flagged measures.  \n3.3 Flagged Measures  \nLet F Pi be the set of flagged measures for part  Pi, that is , \nmeasures whose total dur ations do not correspond to the \ntotal duration implied by the currently active time sign a-\nture, and F = {FP1, …, F Pj} for a score with j parts . (Mea s-\nure stacks where each meas ure number is in F can be r e-\nmoved as probable pickup or otherwise intended inco m-\nplete measures, and long stretches of measures in F in all \nparts can be attributed to incorrectly identified time si g-\nnatures and reevaluated, though neither of these refin e-\nments is  used in this model). It is possible for rhythms \nwithin a measure to be incorrectly recognized without the \nentire measure being in F ; though this problem only aris-\nes in the rare case where two rhythmic errors cancel out \neach other (as in a dotted quarter read as a quarter with an eighth read as a quarter in the same mea sure).  \n                                                             \n1 Rossant and Bloch give probabilities of change given that an error has \noccurred. The numbers given here are renormalizations of those error \nrates after removing the prior probability that an error has taken place.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n644  \n \n4. INTEGR ATING THE PRIORS  \nFor each m ∈ FPi, the measure n in part Pi with the hig h-\nest likelihood of representing the prototype source rhythm \nbefore OMR errors were introduced is the source measure \nSD that maximizes the product of the prior -based -on-\ndistance , that is, the horizontal model,  and the prior -\nbased -on-changes:  \nSD = argmax(PrD n O PrC n) ∀ n ∉ F.  (2) \n(In the highly unlikely case of equal probabilities, a si n-\ngle meas ure is cho sen arbitrarily) Similarly, for each m in \nFP the mea sure t in the measure stack corresponding to m, \nwith the highest likelihood of being the source rhythm for \nm, is the source measure SP that maximizes the product of \nthe prior- based -on-part, that is, the vertical model,  and \nthe prior- based -on-changes:  \nSP = argmax(PrP t  O\u0001PrC t) ∀ t ∉ F.  (3) \nSince the t wo priors PrD and PrP have not been normal-\nized in any way, the best match from SD and SP can be \nobtained by simply taking the maximum of the two:  \n S = arg max(P(m)) ∀ m in [S D, SP]  (4) \nGiven the assumption that the time signature and barlines \nhave accurately been obtained and that each measure \noriginally contained notes and rests whose total durations \nmatched the underlying meter, we do not need to be co n-\ncerned  with whether S is a “better” solution for co rrecting \nm than the rhythms currently in m, since the probability \nof a flagged measure being correct is zero.  Thus any sol u-\ntion has a higher likelihood of being correct than what was already there. (Real-world implementations, howe v-\ner, may wish to place a lower bound on P(S) to avoid \nsubstitutions that are b elow a minimum threshold to pre-\nvent errors being added that would be harder to fix than \nthe original .) \n5. EXAMPLE \nIn this example from Mozart K525, mvmt. 1, measure stack 17, measures in bo th Violin I and Violin II have \nbeen flagged as containing rhythmic errors (marked in \npurple in Figure 2).  \nBoth the OMR software and our implementation of the \nmethod, described below, can identify the violin lines as \ncontaining rhythmic errors, but neither  can know that an \nadded dot in each part has caused the error. The vertical \nmodel ( PrP * PrC) will look to the viola and cello parts \nfor corrections to the violin parts . Violin II and viola \nshare five rhythms (e\n5) and only one omission of a dot is \nrequired  to transform the viola rhythm into violin II ( o1), \nfor a PrC of 0.0076 . The prior on similarities between v i-\nolin II and viola (PrP) is 0.57, so the complete probability \nof this transformation is 0.0043 . The prior on similarities \nbetween violin II and cello is slightly higher, 0.64, but the prior based on changes is much smaller (4 · 10-9). Violin I \nis not considered as a source since its measure has also been flagged as incorrect. Therefore the viola’s measure is used for S\nP. \nA similar search is done for t he other (unflagged) \nmeasures in the rest of the violin II part in order to find S\nD. In this case, the probability of SP exceeds that of S D, \nso the viola measure’s rhythm is, correctly, used for vi o-\nlin II.   \n6. IMPLEMENTATION  \nThe model developed above was impl emented using co n-\nversion and score manipulation  routines from the open -\nsource Python -based toolkit, music21  [4] and has been \ncontributed back to the toolkit as the omr.correctors  \nmodule  in v.1.9 and above. Example 1 demonstrates a \nround -trip in MusicXML o f a raw OMR score to a post -\nprocessed score. \nfrom music21 import * \ns = converter.parse('/tmp/k525omrIn.xml') \nsc = omr.correctors.ScoreCorrector(s) \ns2 = sc.run() \ns2.write('xml', fp='/tmp/k525post.xml') \nExample 1. Python/ music21 code for correcting OMR \nerror s in Mozart K525, I. \nFigure 3, below, shows the types of errors that the m odel \nis able, and in some cases  unable, to correct.  \n7. RESULTS  \nNine scores of four -movemen t quartets by Mozart (5),1 \nHaydn (1), and Beethoven (4) were used for the primary \nevaluatio n. (Mozart K525, mvmt. 1 was used as a test \nsc\nore for development and testing but not for evaluation.)  \nScanned scores came from out -of-copyright editions \n(mainly Brei tkopf & Härtel) via IMSLP and were co n-\nverted to MusicXML using SmartScore X2 Pro \n(v.10.5.5). Ground truth encodings in MuseData and M u-\nsicXML formats came via the music21 corpus originally \nfrom the Stan ford’s CCARH repertories  [6] and Project \nGutenberg.   \n                                                             \n1 Mozart K 156 is a three -movement quartet, however, both the ground \ntruth and the OMR versions include the abandoned first version of the \nAdagio as a fourth movement.     \nFigure 2. Mozart, K525 I, in OMR ( l.) and scanned ( r.) \nversions.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n645  \n \nThe pre-processed OMR movement was aligned  with the \nground truth by finding the minimum edit distance b e-\ntween meas ure hashes. This step was necessary for the \nmany cases where the OMR version contained a different \nnumber of measures than the ground truth. The  number of \ndifferences between the two versions of th e same mov e-\nment was recorded. A total of 29,728 measures with \n7,196 flagged measures were examined. Flag rates ranged \nfrom 0.6% to 79.2% with a weighed mean of 24.2% and median of 21.7%.  \nThe model was then run on each OMR movement and the \nnumber of differences  with the ground truth was recorded \nagain.  (In order to make the outputted score useful for \nperformers and researchers, we added a simple algorithm \nto preserve as much pitch information as possible from \nthe original measure.) F rom 2.1% to 36.1% of flag ged \nmeasures were successfully corrected, with a weighed \nmean of 18.8 % and median of 18.0 %: a substantial i m-\nprovement over the original OMR output .  \nManually checking the pre - and post -processed OMR \nscores against the ground truth showed that the highest rates of differences came from scores where single -pitch \nrepetitions ( tremolos ) were spelled out in one source and \nwritten in abbreviated form in another; such differences could be corrected for in future versions. There was no \nsignificant correlation betwe en the percentage of \nmeasures ori ginally flagged and the correction rate ( r = \n.17, p > .31) . \nThe model was also run on two scores outside the class i-\ncal string quartet repertory to test its further relevance. \nOn a fourteenth- century vocal work (transcribed into \nmodern notation), Gloria : Clemens Deus artifex and the \nfirst movement of Schubert’s “Unfinished” symphony, the results were similar to the previous findings (16.8% \nand 18.7% error reduction, respectively).  The proportion of suggestions taken from the horizontal \n(PrD) and vertical models (PrP) depended significantly \non the number of parts in the piece. In Mozart K525 qua r-\ntet, 72% of the suggestions came from the horizontal \nmodel while for the Schubert symphony (fourteen parts), \nonly 39% came from the horizontal model.   \n8. APPLICATIONS  \nThe model has broad applications for improving the acc u-\nracy of scores already converted via OMR , but it would \nhave greater impact as an element of an improved user \nexperience within existing software. Used to its full p o-\ntential , the model could help systems provide suggestions \nas users examine flagged measures. Even a small scale implementation could greatly improve th e lengthy error -\ncorrecting process that currently must take place before a score is usea ble. See Figure 4  for an example interface. \n \nFigure 4 . A sample interface improvement using the \nmodel described.  \nA similar model to the one proposed here could also be  \nintegrated into OMR software to offer suggestions for pitch corrections if the user selects a measure that was not flagged for rhythmic errors. Integration within OMR software would also potentially give the model access to \n  \n \n   Figure 3 : Comparison of Mozart K525 I, mm. \n35–39 in the original scan (top), SmartScore \nOMR output ( middle ), and after post- OMR \nprocessing ( bot.). Flags 1 –\n3 were corrected \nsuccessfully; Flags 4 and 5 result in metrically plausible but incorrect emendations. The mo d-\nel was able to preserve the correct pitches for Flags 2 (added quarter rest) and Flag 3 (added \naugmentation dot). Flag 1 (omitted eighth \nnote) is considered correct in this evaluation, based solely on rhythm, even though the pitch of the reconstructed eighth note is not correct. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n646  \n \nrejected interpretations for me asures that may become \nmore p lausible when rhythmic similarity within a piece is \ntaken into account.  \nThe model could be expanded to take into account spatial \nseparation between glyphs as part of the probabilities. \nSimple extensions such as ignoring measur es that are \nlikely pickups  or correcting wrong time signatures and \nmissed barlines (resulting in double -length measures) \nhave already been mentioned. Autocorrelation matrices, which would identify repeating sections such as recapit u-\nlations and rondo return s, would improve the prior -based -\non-distance  metric . Although the model runs quickly on \nsmall scores (in far less than the time to run OMR despite the implementation being written in an interpreted la n-\nguage), on larger scores the O(len(F) · len(Part)) com-\nplexity of the horizontal model could become a problem \n(though correction of the lengthy Schubert score took less \nthan ten minutes on an i7 MacBook Air). Because the \nprior -based -on-distance tends to fall off quickly, exami n-\ning only a fixed -sized window worth of measures around \neach flagged measure would offer su bstantial speed- ups. \nLonger scores and scores with more parts offered more \npossibilities for high -probability correcting measures. \nThus we encourage  the creators of OMR competi tions \nand standard OMR test examples [3] to include e ntire \nscores taken from standard repertories in their  evaluation \nsets.  \nThe potential of post -OMR processing based on musical \nknowledge is still largely untapped. Models of tonal b e-\nhavior could identify transposing instruments and thus create better linkages between staves across systems that \nvary in the number of parts displayed. Misidentifications \nof time signatures, clefs, ties, and dynamics could also be reduced through comparison across parts and with similar sections in scores. While more powerful algorithms for \ngraphical recognition will always be necessary, substa n-\ntial improvements can be made quickly with the selective deployment of musical knowledge.   \n9. ACKNOWLEDGEMENTS  \nThe authors thank the Radcliffe Institute of Harvard Un i-\nversity, the Nationa l Endowment for the Human i-\nties/Digging into Data Challenge, the Thomas Temple \nHoopes Prize at Harvard, and the School of Humanities, \nArts, and Social Sciences, MIT, for research support , four \nanonymous readers for suggestions, and Margo Levine , \nBeth Chen, and Suzie Clark of Harvard ’s Applied Math \nand Music departments for a dvice and en couragement. \n10. REFERENCES  \n[1] E. P. Bugge, et al.: “Using sequence alignment and voting to improve optical music recognition from multiple recognizers,” Proc. ISMIR , Vol. 12,  pp. \n405– 410, 2011.  [2] D. Byrd, M. Schindele:  “Prospects fo r improving \nOMR with multiple recognizers,” Proc. ISMIR , Vol. \n7, pp. 41– 47, 2006.  \n[3] D. Byrd, J. G. Simonsen, “Towards a Standard \nTestbed for Optical Music Recognition: Definitions, \nMetrics, and Page Images,” http://www.informatics.indiana.edu/donbyrd/Papers/\nOMRStandardTestbed_Final.pdf,  in progress.  \n[4] M. Cuthbert and C. Ariza : “\nmusic21 : A Toolkit for \nComputer -Aided Musicology and Symbolic Music \nData ,” Proc. ISMIR , Vol. 11, pp.  637– 42, 2010.  \n[5] E. Guo et al.: Petrucci Music Library, imslp.org, \n2006–. \n[6] W. Hewlett, et al.: MuseData: an Electronic Library \nof Classical Music Scores, musedata.org, 1994, \n2000.  \n[7] A. Rebelo,  et al.: “ Optical  music  recognition:  State-\nof-the-art and open issues,” International Journal of \nMultimedia Information Retrieval , Vol. 1, No. 3, pp. \n173– 190, 2012. \n[8] F. Rossant and I. Bloch, “A fuzzy model for optical \nrecognition of musical scores,” Fuzzy sets and \nsystems, Vol. 141, No. 2, pp. 165 –201, 2004. \n[9] F. Ros sant, I. Bloch:  “Robust and adaptive OMR \nsystem including fuzzy modeling, fusion of musical rules, and possible error detection,” EURASIP \nJournal on Advances in Signal Processing , 2007. \n[10] V. Viro: “Peachnote: Music score search and analysis platform,” Proc. ISMIR , Vol. 12,  pp. 359–\n362, 2011.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n647 \n   \n \n \n \n                                                                 This Page Intentionally Left Blank  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n648"
    },
    {
        "title": "Social Music in Cars.",
        "author": [
            "Sally Jo Cunningham",
            "David M. Nichols",
            "David Bainbridge 0001",
            "Hassan Ali"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415676",
        "url": "https://doi.org/10.5281/zenodo.1415676",
        "ee": "https://zenodo.org/records/1415676/files/CunninghamNBA14.pdf",
        "abstract": "This paper builds an understanding of how music is cur- rently experienced by a social group travelling together in a car—how songs are chosen for playing, how music both reflects and influences the group’s mood and social interac- tion, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social expe- rience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data.",
        "zenodo_id": 1415676,
        "dblp_key": "conf/ismir/CunninghamNBA14",
        "keywords": [
            "music",
            "social group",
            "car travel",
            "song selection",
            "group mood",
            "social interaction",
            "hardware/software",
            "ethnographic data",
            "in-car music",
            "user interface"
        ],
        "content": "SOCIAL MUSIC IN CARS \nSally Jo Cunningham, David M. Nichols, David Bainbridge, Hasan Ali \nDepartment of Computer Science, University of Waikato, New Zealand \n{sallyjo, d.nichols, davidb}@waikato.ac.nz, hma4@students.waikato.ac.nz  \n \n  Wayne: “I think we’ll go with a little Bohemian Rhapsody, gentlemen” \nGarth:  “Good call” \n  \nWayne's World \n(1992) \n \nABSTRACT \nThis paper builds an understanding of how music is cur-\nrently experienced by a social group travelling together in a car—how songs are chosen fo r playing, how music both \nreflects and influences the group’s mood and social interac-\ntion, who supplies the music, the hardware/software that \nsupports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trip s. We suggest features and \nfunctionality for music software to enhance the social expe-rience when travelling in cars, and prototype and test a user \ninterface based on design suggestions drawn from the data. \n1. INTRODUCTION \nAutomobile travel occupies a significant space in modern Western lives and culture. The car can become a ‘home-from-home’ for commuters in their largely solitary travels, and for groups of people (friends, families, work col-leagues) in both long and shor t journeys [20]. Music is \ncommonly seen as a natural feature of automotive travel, and as cars become increasingl y computerized [17] the op-\nportunities are increased for providing music tailored to the specific characteristics of a given journey. To achieve this goal, however, we must first come to a more fine-grained understanding of these car-based everyday music experi-\nences. To that end, this paper explores the role of music in \nsupporting the ‘peculiar sociality’ [20] of car travel.  \n2. BACKGROUND \nMost work investigating the experience of music in cars focuses on single-users, (e.g. [4], [5]). Solo drivers are free to create their own audio environment: “the car is a space of performance and communication where drivers report being in dialogue with the radio or singing in their own auditized/privatized space” [5]. Walsh [21] notes that “a   large majority of drivers in the United States declare they sing aloud when driving”.  \nWalsh provides the most detailed discussion of the \nsocial aspects of music in car s, noting the interaction with \nconversation (particularly through volume levels) and music’s role in filling “chasms of silence” [21]. Issues of \nimpression management [9, 21] (music I like but wouldn’t \nwant others to know I like) are more acute in the confined environment of a car and vary depending on the social relationships between the occ upants [21]. Music selections \nare often the result of negotiations between the passengers and the driver [14, 21], where the driver typically has privileged access to the audio controls. \nBull [6] reports a particularly interesting example of the \nintersection between the private environment of personal portable devices and the social  environment of a car with \npassengers: \nJim points to the problematic nature of joint listening in the automobile due to differing musical tastes. The result is that he plays his iPod through the car radio whilst his children listen to theirs independently or playfully in ‘harmony’ resulting in multiple sound-worlds in the same space. \nHere, although the children have personal devices they \ntry to synchronize the playback so that they can experience the same song at the same time; even though their activity will occur in the context of another piece of music on the car audio system. Alternative methods for sharing include \nexplicit (and implicit) recommendation, as in Push!Music   \n[15], and physical sharing of earbuds [3]. Bull [6] also \nhighlights another aspect of  music in cars: selection \nactivities that occur prior to a journey. The classic ‘roadtrip’ activity of choosing music to accompany a long drive is also noted: “drivers would intentionally set up and prepare for their journey by explicitly selecting music to accompany the protracted journey “on the road”” [21].  \nSound Pryer [18] is a joint-listening prototype that \nenables drivers to ‘pry’ into  the music playing in other \ncars. This approach emphasizes driving as a social practice, though it focuses on inter-driver relationships rather than \nthose involving passengers. Sound Pryer  can also be \nthought of as a transfer of some of the mobile music \nsharing concepts in the tunA  system [2] to the car setting. \n © S.J. Cunningham, D.M. Nichol s, D. Bainbridge, H. Ali. \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  S.J. Cunningham, D.M. Nichols, D. \nBainbridge, H. Ali.. “Social Music in  Cars”, 15th International Society \nfor Music Information Retrieval Conference, 2014. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n457  \n \nDriver distraction is known to  be a significant factor in \nvehicle accidents and has led to legislation around the \nworld restricting the use of mobile phones whilst driving. \nIn addition to distraction e ffects caused by operating audio \ndevices there are the separate i ssues of how the music itself \naffects the driver. Driving styl e can be influenced by genre, \nvolume and tempo of music [10]: “at high levels, fast and loud music has been shown to divert attention [from driving]” [11], although drivers frequently use music to relax [11]. Several reports indicate that drivers use music to relieve boredom on long or familiar routes [1, 21], e.g. “as repetitious scenery encourages increasing disinterest … the personalized sounds of travel assume a greater role in allowing the driver-occupants respite via intermitting the sonic activity during protracted driving stints” [21].  \nMany accidents are caused by driver drowsiness; when \nlinked with physiological sensors to assess the driver’s state, music can be used to assist in maintaining an \nappropriate level of driver vigilance [16]. Music can also \ncounteract driver vigilance by masking external sounds and \nauditory warnings, particularly for older drivers where age-related hearing loss is more likely to occur [19].  \nIn summary, music fulfils a variety of different roles in \naffecting the mental state of the driver. It competes and interacts with passenger conversation, the external environmental and with audio functions from the increasingly computerized driving interface of the car. When passengers are present, the selection and playing of \nmusic is a social activity that requires negotiation between \nthe occupants of the vehicle. \n3. DATA COLLECTION AND METHODOLOGY \nOur research uses data collected in a third year university \nHuman Computer Interaction (HCI) course in which stu-\ndents design and prototype a syst em for the set application, \nwhere their designs are informed by an ethnographic inves-tigations into behavior associ ated with the application do-\nmain.  This present paper focuses on the ethnographic data collected that relates to music and car travel, as gathered by \n22 student investigators (Table  1).  All data gathering for \nthis study occurred within New Zealand. \nTo explore the problem of designing a system to support groups of people in selecting and playing music while trav-\neling, The students performed participant observations, with the observations focusing on how the music is chosen \nfor playing, how the music fits in with the other activities being conducted, who supplies the music, and how/who changes the songs or alters the volume. The students then explored subjective social mu sic experiences through auto-\nethnographies [8] and interviews of friends. The data com-prises 19 participant observations, two self-interviews, and \nfour interviews (approximately 45 printed pages). Of the 19 \nparticipant observations, four were of short drives (10 to 30 minutes), 14 were lengthier trips (50 minutes to 2 hours), and one was a classic ‘road trip’ (7 hours). The number of people participating in a trip ranged from one to five (Table 2). Of the 69 total travelers across the nineteen journeys, 45 were male and 24 were female. One set of travelers were all female, 7 were all male, and the remainder (11) were mixed gender. \nTable 1 . Demographics of student investigators  \nMale Female National Origin Count \n17 5 NZ/Australia 5 \n  C h i n a  1 3  \nAge Range: Mid-East 3 \n     20 - 27 Other 1 \nGrounded Theory methods [13] were used to analyze the \nstudent summaries of their participant observations and in-terviews. This present paper teases out the social behaviors that influence, and are influe nced by, music played during \ngroup car travel. Supporting evidence drawn from the eth-nographic data is presented below in italics. \nTable 2 . Number of travelers in observed journeys \n1 2 3 4 5 \n1 0 7 7 4 \n4. MUSIC BEHAVIOR IN CAR TRAVEL \nThis section explores: the physi cal car environment and the \nreported car audio devices; the different reported roles of the driver; observed behaviors surrounding the choice of songs and the setting of volume; music and driving safety; ordering of songs that are selected to be played; and the ‘ac-tivities’ that music supports and influences. \n4.1 Pre-trip Activities \nThe owner of a car often keeps personal music on hand in \nthe vehicle (CDs, an MP3 player  loaded with ‘car music’) \nas well as carrying along a mobile or MP3 player loaded with his/her music collection).  If only the owner’s music is played on the trip, then that person should, logically, also manage the selection of songs  during the journey. Unfortu-\nnately the owner of the car is also often the driver as well—and so safety may be compromised when the driver is ac-tively involved in choosing a nd ordering songs for play. \nPassengers are also likely to have on hand a mobile or \nMP3 player, and for longer trips may select CDs to share.  If two or more people contribut e music to be played on the \njourney, the challenge then becomes to bring all the songs together onto a single device—otherwise they experience the hassle of juggling several players. A consequence of merging collections, however, is that no one person will be familiar with the full set of songs, making on-the-road con-struction of playlists more difficult (particularly given the impoverished display surface of most MP3 players). \nA simple pooling of songs from the passengers’ and \ndriver’s personal music devices is unlikely to provide an \nefficiently utilizable source for selection of songs for a spe-\ncific journey. The music that an individual listens to during \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n458  \n \na usual day’s activities may not be suitable for a particular \ntrip, or indeed for any car journey. People tend to tailor their listening to the activity at hand [7], and so songs that are perfect ‘gym music’ or ‘s tudy music’ may not have the \nappropriate tempo, mood, or em otional tenor.  Further, an \nindividual’s music collection ma y include ‘guilty pleasures’ \nthat s/he may not want others to become aware of [9]: \nWhat mainly made [him] less comfortable in provid-ing music that he likes is b ecause he did [not] want to \ndestroy the hyper atmosphere in the car as a result of the mostly energetic songs being played throughout the trip. His taste is mostly doom and death metal, with harsh emotion and so will create a bleak atmos-phere in the car.  \n4.2 Physical Environment and Audio Equipment \nThe travel described in the participant observations primari-ly occurred in standard sized cars with two seating areas, comfortably seating at most two people in the front and \nthree in the rear sections.  In this environment physical movement is constrained. If the audio device controller is fixed in place then not everyone can easily reach it or view its display; if the controller is a handheld device, then it must be passed around (and even then it may be awkward to move the controller between the two sections).  \nAs is typical of student ve hicles in New Zealand, the \ncars tended to be older (10+ years) and so were less likely to include sophisticated audio options such as configurable \nspeakers and built-in MP3 systems. The range of audio equipment reported included radio, built-in CD player, portable CD player, stand-alone  MP3 player plus speakers, \nand MP3 player connected to the car audio system.  \nThe overwhelming preference evinced in this study is for \ndevices that give more fine-grained control over song selec-tion (i.e., MP3 players over CD  players, CD players over \nradio). The disadvantages of ra dio are that music choice is \nby station rather than by song, reception can be disrupted if \nthe car travels out of range, and most channels include ads. On the other hand, radio can provide news and talkback, to \nbreak up a longer journey. \n4.3 Music in Support of Journey Social Activities \nMusic is seen as integral to the group experience on a trip; \nit would be unacceptable and anti-social for the car’s occu-pants to simply each listen to their individual MP3 player, \nfor example. We identify a wide variety of ways that travel-\ners select songs so as to support group social activities dur-ing travel: \n• Music can contribute to driving safety , by playing songs \nthat will reduce driver drowsiness and keep the driver fo-\ncused ( music… can liven up a drive and keep you enter-\ntained or awake much longer ). For passengers, it can re-\nduce the tedium associated with trips through un-interesting or too-familiar scenery ( music can reduce the \nboredom for you and your friends with the journey ). Conversely, loud, fast tempo music can adversely affect safety ([As the driver, I] changed the volume very high… \nmy body was shaking with the song. I stepped on the ac-celerator in my car;  The driver [was] seen to increase the speed when the songs he liked is on ). \n• Listening to music can be the main source of entertain-\nment during a trip, as the driver and passengers focus on \nthe songs played. \n• Songs need not be listened to passively; travelers may engage in group sing-alongs , with the music providing \nsupport for their ‘performances’. These sessions may be loud and include over-the-top emotive renditions for the amusement of the singer and the group, and be accompa-nied by clapping and ‘dancing’ in the seats ( The partici-\npants would sing along to the ly rics of the songs, and al-\nso sometimes dance along to the music, laughing and smiling throughout it ). \n• A particular song may spark a conversation about the music —to identify a song ( they would know what song \nthey wanted to hear but they would not know the artist or \nname of the song. When this happened, they would … try to think of the artist name together ) or to discuss other \naspects of the artist/song/genre/etc ( ‘In the air tonight,  \nPhil Collins!’ Ann asked Joan and I, ‘did you know that \nit’s top of the charts at the moment’  … There was con-\nversation about Phil Collins re-releasing his m u\nsic.) A \nlively debate can surround the choice and ordering of the songs to play, if playlists are created during the trip itself. \n• Music can provide a background to conversation ; at \nthis point the travelers pay little or no attention to the songs but they mask traffic noises ( when we were chat-\nting… no one really cared what was on as long as there was some ambient sound ). By providing ‘filler’ for awk-\nward silences, music is particularly useful in supporting conversations among groups who don’t know each other particularly well ( it seemed more natural to talk when \nthere was music to break the silence ).  \nFor shorter trips, music might serve only one or two of \nthese social purposes—playing as background to a debate over where to eat, for example.  On longer journeys, the \nfocus of group attention and activity is likely to shift over \ntime, and with that shift the role of the music will vary as well: At some times it would be the focus activity, with eve-\nryone having input on what s ong to choose and then sing-\ning along. While at other times  the group just wanted to \ntalk with each other and so the music was turned right down and became background music…  \n4.4 Selecting and Ordering Songs \nThe physical music device plays a significant role in deter-\nmining who chooses the music on a car trip.  If the device is \nfixed (typically in the center of the dashboard), then it is easily accessible only by the driver or front passenger—and \nso they are likely to have primary responsibility for choos-\ning, or arbitrating the choice, of songs. The driver is often \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n459  \n \nthe owner of the vehicle, and in that case is likely to be as-\nsertive at decision points ( Since I was the driver, I was ba-\nsically the DJ. I would select the CD and the song to be played. I also changed the song if I didn’t like it even if oth-ers in the car did. ).  Given the small display surfaces of \nmost music devices and the complexity of interactions with \nthose devices, it is likely that safety is compromised when \nthe driver acts as DJ.  Consider, for example: \nI select some remixed trance music from the second \nCD at odd slots of the playlist, and then insert some \npop songs from other CDs in the rest of the slots of the \nlist.  … I manually change the play order to random. Also I disable the volume protect. And enable the max volume that from the subwoofer due to the noises from the outside of my car … \nIf the music system has a hand-held controller, then the \nresponsibility for song selection can move through the car. At any one point, however, a single individual will assume responsibility for music management. Friends are often fa-miliar with each other’s tastes, and so decisions can be made amicably with little or no consultation ( I felt comfort-\nable in choosing the music because they were mostly \nfriends and I knew what kind of music they were all into \nand what music some friends were not into… ). Imposing \none’s will might go against the sense of a group experience and social expectations (…having the last word means it \ncould cause problems between friends ), or alternatively \nclose ties might make unilateral decisions more acceptable (I did occasionally get fed up from their music and put back \nmy music again without even a sking them for permission, \nyou know we are all friends. ).  \nAs noted in Section 4.1, song selection on the fly can be \ndifficult because the chooser may not be familiar with the \ncomplete base collection, or because the base collection in-\ncludes songs not suited to the current mood of the trip. A common strategy is to listen to the first few seconds of a song, and if it is unacceptable then to skip to the song that comes up ‘next’ in the CD / shuffle / predetermined playlist. This strategy provides a choppy listening experi-ence, but does have the advantage of simplicity: a song is skipped if any one person in the car expresses an objection to it. It may, however, be embarrassing to ask for a change if one is not in current possession of the control device.  \nSong-by-song selection is appropriate for shorter trips, as \nthe setup time for a playlist may be longer than the journey \nitself. Suggesting and ordering s ongs can also be a part of \nthe fun of the event and engage travelers socially ( My \nfriends would request any songs that they would like to \nhear, and the passenger in control of the iPod acted like a human playlist; trying to memorise the requests in order and playing them as each song finished. )   \nFor longer trips, a set of pre-created playlists or mixes \n(supporting the expected moods or phases of the journey) can create a smoother travel experience.  A diverse set of playlists may be necessary to match the range of social mu-sic behaviors reported in Section 4.2. Even with careful pre-planning, however, a song ma y be rejected at time of \nplay for personal, idiosyncratic reasons (for example, one participant skips particular songs … associated with par-\nticular memories and events so I don’t like to listen to them while driving for example ). \n4.5 Music Volume \nSound volume is likely to change during a trip, signaling a change in the mood of the gathering, an alteration in the group focus, or to intensify / downplay the effects of a giv-en song.  Participant observations included the following reasons for altering sound levels: to focus group attention on a particular song (louder); for the group to sing along with a song (louder); to switc h the focus of group activity \nfrom the music to conversation (softer); to ‘energize’ the \nmood of the group (louder); to calm the group mood, and particularly to permit passengers to sleep (softer); and to move the group focus from conversation back to the music, particularly when conversation falters (louder). \nClearly the ability to modulat e volume to fit to the cur-\nrent activity or mood is crucial. A finer control than is cur-rently available would be desirable, as often speaker place-ment means perceived volume depends on one’s seat in the car ( [he] asked the driver to turn the bass down … because \nthe bass effect was too strong, and the driver … think[s] the bass is fine in the front ).   \nFurther, the physical division of a car into separate rows \nof seats and its restriction of passenger movement can en-courage separate activity ‘zones’ (for example, front seats / back seats)—and the appropriate volume for the music can differ between seating areas:   \nOne of our friends who sets beside the driver is paying more attentions on the music, the rest 3 of us set in the back were communicate a lot more, and didn’t paying too much attention on the music… the front people can hear the music a lot more clear then the people sets in the back, and it’s harder for the front people to join the communication with the back people because he \nneed to turn his head around for the chat sometimes. \n5. IMPLICATIONS FOR A SOCIAL AUDIO \nSYSTEM FOR CAR TRAVEL \nLeveraging upon music information retrieval capabilities, \nwe now describe how our findings can inform the design of software specially targeted for song selection during car trips—personified, the software we seek in essence acts as a \nmusic host.  In general a playlist generator [12] for song \nselection coupled with access to  a distributed network of \nself-contained digital music lib raries for storing, organiz-\ning, and retrieving items (the collections of songs the vari-\nous people travelling have) are useful building blocks to developing such software; however, to achieve a digital music host, what is needed ultimately goes beyond this. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n460  \n \nIn broad terms, we envisage a software application with \ntwo phases: initial configuration and responsive adaptation.  \nDuring configuration, the app lication gathers the pool of \nsongs for the trip from the individuals’ devices, taking into account preferences such as which songs they wish to keep private and which types of songs (genre, artist, tempo, etc.) \nthat they wish to have considered for the trip playlist. The \nusers are then prompted to enter the approximate length of the upcoming road trip, and an initial playlist is constructed based on the user preferences and pool of songs. \nDuring the trip, the application can make use of a varie-\nty of inputs to dynamically adjust the sequence of songs played.  Here significant gains can be made from inventive uses of MIR techniques coupled with temporal and spatial information–even data sensors from the car.  For instance, \nif the application noticed the driver speeding for that sec-tion of road it could alter the selection of the next song to one that is quieter with a sl ower tempo (beat detection); \nalternatively, triggered by the detection of the conversation lapsing into silence (noise cancelling) the next song played could be altered to be one labeled with a higher “interest” value (tagged, for instance, using semantic web technolo-gies, and captured in the playlist as metadata). News sourced from a radio signal (w hichever is currently in \nrange) can be interspersed with the songs being played. \nAs evidenced by our analysis, the role of the driv-\ner/owner of the car takes on special significance in terms of the interface and interaction design.  As the host of the ve-hicle, there is a perception that they are more closely \nlinked to the software (the digital music host) that is mak-ing the decision over what to play next.  While it is not a strict requirement of the software, for the majority of situa-tions it will be an instinctive decision that the key audio device used to play the songs on the trip will be the one owned by the driver. For the adaptive phase of the software then, there is a certain irony that the driver (for reasons of driving safely) has less opportunity to influence the song selection during the trip.  To address this imbalance, an as-pect the software could support is the prioritization of input \nfrom the “master” application at noted times that are deemed safe (such when the car is stationary). \nMore prosaically, the travellers will requires support in \ntweaking the playlist as the trip progresses. We developed and tested a prototype of this aspect of the system, to eval-uate the design’s potential. The existing behaviors explored in Section 3 suggest that this system should be targeted at tablet devices rather than sm aller mobiles: while the device \nshould be lightweight enough to be easily passed between passengers in a vehicle, the users should be able to clearly see the screen details from an arm’s length, and controls should be large and spaced to  minimize input error.  \nFigure 1 presents screenshots for primary functionality \nof our prototype:  the view of the trip playlist, which fea-tures the current song in context with the preceding and succeeding songs (Figure 1a); the lyrics display for the cur-rent song, sized to be viewable by all (Figure 1b); and a screen allowing selected songs to be easily inserted into different points in the playlist (Figure 1c). While it was tempting on a technical level to include mobile-based wire-less voting (using their smart phones) to move the current-ly playing item up or down as an expression of like/dislike \n(relevance feedback), we recognize that face-to-face dis-\ncussion and argument over songs is often a source of en-\njoyment and bonding for fellow travelers—and so we de-liberately support only manua l playlist manipulation. \n \nFigure 1a .  Playlist view. \nFigure 1b. Lyrics view for the active song. \n \nFigure 1c . After searching for a song, ‘smart options’ for \ninserting the song into the current section of the playlist. \nGiven the practical and safety difficulties in evaluating \nour prototype system in a m oving car, we instead used a \nstationary simulation. Two gr oups of four high school aged \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n461  \n \nmales participated in the eval uation, with each trial consist-\ning of approximately 30 minutes in which they listened to \nsongs on a pre-prepared playlist, both collaboratively and individually selected additional songs, inserted them into the playlist, and viewed lyrics to sing along. The research-ers took manual notes of the simulations, and participants \nengaged in focus group discussions post-simulation. \nWhile the participants found the prototype to be general-\nly usable (though usability tweaks were identified), we \nidentified worrying episodes in which the drivers switched focus from the wheel to the ta blet. While we recognize that \nbehavior may be different in a simulation than in real driv-\ning conditions, we also saw strong evidence from the eth-nographic data that drivers—pa rticularly young, male driv-\ners—can prioritize song selection over road safety. Further design iterations must recognize that drivers will inevitably seize control of a car’s music system, and so should priori-tize design that supports fast, one-handed interactions. \n \n6. CONCLUSIONS \nThe primary contribution of this paper is understanding of \nsocial music behavior of sm all groups of people while on \n‘road trips’, developed through a qualitative analysis of ethnographic data (participant observations and inter-views). We prototyped and evaluated the more prosaic as-pects of a system to support social music listening on road \ntrips, and suggest further extensions—including sensor-\nbased input to modify the trip  playlist—for future research. \n7. REFERENCES \n[1] K.P. Åkesson, A. Nil sson: “Designing Leisure \nApplications for the Mundane Car-Commute,” Personal \nand Ubiquitous Computing , 6:3, 176–187, 2002. \n[2] A. Bassoli, J. Moore, S. Agamanolis: “tunA: Socialising \nMusic Sharing on the Move,” In  K. O'Hara and B. Brown \n(eds.), Consuming Music Together: Social and \nCollaborative Aspects of Music Consumption \nTechnologies. Springer, 151-172, 2007. \n[3] T. Bickford: “Earbuds Are Good for Sharing: Children’s Sociable Uses of Headphones at a Vermont Primary \nSchool,” In J. Stanyek and S. Gopinath (eds.), The \nHandbook of Mobile Music Studies , Oxford University \nPress, 2011.  \n[4] M. Bull: “Soundscapes of the car: a critical study of automobile habitation,” In M. Bull and L. Back, (eds.) \nThe Auditory Culture Reader , Berg, 357–374, 2003. \n[5] M. Bull: “Automobility and the power of sound”, Theory, \nCulture & Society , 21:4/5, 243–259, 2004. \n[6] M. Bull: “Investigating the culture of mobile listening: \nfrom Walkman to iPod,” In K. O'Hara and B. Brown \n(eds.), Consuming Music Together . Springer, 131–149, \n2006. [7] \nS.J. Cunningham, D.  Bainbridge, A. Falconer.  More \nof an art than a science:  playlist and mix construction. \nProceedings of ISMIR ’06 , Vancouver, 2006.  \n[8] S.J. Cunningham, M. Jones:  “Autoethnography: a tool \nfor practice and education,” Proceedings of the 6th \nNew Zealand International  Conference on Computer-\nHuman Interaction (CHINZ 2005),  1-8, 2005. \n[9] S.J. Cunningham, M. Jones,  S. Jones: “Organizing \ndigital music for use: an examination of personal music collections”.   Proceedings of ISMIR’04 , \nBarcelona, 447-454, 2004.\n \n[10] B.H. Dalton, D.G. Behm: “E ffects of noise and music on \nhuman and task performance: A systematic review,” \nOccupational Ergonomics, 7:3, 143-152, 2007. \n[11] N. Dibben, V.J. Williamson: “An exploratory survey of in-vehicle music listening,” Psychology of Music , 35: 4, \n571-589, 2007. \n[12] A. Flexer, D. Schnitzer, M. Gasser, G. Widmer. “Playlist \ngeneration using start and end songs”,  Proceedings of \nISMIR’08 , 173-178, 2008. \n[13] B. Glaser, A. Strauss: The Discovery of Grounded \nTheory: Strategies for Qualitative Research , Chicago, \n1967. \n[14] A. E. Greasley, A. Lamont: “Exploring engagement with music in everyday life using experience sampling \nmethodology,” Musicae Scientiae , 15: 45, 45-71, 2011.  \n[15] M. Håkansson, M. Rost, L.E. Holmquist: “Gifts from friends and strangers: a study of mobile music sharing,” \nProceedings of ECSCW’07 , 311-330, 2007.  \n[16] C. Hasegawa, K. Oguri: “The effects of specific musical \nstimuli on driver’s drowsiness,” Proceedings of the \nIntelligent Transportation Systems Conference (ITSC’06) , \n817-822, 2006. \n[17] O. Juhlin: Social Media on the Road: The Future of Car \nBased Computing , Springer, London, 2010.  \n[18] M. Östergren, O. Juhlin: “C ar Drivers Using Sound Pryer \n– Joint Music Listening in Traffic Encounters,” In K. \nO'Hara and B. Brown (eds.), Consuming Music Together: \nSocial and Collaborative Aspect s of Music Consumption \nTechnologies. Springer, 173-190, 2006. \n[19] E.B. Slawinski, J.F. McNeil: (2002) “Age, Music, and \nDriving Performance: Detection of External Warning \nSounds in Vehicles,” Psychomusicology , 18, 123-31, \n2002. \n[20] Urry, J.  “Inhabiting the car,” The Sociological Review , \n54, 17–31, 2006. \n[21] M.J. Walsh: “Driving to the beat of one’s own hum: Au-\ntomobility and musical listening,” In N. K. Denzin (ed.) Studies in Symbolic Interaction, 35, 201-221, 2010. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n462"
    },
    {
        "title": "Evaluating the Evaluation Measures for Beat Tracking.",
        "author": [
            "Matthew E. P. Davies",
            "Sebastian Böck"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415128",
        "url": "https://doi.org/10.5281/zenodo.1415128",
        "ee": "https://zenodo.org/records/1415128/files/DaviesB14.pdf",
        "abstract": "The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evalu- ation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple ac- curacy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the re- lationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow al- ternative metrical levels appear more correlated than those which do not. Second, we explore the effect of param- eterisation of objective evaluation measures, and demon- strate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being over- estimated via objective evaluation.",
        "zenodo_id": 1415128,
        "dblp_key": "conf/ismir/DaviesB14",
        "keywords": [
            "human listeners",
            "judgment performance",
            "alternative metrical levels",
            "objective evalu- ation measures",
            "correlation between objective scores and subjective ratings",
            "effect of parameterisation",
            "correlation maximised",
            "smaller tolerance windows",
            "true beat tracking performance",
            "over-estimated via objective evaluation"
        ],
        "content": "EVALUATING THE EVALUATION MEASURES FOR BEAT TRACKING\nMathew E. P. Davies\nSound and Music Computing Group\nINESC TEC, Porto, Portugal\nmdavies@inesctec.ptSebastian B ¨ock\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nsebastian.boeck@jku.at\nABSTRACT\nThe evaluation of audio beat tracking systems is normally\naddressed in one of two ways. One approach is for human\nlisteners to judge performance by listening to beat times\nmixed as clicks with music signals. The more common\nalternative is to compare beat times against ground truth\nannotations via one or more of the many objective evalu-\nation measures. However, despite a large body of work in\naudio beat tracking, there is currently no consensus over\nwhich evaluation measure(s) to use, meaning multiple ac-\ncuracy scores are typically reported. In this paper, we seek\nto evaluate the evaluation measures by examining the re-\nlationship between objective accuracy scores and human\njudgements of beat tracking performance. First, we present\nthe raw correlation between objective scores and subjective\nratings, and show that evaluation measures which allow al-\nternative metrical levels appear more correlated than those\nwhich do not. Second, we explore the effect of param-\neterisation of objective evaluation measures, and demon-\nstrate that correlation is maximised for smaller tolerance\nwindows than those currently used. Our analysis suggests\nthat true beat tracking performance is currently being over-\nestimated via objective evaluation.\n1. INTRODUCTION\nEvaluation is a critical element of music information re-\ntrieval (MIR) [16]. Its primary use is a mechanism to de-\ntermine the individual and comparative performance of al-\ngorithms for given MIR tasks towards improving them in\nlight of identiﬁed strengths and weaknesses. Each year\nmany different MIR systems are formally evaluated within\nthe MIREX initiative [6].\nIn the context of beat tracking, the concept and purpose\nof evaluation can be addressed in several ways. For exam-\nple, to measure reaction time across changing tempi [2],\nto identify challenging musical properties for beat track-\ners [9] or to drive the composition of new test datasets [10].\nHowever, as with other MIR tasks, evaluation in beat track-\ning is most commonly used to estimate the performance of\none or more algorithms on a test dataset.\nc\rMathew E. P. Davies, Sebastian B ¨ock.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Mathew E. P. Davies, Sebastian B ¨ock.\n“Evaluating the evaluation measures for beat tracking”, 15th International\nSociety for Music Information Retrieval Conference, 2014.This measurement of performance can happen via sub-\njective listening test, where human judgements are used\nto determine beat tracking performance [3], to discover:\nhow perceptually accurate the beat estimates are when\nmixed with the input audio. Alternatively, objective eval-\nuation measures can be used to compare beat times with\nground truth annotations [4], to determine: how consis-\ntent the beat estimates are with the ground truth accord-\ning to some mathematical relationship. While undertak-\ning listening tests and annotating beat locations are both\nextremely time-consuming tasks, the apparent advantage\nof the objective approach is that once ground truth anno-\ntations have been determined, they can easily be re-used\nwithout the need for repeated listening experiments. How-\never, the usefulness of any given objective accuracy score\n(of which there are many [4]) is contingent on its ability\nto reﬂect human judgement of beat tracking performance.\nFurthermore, for the entire objective evaluation process to\nbe meaningful, we must rely on the inherent accuracy of\nthe ground truth annotations.\nIn this paper we work under the assumption that musi-\ncally trained experts can provide meaningful ground truth\nannotations and rather focus on the properties of the ob-\njective evaluation measures. The main question we seek to\naddress is: to what extent do existing objective accuracy\nscores reﬂect subjective human judgement of beat track-\ning performance? In order to answer this question, even\nin principle, we must ﬁrst verify that human listeners can\nmake reliable judgements of beat tracking performance.\nWhile very few studies exist, we can ﬁnd supporting evi-\ndence suggesting human judgements of beat tracking accu-\nracy are highly repeatable [3] and that human listeners can\nreliably disambiguate accurate from inaccurate beat click\nsequences mixed with music signals [11].\nThe analysis we present involves the use of a test\ndatabase for which we have a set of estimated beat loca-\ntions, annotated ground truth and human subjective judge-\nments of beat tracking performance. Access to all of these\ncomponents (via the results of existing research [12, 17])\nallows us to examine the correlation between objective ac-\ncuracy scores, obtained by comparing the beat estimates to\nthe ground truth, with human listener judgements. To the\nbest of our knowledge this is the ﬁrst study of this type for\nmusical beat tracking.\nThe remainder of this paper is structured as follows. In\nSection 2 we summarise the objective beat tracking evalu-\nation measures used in this paper. In Section 3 we describe\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n637the comparison between subjective ratings and objective\nscores of beat tracking accuracy. Finally, in Section 4 we\npresent discussion and areas for future work.\n2. BEAT TRACKING EVALUATION MEASURES\nIn this section we present a brief summary each of the eval-\nuation measures from [4]. While nine different approaches\nwere presented in [4], we reduce them to seven by only pre-\nsenting the underlying approaches for comparing a set of\nbeats with a set of annotations (i.e. ignoring alternate met-\nrical interpretations). We consider the inclusion of differ-\nent metrical interpretations of the annotations to be a sepa-\nrate process which can be applied to any of these evaluation\nmeasures (as in [5, 8, 15]), rather than a speciﬁc property\nof one particular approach. To this end, we choose three\nevaluation conditions: Annotated – comparing beats to an-\nnotations, Annotated+Offbeat – including the “off-beat”\nof the annotations for comparison against beats and An-\nnotated+Offbeat+D/H – including the off-beat and both\ndouble and half the tempo of the annotations. This dou-\nbling and halving has been commonly used in beat track-\ning evaluation to attempt to reﬂect the inherent ambiguity\nin music over which metrical level to tap the beat [13]. The\nset of seven basic evaluation measures are summarised be-\nlow:\nF-measure : accuracy is determined through the propor-\ntion of hits, false positives andfalse negatives for a given\nannotated musical excerpt, where hitscount as beat esti-\nmates which fall within a pre-deﬁned tolerance window\naround individual ground truth annotations, false pos-\nitives are extra beat estimates, and false negatives are\nmissed annotations. The default value for the tolerance\nwindow is \u00060.07s.\nPScore : accuracy is measured as the normalised sum\nof the cross-correlation between two impulse trains, one\ncorresponding to estimated beat locations, and the other\nto ground truth annotations. The cross-correlation is\nlimited to the range covering 20% of the median inter-\nannotation-interval (IAI).\nCemgil : a Gaussian error function is placed around each\nground truth annotation and accuracy is measured as the\nsum of the “errors” of the closest beat to each annotation,\nnormalised by whichever is greater, the number of beats\nor annotations. The standard deviation of this Gaussian\nis set at 0.04s.\nGoto : the annotation interval-normalised timing error is\nmeasured between annotations and beat estimates, and\na binary measure of accuracy is determined based on\nwhether a region covering 25% of the annotations con-\ntinuously meets three conditions – the maximum error is\nless than \u000617.5% of the IAI, and the mean and standard\ndeviation of the error are within \u000610% of the IAI.\nContinuity-based : a given beat is considered accurate if\nit falls within a tolerance window placed around an anno-\ntation and that the previous beat also falls within the pre-ceding tolerance window. In addition, a separate thresh-\nold requires that the estimated inter-beat-interval should\nbe close to the IAI. In practice both thresholds are set\nat\u000617.5% of the IAI. In [4], two basic conditions con-\nsider the ratio of the longest continuously correct region\nto the length of the excerpt (CMLc), and the total propor-\ntion of correct regions (CMLt). In addition, the AMLc\nand AMLt versions allow for additional interpretations of\nthe annotations to be considered accurate. As speciﬁed\nabove, we reduce these four to two principal accuracy\nscores. To prevent any ambiguity, we rename these ac-\ncuracy scores Continuity-C (CMLc) and Continuity-T\n(CMLt).\nInformation Gain : this method performs a two-way\ncomparison of estimated beat times to annotations and\nvice-versa. In each case, a histogram of timing errors is\ncreated and from this the Information Gain is calculated\nas the Kullback-Leibler divergence from a uniform his-\ntogram. The default number of bins used in the histogram\nis 40.\n3. SUBJECTIVE VS. OBJECTIVE COMPARISON\n3.1 Test Dataset\nTo facilitate the comparison of objective evaluation scores\nand subjective ratings we require a test dataset of audio ex-\namples for which we have both annotated ground truth beat\nlocations and a set of human judgements of beat tracking\nperformance for a beat tracking algorithm. For this pur-\npose we use the test dataset from [17] which contains 48\naudio excerpts (each 15s in duration). The excerpts were\nselected from the MillionSongSubset [1] according to a\nmeasurement of mutual agreement between a committee\nof ﬁve state of the art beat tracking algorithms. They cover\na range from very low mutual agreement – shown to be\nindicative of beat tracking difﬁculty, up to very high mu-\ntual agreement – shown to be easier for beat tracking algo-\nrithms [10].\nIn [17] a listening experiment was conducted where\na set of 22 participants listened to these audio examples\nmixed with clicks corresponding to automatic beat esti-\nmates and rated on a 1 to 5 scale how well they considered\nthe clicks represented the beats present in the music. For\neach excerpt these beat times were the output of the beat\ntracker which most agreed with the remainder of the ﬁve\ncommittee members from [10]. Analysis of the subjective\nratings and measurements of mutual agreement revealed\nlow agreement to be indicative of poor subjective perfor-\nmance.\nIn a later study, these audio excerpts were used as one\ntest set in a beat tapping experiment, where participants\ntapped the beat using a custom piece of software [12]. In\norder to compare the mutual agreement between tappers\nwith their global performance against the ground truth, a\nmusical expert annotated ground truth beat locations. The\ntempi range from 62 BPM (beats per minute) up to 181\nBPM and, with the exception of two excerpts, all are in 4/4\ntime. Of the remaining two excerpts, one is in 3/4 time and\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6380 0.5 124F−measure\n(0.77)ratings\n0 0.5 124ratings(0.85)\n0 0.5 124\naccuracyratings(0.85)0 0.5 124PScore\n(0.72)\n0 0.5 124(0.74)\n0 0.5 124\naccuracy(0.82)0 0.5 124Cemgil\n(0.79)\n0 0.5 124(0.84)\n0 0.5 124\naccuracy(0.85)0 0.5 124Goto\n(0.52)\n0 0.5 124(0.51)\n0 0.5 124\naccuracy(0.41)0 0.5 124Continuity−C\n(0.68)\n0 0.5 124(0.65)\n0 0.5 124\naccuracy(0.86)0 0.5 124Continuity−T\n(0.68)\n0 0.5 124(0.61)\n0 0.5 124\naccuracy(0.84)0 524Inf. Gain\n(0.85)\n0 524(0.86)\n0 524\nbits(0.85) A+O+D/HA\nA+OFigure 1. Subjective ratings vs. objective accuracy scores for different evaluation measures. The rows indicate different\nevaluation conditions. (top row) Annotated, (middle row) Annotated+Offbeat, and (bottom row) Annotated+Offbeat+D/H.\nFor each scatter plot, the linear correlation coefﬁcient is provided.\nthe other was deemed to have no beat at all, and therefore\nno beats were annotated.\nIn the context of this paper, this set of ground truth beat\nannotations provides the ﬁnal element required to evaluate\nthe evaluation measures, since we now have: i) automati-\ncally estimated beat locations, ii) subjective ratings corre-\nsponding to these beats and iii) ground truth annotations\nto which the estimated beat locations can be compared.\nWe use each of the seven evaluation measures described in\nSection 2 to obtain the objective accuracy scores according\nto the three versions of the annotations: Annotated, Anno-\ntated+Offbeat andAnnotated+Offbeat+D/H. Since all ex-\ncerpts are short, and we are evaluating the output of an\nofﬂine beat tracking algorithm, we remove the startup con-\ndition from [4] where beat times in the ﬁrst ﬁve seconds\nare ignored.\n3.2 Results\n3.2.1 Correlation Analysis\nTo investigate the relationship between the objective accu-\nracy scores and subjective ratings, we present scatter plots\nin Figure 1. The title of each individual scatter plot in-\ncludes the linear correlation coefﬁcient which we interpret\nas an indicator of the validity of a given evaluation measure\nin the context of this dataset.\nThe highest overall correlation (0.86) occurs for\nContinuity-C when the offbeat and double/half conditions\nare included. However, for all but Goto, the correlation is\ngreater than 0.80 once these additional evaluation criteria\nare included. It is important to note only Continuity-C and\nContinuity-T explicitly include these conditions in [4].\nSince Goto provides a binary assessment of beat track-\ning performance, it is unlikely to be highly correlated with\nthe subjective ratings from [17] where participants were\nexplicitly required to use a ﬁve point scale rather than a\ngood/bad response concerning beat tracking performance.\nNevertheless, we retain it to maintain consistency with [4].Comparing each individual measure across these eval-\nuation conditions, reveals that Information Gain is least\naffected by the inclusion of additional interpretations of\nthe annotations, and hence most robust to ambiguity over\nmetrical level. Referring to the F-measure andPScore\ncolumns of Figure 1 we see that the “vertical” structure\nclose to accuracies of 0.66 and 0.5 respectively is mapped\nacross to 1 for the Annotated+Offbeat+D/H condition.\nThis pattern is also reﬂected for Goto, Continuity-C and\nContinuity-T which also determine beat tracking accuracy\naccording to ﬁxed tolerance windows, i.e. a beat falling\nanywhere inside a tolerance window is perfectly accurate.\nHowever, the fact that a fairly uniform range of subjective\nratings between 3 and 5 (i.e. “fair” to “excellent” [17]) ex-\nists for apparently perfect objective scores indicates a po-\ntential mismatch and over-estimation of beat tracking ac-\ncuracy. While a better visual correlation appears to exist in\nthe scatter plots of Cemgil andInformation Gain, this is\nnot reﬂected in the correlation values (at least not for the\nAnnotated+Offbeat+D/H condition). The use a Gaussian\ninstead of a “top-hat” style tolerance window for Cemgil\nprovides more information regarding the precise localisa-\ntion of beats to annotations and hence does not have this\nclustering at the maximum performance. The Informa-\ntion Gain measure does not use tolerance windows at all,\ninstead it measures beat tracking accuracy in terms of the\ntemporal dependence between beats and annotations, and\nthus shows a similar behaviour.\n3.2.2 The Effect of Parameterisation\nFor the initial correlation analysis, we only considered\nthe default parameterisation of each evaluation measure as\nspeciﬁed in [4]. However, to only interpret the validity of\nthe evaluation measures in this way presupposes that they\nhave already been optimally parameterised. We now ex-\nplore whether this is indeed the case, by calculating the ob-\njective accuracy scores (under each evaluation condition)\nas a function of a threshold parameter for each measure.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6390 0.05 0.100.51F−measureaccuracy\n0 0.500.51PScore\n0 0.05 0.100.51Cemgil\n0 0.500.51Goto\n0 0.500.51Continuity−C\n0 0.500.51Continuity−T\n0 50 100012345Inf. Gainbits\n0 0.05 0.100.51correlation\nthreshold (s)0 0.500.51\nthreshold0 0.05 0.100.51\nthreshold (s)0 0.500.51\nthreshold0 0.500.51\nthreshold0 0.500.51\nthreshold0 50 10000.51\nnum binsFigure 2. (top row) Beat tracking accuracy as a function of threshold (or number of bins for Information Gain) per evalua-\ntion measure. (bottom row) Correlation between subjective ratings and accuracy scores as a function of threshold (or num-\nber of bins). In each plot the solid line indicates the Annotated condition, the dashed–dotted line shows Annotated+Offbeat\nand the dashed line shows Annotated+Offbeat+D/H. For each evaluation measure, the default parameteristation from [4] is\nshown by a dotted vertical line.\nWe then re-compute the subjective vs. objective correla-\ntion. We adopt the following parameter ranges as follows:\nF-measure : the size of the tolerance window increases\nfrom\u00060.001s to \u00060.1s.\nPScore : the width of the cross-correlation increases from\n0.01 to 0.5 times the median IAI.\nCemgil : the standard deviation of the Gaussian error\nfunction grows from 0.001s to 0.1s.\nGoto : to allow a similar one-dimensional representation,\nwe make all three parameters identical and vary them\nfrom\u00060.005 to \u00060.5 times the IAI.\nContinuity-based : the size of the tolerance window in-\ncreases from \u00060.005 to \u00060.5 times the IAI.\nInformation Gain : we vary the number of bins in multi-\nples of 2 from 2 up to 100.\nIn the top row of Figure 2 the objective accuracy scores\nas a function of different parameterisations are shown. The\nplots in the bottom row show the corresponding correla-\ntions with subjective ratings. In each plot the dotted verti-\ncal line indicates the default parameters. From the top row\nplots we can observe the expected trend that, as the size of\nthe tolerance window increases so the objective accuracy\nscores increase. For the case of Information Gain the beat\nerror histograms become increasingly sparse due to having\nmore histogram bins than observations, hence the entropy\nreduces and the information gain increases. In addition,\nInformation Gain does not have a maximum value of 1,\nbut instead, log2of the number of histogram bins [4].\nLooking at the effect of correlation with subjective rat-\nings in the bottom row of Figure 2, we see that for most\nevaluation measures there is rapid increase in the correla-\ntion as the tolerance windows grow from very small sizesDefault Max. Correlation\nParameters Parameters\nF-measure 0.070s 0.049s\nPScore 0.200 0.110\nCemgil 0.040s 0.051s\nGoto 0.175 0.100\nContinuity-C 0.175 0.095\nContinuity-T 0.175 0.090\nInformation Gain 40 38\nTable 1. Comparison of default parameters per eval-\nuation measure with those which provide the maxi-\nmum correlation with subjective ratings in the Anno-\ntated+Offbeat+D/H condition.\nafter which the correlation soon reaches its maximum and\nthen reduces. Comparing these change points with the dot-\nted vertical lines (which show the default parameters) we\nsee that correlation is maximised for smaller (i.e. more re-\nstrictive) parameters than those currently used. By ﬁnding\nthe point of maximum correlation in each of the plots in\nthe bottom row of Figure 2 we can identify the parame-\nters which yield the highest correlation between objective\naccuracy and subjective ratings. These are shown for the\nAnnotated+Offbeat+D/H evaluation condition in Table 1\nfor which the correlation is typically highest. Returning to\nthe plots in the top row of Figure 2 we can then read off the\ncorresponding objective accuracy with the default and then\nmaximum correlation parameters. These accuracy scores\nare shown in Table 2.\nFrom these Tables we see that it is only Cemgil whose\ndefault parameterisation is lower than that which max-\nimises the correlation. However this does not apply for\ntheAnnotated only condition which is implemented in [4].\nWhile there is a small difference for Information Gain, in-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n640Annotated Annotated+Offbeat Annotated+Offbeat+D/H\nDefault Max Corr. Default Max Corr. Default Max Corr.\nParams Params Params Params Params Params\nF-measure 0.673 0.607 0.764 0.738 0.834 0.797\nPScore 0.653 0.580 0.753 0.694 0.860 0.792\nCemgil 0.596 0.559 0.681 0.702 0.739 0.779\nGoto 0.583 0.563 0.667 0.646 0.938 0.813\nContinuity-C 0.518 0.488 0.605 0.570 0.802 0.732\nContinuity-T 0.526 0.505 0.624 0.587 0.837 0.754\nInformation Gain 3.078 2.961 3.187 3.187 3.259 3.216\nTable 2. Summary of objective beat tracking accuracy under the three evaluation conditions: Annotated, Anno-\ntated+Offbeat andAnnotated+Offbeat+D/H per evaluation measure. Accuracy is reported using the default parameter-\nisation from [4] and also using the parameterisation which provides maximal correlation to the subjective ratings. For\nInformation Gain only performance is measured in bits.\nspection of Figure 2 shows that it is unaffected by varying\nthe number of histogram bins in terms of the correlation.\nIn addition, the inclusion of the extra evaluation criteria\nalso leads to a negligible difference in reported accuracy.\nTherefore Information Gain is most robust to parameter\nsensitivity and metrical ambiguity. For the other evalua-\ntion measures the inclusion of the Annotated+Offbeat and\ntheAnnotated+Offbeat+D/H (in particular) leads to more\npronounced differences. The highest overall correlation\nbetween objective accuracy scores and subjective ratings\n(0.89) occurs for Continuity-T for a tolerance window of\n\u00069% of the IAI rather than the default value of \u000617.5%.\nReferring again to Table 2 we see that this smaller tol-\nerance window causes a drop in reported accuracy from\n0.837 to 0.754. Indeed a similar drop in performance can\nbe observed for most evaluation measures.\n4. DISCUSSION\nBased on the analysis of objective accuracy scores and sub-\njective ratings on this dataset of 48 excerpts, we can infer\nthat: i) a higher correlation typically exists when the Anno-\ntated+Offbeat and/or Annotated+Offbeat+D/H conditions\nare included, and ii) for the majority of existing evaluation\nmeasures, this correlation is maximised for a more restric-\ntive parameterisation than the default parameters which are\ncurrently used [4]. A strict following of the results pre-\nsented here would promote either the use of Continuty-T\nfor the Annotated+Offbeat+D/H condition with a smaller\ntolerance window, or Information Gain since it is most re-\nsilient to these variable evaluation conditions while main-\ntaining a high subjective vs. objective correlation.\nIf we are to extrapolate these results to all existing work\nin the beat tracking literature this would imply that any pa-\npers reporting only performance for the Annotated condi-\ntion using F-measure andPScore may not be as represen-\ntative of subjective ratings (and hence true performance) as\nthey could be by incorporating additional evaluation con-\nditions. In addition, we could infer that most presented ac-\ncuracy scores (irrespective of evaluation measure or eval-\nuation condition) are somewhat inﬂated due to the use of\nartiﬁcially generous parameterisations. On this basis, wemight argue that the apparent glass ceiling of around 80%\nfor beat tracking [10] (using Continuity-T for the Anno-\ntated+Offbeat+D/H condition) may in fact be closer to\n75%, or perhaps lower still. In terms of external evidence\nto support our ﬁndings, a perceptual study evaluating hu-\nman tapping ability [7] used a tolerance window of \u000610%\nof the IAI, which is much closer to our “maximum corre-\nlation” Continuity-T parameter of \u00069% than the default\nvalue of \u000617.5% of the IAI.\nBefore making recommendations to the MIR commu-\nnity with regard to how beat tracking evaluation should be\nconducted in the future, we should ﬁrst revisit the makeup\nof the dataset to assess the scope from which we can draw\nconclusions. All excerpts are just 15s in duration, and\ntherefore not only much shorter than complete songs, but\nalso signiﬁcantly shorter than most annotated excerpts in\nexisting datasets (e.g. 40s in [10]). Therefore, based on\nour results, we cannot yet claim that our subjective vs.\nobjective correlations will hold for evaluating longer ex-\ncerpts. We can reasonably speculate that an evaluation\nacross overlapping 15s windows could provide some lo-\ncal information about beat tracking performance for longer\npieces, however this is currently not how beat tracking\nevaluation is addressed. Instead, a single score of accuracy\nis normally reported regardless of excerpt length. With\nthe exception of [3] we are unaware of any other research\nwhere subjective beat tracking performance has been mea-\nsured across full songs.\nRegarding the composition of our dataset, we should\nalso be aware that the excerpts were chosen in an unsuper-\nvised data-driven manner. Since they were sampled from\na much larger collection of excerpts [1] we do not believe\nthere is any intrinsic bias in their distribution other than any\nwhich might exist across the composition of the Million-\nSongSubset itself. The downside of this unsupervised sam-\npling is that we do not have full control over exploring spe-\nciﬁc interesting beat tracking conditions such as off-beat\ntapping, expressive timing, the effect of related metrical\nlevels and non-4/4 time-signatures. We can say that for the\nfew test examples where the evaluated beat tracker tapped\nthe off-beat (shown as zero accuracy points in the Anno-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n641tated condition but non-zero for the Annotated+Offbeat\ncondition in Figure 1), were not rated as “bad”. Likewise,\nthere did not appear to be a strong preference over a single\nmetrical level. Interestingly, the ratings for the unannotat-\nable excerpt were among the lowest across the dataset.\nOverall, we consider this to be a useful pilot study\nwhich we intend to follow up in future work with a more\ntargeted experiment across a much larger musical collec-\ntion. In addition, we will also explore the potential for us-\ning bootstrapping measures from Text-IR [14] which have\nalso been used for the evaluation of evaluation measures.\nBased on these outcomes, we hope to be in a position to\nmake stronger recommendations concerning how best to\nconduct beat tracking evaluation, ideally towards a sin-\ngle unambiguous measurement of beat tracking accuracy.\nHowever, we should remain open to the possibility that dif-\nferent evaluation measures may be more appropriate than\nothers and that this could depend on several factors, includ-\ning: the goal of the evaluation; the types of beat tracking\nsystems evaluated; how the ground truth was annotated;\nand the make up of the test dataset.\nTo summarise, we believe the main contribution of this\npaper is to further raise the proﬁle and importance of\nevaluation in MIR, and to encourage researchers to more\nstrongly consider the properties of evaluation measures,\nrather than merely reporting accuracy scores and assum-\ning them to be valid and correct. If we are to improve un-\nderlying analysis methods through iterative evaluation and\nreﬁnement of algorithms, it is critical to optimise perfor-\nmance according to meaningful evaluation methodologies\ntargeted towards speciﬁc scientiﬁc questions.\nWhile the analysis presented here has only been applied\nin the context of beat tracking, we believe there is scope\nfor similar subjective vs. objective comparisons in other\nMIR topics such as chord recognition or structural segmen-\ntation, where subjective assessments should be obtainable\nvia similar listening experiments to those used here.\n5. ACKNOWLEDGMENTS\nThis research was partially funded by the Media Arts and\nTechnologies project (MAT), NORTE-07-0124-FEDER-\n000061, ﬁnanced by the North Portugal Regional Opera-\ntional Programme (ON.2–O Novo Norte), under the Na-\ntional Strategic Reference Framework (NSRF), through\nthe European Regional Development Fund (ERDF), and\nby national funds, through the Portuguese funding agency,\nFundac ¸ ˜ao para a Ci ˆencia e a Tecnologia (FCT) as well as\nFCT post-doctoral grant SFRH/BPD/88722/2012. It was\nalso supported by the European Union Seventh Frame-\nwork Programme FP7 / 2007-2013 through the GiantSteps\nproject (grant agreement no. 610591).\n6. REFERENCES\n[1] T. Bertin-Mahieux, D. P. W. Ellis, B. Whitman, and\nP. Lamere. The million song dataset. In Proceedings of 12th\nInternational Society for Music Information Retrieval Con-\nference, pages 591–596, 2011.[2] N. Collins. Towards Autonomous Agents for Live Computer\nMusic: Realtime Machine Listening and Interactive Music\nSystems. PhD thesis, Centre for Music and Science, Faculty\nof Music, Cambridge University, 2006.\n[3] R. B. Dannenberg. Toward automated holistic beat tracking,\nmusic analysis, and understanding. In Proceedings of 6th\nInternational Conference on Music Information Retrieval,\npages 366–373, 2005.\n[4] M. E. P. Davies, N. Degara, and M. D. Plumbley. Evaluation\nmethods for musical audio beat tracking algorithms. Tech-\nnical Report C4DM-TR-09-06, Queen Mary University of\nLondon, Centre for Digital Music, 2009.\n[5] S. Dixon. Evaluation of audio beat tracking system beatroot.\nJournal of New Music Research, 36(1):39–51, 2007.\n[6] J. S. Downie. The music information retrieval evaluation\nexchange (2005–2007): A window into music informa-\ntion retrieval research. Acoustical Science and Technology,\n29(4):247–255, 2008.\n[7] C. Drake, A. Penel, and E. Bigand. Tapping in time with me-\nchanically and expressively performed music. Music Percep-\ntion, 18(1):1–23, 2000.\n[8] M. Goto and Y . Muraoka. Issues in evaluating beat tracking\nsystems. In Working Notes of the IJCAI-97 Workshop on Is-\nsues in AI and Music - Evaluation and Assessment, pages\n9–16, 1997.\n[9] P. Grosche, M. M ¨uller, and C. S. Sapp. What Makes Beat\nTracking Difﬁcult? A Case Study on Chopin Mazurkas. In\nProceedings of the 11th International Society for Music In-\nformation Retrieval Conference, pages 649–654, 2010.\n[10] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. Oliveira, and\nF. Gouyon. Selective sampling for beat tracking evaluation.\nIEEE Transactions on Audio, Speech and Language Process-\ning, 20(9):2539–2460, 2012.\n[11] J. R. Iversen and A. D. Patel. The beat alignment test (BAT):\nSurveying beat processing abilities in the general population.\nInProceedings of the 10th International Conference on Mu-\nsic Perception and Cognition, pages 465–468, 2008.\n[12] M. Miron, F. Gouyon, M. E. P. Davies, and A. Holzapfel.\nBeat-Station: A real-time rhythm annotation software. In\nProceedings of the Sound and Music Computing Conference,\npages 729–734, 2013.\n[13] D. Moelants and M. McKinney. Tempo perception and musi-\ncal content: what makes a piece fast, slow or temporally am-\nbiguous? In Proceedings of the 8th International Conference\non Music Perception and Cognition, pages 558–562, 2004.\n[14] T. Sakai. Evaluating evaluation metrics based on the boot-\nstrap. In Proceedings of the International ACM SIGIR confer-\nence on research and development in information retrieval,\npages 525–532, 2006.\n[15] A. M. Stark. Musicians and Machines: Bridging the Seman-\ntic Gap in Live Performance. PhD thesis, Centre for Digital\nMusic, Queen Mary University of London, 2011.\n[16] J. Urbano, M. Schedl, and X. Serra. Evaluation in Music In-\nformation Retrieval. Journal of Intelligent Information Sys-\ntems, 41(3):345–369, 2013.\n[17] J. R. Zapata, A. Holzapfel, M. E. P. Davies, J. L. Oliveira, and\nF. Gouyon. Assigning a conﬁdence threshold on automatic\nbeat annotation in large datasets. In Proceedings of 13th In-\nternational Society for Music Information Retrieval Confer-\nence, pages 157–162, 2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n642"
    },
    {
        "title": "Ten Years of MIREX (Music Information Retrieval Evaluation eXchange): Reflections, Challenges and Opportunities.",
        "author": [
            "J. Stephen Downie",
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Kahyun Choi",
            "Sally Jo Cunningham",
            "Yun Hao"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417181",
        "url": "https://doi.org/10.5281/zenodo.1417181",
        "ee": "https://zenodo.org/records/1417181/files/DownieHLCCH14.pdf",
        "abstract": "The Music Information Retrieval Evaluation eXchange (MIREX) has been run annually since 2005, with the Oc- tober 2014 plenary marking its tenth iteration. By 2013, MIREX has evaluated approximately 2000 individual music information retrieval (MIR) algorithms for a wide range of tasks over 37 different test collections. MIREX has involved researchers from over 29 different countries with a median of 109 individual participants per year. This paper summarizes the history of MIREX from its earliest planning meeting in 2001 to the present. It re- flects upon the administrative, financial, and technologi- cal challenges MIREX has faced and describes how those challenges have been surmounted. We propose new fund- ing models, a distributed evaluation framework, and more holistic user experience evaluation tasks-some evolu- tionary, some revolutionary for the continued success of MIREX. We hope that this paper will inspire MIR com- munity members to contribute their ideas so MIREX can have many more successful years to come.",
        "zenodo_id": 1417181,
        "dblp_key": "conf/ismir/DownieHLCCH14",
        "keywords": [
            "Music Information Retrieval Evaluation eXchange (MIREX)",
            "annual",
            "2005",
            "Oc- tober 2014 plenary",
            "tenth iteration",
            "approximately 2000 individual music information retrieval (MIR) algorithms",
            "wide range of tasks",
            "37 different test collections",
            "over 29 different countries",
            "109 individual participants per year"
        ],
        "content": "T\n(\nt\nM\nm\nr\nh\nw\nT\ne\nf\nc\nc\ni\nh\nt\nM\nm\nh\nM\n(\nt\nE\nI\na\n(\nr\nw\nt\ns\nt\nr\nt\na\np\nt\na\nlT\n(M\nto\nM\nm\nra\nha\nw\nT\nea\nfl\nca\nch\nin\nho\nti\nM\nm\nha\nM\n(M\nth\nE\nIl\na \n(M\nre\nw\nth\nsi\nth\nre\nta\nan\npo\nti\nab\nlu\nS\nA\np\nh\nO\ntr\nTh\nM\nob\nMI\nmu\nan\nas\nwit\nTh\nar\nle\nal\nha\nng\nol\no\nMI\nmu\nav\nMu\nM\nhe\nEv\nlli\nf\nM\nes\nwh\nhe\nig\nhe\nes\nak\nnd\noi\no\nbi\nut\nSa\nAtt\nph\nham\nOp\nrie\nhe\nMI\nbe\nIR\nus\nng\ns \nth\nhis\nrli\nct\nl c\nal\ng m\nli\nn\nIR\nun\nve\nus\nMI\ne \nva\nin\nfr\nMI\nsu\nha\ne t\ngn\ne r\nse\nA\nke\nd \nin\nn\nili\ntio\nlly\ntri\nen\nm\npp\nev\nj\nc\n \nR\ner\nRE\nsic\nge\nin\nh \ns \nie\nts\nch\nlle\nm\nst\nnar\nRE\nni\ne \nsi\nR\nI\nalu\nno\nam\nR\nult\nat \nte\nnif\nre\nea\nAs\ne \nc\nnt\nn f\nit\non\ny \nib\nn \nm, \npor\nva\nj\nc\nM\nRE\nr 2\nEX\nc \ne \nnv\na\np\nes\ns u\nha\nen\nmo\nti\nry\nEX\nty\nm\nc \nRE\nIn\nua\nois\nm\nR)\nts\nT\nex\nfi\nes\narc\ns \nth\nch\nt w\nfr\nty\nns\nJo\nbu\nD\nY\nrtu\nal C\nd\nk\nMu\nEX\n20\nX\nin\no\nvo\na \npa\nst \nup\nal\nng\nod\nc \ny,\nX\ny \nma\nEX\nnte\nat\ns \nme\n r\ns \nTe\nxt \nc\nse\nch\nth\nhi\nha\nw\nram\ny o\ns, \no \ntio\nDo\nYu\nun\nC\nJ\nU\ndo\nU\nka\nus\nX\n0\nX \nn\nof \nol\nm\nap\np\np\nlle\nge\nde\nu\n, s\nX. \nm\nan\nI\nX\ner\ntio\na\new\nre\no\nex\ni\nan\nea\nh \nhi\nis \nall\nwh\nm\nof\na\nC\non\nw\nun \nnit\non\nJ.\nU\now\nU\nah\nsi\nX) \n14\nh\nfo\nt\nlv\nme\npe\npl\no\nen\nes\nel\nus\nso\nW\nme\nny\nIn\nX) \nrn\non\nat \nwo\nes\nf \nxt\nin\nnt\narc\np\nis\no\nle\nhe\nme\nf \nan\n \nCu\nn 4\nwn\nH\ntie\nnf\n. S\nUn\nwn\nK\nUn\nhy\nic\nh\n4 \nha\nor\nta\nve\ned\ner \nla\nn\nng\ns h\ns,\nse\nom\nW\nem\ny \nnf\ni\nna\nn \nU\nor\nse\nt\nt R\nnfo\ntly\nch\npri\ns y\nop\nen\nre\new\nM\nnd\n©\nunn\n4.\nie\nHa\nes\nferS\nniv\nn\nK\nniv\nyu\n \nha\np\nas\nrm\nsk\ned\ndi\ns\nan\nn t\nge\nh\n, a\ner\nm\nWe\nm\nm\nfo\ns \nati\nL\nUr\nrk\near\nth\nR\nor\ny \nh \nio\ny\npp\nng\ne \nwo\nMI\nd h\n© \nni\n.0 \ne, \nao\ns”\nrete\nv\ni\nKa\nv\nu\nIn\nas\npl\ns \nma\nks\nd r\nia\nsu\nnn\nth\nes\nav\na \nr \ne \ne \nmb\nmo\norm\na\nio\nLa\nrb\nk \nrc\nhe\nRe\nrm\nc\nc\nor\nea\npo\nge\na\nor\nIR\nh\nJ.\ning\nIn\nX\no. \n, \nencep\ne\ne\na\ne\n2\nnf\n b\nle\ne\nat\ns \nre\nan\num\nnin\nhe\ns M\nve\nd\ne\nr\nh\nbe\nor\nm\nan\non\nab\nba\na\nch\nir\netr\nm\nco\nco\nrit\nar\nor\nes \na n\nrk\nRE\nop\n S\ngh\nnt\nXia\n“\n15\ncep\nrs\ne@\nh\nrs\n2@\nfo\nbe\nn\nev\ntio\no\nes\nn \nm\nng\ne \nM\ne \ndis\nex\nrev\nho\ners\nre\nma\nn \nna\nbo\nan\nan\nhe\nr \nrie\nat\non\nom\ntie\nr \nrt\no\nne\nk \nE\np\nSte\nha\nter\nao\n“T\n5t\ne, ph\nsi\n@i\nhy\nsi\n@i\nor\nee\nnar\nva\non\nov\nse\no\nmm\ng \nad\nMI\nb\nst\nxp\nv\nop\ns \ne s\nat\nan\nal \nor\nna\nnd\ner\nM\nev\nti\nnt\nmm\nes\nm\ntu\nov\new\nm\nEX\ne \nep\nam\nrn\no H\nTen\nth \n20R\nhe\nity\nil\nyu\nity\nil\nrm\nen\nry\nalu\nn \nve\nea\nof \nma\nm\ndm\nIR\nbe\ntri\npe\no\npe\nto\nsu\n1\ntio\nn\nM\nra\na-\nd \nrs \nMI\nva\no\ntr\nm\ns. \nma\nun\nve\nw\nmu\nX.\nt\nph\nm, \nna\nHu\nn \nI\n01R\nen\ny \nl\nu\ny \nl\nma\nn r\ny \nua\nr\ner \nrc\nf\nar\nm\nm\nRE\nee\nib\nri\nlu\n t\no \nuc\n. \non\nn\nM\nato\n-C\nv\nt\nIR\nal\nn\nrib\nmu\n \nar\nnit\ner \nw f\nus\n W\nth\nhen\nY\ntio\nu,\nY\nnt\n14RE\nn\no\nl\nn\no\nl\nat\nru\nm\nat\nre\n3\nch\n10\nriz\nme\nmin\nEX\nen\nbu\nie\nut\nth\nc\ncc\nI\nn \nu\nMu\nor\nCh\nve\nto\nR \nl C\nn r\nbu\nun\nrk\nty\nt\nfu\nst\nW\nat\nn \nYu\non\n, J\nYe\nte\n4. E\n D\nof\ni\nn C\nof\ni\nA\ntio\nun\nma\nte\netr\n37\nhe\n09\nze\net\nn\nX\nn s\nut\nen\ntio\nha\nco\nce\nIN\nual\nus\nry\nha\nn\no \na\nC\nre\nut\nnit\nks\ny \nth\nun\nt b\nWe\nt \nD\nun\nna\nJin\near\nrnEF\nD\nf I\nin\nC\nf I\nin\nA\non\nn \nar\ned\nri\n7 \ner\n9 \nes\nti\nis\nX h\nsu\nted\nnc\non\nat \non\nes\nNT\nR\nl \nsi\ny \nam\nnu\nc\nal\nCo\netr\nte\nty\ns \nto\nhe \nnd\nbe\ne \nth\nDo\nn H\nal \nn \nrs\nnaF\nDo\nIl\nno\nCh\nIl\nno\nAB\nn \na\nrk\nd \nev\nd\nrs \ni\ns \nin\nstr\nha\nur\nd \ne \nna\nth\nntr\nssf\nT\nR\ne\nc \n(\nmp\nue \no\nlg\non\nri\ned\ny,\nth\no \np\ndin\ne \np\nhi\now\nHa\nL\nH\ns o\natiL\now\nlli\noi\nh\nlli\noi\nBS\nR\nan\nki\na\nva\ndif\nf\nin\nth\nng\nra\nas\nrm\ne\nar\nh\nri\nfu\nTR\net\nev\nI\n(IM\np\nf\nm\ngo\nnf\nev\nd \n a\nhe\nr\npa\nng\nd\npro\nis \nwn\nao\nLic\nHa\nof\nonLE\nw\nin\ni\no\nin\ni\nST\nR\nnn\nin\nap\nal\nff\nfro\nnd\nhe\ng \nat\ns \nmo\nev\nev\nry\nis\nib\nul\nRO\ntr\nva\nIn\nM\nai\nfo\nmp\nori\nfer\nva\nto\nan\ne \nre\nas\ng \nde\nop\nw\nnie\no. \ncen\na L\nf M\nnaE\nwn\nno\ns\noi \nno\ns\nT\nRet\nnu\nng\npp\nl \nfe\nom\ndiv\ne \nin\ntiv\nfa\nou\nva\nva\ny—\ns p\nbu\nl y\nO\nrie\nlu\nnf\nMI\nig\nor\npa\nit\nre\nal\no \nnd\nte\nfl\nst \nm\nev\npo\nw\ne, \nL\nns\nLe\nM\nal EC\nni\noi\n.\noi\n.\nTR\ntr\nua\ng i\npr\n(M\nre\nm\nv\nh\nn \nve\nfac\nun\nalu\nal\n—\np\nute\nye\nOD\nev\nua\nfo\nIR\ngn\nr \nar\nth\nen\nl c\nt\nd \nen\nle\nd\nm\nve\nos\nil\nX\nLic\nse\nee\nMIR\nSC\nie\nis\ne\nis\ne\nRA\nrie\nall\nit\nro\nM\nen\nm \nid\nhi\n2\ne, \nce\nnt\nua\nlu\n—f\nap\ne \nea\nDU\nva\nat\norm\nRS\nn \nm\nre\nhm\nnc\nco\nth\na\nnt\nec\nde\nmo\nelo\nse\nll \nXia\nce\ne (\n, \nRE\nSoCT\ne \ns \ned\ns \ned\nA\nev\nly\ns \nox\nMI\nnt\no\ndu\nis\n20\nf\ned\nte\nat\nua\nfor\npe\nth\nar\nU\nal\ntio\nm\nSE\n(U\nmu\n, \nms\nce\nom\nhe \nals\nth\nt \nec\nod\nop\ne \ns\nao\nen\n(C\nK\nEX\nociT\ndu\ndu\nAC\nva\ny s\nt\nxim\nIR\nt t\nov\nua\nto\n00\nfin\nd \ned\ntio\nati\nr \ner\nhe\nrs \nC\n \non\nma\nE\nU\nus\nc\ns \ne (\nm\ng\nso\nh \nu\nca\nde\npe\no\np\no H\nse\nCC\nKah\nX\nietTI\nu\nu\nCT\nal\nsi\nten\nm\nR)\nte\nver\nal \nor\n01\nn\nan\nd. \non\nio\nth\nr \nei\nt\nCT\nn \nat\nL\nUIU\nsi\nco\na\n(T\nmm\ngr\no \nru\nup\nd\nl \ned\non\nar\nHu\ned\nC B\nhy\n: \ntyIO\n \n \nT \nl \nin\nnt\nma\n) a\nes\nr \np\nry\n1 \nan\nn\nW\nn \non\nhe\nw\nir\no \nTI\nE\nc\nio\nL) \nU\nc \non\nan\nTR\nmu\nro\na\nun\npo\ne\na\nd \nne\nrk\nu,\nd u\nBY\nyu\nR\ny fO\nE\nnc\nth\nate\nal\nst \n2\np\ny \nt\nnc\nd \nW\nf\nn \ne \nwi\nr i\nc\nIO\nEv\nca\non\na\nUC\ni\nntr\nnd\nR\nun\now\nff\nnn\non\n. W\nan\nto\n o\nk \n, J\nun\nY \nun\nRef\nfoO\nEv\ne \nh \nel\nlg\nc\n29\nar\no\no \nci\nd\nWe\nfra\nta\nc\nill\nid\nco\nO\nva\nam\nn \nat\nC)\nin\nra\nd \nRE\nn\nw\nfe\nni\nn \nW\nnd\no \nof\nth\nJin\nnd\n4\nn C\nfle\nor N\nva\n2\nit\nly\ngo\nco\n9 d\nrt\nof \nt\nia\nde\ne p\nam\nas\no\nl \nde\nom\nON\nalu\nmp\nR\nt \n). \nnf\nas\nsy\nEC\nit\nwth\nec\nin\nit\nWe\nd d\ne\nf \nhe\nn H\nder\n4.0\nCh\nec\nMN\nalu\n20\nte\ny \nor\noll\ndi\ntic\nfM\nth\nal,\nes\npr\nm\nsk\nn\nin\nea\nm\nN \nua\npa\nR\nth\nM\nfor\nst,\nys\nC)\nty\nh \ncte\nng\nts\ne \ndi\nen\nm\ne \nH\nr a\n0)\nho\ncti\nMuNS\nua\n00\nera\n2\nrit\nle\nif\nci\nM\nhe\n, \nsc\nro\nme\nks\nnti\nns\nas \ne\nat\nai\nRet\nhe\nM\nrm\n, \nst\n) \ny [\na\ned\ng \ns h\na\nis\nns\nma\nd\nHa \na \n. A\noi,\nio\nusT\nS,\ns\nat\n05\nat\n20\nth\nec\nffe\nip\nMI\ne \nan\ncri\nop\new\ns—\nin\nsp\ns\n. \ntio\nig\ntr\ne \nMI\nm\nan\nte\nh\n[7\nan\nd \no\nhi\nare\ntr\nsu\nan\ndis\nL\nC\nAt\n, S\nns\nsicT\n, \nU\nS\na\ntio\n5,\nti\n00\nhm\ncti\ner\npa\nIR\np\nnd\nib\npo\nwo\n—\nnu\npir\nso\non\ngn\nrie\nU\nIR\nma\nn\nem\nha\n7]\nnd\nth\nof \nis\ne \nrib\nure\nny\nsc\nLe\nCre\ntt\nSa\ns, \nc TE\nC\nUn\nSa\nU\nal\non\n w\non\n00\nms\nio\nre\nan\nRE\npre\nd \nbe\nos\nor\n—s\nue\nre\no M\nn \nn m\nev\nU\nRE\nati\nd \nms\ns \n. \nd \nhe\nfM\nsto\na\nbu\ne \ny \ncu\ne,\nea\nri\nall\nC\nInE\nC\nni\nal\nU\nll\nn \nw\nn\n0 \ns \non\nen\nnts\nEX\nes\nt\nes\nse\nrk\nso\ned\ne \nM\nm\nva\nUn\nEX\nio\nd\ns, \np\nM\nm\ne \nM\nor\nat \nut\nt\np\nus\n, K\nati\nib\nly\nCh\nnfoEN\nH\niv\nx\nll\nUn\nly\ne\nwit\n. \ni\nf\nns\nnt \ns \nX\nse\nte\n h\n n\nk, \nom\nd s\nM\nMI\ne\nma\nal\nniv\nX\non\ndi\ns\npr\nMI\nm\ns\nMI\nry\nth\nte\nh\npo\nss\nKa\nve\nbu\ny J\nha\nforN\nH\nve\nxi\nly\nniv\ny\neX\nth\nB\nin\nfor\ns. \nc\np\nX \nen\nec\nho\nne\na\nme\nsu\nM\nIR\neX\nan\n \nve\nX p\nn \nis\nsi\nro\nIR\nat\nh\nIR\ny,\nhe\ned\ne \nos\nio\nah\ne \nti\nJo\nall\nrmN \nHA\ner\nia\ny \nv\nj\nX\nh t\nBy\nnd\nr \nM\nco\npe\nfr\nnt\nh\now\new\nan\ne \nuc\nIR\nRE\nX\nna\nS\ner\npr\nre\nscu\nim\nov\nRE\ntu\nhap\nRE\n i\ne \nd e\ns\nsi\non\nhy\nC\non\no C\nlen\nmaY\nA\nrs\nao\nJ\nve\no\nXc\nth\ny \ndiv\na\nM\nou\ner \nro\nt. \nn\nw\nw \nnd\ne\ncc\nR\nEX\nXc\nag\nSy\nrs\nro\net\nu\nmi\nvid\nE\nur\np\nEX\nim\nc\nev\nsu\nib\nn \nyun\nCom\nn:\nCu\nng\natiY\nAL\nX\nsit\no\nJo\nrs\no@\nh\nhe\n2\nvi\na w\nMIR\nun\ny\nom\nI\nno\nw t\nf\nd m\nev\nce\nR c\nX\nh\nge\nys\nsit\nov\ntri\nuss\nila\nde\nEX\nrit\nin\nX\nmp\ncri\nva\nus\nble\nin\nn \nm\n: J\nun\nge\nionE\nL\nX\nty\nx\no \nsi\n@w\nhan\ne O\n20\nid\nw\nR\nntr\nye\nm\nIt \nlo\nth\nfu\nm\nvo\ness\nco\nX \nhan\ned\nte\nty\nvi\nie\ns \nar\ned\nX \nty\nng\nX, \np\niti\nal\nsta\ne \nn \nC\nmm\nJ. \nnn\nes \nn EA\nLL\nXia\ny \nxh\nC\nity\nwa\nng\nO\n01\ndu\nwid\nRE\nri\nea\nm i\nr\nog\nho\nun\nmo\nol\ns \nom\nc\nng\nd b\nem\ny \nid\nev\nth\nr \nd \nh\ny \ng \nw\nac\nic\nlu\nai\ns\nth\nCh\nmo\nS\nnin\na\nRA\nL\na\no\nhu\nCu\ny \na\ng\nOc\n13\nua\nd\nEX\nie\nar\nit\nre\ngi\nos\nnd\nor\nlu\no\nm\nan\ng\nby\nm\no\nde\nva\nh\nto\nto\nha\no\no\nw\nct\nca\nua\nin\nso\nh\nhoi\non\nte\nng\nand\nReA\nLE\no\nof \nu@\nu\no\ni\ne \nc-\n3, \nal \ne \nX \ns \nr. \nts \ne-\ni-\ne \nd-\ne \nu-\nof \nm-\nn \ne \ny \nms \nof \ns \nal \ne \no \no \nas \nof \nof \ne \nt, \nal \na-\nn-\no-\ne \ni,\nns\ne-\ng-\nd\ne-R\nE\no H\nfH\n@h\nun\nof\nk\nf\nf\nf\nfR\nN\nH\nH\nh\nnn\nf W\nkaRS\nN\nHu\nHo\nhk\nni\nW\natS \nG\nu\non\nku\nin\nW\ntoO\nG\n \nng\nu.\nn\nWa\no\nM\nc\nw\nM\nu\nT\nt\nt\nG\ny\n“\ne\na\nI\nM\na\nT\nF\ns\ni\nc\ns\nU\nM\nn\nw\ntu\n(\nF\nc\nN\nI\n(\ne\na\nv\na\n2\na\ns\nm\nf\nd\ns\nSO\nGE\ng \n.h\ng\naik\n.\nM\ncr\nwe\nM\nua\nTh\nhr\ni\nGr\nye\n“B\nev\nad\nIS\nM\nan\nTe\nFa\nsc\nn\ncia\nse\nUn\nM\nne\nwo\nu\n(S\nFr\nce\nN\nIll\n(K\nev\nalg\nvo\na \n2)\na \nse\nm\nfic\nde\nso\nSiOF\nE\nK\nh\ngh\nka\na\nMIR\nrit\ne \nMIR\nati\nhe\ne \nie\nro\nea\nBl\nva\ndd\nSM\nMIR\nnd\nI\nec\nab\ncri\nsi\nal\ness\nni\nMIR\new\nor\nra\nSA\nro\neiv\nS\nlin\nKE\nve\ngo\nol\nm\n). \nw\nea\nac\nca\nep\nom\ninF\nES\nKo\nk\nha\nat\nac\nR\ntic\nR\nio\ne \nE\nev\nou\nar\nlo\nalu\ndit\nM\nR\nd t\nIn\nch\nbr\nip\nig\nlly\nsi\niv\nR\nw \nrk\nal\nAL\nom\nve\nF\nno\nET\nen\nB\nor\nv\nme\nT\nwi\narc\nch\nat\npe\nme\nngF M\nS \non\nk \nam\nto\nc\nR c\nci\nm\nRE\non\n2\nh\nE\nva\nup\ns \noo\nua\nti\nIR\nRE\nth\nn \nhn\nra\npti\ngh\ny \nio\nve\nRE\nt\nke\nl \nLA\nm \ned\nF, \noi\nT\nnts\nBy\nri\nved\ned\nTh\nid\nch\nhi\ntio\nen\ne \nginM\nA\nng\nm\no \n.n\nco\nism\nma\nEX\nn \n2. \nhis\nEx\nal \np \nom\nat\no\nR\nEX\nhe\n2\nno\na \nio\nht\nb\non\ner\nEX\nte\ned\nA\nA\n2\nd \nT\nis\nI)\ns \ny \nith\nd \ndi\nhe\nde\nhe\nin\non\nnd\nty\nngM\nA\ng \nm \nn\nom\nm\nak\nX\nta\nH\nst\nxp\nw\nI\nla\nmi\ntio\nn\nR2\nX \ne N\n20\nol\no\non\ns \nbe\nns\nrsi\nX\nec\nd \nAn\nAM\n20\na\nTh\ns, \n). \nin\n2\nhm\nr\nia\ne t\ne \ner\nne\nn)\nd \nyp\ng/MI\nAN\nz\nm\nms\nke\n, \nas\nH\nto\nplo\nw\nIn\nat\nin\non\nna\n0\nf\nN\n00\nlo\nrg\nn \nf\neg\n \nity\n, \nch\nE\nna\nM\n00\nap\nhe\na\nT\nn \n20\nm\nre\nan\nta\ns\nrs \ne-\n), \n(e\npe\n/ HIR\nN\nz \nmm\ns p\ne \nem\nsk\nHI\nory\nor\nwa\nnfo\ner\nng\nn \nl \n0\nfr\nNa\n04\ng\nga\nC\nfo\nga\nin\ny \nth\nhn\nEn\naly\nMI)\n00\npp\ne \nan\nT\nth\n01\nms\nes\nn o\nas\np\ni\nle\n“\ne.\nes\nHuR\nND\nmu\np\nm\nks\nS\ny \nra\ns \nor\nr,\ngt\np\nw\n02\nro\nati\n4,\ngy\nan\nCo\nor \nan\nn \nhr\nno\nnv\ny\n) \n0 \npr\nA\nnd\nab\nhe\n13\n o\nea\nof\nsk\ne\nin\nea\n“lo\ng\ns \numRE\nD\nun\nro\nr\nmp\n. \nST\no\nato\nh\nrm\n \non\npl\nwo\n2 \nom\nio\n \ny \nni\non\nM\nn \nI\no\nre\nolo\nvir\nsi\nan\nto\nro\nAn\nd t\nbl\ne \n3, \nov\nar\nf \nks \nct\nn t\narn\now\ng.,\no\nmE\nD \nn\nov\nec\nph\nTO\nof\nor\nhe\nm\nth\nn \nla\nor\nan\nm \non\nth\nG\niz\nnt\nM\nin\nIS\nf \nee\nog\nro\nis\nn\no \nox\nnd\nth\nle\nd\nM\nve\nrc\n1\na\ntr\nth\nn\nw\n A\nof \nmmEX\nO\nit\nvi\nco\nha\nO\nf M\nry\nel\nmat\nh\natf\nrk\nnd\nb\nna\nhe\nGr\nze\nte\nMI\nn \nSM\nL\ne a\ngi\non\n o\nd \nd\nxi\ndr\nhe\ne \nde\nM\ner\nch\n10\nan\nru\nhe\nin\nw-\nA\nfu\nmX\nO\nty\nid\nom\nas\nOR\nM\ny \nld\nti\ne \nM\nfo\nks\nd \nbo\nal \ne \nro\nd\nes\nIR\n2\nM\nL\nad\nie\nnm\no\nM\nda\nm\nre\ne \n1\nev\nMIR\nr \nhe\n08\nnd\num\ne l\nng\nle\nAu\nus\nminX:\nOP\ny. \nde\nm\nsiz\nRY\nMI\nW\nd \nio\nM\nor\nsh\nS\not\nS\nl\nou\nd \nst \nRE\n20\nIR\no\ndd\nes\nm\nf \nM\nate\nma\new\nK\n1 \nve\nR\n3\ner\n8 \nd \nm \nla\ng \nev\nud\nse\nng: \nP\ny\nIn\ned\nmm\nzi\nY\nIR\nW\na\nn\na\nan\nrm\nho\nS\nth\nSc\nlo\nup\nan\n(\nEX\n00\nR\nn\ndi\ns \nme\nL\nMIR\ne,\nat\nw \nKo\nsu\nlo\nRE\n7\ns \nin\nsu\no\nas\ntr\nve\ndio\ner\ng) \nPP\nU\ny\nn \nd b\nme\nin\n, \nRE\nWo\nas \nn R\natt\nni\nm \nop\nIG\nh \ncie\noc\np \nn \n(A\nX\n05\nR 2\nnd\niti\na\nn\nLa\nR\n, M\ntel\nW\nor\num\nop\nEX\n7 d\nf\nnd\nub\nof\nst \nra\nel\no \n-i\n). P\nU\nyu\na\nby\nen\nng\nS\nEX\nor\np\nR\nte\nif\nf\nps\nG\nth\nen\nal\n(\nAD\nX. \n5 \n2\ndo\nio\nan\nt \nar\nRE\nM\nly\nW\nre\nm\npm\nX \ndi\nfro\ndi\nbt\nf \nd\nain\n” \nB\nis\nTO\nn\nU\nun\nad\ny \nnd\ng m\nST\nX\nrk\np\nRe\nn\nfe\nfo\ns \nGIR\nhe\nnc\nl \n(M\nev\nD\nA\na\n0\nn\non\nnd\nfo\nrg\nEX\nMI\ny \nW. \nea\nmm\nm\nh\nif\nom\niv\nta\nr\nde\nn-\nt\nB\nsu\nThO\nniv\nj\nU\nnh\ndd\nt\nda\nm\nTR\nX c\nks\nar\netr\nnd\nest\nor \no\nR\ne \nce\nc\nM\nva\nDC\nAf\nan\n0\nn. \nna\nd \nor\nge\nX\nIR\n$\nM\nan\nm\nme\nha\nffe\nm\nvi\nas\nre\nec\n-t\nta\nea\nue\nheOR\nve\ni\nUn\nha\ndi\nth\nat\nmo\nRU\nca\nh\nrt\nri\nde\nto\nM\non\nR2\nA\ne \nco\nMT\nal\nC)\nft\nnd\n5\nB\nal \nm\nr M\ne \n: \nRE\n$3\nM\nn E\nmar\nn\nas\ner\nm \ndu\nsk\nse\nca\nte\nsk\nat\ned\ne R\ner\nin\nniv\na\niti\nhe\ntio\nor\nU\nan\nho\nt \nev\nes\no”\nM\nn \n20\nAn\nF\nom\nTG\nlu\n) \nte\nd h\n, \nBu\np\nm\nM\nA\nN\nEX\n3,\nMe\nEl\nri\nnt \ns e\nre\no\nu\nks \nea\nad\nst\nks\nt \nd \neRT\nJ\nrs\nnh\nv\no\nio\n m\non\nre \nUC\nn \np\no\nva\ns \n” \nMI\nM\n00\nnd\nFo\nmm\nG)\nua\n[\nr \nha\nh\nu\npr\no\nMu\nAm\nNe\nX\n,1\nell\nle\niz\no\nev\nen\nv\nal\ne\nar\nde \nt \ns \nT\nm\nevT\nJi\nsi\nha\nY\ner\n2\non\nm\nns\nh\nCT\nb\np \nof \nal\no\nw\nIR\nMI\n03\ndr\nou\nm\n) \nati\n[1\nt\nad\nhe\nil\nroj\nd\nus\nm\nex\nX \n0\nlo\nec\nze\nof \nva\nnt \nver\nl \nev\nrc\n(\nta\no\nTr\nmu\nviU\nin\nity\na\nY\nrs\n2@\nn, \nme\ns \nho\nT\nbe\no\nft\nl \nof\nw\nR \nIR\n3, \nre\nun\nmi\no\nio\n],\nth\nd \neld\nld\noje\nel\nsi\nmo\nxt\nan\n00\non\nctr\ns \nM\nal\nt\nr \np\nva\nch\n(T\nas\non\nac\nus\nidU\nn \ny \nl\nYu\nsi\n@i\nb\nem\nol\nTU\ne t\non\nth\n(S\nf \nwh\nr\nR \nw\new\nnd\nitt\nof\non\n, \nhe\ni\nd \ndin\nec\nls\nic\nu\nt G\nnd\n0,0\nn \nro\nt\nM\nlu\nte\n2\npa\nalu\nh \nTa\nsk\nn w\nck\nsi\ndeUN\nH\no\ne\nun\nity\nil\nba\nmb\no\nlis\nUR\ntr\nn \nhe\nSI\nI\nhic\nre\ne\nw\nw \nda\nte\nf \nn \nw\nse\nts\na\nng\nct\ns \n A\nun\nG\nd \n00\nF\non\nth\nMIR\nua\nst\n20\nart\nua\nin\nab\nks\nw\nki\nic\nnN\nH\nof\nee\nn \ny \nl\nas\nbe\non\nst\nR\nrac\nM\ne \nIG\nIS\nch\nse\nev\nwh\nW\nati\nee \nt\ns\nwh\ne \ns \nat\ng \nts \nto\nA\nnts\nen\nr\n00\nFo\nni\nhe\nR\nate\nt \n0 \ntic\nat\nnt\nbl\n (\nwh\nin\nc q\nnceNI\na\nf W\ne@\nH\no\nl\nse\ner\nn \ntic\nRE\nce\nM\nA\nG\nSM\nh \nea\nva\nhic\nW\nio\nth\nse\nhi\np\nfi\nt \no\nw\no \nn\ns \nne\nre\n0 \nou\nic\nes\nRE\ned\nc\nd\nci\nte\nte\ne \n(e\nhic\nng\nqu\ne IT\na L\nW\n@u\nH\nof\ni\nd\nrs\nf\nc \nE, \ned\nu\nAC\nGIR\nM\nc\nar\nal\nch\nW.\non\no\ne \ness\nic\npr\nir\nQ\non\nwe\nal\no\ner\nela\ni\nun\ns \ne \nEX\nd \no\ndif\nip\ned\nere\n3\ne.g\nch\ng)\nue\ntT\nL\nWa\nu\nHa\nf I\nin\nd o\ns o\nfu\nu\nA\nd \nusi\nC\nR\nMI\nca\nrc\nlu\nh \n. \nn (\nf \nsi\nch\nre\nrst\nQu\nn \ner\nu\nly\nof \nra\nat\nin\nnd\nT\na\nX.\n1\nll\nff\npa\nd i\nes\n3)\ng.\nh \n, \ner\nthTI\nLe\nas\nw\nao\nIl\nno\non\nof\nut\nuse\nAN\nb\nic\nCM\nR) \nIR\nal\nch\nua\nl\nM\n(N\nI\nU\nio\nh \nlu\nt \nue\ns\nre\nup\nys\nfM\nati\nte\nn \nda\nTe\nan\n. \n9\nle\nfer\nan\nin\nst\n), \n., \nm\na\nrie\nhaIE\nee\nsh\nw.\no \nlli\no\nn \nf \ntu\ner\nN\nba\nc \nM \nC\nR2\nlle\nh. \nati\ned\nMe\nN\nIS\nUn\non\np\nud\np\nee\nsu\ne \npli\nsis\nM\nio\nd\nf\nati\nec\nnd\n99\nct\nre\nnts\nn M\nts \nin\nA\nm\nan\nes\nat E\ne \nhi\ne\nin\ni\nth\nM\nure\nr \nND\nac\nI\nC\n20\ned\nA\nio\nd \nel\nS\nSM\nni\nn, \npr\nde\nle\nen\nuc\nfu\nift\ns \nMu\non\nd p\nfu\nio\nch\nd \n97\nti\nen\ns \nM\na\nnc\nAu\nan\nnd\ns \nMS\nin\ned\nno\ns\nh\nM\ne \nex\nD \nck\nn\nS\non\n00\nd \nA\non\nt\nllo\nF\nM\niv\nt\nro\nes\nen\nn \ncc\nun\nft \n(N\nus\nn \npr\nun\non\nhn\no\n7 i\no\nnt \np\nMI\nam\ncl\nud\nny\nd t\n(e\nMS \nng\ndu\noi\ns.\ne \nIR\nxp\nI\nk t\nfo\np\nn\n01\nf\nft\nn \nto\non\nF).\nMI\nve\nth\nv\ns, \nna\nM\nce\nnd\nM\nN\nsi\n(M\nro\nnd\nn, \nno\not\nin\nn\ntc\npe\nIR\nm\nlu\ndi\ny \nta\ne.\nMIRgt\nu\nis\n.e\nf\nR \nd\np\nIM\nto\nor\ne\nfe\n1 \nfo\nte\nw\no \nn \n.  \nR\nrs\nhe\nvid\nM\nar\nM\nes\nde\nM\nNE\nc \nM\noj\ndin\nU\nol\nth\nnd\nns,\nco\ner\nRE\nmo\nud\nio\nM\nas\n.g\nRto\n \ns \ned\nfe\nRc\ndir\ner\nM\no \nrm\nci\ner\np\nor\nerw\nwe\nf\nF\nR, \nsi\ne \nde\nM\nry\nMa\nssf\ned\nMI\nEM\nI\nMI\nec\nng\nUn\nlo\nher\ndiv\n, \nou\nr y\nEX\non\ndin\no \nM\nsk\ng.,\nREon\ndu\nee\nco\nre\nri\nMP\n1\nma\nia\nre\npa\nr \nw\ner\nfu\nFo\nt\nty\nA\ned\nMI\ny a\nary\nfu\nd \nIR\nM\nIn\nIR\nct\ng \nn\nog\nr \nv\nan\nun\nye\nX\nng\nng\nT\nMI\nks \n, \nEXn \nu\ndb\nom\nec\nien\nPA\n9\nat\nal \nen\nas\na\nwa\nre\nun\nou\nth\ny \nAu\nd \nR\nan\ny \nul\nto\nRE\nMA\nnf\nRE\nts\nf\niv\ngy\ni\nid\nn\nnt\nea\nX \ng \ng \nTa\nR\ni\nQ\nX \ndb\nm\ncti\nn\nAC\n9\nti\nI\nnc\nss\na \nard\n h\nnd\nun\nhe\nP\nud\nv\nRE\nnd\nC\nl \no \nEX\nA)\nfo\nEX\n h\nfr\nve\ny I\nim\ndu\nd \ntri\nar\nre\nM\nc\nag\nR \nin\nQu\n hac\nmm\nio\nce\nC\n9 o\nn\nIn\ne\nse\nf\nds\nh\ndin\nnd\ne \nPo\ndi\nva\nEX\nd \nC\nru\ne\nX\n), \norm\nX\nha\nro\ner\nIn\nmp\nua\nh\nie\nr \nep\nM\ncl\ng C\nsy\nnv\nue\nhack\nmu\non\ne \nCT\nw\nn \nnt\n. \ned\nfo\ns,\nel\nng\nda\nM\nom\nio\nal\nX\np\no\nun\nex\nX: \nS\nm\nX: \nav\nom\nrsi\nns\npo\nal \nha\nes\n(T\npr\nMIR\na\nC\nys\nvo\ner\nask \nun\nns\ne\nT \nw\nte\nT\nd \norm\n t\nld\ng \nat\nM\nm\no \nu\nX o\npo\nll\nns\nxp\nN\nSt\nma\nN\nve\nm \nit\nsti\nor\nM\nas\ns w\nT\nre\nR\nss\nCla\nst\nolv\nry\ns a\nni\ns \nev\nh\nR\nre\nTw\nt\nm\ntw\nd \nf\ntio\nMu\nmp\nD\nuab\nof\nos\nle\ns \nplo\nN\ntru\nati\nNG\ne \nt\nty\nitu\nrta\nM\ns \nw\nab\nes\nR \nsi\nas\nte\nvi\ny-b\nsan\nity\no\nva\nhen\nRe\nes\nwo\nth\nma\nwo\nin\nfo\non\nusi\npe\nDe\nbl\nff\nste\ng\no\nor\nNe\nuc\nio\nG\nre\nth\n o\nut\nan\nMI\nin\nwit\nbl\nen\nre\nc\nss\nm\nin\nby\nsignd\ny,\nof\nal-\nn \ne-\nst \no \nhe \nal \no \nn \nor \nn \nic\neu\ne-\nle\nfi-\ner\ne,\nof\nre\net-\nc-\non\nG)\ne-\nhe\nof\nte\nnt\nR\nn-\nth\nle\nnt\ne-\nal\nsi-\nms\nng\ny-\ng-d \n, \nf \n-\nc \nu \n-\ne \n-\nr \n, \nf \ne \n-\n-\nn \n. \n-\ne \nf \ne \nt \nR \n-\nh \ne \nt \n-\nl \n-\ns \ng \n-\n- \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n657  \n \nnificantly grown over the years is clear. The addition and \nretirement of the tasks reflect the shift in interests in the \nfield.  \n1999 Music retrieval workshop at SIGIR proposed a range of \nevaluation scenarios \n2000 First ISMIR held at Plymouth with participants holding \nbrainstorming sessions \n2001 ISMIR at Indiana University; “Bloomington Manifes-\nto” on evaluation published \n2002 Planning grant from the Andrew W. Mellon Foundation \nawarded \n2002 ISMIR at Paris hosted special evaluation workshop \n2003 SIGIR at Toronto held Workshop on the Evaluation of \nMusic Information Retrieval Systems \n2003 Andrew W. Mellon Foundation and NSF funding \nawarded \n2004 Audio Description Contest  run at ISMIR Barcelona \n2005 First MIREX plenary session held at ISMIR London \n2008 NEMA project funded by the Andrew W. Mellon  \nFoundation \n2009 SALAMI funded by the NSF, SSHRC and JISC \n2012 MIREX:NG project funded by the Andrew W. Mellon \nFoundation \nTable 1.  Important Events in MIREX History \n Datasets Individuals Countries Runs \n2005 10 82 19 86 \n2006 13 50 14 92 \n2007 12 73 15 122 \n2008 18 84 19 169 \n2009 26 138 15 289 \n2010 31 152 21 331 \n2011 32 156 16 296 \n2012 35 109 20 302 \n2013 37 116 29 310 \nTable 2.  Descriptive Statistics for MIREX 2005-2013 \nDue to restrictive intellectual property issues surround-\ning music materials, the test data used in MIREX cannot \nbe distributed to participants. This distinguishes the struc-ture of MIREX from those of other major evaluation \nframeworks such as TREC. MIREX has been operated \nunder an “algorithm-to-data” or “non-consumptive com-putation” model: researchers submit their MIR algorithms to IMIRSEL which are then evaluated by IMIRSEL per-sonnel and volunteers against the ground truth data host-ed in IMIRSEL.  \nBeyond the technical infrastructure, the communica-\ntions infrastructure is also critical for MIREX as it is a community-driven endeavor. The MIREX wikis were set up for the community to collaboratively define the evalu-ation tasks, metrics, and general rules in every spring, and \nto publish and archive results data for each task and asso-\nciated algorithms in every autumn. Besides being used by participants for preparing their mandatory presentations \nin the annual MIREX poster session in ISMIR, the \nMIREX results data also provide unique and valuable materials for publications in the field. In addition, the MIREX “EvalFest” mailing list is used for discussions \nabout evaluation issues. To date, 531 people have sub-\nscribed to EvalFest. IMIRSEL also creates task-specific\n mailing  lists  where  researchers  can  have  detailed discus-\nsions about metrics, collections, and input/output formats.  \nFrom its inception, MIREX has had a clear (and grow-\ning) impact on MIR research. Updating an earlier analysis of MIREX-related publications in [3], as of April 2014, 314 MIREX extended abstracts and 1,070 publications based on MIREX trials and results can be found through Google Scholar (Table 4). These publications have re-ceived a total of 18,239 citations (Table 5). We limited \nthe analysis period to the seven years ending in 2011, as \nthere is a considerable lag between the publication of a document and its appearance in Google Scholar (and then a similar lag before the paper can be cited). The growing number of Master’s and PhD dissertations building on MIREX results—and in many cases, participating in \nMIREX trials—is particularly significant; MIREX has \nclearly become a fundamental aspect of MIR research in-frastructure. In addition to this impact on academic re-search, 13 patents have explicitly referenced MIREX ex-\ntended abstracts [4]. \n ‘05 ‘06 ‘07 ‘08 ‘09 ‘10 ‘11 \nTech. report 0 4 4 3 10 5 11 \nBook chapter 0 2 1 2 8 9 20 \nDissertation 1 17 13 25 22 35 48 \nConference  12 46 68 88 127 144 137 \nJournal article 1 15 27 21 29 50 65 \nTotal 14 84 113 139 196 243 281 \nTable 4.  Publication Types for MIREX-derived Papers \nTo elicit further, less easily measured contributions of \nMIREX to the research community, interviews of 18 in-\nfluential MIR researchers were conducted in the MIREX Next Generation project [10]. From these, four key con-tributions were identified: 1) Benchmarking and evalua-tion:  MIREX was born from the recognition that the field \ncould not progress unless MIR researchers could bench-mark their work against each other’s; 2) Training and \ninduction into MIR: Emerging researchers and graduate students gain hands-on experience with MIR research and development, and build a reputation with potential em-ployers within both the music industry and academia; 3) \nDissemination of new research : The annual MIREX tri-\nals and subsequent MIREX session at ISMIR provide a \nnatural focus for the research community, and allow re-searchers to showcase their work to the MIR community at large; 4) Dissemination of data : MIREX has been an \nimportant venue for the community to access previous \nhigh-quality evaluation datasets created by MIREX team \nor donated by researchers.  \n MIREX  \nextended abstracts MIREX-derived \n publications \nYear No. citations mean med. No. citations mean med. \n2005 55 418 7.60 5 14 879 62.79 17.5 \n2006 35 217 6.20 2 51 2656 31.62 13 \n2007 32 403 12.60 4 113 1449 21.27 8 \n2008 39 136 2.61 3 139 3560 26.61 8 \n2009 48 144 3.00 0 196 2790 14.23 5 \n2010 61 135 2.21 0 243 3093 12.73 6 \n2011 44 63 1.43 1 281 2296 8.17 2 \nTable 5.  Overview of MIREX Citation Data, 2005-2011 \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n658  \n \nTASK NAME 2005  2006  2007 2008 2009 2010 2011  2012  2013 \nAudio Artist Identification 7  7 11      \nAudio Beat Tracking  5  15(2) 22(2) 26(2) 24(2) 60(3) 54(3) \nAudio Chord Detection    11 18(2) 15 18 22(2) 36(3) \nAudio Classical Composer ID   7 8 30 27 16 15 14 \nAudio Cover Song Identification  8 8  6(2) 6(2) 4(2)  2 \nAudio Drum Detection 8         \nAudio Genre Classification 15  7 26(2) 65(2) 48(2) 31(2) 31(2) 26(2) \nAudio Key Detection 7     5 8 6 3 \nAudio Melody Extraction 10 10(2)  2 1(3) 12(6) 30(6) 60(6) 24(6) 24(6) \nAudio Mood Classification   9 13 33 36 17 20 23 \nAudio Music Similarity  6 12  15 8 18 10 8 \nAudio Onset Detection 9 13 17  12 18 8 10 11 \nAudio Tag Classification    11 24(3) 26(2) 30(2) 18(2) 8(2) \nAudio Tempo Extraction 13 7    7 6 4 11 \nDiscovery of Repeated Themes & Sections         16 \nMultiple Fundamental Frequency Estimation & Tracking   27(2) 28(2) 26(3) 23(3) 16(2) 16(2) 6(2) \nQuery-by-Singing/Humming  23(2) 20(2) 16(2) 12(4) 20(4) 12(4) 24(4) 28(5) \nQuery-by-Tapping    5 9(3) 6(3) 3(3) 6(3) 6(3) \nReal-time Audio to Score Ali gnment (a.k.a Score Followin g)   2  4  5 2 3 2 \nStructural Segmentation     5 12(2) 12(2) 27(3) 26(3) \nSymbolic Genre Classification 5         \nSymbolic Key Finding 5         \nSymbolic Melodic Similarity 7 18(3) 8   13 11 6 6 \nTotal Number of Runs per Year 86 92 122 169 289 331 296 302 310 \nTotal Number of Runs (2005-2013) 1997 \nNotes: 1) Superscript numbers represent the number of subtasks included. 2) Since 2009, the Audio Classical Composer ID task, A udio \nGenre Classification task, and Audio Mood Classification task have become subtasks of Train-Test Task.  \nTable 3.  MIREX Tasks and the Number of Runs \n3. CHALLENGES \n3.1 Sustainability of Current Administration Model \nThe current model for administrating the evaluations is \ncostly and unsustainable. Since its inception, all MIREX \ntasks have required manual execution of submitted algo-rithms. As algorithms are written in different languages and require a range of executing environments, running \none algorithm takes about 5 hours of focused attention on \naverage, including but not limited to the time spent on communicating with participants, debugging algorithms, reconfiguring input/output interfaces and execution envi-ronment, etc. More often than not, algorithms may have to be updated by participants and tested by IMIRSEL for \nmultiple rounds before they can be executed correctly. \nBesides the algorithms, some tasks require ground truth data in every iteration of MIREX (e.g., similarity tasks, further discussed in Section 3.4), which takes a signifi-cant amount of time to build. To meet all these demands, IMIRSEL has been relying on a small number of graduate \nstudents fully devoted to running MIREX in each fall. \nNonetheless, participants sometimes still have to wait for a long time to receive evaluation results.   \nTo mitigate the problem, the Networked Environment \nfor Music Analysis (NEMA) project was established to “construct a web-service framework that would make \nMIREX evaluation tasks, test collections, and automated \nevaluation scripts available to the community on a yearly basis” (p.113, the so-called “Do-It-Yourself” model) [6]. However, due to the large variety of execution environ-ments of algorithms, the built framework has not been widely adopted in the MIR community, except for the automated evaluation package in the NEMA framework which has been used in recent iterations of MIREX to au-tomate the evaluation of tasks such as Train-test and Au-dio Tag Classification. This has greatly improved the ef-ficiency of MIREX and productivity of IMIRSEL per-\nsonnel, but such procedures still require manual input of \nraw results produced by the algorithms. The sustainability of MIREX calls for new technology and structures that can streamline the entire process of data/algorithm ingest, evaluation code generation/modification, and results post-ing, so that the evaluations can not only be effective, but \nalso efficient, robust, and scalable. \n3.2 Financial Sustainability Challenges \nThe fact that MIREX has been providing significant val-\nue to the MIR community is clearly evident. However, IMIRSEL has effectively offered MIREX as a free ser-vice to the community. This model is unsustainable; in \nJanuary 2015, the current Mellon funding concludes, \nleaving MIREX with no financial support for the first time in its history. A back-of-the envelope calculation using the amount of grant funding ($3,100,000) divided by number of runs (1997) gives an estimate of the cost per run of $1,552. Cost estimates per participant (960 to-\ntal) come in at $3,229. These rough numbers illustrate the \ngeneral magnitude of the funding challenge MIREX is facing.   \n3.3 Knowledge Management and Transfer \nOver the past decade, the leading task organizers of \nMIREX have left IMIRSEL, including Dr. Andreas Eh-\nmann (now at Pandora.com) and Dr. Mert Bay—both in-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n659  \n \nstrumental in creating MIREX processes and techniques. \nConsiderable time and energy are being expended in re-constructing past practices to help new IMIRSEL mem-\nbers and new task organizers complete their assigned du-\nties. MIREX needs more effective mechanisms to man-age corporate memory so as to successfully transfer knowledge to new lab members and external volunteers. Notwithstanding recent efforts to more thoroughly docu-ment MIREX technologies and procedures, more work \nneeds to be done to support hands-on training sessions for \nall who manage and run MIREX tasks.  \n3.4 Ground Truth Data Shortage \nThe lack of ground truth data is one of the primary obsta-\ncles facing the field of MIR. There is a strong demand for large, high-quality ground truth datasets for various eval-\nuation tasks. However, generating any kind of user data is \nexpensive. Crowdsourcing has been suggested as a possi-ble solution by a number of MIR researchers (e.g., [12][16]). Although previous studies have shown that the user evaluation results collected by crowdsourcing and from music experts in the conventional MIREX frame-\nwork are comparable, the issues of representativeness and \nnoise in data still exist.  \nIn order to generate the ground truth data, human eval-\nuators must listen to sample music pieces and manually input their responses. The task must be carried out by in-dividuals who have had a baseline level of training, mak-\ning the data even more expensive to collect. Currently, \nmost ground truth data is generated within academic in-stitutions through the use of graduate and undergraduate student labor. Funding opportunities for generating ground truth data are limited, and the fact that audio data is often not transferrable between multiple researchers or \nlabs due to copyright restrictions further complicates da-\ntaset creation. \nThere are a variety of sources for ground truth data, \nsome released by MIREX, and also by other researchers in an ad hoc fashion. However, academic scholars as well as researchers in industry have difficulty identifying and \nobtaining relevant datasets. Currently, there is no organi-\nzation or lab that is taking the role of creating, maintain-ing, and sharing ground truth data. In other IR domains, there are central organizations that fulfill at least part of this responsibility to support evaluations [10]. For exam-ple, ground truth data in TREC is created and/or managed \nby National Institute of Standards and Technology \n(NIST) and is released after each evaluation [7]. In the field of speech recognition, the Linguistic Data Consorti-um (LDC) creates ground truth datasets that can be pur-chased for use by individual labs [10]. This cycle of re-freshed data allows the research community to conduct \nhigh-quality evaluation. As this has not been the case for \nMIREX, the same ground truth data must sometimes be used for multiple years. \n3.5 Intellectual Property Issues \nAnother major problem facing the MIREX community is \nthe lack of usable music data upon which to build realis-\ntic test collections, due to intellectual property issues sur-\nrounding music materials. The datasets used in MIREX are very limited in terms of size, variety, recency, and novelty. Moreover, the fact that datasets cannot be dis-tributed after being used in MIREX effectively prevents \nresearchers from replicating the evaluation and bench-\nmarking their newly developed algorithms on their own. To tackle this issue that has plagued MIR research since day one, the MIR community needs to work together to explore possible solutions such as negotiating with copy-right holders collectively, using creative audio and/or \nmusic in the public domain, and running algorithms \nagainst multiple datasets hosted in different labs. The lat-ter approach has been attempted by projects such as NEMA. However, none of the possibilities is straightfor-ward and this battle is likely to exist for many years to come. \n3.6 System vs. User-centered Evaluations \nMIREX has followed the conventional, Cranfield IR sys-\ntem-centered evaluation paradigm [2]. Recently, this \nevaluation approach has been criticized by multiple re-searchers for excluding users from the evaluation process. To name a few, Hu and Liu [9], Hu and Kando [8], Lee \n[11], Schedl and Flexer [15], and Lee and Cunningham \n[13] all argued that the goal of MIR systems is to help users meet their music information needs, and thus MIR evaluation must take users into account. For instance, a number of MIR researchers have questioned the validity of system-centered evaluation on tasks that involve hu-\nman judgments such as the similarity tasks [12], [15], \n[16]. Music similarity may be interpreted differently for different people, yet the variance across users is simply ignored in the current evaluation protocol. As noted by Lee and Cunningham [13], a result of system-centered evaluation “may not be effectively translated to some-\nthing meaningful or practical for real users (p. 517).” \nThey suggested introducing tasks that “seems closer to what would be useful for real users” such as playlist gen-eration, known-item search, or personal music collection management.  \nNotwithstanding the importance of traditional system-\ncentered tasks, some suggestions have been made to MIREX to bridge the gap between system-centered and user-centered evaluation (e.g., incorporating user context in test queries, use terms familiar to users, combine mul-tiple tasks in [11][9]), although they are yet to be reflect-ed in the MIREX tasks. As the field matures, in order to \nmove forward, it is vital to explore user-centered and re-\nalistic evaluation tasks.  \n4. FUTURE DIRECTIONS \n4.1 Developing a User Experience Task \nIn keeping with our desire to expand MIREX beyond its current system-centered paradigm, we are conducting the first user-centered grand challenge evaluation task. The “Grand Challenge ‘14 User Experience” (GC14UX)\n1 task \nis unlike any previous MIREX task. The GC14UX is di-rectly inspired by the grand challenge idea proposed in Downie, Crawford and Byrd [5], which noted the persis-\n                                                           \n1 http://www.music-ir.org/mirex/wiki/2014:GC14UX  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n660  \n \ntent absence of complete MIR systems presented at \nISMIR that could be released to the public for music searching and discovery. Thus, the GC14UX has two un-\nderpinning goals: 1) to inspire the development of com-\nplete MIR systems to be shared at ISMIR; and 2) to pro-mote the notion of user experience as a first-class re-search objective in the MIR community.  \nThe choice of “Grand Challenge” to describe our first \nUX task was made, in part, to signify that MIREX will be \nentering into uncharted evaluation territory. By finally \nundertaking a user-centered evaluation task, the GC14UX will require the MIREX team (and the MIR community) to come up with new evaluation methods and criteria that will be made manifest in ways significantly different from our now standard MIREX operation procedures. We \nargue that the current state of the art in conventional \nMIREX tasks is sufficient to support an acceptable de-gree of efficiency and effectiveness for most of the now classic MIREX system-centered tasks. It is now time to look towards the more holistic user experience: subjective explorations of hedonic aspects of use such as satisfac-\ntion, enjoyment, and stimulation. To that end, the MIREX \nteam is proposing several radical departures from MIREX tradition that promise to better support the focus on the user experience. The most radical changes include: 1) no submission of algorithms to IMIRSEL; and 2) distribu-tion of audio data to participants.  \nTo ensure that the GC14UX does not become a sys-\ntem-centered evaluation in disguise, the process is de-signed to remain as agnostic as possible concerning the technological means by which participating systems cre-ate and deliver their experiences to the users. This delib-erate indifference suggests that the GC14UX has no need \nto run or evaluate the underlying system code that deliv-\ners the content to the users. Since the GC14UX will not be evaluating the system-code per se, it makes sense that the GC14UX does not follow MIREX’s usual practice of \nrunning code on behalf of the submitters. There are obvi-\nous benefits to this non-submission approach, including greatly reduced system requirements and significantly \nreduced MIREX staff time requirements for debugging \nand administration.  \nDropping the usual algorithm-to-data procedures does, \nobviously, beg the question about data sources for the systems to use. All the usual copyright reasons why mu-\nsic distribution is problematic for MIREX still apply and \ntherefore we need data sources that are amenable to dis-tribution. For the first running of GC14UX, the test col-lection will be drawn from Creative Commons music. We believe that a set in the magnitude range of 10,000 songs would strike a nice balance between being non-trivial in \nsize and breadth while not posing too great of a data \nmanagement burden for participants. A common dataset helps mitigate against the possible user experience bias induced by the differential presence (or absence) of popu-lar or known music within the participating systems.  \nThe GC14UX task is all about how users perceive \ntheir experiences with the systems. We intend to capture the user perceptions in a minimally intrusive manner un-der as-realistic-as-possible use scenarios. To this end, all participating systems are required to be constructed as websites accessible to users through normal web brows-ers. For user evaluation, we also do not want to burden \nthe users/evaluators with too many questions or required \ndata inputs. Our main goal is to determine whether each system was able to provide a satisfying user experience ([14], [17]). Thus, a question asking about the level of overall satisfaction is posed to each user for each system. An option for open-ended responses is provided so as to \ncapture the expressions of the users in their own words.  \nThere are many potential challenges that could prevent \nGC14UX from being the progenitor of future MIREX \nUX evaluations. For example, the utility and possible side-effects of using Creative Commons music as the common dataset have yet to be ascertained. Also, the ef-\nfectiveness of the current GC14UX user inputs will most \nlikely spark lively debate among MIR researchers after our first round of data is collected. Notwithstanding these known problems, as well as the challenges currently un-known, we are eager to see GC14UX proceed and inspire new evaluations. It is well past time that MIREX act to \ncreate a real user-centered evaluation stream. If we allow \nperfection to be the enemy of the good, MIREX might \nnever be able to launch a vibrant UX evaluation thread.  \n4.2 Funding Models \nIn order to continue providing benefits to the MIR com-\nmunity, MIREX must explore a range of funding options. \nIn order to reduce the dependencies and burdens placed \nupon any one funding source, it is necessary to seek mul-tiple sources of income. Some of the current possibilities include: \n/g120 Lab Memberships:  MIREX is exploring the possibil-\nity of setting up a lab membership system for labs that are active in MIR. Member labs would be represented \non MIREX’s governing committee, and would have \naccess to the new datasets that MIREX creates. \n/g120 Sponsorship:  MIREX would also like set up a spon-\nsorship program for leaders in industry. A sponsor-ship program would give companies a chance to sup-port and/or discover interesting new MIR work by \nemerging researchers. Identification of recruiting op-\nportunities is a valuable benefit that industry currently \nderives from MIREX (Section 2). \n/g120 Institutional Support:  The University of Illinois has \nprovided significant in-kind support for MIREX in the \npast. MIREX seeks to extend this partnership into the future. However, budget shortfalls at the State level are diminishing the prospects of ongoing University support. \n/g120 Data Creation and Curation:  The MIREX team \ncompleted a collaborative project developing ground truth genre and mood data for, and funded by, Korea \nElectronics Technology Institute (KETI) in 2013. The \ndata created is being folded into the MIREX task pool. The success of the KETI project, combined with the precedent set by the LDC (Section 3.4), inspires future data creation actions. In a similar line, we are exploring the possibility of providing fee-based data \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n661  \n \ncuration and management services to those who have \ndata sets that require long-term preservation.  \nWhile it will need to seek more support from its partic-\nipants, MIREX recognizes the need to balance this with \nopenness and accessibility. MIREX aims to remain open \nto any researcher who wants to participate, with a healthy \nfunding mix making this goal more likely to be achieved.  \n4.3 Distributed Management Model: Task Captains \nMIREX is pursuing a more decentralized model in order to reduce the strain on IMIRSEL and to more actively involve the entire MIR community in task creation, or-\nganization and delivery. Under this model, multiple labs \ncan run particular tasks while IMIRSEL functions as a central organizer and algorithm submission point. This model was piloted in 2012 with Query-by-Singing/Humming (QBSH) and Audio Melody Extrac-tion (AME) run by KETI. In MIREX 2013, Audio Beat \nTracking (ABT), Audio Chord Estimation (ACE), Audio \nKey Detection (AKD), Audio Onset Detection (AOD), Audio Tempo Estimation (ATE), and Discovery of Re-peated Themes & Sections (DRTS) were led by non-IMIRSEL volunteer “Task Captains” who managed the tasks from start to finish. While shortcomings in MIREX \ndocumentation were evident, the Task Captain initiative \nwas successful and will be developed further.  \n5. CONCLUSIONS \nIn this paper, we reflect on ten years of experience of MIREX. As the major community-based evaluation \nframework, MIREX has made unprecedented contribu-\ntions to the MIR research field. However, MIREX also faces a number of significant challenges including finan-cial sustainability, restrictions on data and intellectual property, and governance. Future directions of MIREX are proposed to meet these challenges. By moving to-\nwards the evaluation of entire systems and emphasizing \nholistic user experience, MIREX will allow us to com-pare and evaluate startups and experimental systems, as well as commercial MIR systems. We hope this paper will serve as a catalyst for the community to come to-gether and seek answers to the question: what is the fu-\nture of MIREX? More importantly, we hope this paper \nwill inspire MIR community members to actively engage in and contribute to the continuation of MIREX. MIREX has always been a community-driven endeavor; without the active leadership and involvement of MIR research-ers, MIREX simply cannot exist. \n \n6. REFERENCES \n[1] P. Cano, E. Gomez, F. Gouyon, P. Herrera, M. Koppenberger, B. Ong, X. Serra, S. Streich, N. Wack: “ISMIR 2004 audio description contest,” MTG Technical Report, MTG-TR-2006-02 (Music \nTechnology Group, Barcelona, Spain), 2004. \n[2] C. W. Cleverdon and E. M. Keen: “Factors \ndetermining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results,” Cranfield, UK: \nAslib Cranfield Research Project, 1966. [3] S. J. Cunningham, D. Bainbridge, and J. S. Downie: \n“The impact of MIREX on scholarly research,” Proceedings of the ISMIR , pp. 259-264, 2012. \n[4] S. J. Cunningham and J. H. Lee: “Influences of ISMIR and MIREX Research on Technology Patents,” Proceedings of the ISMIR , pp.137-142, \n2013.  \n[5] J. S. Downie, D. Byrd, and T. Crawford: “Ten Years of ISMIR: Reflections on Challenges and Opportunities.” Proceedings of the ISMIR , pp. 13-\n18. 2009. \n[6] J. S. Downie, A. F. Ehmann, M. Bay, and M. C. Jones:  “The music information retrieval evaluation exchange: Some observations and insights.” In Advances in music information retrieval , pp. 93-115, \nSpringer Berlin Heidelberg, 2010. \n[7] D. Harman: “Overview of the Second Text Retrieval Conference (TREC-2).” Information Processing & \nManagement, 31(3), 271–289, 1995. \n[8] X. Hu and N. Kando: “User-centered Measures vs. System Effectiveness in Finding Similar Songs,” Proceedings of the ISMIR,  pp.331-336, 2012. \n[9] X. Hu and J.  Liu: “Evaluation of Music Information Retrieval: Towards a User-Centered Approach”. Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval, 2010.  \n[10] Ithaka S+R: “MIREX Consulting Report and Proposed Business Plan,” 2013. \n[11] J. H. Lee: “Analysis of user needs and information \nfeatures in natural language queries for music \ninformation retrieval,” Journal of the American \nSociety for Information Science & Technology,  \n61(5), pp. 1025-1045, 2010. \n[12] J. H. Lee: “Crowdsourcing Music Similarity \nJudgments using Mechanical Turk,” Proceedings of \nthe ISMIR, pp. 183 - 188, 2010. \n[13] J. H. Lee and S. J. Cunningham: “Toward an \nunderstanding of the history and impact of user \nstudies in music information retrieval”, Journal of \nIntelligent Information Systems, 41, pp. 499-521, \n2013. \n[14] H. Petrie and N. Bevan: “The evaluation of accessi-\nbility, usability and user experience,” The Universal \nAccess Handbook,  pp. 10-20, 2009. \n[15] M. Schedl and A. Flexer: “Putting the User in the Center of Music Information Retrieval.” \nProceedings of the ISMIR , pp. 385-390, 2012. \n[16] J. Urbano: “Information Retrieval Meta-Evaluation: \nChallenges and opportunities in the Music Domain,” \nProceedings of the ISMIR,  pp. 609 - 611, 2011. \n[17] A. Vermeeren, E. L-C Law, V. Roto, M. Obrist, J. \nHoonhout, and K. Väänänen-Vainio-Mattila: “User \nexperience evaluation methods: current state and development needs.” In Proceedings of the 6th \nNordic Conference on HCI , pp. 521-530, 2010. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n662"
    },
    {
        "title": "Extending Harmonic-Percussive Separation of Audio Signals.",
        "author": [
            "Jonathan Driedger",
            "Meinard Müller",
            "Sascha Disch"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415226",
        "url": "https://doi.org/10.5281/zenodo.1415226",
        "ee": "https://zenodo.org/records/1415226/files/DriedgerMD14.pdf",
        "abstract": "In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the com- puted components are often not of purely harmonic or per- cussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, de- pending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive compo- nent and vice versa. In this paper we present two exten- sions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition pro- cess that allows for tightening separation results and for enforcing the components to be clearly harmonic or per- cussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel con- cept is exploited to add a third residual component to the decomposition which captures the sounds that lie in be- tween the clearly harmonic and percussive sounds of the audio signal.",
        "zenodo_id": 1415226,
        "dblp_key": "conf/ismir/DriedgerMD14",
        "keywords": [
            "harmonic",
            "percussive",
            "noise-like",
            "leakage",
            "harmonic sounds",
            "percussive component",
            "state-of-the-art",
            "separation factor",
            "STN audio model",
            "residual component"
        ],
        "content": "EXTENDING HARMONIC-PERCUSSIVE SEPARATION OF AUDIO\nSIGNALS\nJonathan Driedger1, Meinard M ¨uller1, Sascha Disch2\n1International Audio Laboratories Erlangen\n2Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany\nfjonathan.driedger,meinard.muellerg@audiolabs-erlangen.de, sascha.disch@iis.fraunhofer.de\nABSTRACT\nIn recent years, methods to decompose an audio signal into\na harmonic and a percussive component have received a lot\nof interest and are frequently applied as a processing step\nin a variety of scenarios. One problem is that the com-\nputed components are often not of purely harmonic or per-\ncussive nature but also contain noise-like sounds that are\nneither clearly harmonic nor percussive. Furthermore, de-\npending on the parameter settings, one often can observe\na leakage of harmonic sounds into the percussive compo-\nnent and vice versa. In this paper we present two exten-\nsions to a state-of-the-art harmonic-percussive separation\nprocedure to target these problems. First, we introduce a\nseparation factor parameter into the decomposition pro-\ncess that allows for tightening separation results and for\nenforcing the components to be clearly harmonic or per-\ncussive. As second contribution, inspired by the classical\nsines+transients+noise (STN) audio model, this novel con-\ncept is exploited to add a third residual component to the\ndecomposition which captures the sounds that lie in be-\ntween the clearly harmonic and percussive sounds of the\naudio signal.\n1. INTRODUCTION\nThe task of decomposing an audio signal into its harmonic\nand its percussive component has received large interest in\nrecent years. This is mainly because for many applications\nit is useful to consider just the harmonic or the percussive\nportion of an input signal. Harmonic-percussive separa-\ntion has been applied, for example, for audio remixing [9],\nimproving the quality of chroma features [14], tempo es-\ntimation [6], or time-scale modiﬁcation [2, 4]. Several de-\ncomposition algorithms have been proposed. In [3], the\npercussive component is modeled by detecting portions in\nthe input signal which have a rather noisy phase behav-\nior. The harmonic component is then computed by the\ndifference of the original signal and the computed percus-\nsive component. In [10], the crucial observation is that\nc\rJonathan Driedger, Meinard M ¨uller, and Sascha Disch.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Jonathan Driedger, Meinard M ¨uller,\nand Sascha Disch. “Extending Harmonic-Percussive Separation of Au-\ndio Signals”, 15th International Society for Music Information Retrieval\nConference, 2014.\nFigure 1. (a):Input audio signal x.(b): Spectrogram X.\n(c):Spectrogram of the harmonic component Xh(left), the\nresidual component Xr(middle) and the percussive com-\nponentXp(right). (d): Waveforms of the harmonic com-\nponentxh(left), the residual component xr(middle) and\nthe percussive component xp(right).\nharmonic sounds have a horizontal structure in a spectro-\ngram representation of the input signal, while percussive\nsounds form vertical structures. By iteratively diffusing the\nspectrogram once in horizontal and once in vertical direc-\ntion, the harmonic and percussive elements are enhanced,\nrespectively. The two enhanced representations are then\ncompared, and entries in the original spectral representa-\ntion are assigned to either the harmonic or the percussive\ncomponent according to the dominating enhanced spectro-\ngram. Finally, the two components are transformed back to\nthe time-domain. Following the same idea, Fitzgerald [5]\nreplaces the diffusion step by a much simpler median ﬁlter-\ning strategy, which turns out to yield similar results while\nhaving a much lower computational complexity.\nA drawback of the aforementioned approaches is that\nthe computed decompositions are often not very tight in\nthe sense that the harmonic and percussive components\nmay still contain some non-harmonic and non-percussive\nresidues, respectively. This is mainly because of two rea-\nsons. First, sounds that are neither of clearly harmonic nor\nof clearly percussive nature such as applause, rain, or the\nsound of a heavily distorted guitar are often more or less\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n611randomly distributed among the two components. Second,\ndepending on the parameter setting, harmonic sounds of-\nten leak into the percussive component and the other way\naround. Finding suitable parameters which yield satisfac-\ntory results often involves a delicate trade-off between a\nleakage in one or the other direction.\nIn this paper, we propose two extensions to [5] that\nlead towards more ﬂexible and reﬁned decompositions.\nFirst, we introduce the concept of a separation factor (Sec-\ntion 2). This novel parameter allows for tightening decom-\nposition results by enforcing the harmonic and percussive\ncomponent to contain just the clearly harmonic and per-\ncussive sounds of the input signal, respectively, and there-\nfore to attenuate the aforementioned problems. Second,\nwe exploit this concept to add a third residual component\nthat captures all sounds in the input audio signal which\nare neither clearly harmonic nor percussive (see Figure 1).\nThis kind of decomposition is inspired by the classical\nsines+transients+noise (STN) audio model [8, 11] which\naims at resynthesizing a given audio signal in terms of\na parameterized set of sine waves, transient sounds, and\nshaped white noise. While a ﬁrst methodology to com-\npute such a decomposition follows rather straightforward\nfrom the concept of a separation factor, we also propose a\nmore involved iterative decomposition procedure. Build-\ning on concepts proposed in [13], this procedure allows\nfor a more reﬁned adjustment of the decomposition results\n(Section 3.3). Finally, we evaluate our proposed proce-\ndures based on objective evaluation measures as well as\nsubjective listening tests (Section 4). Note that this paper\nhas an accompanying website [1] where you can ﬁnd all\naudio examples discussed in this paper.\n2. TIGHTENED HARMONIC-PERCUSSIVE\nSEPARATION\nThe ﬁrst steps of our proposed decomposition procedure\nfor tightening the harmonic and the percussive component\nare the same as in [5], which we now summarize. Given\nan input audio signal x, our goal is to compute a harmonic\ncomponentxhand a percussive component xpsuch thatxh\nandxpcontain the clearly harmonic and percussive sounds\nofx, respectively. To achieve this goal, ﬁrst a spectrogram\nXof the signal xis computed by applying a short-time\nFourier transform (STFT)\nX(t;k) =N\u00001X\nn=0w(n)x(n+tH) exp(\u00002\u0019ikn=N )\nwitht2[0 :T\u00001]andk2[0 :K], whereTis the number\nof frames,K=N=2is the frequency index corresponding\nto the Nyquist frequency, Nis the frame size and length\nof the discrete Fourier transform, wis a sine-window func-\ntion andHis the hopsize (we usually set H=N=4). A\ncrucial observation is that looking at one frequency band in\nthe magnitude spectrogram Y=jXj(one row of Y), har-\nmonic components stay rather constant, while percussive\nstructures show up as peaks. Contrary, in one frame (one\ncolumn ofY), percussive components tend to be equallydistributed, while the harmonic components stand out. By\napplying a median ﬁlter to Yonce in horizontal and once\nin vertical direction, we get a harmonically enhanced mag-\nnitude spectrogram ~Yhand a magnitude spectrogram ~Yp\nwith enhanced percussive content\n~Yh(t;k) := median( Y(t\u0000`h;k);:::;Y (t+`h;k))\n~Yp(t;k) := median( Y(t;k\u0000`p);:::;Y (t;k+`p))\nfor`h;`p2Nwhere 2`h+ 1and2`p+ 1are the lengths\nof the median ﬁlters, respectively.\nNow, extending [5], we introduce an additional param-\neter\f2R,\f\u00151, called the separation factor. We as-\nsume an entry of the original spectrogram X(t;k)to be\npart of the clearly harmonic or percussive component if\n~Yh(t;k)=~Yp(t;k)> \f or~Yp(t;k)=~Yh(t;k)\u0015\f, respec-\ntively. Intuitively, for a sound to be included in the har-\nmonic component it is required to stand out from the per-\ncussive portion of the signal by at least a factor of \f, and\nvice versa for the percussive component. Using this prin-\nciple, we can deﬁne binary masks MhandMp\nMh(t;k) :=\u0010\n~Yh(t;k)=(~Yp(t;k) +\u000f)\u0011\n>\f\nMp(t;k) :=\u0010\n~Yp(t;k)=(~Yh(t;k) +\u000f)\u0011\n\u0015\f\nwhere\u000fis a small constant to avoid division by zero, and\nthe operators\u0015and>yield a binary result from f0;1g.\nApplying these masks to the original spectrogram Xyields\nthe spectrograms for the harmonic and the percussive com-\nponent\nXh(t;k) :=X(t;k)\u0001Mh(t;k)\nXp(t;k) :=X(t;k)\u0001Mp(t;k):\nThese spectrograms can then be brought back to the time-\ndomain by applying an “inverse” short-time Fourier trans-\nform, see [7]. This yields the desired signals xhandxp.\nChoosing a separation factor \f >1tightens the separation\nresult of the procedure by preventing sounds which are nei-\nther clearly harmonic nor percussive to be included in the\ncomponents. In Figure 2a, for example, you see the spec-\ntrogram of a sound mixture of a violin (clearly harmonic),\ncastanets (clearly percussive), and applause (noise-like,\nand neither harmonic nor percussive). The sound of the\nviolin manifests itself as clear horizontal structures, while\none clap of the castanets is visible as a clear vertical struc-\nture in the middle of the spectrogram. The sound of the\napplause however does not form any kind of directed struc-\nture and is spread all over the spectrum. When decompos-\ning this audio signal with a separation factor of \f=1, which\nbasically yields the procedure proposed in [5], the applause\nis more or less equally distributed among the harmonic\nand the percussive component, see Figure 2b. However,\nwhen choosing \f=3, only the clearly horizontal and ver-\ntical structures are preserved in XhandXp, respectively,\nand the applause is no longer contained in the two compo-\nnents, see Figure 2c.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n612Figure 2. (a):Original spectrogram X.(b):Spectrograms\nXh(left) andXp(right) for\f= 1. (c):Spectrograms Xh\n(left) andXp(right) for\f= 3.\n3. HARMONIC-PERCUSSIVE-RESIDUAL\nSEPARATION\nIn Section 3.1 we show how harmonic-percussive sepa-\nration can be extended with a third residual component.\nAfterwards, in Section 3.2, we show how the parameters\nof the proposed procedure inﬂuence the decomposition re-\nsults. Finally, in Section 3.3, we present an iterative de-\ncomposition procedure which allows for a more ﬂexible\nadjustment of the decomposition results.\n3.1 Basic Procedure and Related Work\nThe concept presented in Section 2 allows us to extend\nthe decomposition procedure with a third component xr,\ncalled the residual component. It contains the portion of\nthe input signal xthat is neither part of the harmonic com-\nponentxhnor the percussive components xp. To compute\nxr, we deﬁne the binary mask\nMr(t;k) := 1\u0000\u0000\nMh(t;k) +Mp(t;k)\u0001\n;\napply it toX, and transform the resulting spectrogram Xr\nback to the time-domain (note that the masks MhandMp\nare disjoint). This decomposition into three components\nis inspired by the STN audio model. Here, an audio sig-\nnal is analyzed to yield parameters for sinusoidal, tran-\nsient, and noise components which can then be used to ap-\nproximately resynthesize the original signal [8, 11]. While\nthe main application of the STN model lies in the ﬁeld of\nlow bitrate audio coding, the estimated parameters can also\nbe used to synthesize just the sinusoidal, the transient, or\nthe noise component of the approximated signal. The har-\nmonic, the percussive, and the residual component result-\ning from our proposed decomposition procedure are often\nperceptually similar to the STN components. However, our\nproposed procedure is conceptually different. STN mod-\neling aims for a parametrization of the given audio sig-\nnal. While the estimated parameters constitute a compact\napproximation of the input signal, this approximation and\nFigure 3. Energy distribution between the harmonic, resid-\nual, and percussive components for different frame sizes N\nand separation factors \f.(a):Harmonic components. (b):\nResidual components. (c):Percussive components.\nthe original signal are not necessarily equal. Our proposed\napproach yields a decomposition of the signal. The three\ncomponents always add up to the original signal again. The\nseparation factor \fhereby constitutes a ﬂexible handle to\nadjust the sound characteristics of the components.\n3.2 Inﬂuence of the Parameters\nThe main parameters of our decomposition procedure are\nthe length of the median ﬁlters, the frame size Nused\nto compute the STFT, and the separation factor \f. Intu-\nitively, the length of the ﬁlters specify the minimal sizes\nof horizontal and vertical structures which should be con-\nsidered as harmonic and percussive sounds in the STFT\nofx, respectively. Our experiments have shown that the\nﬁlter lengths actually do not inﬂuence the decomposition\ntoo much as long as no extreme values are chosen, see\nalso [1]. The frame size Non the other hand pushes the\noverall energy of the input signal towards one of the com-\nponents. For large frame sizes, the short percussive sounds\nlose inﬂuence in the spectral representation and more en-\nergy is assigned to the harmonic component. This results in\na leakage of some percussive sounds to the harmonic com-\nponent. Vice versa, for small frame sizes the low frequency\nresolution often leads to a blurring of horizontal structures,\nand harmonic sounds tend to leak into the percussive com-\nponent. The separation factor \fshows a different behavior\nto the previous parameters. The larger its value, the clearer\nbecomes the harmonic and percussive nature of the com-\nponentsxhandxp. Meanwhile, also the portion of the sig-\nnal that is assigned to the residual component xrincreases.\nTo illustrate this behavior, let us consider a ﬁrst synthetic\nexample where we apply our proposed procedure to the\nmixture of a violin (clearly harmonic), castanets (clearly\npercussive), and applause (neither harmonic nor percus-\nsive), all sampled at 22050 Hertz and having the same en-\nergy. In Figure 3, we visualized the relative energy dis-\ntribution of the three components for varying frame sizes\nNand separation factors \f, while ﬁxing the length of the\nmedian ﬁlters to be always equivalent to 200 milliseconds\nin horizontal direction and 500 Hertz in vertical direction,\nsee also [1]. Since the energy of all three signals is nor-\nmalized, potential leakage between the components is in-\ndicated by components that have either more or less than a\nthird of the overall energy assigned. Considering Fitzger-\nald’s procedure [5] as a baseline (\f =1), we can investigate\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n613its behavior by looking at the ﬁrst columns of the matri-\nces in Figure 3. While the residual component has zero\nenergy in this setting, one can observe by listening that\nthe applause is more or less equally distributed between\nthe harmonic and the percussive component for medium\nframe sizes. This is also reﬂected in Figure 3a/c by the\nenergy being split up roughly into equal portions. For\nvery largeN, most of the signal’s energy moves towards\nthe harmonic component (value close to one in Figure 3a\nfor\f=1;N =4096), while for very small N, the energy is\nshifted towards the percussive component (value close to\none in Figure 3c for \f=1;N =128). With increasing \f,\none can observe how the energy gathered in the harmonic\nand the percussive component ﬂows towards the residual\ncomponent (decreasing values in Figure 3a/c and increas-\ning values in Figure 3b for increasing \f). Listening to\nthe decomposition results shows that the harmonic and the\npercussive component thereby become more and more ex-\ntreme in their respective characteristics. For medium frame\nsizes, this allows us to ﬁnd settings that lead to decompo-\nsitions in which the harmonic component contains the vi-\nolin, the percussive component contains the castanets, and\nthe residual contains the applause. This is reﬂected by Fig-\nure 3, where for N=1024 and\f=2the three sound compo-\nnents all hold roughly one third of the overall energy. For\nvery large or very small frame sizes it is not possible to\nget such a good decomposition. For example, considering\n\f=1andN=4096, we already observed that the harmonic\ncomponent holds most of the signal’s energy and also con-\ntains some of the percussive sounds. However, already for\nsmall\f > 1these percussive sounds are shifted towards\nthe residual component (see the large amount of energy as-\nsigned to the residual in Figure 3b for \f=1:5;N =4096).\nFurthermore, also the energy from the percussive compo-\nnent moves towards the residual. The large frame size\ntherefore results in a very clear harmonic component while\nthe residual holds both the percussive as well as all other\nnon-harmonic sounds, leaving the percussive component\nvirtually empty. For very small Nthe situation is exactly\nthe other way around. This observation can be exploited\nto deﬁne a reﬁned decomposition procedure which we dis-\ncuss in the next section.\n3.3 Iterative Procedure\nIn [13], Tachibana et al. described a method for the extrac-\ntion of human singing voice from music recordings. In this\nalgorithm, the singing voice is estimated by iteratively ap-\nplying the harmonic-percussive decomposition procedure\ndescribed in [9] ﬁrst to the input signal and afterwards\nagain to one of the resulting components. This yields a de-\ncomposition of the input signal into three components, one\nof which containing the estimate of the singing voice. The\ncore idea of this algorithm is to perform the two harmonic-\npercussive separations on spectrograms with two different\ntime-frequency resolutions. In particular, one of the spec-\ntrograms is based on a large frame size and the other on a\nsmall frame size. Using this idea, we now extend our pro-\nposed harmonic-percussive-residual separation procedure\nFigure 4. Overview of the reﬁned procedure. (a): Input\nsignalx.(b): First run of the decomposition procedure\nusing a large frame size Nhand a separation factor \fh.\n(c): Second run of the decomposition procedure using a\nsmall frame size Npand a separation factor \fp.\nFigure 5. Energy distribution between the harmonic, resid-\nual, and percussive components for different separation\nfactors\fhand\fp.(a):Harmonic components. (b):Resid-\nual components. (c):Percussive components.\npresented in Section 3.1. So far, although it is possible to\nﬁnd a good combination of Nand\fsuch that both the\nharmonic as well as the percussive component represent\nthe respective characteristics of the input signal well (see\nSection 3.2), the computation of the two components is\nstill coupled. It is therefore not clear how to adjust the\ncontent of the harmonic and the percussive component in-\ndependently. Having made the observation that large N\nlead to good harmonic but poor percussive/residual compo-\nnents for\f>1, while small Nlead to good percussive com-\nponents but poor harmonic/residual components for \f>1,\nwe build on the idea from Tachibana et al. [13] and com-\npute the decomposition in two iterations. Here, the goal is\nto decouple the computation of the harmonic component\nfrom the computation of the percussive component. First,\nthe harmonic component is extracted by applying our basic\nprocedure with a large frame size Nhand a separation fac-\ntor\fh>1, yielding x\frst\nh,x\frst\nrandx\frst\np. In a second run,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n614SDR SIR SARBL\nHP HP-I\nHPR\nHPR-I\nHPR-IO\nBL\nHP\nHP-I\nHPR HPR-I\nHPR-IO\nBL\nHP\nHP-I HPR\nHPR-I\nHPR-IOViolin -3.10 -5.85 0.08 8.23 7.65 8.85 -3.10 -5.09 1.08 17.69 14.58 21.65 274.25 8.33 9.44 8.82 8.78 9.11\nCastanets -2.93 3.58 2.86 8.29 9.14 9.28 -2.93 6.06 10.45 22.34 20.66 24.41 274.25 8.14 4.07 8.49 9.50 9.44\nApplause -3.04 \u0000 -7.03 4.25 4.93 5.00 -3.04 \u0000 14.69 8.41 12.80 9.04 274.25 \u0000 -6.85 6.95 5.93 7.69\nTable 1. Objective evaluation measures. All values are given in dB.\nthe procedure is applied again to the sum x\frst\nr+x\frst\np, this\ntime using a small frame size Npand a second separation\nfactor\fp>1. This yields the components xsecond\nh ,xsecond\nr\nandxsecond\np . Finally, we deﬁne the output components of\nthe procedure to be\nxh:=x\frst\nh; xr:=xsecond\nh +xsecond\nr; xp:=xsecond\np:\nFor an overview of the procedure see Figure 4. While ﬁx-\ning the values of NhandNpto a small and a large frame\nsize, respectively (in our experiments we chose Nh=4096\nandNp=256), the separation factors \fhand\fpyield han-\ndles that give simple and independent control over the har-\nmonic and percussive component. Figure 5, which is based\non the same audio example as Figure 3, shows the en-\nergy distribution among the three components for differ-\nent combinations of \fhand\fp, see also [1]. For the har-\nmonic components (Figure 5a) we see that the portion of\nthe signals energy contained in this component is indepen-\ndent of\fpand can be controlled purely by \fh. This is\na natural consequence from the fact that in our proposed\nprocedure the harmonic component is always computed di-\nrectly from the input signal xand\fpdoes not inﬂuence its\ncomputation at all. However, we can also observe that the\nenergy contained in the percussive component (Figure 5c)\nis fairly independent of \fhand can be controlled almost\nsolely by\fp. Listening to the decomposition results con-\nﬁrms these observations. Our proposed iterative procedure\ntherefore allows to adjust the harmonic and the percussive\ncomponent almost independently what signiﬁcantly sim-\npliﬁes the process of ﬁnding an appropriate parameter set-\nting for a given input signal. Note that in principle it would\nalso be possible to choose \fh=\fp=1, resulting in an iter-\native application of Fitzgerald’s method [5]. However, as\ndiscussed in Section 3.2, Fitzgerald’s method suffers from\ncomponent leakage when using very large or small frame\nsizes. Therefore, most of the input signal’s energy will be\nassigned to the harmonic component in the ﬁrst iteration\nof the algorithm, while most of the remaining portion of\nthe signal is assigned to the percussive component in the\nsecond iteration. This leads to a very weak, although not\nempty, residual component.\n4. EV ALUATION\nIn a ﬁrst experiment, we applied objective evaluation mea-\nsures to our running example. Assuming that the violin,the castanets, and the applause signal represent the charac-\nteristics that we would like to capture in the harmonic, the\npercussive, and the residual components, respectively, we\ntreated the decomposition task of this mixture as a source\nseparation problem. In an optimal decomposition the har-\nmonic component would contain the original violin sig-\nnal, the percussive component the castanets signal, and the\nresidual component the applause. To evaluate the decom-\nposition quality, we computed the source to distortion ra-\ntios(SDR), the source to interference ratios (SIR), and the\nsource to artifacts ratios (SAR) [15] for the decomposition\nresults of the following procedures.\nAs a baseline (BL), we simply considered the origi-\nnal mixture as an estimate for all three sources. Further-\nmore, we applied the standard harmonic-percussive sep-\naration procedure by Fitzgerald [5] (HP) with the frame\nsize set toN=1024, the HPmethod applied iteratively\n(HP-I) with Nh=4096 andNp=256, the proposed basic\nharmonic-percussive-residual separation procedure (HPR)\nas described in Section 3.1 with N=1024 and\f=2, and\nthe proposed iterative harmonic-percussive-residual sepa-\nration procedure (HPR-I) as described in Section 3.3 with\nNh=4096,Np=256, and\fh=\fp=2. As a ﬁnal method,\nwe also considered HPR-I with separation factor \fh=3\nand\fp=2:5, which were optimized manually for the task at\nhand (HPR-IO). The ﬁlter lengths in all procedures were\nalways ﬁxed to be equivalent to 200 milliseconds in time\ndirection and 500 Hertz in frequency direction. Decompo-\nsition results for all procedures can be found at [1].\nThe results are listed in Table 1. All values are given in\ndB and higher values indicate better results. As expected,\nBLyields rather low SDR and SIR values for all compo-\nnents, while the SAR values are excellent since there are\nno artifacts present in the original mixture. The method\nHPyields low evaluation measures as well. However,\nthese values are to be taken with care since HPdecom-\nposes the input mixture in just a harmonic and a percus-\nsive component. The applause is therefore not estimated\nexplicitly and, as also discussed in Section 2, randomly\ndistributed among the harmonic and percussive compo-\nnent. It is therefore clear that especially the SIR values\nare low in comparison to the other procedures since the\napplause heavily interferes with the remaining two sources\nin the computed components. When looking at HP-I, the\nbeneﬁt of having a third component becomes clear. Al-\nthough here the residual component does not capture the\napplause very well (SDR of \u00007:03 dB) this already suf-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n615Item name Description\nCastanetsViolinApplause Synthetic mixture of a violin, castanets and applause.\nHeavy Recording of heavily distorted guitars, a bass and\ndrums.\nStepdad Excerpt from My Leather, My Fur, My Nails by the\nband Stepdad.\nBongo Regular beat played on bongos.\nGlockenspiel Monophonic melody played on a glockenspiel.\nWinterreise Excerpt from “Gute Nacht” by Franz Schubert which is\npart of the Winterreise song cycle. It is a duet of a male\nsinger and piano.\nTable 2. List of audio excerpts.\nﬁces to yield SDR and SIR values clearly above the base-\nline for the estimates of the violin and the castanets. The\nseparation quality further improves when considering the\nresults of our proposed method HPR. Here the evaluation\nyields high values for all measures and components. The\nvery high SIR values are particularly noticeable since they\nindicate that the three sources are separated very clearly\nwith very little leakage between the components. This\nconﬁrms our claim that our proposed concept of a sepa-\nration factor allows for tightening decomposition results\nas described in Section 2. The results of HPR-I are very\nsimilar to the results for the basic procedure HPR. How-\never, listening to the decomposition reveals that the har-\nmonic and the percussive component still contain some\nslight residue sounds of the applause. Slightly increas-\ning the separation factors to \fh=3and\fp=2:5 (HPR-IO)\neliminates these residues and further increases the evalua-\ntion measures. This straight-forward adjustment is possi-\nble since the two separation factors \fhand\fpconstitute\nindependent handles to adjust the content of the harmonic\nand percussive component, what demonstrates the ﬂexibil-\nity of our proposed procedure.\nThe above described experiment constitutes a ﬁrst case\nstudy for the objective evaluation of our proposed decom-\nposition procedures, based on an artiﬁcially mixed exam-\nple. To also evaluate these procedures on real-world audio\ndata, we additionally performed an informal subjective lis-\ntening tests with several test participants. To this end, we\napplied our procedures to the set of audio excerpts listed\nin Table 2. Among the excerpts are complex sound mix-\ntures as well as purely percussive and harmonic signals,\nsee also [1]. Raising the question whether the computed\nharmonic and percussive components meet the expectation\nof representing the clearly harmonic or percussive portions\nof the audio excerpts, respectively, the performed listen-\ning test conﬁrmed our hypothesis. It furthermore turned\nout that\fh=\fp=2,Nh=4096 andNp=256 seems to be\na setting for our iterative procedure which robustly yields\ngood decomposition results, rather independent of the in-\nput signal. Regarding the residual component, it was often\ndescribed to sound like a sound texture by the test partic-\nipants, which is a very interesting observation. Although\nthere is no clear deﬁnition of what a sound texture exactly\nis, literature states “sound texture is like wallpaper: it can\nhave local structure and randomness, but the characteris-tics of the ﬁne structure must remain constant on the large\nscale” [12]. In our opinion this is not a bad description of\nwhat one can hear in residual components.\nAcknowledgments:\nThis work has been supported by the German Re-\nsearch Foundation (DFG MU 2686/6-1). The Interna-\ntional Audio Laboratories Erlangen are a joint institution\nof the Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg\n(FAU) and Fraunhofer IIS.\n5. REFERENCES\n[1] J. Driedger, M. M ¨uller, and S. Disch. Accompanying website:\nExtending harmonic-percussive separation of audio signals.\nhttp://www.audiolabs-erlangen.de/resources/\n2014-ISMIR-ExtHPSep/.\n[2] J. Driedger, M. M ¨uller, and S. Ewert. Improving time-scale modiﬁ-\ncation of music signals using harmonic-percussive separation. Signal\nProcessing Letters, IEEE, 21(1):105–109, 2014.\n[3] C. Duxbury, M. Davies, and M. Sandler. Separation of transient\ninformation in audio using multiresolution analysis techniques. In\nProceedings of the COST G-6 Conference on Digital Audio Effects\n(DAFX-01), Limerick, Ireland, 12 2001.\n[4] C. Duxbury, M. Davies, and M. Sandler. Improved time-scaling of\nmusical audio using phase locking at transients. In Audio Engineering\nSociety Convention 112, 4 2002.\n[5] D. Fitzgerald. Harmonic/percussive separation using medianﬁltering.\nInProceedings of the International Conference on Digital Audio Ef-\nfects (DAFx), pages 246–253, Graz, Austria, 2010.\n[6] A. Gkiokas, V . Katsouros, G. Carayannis, and T. Stafylakis. Music\ntempo estimation and beat tracking by applying source separation and\nmetrical relations. In ICASSP, pages 421–424, 2012.\n[7] D. W. Grifﬁn and J. S. Lim. Signal estimation from modiﬁed short-\ntime Fourier transform. IEEE Transactions on Acoustics, Speech and\nSignal Processing, 32(2):236–243, 1984.\n[8] S. N. Levine and J. O. Smith III. A sines+transients+noise audio rep-\nresentation for data compression and time/pitch scale modications.\nInProceedings of the 105th Audio Engineering Society Convention,\n1998.\n[9] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama. A real-time\nequalizer of harmonic and percussive components in music signals.\nInProceedings of the International Conference on Music Information\nRetrieval (ISMIR), pages 139–144, Philadelphia, Pennsylvania, USA,\n2008.\n[10] N. Ono, K. Miyamoto, J. LeRoux, H. Kameoka, and S. Sagayama.\nSeparation of a monaural audio signal into harmonic/percussive com-\nponents by complementary diffusion on spectrogram. In European\nSignal Processing Conference, pages 240–244, Lausanne, Switzer-\nland, 2008.\n[11] A. Petrovsky, E. Azarov, and A. Petrovsky. Hybrid signal decompo-\nsition based on instantaneous harmonic parameters and perceptually\nmotivated wavelet packets for scalable audio coding. Signal Process-\ning, 91(6):1489–1504, 2011.\n[12] N. Saint-Arnaud and K. Popat. Computational auditory scene analy-\nsis. chapter Analysis and synthesis of sound textures, pages 293–308.\nL. Erlbaum Associates Inc., Hillsdale, NJ, USA, 1998.\n[13] H. Tachibana, N. Ono, and S. Sagayama. Singing voice en-\nhancement in monaural music signals based on two-stage har-\nmonic/percussive sound separation on multiple resolution spectro-\ngrams. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 22(1):228–237, January 2013.\n[14] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono, and S. Sagayama.\nHMM-based approach for automatic chord detection using reﬁned\nacoustic features. In ICASSP, pages 5518–5521, 2010.\n[15] E. Vincent, R. Gribonval, and C. F ´evotte. Performance measure-\nment in blind audio source separation. IEEE Transactions on Audio,\nSpeech, and Language Processing, 14(4):1462–1469, 2006.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n616"
    },
    {
        "title": "Note-level Music Transcription by Maximum Likelihood Sampling.",
        "author": [
            "Zhiyao Duan",
            "David Temperley"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416534",
        "url": "https://doi.org/10.5281/zenodo.1416534",
        "ee": "https://zenodo.org/records/1416534/files/DuanT14.pdf",
        "abstract": "Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estima- tion (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note- level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experi- ments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions.",
        "zenodo_id": 1416534,
        "dblp_key": "conf/ismir/DuanT14",
        "keywords": [
            "note-level music transcription",
            "pitch estimation",
            "spurious notes",
            "random sampling",
            "MPE model",
            "likelihood evaluation",
            "transcription performance",
            "polyphony",
            "state-of-the-art systems",
            "J.S. Bach chorales"
        ],
        "content": "NOTE-LEVEL MUSIC TRANSCRIPTION BY MAXIMUM LIKELIHOOD\nSAMPLING\nZhiyao Duan\nUniversity of Rochester\nDept. Electrical and Computer Engineering\nzhiyao.duan@rochester.eduDavid Temperley\nUniversity of Rochester\nEastman School of Music\ndtemperley@esm.rochester.edu\nABSTRACT\nNote-level music transcription, which aims to transcribe\nnote events (often represented by pitch, onset and offset\ntimes) from music audio, is an important intermediate step\ntowards complete music transcription. In this paper, we\npresent a note-level music transcription system, which is\nbuilt on a state-of-the-art frame-level multi-pitch estima-\ntion (MPE) system. Preliminary note-level transcription\nachieved by connecting pitch estimates into notes often\nlead to many spurious notes due to MPE errors. In this\npaper, we propose to address this problem by randomly\nsampling notes in the preliminary note-level transcription.\nEach sample is a subset of all notes and is viewed as a note-\nlevel transcription candidate. We evaluate the likelihood\nof each candidate using the MPE model, and select the\none with the highest likelihood as the ﬁnal transcription.\nThe likelihood treats notes in a transcription as a whole\nand favors transcriptions with less spurious notes. Experi-\nments conducted on 110 pieces of J.S. Bach chorales with\npolyphony from 2 to 4 show that the proposed sampling\nscheme signiﬁcantly improves the transcription performance\nfrom the preliminary approach. The proposed system also\nsigniﬁcantly outperforms two other state-of-the-art systems\nin both frame-level and note-level transcriptions.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) is one of the fun-\ndamental problems in music information retrieval. Gen-\nerally speaking, AMT is the task of converting a piece of\nmusic audio into a musical score. A complete AMT sys-\ntem needs to transcribe both the pitch and rhythmic content\n[5]. On transcribing the pitch content, AMT can be per-\nformed at three levels from low to high: frame-level, note-\nlevel, and stream-level [7]. Frame-level transcription (also\ncalled multi-pitch estimation) aims to estimate concurrent\npitches and instantaneous polyphony in each time frame.\nNote-level transcription (also called note tracking) tran-\nscribes notes, which are characterized not only by pitch,\nc\rZhiyao Duan, David Temperley.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Zhiyao Duan, David Temperley.\n“Note-level Music Transcription by Maximum Likelihood Sampling”,\n15th International Society for Music Information Retrieval Conference,\n2014.but also by onset and offset. Stream-level transcription\n(also called multi-pitch streaming) organizes pitches (or\nnotes) into streams according to their instruments. From\nthe frame-level to the stream-level, more parameters and\nstructures need to be estimated, and the system is closer to\na complete transcription system.\nWhile there are many systems dealing with frame-level\nmusic transcription, only a few transcribe music at the note\nlevel [5]. Among these systems, most are built based on\nframe-level pitch estimates. The simplest way to convert\nframe-level pitch estimates to notes is to connect consecu-\ntive pitches into notes [4, 9, 15]. During this process, non-\nsigniﬁcant errors in frame-level pitch estimation can cause\nsigniﬁcant note tracking errors. False alarms in pitch es-\ntimates will cause many notes that are too short, while\nmisses can break a long note into multiple short ones. To\nalleviate these errors, researchers often ﬁll the small gaps\nto merge two consecutive notes with the same pitch [2, 7],\nand apply minimum length pruning to remove too-short\nnotes [4, 6, 7]. This idea has also been implemented with\nmore advanced techniques such as hidden Markov mod-\nels [12]. Besides the abovementioned methods that are en-\ntirely based on frame-level pitch estimates, some methods\nutilize other information in note tracking, such as onset in-\nformation [10, 14] and musicological information [13, 14].\nIn this paper, we propose a new note-level music tran-\nscription system. It is built based on an existing multi-\npitch estimation method [8]. In [8], a multi-pitch likeli-\nhood function was deﬁned and concurrent pitches were es-\ntimated in a maximum likelihood fashion. This likelihood\nfunction tells how well the set of pitches as a whole ﬁt\nto the audio frame. In this paper, we modify [8] to also\ndeﬁne a single-pitch likelihood function. It tells the likeli-\nhood (salience) that a pitch is present in the audio frame.\nThen preliminary note tracking is performed by connect-\ning consecutive pitches into notes and removing too-short\nnotes. The likelihood of each note is calculated as the prod-\nuct of the likelihood of all its pitches. The next step is\nthe key step in the proposed system. We randomly sample\nsubsets of notes according to their likelihood and lengths.\nEach subset is treated as a possible note-level transcrip-\ntion. The likelihood of such a transcription is then deﬁned\nas the product of its multi-pitch likelihood in each frame.\nFinally, the transcription with the maximum likelihood is\nreturned as the output of the system. We carried out exper-\niments on the Bach10 dataset [8] containing Bach chorales\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n181Figure 1. System overview of the proposed note-level\ntranscription system.\nwith different polyphony. Experiments show that the pro-\nposed system signiﬁcantly improves the transcription per-\nformance from the preliminary transcription, and signiﬁ-\ncantly outperforms two state-of-the-art systems at both the\nnote level and frame level on the dataset.\n2. PROPOSED SYSTEM\nFigure 1 illustrates the overview of the system. It consists\nof three main stages: multi-pitch estimation, preliminary\nnote tracking, and ﬁnal note tracking. The ﬁrst stage is\nbased on [8] with some modiﬁcations. The second stage\nadopts the common ﬁlling/prunning strategies used in the\nliterature to convert pitches into notes. The third stage is\nthe main contribution of the paper. Figure 2 shows tran-\nscription results obtained at different stages of the system\non a piece of J.S. Bach 4-part chorale.\n2.1 Multi-pitch Estimation\nIn [8], Duan et al. proposed a maximum likelihood method\nto estimate pitches from the power spectrum of each time\nframe. In the maximum likelihood formulation, pitches\n(and the polyphony) are the parameters to be estimated\nwhile the power spectrum is the observation. The like-\nlihood function Lmp(fp1;\u0001\u0001\u0001; pNg)describes how well\na set of Npitchesfp1;\u0001\u0001\u0001; pNgas a whole ﬁt with the\nobserved spectrum, and hence is called a multi-pitch like-\nlihood function. The power spectrum is represented as\npeaks and the non-peak region, and the likelihood func-\ntion is deﬁned for both parts. The peak likelihood favors\npitch sets whose harmonics can explain peaks, while the\nnon-peak region likelihood penalizes pitch sets whose har-\nmonic positions are in the non-peak region. Parameters of\nthe likelihood function were trained from thousands of mu-\nsical chords mixed with note samples whose ground-truth\npitches were pre-calculated. The maximum likelihood es-\ntimation process uses an iterative greedy search strategy.It starts from an empty pitch set, and in each iteration the\npitch candidate that results in the highest multi-pitch like-\nlihood increase is selected. The process is terminated by\nthresholding on the likelihood increase, which also serves\nfor polyphony estimation. After estimating pitches in each\nframe, a pitch reﬁnement step that utilizes contextual in-\nformation is performed to remove inconsistent errors.\nIn this paper, we use the same method to perform MPE\nin each frame. Differently, we change the instantaneous\npolyphony estimation parameter settings to achieve a high\nrecall rate of the pitch estimates. This is because the note\nsampling module in Stage 3 will only remove false alarm\nnotes but cannot add back missing notes (detailed expla-\nnation in Section 2.3). In addition, we also calculate a\nsingle-pitch likelihood Lsp(p)for each estimated pitch p.\nWe deﬁne it as the multi-pitch likelihood plugged in with\nthe single pitch, i.e., Lsp(p) = Lmp(fpg). This likelihood\ndescribes how well the single pitch can explain the mix-\nture spectrum, which apparently will not be very good. But\nfrom another perspective, this likelihood can be viewed as\na salience of the pitch. One important property of multi-\npitch likelihood is that it is not additive, i.e., the multi-pitch\nlikelihood of a set of pitches is usually much smaller than\nthe sum of their single-pitch likelihoods:\nLmp(fp1;\u0001\u0001\u0001; pNg)<NX\ni=1Lmp(fpig) =NX\ni=1Lsp(pi)\n(1)\nThe reason is that the multi-pitch likelihood deﬁnition in\n[8] considers the interaction between pitches. For exam-\nple, in the peak likelihood deﬁnition, a peak will be ex-\nplained by only one pitch in the pitch set, the one whose\ncorresponding harmonic gives the best ﬁt to the frequency\nand amplitude of the peak, even if the peak could be ex-\nplained by multiple pitches. In other words, the single-\npitch likelihood considers each pitch independently while\nthe multi-pitch likelihood considers the set as a whole.\nThe reason of calculating the single-pitch likelihood is\nbecause we need to calculate a likelihood (salience) for\neach note in the second stage, which is further because\nwe need to sample notes using their likelihood in the third\nstage. Since pitches in the same frame belong to differ-\nent notes, we need to ﬁgure out the likelihood (salience) of\neach pitch instead of the likelihood of the whole pitch set.\nFigure 2(a) shows the MPE result on the example piece.\nCompared to the ground-truth in (d), it is quite noisy and\ncontains many false alarm pitches, although the main notes\ncan be inferred visually.\n2.2 Preliminary Note Tracking\nIn this stage, we implement a preliminary method to con-\nnect pitches into notes, with the ideas of ﬁlling and prun-\ning that were commonly used in the literature [2, 4, 6, 7].\nWe ﬁrst connect pitches whose frequency difference is less\nthan 0.3 semitones and time difference is less than 100 ms.\nEach connected component is then viewed as a note. Then\nnotes shorter than 100 ms are removed. The 0.3 semitones\nthreshold corresponds to the range within which the pitch\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n182Figure 2. Transcription results on the ﬁrst 11 seconds of Ach Lieben Christen, a piece of 4-part chorales by J.S. Bach. In\n(a), each pitch is plotted as a point. In (b)-(d), each note is plotted as a line whose onset is marked by a red circle.\noften ﬂuctuates within a note, while the 100 ms threshold\nis a reasonable length of a fast note, as it is the length of a\n32nd note in music with a tempo of 75 beats per minute.\nEach note is characterized by its pitch, onset, offset, and\nnote likelihood. The onset and offset times are the time of\nthe ﬁrst and last pitch in the note, respectively. The pitch\nand likelihood are calculated by averaging the pitches and\nsingle-pitch likelihood values of all the pitches within the\nnote. Again, this likelihood describes the salience of the\nnote in the audio.\nFigure 2(b) shows the preliminary note tracking result.\nCompared to (a), many noisy isolated pitches have been\nremoved. However, compared to (d), there are still a num-\nber of spurious notes, caused by consistent MPE errors\n(e.g., the long spurious note starting at 10 seconds around\nMIDI number 80, and a shorter note starting at 4.3 seconds\naround MIDI number 60). A closer look tells us that both\nnotes and many other spurious notes are higher octave er-\nrors of some already estimated notes. This makes sense as\noctave errors take about half of all errors in MPE [8].\nDue to the spurious notes, the instantaneous polyphony\nconstraint is often violated. The example piece has four\nmonophonic parts and at any time there should be no more\nthan four pitches. However, it is often to see more than four\nnotes going simultaneous in Figure 2(b) (e.g., 0-1 seconds,\n4-6 seconds, and 10-11 seconds). On the other hand, these\nspurious notes are hard to remove if we consider them in-\ndependently: They are long enough from being pruned by\nthe minimum length; They also have high enough likeli-\nhood, as the note likelihood is the average likelihood of its\npitches. Therefore, we need to consider the interaction be-tween different notes to remove these spurious notes. This\nleads to the next stage of the system.\n2.3 Final Note Tracking\nThe idea of this stage is quite simple. Thanks to the MPE\nalgorithm in Stage 1, the transcription obtained in Stage 2\ninherits the high recall and low precision property. There-\nfore, a subset of the notes that do not contain many spu-\nrious notes but contain almost all correct notes must be a\nbetter transcription. The only question now is how can we\nknow which subset is a good transcription. This question\ncan be addressed by an exploration-evaluation strategy: we\nﬁrst explore a number of subsets, and then we evaluate\nthese subsets according to some criterion. But there are\ntwo problems of this strategy: 1) how can we efﬁciently\nexplore the subsets? The number of all subsets is two to\nthe power of the number of notes, hence it is inefﬁcient to\nenumerate all the subsets. 2) What criterion should we use\nto evaluate the subsets? If our criterion considers notes in-\ndependently, then it would not work well, as the spurious\nnotes are hard to distinguish from correct notes in terms of\nindividual note properties such as length and likelihood.\n2.3.1 Note Sampling\nOur idea to address the exploration problem is to perform\nnote sampling. We randomly sample notes without re-\nplacement according to their weights. The weight equals\nto the product of the note length and the inverse of the neg-\native logarithmic note likelihood. Essentially, longer notes\nwith higher likelihood are more likely to be sampled into\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n183the subset. In this way, we can explore different note sub-\nsets, and can guarantee that notes contained in each subset\nare mostly correct. During the sampling, we also consider\nthe instantaneous polyphony constraint. A note will not be\nsampled if adding it to the subset would violate the instan-\ntaneous polyphony constraint. The sampling process stops\nif there is no valid note to sample any more.\nWe perform the sampling process Mtimes to gener-\nateMsubsets of the notes output in Stage 2. Each sub-\nset is viewed as a transcription candidate. We then eval-\nuate the transcription likelihood for each candidate and\nselect the one with the highest likelihood. The transcrip-\ntion likelihood is deﬁned as the product of the multi-pitch\nlikelihood of all time frames in the transcription. Since\nmulti-pitch likelihood considers interactions between si-\nmultaneous pitches, the transcription likelihood also con-\nsiders interactions between simultaneous notes. This can\nhelp remove spurious notes which are higher octave errors\nof some correctly transcribed notes. This is because all the\npeaks that a higher octave error pitch can explain can also\nbe explained by the correct pitch, hence having the octave\nerror pitch in addition to the correct pitch would not in-\ncrease the multi-pitch likelihood much.\n2.3.2 Chunking\nThe number of subsets (i.e., the sampling space) increases\nwith the number of notes exponentially. If we perform\nsampling on a entire music piece that contains hundreds\nof notes, it is likely to require many times of sampling to\nreach a good subset (i.e., transcription candidate). In or-\nder to reduce the sampling space, we segment the prelimi-\nnary note tracking transcription into one-second long non-\noverlapping chunks and perform sampling and evaluation\nin each chunk. Finally, selected transcriptions of differ-\nent chunks are merged together to get the ﬁnal transcrip-\ntion of the entire piece. Notes that span across multiple\nchunks can be sampled in all the chunks, and they will ap-\npear in the ﬁnal transcription if they appear in the selected\ntranscription of some chunk. Depending on the tempo and\npolyphony of the piece, the number of notes within a chunk\ncan be different. For the 4-part Bach chorales tested in this\npaper, there are about 12 notes per chunk, and we found\nsampling 100 subsets gives good accuracy and efﬁciency.\nFigure 2(c) shows the ﬁnal transcription of the system.\nWe can see that many spurious notes are removed from (b)\nwhile most correct notes remain, resulting a much better\ntranscription. The ﬁnal transcription is very close to the\nground-truth transcription.\n3. EXPERIMENTS\n3.1 Data Set\nWe use the Bach10 dataset [8] to evaluate the proposed\nsystem. This dataset consists of real musical instrumental\nperformances of ten pieces of J.S. Bach four-part chorales.\nEach piece is about thirty seconds long and was performed\nby a quartet of instruments: violin, clarinet, tenor saxo-\nphone and bassoon. Both the frame-level and note-levelground-truth transcriptions are provided with the dataset.\nIn order to evaluate the system on music pieces with differ-\nent polyphony, we use the dataset-provided matlab script\nto create music pieces with different polyphony, which are\ndifferent combinations of the four parts of each piece. Six\nduets, four trios and one quartet for each piece was created,\ntotaling 110 pieces of music with polyphony from 2 to 4.\n3.2 Evaluation Measure\nWe evaluate the proposed transcription system with com-\nmonly used note-level transcription measures [1]. A note\nis said to be correctly transcribed, if it satisﬁes both the\npitch condition and the onset condition: its pitch is within\na quarter tone from the pitch of the ground-truth note, and\nits onset is within 50 ms from the ground-truth onset. Off-\nset is not considered in determining correct notes. Then\nprecision, recall, and F-measure are deﬁned as\nP=TP\nTP+FP; R=TP\nTP+FN; F=2PR\n(P+R);(2)\nwhere TP(true positives) is the number of correctly tran-\nscribed notes, FP (false positives) is the number of re-\nported notes not in the ground-truth, and FN (false nega-\ntives) is the number of ground-truth notes not reported .\nAlthough note offest is not used in determining correct\nnotes, we do measure the Average Overlap Ratio (AOR)\nbetween correctly transcribed notes and their correspond-\ning ground-truth notes. It is deﬁned as\nAOR =min(offsets)\u0000max(onsets)\nmax(offsets)\u0000min(onsets)(3)\nAOR ranges between 0 and 1, where 1 means that the tran-\nscribed note overlaps exactly with the ground-truth note.\nTo see the improvement of different stages of the pro-\nposed system, we also evaluate the system using frame-\nlevel measures. Again, we use precision, recall, and F-\nmeasures deﬁned in Eq. (2), but here the counts are on the\npitches instead of notes. A pitch is considered correctly\ntranscribed if its frequency is within a quarter tone from a\nground-truth pitch in the same frame.\n3.3 Comparison Methods\n3.3.1 Benetos et al. ’s System\nWe compare our system with a state-of-the-art note-level\ntranscription system proposed by Benetos et al. [3]. This\nsystem ﬁrst uses shift-invariant Probabilistic Latent Com-\nponent Analysis (PLCA) to decompose the magnitude spec-\ntrogram of the music audio with a pre-learned dictionary\ncontaining spectral templates of all semitone notes of 13\nkinds of instruments (including the four kinds used in the\nBach10 dataset). The activation weights of the dictionary\nelements provide the soft version of the frame-level tran-\nscription. It is then binarized to obtain the hard version\nof the frame-level transcription. Note-level transcription\nis obtained by connecting consecutive pitches, ﬁlling short\ngaps between pitches, and pruning short notes.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n184Figure 3. Note-level transcription performances.\nThe author’s own implementation is available online to\ngenerate the soft version frame-level transcription. We then\nimplemented the postprocessing steps according to [3]. Since\nthe binarization threshold is very important in obtaining\ngood transcriptions, we performed a grid search between 1\nand 20 with a step size of 1 on the trio pieces. We found\n12 gave the best note-level F-measure and used it in all ex-\nperiments. The time threshold for ﬁlling and pruning were\nset to 100 ms, same as the other comparison methods. We\ndenote this comparison system by “Benetos13”.\n3.3.2 Klapuri’s System\nKlapuri’s system [11] is a state-of-the-art general-purposed\nframe-level transcription system. It employs an iterative\nspectral subtraction approach. At each iteration, a pitch\nis estimated according to a salience function and its har-\nmonics are subtracted from the mixture spectrum. We use\nKlapuri’s original implementation and suggested param-\neters. Since Klapuri’s system does not output note-level\ntranscriptions, we employ the preliminary note tracking\nstage in our system to convert Klapuri’s frame-level tran-\nscriptions into note-level transcriptions. We denote this\ncomparison system by “Klapuri06+”.\n3.4 Results\nFigure 3 compares the note-level transcription performance\nof the preliminary and ﬁnal results of the proposed system\nwith Benetos13 and Klapuri06+. It can be seen that the\nprecision of the ﬁnal transcription of the proposed system\nis improved signiﬁcantly from the preliminary transcrip-\ntion for all polyphony. This is accredited to the note sam-\npling stage of the proposed system. As shown in Figure\n2, note sampling removes many spurious notes and leads\nto higher precision. On the other hand, the recall of the\nﬁnal transcription is just slightly decreased (about 3%),\nFigure 4. Frame-level transcription performances.\nwhich means most correct notes survive during the sam-\npling. Therefore, the F-measure of the ﬁnal transcription\nis signiﬁcantly improved from the preliminary transcrip-\ntion for all polyphony, leading to a very promising per-\nformance on this dataset. The average F-measure on the\n60 duets is about 79%, which is about 35% higher than\nthe preliminary result in absolute value. The average F-\nmeasure on the 10 quartets is about 64%, which is also\nabout 22% higher than the preliminary transcription.\nCompared to the two state-of-the-art methods, the ﬁnal\ntranscription of the proposed system also achieves much\nhigher F-measure. In fact, the preliminary transcription is\na little inferior to Benetos13. However, the note sampling\nstage makes the ﬁnal transcription surpass Benetos13.\nIn terms of average overlap ratio (AOR) of the correctly\ntranscribed notes with the ground-truth notes, both prelim-\ninary and the ﬁnal transcription of the proposed system and\nBenetos13 achieve a similar performance, which is about\n80% for all polyphony. This is about 5% higher than Kla-\npuri06+. It is noted that 80% AOR indicates a very good\nestimation of the note lengths/offsets.\nFigure 4 presents the frame-level transcription perfor-\nmance. In this comparison, we also include the MPE re-\nsult which is the output of Stage 1. There are several in-\nteresting observations. First of all, similar to the results\nin Figure 3, the ﬁnal transcription of the proposed system\nimproves from the preliminary transcription signiﬁcantly\nin both precision and F-measure, and degrades slightly in\nrecall. This is accredited to the note sampling stage. Sec-\nond, preliminary transcription of the proposed system has\nactually improved from the MPE result in F-measure. This\nvalidates the ﬁlling and pruning operations in the second\nstage, although the increase is only about 3%. Third, the\nﬁnal transcription of the proposed system achieves signif-\nicantly higher precision and F-measure than the two com-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n185parison methods, leading to about 91%, 88%, and 85% F-\nmeasure for polyphony 2, 3, and 4, respectively. This per-\nformance is very promising and may be accurate enough\nfor many other applications.\n4. CONCLUSIONS AND DISCUSSIONS\nIn this paper, we built a note-level music transcription sys-\ntem based on an existing frame-level transcription approach.\nThe system ﬁrst performs multi-pitch estimation in each\ntime frame. It then employs a preliminary note tracking to\nconnect pitch estimates into notes. The key step of the sys-\ntem is to perform note sampling to generate a number of\nsubsets of the notes, where each subset is viewed as a tran-\nscription candidate. The sampling was based on the note\nlength and note likelihood, which was calculated using the\nsingle-pitch likelihood of pitches in the note. Then the\ntranscription candidates are evaluated using the multi-pitch\nlikelihood of simultaneous pitches in all the frames. Fi-\nnally the candidate with the highest likelihood is returned\nas the system output. The system is simple and effective.\nTranscription performance was signiﬁcantly improved due\nto the note sampling and likelihood evaluation step. The\nsystem also signiﬁcantly outperforms two other state-of-\nthe-art systems on both note-level and frame-level mea-\nsures on music pieces with polyphony from 2 to 4.\nThe technique proposed in this paper is very simple, but\nthe performance improvement is unexpectedly signiﬁcant.\nWe think the main reason is twofold. First, the note sam-\npling step lets us explore the transcription space, especially\nthe good regions of the transcription space. The single-\npitch likelihood of each estimated pitch plays an important\nrole in sampling the notes. In fact, we think that prob-\nably any kind of single-pitch salience function that have\nbeen proposed in the literature can be used to perform note\nsampling. The second reason is that we use the multi-\npitch likelihood, which considers interactions between si-\nmultaneous pitches, to evaluate these sampled transcrip-\ntions. This is important because notes contained in a sam-\npled transcription must have high salience, however, when\nconsidered as a whole, they may not ﬁt with the audio as\nwell as another sampled transcription. One limitation of\nthe proposed sampling technique is that it can only remove\nfalse alarm notes in the preliminary transcription but not\nadding back missing notes. Therefore, it is important to\nmake the preliminary transcription have a high recall rate\nbefore sampling.\n5. ACKNOWLEDGEMENT\nWe thank Emmanouil Benetos and Anssi Klapuri for pro-\nviding the source code or executable program of their tran-\nscription systems for comparison.\n6. REFERENCES\n[1] M. Bay, A.F. Ehmann, and J.S. Downie, “Evaluation of\nmultiple-F0 estimation and tracking systems,” in Proc.\nISMIR, 2009, pp. 315-320.[2] J.P. Bello, L. Daudet, M.B., Sandler, “Automatic pi-\nano transcription using frequency and time-domain in-\nformation,” IEEE Transactions on Audio, Speech, and\nLanguage Processing, vol. 14, no. 6, pp. 2242-2251,\n2006.\n[3] E. Benetos, S. Cherla, and T. Weyde, “An efﬁcient\nshift-invariant model for polyphonic music transcrip-\ntion,” in Proc. 6th Int. Workshop on Machine Learning\nand Music, 2013.\n[4] E. Benetos and S. Dixon, “A shift-invariant latent vari-\nable model for automatic music transcription,” Com-\nputer Music J., vol. 36, no. 4, pp. 81-94, 2012.\n[5] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff,\nand A. Klapuri, “Automatic music transcription: chal-\nlenges and future directions,” J. Intelligent Information\nSystems, vol. 41, no. 3, pp. 407-434, 2013.\n[6] A. Dessein, A. Cont, G. Lemaitre, “Real-time poly-\nphonic music transcription with nonnegative matrix\nfactorization and beta-divergence,” in Proc. ISMIR,\n2010, pp. 489-494.\n[7] Z. Duan, J. Han, and B. Pardo, “Multi-pitch stream-\ning of harmonic sound mixtures,” IEEE Trans. Audio\nSpeech Language Processing, vol. 22, no. 1, pp. 1-13,\n2014.\n[8] Z. Duan, B. Pardo, and C. Zhang, “Multiple fundamen-\ntal frequency estimation by modeling spectral peaks\nand non-peak regions,” IEEE Trans. Audio Speech Lan-\nguage Processing, vol. 18, no. 8, pp. 2121-2133, 2010.\n[9] G. Grindlay and D. Ellis, “Transcribing multi-\ninstrument polyphonic music with hierarchical\neigeninstruments,” IEEE Journal of Selected Topics in\nSignal Processing, vol. 5, no. 6, pp. 1159-1169, 2011.\n[10] P. Grosche, B. Schuller, M. Mller, and G. Rigoll, “Au-\ntomatic transcription of recorded music,” Acta Acustica\nUnited with Acustica, vol. 98, no. 2, pp. 199-215, 2012.\n[11] A. Klapuri, “Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes,” in Proc. IS-\nMIR, 2006, pp. 216-221.\n[12] G. Poliner, and D. Ellis, “A discriminative model for\npolyphonic piano transcription,” in EURASIP J. Ad-\nvances in Signal Processing, vol. 8, pp. 154-162, 2007.\n[13] S.A. Raczy `nski, N. Ono, and S. Sagayama. “Note de-\ntection with dynamic bayesian networks as a post-\nanalysis step for NMF-based multiple pitch estimation\ntechniques,” in Proc. WASPAA, 2009, pp. 49-52.\n[14] M. Ryyn ¨anen and A. Klapuri, “Polyphonic music tran-\nscription using note event modeling,” in Proc. WAS-\nPAA, 2005, pp. 319-322.\n[15] E. Vincent, N. Bertin, and R. Badeau, “Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation,” IEEE Trans. Audio, Speech, and Language\nProcessing, vol. 18, no. 3, pp. 528-537, 2010.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n186"
    },
    {
        "title": "Discovering Typical Motifs of a Raga from One-Liners of Songs in Carnatic Music.",
        "author": [
            "Shrey Dutta",
            "Hema A. Murthy"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418157",
        "url": "https://doi.org/10.5281/zenodo.1418157",
        "ee": "https://zenodo.org/records/1418157/files/DuttaM14.pdf",
        "abstract": "Typical motifs of a r¯aga can be found in the various songs that are composed in the same r¯aga by different composers. The compositions in Carnatic music have a definite struc- ture, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic pro- gramming based algorithm. The density of match between the one-liners and normalized cost along-with a new mea- sure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and lo- cate the matched pattern. The typical motifs of a r¯aga are then filtered using compositions of various r¯agas. Motifs are considered typical if they are present in the composi- tions of the given r¯aga and are not found in compositions of other r¯agas.",
        "zenodo_id": 1418157,
        "dblp_key": "conf/ismir/DuttaM14",
        "keywords": [
            "r¯aga",
            "songs",
            "composers",
            "structure",
            "pallavi",
            "anupallavi",
            "charanam",
            "tala",
            "one-liners",
            "matched pattern"
        ],
        "content": "DISCOVERING TYPICAL MOTIFS OF A R¯AGA FROM ONE-LINERS OF\nSONGS IN CARNATIC MUSIC\nShrey Dutta\nDept. of Computer Sci. & Engg.\nIndian Institute of Technology Madras\nshrey@cse.iitm.ac.inHema A. Murthy\nDept. of Computer Sci. & Engg.\nIndian Institute of Technology Madras\nhema@cse.iitm.ac.in\nABSTRACT\nTypical motifs of a r¯agacan be found in the various songs\nthat are composed in the same r¯agaby different composers.\nThe compositions in Carnatic music have a deﬁnite struc-\nture, the one commonly seen being pallavi, anupallavi and\ncharanam. The talais also ﬁxed for every song.\nTaking lines corresponding to one or more cycles of the\npallavi, anupallavi and charanam as one-liners, one-liners\nacross different songs are compared using a dynamic pro-\ngramming based algorithm. The density of match between\nthe one-liners and normalized cost along-with a new mea-\nsure, which uses the stationary points in the pitch contour\nto reduce the false alarms, are used to determine and lo-\ncate the matched pattern. The typical motifs of a r¯agaare\nthen ﬁltered using compositions of various r¯agas. Motifs\nare considered typical if they are present in the composi-\ntions of the given r¯agaand are not found in compositions\nof other r¯agas.\n1. INTRODUCTION\nMelody in Carnatic music is based on a concept called\nr¯aga. A r¯aga in Carnatic music is characterised by typ-\nical phrases or motifs. The phrases are not necessarily\nscale-based. They are primarily pitch trajectories in the\ntime-frequency plane. Although for annotation purposes,\nr¯agas in Carnatic are based on 12 srutis (or semitones), the\ngamak ¯as associated with the same semitone can vary sig-\nniﬁcantly across r¯agas. Nevertheless, although the phrases\ndo not occupy ﬁxed positions in the time-frequency (t-f)\nplane, a listener can determine the identity of a r¯agawithin\nfew seconds of an ¯al¯apana. An example, is a concert dur-\ning the “music season” in Chennai, where more than 90%\nof the audience can ﬁgure out the r¯aga. This despite the\nfact that more than 80% of the audience are nonprofession-\nals. The objective of the presented work is to determine\ntypical motifs of a r¯aga automatically. This is obtained\nby analyzing various compositions that are composed in a\nparticular r¯aga. Unlike Hindustani music, there is a huge\nc\rShrey Dutta, Hema A. Murthy.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Shrey Dutta, Hema A. Murthy. “Dis-\ncovering typical motifs of a R¯agafrom one-liners of songs in Carnatic\nMusic”, 15th International Society for Music Information Retrieval Con-\nference, 2014.repository of compositions that have been composed by a\nnumber of composers in different r¯agas. It is often stated\nby musicians that the famous composers have composed\nsuch that a single line of a composition is replete with the\nmotifs of the r¯aga. In this paper, we therefore take one-\nliners of different compositions and determine the typical\nmotifs of the r¯aga.\nEarlier work, [9, 10], on identifying typical motifs de-\npended on a professional musician who sung the typical\nmotifs for that r¯aga. These typical motifs were then spot-\nted in ¯al¯apanas which are improvisational segments. It was\nobserved that the number of false alarms were high. High\nranking false alarms were primarily due to partial matches\nwith the given query. Many of these were considered as\nan instance of the queried motif by some musicians. As\nalapana is an improvisational segment, the rendition of the\nsame motif could be different across alapanas especially\namong different schools. On the other hand, compositions\nin Carnatic music are rendered more or less in a similar\nmanner. Although the music evolved through the oral tra-\ndition and fairly signiﬁcant changes have crept into the mu-\nsic, compositions renditions do not vary very signiﬁcantly\nacross different schools. The number of variants for each\nline of the song can vary quite a lot though. Nevertheless,\nthe meter of motifs and the typical motifs will generally be\npreserved.\nIt is discussed in [15] that not all repeating patterns are\ninteresting and relevant. In fact, the vast majority of ex-\nact repetitions within a music piece are not musically in-\nteresting. The algorithm proposed in [15] mostly gener-\nates interesting repeating patterns along with some non-\ninteresting ones which are later ﬁltered during post pro-\ncessing. The work presented in this paper is an attempt\nfrom a similar perspective. The only difference is that typ-\nical motifs of r¯agas need not be interesting to a listener.\nThe primary objective for discovering typical motifs, is\nthat these typical motifs can be used to index the audio\nof a rendition. Typical motifs could also be used for r¯aga\nclassiﬁcation. The proposed approach in this work gen-\nerates similar patterns across one-liners of a r¯aga. From\nthese similar patterns, the typical motifs are ﬁltered by us-\ning compositions of various r¯agas. Motifs are considered\ntypical of a r¯agaif they are present in the compositions\nof a particular r¯aga and are NOT found in other r¯agas.\nThis ﬁltering approach is similar to anti-corpus approach\nof Conklin [6, 7] for the discovery of distinctive patterns.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n397Figure 1. RLCS matching two sequences partially\nMost of the previous work, regarding discovery of re-\npeated patterns of interest in music, is on western music.\nIn [11], B. Jansen et al discusses the current approaches on\nrepeated pattern discovery. It discusses string based meth-\nods and geometric methods for pattern discovery. In [14],\nLie Lu et al used constant Q transforms and proposed a\nsimilarity measure between musical features for doing re-\npeated pattern discovery. In [15], Meredith et. al. pre-\nsented Structure Induction Algorithms (SIA) using a geo-\nmatric approach for discovering repeated patterns that are\nmusically interesting to the listener. In [4, 5], Collins et.\nal.introduced improvements in Meredith’s Structure In-\nduction Algorithms. There has also been some signiﬁcant\nwork on detecting melodic motifs in Hindustani music by\nJoe Cheri Ross et. al. [16]. In this approach, the melody\nis converted to a sequence of symbols and a variant of dy-\nnamic programming is used to discover the motif.\nIn a Carnatic music concert, many listeners from the au-\ndience are able to identify the r¯agaat the very beginning of\nthe composition, usually during the ﬁrst line itself — a line\ncorresponds to one or more tala cycles. Thus, ﬁrst lines of\nthe compositions could contain typical motifs of a r¯aga. A\npattern which is repeated within a ﬁrst line could still be\nnotspeciﬁc to a r¯aga. Whereas, a pattern which is present\nin most of the ﬁrst lines could be a typical motif of that\nr¯aga. Instead of just using ﬁrst lines, we have also used\nother one-liners from compositions, namely, lines from the\npallavi, anupallavi and charanam. In this work, an attempt\nis made to ﬁnd repeating patterns across one-liners and\nnotwithin a one-liner. Typical motifs are ﬁltered from the\ngenerated repeating patterns during post processing. These\ntypical motifs are available online1\nThe length of the typical motif to be discovered is not\nknown a priori. Therefore there is a need for a technique\nwhich can itself determine the length of the motif at the\ntime of discovering it. Dynamic Time Warping (DTW)\nbased algorithms can only ﬁnd a pattern of a speciﬁc length\nsince it performs end-to-end matching of the query and\ntest sequence. There is another version of DTW known as\n1http://www.iitm.ac.in/donlab/typicalmotifs.\nhtmlUnconstrained End Point-DTW (UE-DTW) that can match\nthe whole query with a partial test but still the query is not\npartially matched. Longest Common Subsequence (LCS)\nalgorithm on the other hand can match the partial query\nwith partial test sequence since it looks for a longest com-\nmon subsequence which need not be end-to-end. LCS by\nitself is not appropriate as it requires discrete symbols and\ndoes not account for local similarity. A modiﬁed version\nof LCS known as Rough Longest Common Subsequence\ntakes continuous symbols and takes into account the local\nsimilarity of the longest common subsequence. The algo-\nrithm proposed in [13] to ﬁnd rough longest common se-\nquence between two sequences ﬁts the bill for our task of\nmotif discovery. An example of RLCS algorithm match-\ning two partial phrases is shown in Figure 1. The two\nmusic segments are represented by their tonic normalized\nsmoothed pitch contours [9, 10]. The stationary points,\nwhere the ﬁrst derivative is zero, of the tonic normalized\npitch contour are ﬁrst determined. The points are then in-\nterpolated using cubic Hermite interpolation to smooth the\ncontour.\nIn previous uses of RLCS for motif spotting task [9,10],\na number of false alarms were observed. One of the most\nprevalent false alarms is the test phrase with a sustained\nnote which comes in between the notes of the query. The\nslope of the linear trend in stationary points along with its\nstandard deviation is used to address this issue.\nThe rest of the paper is organized as follows. In Sec-\ntion 2 the use of one-liners of compositions to ﬁnd motifs\nis discussed. Section 3 discusses the optimization criteria\nto ﬁnd the rough longest common subsequence. Section\n4 describes the proposed approach for discovering typical\nmotifs of r¯agas. Section 5 describe the dataset used in this\nwork. Experiments and results are presented in Section 6.\n2. ONE-LINERS OF SONGS\nAs previously mentioned, ﬁrst line of the composition con-\ntains the characteristic traits of a r¯aga. The importance of\nthe ﬁrst lines and the r¯againformation it holds is illustrated\nin great detail in the T. M. Krishna’s book on Carnatic mu-\nsic [12]. T. M. Krishna states that opening section called\n“pallavi” directs the melodic ﬂow of the r¯aga. Through its\nrendition, the texture of the r¯agacan be felt. Motivated by\nthis observation, an attempt is made to verify the conjec-\nture that typical motifs of a r¯agacan be obtained from the\nﬁrst lines of compositions.\nAlong with the lines from pallavi, we have also selected\nfew lines from other sections, namely, ‘anupallavi’ and\n‘charanam’. Anupallavi comes after pallavi and the melodic\nmovements in this section tend to explore the r¯again the\nhigher octave [12]. These lines are referred to as one-liners\nfor a r¯aga.\n3. OPTIMIZATION CRITERIA TO FIND ROUGH\nLONGEST COMMON SUBSEQUENCE\nThe rough longest common subsequence (rlcs) between\ntwo sequences, X=hx1;x2;\u0001\u0001\u0001;xniandY=hy1;y2;\u0001\u0001\u0001\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n398Figure 2. (a) Pitch contour of the ﬁve phrases which are considered similar. Stationary points are marked in green and red\nfor the true positives and false alarms respectively. (b) Pitch values only at the stationary points. Slope of the linear trend\nin stationary points along-with its standard deviation helps in reducing the false alarms.\n;ymi, of length nandmis deﬁned as the longest com-\nmon subsequence (lcs) ZXY=h(xi1;yj1);(xi2;yj2);\u0001\u0001\u0001;\n(xip;yjp)i,1\u0014i1< i2<\u0001\u0001\u0001< ip\u0014n,1\u0014j1< j2<\n\u0001\u0001\u0001<jp\u0014m; such that the similarity between xikandyjk\nis greater than a threshold, \u001csim, fork= 1;\u0001\u0001\u0001;p. There\nare no constraints on the length and on the local similarity\nof the rlcs. Some applications demand the rlcs to be lo-\ncally similar or its length to be in a speciﬁc range. For the\ntask of motif discovery along with these constraints, one\nmore constraint is used to reduce false alarms. Before dis-\ncussing the optimization measures used to ﬁnd the rlcs in\nthis work, a few quantities need to be deﬁned.\nlw\nSXY=sX\nk=1sim(x ik;yjk) (1)\ngX=is\u0000i1+ 1\u0000s (2)\ngY=js\u0000j1+ 1\u0000s (3)\nLetSXY=h(xi1;yj1);(xi2;yj2);\u0001\u0001\u0001;(xis;yjs)i;1\u0014\ni1< i2<\u0001\u0001\u0001< is\u0014n;1\u0014j1< j 2<\u0001\u0001\u0001< js\u0014\nm; be a rough common subsequence (rcs) of length sand\nsim(x ik;yik)2[0;1]be the similarity between xikand\nyikfork= 1;\u0001\u0001\u0001;s. Equation (1) deﬁnes the weighted\nlength ofSXYas sum of similarities, sim(x ik;yik),k=\n1;\u0001\u0001\u0001;s. Thus, weighted length is less than or equal to s.\nThe number of points in the shortest substring of sequence\nX, containing the rcs SXY, that are not the part of the rcs\nSXYare termed as gaps in SXYwith respect to sequence\nXas deﬁned by Equation (2). Similarly, Equation (3) de-ﬁnes the gaps in SXYwith respect to sequence Y. Small\ngaps indicate that the distribution of rcs is dense in that\nsequence.\nThe optimization measures to ﬁnd the rlcs are described\nas follows.\n3.1 Density of the match\nEquation (4) represents the distribution of the rcs SXYin\nthe sequences XandY. This is called density of match,\n\u000eSXY. This quantity needs to be maximized to make sure\nthe subsequence, SXY, is locally similar. \f2[0;1]weighs\nthe individual densities in sequences XandY.\n\u000eSXY=\flw\nSXY\nlw\nSXY+gX+ (1\u0000\f)lw\nSXY\nlw\nSXY+gY(4)\n3.2 Normalized weighted length\nThe weighted length of rcs is normalized as shown in Equa-\ntion (5) to restrict its range to [0;1].nandmare the lengths\nof sequences XandY, respectively.\n^lw\nSXY=lw\nSXY\nmin(m;n)(5)\n3.3 Linear trend in stationary points\nAs observed in [9, 10], the rlcs obtained using only the\nabove two optimization measures suffered from a large num-\nber of false alarms for the motif spotting task. The false\nalarms generally constituted of long and sustained notes.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n399This resulted in good normalised weighted lengths and den-\nsity. To address this issue, the slope and standard deviation\nof the slope of the linear trend in stationary points of a\nphrase are estimated. Figure 2 shows a set of phrases. This\nset has ﬁve phrases which are termed as similar phrases\nbased on their density of match and normalized weighted\nlength. The ﬁrst two phrases, shown in green, are true pos-\nitives while the remaining, shown in red, are false alarms.\nFigure 2 also shows the linear trend in stationary points for\nthe corresponding phrases. It is observed that the trends are\nsimilar for true positives when compared to that of the false\nalarms. The slope of the linear trend for the ﬁfth phrase\n(false alarm) is similar to the true positives but its standard\ndeviation is less. Therefore, a combination of the slope and\nthe standard deviation of the linear trend is used to reduce\nthe false alarms.\nLet the stationary points in the shortest substring of se-\nquencesXandYcontaining the rcs SXYbehxq1;xq2;\u0001\u0001\u0001;\nxqtxiandhyr1;yr2;\u0001\u0001\u0001;yrtyirespectively, where txand\ntyare the number of stationary points in the respective sub-\nstrings. Equation (6) estimates the slope of the linear trend,\nof stationary points in the substring of sequence X, as the\nmean of the ﬁrst difference of stationary points, which is\nsame asxqtx\u0000xq1\ntx\u00001[8]. Its standard deviation is estimated\nusing Equation (7). Similarly, \u0016Y\nSXYand\u001bY\nSXYare also\nestimated for substring of sequence Y.\n\u0016X\nSXY=1\ntx\u00001tx\u00001X\nk=1(xqk+1\u0000xqk) (6)\n\u001bX\nSXY2=1\ntx\u00001tx\u00001X\nk=1((xqk+1\u0000xqk)\u0000\u0016X\nSXY)2(7)\nLetz1=\u0016X\nSXY\u001bY\nSXYandz2=\u0016Y\nSXY\u001bX\nSXY. For a true\npositive, the similarity in the linear trend should be high.\nEquation (8) calculates this similarity which needs to be\nmaximized. This similarity has negative value when the\ntwo slopes are of different sign and thus, the penalization\nis more.\n\u001aSXY=8\n><\n>:max(z 1;z2)\nmin(z 1;z2)if z 1<0;z2<0\nmin(z 1;z2)\nmax(z 1;z2)otherwise(8)\nFinally, Equation (9) combines these three optimization\nmeasures to get a score value which is maximized. Then\nthe rlcs,RXY, between the sequences XandYis deﬁned,\nas an rcs with a maximum score, in Equation (10). The rlcs\nRXYcan be obtained using dynamic programming based\napproach discussed in [9, 13].\nScore SXY=\u000b\u000eSXY^lw\nSXY+ (1\u0000\u000b)\u001aSXY (9)\nRXY= argmax\nSXY(Score SXY) (10)R¯aga Number Average\nName of duration\none-liners (secs)\nBhairavi 17 16.87\nKamboji 12 13\nKalyani 9 12.76\nShankarabharanam 12 12.55\nVarali 9 9.40\nOverall 59 12.91\nTable 1. Database of one-liners\n4. DISCOVERING TYPICAL MOTIFS OF R¯AGAS\nTypical motifs of a r¯agaare discovered using one-liners\nof songs in that r¯aga. For each voiced part in a oneliner\nof a r¯aga, rlcs is found with the overlapping windows in\nvoiced parts of other one-liners of that r¯aga. Only those\nrlcs are selected whose score values and lengths (in sec-\nonds) are greater than thresholds \u001cscrand\u001clenrespectively\nThe voiced parts which generated no rlcs are interpreted to\nhave no motifs. The rlcs generated for a voiced part are\ngrouped and this group is interpreted as a motif found in\nthat voiced part. This results in a number of groups (mo-\ntifs) for a r¯aga. Further, ﬁltering is performed to isolate\ntypical motifs of that r¯aga.\n4.1 Filtering to get typical motifs of a r¯aga\nThe generated motifs are ﬁltered to get typical motifs of a\nr¯agausing compositions of various r¯agas. The most rep-\nresentative candidate of a motif, a candidate with highest\nscore value, is selected to represent that motif or group.\nThe instances of a motif are spotted in the compositions of\nvarious r¯agas as explained in [9,10]. Each motif is consid-\nered as a query to be searched for in a composition. The\nrlcs is found between the query and overlapped windows in\na composition. From the many generated rlcs from many\ncompositions of a r¯aga, top\u001cnrlcs with highest score val-\nues are selected. The average of these score values deﬁnes\nthe presence of this motif in that r¯aga. A motif of a r¯aga\nis isolated as its typical motif if the presence of this motif\nis more in the given r¯agathan in other r¯agas. The value of\n\u001cnis selected empirically.\n5. DATASET\nThe one-liners are selected from ﬁve r¯agas as shown in Ta-\nble 1. The lines are sung by a musician in isolation. This\nis done to ensure that the pitch estimation does not get af-\nfected due to the accompanying instruments. The average\nduration of the one-liners is 12.91 seconds. As mentioned\nearlier, these one-liners come from the various sections of\nthe composition, primarily from the pallavi.\nThe compositions used for ﬁltering also comes from the\nsame ﬁve r¯agas as shown in Table 2. These compositions\nare taken from the Charsur collection [1]. These are seg-\nments from live concerts with clean recording.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n400R¯aga Number Average\nName of duration\ncompositions (secs)\nBhairavi 20 1133\nKamboji 10 1310.3\nKalyani 16 1204.3\nShankarabharanam 10 1300.6\nVarali 18 1022\nOverall 74 1194\nTable 2. Database of compositions\n6. EXPERIMENTS AND RESULTS\nThe pitch of the music segment is used as a basic feature in\nthis work. This pitch is estimated from Justin Solomon’s\nalgorithm [17] which is efﬁciently implemented in the es-\nsentia open-source C++ library [2]. This pitch is further\nnormalized using tonic and then smoothed by interpolat-\ning the stationary points of the pitch contour using cubic\nspline interpolation.\nThe similarity, sim(x ik;yjk), between two symbols xik\nandyjkis deﬁned in the Equation (11), where stis the\nnumber of cent values that represent one semitone. For\nthis work, the value of stis 10. The penalty is low when\nthe two symbols are within one semitone while the penalty\nis signiﬁcant for larger deviations. This is performed to\nensure that although signiﬁcant variations are possible in\nCarnatic music, variations larger than a semitone might re-\nsult in a different r¯aga.\nsim(x ik;yjk) =(\n1\u0000jxik\u0000yjkj3\n(3st)3ifjxik\u0000yjkj<3st\n0 otherwise\n(11)\nThe similarity threshold, \u001csim, is empirically set to 0.45\nwhich accepts similarities when two symbols are less than\n2.5 semitones (approx.) apart, although penalty is high af-\nter a semitone. The threshold on the score of rlcs, \u001cscr, is\nempirically set to 0.6 to accept rlcs with higher score val-\nues. The threshold on the length of the rlcs, \u001clen, is set to\n2 seconds to get longer motifs. The value of \fis set to 0.5\nto give equal importance to the individual densities in both\nthe sequences and \u000bvalue is set to 0.6 which gives more\nimportance to density of match and normalized weighted\nlength as compared to linear trend in stationary points. \u001cn\nis empirically set to 3.\nThe similar patterns found across one-liners of a r¯aga\nare summarized in Table 3. Some of these similar pat-\nterns are not typical of the r¯aga. These are therefore ﬁl-\ntered out by checking for their presence in various com-\npositions. The summary of the resulting typical motifs is\ngiven in Table 4. The average length of all the typical mo-\ntifs is sufﬁciently longer than what were used in [10]. The\nshorter motifs used in [10] also resulted in great deal of\nfalse alarms. The importance of longer motifs was dis-\ncussed in [9] where the longer motifs were inspired from\nther¯agatest conducted by Rama Verma [3]. Rama VermaR¯aga Number of Average\nName discovered duration\npatterns (secs)\nBhairavi 10 3.52\nKamboji 5 3.40\nKalyani 6 4.48\nShankarabharanam 6 3.42\nVarali 3 3.84\nOverall 30 3.73\nTable 3. Summary of discovered similar patterns across\none-liners\nR¯aga Number of Average\nName typical duration\nmotifs (secs)\nBhairavi 5 4.52\nKamboji 0 NA\nKalyani 0 NA\nShankarabharanam 5 3.64\nVarali 2 4.79\nOverall 12 4.32\nTable 4. Summary of typical motifs isolated after ﬁltering\nused motifs of approximately 3 seconds duration. The typ-\nical motifs discovered in our work are also of similar dura-\ntion. All the patterns of Kamboji and Kalyani are ﬁltered\nout resulting in no typical motifs for these r¯agas. We have\nearlier discussed that the compositions in Carnatic music\nare composed in a way that the r¯againformation is present\nat the very beginning. Therefore, without a doubt we are\nsure that the typical motifs are present in the one-liners we\nhave used for Kalyani and Kamboji. But, it is possible that\nthese typical motifs are not repeating sufﬁcient number of\ntimes across one-liners (two times in our approach) or their\nlengths are shorter than the threshold we have used. These\ncould be the reasons we are not able to pick them up. All\nthe typical patterns are veriﬁed by a musician. According\nto his judgment, all the ﬁltered patterns were indeed typi-\ncal motifs of the corresponding r¯agas. Although, he noted\nthat one typical motif in Varali is a smaller portion of the\nother discovered typical motif of Varali. This repetition of\nsmaller portion is observed in Shankarabharanam as well.\n7. CONCLUSION AND FUTURE WORK\nThis paper presents an approach to discover typical motifs\nof a r¯agafrom the one-liners of the compositions in that\nr¯aga. The importance of one-liners is discussed in detail.\nA new measure is introduced, to reduce the false alarms,\nin the optimization criteria for ﬁnding rough longest com-\nmon subsequence between two given sequences. Using\nthe RLCS algorithm, similar patterns across one-liners of\nar¯agaare found. Further, the typical motifs are isolated\nby a ﬁltering technique, introduced in this paper, which\nuses compositions of various r¯agas. These typical motifs\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n401are validated by a musician. All the generated typical mo-\ntifs are found to be signiﬁcantly typical of their respective\nr¯agas.\nIn this work, only one musician’s viewpoint is consid-\nered on validating the characteristic nature of the discov-\nered typical motifs. In future, we would like to conduct a\nMOS test, asking other experts and active listeners to de-\ntermine the r¯agafrom the typical motifs. We would also\nlike to perform r¯agaclassiﬁcation of the compositions and\nalapanas using the typical motifs. In future, we would also\nlike to do a thorough comparison of our approach with\nother methods. In this paper, we have only addressed one\nprevalent type of false alarms. Other types of false alarms\nalso need to be identiﬁed and addressed. It should be con-\nsidered that approaches taken to reduce the false alarms do\nnot affect the true positives signiﬁcantly. Further, these ex-\nperiments need to be repeated for a much larger number of\none-liners from many r¯agas such that the typical motifs re-\npeat signiﬁcantly across one-liners and thus get captured.\nIt will also be interesting to automatically detect and ex-\ntract the one-liners from the available compositions. This\nwill enable the presented approach to scale to a large num-\nber of r¯agas.\n8. ACKNOWLEDGMENTS\nThis research was partly funded by the European Research\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583). We are grateful to Vignesh Ishwar for\nrecording the one-liners. We would also like to thank Srid-\nharan Sankaran, Nauman Dawalatabad and Manish Jain\nfor their invaluable and unconditional help in completing\nthis paper.\n9. REFERENCES\n[1] Charsur. http://charsur.com/in/. Accessed:\n2014-07-18.\n[2] Essentia open-source c++ library. http:\n//essentia.upf.edu. Accessed: 2014-07-\n18.\n[3] Rama verma, raga test. http://www.youtube.\ncom/watch?v=3nRtz9EBfeY. Accessed: 2014-\n07-18.\n[4] Tom Collins, Andreas Arzt, Sebastian Flossmann, and\nGerhard Widmer. Siarct-cfp: Improving precision and\nthe discovery of inexact musical patterns in point-set\nrepresentations. In Alceu de Souza Britto Jr., Fabien\nGouyon, and Simon Dixon, editors, ISMIR, pages 549–\n554, 2013.\n[5] Tom Collins, Jeremy Thurlow, Robin Laney, Alistair\nWillis, and Paul H. Garthwaite. A comparative eval-\nuation of algorithms for discovering translational pat-\nterns in baroque keyboard works. In J. Stephen Downie\nand Remco C. Veltkamp, editors, ISMIR, pages 3–8.International Society for Music Information Retrieval,\n2010.\n[6] Darrell Conklin. Discovery of distinctive patterns in\nmusic. In Intelligent Data Analysis, pages 547–554,\n2010.\n[7] Darrell Conklin. Distinctive patterns in the ﬁrst move-\nment of brahms string quartet in c minor. Journal of\nMathematics and Music, 4(2):85–92, 2010.\n[8] Jonathan D. Cryer and Kung-Sik Chan. Time Series\nAnalysis: with Applications in R. Springer, 2008.\n[9] Shrey Dutta and Hema A Murthy. A modiﬁed rough\nlongest common subsequence algorithm for motif spot-\nting in an alapana of carnatic music. In Communica-\ntions (NCC), 2014 Twentieth National Conference on,\npages 1–6, Feb 2014.\n[10] Vignesh Ishwar, Shrey Dutta, Ashwin Bellur, and\nHema A. Murthy. Motif spotting in an alapana in\ncarnatic music. In Alceu de Souza Britto Jr., Fabien\nGouyon, and Simon Dixon, editors, ISMIR, pages 499–\n504, 2013.\n[11] Berit Janssen, W. Bas de Haas, Anja V olk, and Peter\nvan Kranenburg. Discovering repeated patterns in mu-\nsic: state of knowledge, challenges, perspectives. In-\nternational Symposium on Computer Music Modeling\nand Retrieval (CMMR), pages 225–240, 2013.\n[12] T. M. Krishna. A Southern Music: The Karnatic Story,\nchapter 5. HarperCollins, India, 2013.\n[13] Hwei-Jen Lin, Hung-Hsuan Wu, and Chun-Wei Wang.\nMusic matching based on rough longest common sub-\nsequence. Journal of Information Science and Engi-\nneering, pages 27, 95–110., 2011.\n[14] Lie Lu, Muyuan Wang, and Hong-Jiang Zhang. Re-\npeating pattern discovery and structure analysis from\nacoustic music data. In Proceedings of the 6th ACM\nSIGMM International Workshop on Multimedia Infor-\nmation Retrieval, MIR ’04, pages 275–282, New York,\nNY , USA, 2004. ACM.\n[15] David Meredith, Kjell Lemstrom, and Geraint A. Wig-\ngins. Algorithms for discovering repeated patterns in\nmultidimensional representations of polyphonic music.\nJournal of New Music Research, pages 321–345, 2002.\n[16] Joe Cheri Ross, Vinutha T. P., and Preeti Rao. Detect-\ning melodic motifs from audio for hindustani classical\nmusic. In Fabien Gouyon, Perfecto Herrera, Luis Gus-\ntavo Martins, and Meinard Mller, editors, ISMIR, pages\n193–198. FEUP Edies, 2012.\n[17] J. Salamon and E. Gomez. Melody extraction from\npolyphonic music signals using pitch contour char-\nacteristics. IEEE Transactions on Audio, Speech and\nLanguage Processing, pages 20(6):1759–1770, Aug.\n2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n402"
    },
    {
        "title": "Impact of Listening Behavior on Music Recommendation.",
        "author": [
            "Katayoun Farrahi",
            "Markus Schedl",
            "Andreu Vall",
            "David Hauger",
            "Marko Tkalcic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417505",
        "url": "https://doi.org/10.5281/zenodo.1417505",
        "ee": "https://zenodo.org/records/1417505/files/FarrahiSVHT14.pdf",
        "abstract": "The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommen- dations with features extracted from a user’s history of lis- tening events. We investigate the impact of listener’s be- havior by considering features such as play counts, “main- streaminess”, and diversity in music taste on the perfor- mance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user’s listening behavior is characterized into a three dimensional feature space consisting of play count, “mainstreaminess” (i.e. the degree to which the observed user listens to cur- rently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influ- ence figures of merit of various music recommendation ap- proaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country.",
        "zenodo_id": 1417505,
        "dblp_key": "conf/ismir/FarrahiSVHT14",
        "keywords": [
            "music recommendation systems",
            "increasingly intelligent",
            "user behavior",
            "personalized recommendations",
            "features extracted",
            "users listening events",
            "three dimensional feature space",
            "play counts",
            "mainstreaminess",
            "diversity"
        ],
        "content": "IMPACT OF LISTENING BEHA VIOR ON MUSIC RECOMMENDATION\nKatayoun Farrahi\nGoldsmiths, University of London\nLondon, UK\nk.farrahi@gold.ac.ukMarkus Schedl, Andreu Vall, David Hauger, Marko Tkal ˇciˇc\nJohannes Kepler University\nLinz, Austria\nfirstname.lastname@jku.at\nABSTRACT\nThe next generation of music recommendation systems will\nbe increasingly intelligent and likely take into account user\nbehavior for more personalized recommendations. In this\nwork we consider user behavior when making recommen-\ndations with features extracted from a user’s history of lis-\ntening events. We investigate the impact of listener’s be-\nhavior by considering features such as play counts, “main-\nstreaminess”, and diversity in music taste on the perfor-\nmance of various music recommendation approaches. The\nunderlying dataset has been collected by crawling social\nmedia (speciﬁcally Twitter) for listening events. Each user’s\nlistening behavior is characterized into a three dimensional\nfeature space consisting of play count, “mainstreaminess”\n(i.e. the degree to which the observed user listens to cur-\nrently popular artists), and diversity (i.e. the diversity of\ngenres the observed user listens to). Drawing subsets of\nthe 28,000 users in our dataset, according to these three\ndimensions, we evaluate whether these dimensions inﬂu-\nence ﬁgures of merit of various music recommendation ap-\nproaches, in particular, collaborative ﬁltering (CF) and CF\nenhanced by cultural information such as users located in\nthe same city or country.\n1. INTRODUCTION\nEarly attempts in collaborative ﬁltering (CF) recommender\nsystems for music content have generally treated all users\nas equivalent in the algorithm [1]. The predicted score (i.e.\nthe likelihood that the observed user would like the ob-\nserved music piece) was a weighted average of the Knear-\nest neighbors in a given similarity space [8]. The only way\nthe users were treated differently was the weight, which\nreﬂected the similarity between users. However, users’ be-\nhavior in the consumption of music (and other multimedia\nmaterial in general) has more dimensions than just ratings.\nRecently, there has been an increase of research in mu-\nsic consumption behavior and recommender systems that\ndraw inspiration from psychology research on personal-\nity. Personality accounts for the individual difference in\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2014 International Society for Music Information Retrieval.users in their behavioral styles [9]. Studies showed that\npersonality affects rating behavior [6], music genre prefer-\nences [11] and taste diversity both in music [11] and other\ndomains (e.g. movies in [2]).\nThe aforementioned work inspired us to investigate how\nuser features intuitively derived from personality traits af-\nfect the performance of a CF recommender system in the\nmusic domain. We chose three user features that are ar-\nguably proxies of various personality traits for user clus-\ntering and ﬁne-tuning of the CF recommender system. The\nchosen features are play counts, mainstreaminess anddi-\nversity. Play count is a measure of how often the observed\nuser engages in music listening (intuitively related to ex-\ntraversion). Mainstreaminess is a measure that describes\nto what degree the observed user prefers currently popular\nsongs or artists over non-popular (and is intuitively related\nto openness and agreeableness). The diversity feature is\na measure of how diverse the observed user’s spectrum of\nlistened music is (intuitively related to openness).\nIn this paper, we consider the music listening behavior\nof a set of 28,000 users, obtained by crawling and ana-\nlyzing microblogs. By characterizing users across a three\ndimensional space of play count, mainstreaminess, and di-\nversity, we group users and evaluate various recommenda-\ntion algorithms across these behavioral features. The goal\nis to determine whether or not the evaluated behavioral\nfeatures inﬂuence the recommendation algorithms, and if\nso which directions are most promising. Overall, we ﬁnd\nthat recommending with collaborative ﬁltering enhanced\nby continent and country information generally performs\nbest. We also ﬁnd that recommendations for users with\nlarge play counts, higher diversity and mainstreaminess\nvalues are better.\n2. RELATED WORK\nThe presented work stands at the crossroads of personality-\ninspired user features and recommender systems based on\ncollaborative ﬁltering.\nAmong various models of personality, the Five-factor\nmodel (FFM) is the most widely used and is composed\nof the following traits: openness, conscientiousness, ex-\ntraversion, agreeableness andneuroticism [9]. The per-\nsonality theory inspired several works in the ﬁeld of rec-\nommender systems. For example, Pu et al. [6] showed that\nuser rating behavior is correlated with personality factors.\nTkal ˇciˇc et al. [13] used FFM factors to calculate similari-\nties in a CF recommender system for images. A study by\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n483Rentfrow et al. [11] showed that scoring high on certain\npersonality traits is correlated with genre preferences and\nother listening preferences like diversity. Chen et al. [2]\nargue that people who score high in openness to new expe-\nriences prefer more diverse recommendations than people\nwho score low. The last two studies explore the relations\nbetween personality and diversity. In fact, the study of di-\nversity in recommending items has become popular after\nthe publishing of two popular books, The Long Tail [4]\nandThe Filter Bubble [10]. However, most of the work\nwas focused on the trade-off between recommending di-\nverse and similar items (e.g. in [7]). In our work, we treat\ndiversity not as a way of presenting music items but as a\nuser feature, which is a novel way of addressing the usage\nof diversity in recommender systems.\nThe presented work builds on collaborative ﬁltering (CF)\ntechniques that are well established in the recommender\nsystems domain [1]. CF methods have been improved us-\ning context information when available [3]. Recently, [12]\nincorporated geospatial context to improve music recom-\nmendations on a dataset gathered through microblog crawl-\ning [5]. In the presented work, we advance this work by\nincluding personality-inspired user features.\n3. USER BEHA VIOR MODELING\n3.1 Dataset\nWe use the “Million Musical Tweets Dataset”1(MMTD)\ndataset of music listening activities inferred from micro-\nblogs. This dataset is freely available [5], and contains ap-\nproximately 1;100;000 listening events of 215;000 users\nlistening to a total of 134;000 unique songs by 25;000 art-\nists, collected from Twitter. The data was acquired crawl-\ning Twitter and identifying music listening events in tweets,\nusing several databases and rule-based ﬁlters. Among oth-\ners, the dataset contains information on location for each\npost, which enables location-aware analyses and recom-\nmendations. Location is provided both as GPS coordi-\nnates and semantic identiﬁers, including continent, coun-\ntry, state, county, and city.\nThe MMTD contains a large number of users with only\na few listening events. These users are not suitable for re-\nliable recommendation and evaluation. Therefore, we con-\nsider a subset of users who had at least ﬁve listening events\nover different artists. This subset consists of 28;000 users.\nBasic statistics of the data used in all experiments are\ngiven in Table 1. The second column shows the total amount\nof the entities in the corresponding ﬁrst row, whereas the\nright-most six columns show principal statistics based on\nthe number of tweets.\n3.2 Behavioral Features\nEach user is deﬁned by a set of three behavioral features:\nplay count, diversity, and mainstreaminess, deﬁned next.\nThese features are used to group users and to determine\nhow they inﬂuence the recommendation process.\n1http://www.cp.jku.at/datasets/MMTDPlay count The play count of a user, P(u), is a measure\nof the quantity of listening events for a user u. It is com-\nputed as the total number of listening events recorded over\nall time for a given user.\nDiversity The diversity of a user, D(u), can be thought\nof as a measure which captures the range of listening tastes\nby the user. It is computed as the total number of unique\ngenres associated with all of the artists listened to by a\ngiven user. Genre information was obtained by gathering\nthe top tags from Last.fm for each artist in the collection.\nWe then identiﬁed genres within these tags by matching the\ntags to a selection of 20 genres indicated by Allmusic.com.\nMainstreaminess The mainstreaminess M(u)is a mea-\nsure of how mainstream a user uis in terms of her/his lis-\ntening behavior. It reﬂects the share of most popular artists\nwithin all the artists user uhas listened to. Users that listen\nmostly to artists that are popular in a given time window\ntend to have high M(u), while users who listen more to\nartists that are rarely among the most popular ones tend to\nscore low.\nFor each time window i2 f1: : : Igwithin the dataset\n(where Iis the number of all time windows in the dataset)\nwe calculated the set of the most popular artists Ai. We\ncalculated the most popular artists in an observed time pe-\nriod as follows. For the given period we sorted the artists\nby the aggregate of the listening events they received in a\ndecreasing order. Then, the top kartists, that cover at least\n50% of all the listening events of the observed period are\nregarded as popular artists. For each user uin a given time\nwindow iwe counted the number of play counts of popu-\nlar artists Pp\ni(u)and normalized it with all the play counts\nof that user in the observed time window Pa\ni(u). The ﬁnal\nvalue M(u)was aggregated by averaging the partial values\nfor each time window:\nM(u) =1\nIIX\ni=1Pp\ni(u)\nPa\ni(u)(1)\nIn our experiments, we investigated time windows of six\nmonths and twelve months.\nTable 3 shows the correlation between individual user\nfeatures. No signiﬁcant correlation was found, except for\nthe mainstreaminess using an interval of six months and an\ninterval of twelve months, which is expected.\n3.3 User Groups\nEach user is characterized by a three dimensional feature\nvector consisting of M(u),D(u),P(u). The distribution\nof users across these features are illustrated in Figures 1\nand 2. In Figure 3, mainstreaminess is considered with a\n6month interval. The results illustrate the even distribu-\ntion of users across these features. Therefore, for group-\ning users, we consider each feature individually and divide\nusers between groups considering a threshold.\nFor mainstreaminess, we consider the histogram of M(u)\n(Figure 2 for a 6month (top) and 12month (bottom)) in\nmaking the groups. We consider 2 different cases for group-\ning users. First, we divide the users into 2 groups according\nto the median value (referred to as M6(12)-median-G1(2)).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n484Level Amount Min. 1st Qu. Median Mean 3rd Qu. Max.\nUsers 27,778 5 7 10 27.69 17 89,320\nArtists 21,397 1 1 2 35.95 9 11,850\nTracks 108,676 1 1 1 7.08 4 2,753\nContinents 7 9 4,506 101,400 109,900.00 142,200 374,300\nCountries 166 1 12 71 4,633.00 555 151,600\nStates 872 1 7 40 882.00 195 148,900\nCounties 3557 1 2 10 216.20 41 191,900\nCities 15123 1 1 5 50.86 16 148,900\nTable 1. Basic dataset characteristics, where “Amount” is the number of items, and the statistics correspond to the values\nof the data.\nRB Ccnt Ccry Csta Ccty Ccit CF CCcnt CCcry CCsta CCcty CCcit\nP-top10 10.28 11.75 11.1 5.70 5.70 5.70 11.22 10.74 10.47 5.89 5.89 5.89\nP-mid5k 1.33 1.75 2.25 2.43 1.46 1.96 4.47 4.59 4.51 3.56 1.96 2.56\nP-bottom22k 0.64 0.92 1.10 1.03 0.77 1.07 1.85 1.95 1.95 1.56 0.96 1.16\nP-G1 0.45 0.67 0.72 0.68 0.44 0.56 1.13 1.26 1.17 0.78 0.26 0.35\nP-G2 0.65 1.32 1.34 1.01 0.69 0.92 1.71 1.78 1.77 1.32 0.80 0.89\nP-G3 1.08 2.04 2.02 1.88 1.30 1.73 3.51 3.60 3.59 2.90 1.68 2.16\nD-G1 0.64 0.85 1.16 1.04 0.87 0.88 2.22 2.24 2.16 1.59 0.97 0.93\nD-G2 0.73 0.93 1.05 1.23 0.84 1.02 2.04 2.21 2.20 1.68 0.98 1.08\nD-G3 0.93 1.63 1.49 1.56 0.93 1.41 2.49 2.56 2.59 2.03 1.08 1.54\nM6-03-G1 0.50 0.88 0.95 0.96 0.64 0.88 1.76 1.84 1.84 1.43 0.81 1.00\nM6-03-G2 1.34 2.73 2.43 2.22 1.49 2.00 3.36 3.50 3.50 2.81 1.67 2.08\nM6-median-G1 0.35 0.58 0.62 0.65 0.48 0.61 1.35 1.46 1.45 1.04 0.56 0.66\nM6-median-G2 1.25 2.49 2.89 2.25 1.47 1.97 3.14 3.27 3.29 2.67 1.66 2.07\nM12-05-G1 1.35 2.02 2.27 2.25 1.50 1.93 2.90 3.02 3.04 2.47 1.54 1.94\nM12-05-G2 0.36 0.59 0.69 0.61 0.41 0.57 1.30 1.38 1.38 1.01 0.52 0.66\nM12-median-G1 0.36 0.62 0.71 0.64 0.43 0.59 1.41 1.50 1.50 1.10 0.56 0.71\nM12-median-G2 1.34 2.09 2.33 2.34 1.57 2.01 3.10 3.24 3.26 2.66 1.67 2.10\nTable 2. Maximum F-score for all combinations of methods and user sets. Crefers to the CULT approaches, CC to\nCFCULT; cntindicates continent, crycountry, stastate, ctycounty, and citcity. The best performing recommenders for\na given group are in bold.\nSecond, we divide users into 2 groups for which borders\nare deﬁned by a mainstreaminess of 0:3and0:5, respec-\ntively, for the 6month case and the 12month case (referred\nto as M6(12)-03(05)-G1(2)). These values were chosen\nby considering the histograms in Figure 2 and choosing\nvalues which naturally grouped users. For the diversity,\nwe create 3 groups according to the 0:33 and0:67 per-\ncentiles (referred to as D-G1(2,3)). For play counts, we\nconsider 2 different groupings. The ﬁrst is the same as\nfor diversity, i.e. dividing groups according to the 0:33\nand0:67 percentiles (referred to as P-G1(2,3)). The sec-\nond splits the users according to the accumulative play\ncounts into the following groups, each of which accounts\nfor approximately a third of all play counts: top 10 users,\nmid 5,000 users, bottom 22,000 users (referred to as P-\ntop10(mid5k,bottom22k)).\n4. RECOMMENDATION MODELS\nIn the considered music recommendation models, each user\nu2Uis represented by a list of artists listened to A(u).\nAll approaches determine for a given seed user ua num-\nberKof most similar neighbors VK(u), and recommend\nthe artists listened to by these VK(u), excluding the artistsD(u) M(u) (6 mo.) P(u)\nD(u) - 0.119 0.292\nM(u) (12 mo.) 0.069 0.837 0.013\nP(u) 0.292 0.021 -\nTable 3. Feature correlations. Note due to the symmetry of\nthese featuers, mainstreaminess is presented for 6 months\non one dimension and 12 months on another. Overall, none\nof the features are highly correlated other than the main-\nstreaminess 6 and 12 month features, which is expected.\nA(u)already known by u. The recommended artists R(u)\nfor user uare computed as R(u) =S\nv2VK(u)A(v)nA(u)\nandVK(u) = argmaxK\nv2Unfugsim(u; v ), where argmaxK\nv\ndenotes the Kusers vwith highest similarities to u. In con-\nsidering geographical information for user-context models,\nwe investigate the following approaches, which differ in\nthe way this similarity term sim(u; v )is computed. The\nfollowing approaches were investigated:\nCULT: In the cultural approach, we select the neighbors\nfor the seed user only according to a geographical similar-\nity computed by means of the Jaccard index on listening\ndistributions over semantic locations. We consider as such\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n48500.511.522.53\nx 104100101102103104105\nplay count# users\n0 5 10 15 20010002000300040005000\ndiversity# usersFigure 1. Histogram of (top) play counts (note the log\nscale on the y-axis) and (bottom) diversity over users.\nsemantic categories continent, country, state, county, and\ncity. For each user, we obtain the relevant locations by\ncomputing the relative frequencies of his listening events\nover all locations. To exclude the aforementioned geoen-\ntities that are unlikely to contribute to the user’s cultural\ncircle, we retain only locations at which the user has lis-\ntened to music with a frequency above his own average2.\nOn the corresponding listening vectors over locations of\ntwo users uandv, we compute the Jaccard index to obtain\nsim(u; v ). Depending on the location category user simi-\nlarities are computed on, we distinguish CULT continent,\nCULT country, CULT state, CULT county, and CULT city.\nCF: We also consider a user-based collaborative ﬁlter-\ning approach. Given the artist play counts of seed user\nuas a vector ~P(u)over all artists in the corpus, we ﬁrst\nomit the artists that occur in the test set (i.e. we set to 0the\nplay count values for artists we want our algorithm to pre-\ndict). We then normalize ~P(u)so that its Euclidean norm\nequals 1and compute similarities sim(u; v )as the inner\nproduct between ~P(u)and~P(v).\nCFCULT: This approach works by combining the CF\nsimilarity matrix with the CULT similarity matrix via point-\nwise multiplication, in order to incorporate both music pref-\nerence and cultural information.\nRB: For comparison, we implemented a random base-\nline model that randomly picks Kusers and recommends\n2This way we exclude, for instance, locations where the user might\nhave spent only a few days during vacation.\n00.2 0.4 0.6 0.8 10500100015002000\nmainstreaminess 6mo.# users\n00.2 0.4 0.6 0.8 10500100015002000\nmainstreaminess 12mo.# usersFigure 2. Histogram of mainstreaminess considering a\ntime interval of (top) 6months and (bottom) 12months.\nthe artists they listened to. The similarity function can thus\nbe considered sim(u; v ) =rand [0;1].\n5. EV ALUATION\n5.1 Experimental Setup\nFor experiments, we perform 10-fold cross validation on\nthe user level. For each user, we predict 10% of the artists\nbased on the remaining 90% used for training. We com-\npute precision, recall, and F-measure by averaging the re-\nsults over all folds per user and all users in the dataset. To\ncompare the performance between approaches, we use a\nparameter Nfor the number of recommended artists, and\nadapt dynamically the number of neighbors Kto be con-\nsidered for the seed user u. This is necessary since we do\nnot know how many artists should be predicted for a given\nuser (this number varies over users and approaches). To\ndetermine a suited value of Kfor a given recommenda-\ntion approach and a given N, we start the approach with\nK= 1and iteratively increase Kuntil the number of rec-\nommended artists equals or exceeds N. In the latter case,\nwe sort the returned artists according to their overall popu-\nlarity among the Kneighbors and recommend the top N.\n5.2 Results\nTable 2 depicts the maximum F-score (over all values of\nN) for each combination of user set and method. We de-\ncided to report the maximum F-scores, because recall and\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n48610010505101520\nplay countdiversity\n0 5 10 15 2000.20.40.60.81\ndiversitymainstreaminess 6Figure 3. Users plot as a function of (top) D(u)vsP(u)\nand (bottom) M(u)(6 months) vs D(u). Note the log scale\nforP(u)only. These ﬁgures illustrate the widespread,\neven distribution of users across the feature space.\nprecision show an inverse characteristics over N. Since\nthe F-score equals the harmonic mean of precision and\nrecall, it is less inﬂuenced by variations of N, neverthe-\nless aggregate performance in a meaningful way. We fur-\nther plot precision/recall-curves for several cases reported\nin Table 2. In Figure 4, we present the results of all of\nthe recommendation algorithms for one group on the play\ncounts. For this case, the CF approach with integrated con-\ntinent and country information performed best, followed\nby the CF approach. Predominantly, these three methods\noutperformed all of the other approaches for the various\ngroups, which is also apparent in Table 2. The only ex-\nception was the P-top10 case, where the CULT continent\napproach outperformed CF approaches. However, consid-\nering the small number of users in this subset (10), the dif-\nference of one percentage point between CULT continent\nand CF CULT continent is not signiﬁcant. We observe the\nCF approach with the addition of the continent and coun-\ntry information are very good recommenders in general for\nthe data we are using.\nNow we are interested to know how the recommenda-\ntions performed across user groups and respective features.\n01020304050607000.511.522.53\nrecallprecision\n  \nUS−P−G3−RB\nUS−P−G3−CULT_continent\nUS−P−G3−CULT_country\nUS−P−G3−CULT_state\nUS−P−G3−CULT_county\nUS−P−G3−CULT_city\nUS−P−G3−CF\nUS−P−G3−CF_CULT_continent\nUS−P−G3−CF_CULT_country\nUS−P−G3−CF_CULT_state\nUS−P−G3−CF_CULT_county\nUS−P−G3−CF_CULT_cityFigure 4. Recommendation performance of investigated\nmethods on user group P-G3.\nIn terms of play counts, we observe as the user has a larger\nnumber of events in the dataset, the performance increases\nsigniﬁcantly (P-G3 and P-top10). This can be explained by\nthe fact that more comprehensive user models can be cre-\nated for users about whom we know more, which in turn\nyields better recommendations.\nAlso in terms of diversity, there are performance dif-\nferences across groups given a particular recommender al-\ngorithm. Especially between the high diversity listeners\nD-G3 and low diversity listeners D-G1, results differ sub-\nstantially. This can be explained by the fact that it is eas-\nier to ﬁnd a considerable amount of like-minded users for\nseeds who have a diverse music taste, in technical terms,\nless sparse A(u)vector.\nWhen considering mainstreaminess, taking either a 6\nmonth or 12 month interval does not appear to have a sig-\nniﬁcant impact on recommendation performance. There\nare minor differences depending on the recommendation\nalgorithm. However, in general, the groups with larger\nmainstreaminess (M6-03-G2, M6-med-G2, M12-med-G2)\nalways performed much better for all approaches than the\ngroups with smaller mainstreaminess. It hence seems eas-\nier to satisfy users with a mainstream music taste than users\nwith diverging taste.\n6. CONCLUSIONS AND FUTURE WORK\nIn this paper, we consider the role of user listening be-\nhavior related to the history of listening events in order\nto evaluate how this may effect music recommendation,\nparticularly considering the direction of personalization.\nWe investigate three user characteristics, play count, main-\nstreaminess, and diversity, and form groups of users along\nthese dimensions. We evaluate several different recom-\nmendation algorithms, particularly collaborative ﬁltering\n(CF), and CF augmented by location information. We ﬁnd\nthe CF and CF approaches augmented by continent and\ncountry information about the listener to outperform the\nother methods. We also ﬁnd recommendation algorithms\nfor users with large play counts, higher diversity, and higher\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n48701020304050607000.511.522.53\nrecallprecision\n  \nUS−P−G1−RB\nUS−P−G1−CF\nUS−P−G1−CF_CULT_continent\nUS−P−G1−CF_CULT_country\nUS−P−G2−RB\nUS−P−G2−CF\nUS−P−G2−CF_CULT_continent\nUS−P−G2−CF_CULT_country\nUS−P−G3−RB\nUS−P−G3−CF\nUS−P−G3−CF_CULT_continent\nUS−P−G3−CF_CULT_country\n010 20 30 40 50 60 7000.20.40.60.811.21.41.61.82\nrecallprecision\n  \nUS−D−G1−RB\nUS−D−G1−CF\nUS−D−G1−CF_CULT_continent\nUS−D−G1−CF_CULT_country\nUS−D−G2−RB\nUS−D−G2−CF\nUS−D−G2−CF_CULT_continent\nUS−D−G2−CF_CULT_country\nUS−D−G3−RB\nUS−D−G3−CF\nUS−D−G3−CF_CULT_continent\nUS−D−G3−CF_CULT_country\n0102030405060708000.511.522.5\nrecallprecision\n  US−M12−median−G1−RB\nUS−M12−median−G1−CF\nUS−M12−median−G1−CF_CULT_continent\nUS−M12−median−G1−CF_CULT_country\nUS−M12−median−G2−RB\nUS−M12−median−G2−CF\nUS−M12−median−G2−CF_CULT_continent\nUS−M12−median−G2−CF_CULT_countryFigure 5. Precision vs. recall for play count (top), diver-\nsity (middle), and mainstreaminess with a 12 month inter-\nval (bottom) experiments over groups and various recom-\nmendation approaches.mainstreaminess have better performance.\nAs part of future work, we will investigate content-based\nmusic recommendation models as well as combinations of\ncontent-based, CF-based, and location-based models. Ad-\nditional characteristics of the user, such as age, gender, or\nmusical education, will be addressed, too.\n7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Funds\n(FWF): P22856 and P25655, and by the EU FP7 project\nno. 601166 (“PHENICX”).\n8. REFERENCES\n[1] G. Adomavicius and A. Tuzhilin. Toward the next generation\nof recommender systems: A survey of the state-of-the-art and\npossible extensions. IEEE Transactions on Knowledge and\nData Engineering, 17(6):734–749, 2005.\n[2] L. Chen, W. Wu, and L. He. How personality inﬂuences\nusers’ needs for recommendation diversity? CHI ’13 Ex-\ntended Abstracts on Human Factors in Computing Systems\non - CHI EA ’13, 2013.\n[3] N. Hariri, B. Mobasher, and R. Burke. Context-aware music\nrecommendation based on latent topic sequential patterns. In\nProc. ACM RecSys ’12, New York, NY , USA, 2012.\n[4] M. Hart. The long tail: Why the future of business is selling\nless of more by chris anderson. Journal of Product Innovation\nManagement, 24(3):274–276, 2007.\n[5] D. Hauger, M. Schedl, A. Ko ˇsir, and M. Tkal ˇciˇc. The Mil-\nlion Musical Tweets Dataset: What Can We Learn From Mi-\ncroblogs. In Proc. ISMIR, Curitiba, Brazil, November 2013.\n[6] R. Hu and P. Pu. Exploring Relations between Personality\nand User Rating Behaviors. 1st Workshop on Emotions and\nPersonality in Personalized Services (EMPIRE), June 2013.\n[7] N. Hurley and M. Zhang. Novelty and diversity in top-n rec-\nommendation – analysis and evaluation. ACM Trans. Internet\nTechnol., 10(4):14:1–14:30, March 2011.\n[8] J. Konstan and J. Riedl. Recommender systems: from algo-\nrithms to user experience. User Modeling and User-Adapted\nInteraction, 22(1-2):101–123, March 2012.\n[9] R. McCrae and O. John. An Introduction to the Five-\nFactor Model and its Applications. Journal of Personality,\n60(2):175–215, 1992.\n[10] E. Pariser. The ﬁlter bubble: What the Internet is hiding from\nyou. Penguin UK, 2011.\n[11] P. Rentfrow and S. Gosling. The do re mi’s of everyday life:\nThe structure and personality correlates of music preferences.\nJournal of Personality and Social Psychology, 84(6):1236–\n1256, 2003.\n[12] M. Schedl and D. Schnitzer. Hybrid Retrieval Approaches to\nGeospatial Music Recommendation. In Proc. ACM SIGIR,\nDublin, Ireland, July–August 2013.\n[13] M. Tkal ˇciˇc, M. Kunaver, A. Ko ˇsir, and J. Tasi ˇc. Addressing\nthe new user problem with a personality based user similarity\nmeasure. Joint Proc. DEMRA and UMMS, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n488"
    },
    {
        "title": "On Inter-rater Agreement in Audio Music Similarity.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416970",
        "url": "https://doi.org/10.5281/zenodo.1416970",
        "ee": "https://zenodo.org/records/1416970/files/Flexer14.pdf",
        "abstract": "One of the central tasks in the annual MIREX evaluation campaign is the ”Audio Music Similarity and Retrieval (AMS)” task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judg- ment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of per- formance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algo- rithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity.",
        "zenodo_id": 1416970,
        "dblp_key": "conf/ismir/Flexer14",
        "keywords": [
            "Audio Music Similarity and Retrieval (AMS)",
            "MIREX evaluation campaign",
            "Human graders",
            "Subjective judg- ment",
            "Inter-rater agreement",
            "Performance upper bound",
            "Participating algorithms",
            "2009",
            "Upper bound achievement",
            "Future evaluations"
        ],
        "content": "ON INTER-RATER AGREEMENT IN AUDIO MUSIC SIMILARITY\nArthur Flexer\nAustrian Research Institute for Artiﬁcial Intelligence (OFAI)\nFreyung 6/6, Vienna, Austria\narthur.flexer@ofai.at\nABSTRACT\nOne of the central tasks in the annual MIREX evaluation\ncampaign is the ”Audio Music Similarity and Retrieval\n(AMS)” task. Songs which are ranked as being highly\nsimilar by algorithms are evaluated by human graders as\nto how similar they are according to their subjective judg-\nment. By analyzing results from the AMS tasks of the\nyears 2006 to 2013 we demonstrate that: (i) due to low\ninter-rater agreement there exists an upper bound of per-\nformance in terms of subjective gradings; (ii) this upper\nbound has already been achieved by participating algo-\nrithms in 2009 and not been surpassed since then. Based\non this sobering result we discuss ways to improve future\nevaluations of audio music similarity.\n1. INTRODUCTION\nProbably the most important concept in Music Information\nRetrieval (MIR) is that of music similarity. Proper model-\ning of music similarity is at the heart of every application\nallowing automatic organization and processing of music\ndatabases. Consequently, the ”Audio Music Similarity and\nRetrieval (AMS)” task has been part of the annual ”Music\nInformation Retrieval Evaluation eXchange” (MIREX1)\n[2] since 2006. MIREX is an annual evaluation campaign\nfor MIR algorithms allowing for a fair comparison in stan-\ndardized settings in a range of different tasks. As such\nit has been of great value for the MIR community and an\nimportant driving force of research and progress within the\ncommunity. The essence of the AMS task is to have human\ngraders evaluate pairs of query/candidate songs. The query\nsongs are randomly chosen from a test database and the\ncandidate songs are recommendations automatically com-\nputed by participating algorithms. The human graders rate\nwhether these query/candidate pairs ”sound similar” using\nboth a BROAD (”not similar”, ”somewhat similar”, ”very\nsimilar”) and a FINE score (from 0 to 10 or from 0 to 100,\ndepending on the year the AMS task was held, indicating\ndegrees of similarity ranging from failure to perfection).\n1http://www.music-ir.org/mirex\nc\rArthur Flexer.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Arthur Flexer. “On inter-rater agree-\nment in audio music similarity”, 15th International Society for Music In-\nformation Retrieval Conference, 2014.It is precisely this general notion of ”sounding simi-\nlar” which is the central point of criticism in this paper.\nA recent survey article on the ”neglected user in music\ninformation retrieval research” [13] has made the impor-\ntant argument that users apply very different, individual\nnotions of similarity when assessing the output of music\nretrieval systems. It seems evident that music similarity\nis a multi-dimensional notion including timbre, melody,\nharmony, tempo, rhythm, lyrics, mood, etc. Nevertheless\nmost studies exploring music similarity within the ﬁeld\nof MIR, which are actually using human listening tests,\nare restricted to overall similarity judgments (see e.g. [10]\nor [11, p. 82]) thereby potentially blurring the many im-\nportant dimensions of musical expression. There is very\nlittle work on what actually are important dimensions for\nhumans when judging music similarity (see e.g. [19]).\nThis paper therefore presents a meta analysis of all\nMIREX AMS tasks from 2006 to 2013 thereby demon-\nstrating that: (i) there is a low inter-rater agreement due\nto the coarse concept of music similarity; (ii) as a conse-\nquence there exists an upper bound of performance that can\nbe achieved by algorithmic approaches to music similarity;\n(iii) this upper bound has already been achieved years ago\nand not surpassed since then. Our analysis is concluded by\nmaking recommendations on how to improve future work\non evaluating audio music similarity.\n2. RELATED WORK\nIn our review on related work we focus on papers directly\ndiscussing results of the AMS task thereby adressing the\nproblem of evaluation of audio music similarity.\nAfter the ﬁrst implementation of the AMS task in 2006,\na meta evaluation of what has been achieved has been\npublished [8]. Contrary to all subsequent editions of the\nAMS task, in 2006 each query/candidate pair was evalu-\nated by three different human graders. Most of the study\nis concerned with the inter-rater agreement of the BROAD\nscores of the AMS task as well as the ”Symbolic Melodic\nSimilarity (SMS)” task, which followed the same evalu-\nation protocol. To access the amount of agreement, the\nauthors use Fleiss’s Kappa [4] which ranges between 0\n(no agreement) and 1 (perfect agreement). Raters in the\nAMS task achieved a Kappa of 0.21 for the BROAD task,\nwhich can be seen as a ”fair” level of agreement. Such\na ”fair” level of agreement [9] is given if the Kappa re-\nsult is between 0.21 and 0.40, therefore positioning the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n245BROAD result at the very low end of the range. Agree-\nment in SMS is higher (Kappa of 0.37), which is attributed\nto the fact that the AMS task is ”less well-deﬁned” since\ngraders are only informed that ”works should sound sim-\nilar” [8]. The authors also note that the FINE scores for\nquery/candidate pairs, which have been judged as ”some-\nwhat similar”, show more variance then the one judged as\n”very” or ”not” similar. One of the recommendations of\nthe authors is that ”evaluating more queries and more can-\ndidates per query would more greatly beneﬁt algorithm de-\nvelopers” [8], but also that a similar analysis of the FINE\nscores is also necessary.\nFor the AMS task 2006, the distribution of differ-\nences between FINE scores of raters judging the same\nquery/candidate pair has already been analysed [13]. For\nover 50%, the difference between rater FINE scores is\nlarger than 20. The authors also note that this is very prob-\nlematic since the difference between the best and worst\nAMS 2012 systems was just 17.\nIn yet another analysis of the AMS task 2006, it has\nbeen reported [20] that a range of so-called ”objective”\nmeasures of audio similarity are highly correlated with\nsubjective ratings by human graders. These objective mea-\nsures are based on genre information, which can be used\nto automatically rank different algorithms producing lists\nof supposedly similar songs. If the genre information of\nthe query and candidate songs are the same, a high degree\nof audio similarity is achieved since songs within a genre\nare supposed to be more similar than songs from differ-\nent genres. It has therefore been argued that, at least for\nlarge-scale evaluations, these objective measures can re-\nplace human evaluation [20]. However, this is still a mat-\nter of controversy within the music information retrieval\ncommunity, see e.g. [16] for a recent and very outspoken\ncriticism of this position.\nA meta study of the 2011 AMS task explored the con-\nnection between statistical signiﬁcance of reported results\nand how this relates to actual user satisfaction in a more\nrealistic music recommendation setting [17]. The authors\nmade the fundamental clariﬁcation that the fact of ob-\nserving statistically signiﬁcant differences is not sufﬁcient.\nMore important is whether this difference is noticeable\nand important to actual users interacting with the systems.\nWhereas a statistically signiﬁcant difference can alway be\nachieved by enlarging the sample size (i.e. the number of\nquery/candidate pairs), the observed difference can nev-\nertheless be so small that it is of no importance to users.\nThrough a crowd-sourced user evaluation, the authors are\nable to show that there exists an upper bound of user satis-\nfaction with music recommendation systems of about 80%.\nMore concretely, in their user evaluation the highest per-\ncentage of users agreeing that two systems ”are equally\ngood” never exceeded 80%. This upper bound cannot be\nsurpassed since there will always be users that disagree\nconcerning the quality of music recommendations. In ad-\ndition the authors are able to demonstrate that differences\nin FINE scores, which are statistically signiﬁcant, are so\nsmall that they make no practical difference for users.3. DATA\nFor our meta analysis of audio music similarity (AMS)\nwe use the data from the “Audio Music Similarity and\nRetrieval” tasks from 2006 to 20132within the annual\nMIREX [2] evaluation campaign for MIR algorithms.\nFor the AMS 2006 task, 5000 songs were chosen from\nthe so-called ”uspop”, ”uscrap” and ”cover song” collec-\ntions. Each of the participating 6 system then returned\na 5000x5000 AMS distance matrix. From the complete\nset of 5000 songs, 60 songs were randomly selected as\nqueries and the ﬁrst 5 most highly ranked songs out of\nthe 5000 were extracted for each query and each of the\n6 systems (according to the respective distance matrices).\nThese 5 most highly ranked songs were always obtained\nafter ﬁltering out the query itself, results from the same\nartist (i.e. a so-called artist ﬁler was employed [5]) and\nmembers of the cover song collection (since this was es-\nsentially a separate task run together with the AMS task).\nThe distribution for the 60 chosen random songs is highly\nskewed towards rock music: 22 ROCK songs, 6 JAZZ, 6\nRAP&HIPHOP, 5 ELECTRONICA&DANCE, 5 R&B, 4\nREGGAE, 4 COUNTRY , 4 LATIN, 4 NEWAGE. Unfor-\ntunately the distribution of genres across the 5000 songs\nis not available, but there is some information concern-\ning the ”excessively skewed distribution of examples in\nthe database (roughly 50% of examples are labeled as\nRock/Pop, while a further 25% are Rap & Hip-Hop)”3.\nFor each query song, the returned results (candidates) from\nall participating systems were evaluated by human graders.\nFor each individual query/candidate pair, three different\nhuman graders provided both a FINE score (from 0 (fail-\nure) to 10 (perfection)) and a BROAD score (not simi-\nlar, somewhat similar, very similar) indicating how sim-\nilar the songs are in their opinion. This altogether gives\n6\u000260\u00025\u00023 = 5400 human FINE and BROAD gradings.\nPlease note that since some of the query/candidate pairs are\nidentical for some algorithms (i.e. different algorithms re-\nturned identical candidates) and since such identical pairs\nwere not graded repeatedly, the actual number of different\nFINE and BROAD gradings is somewhat smaller.\nStarting with the AMS task 2007, a number of small\nchanges to the overall procedure was introduced. Each\nparticipating algorithm was given 7000 songs chosen from\nthe ”uspop”, ”uscrap” and ”american” ”classical” and\n”sundry” collections. Therefore there is only a partial over-\nlap in music collections (”uspop” and ”uscrap”) compared\nto AMS 2006. From now on 30 second clips instead of\nthe full songs were being used both as input to the algo-\nrithms and as listening material for the human graders. For\nthe subjective evaluation of music similarity, from now\non 100 query songs were randomly chosen representing\nthe 10 genres found in the database (i.e., 10 queries per\ngenre). The whole database consists of songs from equally\nsized genre groups: BAROQUE, COUNTRY , EDANCE,\n2The results and details can be found at:\nhttp://www.music-ir.org/mirex/wiki/MIREX HOME\n3This is stated in the 2006 MIREX AMS results:\nhttp://www.music-ir.org/mirex/wiki/2006:\nAudio Music Similarity andRetrieval Results\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n246JAZZ, METAL, RAPHIPHOP, ROCKROLL, ROMAN-\nTIC, BLUES, CLASSICAL. Therefore there is only a par-\ntial overlap of genres compared to AMS 2006 (COUNTRY ,\nEDANCE, JAZZ, RAPHIPHOP, ROCKROLL). As with\nAMS 2006, the 5 most highly ranked songs were then re-\nturned per query as candidates (after ﬁltering for the query\nsong and songs from the same artist). For AMS tasks 2012\nand 2013, 50 instead of 100 query songs were chosen and\n10 instead of 5 most highly ranked songs returned as can-\ndidates.\nProbably the one most important change to the\nAMS 2006 task is the fact that from now on every\nquery/candidate pair was only being evaluated by a single\nuser. Therefore the degree of inter-rater agreement cannot\nbe analysed anymore. For every AMS task, the subjective\nevaluation therefore results in a\u0002100\u00025human FINE\nand BROAD gradings, with abeing the number of partic-\nipating algorithms, 100 the number of query songs and 5\nthe number of candidate songs. For AMS 2012 and 2013\nthis changed to a\u000250\u000210, which yields the same over-\nall number. These changes are documented on the respec-\ntive MIREX websites, but also in a MIREX review article\ncovering all tasks of the campaign [3]. For AMS 2007 and\n2009, the FINE scores range from 0 to 10, from AMS 2010\nonwards from 0 to 100. There was no AMS task in MIREX\n2008.\n4. RESULTS\nIn our meta analysis of the AMS tasks from years 2006\nto 2013, we will focus on the FINE scores of the subjec-\ntive evaluation conducted by the human graders. The rea-\nson is that the FINE scores provide more information than\nthe BROAD scores which only allow for three categorical\nvalues. It has also been customary for the presentation of\nAMS results to mainly compare average FINE scores for\nthe participating algorithms.\n4.1 Analysis of inter-rater agreement\nOur ﬁrst analysis is concerned with the degree of inter-\nrater agreement achieved in the AMS task 2006, which is\nthe only year every query/candidate pair has been evalu-\nated by three different human graders. Previous analysis\nof AMS results has concentrated on BROAD scores and\nused Fleiss’s Kappa as a measure of agreement (see Sec-\ntion 2). Since the Kappa measure is only deﬁned for the\ncategorical scale, we use the Pearson correlation \u001abetween\nFINE scores of pairs of graders. As can be seen in Table 1,\nthe average correlations range from 0:37 to0:43. Taking\nthe square of the observed values of \u001a, we can see that\nonly about 14to18percent of the variance of FINE scores\nobserved in one grader can be explained by the values ob-\nserved for the respective other grader (see e.g. [1] on \u001a2\nmeasures). Therefore, this is the ﬁrst indication that agree-\nment between raters in the AMS task is rather low.\nNext we plotted the average FINE score of a rater i\nfor all query/candidate pairs, which he or she rated within\na certain interval of FINE scores v, versus the averagegrader1 grader2 grader3\ngrader1 1.00 0.43 0.37\ngrader2 1.00 0.40\ngrader3 1.00\nTable 1. Correlation of FINE scores between pairs of hu-\nman graders.\ngrader1 grader2 grader3\ngrader1 9.57 6.66 5.99\ngrader2 6.60 9.55 6.67\ngrader3 6.62 6.87 9.69\nTable 2. Pairwise inter-rater agreement for FINE scores\nfrom interval v= [9;10].\nFINE scores achieved by the other two raters j6=ifor\nthe same query/candidate pairs. We therefore explore how\nhuman graders rate pairs of songs which another human\ngrader rated at a speciﬁc level of similarity. The average\nresults across all raters and for intervals vranging from\n[0;1);[1;2)::: to[9;10]are plotted in Figure 1. It is evi-\ndent that there is a considerable deviation from the theoret-\nical perfect agreement which is indicated as a dashed line.\nPairs of query/candidate songs which are rated as being\nvery similar (FINE score between 9 and 10) by one grader\nare on average only rated at around 6:5by the two other\nraters. On the other end of the spectrum, query/candidate\npairs rated as being not similar at all (FINE score between\n0 and 1) receive average FINE scores of almost 3 by the re-\nspective other raters. The degree of inter-rater agreement\nfor pairs of raters at the interval v= [9; 10]is given in\nTable 2. There are 333 pairs of songs which have been\nrated within this interval. The main diagonal gives the av-\nerage rating one grader gave to pairs of songs in the inter-\nvalv= [9; 10]. The off-diagonal entries show the level\nof agreement between different raters. As an example,\nquery/candidate pairs that have been rated between 9 and\n10 by grader1 have received an average rating of 6:66 by\ngrader2. The average of these pairwise inter-rater agree-\nments given in Table 2 is 6:54 and is an upper bound for\nthe average FINE scores of the AMS task 2006. This up-\nper bound is the maximum of average FINE scores that can\nbe achieved within such an evaluation setting. This upper\nbound is due to the fact that there is a considerable lack\nof agreement between human graders. What sounds very\nsimilar to one of the graders will on average not receive\nequally high scores by other graders.\nThe average FINE score achieved by the best partici-\npating system in AMS 2006 (algorithm EP) is 4:30\u00068:8\n(mean\u0006variance). The average upper bound inter-rater\ngrading is 6:54\u00066:96. The difference between the best\nFINE scores achieved by the system EP and the upper\nbound is signiﬁcant according to a t-test:jtj=j\u0000\n12:0612j> t 95;df =1231 = 1:96(conﬁdence level of 95%,\ndegrees of freedom = 1231). We can therefore conclude\nthat for the AMS 2006 task, the upper bound on the av-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2470 1 2 3 4 5 6 7 8 9 10012345678910\naverage FINE score of rater iaverage FINE score of raters j ≠ iFigure 1. Average FINE score inter-rater agreement for\ndifferent intervals of FINE scores (solid line). Dashed line\nindicates theoretical perfect agreement.\nerage FINE score had not yet been reached and that there\nstill was room for improvement for future editions of the\nAMS task.\n4.2 Comparison to the upper bound\nWe will now compare the performance of the respective\nbest participating systems in AMS 2007, 2009 to 2013\nto the upper bound of average FINE scores we have re-\ntrieved in Section 4.1. This upper bound that can possibly\nbe achieved due to the low inter-rater agreement results\nfrom the analysis of the AMS 2006 task. Although the\nwhole evaluation protocol in all AMS tasks over the years\nis almost identical, AMS 2006 did use a song database that\nis only overlapping with that of subsequent years. It is\ntherefore of course debatable how strictly the upper bound\nfrom AMS 2006 applies to the AMS results of later years.\nAs outlined in Section 3, AMS 2006 has a genre distribu-\ntion that is skewed to about 50% of rock music whereas\nall other AMS databases consist of equal amounts of songs\nfrom 10 genres. One could make the argument that in gen-\neral songs from the same genre are being rated as being\nmore similar than songs from different genres. As a conse-\nquence, agreement of raters for query/candidate pairs from\nidentical genres might also be higher. Therefore inter-\nrater agreement within such a more homogeneous database\nshould be higher than in a more diverse database and it can\nbe expected that an upper bound of inter-rater agreement\nfor AMS 2007 to 2013 is even lower than the one we ob-\ntained in Section 4.1. Of course this line of argument is\nsomewhat speculative and needs to be further investigated.\nIn Figure 2 we have plotted the average FINE score of\nthe highest performing participants of AMS tasks 2007,\n2009 to 2013. These highest performing participants are\nthe ones that achieved the highest average FINE scores in\nthe respective years. In terms of statistical signiﬁcance, the\nperformance of these top algorithms is often at the same\nlevel as a number of other systems. We have also plotted\n2007 2009 2010 2011 2012 20130102030405060708090100\nyearaverage FINE scoreFigure 2. Average FINE score of best performing system\n(y-axis) vs. year (x-axis) plotted as solid line. Upper bound\nplus conﬁdence interval plotted as dashed line.\nyear system mean var t\n2007 PS 56.75 848.09 -4.3475\n2009 PS2 64.58 633.76 -0.4415\n2010 SSPK2 56.64 726.78 -4.6230\n2011 SSPK2 58.64 687.91 -3.6248\n2012 SSKS2 53.19 783.44 -6.3018\n2013 SS2 55.21 692.23 -5.4604\nTable 3. Comparison of best system vs. upper bound due\nto lack of inter-rater agreement.\nthe upper bound (dashed line) and a 95% conﬁdence inter-\nval (dot-dashed lines). As can be seen the performance\npeaked in the year 2009 where the average FINE score\nreached the conﬁdence interval. Average FINE scores in\nall other years are always a little lower. In Table 3 we show\nthe results of a number of t-tests always comparing the per-\nformance to the upper bound. Table 3 gives the AMS year,\nthe abbreviated name of the winning entry, the mean per-\nformance, its variance and the resulting t-value (with 831\ndegrees of freedom and 95% conﬁdence). Only the best\nentry from year 2009 (PS2) reaches the performance of the\nupper bound, the best entries from all other years are sta-\ntistically signiﬁcant below the upper bound (critical value\nfor all t-tests is again 1:96).\nInterestingly, this system PS2 which gave the peak per-\nformance of all AMS years has also participated in 2010 to\n2013. In terms of statistical signiﬁcance (as measured via\nFriedman tests as part of the MIREX evaluation), PS2 has\nperformed on the same level with the top systems of all fol-\nlowing years. The systems PS2 has been submitted by Tim\nPohle and Dominik Schnitzer and essentially consists of a\ntimbre and a rhythm component [12]. Its main ingredients\nare MFCCs modeled via single Gaussians and Fluctuation\npatterns. It also uses the so-called P-norm normalization\nof distance spaces for combination of timbre and rhythm\nand to reduce the effect of hubness (anormal behavior of\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n248distance spaces due to high dimensionality, see [6] for a\ndiscussion related to the AMS task and [14] on re-scaling\nof distance spaces to avoid these effects).\nAs outlined in Section 3, from 2007 on the same\ndatabase of songs was used for the AMS tasks. However,\neach year a different set of 100 or 50 songs was chosen for\nthe human listening tests. This fact can explain that the one\nalgorithm participating from 2009 to 2013 did not always\nperform at the exact same level. After all, not only the\nchoice of different human graders is a source of variance\nin the obtained FINE scores, but also the choice of differ-\nent song material. However, the fact that the one algorithm\nthat reached the upper bound has so far not been outper-\nformed adds additional evidence that the upper bound that\nwe obtained indeed is valid.\n5. DISCUSSION\nOur meta analysis of all editions of the MIREX ”Audio\nMusic Similarity and Retrieval” tasks conducted so far has\nproduced somewhat sobering results. Due to the lack of\ninter-rater agreement there exists an upper bound of perfor-\nmance in subjective evaluation of music similarity. Such\nan upper bound will always exist when a number of differ-\nent people have to agree on a concept as complex as that of\nmusic similarity. The fact that in the MIREX AMS task the\nnotion of similarity is not deﬁned very clearly adds to this\ngeneral problem. After all, to ”sound similar” does mean\nsomething quite different to different people listening to\ndiverse music. As a consequence, an algorithm that has\nreached this upper bound of performance already in 2009\nhas not been outperformed ever since. Following our ar-\ngumentation, this algorithm cannot be outperformed since\nany additional performance will be lost in the variance of\nthe different human graders.\nWe now like to discuss a number of recommendations\nfor future editions of the AMS task. One possibility is to\ngo back to the procedure of AMS 2006 and again have\nmore than one grader rate the same query/candidate pairs.\nThis would allow to always also quantify the degree of\ninter-rater agreement and obtain upper bounds speciﬁc\nto the respective test songs. As we have argued above,\nwe believe that the upper bound we obtained for AMS\n2006 is valid for all AMS tasks. Therefore obtaining spe-\nciﬁc upper bounds would make much more sense if future\nAMS tasks would use an entirely different database of mu-\nsic. Such a change of song material would be a healthy\nchoice in any case. Re-introducing multiple ratings per\nquery/candidate pair would of course multiply the work\nload and effort if the number of song pairs to be evaluated\nshould stay the same. However, using so-called ”minimal\ntest collections”-algorithms allows to obtain accurate esti-\nmates on much reduced numbers of query/candidate pairs\nas has already been demonstrated for the AMS task [18]. In\naddition rater-speciﬁc normalization should be explored.\nWhile some human graders use the full range of available\nFINE scores when grading similarity of song pairs, others\nmight e.g. never rate song pairs as being very similar or\nnot similar at all, thereby staying away from the extremesof the scale. Such differences in rating style could add even\nmore variance to the overall task and should therefore be\ntaken care of via normalization.\nHowever, all this would still not change the fundamen-\ntal problem that the concept of music similarity is for-\nmulated in such a diffuse way that high inter-rater agree-\nment cannot be expected. Therefore, it is probably neces-\nsary to research what the concept of music similarity ac-\ntually means to human listeners. Such an exploration of\nwhat perceptual qualities are relevant to human listeners\nhas already been conducted in the MIR community for the\nspeciﬁc case of textural sounds [7]. Textural sounds are\nsounds that appear stationary as opposed to evolving over\ntime and are therefore much simpler and constrained than\nreal songs. By conducting mixed qualitative-quantitative\ninterviews the authors were able to show that qualities like\n”high-low”, ”smooth-coarse” or ”tonal-noisy” are impor-\ntant to humans discerning textural sounds. A similar ap-\nproach could be explored for real song material, probably\nstarting with a limited subset of genres. After such percep-\ntual qualities have then been identiﬁed, future AMS tasks\ncould ask human graders how similar pairs of songs are\naccording to a speciﬁc quality of the music. Such qualities\nmight not necessarily be straight forward musical concepts\nlike melody, rhythm, or tempo, but rather more abstract\nnotions like instrumentation, genre or speciﬁc recording\neffects signifying a certain style. Such a more ﬁne-grained\napproach to music similarity would hopefully raise inter-\nrater agreement and make more room for improvements in\nmodeling music similarity.\nLast but not least it has been noted repeatedly that evalu-\nation of abstract music similarity detached from a speciﬁc\nuser scenario and corresponding user needs might not be\nmeaningful at all [13]. Instead the MIR community might\nhave to change to evaluation of complete music retrieval\nsystems, thereby opening a whole new chapter for MIR\nresearch. Such an evaluation of a complete real life MIR\nsystem could center around a speciﬁc task for the users\n(e.g. building a playlist or ﬁnding speciﬁc music) thereby\nmaking the goal of the evaluation much clearer. Inciden-\ntally, this has already been named as one of the grand chal-\nlenges for future MIR research [15]. And even more im-\nportantly, exactly such a user centered evaluation will hap-\npen at this year’s tenth MIREX anniversary: the ”MIREX\nGrand Challenge 2014: User Experience (GC14UX)”4.\nThe task for participating teams is to create a web-based\ninterface that supports users looking for background music\nfor a short video. Systems will be rated by human evalua-\ntors on a number of important criteria with respect to user\nexperience.\n6. CONCLUSION\nIn our paper we have raised the important issue of the lack\nof inter-rater agreement in human evaluation of music in-\nformation retrieval systems. Since human appraisal of phe-\nnomena as complex and multi-dimensional as music sim-\n4http://www.music-ir.org/mirex/wiki/2014:GC14UX\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n249ilarity is highly subjective and depends on many factors\nsuch as personal preferences and past experiences, evalua-\ntion based on human judgments naturally shows high vari-\nance across subjects. This lack of inter-rater agreement\npresents a natural upper bound for performance of auto-\nmatic analysis systems. We have demonstrated and anal-\nysed this problem in the context of the MIREX ”Audio\nMusic Similarity and Retrieval” task, but any evaluation of\nMIR systems that is based on ground truth annotated by\nhumans has the same fundamental problem. Other exam-\nples from the MIREX campaign include such diverse tasks\nas ”Structural Segmentation”, ”Symbolic Melodic Simi-\nlarity” or ”Audio Classiﬁcation”, which are all based on\nhuman annotations of varying degrees of ambiguity. Fu-\nture research should explore upper bounds of performance\nfor these many other MIR tasks based on human annotated\ndata.\n7. ACKNOWLEDGEMENTS\nWe would like to thank all the spiffy people who have made\nthe MIREX evaluation campaign possible over the last ten\nyears, including of course J. Stephen Downie and his peo-\nple at IMIRSEL. This work was supported by the Austrian\nScience Fund (FWF, grants P27082 and Z159).\n8. REFERENCES\n[1] Cohen J.: Statistical power analysis for the behav-\nioral sciences, L. Erlbaum Associates, Second Edition,\n1988.\n[2] Downie J.S.: The Music Information Retrieval Eval-\nuation eXchange (MIREX), D-Lib Magazine, V olume\n12, Number 12, 2006.\n[3] Downie J.S., Ehmann A.F., Bay M., Jones M.C.:\nThe music information retrieval evaluation exchange:\nSome observations and insights, in Advances in music\ninformation retrieval, pp. 93-115, Springer Berlin Hei-\ndelberg, 2010.\n[4] Fleiss J.L.: Measuring nominal scale agreement among\nmany raters, Psychological Bulletin, V ol. 76(5), pp.\n378-382, 1971.\n[5] Flexer A., Schnitzer D.: Effects of Album and Artist\nFilters in Audio Similarity Computed for Very Large\nMusic Databases, Computer Music Journal, V olume\n34, Number 3, pp. 20-28, 2010.\n[6] Flexer A., Schnitzer D., Schl ¨uter J.: A MIREX meta-\nanalysis of hubness in audio music similarity, Proceed-\nings of the 13th International Society for Music Infor-\nmation Retrieval Conference (ISMIR’12), 2012.\n[7] Grill T., Flexer A., Cunningham S.: Identiﬁcation of\nperceptual qualities in textural sounds using the reper-\ntory grid method, in Proceedings of the 6th Audio\nMostly Conference, Coimbra, Portugal, 2011.[8] Jones M.C., Downie J.S., Ehmann A.F.: Human Simi-\nlarity Judgments: Implications for the Design of For-\nmal Evaluations, in Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’07), pp. 539-542, 2007.\n[9] Landis J.R., Koch G.G.: The measurement of observer\nagreement for categorical data, Biometrics, V ol. 33, pp.\n159174, 1977.\n[10] Novello A., McKinney M.F., Kohlrausch A.: Percep-\ntual Evaluation of Music Similarity, Proceedings of\nthe 7th International Conference on Music Information\nRetrieval (ISMIR 2006), Victoria, Canada, 2006.\n[11] Pampalk E.: Computational Models of Music Sim-\nilarity and their Application to Music Information\nRetrieval, Vienna University of Technology, Austria,\nDoctoral Thesis, 2006.\n[12] Pohle T., Schnitzer D., Schedl M., Knees P., Widmer\nG.: On Rhythm and General Music Similarity, Pro-\nceedings of the 10th International Society for Music\nInformation Retrieval Conference (ISMIR09), 2009.\n[13] Schedl M., Flexer A., Urbano J.: The neglected user in\nmusic information retrieval research, Journal of Intelli-\ngent Information Systems, 41(3), pp. 523-539, 2013.\n[14] Schnitzer D., Flexer A., Schedl M., Widmer G.: Lo-\ncal and Global Scaling Reduce Hubs in Space, Journal\nof Machine Learning Research, 13(Oct):2871-2902,\n2012.\n[15] Serra X., Magas M., Benetos E., Chudy M., Dixon S.,\nFlexer A., Gomez E., Gouyon F., Herrera P., Jorda S.,\nPaytuvi O., Peeters G., Schl ¨uter J., Vinet H., Widmer\nG., Roadmap for Music Information ReSearch, Peeters\nG. (editor), 2013.\n[16] Sturm B.L.: Classiﬁcation accuracy is not enough,\nJournal of Intelligent Information Systems, 41(3), pp.\n371-406, 2013.\n[17] Urbano J., Downie J.S., McFee B., Schedl M.: How\nSigniﬁcant is Statistically Signiﬁcant? The case of Au-\ndio Music Similarity and Retrieval, in Proceedings of\nthe 13th International Society for Music Information\nRetrieval Conference (ISMIR’12), pp. 181-186, 2012.\n[18] Urbano J., Schedl M.: Minimal test collections for low-\ncost evaluation of audio music similarity and retrieval\nsystems, International Journal of Multimedia Informa-\ntion Retrieval, 2(1), pp. 59-70, 2013.\n[19] Vignoli F.: Digital Music Interaction Concepts: A\nUser Study, Proceedings of the 5th International Con-\nference on Music Information Retrieval (ISMIR’04),\nBarcelona, Spain, 2004.\n[20] West K.: Novel techniques for audio music classiﬁca-\ntion and search, PhD thesis, University of East Anglia,\n2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n250"
    },
    {
        "title": "Automatic Instrument Classification of Ethnomusicological Audio Recordings.",
        "author": [
            "Dominique Fourer",
            "Jean-Luc Rouas",
            "Pierre Hanna",
            "Matthias Robine"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417655",
        "url": "https://doi.org/10.5281/zenodo.1417655",
        "ee": "https://zenodo.org/records/1417655/files/FourerRHR14.pdf",
        "abstract": "Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of in- terest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approa- ches which require qualitative and diversified training data- bases to obtain accurate results. In this paper, we intro- duce a diversified database composed of worldwide non- western instruments audio recordings on which is evalu- ated an effective timbre classification method. A compar- ative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicologi- cal indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis.",
        "zenodo_id": 1417655,
        "dblp_key": "conf/ismir/FourerRHR14",
        "keywords": [
            "Automatic timbre characterization",
            "audio signals",
            "measuring similarities",
            "machine learning approaches",
            "training data-bases",
            "effective timbre classification",
            "worldwide non-western instruments",
            "evaluation",
            "Iowa musical instruments database",
            "practical solution"
        ],
        "content": "AUTOMATIC TIMBRE CLASSIFICATION OF\nETHNOMUSICOLOGICAL AUDIO RECORDINGS\nDominique Fourer, Jean-Luc Rouas, Pierre Hanna, Matthias Robine\nLaBRI - CNRS UMR 5800 - University of Bordeaux\nffourer, rouas, hanna, robineg@labri.fr\nABSTRACT\nAutomatic timbre characterization of audio signals can\nhelp to measure similarities between sounds and is of in-\nterest for automatic or semi-automatic databases indexing.\nThe most effective methods use machine learning approa-\nches which require qualitative and diversiﬁed training data-\nbases to obtain accurate results. In this paper, we intro-\nduce a diversiﬁed database composed of worldwide non-\nwestern instruments audio recordings on which is evalu-\nated an effective timbre classiﬁcation method. A compar-\native evaluation based on the well studied Iowa musical\ninstruments database shows results comparable with those\nof state-of-the-art methods. Thus, the proposed method\noffers a practical solution for automatic ethnomusicologi-\ncal indexing of a database composed of diversiﬁed sounds\nwith various quality. The relevance of audio features for\nthe timbre characterization is also discussed in the context\nof non-western instruments analysis.\n1. INTRODUCTION\nCharacterizing musical timbre perception remains a chal-\nlenging task related to the human auditory mechanism and\nto the physics of musical instruments [4]. This task is full\nof interest for many applications like automatic database\nindexing, measuring similarities between sounds or for au-\ntomatic sound recognition. Existing psychoacoustical stud-\nies model the timbre as a multidimensional phenomenon\nindependent from musical parameters (e.g. pitch, dura-\ntion or loudness) [7, 8]. A quantitative interpretation of\ninstrument’s timbre based on acoustic features computed\nfrom audio signals was ﬁrst proposed in [9] and pursued\nin more recent studies [12] which aim at organizing au-\ndio timbre descriptors efﬁciently. Nowadays, effective au-\ntomatic timbre classiﬁcation methods [13] use supervised\nstatistical learning approaches based on audio signals fea-\ntures computed from analyzed data. Thus, the performance\nobtained with such systems depends on the taxonomy, the\nsize and the diversity of training databases. However, most\nc\rDominique Fourer, Jean-Luc Rouas, Pierre Hanna,\nMatthias Robine.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Dominique Fourer, Jean-Luc Rouas,\nPierre Hanna, Matthias Robine. “Automatic timbre classiﬁcation of\nethnomusicological audio recordings”, 15th International Society for Mu-\nsic Information Retrieval Conference, 2014.of existing research databases (e.g. RWC [6], Iowa [5])\nare only composed of common western instruments an-\nnotated with speciﬁc taxonomies. In this work, we re-\nvisit the automatic instrument classiﬁcation problem from\nan ethnomusicological point of view by introducing a di-\nversiﬁed and manually annotated research database pro-\nvided by the Centre de Recherche en Ethno-Musicologie\n(CREM). This database is daily supplied by researchers\nand has the particularity of being composed of uncommon\nnon-western musical instrument recordings from around\nthe world. This work is motivated by practical applications\nto automatic indexing of online audio recordings database\nwhich have to be computationally efﬁcient while providing\naccurate results. Thus, we aim at validating the efﬁciency\nand the robustness of the statistical learning approach us-\ning a constrained standard taxonomy, applied to recordings\nof various quality. In this study, we expect to show the\ndatabase inﬂuence, the relevance of timbre audio features\nand the choice of taxonomy for the automatic instrument\nclassiﬁcation process. A result comparison and a cross-\ndatabase evaluation is performed using the well-studied\nuniversity of Iowa musical instrument database. This pa-\nper is organized as follows. The CREM database is in-\ntroduced in Section 2. The timbre quantization principle\nbased on mathematical functions describing audio features\nis presented in Section 3. An efﬁcient timbre classiﬁcation\nmethod is described in Section 4. Experiments and results\nbased on the proposed method are detailed in Section 5.\nConclusion and future works are ﬁnally discussed in Sec-\ntion 6.\n2. THE CREM ETHNOMUSICOLOGICAL\nDATABASE\nThe CREM research database1is composed of diversiﬁed\nsound samples directly recorded by ethnomusicologists in\nvarious conditions (i.e. no recording studio) and from di-\nversiﬁed places all around the world. It contains more than\n7000 hours of audio data recorded since 1932 to nowadays\nusing different supports like magnetic tapes or vinyl discs.\nThe vintage audio recordings of the database were care-\nfully digitized to preserve the authenticity of the originals\nand contain various environment noise. The more recent\naudio recordings can be directly digital recorded with a\nhigh-quality. Most of the musical instruments which com-\n1CREM audio archives freely available online at:\nhttp://archives.crem-cnrs.fr/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n295pose this database are non-western and can be uncommon\nwhile covering a large range of musical instrument families\n(see Figure 1(a)). Among uncommon instruments, one can\nﬁnd the lute or the Ngbaka harp as cordophones. More un-\ncommon instruments like Oscillating bamboo, struck ma-\nchete and struck girder were classiﬁed by ethnomusicolo-\ngists as idiophones. In this paper, we restricted our study\nto the solo excerpts (where only one monophonic or poly-\nphonic instrument is active) to reduce the interference prob-\nlems which may occur during audio analysis. A descrip-\ntion of the selected CREM sub-database is presented in\nTable 1. According to this table, one can observe that\nthis database is actually inhomogeneous. The aerophones\nare overrepresented while membranophones are underrep-\nresented. Due to its diversity and the various quality of\nthe composing sounds, the automatic ethnomusicological\nclassiﬁcation of this database may appear as challenging.\nClass name Duration (s) #\naerophones-blowed 1,383 146\ncordophones-struck 357\n1,22937\n128 cordophones-plucked 715 75\ncordophones-bowed 157 16\nidiophones-struck 522\n75358\n82 idiophones-plucked 137 14\nidiophones-clinked 94 10\nmembranophones-struck 170 19\nTotal 3,535 375\nTable 1. Content of the CREM sub-database with duration\nand number of 10-seconds segmented excerpts.\n3. TIMBRE QUANTIZATION AND\nCLASSIFICATION\n3.1 Timbre quantization\nSince preliminaries works on the timbre description of per-\nceived sounds, Peeters et al. proposed in [12] a large set of\naudio features descriptors which can be computed from au-\ndio signals. The audio descriptors deﬁne numerical func-\ntions which aim at providing cues about speciﬁc acoustic\nfeatures (e.g. brightness is often associated with the spec-\ntral centroid according to [14]). Thus, the audio descriptors\ncan be organized as follows:\n\u000fTemporal descriptors convey information about the\ntime evolution of a signal (e.g. log attack time, tem-\nporal increase, zero-crossing rate, etc.).\n\u000fHarmonic descriptors are computed from the detected\npitch events associated with a fundamental frequency\n(F0). Thus, one can use a prior waveform model of\nquasi-harmonic sounds which have an equally spaced\nDirac comb shape in the magnitude spectrum. The\ntonal part of sounds can be isolated from signal mix-\nture and be described (e.g. noisiness, inharmonicity,\netc.).\n\u000fSpectral descriptors are computed from signal time-\nfrequency representation (e.g. Short-Term FourierTransform) without prior waveform model (e.g. spec-\ntral centroid, spectral decrease, etc.)\n\u000fPerceptual descriptors are computed from auditory-\nﬁltered bandwidth versions of signals which aim at\napproximating the human perception of sounds. This\ncan be efﬁciently computed using Equivalent Rect-\nangular Bandwidth (ERB) scale [10] which can be\ncombined with gammatone ﬁlter-bank [3] (e.g. loud-\nness, ERB spectral centroid, etc.)\nIn this study, we focus on the sound descriptors listed in\ntable 2 which can be estimated using the timbre toolbox2\nand detailed in [12]. All descriptors are computed for each\nanalyzed sound excerpt and may return null values. The\nharmonic descriptors of polyphonic sounds are computed\nusing the prominent detected F0candidate (single F0es-\ntimation). To normalize the duration of analyzed sound,\nwe separated each excerpt in 10-seconds length segments\nwithout distinction of silence or pitch events. Thus, each\nsegment is represented by a real vector where the corre-\nsponding time series of each descriptor is summarized by\na statistic. The median and the Inter Quartile Range (IQR)\nstatistics were chosen for their robustness to outliers.\nAcronym Descriptor name #\nAtt Attack duration (see ADSR model [15]) 1\nAttSlp Attack slope (ADSR) 1\nDec Decay duration (ADSR) 1\nDecSlp Decay slope (ADSR) 1\nRel Release duration (ADSR) 1\nLAT Log Attack Time 1\nTcent Temporal centroid 1\nEdur Effective duration 1\nFreqMod, AmpMod Total energy modulation (frequency,amplitude) 2\nRMSenv RMS envelope 2\nACor Signal Auto-Correlation function (12 ﬁrst coef.) 24\nZCR Zero-Crossing Rate 2\nHCent Harmonic spectral centroid 2\nHSprd Harmonic spectral spread 2\nHSkew Harmonic skewness 2\nHKurt Harmonic kurtosis 2\nHSlp Harmonic slope 2\nHDec Harmonic decrease 2\nHRoff Harmonic rolloff 2\nHVar Harmonic variation 2\nHErg, HNErg, HFErg, Harmonic energy, noise energy and frame energy 6\nHNois Noisiness 2\nHF0 Fundamental frequency F0 2\nHinH Inharmonicity 2\nHTris Harmonic tristimulus 6\nHodevR Harmonic odd to even partials ratio 2\nHdev Harmonic deviation 2\nSCent, ECent Spectral centroid of the magnitude and energy spectrum 4\nSSprd, ESprd Spectral spread of the magnitude and energy spectrum 4\nSSkew, ESkew Spectral skewness of the magnitude and energy spectrum 4\nSKurt, EKurt Spectral kurtosis of the magnitude and energy spectrum 4\nSSlp, ESlp Spectral slope of the magnitude and energy spectrum 4\nSDec, EDec Spectral decrease of the magnitude and energy spectrum 4\nSRoff, ERoff Spectral rolloff of the magnitude and energy spectrum 4\nSVar, EVar Spectral variation of the magnitude and energy spectrum 4\nSFErg, EFErg Spectral frame energy of the magnitude and energy spectrum 4\nSﬂat, ESﬂat Spectral ﬂatness of the magnitude and energy spectrum 4\nScre, EScre Spectral crest of the magnitude and energy spectrum 4\nErbCent, ErbGCent ERB scale magnitude spectrogram / gammatone centroid 4\nErbSprd, ErbGSprd ERB scale magnitude spectrogram / gammatone spread 4\nErbSkew, ErbGSkew ERB scale magnitude spectrogram / gammatone skewness 4\nErbKurt, ErbGKurt ERB scale magnitude spectrogram / gammatone kurtosis 4\nErbSlp, ErbGSlp ERB scale magnitude spectrogram / gammatone slope 4\nErbDec, ErbGDec ERB scale magnitude spectrogram / gammatone decrease 4\nErbRoff, ErbGRoff ERB scale magnitude spectrogram / gammatone rolloff 4\nErbVar, ErbGVar ERB scale magnitude spectrogram / gammatone variation 4\nErbFErg, ErbGFErg ERB scale magnitude spectrogram / gammatone frame energy 4\nErbSﬂat, ErbGSﬂat ERB scale magnitude spectrogram / gammatone ﬂatness 4\nErbScre, ErbGScre ERB scale magnitude spectrogram / gammatone crest 4\nTotal 164\nTable 2. Acronym, name and number of the used timbre\ndescriptors.\n2MATLAB code available at http://www.cirmmt.org/research/tools\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n296instrument\nbowed plucked struck plucked struckmembranophones idiophones\nclinked struck blowedcordophones aerophones(a) Hornbostel and Sachs taxonomy (T1)\ninstrument\npizzicato sustained\nstruck strings plucked strings bowed strings flute/reeds brass\npiano violin\nviola\ncello\ndoublebassviolin\nviola\ncello\ndoublebassflute\nclarinet\noboe\nsaxophone\nbassoontrumpet\ntrombone\ntuba\n(b) Musician’s instrument taxonomy (T2)\nFigure 1. Taxonomies used for the automatic classiﬁca-\ntion of musical instruments as proposed by Hornbostel and\nSachs taxonomy in [16] (a) and Peeters in [13] (b).\n3.2 Classiﬁcation taxonomy\nIn this study, we use two databases which can be anno-\ntated using different taxonomies. Due to its diversity, the\nCREM database was only annotated using the Hornbostel\nand Sachs taxonomy [16] (T1) illustrated in Figure 1(a)\nwhich is widely used in ethnomusicology. This hierarchi-\ncal taxonomy is general enough to classify uncommon in-\nstruments (e.g. struck bamboo) and conveys information\nabout sound production materials and playing styles. From\nan another hand, the Iowa musical instruments database [5]\nused in our experiments was initially annotated using a mu-\nsician’s instrument taxonomy (T2) as proposed in [13] and\nillustrated in Figure 1(b). This database is composed of\ncommon western pitched instruments which can easily be\nannotated using T1 as described in Table 3. One can notice\nthat the Iowa database is only composed of aerophones and\ncordophones instruments. If we consider the playing style,\nonly 4 classes are represented if we apply T1 taxonomy to\nthe Iowa database.\nT1 class name T2 equivalence Duration (s) #\naero-blowed reed/ﬂute and brass 5,951 668\ncordo-struck struck strings 5,564 646\ncordo-plucked plucked strings 5,229 583\ncordo-bowed bowed strings 7,853 838\nTotal 24,597 2,735\nTable 3. Content of the Iowa database using musician’s\ninstrument taxonomy (T2) and equivalence with the Horn-\nbostel and Sachs taxonomy (T1).\n4. AUTOMATIC INSTRUMENT TIMBRE\nCLASSIFICATION METHOD\nThe described method aims at estimating the correspond-\ning taxonomy class name of a given input sound.4.1 Method overview\nHere, each sound segment (cf. Section 3.1) is represented\nby vector of length p= 164 where each value corresponds\nto a descriptor (see Table 2). The training step of this\nmethod (illustrated in Figure 2) aims at modeling each tim-\nbre class using the best projection space for classiﬁcation.\nA features selection algorithm is ﬁrst applied to efﬁciently\nreduce the number of descriptors to avoid statistical over-\nlearning. The classiﬁcation space is computed using dis-\ncriminant analysis which consists in estimating optimal\nweights over the descriptors allowing the best discrimina-\ntion between timbre classes. Thus, the classiﬁcation task\nconsists in projecting an input sound into the best classiﬁ-\ncation space and to select the most probable timbre class\nusing the learned model.\nfeatures\ncomputation\nfeatures selection\n(LDA, MI, IRMFSP)\nclassification space\ncomputation\n(LDA)class affectation\n(annotated)input sound\nclass modeling\nFigure 2. Training step of the proposed method.\n4.2 Linear discriminant analysis\nThe goal of Linear Discriminant Analysis (LDA) [1] is to\nﬁnd the best projection or linear combination of all descrip-\ntors which maximizes the average distance between classes\n(inter-class distance) while minimizing distance between\nindividuals from the same class (intra-class distance). This\nmethod assumes that the class affectation of each individ-\nual is a priori known. Its principle can be described as\nfollows. First consider the n\u0002preal matrixMwhere each\nrow is a vector of descriptors associated to a sound (indi-\nvidual). We assume that each individual is a member of a\nunique class k2[1;K ]. Now we deﬁne Was the intra-\nclass variance-covariance matrix which can be estimated\nby:\nW=1\nnKX\nk=1nkWk; (1)\nwhereWkis the variance-covariance matrix computed from\nthenk\u0002psub-matrix of Mcomposed of the nkindivid-\nuals included into the class k.\nWe also deﬁne Bthe inter-class variance-covariance ma-\ntrix expressed as follows:\nB=1\nnKX\nk=1nk(\u0016k\u0000\u0016)(\u0016k\u0000\u0016)T; (2)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n297where\u0016kcorresponds to the mean vector of class kand\u0016\nis the mean vector of the entire dataset. According to [1],\nit can be shown that the eigenvectors of matrix D= (B +\nW)\u00001Bsolve this optimization problem. When the matrix\nA= (B+W)is not invertible, a computational solution\nconsists in using pseudoinverse of matrix Awhich can be\ncalculated using AT(AAT)\u00001.\n4.3 Features selection algorithms\nFeatures selection aims at computing the optimal relevance\nof each descriptor which can be measured with a weight or\na rank. The resulting descriptors subset has to be the most\ndiscriminant as possible with the minimal redundancy. In\nthis study, we investigate the three approaches described\nbelow.\n4.3.1 LDA features selection\nThe LDA method detailed in Section 4.2 can also be used\nfor selecting the most relevant features. In fact, the com-\nputed eigenvectors which correspond to linear combination\nof descriptors convey a relative weight applied to each de-\nscriptor. Thus, the signiﬁcance (or weight) Sdof a descrip-\ntordcan be computed using a summation over a deﬁned\nrange [1;R]of the eigenvectors of matrix Das follows:\nSd=RX\nr=1jvr;dj; (3)\nwherevr;dis thed-th coefﬁcient of the r-th eigenvector as-\nsociated to the eigenvalues sorted by descending order (i.e.\nr= 1 corresponds to the maximal eigenvalue of matrix\nD). In our implementation, we ﬁxed R= 8.\n4.3.2 Mutual information\nFeatures selection algorithms aim at computing a subset of\ndescriptors that conveys the maximal amount of informa-\ntion to model classes. From a statistical point of view, if\nwe consider classes and feature descriptors as realizations\nof random variables CandF. The relevance can be mea-\nsured with the mutual information deﬁned by:\nI(C;F ) =X\ncX\nfP(c;f)P(c;f)\nP(c)P(f); (4)\nwhereP(c)denotes the probability of C=cwhich can\nbe estimated from the approximated probability density\nfunctions (pdf) using a computed histogram. According\nto Bayes theorem one can compute P(c;f) =P(fjc)P(c)\nwhereP(fjc)is the pdf of the feature descriptor value f\ninto classc. This method can be improved using [2] by re-\nducing simultaneously the redundancy by considering the\nmutual information between previously selected descrip-\ntors.\n4.3.3 Inertia Ratio Maximisation using features space\nprojection (IRMFSP)\nThis algorithm was ﬁrst proposed in [11] to reduce the\nnumber of descriptors used by timbre classiﬁcation meth-\nods. It consists in maximizing the relevance of the de-scriptors subset for the classiﬁcation task while minimiz-\ning the redundancy between the selected ones. This itera-\ntive method (\u0013\u0014p) is composed of two steps. The ﬁrst one\nselects at iteration \u0013the non-previously selected descriptor\nwhich maximizes the ratio between inter-class inertia and\nthe total inertia expressed as follow:\n^d(\u0013)= arg max\ndKX\nk=1nk(\u0016d;k\u0000\u0016d)(\u0016d;k\u0000\u0016d)T\nnX\ni=1(f(\u0013)\nd;i\u0000\u0016d)(f(\u0013)\nd;i\u0000\u0016d)T;(5)\nwheref(\u0013)\nd;idenotes the value of descriptor d2[1;p] af-\nfected to the individual i.\u0016d;kand\u0016drespectively denote\nthe average value of descriptor dinto the class kand for\nthe total dataset. The second step of this algorithm aims at\northogonalizing the remaining data for the next iteration as\nfollows:\nf(\u0013+1)\nd=f(\u0013)\nd\u0000\u0010\nf(\u0013)\nd\u0001g^d\u0011\ng^d8d6=^d(\u0013); (6)\nwheref(\u0013)\n^dis the vector of the previously selected descrip-\ntor^d(\u0013)for all the individuals of the entire dataset and\ng^d=f(\u0013)\n^d=kf(\u0013)\n^dkis its normalized form.\n4.4 Class modeling and automatic classiﬁcation\nEach instrument class is modeled into the projected classi-\nﬁcation space resulting from the application of LDA. Thus,\neach class can be represented by its gravity center ^\u0016kwhich\ncorresponds to the vector of the averaged values of the pro-\njected individuals which compose the class k. The classi-\nﬁcation decision which affect a class ^kto an input sound\nrepresented by a projected vector ^xis simply performed by\nminimizing the Euclidean distance with the gravity center\nof each class as follows:\n^k= arg min\nkk^\u0016k\u0000^xk28k2[1;K ]; (7)\nwherekvk2denotes the l2norm of vector v. Despite its\nsimplicity, this method seems to obtain good results com-\nparable with those of the literature [12].\n5. EXPERIMENTS AND RESULTS\nIn this section we present the classiﬁcation results obtained\nusing the proposed method described in Section 4.\n5.1 Method evaluation based on self database\nclassiﬁcation\nIn this experiment, we evaluate the classiﬁcation of each\ndistinct database using different taxonomies. We applied\nthe 3-fold cross validation methodology which consists in\npartitioning the database in 3 distinct random subsets com-\nposed with 33% of each class (no collision between sets).\nThus, the automatic classiﬁcation applied on each subset\nis based on training applied on the remaining 66% of the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n298database. Figure 5.1 compares the classiﬁcation accuracy\nobtained as a function of the number of used descriptors.\nThe resulting confusion matrix of the CREM database us-\ning 20 audio descriptors is presented in Table 4 and shows\nan average classiﬁcation accuracy of 80% where each in-\nstrument is well classiﬁed with a minimal accuracy of 70%\nfor the aerophones. These results are good and seems com-\nparable with those described in the literature [11] using\nthe same number of descriptor. The most relevant feature\ndescriptors (selected among the top ten) estimated by the\nIRMSFP and used for the classiﬁcation task are detailed in\nTable 7. This result reveals signiﬁcant differences between\nthe two databases. As an example, harmonic descriptors\nare only discriminative for the CREM database but not for\nthe Iowa database. This may be explained by the pres-\nence of membranophone in the CREM database which are\nnot present in the Iowa database. Contrarily, spectral and\nperceptual descriptors seems more relevant for the Iowa\ndatabase than for the CREM database. Some descriptors\nappear to be relevant for both database like the Spectral\nﬂatness (Sﬂat) and the ERB scale frame energy (ErbFErg)\nwhich describe the spectral envelope of signal.\naero c-struc c-pluc c-bowed i-pluc i-struc i-clink membr\naero 70 3 9 5 7 5\nc-struc 6 92 3\nc-pluc 5 8 73 4 8 1\nc-bowed 13 80 7\ni-pluc 79 14 7\ni-struc 9 2 5 2 79 4\ni-clink 100\nmembr 11 17 72\nTable 4. Confusion matrix (expressed in percent of the\nsounds of the original class listed on the left) of the CREM\ndatabase using the 20 most relevant descriptors selected by\nIRMSFP.\n5.2 Cross-database evaluation\nIn this experiments (see Table 5), we merged the two data-\nbases and we applied the 3-fold cross validation method\nbased on the T1 taxonomy to evaluate the classiﬁcation ac-\ncuracy on both database. The resulting average accuracy\nis about 68% which is lower than the accuracy obtained\non the distinct classiﬁcation of each database. The results\nof cross-database evaluation applied between databases us-\ning the T1 taxonomy are presented in Table 6 and obtain a\npoor average accuracy of 30%. This seems to conﬁrm our\nintuition that the Iowa database conveys insufﬁcient infor-\nmation to distinguish the different playing styles between\nthe non-western cordophones instruments of the CREM\ndatabase.\n6. CONCLUSION AND FUTURE WORKS\nWe applied a computationally efﬁcient automatic timbre\nclassiﬁcation method which was successfully evaluated on\nan introduced diversiﬁed database using an ethnomusico-\nlogical taxonomy. This method obtains good classiﬁcation\nresults (> 80% of accuracy) for both evaluated databases\nwhich are comparable to those of the literature. However,\n020 40 60 80100 120 140 16000.10.20.30.40.50.60.70.80.91\nnumber of descriptorsAccuracy ratioAccuracy as a function of the number of descriptor [17 classes]\n  \nLDA\nMI\nIRMFSP(a) Iowa database using T2\n020 40 60 80100 120 140 16000.10.20.30.40.50.60.70.80.91\nnumber of descriptorsAccuracy ratioAccuracy as a function of the number of descriptor [4 classes]\n  \nLDA\nMI\nIRMFSP\n(b) Iowa database using T1\n020 40 60 80100 120 140 16000.10.20.30.40.50.60.70.80.91\nnumber of descriptorsAccuracy ratioAccuracy as a function of the number of descriptor [8 classes]\n  \nLDA\nMI\nIRMFSP\n(c) CREM database using T1\nFigure 3. Comparison of the 3-fold cross validation classi-\nﬁcation accuracy as a function of the number of optimally\nselected descriptors.\nthe cross-database evaluation shows that each database can-\nnot be used to infer a classiﬁcation to the other. This can\nbe explained by signiﬁcant differences between these data-\nbases. Interestingly, results on the merged database obtain\nan acceptable accuracy of about 70%. As shown in pre-\nvious work [11], our experiments conﬁrm the efﬁciency\nof IRMFSP algorithm for automatic features selection ap-\nplied to timbre classiﬁcation. The interpretation of the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n299aero c-struc c-pluc c-bowed i-pluc i-struc i-clink membr\naero 74 14 5 3 2 1\nc-struc 12 69 10 5 1 2\nc-pluc 1 7 58 29 1 2 2\nc-bowed 3 6 33 52 1 3\ni-pluc 7 14 79\ni-struc 2 2 4 11 2 51 30\ni-clink 11 89\nmembr 6 17 78\nTable 5. Confusion matrix (expressed in percent of the\nsounds of the original class listed on the left) of the evalu-\nated fusion between the CREM and the Iowa database us-\ning the 20 most relevant descriptors selected by IRMSFP.\naero c-struc c-pluc c-bowed\naero 72 9 10 9\nc-struc 12 12 34 42\nc-pluc 23 47 28 3\nc-bowed 28 34 24 14\nTable 6. Confusion matrix (expressed in percent of the\nsounds of the original class listed on the left) of the CREM\ndatabase classiﬁcation based on Iowa database training.\nCREM T1 Iow\na T1 Iow\na T2 CREM+Io w\na T1\nEdur AttSlp AttSlp AmpMod\nAcor Dec Acor Acor\nZCR RMSen v\nHdev\nHnois\nHTris3\nSﬂat SFEr g Sﬂat Sﬂat\nERof f SRof f SVar\nSSke\nw SKurt\nScre\nErbGK urt ErbK urt ErbSprd\nErbFEr g ErbFEr g ErbFEr g\nErbRoff ErbRoff\nErbSlp ErbGSprd\nErbGCent\nTable 7. Comparison of the most relevant descriptors esti-\nmated by IRMFSP.\nmost relevant selected features shows a signiﬁcant effect of\nthe content of database rather than on the taxonomy. How-\never the timbre modeling interpretation applied to timbre\nclassiﬁcation remains difﬁcult. Future works will consist\nin further investigating the role of descriptors by manually\nconstraining selection before the classiﬁcation process.\n7. ACKNOWLEDGMENTS\nThis research was partly supported by the French ANR\n(Agence Nationale de la Recherche) DIADEMS (Descrip-\ntion,Indexation, Acces aux Documents Ethnomusicologiques\net Sonores) project (ANR-12-CORD-0022).\n8. REFERENCES\n[1] T. W. Anderson. An Introduction to Multivariate Sta-\ntistical Analysis. Wiley-Blackwell, New York, USA,\n1958.\n[2] R. Battiti. Using mutual information for selecting fea-\ntures in supervised neural net learning. IEEE Trans. on\nNeural Networks, 5(4):537–550, Jul. 1994.\n[3] E.Ambikairajah, J. Epps, and L. Lin. Wideband speech\nand audio coding using gammatone ﬁlter banks. In\nProc. IEEE ICASSP’01, volume 2, pages 773–776,\n2001.[4] N. F. Fletcher and T. D. Rossing. The Physics of Musi-\ncal Instruments. Springer-Verlag, 1998.\n[5] L. Fritts. Musical instrument samples. Univ. Iowa\nElectronic Music Studios, 1997. [Online]. Available:\nhttp://theremin.music.uiowa.edu/MIS.html.\n[6] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRwc music database: Music genre database and musi-\ncal instrument sound database. In Proc. ISMIR, pages\n229–230, Oct. 2003.\n[7] J. M. Grey and J. W. Gordon. Perceptual effects of spc-\ntral modiﬁcations on musical timbre. Journal of Acous-\ntic Society of America (JASA), 5(63):1493–1500, 1978.\n[8] S. McAdams, S. Winsberg, S. Donnadieu, G. Soete,\nand J. Krimphoff. Perceptual scaling of synthesized\nmusical timbres: Common dimensions, speciﬁcities,\nand latent subject classes. Psychological Research,\n58(3):177–192, 1995.\n[9] N. Misdariis, K. Bennett, D. Pressnitzer, P. Susini, and\nS. McAdams. Validation of a multidimensional dis-\ntance model for perceptual dissimilarities among musi-\ncal timbres. In Proc. ICA & ASA, volume 103, Seattle,\nUSA, Jun. 1998.\n[10] B.C.J. Moore and B.R. Glasberg. Suggested formu-\nlae for calculating auditory-ﬁlter bandwidths and ex-\ncitation patterns. Journal of the Acoustical Society of\nAmerica, 74:750–753, 1983.\n[11] G. Peeters. Automatic classiﬁcation of large musi-\ncal instrument databases using hierarchical classiﬁers\nwith intertia ratio maximization. In 115th convention\nof AES, New York, USA, Oct. 2003.\n[12] G. Peeters, B. Giordano, P. Susini, N. Misdariis, and\nS. McAdams. The timbre toolbox: Audio descrip-\ntors of musical signals. Journal of Acoustic Society of\nAmerica (JASA), 5(130):2902–2916, Nov. 2011.\n[13] G. Peeters and X. Rodet. Automatically selecting sig-\nnal descriptors for sound classiﬁcation. In Proc. ICMC ,\nG¨oteborg, Sweden, 2002.\n[14] E. Schubert, J. Wolfe, and A. Tarnopolsky. Spectral\ncentroid and timbre in complex, multiple instrumental\ntextures. In Proc. 8th Int. Conf. on Music Perception &\nCognition (ICMPC), Evanston, Aug. 2004.\n[15] G. Torelli and G. Caironi. New polyphonic sound\ngenerator chip with integrated microprocessor-\nprogrammable adsr envelope shaper. IEEE Trans. on\nConsumer Electronics, CE-29(3):203–212, 1983.\n[16] E. v. Hornbostel and C. Sachs. The classiﬁcation of\nmusical instruments. Galpin Society Journal, 3(25):3–\n29, 1961.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n300"
    },
    {
        "title": "A Proximity Grid Optimization Method to Improve Audio Search for Sound Design.",
        "author": [
            "Christian Frisson",
            "Stéphane Dupont",
            "Willy Yvart",
            "Nicolas Riche",
            "Xavier Siebert",
            "Thierry Dutoit"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417245",
        "url": "https://doi.org/10.5281/zenodo.1417245",
        "ee": "https://zenodo.org/records/1417245/files/FrissonDYRSD14.pdf",
        "abstract": "Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research ap- plications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student- t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a stan- dard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This op- timization method can serve as a human-readable metric for the comparison of dimension reduction techniques.",
        "zenodo_id": 1417245,
        "dblp_key": "conf/ismir/FrissonDYRSD14",
        "keywords": [
            "sound designers",
            "dedicated applications",
            "default file browsers",
            "content-based research applications",
            "cloud-like similarity layouts",
            "feature extraction",
            "dimension reduction",
            "Student- t Stochastic Neighbor Embedding",
            "proximity grid",
            "proximity grid optimization"
        ],
        "content": "A PROXIMITY GRID OPTIMIZATION METHOD\nTO IMPROVE AUDIO SEARCH FOR SOUND DESIGN\nChristian Frisson, St ´ephane Dupont, Willy Yvart, Nicolas Riche, Xavier Siebert, Thierry Dutoit\nnumediart Institute, University of Mons, Boulevard Dolez 31, 7000 Mons, Belgium\nfchristian.frisson;stephane.dupont;willy.yvart;nicolas.riche;xavier.siebert;thierry.dutoitg@umons.ac.be\nABSTRACT\nSound designers organize their sound libraries either with\ndedicated applications (often featuring spreadsheet views),\nor with default ﬁle browsers. Content-based research ap-\nplications have been favoring cloud-like similarity layouts.\nWe propose a solution combining the advantages of these:\nafter feature extraction and dimension reduction (Student-\nt Stochastic Neighbor Embedding), we apply a proximity\ngrid, optimized to preserve nearest neighborhoods between\nthe adjacent cells. By counting direct vertical / horizontal /\ndiagonal neighbors, we compare this solution over a stan-\ndard layout: a grid ordered by ﬁlename. Our evaluation is\nperformed on subsets of the One Laptop Per Child sound\nlibrary, either selected by thematic folders, or ﬁltered by\ntag. We also compare 3 layouts (grid by ﬁlename without\nvisual icons, with visual icons, and proximity grid) by a\nuser evaluation through known-item search tasks. This op-\ntimization method can serve as a human-readable metric\nfor the comparison of dimension reduction techniques.\n1. INTRODUCTION\nSound designers source sounds in massive collections, heav-\nily tagged by themselves and sound librarians. If a set of\nsounds to compose the desired sound effect is not avail-\nable, a Foley artist records the missing sound and tags these\nrecordings as accurately as possible, identifying many phys-\nical (object, source, action, material, location) and digital\n(effects, processing) properties. When it comes to looking\nfor sounds in such collections, successive keywords can\nhelp the user to ﬁlter down the results. But at the end of\nthis process, hundreds of sounds can still remain for fur-\nther review. This creates an opportunity for content-based\ninformation retrieval approaches and other means for pre-\nsenting the available content. From these observations, we\nelicited the following research question: can content-based\norganization complement or outperform context-based or-\nganization once a limit is reached when ﬁltering by tag?\nc\rChristian Frisson, St ´ephane Dupont, Willy Yvart, Nico-\nlas Riche, Xavier Siebert, Thierry Dutoit.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Christian Frisson, St ´ephane Dupont,\nWilly Yvart, Nicolas Riche, Xavier Siebert, Thierry Dutoit. “A proxim-\nity grid optimization method to improve audio search for sound design”,\n15th International Society for Music Information Retrieval Conference,\n2014.This work partly addresses this question and presents\na solution to interactively browse collections of textural\nsounds after these have been ﬁltered by tags.\nWe organize sounds in a two-dimensional map using\ncontent-based features extracted from their signal. These\nfeatures are mapped to two visual variables. First, the po-\nsition of the sample on the screen is obtained after apply-\ning dimension reduction over the features followed by a\nproximity grid that structures items on a grid which facil-\nitates navigation and visualization, in particular by reduc-\ning the cluttering. The organization of the samples on the\ngrid is optimized using a novel approach that preserves the\nproximity on the grid of a maximum of nearest neighbors\nin the original high-dimensional feature space. Second,\nthe shape of the sample is designed to cue one important\ncontent-based feature, the perceptual sharpness (a measure\nthe “brightness” of the sound).\nThis approach is evaluated through a known-item search\ntask. Our experiments provide one of the ﬁrst positive re-\nsult quantitatively showing the interest of MIR-based vi-\nsualization approaches for sound search, when then proper\nacoustic feature extraction, dimension reduction, and visu-\nalization approaches are being used.\nThe paper is organized as follows. First, in section 2,\nwe examine the landscape of existing systems dedicated to\nbrowsing ﬁles in sound design. We then describe how we\ndesigned our system in Section 3. In section 4, we describe\nour evaluation approach, experiments and obtained results.\nWe ﬁnish by summarizing our contributions and provide\nan glimpse of future research directions.\n2. BACKGROUND\nThis section provides a review of the literature and em-\npirical ﬁndings on systems for sound design, and outlines\nsome results and gaps that motivated this work.\nSystems for mining sounds, particularly for sound de-\nsign, are actually rather scarce. These may however share\nsome similarities with systems targeted to the management\nof music collections, in particular in the content-based pro-\ncessing workﬂow that allows to organize the audio ﬁles.\nA comprehensive survey on these aspects has been pro-\nposed by Casey et al. [4]. We nevertheless believe that\nthe design of the user interface of each system class might\nbeneﬁt from different cues from information visualization\nand human-computer interaction, and that major progress\nis still possible in all these areas.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3492.1 Research-grade systems\nThe work presented in [18] underlines that few published\nresearch provide accurate usability evaluations on such sys-\ntems, beyond informal and heuristic ones. The author jus-\ntiﬁes that this may have occurred because complementary\nresearch communities have actually been evolving essen-\ntially in separate silos. These include the music infor-\nmation retrieval and the human-computer interaction com-\nmunities. In that work, 20 systems with auditory display\nare nevertheless reviewed and compared, including 2 au-\ndio browsers that are presented hereafter.\nSonic Browser focused on information visualization [7],\nand later approached content-based organization through\nthe Marsyas framework [3]. A 2D starﬁeld display al-\nlows to map the metadata of audio ﬁles to visual variables.\nItsHyperTree view consists in a spring-layout hierarchical\ngraph visualization for browsing the ﬁle tree of sound col-\nlections. They qualitatively evaluated these views with 15\nstudents through timed tasks and a questionnaire [2]; and\ntheir system against the Microsoft Windows 2000 explorer\nthrough a think-aloud protocol with 6 students [7].\nSoundTorch, the most recent content-based audio brow-\nser, has been designed by people aware of audio engineer-\ning practices [11]. It relies on Mel-Frequency Cepstral\nCoefﬁcients (MFCCs) as features, clustered with a Self-\nOrganizing Map (SOM) but initialized with smooth gradi-\nents rather than randomly, so that the horizontal axis corre-\nsponds to a tonal-to-noisy continuum and the vertical axis\nto pitch increase / dull-to-bright. In addition to cueing in\nthe variety of content through the position of the nodes cor-\nresponding to sounds, SoundTorch makes use of the node\nshape to convey additional information: the temporal evo-\nlution of the power of the signal is mapped to a circle.\nIt is the only related work to provide a quantitative user\nevaluation. They positively evaluated known- and descri-\nbed-item search tasks comparatively to a list-based appli-\ncation. A dozen of users were involved. However, it is\nnot clear from this comparison whether the approach out-\nperforms the list-based application because of its content-\nbased capabilities, or else because of its interactive abil-\nities (particularly its instant playback of closely-located\nnodes in the map), or both. Moreover, it has been chosen\nto randomize the sound list. Sound designers either buy\ncommercial sound libraries that are tagged properly and\nnamed accordingly, or else record their own. They also\nusually spend a signiﬁcant amount of time to tag these li-\nbraries. Therefore, to our opinion, a more realistic baseline\nfor comparison should be a basic ordering by ﬁlename.\nCataRT is an application developed in the Max/MSP\nmodular dataﬂow framework, that “mosaices” sounds into\nsmall fragments for concatenative synthesis. A 2D scat-\nter plot allows to browse the sound fragments, assigning\nfeatures to the axes. The authors recently applied a distri-\nbution algorithm that optimizes the spreading of the plotted\nsounds by means of iterative Delaunay triangulation and a\nmass-spring model, so as to solve the non-uniform density\ninherent to a scatter plot, and open new perspectives for\nnon-rectangular interfaces such as the circular reacTableand complex geometries of physical spaces to sonify. To\nour knowledge, no user study has yet been published for\nthis tool. It is however claimed as future work [12].\nIn summary, it appears that no evaluation have been pro-\nposed previously on the speciﬁc contribution of content-\nbased analysis to the efﬁciency of sound search. This is a\ngap we started to address in this work.\n2.2 Commercial systems\nIt is worth mentioning here that some commercial sys-\ntems, some making use of content-based approaches, have\nalso been proposed, although no quantitative evaluation of\nthose can be found in the literature. A pioneering applica-\ntion is SoundFisher by company Muscle Fish [21], start-up\nof scientists that graduated in the ﬁeld of audio retrieval.\nTheir application allowed to categorize sounds along sev-\neral acoustic features (pitch, loudness, brightness, band-\nwidth, harmonicity) whose variations over time are esti-\nmated by average, variance and autocorrelation. Sounds\nare compared from the Euclidean distance over these fea-\ntures. The browser offers several views: a detail of sound\nattributes (ﬁlename, samplerate, ﬁle size...) in a spread-\nsheet, a tree of categories resulting from classiﬁcation by\nexample (the user providing a set of sounds), and a scatter\nplot to sort sounds along one feature per axis.\nA second product, AudioFinder by Iced Audio1mimics\npersonal music managers such as Apple iTunes: on top a\ntextual search input widget allows to perform a query, a top\npane proposes a hierarchical view similar to the “column”\nview of the Finder to browse the ﬁle tree of the collection,\na central view features a spreadsheet to order the results\nalong audio and basic ﬁle metadata, a left pane lists saved\nresults like playlists. A bottom row offers waveform visu-\nalizations and the possibility to apply audio effect process-\ning to quickly proof the potential variability of the sounds\nbefore dropping these into other creative applications.\nA major product, Soundminer HD2, provides a similar\ninterface, plus an alternative layout named 3D LaunchPad\nthat allows, similarly to the Apple Finder CoverFlow view,\nto browse sounds (songs) by collection (album) cover, with\nthe difference that the former is a 2D grid and the latter a\n1D rapid serial visualization technique.\nOther companies facilitating creativity such as Adobe\nwith Bridge3provide more general digital asset manage-\nment solutions that are accessible through their entire ap-\nplication suite. These focus on production-required capa-\nbilities and seem to avoid content-based functionalities.\nFrom our contextual inquiry we noticed that sound de-\nsigners also make use of simple browsers, such as the de-\nfault provided by the operating system, optionally associ-\nated to a spreadsheet to centralize tags.\n1http://www.icedaudio.com\n2http://www.soundminer.com\n3http://www.adobe.com/products/bridge.html\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3503. OUR SOLUTION\nOur system blends knowledge gained from the ﬁelds of\nmultimedia information retrieval (content-based organiza-\ntion), human-computer interaction (usability evaluation)\nand information visualization (visual variables).\n3.1 A multimedia information retrieval pipeline\nOne ﬁrst step is feature extraction. For sound and music,\na large variety of temporal and/or spectral features have\nbeen proposed in the literature [4, 15]. We based our fea-\ntures set from [6] since their evaluation considered textural\nsounds. In short, we used a combination of derivatives of\nand statistics (standard deviation, skewness and/or kurto-\nsis) over MFCCs and Spectral Flatness (SF). We did not\nperform segmentation as our test collections contain textu-\nral sounds of short length and steady homogeneity.\nAnother important step is dimension reduction. From\nour perspective, one of the most promising approach is\nStochastic Neighborhood Embedding (SNE) using Student-\nt distributions (t-SNE) [13]. It has been previously quali-\ntatively evaluated on sound collection visualization [6, 9].\nThe method has an interesting information retrieval per-\nspective, as it actually aims at probabilistically preserving\nhigh-dimensional neighbors in a lower-dimensional pro-\njection (2D in our work), and actually maximizes conti-\nnuity (a measure that can intuitively be related to recall in\ninformation retrieval) in the projected space. One emergent\nresult is that recordings from the same sound source with\nonly slight variations are almost always neighbors in the\n2D representation, as the recall is high. Another popular\nbut older approach for dimensionality reduction are SOMs.\nIn [14], it has been compared with most recent techniques,\nand in particular the Neighbor Retrieval Visualizer (NeRV ,\na generalization of SNE). SOMs produced the most trust-\nworthy (a measure that can intuitively be related to pre-\ncision in information retrieval) projection but the NeRV\nwas superior in terms of continuity and smoothed recall.\nAs SNE is a special case of NeRV where a tradeoff is set\nso that only recall is maximized, we infer from those re-\nsults that SNE is a better approach for our purposes than\nSOM. Qualitative evaluations of different approaches ap-\nplied to music retrieval have been undertaken [19]: Mul-\ntidimensional Scaling (MDS), NeRV and Growing SOMs\n(GSOM). Users described MDS to result in less positional\nchanges, NeRV to better preserve cluster structures and\nGSOM to have less overlappings. NeRV and presumably\nt-SNE seem beneﬁcial in handling cluster structures.\nBesides, we propose in this paper an approach to reduce\nthe possible overlappings in t-SNE. An undesirable arti-\nfact of the original t-SNE approach however comes from\nthe optimization procedure, which relies on gradient de-\nscent with a randomly initialized low-dimensional repre-\nsentation. It creates a stability issue, where several runs of\nthe algorithm may end up in different representations af-\nter convergence. This works against the human memory.\nWe thus initialized the low-dimensional representation us-\ning the two ﬁrst axes of a Principal Component Analysis\n(PCA) of the whole feature set.3.2 Mapping audio features to visual variables\nDisplaying such a representation results in a scatter plot or\nstarﬁeld display. We address two shortcomings: 1) clusters\nof similar sounds might not be salient, and 2) this visual-\nization technique may cause overlap in some areas. Son-\nicBrowser [7], that we analyzed in the previous section,\nand the work of Thomas Grill [9], dedicated to textural\nsounds, approached the ﬁrst issue by mapping audio fea-\ntures to visual variables. Ware’s book [20] offer great ex-\nplanations and recommendations to use visual variables to\nsupport information visualization tailored for human per-\nception. Thomas Grill’s approach was to map many per-\nceptual audio features to many visual variables (position,\ncolor, texture, shape), in one-to-one mappings.\n3.2.1 Content-based glyphs as sound icons\nGrill et al. designed a feature-ﬂedged visualization tech-\nnique mapping perceptual qualities in textural sounds to\nvisual variables [9]. They chose to fully exploit the visual\nspace by tiling textures: items are not represented by a dis-\ntinct glyph, rather by a textured region. In a ﬁrst attempt to\ndiscriminate the contribution of information visualization\nversus media information retrieval in sound browsing, we\nopted here for a simpler mapping. We mapped the mean\nover time of perceptual sharpness to the value in the Hue\nSaturation Value (HSV) space of the node color for each\nsound, normalized against the Values for all sounds in each\ncollection. A sense of brightness is thus conveyed in both\nthe audio and visual channels through perceptual sharp-\nness and value. We also used the temporal evolution of\nperceptual sharpness to deﬁne a clockwise contour of the\nnodes, so that sounds of similar average brightness but dif-\nferent temporal evolution could be better discriminated. To\ncompute positions, perceptual sharpness was also added to\nthe feature selection, intuiting it would gather items that\nare similar visually. The choice of perceptual sharpness\nwas motivated by another work of Grill et al. [10]: they\naimed at deﬁning features correlated to perceived charac-\nteristics of sounds that can be named or verbalized through\npersonal constructs. High-low, or brightness of the sound,\nwas the construct the most correlated to an existing feature:\nperceptual sharpness.\n3.2.2 A proximity grid optimizing nearest neighbors\nFor the removal of clutter in 2D plots, two major approaches\nexist: reducing the number of items to display, or read-\njusting the position of items. In our context, we want to\ndisplay all the items resulting of search queries by tag ﬁl-\ntering. For this purpose, we borrow a method initially de-\nsigned to solve the problem of overlap for content-based\nimage browsing [16]: a proximity grid [1]. Their work\nis heavily cited respectively for the evaluation of multi-\ndimensional scaling techniques [1] and as a pioneering ap-\nplication of usability evaluation for multimedia informa-\ntion retrieval [16], but almost never regarding the proxim-\nity grid. To our knowledge, no audio or music browser\napproached this solution.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n351Figure 1. Different layouts with glyphs for the same sound collection ﬁltered by keyword “water”, from left to right:\n“album”, “cloud”, “metro”, and most dense proximity grid.\nA proximity grid consists in adapting the coordinates\nof each item of a 2D plot to magnetize these items on an\nevenly-distributed grid. Basalaj proposed several variants\nto compute a proximity grid: greedy methods with spiral\nsearch to ﬁnd empty cells and empty/swap/bump strate-\ngies to assign items to cells; an improved greedy method\nreplacing spiral search by shortest distance estimation; a\n“squeaky wheel” optimization using simulated annealing,\nand a genetic algorithm [1]. We implemented the simplest\ngreedy method with all strategies. To determine the order\nof the items to assign to cells, we used the fast minimum\nspanning tree algorithm implementation from the machine\nlearning library mlpack of Boruvka’s dual-tree based on\nk-dimensional trees [5]. Applied in high dimension of the\naudio features, the empty strategy starts with shortest edges\nwhile it is the opposite for swap and bump strategies, ac-\ncording to Basalaj. We opted for a simpliﬁcation: a spiral\nsearch always turning clockwise and starting above the de-\nsired cell, while it is recommended to choose the rotation\nand ﬁrst next cell from exact distance computation between\nthe actual coordinates of the node and the desired cell.\nThe minimal side of a square grid is the ceil of the\nsquare root of the collection size, providing the most space\nefﬁcient density. To approximate a least distorted grid, the\ncollection size can be taken as grid side. To come up with\na tradeoff between density and neighborhood preservation,\nwe estimate the number of high-dimensional nearest neigh-\nbors (k=1) preserved in 2D at a given grid resolution sim-\nply by counting the number of pairs in adjacent cells. We\ndistinguish the amounts of horizontal and vertical and di-\nagonal neighbors since different search patterns may be\nopted by users: mostly horizontal or vertical for people ac-\ncustomed respectively to western and non-western reading\norder, diagonal may be relevant for grids of light density.\nFor our experiments described in the next section, we\nprepared the collections by qualitative selection of the op-\ntimal grid resolution based on the amounts of horizontal,\nvertical and diagonal adjacent neighbors computed for each\nresolution between the minimal side and the least distorted\napproximate, comparing such amounts between a proxim-\nity grid applied after dimension reduction and a grid or-\ndered by ﬁlename. Not all collections presented a prox-\nimity grid resolution that outperformed a simple grid by\nﬁlename in terms of neighbor preservation.4. EXPERIMENTS\n4.1 Open dataset\nThe One Laptop Per Child (OLPC) sound library4was\nchosen so as to make the following tests easily reproducible,\nfor validation and comparison perspectives, and because it\nis not a dataset artiﬁcially generated to ﬁt with expected\nresults when testing machine learning algorithms. It is\nlicensed under a Creative Commons BY license (requir-\ning attribution). It contains 8458 sound samples, 90 sub-\nlibraries combine diverse types of content or specialize into\none type, among which: musical instruments riffs or sin-\ngle notes, ﬁeld recordings, Foley recording, synthesized\nsounds, vocals, animal sounds. It is to be noted, especially\nfor subset libraries curated by Berklee containing Foley\nsound design material, that within a given subset most sam-\nples seem to have been recorded, if not named, by a same\nauthor per subset. It is thus frequent to ﬁnd similar sounds\nnamed incrementally, for instance Metal on the ground [n]\nwithnvarying from 1 to 4. These are likely to be different\ntakes of a recording session on a same setting of sound-\ning object and related action performed on it. Ordering\nsearch results by tag ﬁltering in a list by path and ﬁlename,\nsimilarly to a standard ﬁle browser, will thus imprint local\nneighborhoods to the list.\n4.2 Evaluation method\nWe chose to perform a qualitative and quantitative evalua-\ntion: qualitative through a feedback questionnaire, quan-\ntitative through known-item search tasks as popularized\nrecently for video browsers by the Video Browser Show-\ndown [17]. In the context of audio browsers, for each task\nthe target sound is heard, the user has to ﬁnd it back as fast\nas possible using a given layout. Font’s thesis compared\nlayouts for sound browsing: automatic (PCA), direct map-\nping (scatter plot) and random map [8]. Time and speeds\nwere deliberately not investigated, claiming that people em-\nploy different search behaviors.\n4http://wiki.laptop.org/go/Free_sound_samples\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n352Figure 2. Sequence of tasks in the last experiment. In\nrows subsets of the One Laptop Per Child (OLPC) sound\nlibrary ﬁltered by keyword, respectively “water”, “spring”,\n“metal”. In columns: permutations of layouts.\n4.3 Design\nWe undertook four experiments: the ﬁrst comparing grid\nand glyph-less cloud layouts motivated us to add glyph\nrepresentations (cloud was outperformed), the second and\nthird conﬁrmed that a proximity grid was to be investigated\n(cloud still outperformed), the last validated these choices.\nWe recorded several metrics (success times, pointer dis-\ntances and speeds, audio hovers) and ratings from feedback\nquestionnaires. Here we only report the last experiment\nand only analyze times taken to successfully ﬁnd targets.\nThe fourth experiment was designed as a within-subject\nsummative evaluation. Figure 2 shows the exact sequence\nof tasks presented to the users. An additional collection\nwas used for training tasks with each layout.\nEach layout was given a nickname: grid for the sim-\nple grid ordered by ﬁlename, album for its upgrade with\nglyphs, metro for the proximity grid of optimal resolu-\ntion for neighbors preservation. These short nicknames\nbrought two advantages: facilitating their instant recog-\nnition when announced by the test observer at the begin-\nning of each task, and suggesting search patterns: horizon-\ntal land mowing for grid andalbum, adjacent cell brows-\ning for metro. The metro layout was described to users\nusing the metaphor of metro maps: items (stations) can\nform (connect) local neighborhoods and remote “friends”\n(through metro lines usually identiﬁed by color).\n4.4 Participants and apparatus\n16 participants (5 female) of mean age 28 (+/- 6.3) each\nperformed 9 tasks on 3 different collections. Besides 2\nsubjects, all the participants have studied or taught audiovi-\nsual communication practices (sound design, ﬁlm edition).They were asked which human sense they favored in their\nwork (if not, daily) on a 5-point Likert scale, 1 for audition\nto 5 for vision: on average 3.56 (+/- 0.60). All self-rated\nthemselves with normal audition, 10 with corrected vision.\nWe used an Apple Macbook Pro Late 2013 laptop with\n15-inch Retina display, with a RME FireFace UCX sound\ncard, and a pair of Genelec active loudspeakers. A 3Dcon-\nnexion Space Navigator 3D mouse was repurposed into a\nbuzzer to submit targets hovered by the touchpad, with au-\ndio feedback of the closest node to the pointer.\n4.5 Results\nA one-way ANOV A shows that there is a quite signiﬁ-\ncant difference between views within subjects on success\ntimes (p=.02), more on self-reported ratings of efﬁciency\n(p<.001) and pleasurability (p<.001). Mean and standard\ndeviations are compared in table 1. A Tukey multiple com-\nparisons of success times means at a 95% family-wise con-\nﬁdence level on layouts shows that metro outperformed\ngrid (p=.01), but album was not signiﬁcantly better than\ngrid (p=.34) or worse than metro (p=.26).\ngrid album metro\nsuccess times (s) 53.0(46.6) 43.1(38.0) 31.3(22.9)\nefﬁciency [1-5] 1.87(1.01) 3.75(1.00) 4.12(0.96)\npleasurability [1-5] 2.25(1.18) 3.62(0.81) 4.25(0.86)\nTable 1. Mean (standard deviations) of evaluation metrics\n4.6 Discussion\nFeature extraction is a one-shot ofﬂine process at index-\ning time. Dimension reduction for layout computation is a\nprocess that should be close to real-time so as not to slow\ndown search tasks and that is likely to be performed at least\nonce per query. Decent results can be achieved by combin-\ning only content-based icons and simple ordering by ﬁle-\nname. A content-based layout comes at a greater compu-\ntational cost but brings signiﬁcant improvements.\n5. CONCLUSION, FUTURE WORKS\nWe proposed a method to assist sound designers in review-\ning results of queries by browsing a sound map optimized\nfor nearest neighbors preservation in adjacent cells of a\nproximity grid, with content-based features cued through\nglyph-based representations. Through a usability evalua-\ntion of known-item search tasks, we showed that this so-\nlution was more efﬁcient and pleasurable than a grid of\nsounds ordered by ﬁlenames.\nAn improvement to this method would require to inves-\ntigate all blocks from the multimedia information retrieval\ndata ﬂow. First, other features tailored for sound effects\nshould be tried. Second, we have noticed that some of\nthe ﬁrst high-dimensional nearest neighbors are positioned\nquite far away in 2D, already past dimension reduction.\nReducing pairwise distance preservation errors may be an\ninvestigation track.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3536. ACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their careful rec-\nommendations. We thank all the testers for their time and\npatience in performing tasks that were sometimes too dif-\nﬁcult. This work has been partly funded by the Walloon\nRegion of Belgium through GreenTIC grant SONIXTRIP.\n7. REFERENCES\n[1] Wojciech Basalaj. Proximity visualisation of abstract\ndata. PhD thesis, University of Cambridge, 2000.\n[2] Eoin Brazil. Investigation of multiple visualisation\ntechniques and dynamic queries in conjunction with\ndirect soniﬁcation to support the browsing of audio\nresources. Master’s thesis, Interaction Design Centre,\nDept. of Computer Science & Information Systems\nUniversity of Limerick, 2003.\n[3] Eoin Brazil, Mikael Fernstr ¨om, George Tzanetakis,\nand Perry Cook. Enhancing sonic browsing using audio\ninformation retrieval. In Proceedings of the Interna-\ntional Conference on Auditory Display (ICAD), 2002.\n[4] M. A. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges. In Proceedings of the IEEE, volume 96, 2008.\n[5] Ryan R. Curtin, James R. Cline, Neil P. Slagle,\nWilliam B. March, P. Ram, Nishant A. Mehta, and\nAlexander G. Gray. MLPACK: A scalable C++ ma-\nchine learning library. Journal of Machine Learning\nResearch, 14:801–805, 2013.\n[6] St ´ephane Dupont, Thierry Ravet, C ´ecile Picard-\nLimpens, and Christian Frisson. Nonlinear dimension-\nality reduction approaches applied to music and textu-\nral sounds. In IEEE International Conference on Mul-\ntimedia and Expo (ICME), 2013.\n[7] Mikael Fernstr ¨om and Eoin Brazil. Sonic browsing:\nAn auditory tool for multimedia asset management. In\nProceedings of the 2001 International Conference on\nAuditory Display, 2001.\n[8] Frederic Font. Design and evaluation of a visualiza-\ntion interface for querying large unstructured sound\ndatabases. Master’s thesis, Universitat Pompeu Fabra,\nMusic Technology Group, 2010.\n[9] Thomas Grill and Arthur Flexer. Visualization of per-\nceptual qualities in textural sounds. In Proceedings of\nthe Intl. Computer Music Conference, ICMC, 2012.\n[10] Thomas Grill, Arthur Flexer, and Stuart Cunningham.\nIdentiﬁcation of perceptual qualities in textural sounds\nusing the repertory grid method. In Proceedings of the\n6th Audio Mostly Conference: A Conference on Inter-\naction with Sound, ACM, 2011.[11] Sebastian Heise, Michael Hlatky, and J ¨orn Loviscach.\nSoundtorch: Quick browsing in large audio collections.\nIn125th Audio Engineering Society Convention, 2008.\n[12] Ianis Lallemand and Diemo Schwarz. Interaction-\noptimized sound database representation. In Proceed-\nings of the 14th International Conference on Digital\nAudio Effects (DAFx-11), 2011.\n[13] Joshua M. Lewis, Laurens van der Maaten, and Vir-\nginia de Sa. A behavioral investigation of dimension-\nality reduction. In N. Miyake, D. Peebles, and R. P.\nCooper, editors, Proceedings of the 34th Annual Con-\nference of the Cognitive Science Society, 2012.\n[14] Kristian Nybo, Jarkko Venna, and Samuel Kaski.\nThe self-organizing map as a visual neighbor retrieval\nmethod. In Proceedings of the 6th International Work-\nshop on Self-Organizing Maps (WSOM), 2007.\n[15] G. Peeters. Sequence representation of music structure\nusing higher-order similarity matrix and maximum-\nlikelihood approach. In Proc. of the Intl. Symposium\non Music Information Retrieval (ISMIR), 2007.\n[16] Kerry Rodden, Wojciech Basalaj, David Sinclair, and\nKenneth Wood. Does organisation by similarity assist\nimage browsing? In Proc. of the SIGCHI Conf. on Hu-\nman Factors in Computing Systems, CHI. ACM, 2001.\n[17] Klaus Schoeffmann, David Ahlstr ¨om, Werner Bailer,\nClaudiu Cob ˆarzan, Frank Hopfgartner, Kevin McGuin-\nness, Cathal Gurrin, Christian Frisson, Duy-Dinh Le,\nManfred Fabro, Hongliang Bai, and Wolfgang Weiss.\nThe video browser showdown: a live evaluation of in-\nteractive video search tools. International Journal of\nMultimedia Information Retrieval, pages 1–15, 2013.\n[18] Rebecca Stewart. Spatial Auditory Display for Acous-\ntics and Music Collections. PhD thesis, School of\nElectronic Engineering and Computer Science Queen\nMary, University of London, 2010.\n[19] Sebastian Stober, Thomas Low, Tatiana Gossen,\nand Andreas N ¨urnberger. Incremental visualization of\ngrowing music collections. In Proceedings of the 14th\nConference of the International Society for Music In-\nformation Retrieval (ISMIR), 2013.\n[20] Colin Ware. Visual Thinking: for Design. Interactive\nTechnologies. Morgan Kaufmann, 2008.\n[21] E. Wold, T. Blum, D. Keislar, and J. Wheaten. Content-\nbased classiﬁcation, search, and retrieval of audio.\nMultiMedia, IEEE, 3(3):27–36, 1996.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n354"
    },
    {
        "title": "Towards Modeling Texture in Symbolic Data.",
        "author": [
            "Mathieu Giraud",
            "Florence Levé",
            "Florent Mercier",
            "Marc Rigaudière",
            "Donatien Thorez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415030",
        "url": "https://doi.org/10.5281/zenodo.1415030",
        "ee": "https://zenodo.org/records/1415030/files/GiraudLMRT14.pdf",
        "abstract": "Studying texture is a part of many musicological analy- ses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature com- monly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to for- malize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an al- gorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evalua- tion of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn.",
        "zenodo_id": 1415030,
        "dblp_key": "conf/ismir/GiraudLMRT14",
        "keywords": [
            "texture",
            "musicological analyses",
            "cognition of musical structures",
            "melody",
            "accompaniment layers",
            "polyphonic data",
            "homorhythmic layers",
            "polyphonic data",
            "parallel motions",
            "ground truth analysis"
        ],
        "content": "TOWARDS MODELING TEXTURE IN SYMBOLIC DATA\nMathieu Giraud\nLIFL,\nCNRS\nUniv. Lille 1, Lille 3Florence Lev ´e\nMIS, UPJV , Amiens\nLIFL, Univ. Lille 1Florent Mercier\nUniv. Lille 1Marc Rigaudi `ere\nUniv. LorraineDonatien Thorez\nUniv. Lille 1\n{mathieu, florence, florent, marc, donatien}@algomus.fr\nABSTRACT\nStudying te\nxture is a part of many musicological analy-\nses. The change of texture plays an important role in the\ncognition of musical structures. Texture is a feature com-\nmonly used to analyze musical audio data, but it is rarely\ntaken into account in symbolic studies. We propose to for-\nmalize the texture in classical Western instrumental music\nas melody and accompaniment layers, and provide an al-\ngorithm able to detect homorhythmic layers in polyphonic\ndata where voices are not separated. We present an evalua-\ntion of these methods for parallel motions against a ground\ntruth analysis of ten instrumental pieces, including the ﬁrst\nmovements of the six quatuors op. 33 by Haydn.\n1. INTRODUCTION\n1.1 Musical Texture\nAccording to Grove Music Online, texture refers to the\nsound aspects of a musical structure. One usually differen-\ntiates homophonic textures (rhythmically similar parts) and\npolyphonic textures (different layers, for example melody\nwith accompaniment or countrapuntal parts). Some more\nprecise categorizations have been proposed, for example\nby Rowell [17, p. 158 – 161] who proposes eight “textural\nvalues”: orientation (vertical / horizontal), tangle (inter-\nweaving of melodies), ﬁguration (organization of music in\npatterns), focus vs. interplay, economy vs. saturation, thin\nvs. dense, smooth vs. rough, and simple vs. complex. What\nis often interesting for the musical discourse is the change\nof texture: J. Dunsby, recalling the natural tendency to con-\nsider a great number of categories, asserts that “one has\nnothing much to say at all about texture as such, since all\ndepends on what is being compared with what” [5].\nOrchestral texture. The term texture is used to describe or-\nchestration, that is the way musical material is layed out on\ndifferent instruments or sections, taking into account regis-\nters and timbres. In his 1955 Orchestration book, W. Piston\npresents seven types of texture: orchestral unison, melody\nand accompaniment, secondary melody, part writing, con-\ntrapuntal texture, chords, and complex textures [15].\nc\rMathieu Giraud, Florence Lev ´e, Florent Mercier, Marc\nRigaudi `ere, Donatien Thorez. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Math-\nieu Giraud, Florence Lev ´e, Florent Mercier, Marc Rigaudi `ere, Donatien\nThorez. “Towards modeling texture in symbolic data”, 15th International\nSociety for Music Information Retrieval Conference, 2014.In 1960, Q. R. Nordgren [13] asks: “Is it possible to\nmeasure texture?”. He proposes to quantify the horizontal\nand vertical relationships of sounds making up the texture\nbeyond the usual homophonic/polyphonic or light/heavy\ncategories. He considers eight features, giving them nu-\nmerical values: the number of instruments, their range,\ntheir register and their spacing, the proportion and reg-\nister of gap, and doubling concentrations with their reg-\nister. He then analyzes eight symphonies by Beethoven,\nMendelssohn, Schumann and Brahms with these criteria,\nﬁnding characteristic differences between those composers.\nNon-orchestral texture. However, the term texture also re-\nlates to music produced by a smaller group of instruments,\neven of same timbre (such as a string quartet), or to mu-\nsic produced by a unique polyphonic instrument such as\nthe piano or the guitar. As an extreme point of view, one\ncan consider texture on a monophonic instrument: a sim-\nple monophonic sequence of notes can sound as a melody,\nbut also can ﬁgure accompaniment patterns such as arpeg-\ngiated chords or Alberti bass.\nTexture in musical analysis. Studying texture is a part\nof any analysis, even if texture often does not make sense\non its own. As stated by J. Levy, “although it cannot ex-\nist independently, texture can make the functional and sign\nrelationships created by the other variables more evident\nand fully effective” [10]. Texture plays a signiﬁcant role\nin the cognition of musical structures. J. Dunsby attributes\ntwo main roles to texture: the illusion it creates and the\nexpectation it arouses from the listeners towards familiar\ntextures [5]. J. Levy shows with many examples how tex-\nture can be a sign in Classic and Early Romantic music,\ndescribing the role of accompaniment patterns, solos and\nunison to raise the attention of the listener before impor-\ntant structural changes [10].\n1.2 Texture and Music Information Retrieval\nTexture was often not as deeply analyzed and formalized\nas other parameters (especially melody or harmony). In\nthe ﬁeld of Music Information Retrieval (MIR), the notion\nof texture is often used in audio analysis, reduced to tim-\nbral description. Any method dealing with audio signals is\nsomewhat dealing with timbre and texture [3, 9]. Based on\naudio texture, there were for example studies on segmenta-\ntion. More generally, the term “sound texture” can be used\nto describe or synthesize non-instrumental audio signals,\nsuch as ambient sounds [18, 19].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n59Among the studies analyzing scores represented by sym-\nbolic data, few of them take texture into account. In 1989,\nD. Huron [7] explains that the three common meanings\nabout the texture term are the volume, the diversity of el-\nements used and the “surface” description, the ﬁrst two\nbeing more easily formalizable. Using a two-dimensional\nspace based on onset synchronization and similar pitch mo-\ntion, he was able to capture four broad categories of tex-\ntures: monophony, homophony, polyphony and heteropho-\nny. He found also that different musical genres occupy a\ndifferent region of the deﬁned space.\nSome of the features of the jSymbolic library, used for\nclassiﬁcation of MIDI ﬁles, concern musical texture [11,\n12]. “[They] relate speciﬁcally to the number of indepen-\ndent voices in a piece and how these voices relate to one\nanother. ” [11, p. 209]. The features are computed on MIDI\nﬁles where voices are separated, and include statistical fea-\ntures on choral or orchestral music organization: maxi-\nmum, average and variability of the number of notes, vari-\nability between features of individual voices (number of\nnotes, duration, dynamics, melodic leaps, range), features\nof the loudest voice, highest and lowest line, simultane-\nity, voice overlap, parallel motion and pitch separation be-\ntween voices.\nMore recently, Tenkanen and Gualda [20] detect articu-\nlative boundaries in a musical piece using six features in-\ncluding pitch-class sets and onset density ratios. D. Rafai-\nlidis and his colleagues segment the score in several textu-\nral streams, based on pitch and time proximity rules [2,16].\n1.3 Contents\nAs we saw above, there are not many studies on modeling\nor automatic analysis of texture. Even if describing musical\ntexture could be done on a local level of a score, it requires\nsome high-level musical understanding. We thus think that\nit is a natural challenge, both for music modeling and for\nMIR studies.\nIn this paper, we propose some steps towards the mod-\neling and the computational analysis of texture in West-\nern classical instrumental music. We choose here not to\ntake into account orchestration parameters, but to focus on\ntextural features given by local note conﬁgurations, taking\ninto account the way these may be split into several lay-\ners. For the same reason, we do not look at harmony or at\nmotives, phrases, or pattern large-scale repetition.\nThe following section presents a formal modeling of the\ntexture and a ground truth analysis of ﬁrst movements of\nten string quartets. Then we propose an algorithm discov-\nering texture elements in polyphonic scores where voices\nare not separated, and ﬁnally we present an evaluation of\nthis algorithm and a discussion on the results.\n2. FORMALIZATION OF TEXTURE\n2.1 Modeling Texture as Layers\nWe choose to model the texture, by grouping notes into sets\nof“layers”, also called “streams”, sounding as a wholegrouped by perceptual characteristics. Auditory stream seg-\nregation was introduced by Bregman, who studied many\nparameters inﬂuencing this segregation [1]. Focusing on\nthe information contained on a symbolic score, notes can\nbe grouped in such layers using perceptual rules [4, 16].\nThe number of layers is not directly the number of ac-\ntual (monophonic) voices played by the instruments. For\ninstance, in a string quartet where all instruments are play-\ning, there can be as few as only one perceived layer, several\nvoices blending in homorhythmy. On the contrary, some\nﬁgured patterns in a unique voice can be perceived as sev-\neral layers, as in a Alberti bass.\nMore precisely, we model the texture in layers accord-\ning to two complementary views. First, we consider two\nmain roles for the layers, that is how they are perceived by\nthe listeners: melodic (mel) layers (dominated by contigu-\nous pitch motion), and accompaniment (acc) layers (dom-\ninated by harmony and/or rhythm). Second, we describe\nhow each layer is composed.\n\u000fA melodic layer can be either a monophonic voice\n(solo), or two or more monophonic voices in ho-\nmorhythmy (h), or within a tighter relation, such as\n(from most generic to most similar) parallel motion\n(p), octave (o) or unison (u) doubling. The h/p/o/u\nrelations do not need to be exact: for example, a par-\nallel motion can be partly in thirds, partly in sixths,\nand include some foreign notes (see Figure 1).\n\u000fAn accompaniment layer can also be described by\nh/p/o/u relations, but it is often worth focusing on its\nrhythmic component: for example, such a layer can\ncontain sustained, sparse or repeated chords, Alberti\nbass, pedal notes, or syncopation.\nThe usual texture categories can then be described as:\n\u000fmel/acc – the usual accompanied melody;\n\u000fmel/mel – two independent melodies (counterpoint,\nimitation...);\n\u000fmel– one melody (either solo, or several voices in\nh/p/o/u relation), no accompaniment;\n\u000facc– only accompaniment, when there is no notice-\nable melody that can be heard (as in some transitions\nfor example).\nThe formalism also enables to describe more layers,\nsuch as mel/mel/mel/mel, acc/acc, or mel/acc/acc.\nLimitations. This modeling of texture is often ambigu-\nous, and has limitations. The distinction between melody\nand accompaniment is questionable. Some melodies can\ncontain repeated notes, arpeggiated motives, and strongly\nimply some harmony. Limiting the role of the accompani-\nment to harmony and rhythm is also over-simpliﬁed. More-\nover, some textural gestures are not modeled here, such as\nupwards or downwards scales. Finally, what Piston calls\n“complex textures” (and what is perhaps the most inter-\nesting), interleaving different layers [15, p. 405], can not\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n60::: ./texture -T truth/sonata-quartet.truth --polyphonic data/Mozart-K157-4.krn --score --no-analysisdefault_files='': ['../../../sonata/data/Beethoven-19-1.krn'], 'bla': ['data/file1.krn', 'data/file2-*.krn'] :: 2014-07-19 11:01:49.599179<Truth 'truth/sonata-quartet.truth'>Mozart-K157-4==== Mozart-K157-4 - Mozart String Quartet n°4, K 157 ====     C Major, sonata form, 4 voices (SATB): h: homorythmy>>> texture  * 1,   75 : mel/acc (SAp / TB)  * 8,   82 : mel/acc (SA / TBp)  * 9,   83 : mel/acc (SAp / T syncopation, B)  * 13,  87 : mel/acc (SAp / TB imitation)  * 19,  93 : mel/acc (SAo / TBh)  * 20,  94 : mel/acc (SAp / TBhr)  * 21,  95 : IMITATION/acc (SA / TB)  * 25,  99 : IMITATION/acc (SA / TB)  *     102 : mel/acc (S / ATB)    * 29, 103 : mel/acc (S / ATBhr)   * 30      : mel/acc (S / ATBh)  *     104 : mel/acc (SAh/ TBh)  * 31, 105 : mel/acc (S / ATBh SPARSE CHORDS)    * 33, 107 : acc/mel/acc (S / ATp / B)  * 35, 109 : acc/mel (SAT / B)  * 36, 110 : acc/mel (SA / TBp)  * 37      : mel/acc (SAp / TBp)  *     111 : mel/acc (S / ATh, B)  * 38, 112 : mel     (S)  * 39, 113 : acc/mel (ST / ABp)  * 41, 115 : mel/acc (S / ATB)    * 42, 116 : mel/acc (S / ATBh SPARSE CHORDS)    * 46, 120 : homorhythmy (SATop, B)  * 49, 120 : mel/acc (SAp / TB)  *     124 : mel/acc (STh / AB)  * 53      : mel/acc (S / ATBh SPARSE CHORDS)  * 63      : acc/mel/acc  (S / ATh / B)  * 67      : acc/mel  (SA / TBh)  * 70      : intensification (?)::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::/dots.dot/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/flags.u3/flags.u3/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / TB)/dots.dot/dots.dot/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/timesig.C44/clefs.C/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/accidentals.sharp/rests.2/noteheads.s2/noteheads.s2/rests.2/rests.2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3mel/acc(SAp / TB imitation)/dots.dot/dots.dot/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/rests.2/rests.2/flags.d3/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/rests.1/noteheads.s2/noteheads.s2/dots.dot/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / T syncopation, B)/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G7/clefs.G/clefs.C/clefs.F/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2mel/acc(SA / TBp)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/flags.d3/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dotmel/acc(SAo / TBh)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/clefs.C/clefs.G/clefs.G14/clefs.F/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/accidentals.sharp/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/dots.dot/noteheads.s2/flags.d3/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2IMITATION/acc(SA / TB)/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/dots.dot/noteheads.s1/rests.0/noteheads.s2/noteheads.s2IMITATION/acc(SA / TB)/rests.2/rests.2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / TBhr)20/clefs.G/clefs.G/clefs.C/clefs.F/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/dots.dot/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/accidentals.sharp/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.d3/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/rests.2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/rests.2/rests.2/rests.2/rests.2/noteheads.s2mel/acc(S / ATBhr)/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBh)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp26/clefs.G/clefs.G/clefs.C/clefs.F/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/dots.dot/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/rests.2/noteheads.s2\n/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2acc/mel(SA / TBp)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/rests.3/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/accidentals.sharpacc/mel(SAT / B)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3acc/mel/acc(S / ATp / B)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s0/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.natural/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/clefs.C/clefs.G/clefs.G31mel/acc(S / ATBh SPARSE CHORDS)/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/rests.3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.2/rests.2/noteheads.s2/rests.2/accidentals.sharp/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/rests.3/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharpacc/mel(ST / ABp)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/accidentals.sharp/noteheads.s1/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/accidentals.sharp/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/rests.1/rests.1/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel    (S)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/clefs.F/noteheads.s2mel/acc(SAp / TBp)37/clefs.G/clefs.G/clefs.C/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/flags.u3/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dotmel/acc(S / ATB)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBh SPARSE CHORDS)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2mel/acc(SAp / TB)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s0/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/clefs.F/clefs.C/clefs.G/clefs.G44/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2homorhythmy(SATop, B)/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/rests.1/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/rests.1/rests.1/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBh SPARSE CHORDS)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s252/clefs.G/clefs.G/clefs.C/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/rests.1/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/rests.2/noteheads.s2/rests.2/rests.2/rests.2/rests.2/noteheads.s2/accidentals.sharp/noteheads.s2/rests.3/rests.3/noteheads.s2acc/mel/acc (S / ATh / B)/rests.2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s259/clefs.G/clefs.G/clefs.F/clefs.C/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/accidentals.sharp/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2intensification(?)/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2acc/mel (SA / TBh)/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/rests.3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/clefs.C/clefs.F/noteheads.s266/clefs.G/clefs.G/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.natural/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.3/rests.3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/accidentals.flat/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/noteheads.s272/clefs.G/clefs.G/clefs.C/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/rests.1/rests.1/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/flags.u3/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dotmel/acc(SAp / TB)/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/accidentals.natural\n/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/rests.1/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / T syncopation, B)/flags.d3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2mel/acc(SA / TBp)/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/flags.d3/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/dots.dot/dots.dot/noteheads.s2/rests.2/rests.2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s278/clefs.G/clefs.G/clefs.C/clefs.F/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/flags.d3/noteheads.s2/rests.2/rests.2/noteheads.s2mel/acc(SAp / TB imitation)/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/rests.2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/clefs.F/clefs.C/clefs.G/clefs.G85/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatIMITATION/acc(SA / TB)/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dotmel/acc(SAo / TBh)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/clefs.C/clefs.G/clefs.G92/noteheads.s2/clefs.F/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / TBhr)/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBhr)/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATB)/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.C/clefs.G/clefs.F99IMITATION/acc(SA / TB)/clefs.G/dots.dot/rests.2/noteheads.s2/rests.2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/dots.dot/rests.2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2acc/mel/acc(S / ATp / B)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBh SPARSE CHORDS)/rests.2/rests.2/rests.2/noteheads.s2/rests.2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2acc/mel(SA / TBp)/noteheads.s1/noteheads.s2/accidentals.sharp/noteheads.s2/rests.3/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/clefs.Cmel/acc(SAh/ TBh)104/clefs.G/clefs.G/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2acc/mel(SAT / B)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATBh SPARSE CHORDS)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2acc/mel(ST / ABp)/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.3/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel    (S)/clefs.F/clefs.C/clefs.G/clefs.G111mel/acc(S / ATh, B)/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/rests.1/rests.1/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(S / ATB)/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s1/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s0mel/acc(STh / AB)/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2118/clefs.G/clefs.G/clefs.C/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/rests.2/rests.2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2mel/acc(SAp / TB)/noteheads.s21, 75 : mel/acc (SAp / TB)\n8, 82 : mel/acc (SA / TBp)\n9, 83 : mel/acc (SAp / T syncopation, B)\n13, 87 : mel/acc (SAp / TB imitation)\n19, 93 : mel/acc (SAo / TBh)\n20, 94 : mel/acc (SAp / TBhr)\n21, 95 : imitation/acc (SA / TB)\n25, 99 : imitation/acc (SA / TB)\n102 : mel/acc (S / ATB)\n29, 103 : mel/acc (S / ATBhr)\n30 : mel/acc (S / ATBh)\n104 : mel/acc (SAh/ TBh)\n31, 105 : mel/acc (S / ATBh sparse chords)\n33, 107 : acc/mel/acc (S / ATp / B)\n...\nFigure 1. Beginning of the string quartet K. 157 no. 4 by W. A. Mozart, with the ground truth analysis describing textures.\nWe label as S / A / T / B (sopran / alt / tenor / bass) the four instruments (violin I / violin II / viola / cello). The ﬁrst\neight measures have a melodic layer “SAp” made by a parallel motion (with thirds), however the parallel motion has some\nexceptions (unison on c, strong beat on measures 1 and 8, and small interruption at the beginning of measure 5).\nalways be modeled by this way. Nevertheless, the above\nformalization is founded for most music of the Classical\nand of the Romantic period, and corresponds to a way of\nmelody/accompaniment writing.\n2.2 A Ground Truth for Texture\nWe manually analyzed the texture on 10 ﬁrst movements\nof string quartets: the six quartets op. 33 by Haydn, three\nearly quartets by Mozart (K. 80 no. 1, K. 155 no. 2 and\nK. 157 no. 4), and the quartet op. 125 no. 1 by Schubert.\nThese pieces covered the textural features we wanted to\nelucidate. We segmented each piece into non-overlapping\nsegments based only on texture information, using the for-\nmalism described above.\nIt is difﬁcult to agree on the signiﬁance on short seg-\nments and on their boundaries. Here we choose to report\nthe texture with a resolution of one measure: we consider\nonly segments during at least one measure (or ﬁlling the\nmost part of the measure), and round the boundaries of\nthese segments to bar lines.\nWe identiﬁed 691 segments in the ten pieces, and Ta-\nble 1 details the repartition of these segments. The ground\ntruth ﬁle is available at www.algomus.fr/truth, and\nFigure 1 shows the analysis for the beginning of the string\nquartet K. 157 no. 4 by Mozart.\nThe segments are further precised by the involved voices\nand the h/p/o/u relations. For example, focusing on the\nmost represented category “mel/acc”, there are 254 seg-\nments labelled either “S / ATB” or “S / ATBh” (melodic\nlayer at the ﬁrst violin) and 81 segments labelled “SAp /\nTB” or “SAp / TBh” (melodic layer at the two violins, in a\nparallel move). Note that h/p/o/u relations were evaluated\nhere in a subjective way. The segments may contain some\nsmall interruptions that do not alter the general perception\nof the h/p/o/u relation.\n3. DISCOVERING SYNCHRONIZED LAYERS\nWe now try to provide a computational analysis of tex-\nture starting from a polyphonic score where voices are not\nseparated. A ﬁrst idea is to ﬁrst segment the score intolayers by perception principles, and then to try to qual-\nify some of these layers. One can for example use the\nalgorithm of [16] to segment the musical pieces into lay-\ners (called “streams”). This algorithm relies on a distance\nmatrix, which tells for each possible pair of notes whether\nthey are likely to belong to the same layer. The distance\nbetween two notes is computed according to their syn-\nchronicity, pitch and onset proximity (among others cri-\nteria); then for each note, the list of its k-nearest neigh-\nbors is established. Finally, notes are gathered in clusters.\nA melodic stream can be split into several small chunks,\nsince the diversity of melodies does not always ensure co-\nherency within clusters; working on larger layers encom-\npass them all. Even if this approach produces good re-\nsults in segmentation, many layers are still too scattered\nto be detected as full melodic or accompaniment layers.\nNonetheless, classiﬁcation algorithms could label some of\nthese layers as melodies or accompaniments, or even detect\nthe type of the accompaniment.\nThe second idea, that we will develop in this paper, is\ntodetect directly noteworthy layers from the polyphonic\ndata. Here, we choose to focus on perceptually signiﬁcant\nrelations based on homorhythmic features. The following\nparagraphs deﬁne the notion of synchronized layers, that is\nsequences of notes related by some homorhythmy relation\n(h/p/o/u), and show how to compute them.\n3.1 Deﬁnitions: Synchronized Layers\nAnotenis given as a triplet (n:pitch;n:start;n:end ),\nwheren:pitch belongs to a pitch scale (that can be deﬁned\ndiatonically or by semitones), and n:start andn:end are\ntwo positions with n:start < n:end. Two notes nandm\naresynchronized (denoted by n\u0011hm) if they have the\nsame start and the same end.\nAsynchronized layer (SL) is a set of two sequences\nof consecutive synchronized notes (in other words, these\nsequences correspond to two “voices” in homorhythmy).\nFormally, two sequences of notes n1;n2:::nkandm1;m2:::mk\nform a synchronized layer when:\n\u000ffor alliinf1;:::;kg,ni:start =mi:start\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n61tonality length mel/acc mel/mel acc/mel/acc acc/mel mel acc others h p o u\nHaydn op. 33 no. 1 B minor 91m 38 0 8 1 0 0 5 19 21 1 0\nHaydn op. 33 no. 2 E-ﬂat major 95m 37 0 2 4 0 0 7 34 13 0 0\nHaydn op. 33 no. 3 C major 172m 68 0 0 0 3 13 6 29 50 1 0\nHaydn op. 33 no. 4 B-ﬂat major 90m 25 0 1 0 0 0 6 16 6 0 0\nHaydn op. 33 no. 5 G major 305m 68 0 3 4 7 0 5 56 45 6 0\nHaydn op. 33 no. 6 D major 168m 58 0 1 3 15 0 29 43 42 0 2\nMozart K. 80 no. 1 G major 67m 36 4 6 0 2 0 3 5 33 3 0\nMozart K. 155 no. 2 D major 119m 51 0 0 0 1 0 0 21 32 4 1\nMozart K. 157 no. 4 C major 126m 29 0 3 6 2 0 7 18 22 2 0\nSchubert op. 125 no. 1 E-ﬂat major 255m 102 0 0 0 20 2 0 54 8 46 2\n1488m 512 4 24 18 50 15 68 295 272 63 5\nTable 1. Number of segments in the ground truth analysis of the ten string quartets (ﬁrst movements), and number of\nh/p/o/u labels further describing these layers.\n\u000ffor alliinf1;:::;kg,ni:end=mi:end\n\u000ffor alliinf1;:::;k\u00001g,ni:end=ni+1:start\nThis deﬁnition can be extended to any number of voices.\nAs p/o/u relations have a strong musical signiﬁcation, we\nwant to be able to enforce them. One can thus restrain the\nrelation\u0011h, considering the pitch information:\n\u000fwe denoten\u0011\u000emif the interval between the two\nnotesnandmis\u000e. The nature of the interval \u000ede-\npends on the pitch model: for example, the interval\ncan be diatonic, such as in “third” (minor or major),\nor an approximation over the semitone information,\nsuch as in “3 or 4 semitones”. Some synchronized\nlayers with\u0011\u000erelations correspond to parallel mo-\ntions;\n\u000fwe denoten\u0011omif notesnandmare separated\nby any number of octaves;\n\u000fwe denoten\u0011umwhere there is an exact equality\nof pitches (unison).\nGiven a relation\u00112f\u0011h;\u0011\u000e;\u0011o;\u0011ug, we say that a\nsynchronized layer respects the relation \u0011if its notes are\npairwise related according to this relation. The relation \u0011h\nis an equivalence relation, but the restrained relations do\nnot need to be equivalence relations: Some \u0011\u000erelations\nare not transitive.\nFor example, in Figure 1, there is between voices S and\nA (corresponding to violins I and II), in the ﬁrst two mea-\nsures:\n\u000fa synchronized layer (\u0011 h) on the two measures;\n\u000fand a synchronized layer (\u0011 third) on the two mea-\nsures, except the ﬁrst note.\nNote that this does not correspond exactly to the “musical”\nground truth (parallel move on at least the ﬁrst four mea-\nsures) because of some rests and of the ﬁrst synchronized\nnotes that are not in thirds.\nA synchronized layer is maximal if it is not strictly in-\ncluded in another synchronized layer. Note that two maxi-\nmal synchronized layers can be overlapping, if they are not\nsynchronized. Note also that the number of synchronized\nlayers may grow exponentially with the number of notes.3.2 Detection of a Unique Synchronized Layer\nA very noticeable textural effect is when allvoices use the\nsame texture at the same time. For example, a sudden strik-\ning unison raises the listener’s attention. We can ﬁrst check\nif all notes in a segment of the score belong to a unique syn-\nchronized layer (within some relation). For example, we\nconsider that all voices are in octave doubling or unison if\nit lasts at least two quarters.\n3.3 Detection of Maximal Synchronized Layers\nIn the general case, the texture has several layers, and the\ngoal is thus to extract layers using some of the notes. Re-\nmember that we work on ﬁles where polyphony is not sepa-\nrated into voices: moreover, it is not always possible to ex-\ntract voices from a polyphonic score, for example on piano\nmusic. We want to extract maximal synchronized layers.\nHowever, as their number may grow exponentially with\nthe number of notes, we will compute only the start and\nend positions of maximal synchronized layers.\nThe algorithm is a kind of 1-dimension interval chain-\ning [14]. The idea is as follows. Recursively, two voices\nn1;:::;nkandm1;:::;mkare synchronized if and only if\nn1;:::;nk\u00001andm1;:::;mk\u00001are synchronized, nkand\nmkare synchronized and ﬁnally nk\u00001:end =nk:start .\nFormally, the algorithm is described by the following:\nStep 1. Compute a table with left-maximal SL. Build the table\nleftmost start\u0011[j]containing, for each ending position j, the left-\nmost starting position of a SL respecting \u0011ending in j. This can\nbe done by dynamic programming with the following recurrence:\nleftmost start\u0011[j] =8\n<\n:minfleftmost start\u0011[i]ji2S\u0011(j)g\nifS\u0011(j)is not empty\njifS\u0011(j)is empty\nwhere S\u0011(j)is the set of all starting positions of synchronized\nnotes ending at jrespecting the relation \u0011:\nS\u0011(j) =\u001a\nn:start\f\f\f\fthere are two different notes n\u0011m\nsuch that n:end =j\u001b\nStep 2. Output only (left and right) maximal SL. Output (i; j)\nwithi=leftmost start\u0011[j]for each j, such that j= max\nfjojleftmost start\u0011[jo] =leftmost start\u0011[j]g\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n62                             1                   2                   3                   4                   5                   6                    1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4            SA- SA-  SA---  SA- SA- SA---- SA-  SB-                 AB             TB       AT        SB---- SA----              AB       AT                         TB              SA  SA                    AB-             SA                AT  SB-                                                                              SB-                                                     ST--                                                     7                   8                   9                   10                  11                  12                      5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5                                                                                                                                                         TB              SA- SA-  SA---  SA- SA- SA---- SA-  SB-  TB-  TB        AB              AT      AT        SB ST--------                   TB--                        TB              SA  SA                    AB-                               SB- SB                                                                            SB-                  SA                               ST  AT-         Figure 2. Result on the parallel move detection on the ﬁrst movement of the string quartet K. 157 no. 4 by Mozart. The top\nlines display the measure numbers. The algorithm detects 52 synchronized layers respecting the \u0011prelation. 39 of these 52\nlayers overlap layers identiﬁed in the truth with p/o/u relations. The parallel motions are further identiﬁed by their voices\n(S / A / T / B), but this information is not used in the algorithm which works on non-separated polyphony.\nThe ﬁrst step is done in O(nk)time, where nis the\nnumber of notes and k\u0014nthe maximal number of simul-\ntaneously sounding notes, so in O(n2)time. The second\nstep is done in O(n)time by browsing from right to left\nthe table leftmost start\u0011, outputing values iwhen they are\nseen for the ﬁrst time.\nTo actually retrieve intervals, we can store in the ta-\nbleleftmost start\u0011[j]a pair (i;`), where `is the list of\nnotes/intervals from which the set of SL can be built (this\nset may be very large, but not `). The time complexity is\nnowO(n(k +w)), wherew\u0014nis the largest possible\nsize of`. Thus the time complexity is still in O(n2). This\nallows, in the second step, to ﬁlter the SL candidates ac-\ncording to additional criteria on `.\nNote ﬁnally that the deﬁnition of synchronized layer can\nbe extended to include consecutive notes separated with\nrests. The same algorithm still applies, but the value of k\nrises to the maximum number of notes that can be linked\nin that way.\n4. RESULTS AND DISCUSSION\nWe tested the proposed algorithm to look for synchronized\nlayers respecting\u0011\u000erelation (constant pitch interval, in-\ncluding parallel motion) on the ten pieces of our corpus\ngiven as .krn Humdrum ﬁles [8]. Although the pieces are\nstring quartets, we consider them as non-separated poly-\nphonic data, giving as input to the algorithm a single set of\nnotes. The algorithm ﬁnds 434 layers. Figure 2 shows an\nexample of the output of the algorithm. Globally, on the\ncorpus, the algorithm labels 797 measures (that is 53.6%\nof the length) as synchronized layers.\nEvaluation against the truth. There are in the truth 354\nlayers with p/o/u relations: mainly parallel moves, and\nsome octave doubling and unisons. As discussed earlier,\nthese layers reported in the truth correspond to a musical\ninterpretation: they are not as formalized as our deﬁnition\nof synchronized layer. Moreover, less information is pro-\nvided by the algorithm than in the ground truth: when a\nparallel motion is found, the algorithm cannot provide at\nwhich voice/instrument it appears, since we worked from\npolyphonic data with no voice separation.\nNevertheless, we compared the layers predicted by the\nalgorithms with the ones of the truth. Results are summa-\nrized on Table 2. A computed layer is marked as true pos-\nitive (TP) as soon as it overlaps a p/o/u layer of the truth.356 of the 434 computed synchronized layers are over-\nlapping the p/o/u layers of the truth, thus 82.0% of the com-\nputed synchronized layers are (at least partially) musically\nrelevant. These 356 layers map to 194 p/o/u layers in the\ntruth (among 340, that is a sensitivity of 58.0%): a major-\nity of the parallel moves described in the truth are found\nby the algorithm.\nFigure 3. Haydn, op. 33 no. 6, m. 28-33. The truth con-\ntains four parallel moves.\nMerged parallel moves. If one restricts to layers where\nborders coincide with the ones in the truth (same start,\nsame end, with a tolerance of 2 quarters), the number of\ntruth layers found falls from 194 to 117. This is because\nthe algorithm often merge consecutive parallel moves. An\nexample of this drawback is depicted on Figure 3. Here a\nmelody is played in imitation, resulting in parallel moves\ninvolving all voices in turn. The algorithm detects a unique\nsynchronized layer, which corresponds to a global percep-\ntion but gives less information about the texture. We should\nremember here that the algorithm compute boundaries of\nsynchronized layers and not actual instances, which would\nrequire some sort of voice separation and possibly generate\na large number of instances.\nFalse positives. Only 78 false positives are found by the\nalgorithm. Many false positives (compared to the truth) are\nparallel moves detected inside a homorhythmy \u0011hrelation\nbetween 3 ou 4 voices. In particular, the algorithm detects\na parallel move as soon as there are sequences of repeated\nnotes in at least two voices. This is the case in in op. 33 no.\n4 by Haydn which contains many homorhythmies in re-\npeated notes, for which we obtain 30 false positives. Even\nfocusing on layers with a real “move”, false positive could\nalso appear between a third voice and two voices with re-\npeated notes. Further research should be carried to discard\nthese false positives either in the algorithm or at a later ﬁl-\ntering stage.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n63hits length hits TP FP truth-overlap truth-exact\nHaydn op. 33 no. 1 40m (44%) 37 32 (86.5%) 5 14 / 22 (63.6%) 7 / 22\nHaydn op. 33 no. 2 21m (22%) 17 15 (88.2%) 2 7 / 13 (53.9%) 7 / 13\nHaydn op. 33 no. 3 73m (42%) 48 44 (91.7%) 4 27 / 51 (52.9%) 15 / 51\nHaydn op. 33 no. 4 19m (21%) 47 17 (36.2%) 30 5 / 6 (83.3%) 3 / 6\nHaydn op. 33 no. 5 235m (77%) 58 47 (81.0%) 11 27 / 51 (52.9%) 11 / 51\nHaydn op. 33 no. 6 63m (37%) 24 21 (87.5%) 3 19 / 44 (43.2%) 11 / 44\nMozart K. 80 no. 1 45m (67%) 27 26 (96.3%) 1 20 / 36 (55.6%) 14 / 36\nMozart K. 155 no. 2 76m (64%) 46 44 (95.7%) 2 27 / 37 (73.0%) 15 / 37\nMozart K. 157 no. 4 62m (49%) 52 39 (75.0%) 13 15 / 24 (62.5%) 8 / 24\nSchubert op. 125 no. 1 163m (64%) 78 71 (91.0%) 7 33 / 56 (58.9%) 20 / 56\n797m (54%) 434 356 (82.0%) 78 194 / 340 (57.1%) 111 / 340\nTable 2. Evaluation of the algorithm on the ten string quartets of our corpus. The columns TP and FP show respectively\nthe number of true and false positives, when comparing computed parallel moves with the truth. The columns truth-overlap\nshows the number of truth parallel moves that were matched by this way. The column truth-exact restricts these matchings\nto computed parallel moves for which borders coincide to the ones in the truth (tolerance: two quarters).\n5. CONCLUSION AND PERSPECTIVES\nWe proposed a formalization of texture in Western classi-\ncal instrumental music, by describing melodic or accom-\npaniment “layers” with perceptive features (h/p/o/u rela-\ntions). We provided a ﬁrst algorithm able to detect some\nof these layers inside a polyphonic score where tracks are\nnot separated, and tested it on 10 ﬁrst movements of string\nquartets. The algorithm detects a large part of the parallel\nmoves found by manual analysis. We believe that other al-\ngorithms implementing textural features, beyond h/p/o/u\nrelations, should be designed to improve computational\nmusic analysis. The corpus should also be extended, for\nexample with music from other periods or piano scores.\nFinally, we believe that this search of texture, combined\nwith other elements such as patterns and harmony, will im-\nprove algorithms for music structuration. The ten pieces of\nour corpus have a sonata form structure. The tension cre-\nated by the exposition and the development is resolved dur-\ning the recapitulation, and textural elements contribute to\nthis tension and its resolution [10]. For example, the medial\ncaesura (MC), before the beginning of theme S, has strong\ntextural characteristics [6]. Textural elements predicted by\nalgorithms could thus help the structural segmentation.\n6. REFERENCES\n[1] A. S. Bregman. Auditory scene analysis. Bradford,\nCambridge, 1990.\n[2] E. Cambouropoulos. V oice separation: theoretical, per-\nceptual and computational perspectives. In Int. Conf.\non Music Perception and Cognition (ICMPC), 2006.\n[3] R. B. Dannenberg and M. Goto. Handbook of Signal\nProcessing in Acoustics, chapter Music Structure Anal-\nysis, pages 305–331. Springer, 2008.\n[4] D. Deutsch and J. Feroe. The internal representation of\npitch sequences in tonal music. Psychological Review,\n88(6):503–522, 1981.\n[5] J. M. Dunsby. Considerations of texture. Music and let-\nters, 70(1):46–57, 1989.\n[6] J. Hepokoski and W. Darcy. The medial caesura and its\nrole in the eighteenth-century sonata exposition. Music\nTheory Spectrum, 19(2):115–154, 1997.[7] D. Huron. Characterizing musical textures. In Int.\nComputer Music Conf. (ICMC), pages 131–134, 1989.\n[8] D. Huron. Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons.\nComputer Music J., 26(2):11–26, 2002.\n[9] A. Klapuri and M. Davy. Signal Processing Methods\nfor Music Transcription. Springer, 2006.\n[10] J. M. Levy. Texture as a sign in classic and early ro-\nmantic music. J. of the American Musicological Soci-\nety, 35(3):482–531, 1982.\n[11] C. McKay. Automatic music classiﬁcation with jMIR.\nPhD thesis, McGill University, 2010.\n[12] C. McKay and I. Fujinaga. jSymbolic: A feature ex-\ntractor for MIDI ﬁles. In Int. Computer Music Conf.\n(ICMC), pages 302–305, 2006.\n[13] Q. R. Nordgren. A measure of textural patterns and\nstrengths. J. of Music Theory, 4(1):19–31, Apr. 1960.\n[14] E. Ohlebusch and M. I. Abouelhoda. Handbook of\nComputational Molecular Biology, chapter Chain-\ning Algorithms and Applications in Comparative Ge-\nnomics. 2005.\n[15] W. Piston. Orchestration. Norton, 1955.\n[16] D. Rafailidis, A. Nanopoulos, Y . Manolopoulos, and\nE. Cambouropoulos. Detection of stream segments in\nsymbolic musical data. In Int. Conf. on Music Informa-\ntion Retrieval (ISMIR), pages 83–88, 2008.\n[17] L. Rowell. Thinking about Music: An Introduction to\nthe Philosophy of Music. Univ. of Massachusetts, 1985.\n[18] N. Saint-Arnaud and K. Popat. Computational audi-\ntory scene analysis. chapter Analysis and Synthesis of\nSound Textures, pages 293–308. Erlbaum, 1998.\n[19] G. Strobl, G. Eckel, and D. Rocchesso. Sound texture\nmodeling: a survey. In Sound and Music Computing\n(SMC), 2006.\n[20] A. Tenkanen and F. Gualda. Detecting changes in mu-\nsical texture. In Int. Workshop on Machine Learning\nand Music, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n64"
    },
    {
        "title": "Towards Automatic Content-Based Separation of DJ Mixes into Single Tracks.",
        "author": [
            "Nikolay Glazyrin"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415566",
        "url": "https://doi.org/10.5281/zenodo.1415566",
        "ee": "https://zenodo.org/records/1415566/files/Glazyrin14.pdf",
        "abstract": "DJ mixes and radio show recordings constitute an impor- tant and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural seg- mentation, this problem has a number of distinctive fea- tures that make difficulties for structural segmentation al- gorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usu- ally comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available times- tamped DJ mix recordings of various styles.",
        "zenodo_id": 1415566,
        "dblp_key": "conf/ismir/Glazyrin14",
        "keywords": [
            "DJ mixes",
            "radio show recordings",
            "continuous DJ mix separation",
            "single tracks",
            "timestamping a mix",
            "music and data source",
            "structural segmentation",
            "distinctive features",
            "algorithm",
            "content-based data"
        ],
        "content": "TOWARDS AUTOMATIC CONTENT-BASED SEPARATION OF DJ MIXES\nINTO SINGLE TRACKS\nNikolay Glazyrin\nUral Federal University\nnglazyrin@gmail.com\nABSTRACT\nDJ mixes and radio show recordings constitute an impor-\ntant and underexploited music and data source. In this\npaper we try to approach the problem of separation of a\ncontinuous DJ mix into single tracks or timestamping a\nmix. Sharing some aspects with the task of structural seg-\nmentation, this problem has a number of distinctive fea-\ntures that make difﬁculties for structural segmentation al-\ngorithms designed to work with a single track. We use the\ninformation derived from spectrum data to separate tracks\nfrom each other. We show that the metadata that usu-\nally comes with DJ mixes can be exploited to improve the\nseparation. An iterative algorithm that can consider both\ncontent-based data and user provided metadata is proposed\nand evaluated on a collection of freely available times-\ntamped DJ mix recordings of various styles.\n1. INTRODUCTION\nDJ mixes provide a great source of music data, which does\nnot gain much attention from the MIR community yet. The\nwork by Kell and Tzanetakis [6], which gives an analysis\nof track selection and ordering in DJ mixes is one of the\nfew exceptions.\nBesides playing in clubs many DJs nowadays produce\nweekly radio shows with latest and greatest and sometimes\nexclusive tracks. These shows are often freely available\nthrough the internet and are very popular among electronic\nmusic lovers. Tracklists for the shows are often provided\nby DJs themselves or by their fans.\nFor many people it is important to know which track is\nplaying now. The cue sheet ﬁle format [2] suits perfectly to\ncarry this kind of information. It was designed to describe\nhow the tracks on CD are laid out, but later it was supported\nby many audio players and CD burning software. There\nare communities, such as http://cuenation.com or\nhttp://themixingbowl.org, which bring together\nthe people who create cue sheets for DJ mixes and radio\nshows. But the wiki page [1] on the ﬁrst site says nothing\nabout any tools for automatical or semi-automatical gener-\nation of cue sheets.\nc\rNikolay Glazyrin.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Nikolay Glazyrin. “Towards Auto-\nmatic Content-Based Separation of DJ Mixes into Single Tracks”, 15th\nInternational Society for Music Information Retrieval Conference, 2014.The most time consuming part of this process is ﬁnd-\ning the moments when one track gives place to another.\nThis may be a big problem for an untrained listener, be-\ncause making smooth transitions between tracks is one of\nthe skills every DJ should have. For a trained person it is\nnot so hard, but to ﬁnd a precise position of a transition\none has to listen carefully through dozens of seconds of\nthe audio. A tool that can propose most probable transi-\ntion positions can facilitate this task. Such a tool can also\nbe used by DJs who upload their mixes to special sharing\nservices or online radio stations. These services will be\nable to timestamp the mix automatically instead of forcing\nthe uploader to do this. The timestamps may be then used\nto provide fast access to particular tracks within the mix\nand to easily share previews of unreleased tracks played\nin radio shows. Timestamped recordings of DJ mixes can\nbe used by recommendation systems to calculate content-\nbased features and relate them to sequential tracks.\nThe task of DJ mix separation is essentially the task of\naudio segmentation, so the concepts and approaches can\nbe shared between these tasks. But some conditions and\nrequirements make them different. These differences will\nbe discussed in section 2. In section 3 we describe the pro-\nposed method to separate tracks in DJ mix recordings. In\nsection 4 we describe the experiments and the evaluation\nmethodology. Finally, in section 5 we conclude and for-\nmulate open problems and directions for future work.\n2. PROBLEM FORMULATION AND RELATED\nWORK\nMusic structural segmentation is a very popular and elab-\norated task. Paulus et al. in [9] distinguish three differ-\nent classes of music segmentation methods. Repetition-\nbased methods try to identify recurring patterns. Novelty-\nbased methods try to ﬁnd transitions between contrasting\nparts. Homogenity-based methods, contrary to novelty-\nbased ones, try to determine fragments that are consistent\nwith respect to some characteristic. Combined methods\nhave also been proposed. Some recent ones try to com-\nbine novelty-based and homogenity-based approaches [4]\nor combine novelty-based approach with harmonical infor-\nmation in a joint probabilistic model [10].\nA DJ mix can be viewed as a very long composition of\nindividual tracks. These tracks constitute the segments in\nour task. It is important that no track can occur more than\nonce within a typical mix. So repetition-based methods are\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n149not suitable at the level of tracks.\nNovelty-based approach seems to be the most suitable\nfor track boundaries detection. Algorithms that implement\nthis approach generally have 2 main steps: segmentation\nand grouping.\nSegmentation is usually done using an intermediate rep-\nresentation in the form of self-similarity matrix (or self-\ndistance matrix). Since the original audio is not very infor-\nmative, it needs to be transformed into a sequence of fea-\nture vectors, for which this matrix is calculated. The list\nof features often used for this includes MFCCs, constant-\nQ spectrum, various low-level spectrum features, such as\nspectral centroid, spectral spread and others.\nThe most popular method of obtaining initial segmenta-\ntion from a self-similarity matrix was proposed by Foote [3].\nIt is based on so called checkerboard novelty kernels, which\nare essentially an M\u0002Mmatrix with checkerboard-like\nstructure. Novelty estimations can be obtained by con-\nvolving this kernel along the main diagonal of the self-\nsimilarity matrix. Peaks of the resulting novelty function\nprovide the initial segment borders.\nHomogenity-based methods come up as a direct con-\ntinuation of this novelty-based segmentation. They group\nsimilar segments together. A good review of the whole\nvariety of methods can be found in [9]. Many of them per-\nform clustering of segments, e.g. [7], [5]. Any information\nabout the desired result can be helpful at this stage to build\nthe most effective grouping procedure.\nIn case of DJ mix separation the grouping procedure be-\ncomes especially important. It is quite common for dance\ncompositions to have a so called “break” in the middle,\nwhere the sound can change dramatically. Such breaks\nshould be overcome to properly detect track boundaries.\nAt the same time, two adjacent segments that belong to\ndifferent tracks should not be joined.\nA typical DJ mix lasts considerably longer than a typi-\ncal musical composition. So the method must be able to\nwork with recordings that span hours of audio. On the\nother hand, this loosens the requirements to border detec-\ntion: an error of seconds or sometimes even tens of sec-\nonds can be acceptable. Even humans can have different\nopinions about one exact moment when a track has tran-\nsitioned to the next one. An interesting task of detecting\ntransition periods (where two or more tracks are playing\nsimultaneously) comes up here, but we don’t consider it in\nthis paper. Marolt in [8] works with similar time scale and\nboundaries requirements, but with a limited set of possible\nsegment types that sound quite differently.\nTransitions can vary signiﬁcantly for different music\nstyles. It is more likely to ﬁnd sharp cuts in drum’n’bass\nmixes, than in deep house mixes, which tend to have long\ngradual transitions. Average track length is also dependent\non music style. But these are generally not the strict rules.\nRadio shows often have an intro, which is played in the\nbeginning and often becomes a part of the tracklist. Jin-\ngles, interludes or talks where the music gets faded can\noccur at random places within a recording. But it is not\nrequired to discriminate them, as they usually don’t get in-cluded into tracklists.\nThe existence of tracklists also makes a great difference\nfrom structural segmentation task. It can be seen from the\npotential applications described in section 1, that the sep-\naration of a DJ mix is not much valuable per se. But it\nbecomes really useful when it can be connected with meta-\ndata: artist name and track title. Because this metadata is\noften available, it can also be used in the algorithm. For\nexample, the information about the number of segments\nin the separation gives a barrier to segmentation and/or\ngrouping process. And if a large music base is available\nto the algorithm, parts of a mix can be matched to corre-\nsponding music recordings to provide even better estima-\ntion of track borders.\nThere may be the cases where matching is not possi-\nble though. Sometimes DJs play tracks that are not yet\nreleased ofﬁcially, and therefore cannot appear in any cat-\nalogue or database. Some tracks never get released ofﬁ-\ncially. Some tracks have been released years ago, and it’s\nalmost impossible to obtain rights on them or ﬁnd them in\nany database. That is why the development of the informed\nautomatic DJ mix separation system cannot be reduced to\na number of calls to track identiﬁcation software.\nTherefore, further we will suppose that there is a track-\nlist available for a mix recording, but not the timestamps,\nand no identiﬁcation software is available. And the task\nwill be to determine those timestamps based on the audio\ndata and the information from the tracklist, or to align the\nmix tracklist to the audio. The authors are not informed\nabout any works on this task existing at the moment.\n3. SYSTEM DESCRIPTION\nWe adopt the approach based on novelty-based segmenta-\ntion followed by grouping of similar segments.\n3.1 Features\nConstant-Q log-spectrograms are calculated at ﬁrst for au-\ndio recordings, which sampling frequency has been left at\nthe default value of 44100 Hz. We used Constant Q plu-\ngin from Queen Mary vamp plugin set1with the follow-\ning parameters: step size and block size are both equal to\n16384 samples (0.37 s), 12 components per octave, span-\nning MIDI pitches from 36 to 84 (65 to 936 Hz) with tun-\ning frequency of 440 Hz. A relatively large block size and\nzero overlap have been chosen because of the large time\nscale and to speed up computations. Low frequencies are\ncaptured, because electronic dance music often has very\naccented bass that changes from track to track. The upper\nfrequency limit has been chosen rather arbitrarily, and we\ndo not investigate its inﬂuence in this study.\nA sliding 2D median ﬁlter is then applied to spectro-\ngram with window size (31, 1) (which corresponds to 11.5\nseconds and 1 spectral component) to smooth it.\n1http://www.vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1503.2 Segmentation\nTo accelerate calculations, the self-distance matrix is cal-\nculated for a spectrogram with 10 times less resolution by\ntime axis (3.7 s per column), where each 10 sequential\ncolumns of original spectrogram are replaced with their\naverage. We also restricted it to only include cosine dis-\ntances between segments which are no more than 10 min-\nutes apart from each other, because it is very unlikely to\nmeet a track that lasts longer than that in a DJ mix.\nNovelty score is then calculated from the self-distance\nmatrix using the checkerboard kernels with gaussian taper\nproposed in [3]. We used relatively small kernels of size 16\n(composed of 4 squares of size 8\u00028). All the peaks of the\nresulting novelty function form the initial set of borders.\n3.3 Clustering\nHere we ﬁnd a use for the information from the mix track-\nlist. The total number of tracks provides the desired num-\nber of clusters. This is an imporant advantage over the tra-\nditional segmentation task, where the number of segments\nis unknown. On the other hand, there is a very strong re-\nquirement to the borders between segments. If one true\nborder is not detected or one false border is detected in\nthe beginning of the mix, all the subsequent tracks become\nmisaligned with the real audio, even if all the other borders\nare detected perfectly.\nAnother piece of information from the tracklist that can\nbe used here is the presence of intro and outro. Many ra-\ndioshows and regular podcasts have such an intro, fewer\nones have also an outro. These segments are relatively\nshort (shorter than 1 minute), but are often included in\ntracklists. A reasonable assumption is that if the name of\nthe ﬁrst track contains the string intro and/or the name of\nthe last track contains the string outro, then an intro and/or\nan outro should be expected. A good clustering algorithm\ncould be able to detect them automatically, but we add a\nspecial handling for these cases. If an intro is expected,\namong the novelty function peaks during the ﬁrst 60 sec-\nonds of audio the highest one is selected and declared as\nthe intro right border. The same is done at the end of the\nrecording if an outro is expected there.\nFor the remainder of the recording an iterative cluster-\ning procedure is applied. Within each segment the average\nof all its feature vectors is calculated and normalized by di-\nviding all its components by the maximal one. All the pair-\nwise distances between segments whose beginnings are not\nmore than 600 seconds away from each other are calcu-\nlated as Euclidean distances between their average feature\nvectors. This gives a Segment Distance Matrix similar to\nthe one introduced in [4].\nAll the segment pairs ((li; ri);(lj; rj)); i < j (where li\nandriare correspondingly segment’s left and righ borders)\nfor which the distance was calculated are sorted according\nto the following condition: Dij\u0001(rj\u0000li), where Dijis the\ndistance between i-th and j-th segments. Only the pair that\nproduces the smallest value is then merged. If the segments\nfrom this pair are not contiguous, all the intermediate ones\nare also included. To avoid too big segments, a pair gets apenalty when rj\u0000li>1:25\u0001average track length: its\ncondition becomes 100000 \u0001(rj\u0000li).\n4. RESULTS\nThe proposed method was evaluated on a collection of 103\nDJ mix recordings2downloaded from free online sources.\nThe corresponding timestamped tracklists in the form of\n.cue ﬁles were downloaded from http://cuenation.\ncom and used without any correctons. Timestamps have\nonly been used to validate the correctness of track sep-\naration. All recordings were taken from different radio\nshows and live sessions of different disk jokeys. Most\nof recordings are dated 2014, but there were also record-\nings from 2007-2013. The dominant music style within\nthe selected recordings is trance (uplifting, progressive, big\nroom, psychedelic), probably due to overall popularity of\nDJs playing this music. But house, drum’n’bass, break-\nbeat, techno, hardstyle, downtempo mixes are also included.\nFor the reasons described in section 3.3 we pay less at-\ntention to the conventional precision and recall metrics. In-\nstead, two values have been calculated for each mix: the\naverage and the maximum absolute distances in seconds\nfrom true track beginnings to detected ones. This way we\ncan evaluate the usefulness of the method in real life ap-\nplications: if the average absolute distance approaches the\naverage track length within a mix, the method becomes\nnearly useless for this mix. The maximum absolute dis-\ntance gives an estimation of the worst case. These values\nare then averaged across the whole collection to give an\nintegral measure of method performance.\nFrame-based pairwise precision, recall and F-measure\nhave also been calculated to provide more traditional esti-\nmation of segmentation quality. They are deﬁned as fol-\nlows. Each recording is separated into 1 second frames.\nAll frame pairs where both frames belong to the same track\nform the sets PE(for the system result) and PA(for the\nground truth). The pairwise precision rate can be cal-\nculated by P=jPE\\PAj\njPEj,pairwise recall rate byR=\njPE\\PAj\njPAj, and pairwise F-measure byF=2PR\nP+R. These\nvalues are then also averaged across the collection.\nAs a baseline we will use the same values calculated\nfor the naive separation, where all track borders are evenly\nspaced within the mix and all tracks have the same dura-\ntion. In case of explicit intro/outro information the naive\nseparation will allocate them 30 seconds in the beginning\nor in the end of the mix.\nIn the ﬁrst experiment3the system was not informed\nabout the presence of intro and outro sections in the mixes.\nThe results are shown in Table 1. The “Good” column\nshows the number of mixes where the average absolute dis-\ntance is less than 90 seconds (rather arbitrary limit). From\nthe numbers in this table it seems that the proposed method\nperforms not much better than the naive separation, which\n2The list of ﬁle names is available from https://github.com/\nnglazyrin/MixSplitter/blob/master/mix_list.txt\n3Full log is available from https://github.com/\nnglazyrin/MixSplitter/blob/master/logs/paper_\ntest.log\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n151Separation CAvg. abs. dist. CAvg. max dist. Good\nProposed 143.73 s 328.99 s 42\nBaseline 152.83 s 318.35 s 30\nTable 1. Results with no information about intro and outro.\nSeparation CAvg. abs. dist. CAvg. max dist. Good\nProposed 111.82 s 286.61 s 62\nBaseline 126.87 s 284.41 s 49\nTable 2. Results with information about intro and outro.\nis conﬁrmed by p-value of 0.096 returned by Wilcoxon\ntest. But looking closer at the performance on particular\nmixes, we can see that in some cases the proposed method\nhas real advantage. E.g. for the mix M.PRAVDA - Best of\n2013 (Part 2) (promodj.com).mp3 it gives average absolute\ndistance of 8.59 s (which is great) versus 60.22 s obtained\nby the naive separation. On the other hand, for some mixes\n(e.g. Trancecoda Podcast 008 - GMix Eddie Bitar.mp3) the\naverage absolute distance exceeds 6 minutes, which is ab-\nsolutely inacceptable.\nIn the second experiment4the system was informed\nabout the presence of intro and outro secions and could\nreact appropriately. From the Table 2 we can see that this\ninformation can be really helpful. In this experiment the\np < 0:01 was returned by Wilcoxon test. The result has\nmoved nearer to the “Good” limit of 90 seconds average\ndifference, and the difference between the proposed and\nthe baseline methods became bigger. And if the limit of\n“goodness” has decreased to 60 seconds, the difference\ngets more explicit: 54 good separations by the proposed\nmethod versus 24 good naive separations. For 30 seconds\nlimit on average absolute difference only 25 versus 6 good\nseparations are left.\nThis result shows that the proposed method can give\ngood result for a reasonable amount of mixes (62 out of\n103 here). But for some mixes the results are still too bad.\nWe provide two case-studies that describe common errors\nof the method.\nTable 3 shows the comparison of true and detected bor-\nders for one of the mixes – 4HCommunity Guest Mix The\n2ndAnniversary ofRoom51 Show byBreeze Quadrat\nPureFM.mp3 – with average absolute difference of 177.11\nseconds. First 3 tracks are aligned good, but then the sys-\ntem detects wrong border in the middle of 4th track. In\nspite of more or less properly detected other borders (the\ndetected value in row i+ 1is near the true value in row i),\nthey all mark beginnings of track i+1instead of i-th track.\nThe same information is represented graphically on Fig-\nure 1. Vertical yellow lines on the constant Q spectrogram\nmark the true borders, vertical black lines correspond to\ndetected borders.\nThe errors of this kind can be overcome with a better\n4Full log is available from https://github.com/\nnglazyrin/MixSplitter/blob/master/logs/paper_\ntest_explicit_intro_outro.logNo. Detected True Difference\n1 0.00 s 0.00 s 0.00 s\n2 308.38 s 312.08 s 3.70 s\n3 628.57 s 613.00 s -15.56 s\n4 872.56 s 1029.11 s 156.55 s\n5 1025.19 s 1363.48 s 338.29 s\n6 1360.54 s 1757.29 s 396.75 s\n7 1757.15 s 1961.62 s 204.47 s\n8 1970.53 s 2292.27 s 321.74 s\n9 2321.36 s 2552.34 s 230.98 s\n10 2748.30 s 2979.58 s 231.28 s\n11 3247.66 s 3198.78 s -48.88 s\nTable 3. Detailed result for the mix by 4H Community.\nSeparation Precision Recall F-measure\nProposed (1) 0.8145 0.7761 0.7941\nBaseline (1) 0.7024 0.6397 0.6688\nProposed (2) 0.8077 0.7892 0.7977\nBaseline (2) 0.7069 0.6637 0.6839\nTable 4. Framewise precision, recall and F-measure.\nsorting function for segment pairs or with a different seg-\nment grouping strategy. As can be seen from Table 4 (the\nnumber in parentheses in the ﬁrst column corresponds to\nthe experiment number), the proposed method really lo-\ncates borders much better than the baseline. But since\nsome borders are misplaced, the ﬁnal pairwise precision\nand recall rates are not so close to 1 as they could be.\nAnother source of errors are mixes that contain tracks\nof various durations, e.g. a pile of 1 minute long tracks\nfollowed by 4 minute long tracks, or several interludes\nthroughout the recording. An example of such mix is 01-\nfriction -bbcradio1 (chase andstatus special)-sat-10-13-\n2013-talion.mp3, which contains 35 tracks per 2 hours,\nand 6 of them are grouped between 55 and 65 minutes.\nThe separation is shown on the Figure 2. The described\nmethod tends to join short segments and to return more\nor less evenly spaced track borders because of the sorting\ncondition and the penalty for long tracks. So it does not ﬁt\nto these highly-variable mixes, which are characteristic for\nmusic genres such as drum’n’bass. But the separations ob-\ntained without using the penalty were worse than the ones\nobtained by the baseline method.\nTable 5 groups the resutls by music genres, which were\nmanually annotated for each mix. The mixes labeled as\nhaving various genre contain tracks from two or more very\ndifferent genres, such as house and drum’n’bass. The Cnt\ncolumn gives the total count of mixes of a given genre\nwithin our test collection.\nBecause the test set is very unbalanced by music genre\n(which is dictated by the available cue sheet ﬁles), it’s hard\nto make conclusions for music genres other than house\nand trance (which can be themselves separated into various\nsubgenres). The proposed system outperforms the baseline\nmethod on these genres, but both methods are failing on\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n152Figure 1. The separation for the mix by 4H Community.\nFigure 2. The separation for the mix by Chase & Status.\nStyle Cnt Separation Abs. dist. Max dist.\ntrance 59Proposed 91.43 s 244.26 s\nBaseline 114.85 s 255.38 s\nhouse 29Proposed 106.55 s 280.78 s\nBaseline 117.88 s 270.36 s\ntechno 4Proposed 122.11 s 284.51 s\nBaseline 104.61 s 219.00 s\ndowntempo 3Proposed 304.59 s 702.77 s\nBaseline 308.58 s 609.34 s\nhardstyle 2Proposed 81.91 s 232.66 s\nBaseline 93.22 s 221.95 s\ndrum’n’bass 2Proposed 330.67 s 798.21 s\nBaseline 343.03 s 865.92 s\nvarious 2Proposed 211.28 s 429.65 s\nBaseline 203.85 s 407.48 s\nbreakbeat 2Proposed 191.88 s 399.71 s\nBaseline 124.22 s 346.01 s\nTable 5. Results by music genre.\ndowntempo and drum’n’bass music.\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper, we proposed a method for informed content-\nbased separation of DJ mixes into single tracks that out-\nperforms a naive baseline evenly separating method. We\nshowed that this method provides good results for a rea-\nsonable amount of mixes. The resulting separations are\ngood enough to use them for further applications. We also\nshowed how a simple information about the presence of\nintro and outro sections in the mix can improve the separa-\ntion quality.\nThis paper establishes a basis for further work on DJ\nmixes separation. Another clustering methods need to be\ndeveloped to prevent false border detection errors and bor-\nder miss errors. It makes sense also to include higher fre-\nquencies into the initial spectrum, as they may carry somemeaningful details. On the other hand, the novelty detec-\ntion method does not seem to have a major impact, because\nthe initial border candidate set if sufﬁciently large to select\nvalues nearby the true borders.\nMore feature types need to be exploited. It also makes\nsense to consider the tempo information to avoid false bor-\nder detections, because the tempo does not change often\nduring transitions, but changes within a track when a break\nstarts or ends. A deeper modiﬁcation or a new method is\nneeded to handle mixes that contain tracks with highly-\nvarying durations. A separate method to detect interludes\nand talks can be helpful here.\nFinally, a signiﬁcant improvement may be expected from\nthe usage of a track identiﬁcation system, as it may help to\nalign at least some of the tracks properly. But this poses a\nseparate technical and legal task.\n6. REFERENCES\n[1] Online: http://wiki.themixingbowl.org/\nCue_sheet, acessed on May 5, 2014.\n[2] Online: http://wiki.hydrogenaudio.org/\nindex.php?title=Cue_sheet, accessed on\nMay 5, 2014.\n[3] J. Foote: “Automatic audio segmentation using a mea-\nsure of audio novelty” Proceedings of IEEE Interna-\ntional Conference on Multimedia and Expo, V ol. 1,\npp. 452–455, 2000.\n[4] F. Kaiser, and G. Peeters: “A simple fusion method\nof state and sequence segmentation for music struc-\nture discovery” Proceedings of the 14th International\nSociety for Music Information Retrieval Conference,\npp. 257-262, 2013.\n[5] F. Kaiser, and T. Sikora: “Music Structure Discovery\nin Popular Music using Non-negative Matrix Factor-\nization” Proceedings of the 11th International Society\nfor Music Information Retrieval Conference, pp. 429–\n434, 2010.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n153[6] T. Kell and G. Tzanetakis: “Empirical analysis of track\nselection and ordering in electronic dance music us-\ning audio feature extraction” Proceedings of the 14th\nInternational Society for Music Information Retrieval\nConference, pp. 505-510, 2013.\n[7] M. Levy, M. Sandler, and M. Casey: “Extraction of\nHigh-Level Musical Structure From Audio Data and\nIts Application to Thumbnail Generation” Proceedings\nof the International Conference on Acoustics, Speech\nand Signal Processing 2006, V ol. 5, 2006.\n[8] M. Marolt: “Probabilistic Segmentation and Labeling\nof Ethnomusicological Field Recordings” Proceedings\nof the 10th International Society for Music Information\nRetrieval Conference, pp. 75–80, 2009.\n[9] J. Paulus, M. M ¨uller, and A. Klapuri: “Audio-based\nMusic Structure Analysis” Proceedings of the 11th In-\nternational Society for Music Information Retrieval\nConference (ISMIR 2010), pp. 625–636, 2010.\n[10] J. Pauwels, F. Kaiser, and G. Peeters: “Combin-\ning harmony-based and novelty-based approaches for\nstructural segmentation” Proceedings of the 14th Inter-\nnational Society for Music Information Retrieval Con-\nference, pp. 601-606, 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n154"
    },
    {
        "title": "Estimating Musical Time Information from Performed MIDI Files.",
        "author": [
            "Harald Grohganz",
            "Michael Clausen",
            "Meinard Müller"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417925",
        "url": "https://doi.org/10.5281/zenodo.1417925",
        "ee": "https://zenodo.org/records/1417925/files/GrohganzCM14.pdf",
        "abstract": "Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score- related parameters. MIDI allows for representing musi- cal time information as specified by sheet music as well as physical time information that reflects performance as- pects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this pa- per, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert per- formed MIDI files into semantically enriched score-like MIDI files.",
        "zenodo_id": 1417925,
        "dblp_key": "conf/ismir/GrohganzCM14",
        "keywords": [
            "MIDI",
            "musical time information",
            "sheet music",
            "physical time information",
            "score-related parameters",
            "musical beat",
            "tempo information",
            "performed MIDI file",
            "global estimate of the time signature",
            "local errors in the pulse grid estimation"
        ],
        "content": "ESTIMA TING MUSICAL TIME INFORMA TION FROM PERFORMED\nMIDI FILES\nHarald Grohganz, Michael Clausen\nBonn University\n{grohganz,clausen} @cs.uni-bonn.deMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nEven though originally developed for exchanging control\ncommands between electronic instruments, MIDI has beenused as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musi-cal time information as speciﬁed by sheet music as wellas physical time information that reﬂects performance as-pects. However, in many of the available MIDI ﬁles themusical beat and tempo information is set to a preset valuewith no relation to the actual music content. In this pa-per, we introduce a procedure to determine the musicalbeat grid from a given performed MIDI ﬁle. As one maincontribution, we show how the global estimate of the timesignature can be used to correct local errors in the pulsegrid estimation. Different to MIDI quantization, whereone tries to map MIDI note onsets onto a given musicalpulse grid, our goal is to actually estimate such a grid.In this sense, our procedure can be used in combinationwith existing MIDI quantization procedures to convert per-formed MIDI ﬁles into semantically enriched score-likeMIDI ﬁles.\n1. INTRODUCTION\nMIDI (Music Instrument Digital Interface) is used as astandard protocol for controlling and synchronizing elec-tronic instruments and synthesizers [10]. Even thoughMIDI has not originally been developed to be used as asymbolic music format and imposes many limitation ofwhat can be actually represented [11, 13], the importanceof MIDI results from its widespread usage over the lastthree decades and the abundance of MIDI data freely avail-able on the web. An important feature of the MIDI for-mat is that it can handle musical as well as physical on-set times and note durations. In particular, the header ofa MIDI ﬁle speciﬁes the number of basic time units (re-ferred to as ticks) per quarter note. Physical timing is thengiven by means of additional tempo messages that deter-mine the number of microseconds per quarter note. On theone hand, disregarding the tempo messages makes it pos-\nc/circlecopyrtHarald Grohganz, Michael Clausen, Meinard M ¨uller.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Harald Grohganz, Michael Clausen,\nMeinard M ¨uller. “Estimating Musical Time Information from Performed\nMIDI Files”, 15th International Society for Music Information RetrievalConference, 2014.(a)\n(b)\n(c)\nFigure 1. The ﬁrst measure of the prelude BWV 888 by\nJ. S. Bach. (a)Original score. (b) Score from P-MIDI of\na performed version without musical pulse grid. (c)Score\nfrom S-MIDI based on an estimated musical pulse grid.\nsible to generate a mechanical version of constant tempo,\nwhich closely relates to the musical time axis (given inbeats) of a score. On the other hand, by including thetempo messages, one may generate a performed versionwith a physical time axis (given in seconds). However,many of the available MIDI ﬁles do not follow this conven-tion. For example, MIDI ﬁles are often generated by freelyperforming a piece of music on a MIDI instrument with-out explicitly specifying the tempo. As a result, neither theticks-per-quarter-note parameter nor the tempo messagesare set in a musically meaningful way. Instead, these pa-rameters are given by presets, which makes it possible toderive the physical but not the musical time information.\nIn the following, we distinguish between two types of\nMIDI ﬁles. When the musical beat and tempo messagesare set correctly in a MIDI ﬁle, then a musical time axis asspeciﬁed by a score can be derived. In this case, we speakof a score-informed MIDI ﬁle or simply S-MIDI. When\nthe actual tempo and beat positions are not known (usingsome presets), we speak of a performed MIDI ﬁle or sim-\nply P-MIDI. This paper deals with the general problem ofconverting a P-MIDI into a reasonable approximation ofan S-MIDI ﬁle. The main step is to estimate a musicallyinformed beat or pulse grid from which one can derive themusical time axis. The general problem of estimating beat-and rhythm-related information from music representation(including MIDI and audio representations) is a difﬁcultproblem [1, 7]. Typically approaches are based on Hidden\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n35PǦMIDI SǦMIDI\n\u0012\u0003\nȟǡݓ\n\u0013\u0003\nPǡߪ\u0006\n\u0003\n\u0017\u0003\nܭ଴Ȁܭଵ\u0013\u0003\nȣǡTǡȞ\u0011\u0003\u0003\nM\n\u0016\u0003\nA௄\f\u0003\n\u0003\u0003\u0003ߪ തǡS\u0017\u0003\n\nFigure 2. Overview of algorithmic pipeline.\nMarkov Models [12] and dynamic programming [6, 15].\nEven when knowing the note onset positions explicitly (asis the case for MIDI ﬁles), ﬁnding beats and measure is byfar not trivial—in particular when dealing with performedMIDIs having local tempo ﬂuctuations. In [2], an approachbased on salience proﬁles of MIDI notes is used for esti-mating the time signature and measure positions. Based ona trained dictionary of rhythmical patterns, a more generalapproach for detecting beat, measure, and rhythmic infor-mation is described in [14]. Note that the extraction of suchmusical time information from MIDI ﬁles is required be-fore software for MIDI quantization and score generationcan be applied in a meaningful way. This is demonstratedby Figure 1, which shows the original score, the score gen-erated from a P-MIDI, and a score generated from an esti-mated S-MIDI.\nIn this paper, we introduce a procedure for estimating\nthe musical beat grid as well as the time signature froma given P-MIDI ﬁle, which can then be converted into anapproximation of an S-MIDI ﬁle.\n1The main idea is to\nadapt a beat tracking procedure originally developed foraudio representations to estimate a ﬁrst pulse grid. Despiteof local errors, this information sufﬁces to derive an esti-mate of a global time signature. This information, in turn,is then used to correct the local pulse irregularities. In Sec-tion 2, we describe the algorithmic details of our proposedmethod. Then, in Section 3, we evaluate our method anddiscuss a number of explicit examples to illustrate bene-ﬁts and limitations. We conclude the paper with Section 4with possible applications and an outlook on future work.Further related work is discussed in the respective sections.\n2. ALGORITHMIC PIPELINE\nIn this section, we describe our procedure for converting P-MIDI ﬁles into (approximations of) S-MIDI ﬁles by map-ping the physical time axis of the P-MIDI to an appropri-ate musical time axis. As shown in Figure 2, we extractan onset curve from the P-MIDI, and perform periodicity\n1Our implementation in Java with GUI is available at http://\nmidi.sechsachtel.de(a)\n31 31.5 32 32.5 33 33.5 34 34.5 35 35.5 360200400\ntime (seconds)onset strength\n(b)\n31 31.5 32 32.5 33 33.5 34 34.5 35 35.5 3600.51\ntime (seconds)PLP value\n98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 11 3\n(c)\n98 99100 101 102 103 104105106 107 108 109 110 111 112 1130200400\ntime (pulse candidates)onset strenght\n(d)\n98 99100 101 102 103 104105106 107 108 109 110 111 112 11302040\ntime (pulse candidates)pulse salience\nFigure 3. Computation of pulse salience for a 5-secondexcerpt of BWV 888: (a) MIDI onset curve Δ, (b) PLP\ncurveΓwith pulse region boundaries b,(c)Onset curve Δ\nwith boundaries b,(d)Pulse salience sequence σ.\nanalysis by adapting a pulse tracking method to obtain asequence of pulse candidates (Section 2.1). In Section 2.2we introduce a method to estimate the global time signa-ture by analyzing the stress distribution of the pulses. Thisinformation is used for detecting and resolving inconsis-tencies in the pulse sequence.\nIn the following we use the notation [a:n:b]: =\n(⌈a⌉+nN\n0)∩[a,b], where[a,b]: ={t∈R|a≤t≤b}\nfora,b∈Randn∈N.I fn=1 , we use the notation\n[a:b]: =[a:1:b] .\n2.1 Pulse Detection\nFor pulse tracking, we build upon the method introduced\nby [9] which detects the local predominant periodicity inonset curves, and generates a pulse curve indicating themost likely positions for a pulse-grid. The peaks of thiscurve are then interpreted as pulse candidates. Althoughthis method was originally developed for audio data likeother beat tracking methods (see, e. g., [4,6]), it also worksfor onset curves derived from MIDI ﬁles.\nWe assume that the MIDI ﬁle is already converted to a\nphysical time axis [0,T], whereTdenotes the end of the\nlast MIDI note, and we have a MIDI note list for a suitable\nﬁnite index set I⊂N:\nM:= (t\ni,di,pi,vi)i∈I,\nwhereti∈[0,T)describes the start time of the ithMIDI\nnote,diits duration (also in seconds), pi∈[0 : 127] its\npitch, and vi∈[0 : 127] its note onset velocity. Based\non these notes, we deﬁne for a weighting parameter w=\n(w1,w2,w3)∈R3, a MIDI onset curve\nΔw(t): =/summationtext\ni∈I(w1+w2·di+w3·vi)·h(t−ti),\nfort∈[0,T], withhdescribing a Hann window centered\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n36at0of length 50ms, cf. Figure 3a. Thus the components\nof the parameter wcorresponds to weights of the presence\nof an onset, the duration, and the velocity, respectively. In\nour procedure, we ﬁx w:=/parenleftbig\n1,20,50\n128/parenrightbig\nto balance the\ncomponents of each MIDI note, so it will be omitted inthe notation. Experiments have shown that the method isrobust to slight changes of these values.\nUsing a short-time Fourier transform, we compute from\nΔatempogram T:[ 0,T]×Θ→Cfor a given set Θ\nof considered BPM values as explained in [9], using pa-rameters for smoothness (window length) and time granu-larity (step size). First, we compute a coarse tempogramT\ncoarseusing the tempo set Θ = [40 : 4 : 240], win-\ndow length 8sec, and step size 1sec. The dominant global\ntempoT0is derived by summing up the absolute values of\nTcoarserow-wise and by detecting the maximum. Next,\nwe compute a second tempogram Tﬁnebased on the new\nsetΘ=/bracketleftBig\n1√\n2·T0,√\n2·T0/bracketrightBig\n∩N, which is the tempo oc-\ntave aroundT0. For this tempogram, the window length\nis set to5·60\nT0sec, and we use a ﬁner step size of 0.2sec.\nChoosing the BPM range in such a manner prevents unex-pected jumps between multiples of the detected tempo; thewindow length corresponds to ﬁve expected pulses basedon the assumption that a stable tempo remains almost con-stant for at least ﬁve beats.\nFollowing [9], we estimate the predominant tempo for\neach time position from the tempogram T\nﬁne, and use this\ninformation to derive sinusoidal kernels which best de-scribe local periodicity of the underlying onset curve Δ.\nThese kernels are combined to a predominant local pulse\n(PLP) curve Γ:[ 0,T]→[0,1], which indicates positions\nof pulses on the physical time axis, see Figure 3b. Thepoints in time corresponding to the local maxima of Γform\napulse candidate sequence P=(P\n1,...,PN),which is\nsuitable to estimate the beats in a ﬁrst approximation. Butthis sequence may contain additional pulses (not describ-ing a musical beat) or missing pulses. Thus we introducea post-processing method in the next section which detectsand corrects these errors.\n2.2 Optimizing the pulse sequence\nThe main idea of the method described in this section relies\non ﬁnding a global time signature and using it for resolv-ing inconsistencies of the detected pulse sequence. Here,we assume that the measure type of the considered musi-cal piece is not changed throughout the piece. The timesignature can be estimated by periodicity analysis of pulsestress using short-time autocorrelation. In a second step,we compare the relative position of each pulse candidateto a measure grid induced by the time signature and detectdeviations to correct isolated erroneous pulses. Finally, the\npulses are interpreted as a new musical time axis, and thetick position of all MIDI events are mapped to this axis.\nNow we describe our optimization procedure in more\ndetail. First, we accumulate the onset strength for the n\nth\npulse candidate by deﬁning its pulse salience\nσ(n): =/integraltextb(n)\nb(n− 1)Δ(t)dt(n∈[1 :N]), (1)where the pulse region boundaries are given by b(n)=\n1\n2·(Pn+Pn+1)for1≤n<N ,b(0) = 0, and b(N)=T.\nThe boundaries bare illustrated in Figures 3b and 3c, and\nfor the salience values σsee Figures 3d and 4a.\nOur next goal is to compute an estimation of the time\nsignature K0/K 1. To this end we perform a salience anal-\nysis via autocorrelation. However, to ensure that errors inPandσhave only a local inﬂuence, we use short-time au-\ntocorrelation. For a ﬁxed window size K>12(K=3 2\nin our implementation), we consider the K×Nmatrix\nA(k,n): =|I\nk|−1/summationdisplay\ni∈Ikσ(n+i)·σ(n+i+k),\nwhereIk:= [0 :k:K−k−1]andσ(n): =0 for\nn∈Z\\[1 :N]. ThusA(k,n)quantiﬁes the plausibil-\nity of period length karound the nthpulse candidate. Our\npredominant salience period K0, the nominator of the esti-\nmated time signature, is obtained by row-wise summationand maximum picking of parts of A:\nK\n0:= argmax\nk∈[3:12]/summationtextN\nn=1A(k,n).\nFor robustness and musical reasons we have excluded the\ncasesk<3andk>12, respectively. (Excluding the case\nk=2 is not a serious problem as we can use, e. g., 4/8\nas surrogate for 2/4.) The relevant rows of the matrix A\nare illustrated in Figure 4b where K0=6 . The denom-\ninatorK1is not necessary for further computation. It is\nchosen accordingly to the main tempo T0to ensure a value\nbetween70and140 quarter notes per minute.\nWith the help of K0we are now able to perform the in-\nconsistency analysis. For now, we primarily consider thecase where all detected pulse candidates are actually cor-rect beats. In this idealized scenario, the restriction of Pto\nthen\nthK0-congruence class [n:K0:N],n∈[1 :K0],\ndescribes the nthposition within the measures in a se-\nmantically meaningful way. In particular, the ﬁrst class(n=1 ) corresponds to all downbeat positions if the con-\nsidered piece does not start with an upbeat. An analogousdecomposition applied to σleads to salience patterns of\neach position in the measure. Due to rhythmic variations,we expect that the ﬁrst class of σmostly shows the high-\nest salience value. To enhance robustness, σis smoothed\nlocally within the K\n0-congruence classes\n¯σ(n): =σ(n)+⌊K/K 0⌋/summationdisplay\nk=1σ(n±k·K0),\nas illustrated in Figure 4c. Since the restriction to a con-gruence class is reminiscent of a comb, we call ¯σtheK\n0-\ncombed version of σ.\nErroneously detected pulse candidates disturb the as-\nsignment of all downbeats to a speciﬁc class. In suchcases, the class containing highest salience values changesat some points of time. To make this visible, a K\n0×Nma-\ntrixSis deﬁned which shows the local salience distribution\nof the congruence classes. More precisely, we deﬁne\nS(k,n): =¯σ(k)·δ(k≡K0n),\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n37(a)\n50 100 150 200 250 300 350 40001020304050\npulse candidatespulse salience(b)\npulse periodicity\npulse candidates50 100 150 200 250 300 350 40024681012\n00.20.40.60.81\n(c)\n50 100 150 200 250 300 35050100150200250\npulse candidatessmoothed pulse salience(d)\n50 100 150 200 250 300 350 40050100150200250\npulse candidatessmoothed pulse salience\n(e)\nbeat congruence class\npulse candidates50 100 150 200 250 300 350 400123456\n00.20.40.60.81 (f)\nbeat congruence class\npulse candidates50 100 150 200 250 300 350 400123456\n00.20.40.60.81\nFigure 4. Step-by-step illustration how to detect inconsistencies in the stress sequence for BWV 888: (a)Pulse salience\nsequence σas in Figure 3d. (b) Excerpt of short-time autocorrelation matrix Aofσshowing maximal energy in 6throw.\n(c)6-combed salience sequence ¯σif all pulse candidates are correctly detected. (d)6-combed salience sequence ¯σif two\npulses are additionally inserted. (e)Stressgram showing maximal salience in 1stcongruence class. (f)Stressgram showing\ntwo stress changes and path of highest salience.\nwhereδ(A): =1 if statement Aholds and 0else. Smooth-\ningSalong the temporal axis using a Hann window of\nlength2·K0yields a so-called stressgram S. Such stress-\ngrams are visualized for the ideal scenario (Figure 4e) as\nwell as under presence of two additional pulses (Figure 4f).\nNow we discuss this last case in more detail. First, the\nestimation of K0is only locally disturbed which does not\nlead to a change of the estimated time signature. However,the decomposition into K\n0-congruence classes does no\nlonger coincide semantically with the position in the mea-sures, since all pulses after the additional one are shifted byone beat position. In the stressgram Sthis is indicated by\nchanges of the rows showing high salience. To enhance ro-bustness, we switch to a more global point of view by com-puting a path of highest energy through Susing dynamic\nprogramming. Each point in this path shows the congru-ence class with the highest coincidence of representing thedownbeats at a speciﬁc time. More precisely, if the down-beats are in the class having index i, then a change to index\ni+1 near the additional pulse can be noticed, see Figure 4f.\nThe case of a missing pulse is similar, here the row indexof the maximal salience changes to i−1.\nThese detected irregularities can now be solved by\nchoosing either falsely added pulse candidates or ﬁndingpositions to insert an apparently missing pulse. For lackof space, we only sketch our correction procedure. To re-move a candidate, one can delete the pulse having lowestsalienceσor lowest PLP score (for this, replace ΔbyΓin\nEquation 1). For adding an additional pulse, one may lookfor two adjacent relatively low values of the PLP curve.Finally, this corrected pulse sequence deﬁnes a beat grid\nin the P-MIDI ﬁle, which allows to detect a sequence oftick positions corresponding to beats. By mapping themto equally distributed new tick positions, adding appropri-ate tempo change MIDI messages, and performing linearinterpolation between the beat positions, the previous timeaxis of the P-MIDI is replaced by a musical time axis. Incase of an upbeat, additional beats are added to the begin-ning of the piece such that the ﬁrst K\n0-congruence class\ncorresponds to the estimated downbeats. Lastly, the timesignature K\n0/K 1is added to the new MIDI ﬁle.\n3. EXPERIMENTS AND DISCUSSION\nEvaluating the output of a beat tracking procedure is a non-trivial task due to the vague deﬁnition of beat times asdescribed in [5]. Particularly determining the beat gran-ularity, i. e., the decision between similar time signatureslike6/8 and3/4, or multiples such as 4/4 or8/4, appears\nas an ill-posed and negligible problem. Even for humans,beat and measure tracking can be challenging especially inthe presence of rhythmic variations and expressive timing.Our evaluation is inspired by [14], where among others thevisual impression of the computed score is considered, andby [5], where comparison to a ground truth annotation bya human and listening tests for a perceptual evaluation aresuggested.\nBecause of its modeling, our procedure is not suitable\nfor all kinds of P-MIDI ﬁles. The PLP approach describedin Section 2.1 has some constraints like a stable rhythm oran almost stable tempo for a certain amount of time (in our\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n38(a)\n(b)\n(c)\nbeat congruence class\npulse candidates20 40 60 80 100 120 140 160 1802468\n00.20.40.60.81 (d)\npulse periodicity\npulse candidates20 40 60 80 100 120 140 160 18024681012\n00.20.40.60.81\nFigure 5. Prelude No. 4 from Chopin (Op. 28). The score excerpts show the detected pulses (upper row) and post-processed\nmeasure grid (lower row). Downbeats are indicated by bold lines. (a)Correctly detected upbeat. (b) Joint correction of\ntwo subsequent errors. (c)Stressgram with path of highest salience. (d)Short-time autocorrelation matrix.\nimplementation, this time window is roughly ﬁve seconds).In addition, tempo octave confusions are not consideredhere. For the optimization step described in Section 2.2, aglobal time signature is required. Furthermore, downbeatsmust be detectable by their length or stress. In the follow-ing, we discuss in detail two typical examples of P-MIDIﬁles, and then perform an automatic analysis on a smalltest set of artiﬁcially distorted MIDI ﬁles.\n3.1 Qualitative Evaluation\nThe performance of the proposed procedure for Bach’s\nPrelude BWV 888 is already indicated in Figures 1, 3, and4. Two insertions of additional pulses caused by ritardandiare corrected well, and also the erroneous rests at the be-ginning are eliminated. The estimated time signature 6/8\nis perceptually similar to the notated time signature 12/8.\nOur second example is the Prelude No. 4 from Chopin’s\nromantic Piano Preludes. Figure 5 shows two score ex-cerpts together with the detected pulse candidates and theestimated measure structure as well as the correspondingstressgram and the short-time autocorrelation matrix forthe whole piece.\n2Prelude No. 4 contains some long notes\nat downbeat positions leading to a good measure track-ing result. Because of their strong presence, an eighthnote pulse grid was detected, see Figure 5d. This pieceof music starts with an upbeat of a quarter note. Since theMIDI format does not support upbeat information directly,our method adds enough additional pulses such that theﬁrst pulse lies in the congruence class of the downbeats asshown in Figure 5a.\nNoticeable tempo changes together with short appog-\ngiatura and a triplet around pulse No. 90 causes the PLPprocedure to detect two additional pulses in consecutivemeasures (Fig. 5b). In the stressgram this is indicated by\n2The examples are recordings on a MIDI piano taken from Saar-\nland Music Data ( http://www.mpi-inf.mpg.de/resources/\nSMD/), and the scores are picked from Mutopia (http://www.\nmutopiaproject.org/).a jump of the salience path across two classes (Fig. 5c).\nNote that this error has only little inﬂuence on the estima-tion of the time signature (Fig. 5d). As indicated by thestressgram, two pulses are removed in this region duringour post-processing. Although the deletion of a correctlydetected pulse in the 2\nndmeasure of Figure 5b leads to a\nwrong downbeat position in the subsequent measure, theglobal measure grid is restored in the 4\nthmeasure. This\nshows how our method optimizes the measure grid with-out having to correct each single pulse error.\n3.2 Automatic Evaluation\nFurthermore, we evaluated our method on score-like MIDI\nﬁles which have been automatically disturbed by addingadditional tempo change events. A similar approach wasused in [8] to show that smooth tempo changes are detectedwell by the PLP method.\nIn particular, the goal of our procedure is to recognize\nmeasure positions correctly, therefore we use standard pre-cision (P), recall (R), and F-measure (F ) on the set of the\nMIDI notes. A note is considered relevant if it starts at adownbeat position in the S-MIDI ﬁle, and it is “retrieved”if it was mapped by our method to a downbeat position ofthe distorted MIDI ﬁle. Since no quantization step is in-cluded, we allow a tolerance of ±5% of each measure as\nits downbeat position.\nBy neglecting the musical time axis and using only the\nphysical time position (in milliseconds) of all MIDI events,we simulate a performed MIDI of an S-MIDI ﬁle. Thesystematic distortion is done by adding a tempo change of±20% around the normal tempo each 10seconds.\nAs test set, we consider the Fifteen Fugues by\nBeethoven from IMSLP\n3. Here, the note durations are suf-\nﬁcient for a good estimation of the PLP pulses. Addingnote velocity information from real performed MIDI ﬁlessuggests an further improvement of the results.\n3Petrucci Music Library, http://imslp.org/wiki/15_\nFugues_(Beethoven,_Ludwig_van)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n39Piece Full method PLP only # Corrections\nFPR FPR add delete upbeat\nNo. 1 0.477 0.587 0.402 0.372 0.509 0.293 02 0\nNo. 2 0.978 1 0.956 0.397 0.56 0.308 10 1\nNo. 3 0.656 0.663 0.649 0.144 0.196 0.113 10 0\nNo. 4 0.945 0.984 0.909 0.738 0.804 0.682 20 0\nNo. 5 0.966 0.971 0.962 000 00 2\nNo. 6 0.996 1 0.993 0.996 1 0.993 00 0\nNo. 7 0.826 0.832 0.82 0.324 0.386 0.28 14 1\nNo. 8 0.953 0.985 0.923 0.821 0.945 0.725 01 0\nNo. 9 0.896 0.916 0.876 0.787 0.855 0.73 10 1\nNo. 10 0.581 0.579 0.582 0.008 0.013 0.005 22 1\nNo. 11 111 111 00 0\nNo. 12 0.994 1 0.988 0.792 0.842 0.748 31 0\nNo. 13 0.656 0.884 0.522 0.245 0.393 0.178 02 2\nNo. 14 0.975 0.995 0.957 0.432 0.75 0.303 13 0\nNo. 15 0.692 0.98 0.535 0.633 0.992 0.465 00 4\nMean 0.839 0.892 0.805 0.513 0.616 0.455 0.8 1 0.8\nTable 1. Evaluation results for 15 Fugues from Beethoven\nfor full method and PLP-based beat tracking only\nThe results for the automatic evaluation are shown in\nTable 1. We evaluated both the original pulse sequencederived from the PLP pulse tracking method introducedin Section 2.1, and the post-processed version. In bothcases, the detected time signature was used to locate thedownbeats. For all pieces except No. 11, the annotated 2/2\nsignature was mostly detected as 4/4, sometimes as 8/4.\nCompared to the results of the PLP pulse tracker, which isnot designed for detecting downbeats, the results for somepieces were improved signiﬁcantly by our method. For ex-ample, in Fugue No. 7 our post-processing method addedone pulse and removed four other pulses. At the begin-ning, another single pulse was inserted to prevent upbeatshifts. These changes lead to an increase of the F-measure\nfrom0.324 to0.826, which has major consequences, e. g.,\non the amount of additional work for a human importingthis MIDI ﬁle into a score notation software to optimizethe score manually.\n4. CONCLUSION\nWe presented a bottom-up method to derive a musicallymeaningful time axis for performed MIDI ﬁles, and con-verting them into semantically enriched score-like MIDIﬁles. Our proposed procedure optimized an estimatedpulse sequence by insertion of missing pulses as well asremoval of spurious pulses to derive an overall consistentmeasure grid.\nSince the output of the presented method is another\nMIDI ﬁle, our procedure can be used in combination withany MIDI quantization software by using it for preprocess-ing performed MIDI ﬁles having no musically meaningfultime information. Essentially, the physical time axis re-mains unchanged, so it can be used further in combinationwith rhythm transcription approaches. Deriving a musicaltime axis without quantization is also meaningful for real-time interaction with MIDI synthesizers, e. g., as a varia-tion of [3]. Because of its generality, our procedure can besimply extended by including other rhythmic or harmonicaspects.\nAcknowledgments: This work has been supported\nby the German Research Foundation (DFG CL 64/8-1,DFG MU 2686/5-1). The International Audio Laboratories\nErlangen are a joint institution of the Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg (FAU) and Fraunhofer IIS.\n5. REFERENCES\n[1] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: challengesand future directions. Journal of Intelligent Informa-\ntion Systems, 41(3):407–434, 2013.\n[2] E. Cambouropoulos. From MIDI to traditional musical\nnotation. In Proc. of the AAAI W orkshop on Artiﬁcial\nIntelligence and Music, vol. 30, 2000.\n[3] R. B. Dannenberg and C. Raphael. Music score align-\nment and computer accompaniment. Communications\nof the ACM, Special Issue: Music information re-trieval, 49(8):38–43, 2006.\n[4] M. E. P . Davies and M. D. Plumbley. Context-depend-\nent beat tracking of musical audio. IEEE Transact. on\nAudio, Speech and Language Processing, 15(3):1009–1020, 2007.\n[5] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30:39–58, 2001.\n[6] D. P . W. Ellis. Beat tracking by dynamic programming.\nJournal of New Music Research, 36(1):51–60, 2007.\n[7] F. Gouyon and S. Dixon. A review of automatic rhythm\ndescription systems. Computer Music Journal, 29:34–\n54, 2005.\n[8] P . Grosche and M. M ¨uller. A mid-level representation\nfor capturing dominant tempo and pulse information inmusic recordings. In Proc. of the Intern. Conf. on Mu-\nsic Information Retrieval (ISMIR), pp. 189–194, 2009.\n[9] P . Grosche and M. M ¨uller .\n Extracting predominant lo-\ncal pulse information from music recordings. IEEE\nTransact. on Audio, Speech, and Language Processing,19(6):1688–1701, 2011.\n[10] D. M. Huber. The MIDI manual. Focal Press, 3rd edi-\ntion, 2006.\n[11] F. R. Moore. The dysfunctions of MIDI. Computer Mu-\nsic Journal, 12(1):19–28, 1988.\n[12] C. Raphael. Automated rhythm transcription. In Proc.\nof the Intern. Conf. on Music Information Retrieval (IS-MIR), 2001.\n[13] E. Selfridge-Field, editor. Beyond MIDI: the handbook\nof musical codes . MIT Press, Cambridge, MA, USA,\n1997.\n[14] H. Takeda, T. Nishimoto, and S. Sagayama. Rhythm\nand tempo analysis toward automatic music transcrip-tion. In IEEE Intern. Conf. on Acoustics, Speech and\nSignal Processing, vol. 4, pp. IV–1317, 2007.\n[15] A. C. Yang, E. Chew, and A. V olk. A dynamic pro-\ngramming approach to adaptive tatum assignment forrhythm transcription. In IEEE Intern. Symposium on\nMultimedia, 2005.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n40"
    },
    {
        "title": "Musical Structural Analysis Database Based on GTTM.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415658",
        "url": "https://doi.org/10.5281/zenodo.1415658",
        "ee": "https://zenodo.org/records/1415658/files/HamanakaHT14.pdf",
        "abstract": "This paper, we present the publication of our analysis da- ta and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical piec- es with standard MIDI files and annotated data are key to advancements in the field of music information technolo- gy. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical struc- ture analysis, we are now publicizing 300 pieces of anal- ysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations.",
        "zenodo_id": 1415658,
        "dblp_key": "conf/ismir/HamanakaHT14",
        "keywords": [
            "generative theory of tonal music",
            "musical databases",
            "music information technology",
            "analysis data",
            "analyzing tool",
            "step-by-step collection",
            "research advancements",
            "musical structure analysis",
            "new musicologist results",
            "original GTTM database"
        ],
        "content": "MUSICAL STRUCT URAL ANALYSIS DATABASE BASED ON GTTM  \nMasatoshi Hamanaka  Keiji Hirata  Satoshi Tojo  \nKyoto University  \nmasatosh@ kuhp.kyoto- u.ac.jp Future University Hakodate  \nhirata@fun.ac.jp  JAIST  \ntojo@jaist.ac.jp  \nABSTRACT  \nThis paper, we present the publication of our analysis d a-\nta and analyzing tool based on the generative theory of \ntonal music (GTTM). Musical databases such as score \ndatabases, instrument sound databases, and musical piec-\nes with standard MIDI files and annotated data are key to advancements in the field of music information technol o-\ngy. We started implementing the GTTM on a computer in \n2004 and ever since have collected and publicized test \ndata by musicologists in a step- by-step manner. In our \nefforts to further advance the research on musical stru c-\nture analysis, we are now publicizing 300 pieces of ana l-\nysis data as well as the analyzer. Experiments showed \nthat for 267 of 300 pieces the analysis results obtained by \na new musicologist were almost the same as the original \nresults in the GTTM database and that the other 33 pieces \nhad differ ent interpretations.  \n1. INTRODUCTION  \nFor over ten years we have been constructing a musical \nanalysis tool based on the generative theory of tonal music (GTTM) [1, 2]. The GTTM, proposed by Lerdahl and Jackendoff, is one in which the abstract structure of a musical piece is acquired from a score [3]. Of the \nmany music analysis theories that have been proposed \n[4–6], we feel that the GTTM is the most promising in \nterms of its ability to formalize musical knowledge b e-\ncause it captures aspects of musical phenomena based \non the Gestalt occurring in music and then presents these aspects with relatively rigid rules.  \n   The time -span tree and prolongational trees acquired \nby GTTM analysis can be used for melody morphing, which generates an intermediate melody between  two \nmelodies with a systematic order [7]. It can also be used for performance rendering [8 –10] and reproduc-\ning music [11] and provides a summarization of the \nmusic that can be used as a search representation in \nmusic retrieval systems [12].  \nIn constructing a musical analyzer, test data from \nmusical databases is very useful for evaluating and improving the performance of the analyzer. The Essen folk song collection is a database for folk -music r e-\nsearch that contains score data on 20,000 songs along with phrase segmentation information and also pr o-\nvides software for processing the data [13]. The Répe r-toire International des Sources Musicales (RISM), an international, non- profit organization with the aim of \ncomprehensively documenting extant musical sources \naround the world, provides an online catalogue co n-\ntaining over 850,000 records, mostly for music man u-\nscripts [14]. The Variations3 project provides online \naccess to streaming audio and scanned score images for the music community with a flexible access contr ol \nframework [15], and the Real World Computing \n(RWC) Music Database is a copyright -cleared music \ndatabase that contains the audio signals and corr e-\nsponding standard MIDI files for 315 musical pieces \n[16,17]. The Digital Archive of Finnish Folk Tunes provi des 8613 finish folk song midi files with annota t-\ned meta data and Matlab data matrix encoded by midi \ntoolbox [18]. The Codaich contains 20,849 MP3 r e-\ncordings, from 1941 artists, with high- quality annot a-\ntions [19], and the Latin Music Database contains \n3,227 MP3 files from different music genres [20].  \nWhen we first started constructing the GTTM analyzer, \nhowever, there was not much data that included both a \nscore and the results of analysis by musicologists. This \nwas due to the following reasons:  \n• There were no computer tools for GTTM analysis.  \nOnly a few paper -based analyses of GTTM data had \nbeen done because a data- saving format for computer \nanalysis had not yet been defined. We therefore defined \nan XML -based format for analyzing GTTM results and \ndeveloped a manual editor for the editing.  \n• Editing the tree was difficult.  \nMusicologists using the manual editor to acquire ana l-\nysis results need to perform a large number of manual \noperatio ns. This is because the time -span and prolong a-\ntional trees acquired by GTTM analysis are binary trees, \nand the number of combinations of tree structures in a \nscore analysis increases exponentially with the number of notes. We therefore developed an automatic analyzer based on the GTTM.  \n• There was a lack of musicologists.  \nOnly a few hundred musicologists can analyze scores \nby using the GTTM. In order to encourage musicologists \nto co -operate with expanding the GTTM database, we \npublicized our analysis tool and  analysis data based on \nthe GTTM.  \n• The music analysis was ambiguous.  \nA piece of music generally has more than one interpr e-\ntation, and dealing with such ambiguity is a major prob-\nlem when constructing a music analysis database. We \nperformed experiments to compare the different analysis \nresults obtained by different musicologists.  \n © Masatoshi Hamanaka , Keiji Hirata , Satoshi Tojo. \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution: Masatoshi Hamanaka, Keiji Hirata , \nSatoshi Tojo . “Musical Structural Analysis Database based on GTTM ”, \n15th International Society for Music Information Retrieval Conference, \n2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n325W\ne started implementing our GTTM analyzer on a \ncomputer in 2004, immediately began collecting test data \nproduced by musicologists, and in 2009 started publici z-\ning the GTTM database and analysis system. We started \nthe GTTM database with 100 pairs of scores and time -\nspan trees comprising and then added the prolongational \ntrees and chord progression data. At present, we have 300 data sets that are being used for researching music stru c-\ntural analysis [1]. The tool we use for analyzing has changed from its original form. We originally constructed a standalone application for the GTTM -based analysis \nsystem, but when we started having problems with bugs in the automatic analyzer, we changed  the application to a \nclient -server system.  \nIn experiments we compared the analysis results of two \ndifferent musicologists, one of whom was the one who provided the initial analysis data in the GTTM database. For 267 of 300 pieces of music the two results were the \nsame, but the other 33 pieces had different interpretations. Calculating the coincidence of the time -spans in those 33 \npieces revealed that 233 of the 2310 time -spans did not \nmatch.  \nThis rest of this paper is organized as follows. In se c-\ntion 2 we describe the database design policy and data sets, in section 3 we explain our GTTM analysis tool, in \nsection 4 we present the experimental results, and in sec-\ntion 5 we conclude with a brief summary.  \n2. GTTM DATABASE  \nThe GTTM is composed of four modules, each  of which \nassigns a separate structural description to a listener’s \nunder -standing of a piece of music. Their output is a \ngrouping structure, a metrical structure, a time -span tree, \nand a prolongational tree (Fig. 1).  \n   The grouping structure is intended to formalize the i n-\ntuitive belief that tonal music is organized into groups comprising subgroups. The metrical structure describes \nthe rhythmical hierarchy of the piece by identifying the \nposition of strong beats at the levels of a quarter note, \nhalf note,  one measure, two measures, four measures,  \n  \nFigure 1. Grouping structure, metrical structure, time -\nspan tree, and prolongational tree. and so on. The time -span tree is a binary tree and is a h i-\nerarchical structure describing the relative structural i m-\nportance of notes that differentiate the essential parts of \nthe melody from the ornamentation. The prolongational tree is a binary tree that ex presses the structure of tension \nand relaxation in a piece of music.  \n2.1 Design  policy of analysis database  \nAs at this stage several rules in the theory allow only monophony, we restrict the target analysis data to mon o-\nphonic music in the GTTM database. \n2.1.1 Ambiguity in music analysis  \nWe have to consider two types of ambiguity in music \nanalysis. One involves human understanding of music \nand tolerates subjective interpretation, while the latter \nconcerns the representation of music theory and is \ncaused by  the incompleteness of a formal theory like the \nGTTM. We therefore assume because of the former type \nof ambiguity that there is more than one correct result.  \n2.1.2 XML -based data structure  \nWe use an XML format for all analysis data. MusicXML \n[22] was chose n as a primary input format because it \nprovides a common ‘interlingua’ for music notation, \nanalysis, retrieval, and other applications. We designed \nGroupingXML, MetricalXML, TimespanXML, and Pr o-\nlongationalXML as the export formats for our analyzer. We also  designed HarmonicXML to express the chord \nprogression. The XML format is suitable for expressing the hierarchical grouping structures, metrical structures, time-span trees, and prolongational trees.  \n2.2 Data  sets in GTTM database  \nThe database should contain a variety of different mus i-\ncal pieces, and when constructing it we cut 8 -bar-long \npieces from whole pieces of music because the time r e-\nquired for analyzing and editing would be too long if \nwhole pieces were analyzed. \n2.2.1 Score data  \nWe collected 300 8 -bar-long monophonic classical music \npieces that include notes, rests, slurs, accents, and articu-\nlations entered manually with music notation software \ncalled Finale [22]. We exported the MusicXML by using \na plugin called Dolet. The 300 whole pieces and the \neight bars were selected by a musicologist.  \n2.2.2 Analysis data  \nWe asked a musicology expert to manually analyze the \nscore data faithfully with regard to the GTTM, using the \nmanual editor in the GTTM analysis tool to assist in edit-\ning the grouping structur e, metrical structure, time -span \ntree, and prolongational tree. She also analyzed the chord progression. Three other experts crosschecked these \nmanually produced results.  \nGrouping  \nstructure  \nMetrical  \nstructure  \nTime -span Tree  \nProlongational Tree  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n326 \n3. I\nNTERACTIVE GTTM ANALYZER  \nOur GTTM analysis tool, called the Interactive GTTM \nanalyzer, consists of automatic analyzers and an editor that can be used to edit the analysis results manually (Fig. \n2). The graphic user interface of the tool was constructed \nin Java, making it usable on multiple platform s. However, \nsome functions of the manual editor work only on MacOSX, which must use the MacOSX API. \n3.1 Automatic analyzer for GTTM  \nWe have constructed four types of GTTM analyzers: \nATTA, FATTA, σGTTM, and σGTTMII [2, 23 –25]. \nThe Interactive GTTM analyzer  can use either the ATTA \nor the σGTTMII, and there is a trade- off relationship b e-\ntween the automation of the analysis process and the va r-\niation of the analysis results (Fig. 3).  \n \nFigure 3. Trade -off between automation of analysis pr o-\ncess and variation of analysis results.   3.1.1 ATTA: Automatic Time -Span Tree Analyzer  \nWe extended the  original theory  of GTTM  with a full \nexternalization and parameterization and proposed a m a-\nchine -executable extension of the GTTM called \nexGTTM [2]. The externalization includes introducing \nan algorithm to generate a hierarchical structure of the \ntime-span tree in a mixed top -down and bottom -up ma n-\nner and the parameterization includes introducing a pa-rameter for controlling the priorities of rules to avoid conflict among the rules as well as parameters for co n-\ntrolling the shape of the hierarchical time -span tree. We \nimplemented the exGTTM on a computer called the \nATTA, which can output multiple analysis results by \nconfiguring the parameters.  \n3.1.2 FATTA: Full Automatic Time -Span Tree Analyze r \nAlthough the ATTA has adjustable parameters for control-ling the weight or priority of each rule, these parameters \nhave to be set manually. This takes a long time because \nfinding the optimal values of the settings themselves takes \na long time. The FATTA can automatically estimate the \noptimal parameters by introducing a feedback loop from higher -level structures to lower -level structures on the b a-\nsis of the stability of the time -span tree [ 23]. The FATTA \ncan output only one analysis result without manual c onfig-\nuration. However, our experimental results showed that the performance of the FATTA is not good enough for grou p-\ning structure or time -span tree analyses.  Analysis process\nAutomatic Manual\nUser labor \nSmall Big\nAnalysis results\nOnly one Various\nATTA\n σGTTM\n σGTTM II\nＦATTA\nFigure 2. Interactive GTTM analyzer.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3273\n.1.3 σGTTM  \nWe have developed σ GTTM, a system that can detect the \nlocal grouping boundaries in GTTM an alysis, by combi n-\ning GTTM with statistical learning [24]. The σGTTM \nsystem statistically learns the priority of the GTTM rules from 100 sets of score and grouping structure data an a-\nlyzed by a musicologist and does this by using a decision tree. Its perform ance, however, is not good enough b e-\ncause it can construct only one decision tree from 100 \ndata sets and cannot output multiple results.  \n3.1.4 \nσGTTM II  \nThe σGTTM II system assumes that a piece of music has \nmultiple interpretations and thus it constructs multiple \ndecision trees (each corresponding to an interpretation) by iteratively clustering the training data and training the \ndecision trees. Experimental res ults showed that the \nσGTTM II system outperformed both the ATTA and \nσGTTM systems [25].  \n3.2 Manual editor for  the GTTM \nIn some cases the GTTM analyzer may produce an ac-\nceptable result that reflects the user’s interpretation, but \nin other cases it may not. A user who wants to change \nthe analysis result according to his or her interpretation \ncan use the GTTM manual editor. This editor has n u-\nmerous functions that can load and save the analysis r e-\nsults, call the ATTA or σ GTTM II analyzer, record the \nediting history , undo the editing, and autocorrect inco r-\nrect structures.  \n3.3 Implement ation on client -server system  \nOur analyzer is updated frequently, and sometimes it is a \nlittle difficult for users to download an updated program. \nWe therefore implement our Interactive GTTM analyzer \non a client -server system. The graphic user interface on \nthe client side runs as a Web application written in Java, while the analyzer on the server side runs as a program written in Perl. This enables us to update the analyzer \nfrequently while allowing users to access the most recent \nversion automatically.  \n4. EXPERIMENTAL RESULTS  \nGTTM analysis of a piece of music can produce multiple \nresults because the interpretation of a piece of music is \nnot unique. We compared the different analysis results \nobtained by different musicologists.  \n4.1 Condition of experiment  \nA new musicologist who had not been involved in the construction of the GTTM database was asked to man u-\nally analyze the 300 scores in the database faithfully with regard to the GTTM. We provided onl y the 8 -bar-long \nmonophonic pieces of music to the musicologist, but she could refer the original score as needed. When analyzing pieces of music, she could not see the analysis results \nalready in GTTM database. She was told to take however \nmuch time she needed, and the time needed for analyzing one song ranged from fifteen minutes to six hours.  \n4.2 Analysis result s \nExperiments showed that the analysis results for 267 of \n300 pieces were the same as the original results in the \nGTTM database. The remaining 33 pieces had different \ninterpretations, so we added the 33 new analysis results \nto the GTTM database after they were cross -checked by \nthree other experts.  \n   For those 33 pieces with different interpretations, we \nfound the grouping structure in the database to be the \nsame as the grouping structure obtained by the new m u-\nsicologist. And for all 33 pieces, in the time -span tree the \nroot branch and branches directly connected to the root branch in the database were the same as the ones in the \nnew musicologist’s r esults.  \n   We also calculated the coincidence of time -spans in \nboth sets of results  for those  33 pieces . A time -span tree \nis a binary tree and each branch of a time -span tree  has a \ntime-span.  In the ramification of two branches, there is a \nprimary (salient ) time-span and secondary (nonsalient) \ntime-span in a parent time -span (Fig. 4). Two time -\nspans match  when the start and end times of the primary \nand secondary time -spans are the same.  We found that \n233 of  the 2310 time -spans  in those  33 pieces of music  \ndid not match .  \n \nFigure 4. Parent and primary and secondary time -spans.   \n4.3 An e xample  of analysis  \n\"Fuga C dur\" composed by Johann Pachelbel had the most unmatched time -spans when the analysis results in \nthe GTTM database (Fig. 5a) were compared with the analysis results by the new musicologist (Fig. 5b). From another musicologist we got the following comments \nabout different analysis results for this piece of music.  \n(a) Analysis result in GTTM database In the analysis result (a), note 2 was interpreted as  the \nstart of the subject of the fuga. Note 3 is more salient than note 2 because note 2 is a non -chord tone. Note 5 is \nthe most salient note in the time -span tree of first bar b e-\ncause notes 4 to 7 are a fifth chord and note 5 is a tonic of the chord. The reason that note 2 was interpreted as qqPrimary (salient) branch\nSecondary ( nonsalient ) branchParent time -span\nPrimary \ntime -spanSecondary\ntime -span\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n328t\nhe start of the subject of the fuga is uncertain, but a m u-\nsicologist who is familiar with music before the Baroque \nera should be able to see that note 2 is the start of the \nsubject of the fuga.   \n(b) Analysis result b y the musicologist  \nThe analysis result (b) was a more simple interpretation \nthan (a) that note 1 is the start of the subject of the fuga. \nHowever, it is curious that the trees of second and third \nbeats of the third bar are separated, because both are the \nfifth chord.  \nThe musicologist who made this comment said that it \nis difficult to analyze a monophonic piece of music from the contrapuntal piece of music without seeing other \nparts. Chord information is necessary for GTTM analysis, \nand a musicologist who is  using only a monophonic \npiece of music has to imagine other parts. This imagining \nresults in multiple interpretations.  \n5. CONCLUSION  \nWe described the publication of our Interactive GTTM \nanalyzer and the GTTM database. The analyzer and d a-\ntabase can be downloaded from the following website:  \nhttp:// www.gttm.jp/  \nThe GTTM database has the analysis data for the three \nhundred monophonic music pieces. Actually, the manual editor in our Interactive GTTM analyzer  enables one to \ndeal with polyphonic pieces. Although the analyzer itself works only on monophonic pieces, a user can analyze \npolyphonic pieces by using the analyzers’s manual editor to divide polyphonic pieces into monophonic parts. We \nalso a ttempted to e xtend the GTTM framework to enable \nthe analysis of polyphonic pieces [23]. We plan to publ i-\ncize a hundred pairs of po lyphonic score and musicol o-\ngists’ analysis results.  \nAlthough the 300 pieces in the current GTTM dat a-\nbase are only 8 bars long, we also plan  to analyse whole \npieces of music by using the analyzer’s slide bar for \nzooming piano roll scores and GTTM structures.  \n6. REFERENCES  \n[1] M. Hamanaka,  K. Hirata,  and S.  Tojo: “Time -Span \nTree Analyzer for Polyphonic Music,” 10th \nInternational Symposium on Computer Music \nMultidisciplinary Research  (CMMR2013), October \n2013.  \n[2] M. Hamanaka,  K. Hirata, and S. Tojo: “ATTA: \nAutomatic Time -span Tree Analyzer based on \nExtended GTTM,” Proceedings of the 6th \nInternational Conference on Music Information \nRetrieval  (ISMIR2005), pp. 358–365, September \n2005.  [3] F. Lerdahl and R. Jackendoff : A Generative Theory  \nof Tonal Music . MIT Press, Cambridge, 1983.  \n[4] G. Cooper and L. Meyer : The Rhythmic Structure  of \nMusic . University of Chicago Press, 1960.  \n[5] E. Narmour : The Analysis and Cognition of Basic  \nMelodic Structure . University of Chicago Press,  \n1990.  \n[6] D. Temperley : The Congnition of Basic Musical  \nStructures . MIT Press, Cambridge, 2001.  \n[7] M. Hamanaka, K. Hirata, and S. Tojo: Melody morphing method based on GTTM, Proceedings of the 2008 International Computer Music C onference \n(ICMC2008), pp. 155– 158, 2008.  \n[8] N. Todd: “ A Model of Expressive Timing in Tonal  \nMusic,” Musical Perception , 3:1, 33 –58, 1985.  \n[9] G. Widmer : “Understanding and Learning Musical  \nExpression, ” Proceedings of 1993 International  \nComputer  Music Conference (ICMC1993), pp. 268– \n275, 1993.  \n[10] K. Hirata, and R. Hiraga : “Ha-Hi-Hun plays  \nChopin’s Etude, ” Working Notes of IJCAI -03 \nWorkshop on Methods for Automatic Music  \nPerformance and their Applications in a Public  \nRendering Contest , pp. 72 –73, 2003.  \n[11] K. Hirata and S. Matsuda: “ Annotated Music for  \nRetrieval, Reproduction, and Sharing, ” Proceedings  \nof 2004 International Computer Music Conference  \n(ICMC2004), pp. 584– 587, 2004.  \n[12] K. Hirata  and S. Matsuda: “ Interactive Music  \nSummarization based on Generative Theory of  \nTonal Music, ” Journal of New Music Research, 32:2,  \n165– 177, 2003.  \n[13] H. Schaffrath : The Essen associative code: A code  \nfor folksong analysis. In E. Selfridge -Field (Ed.), \nBeyond  MIDI: The Handbook of Musical Codes , \nChapter  24, pp. 343 –361. MIT Press , Cambridge , \n1997.  \n[14] RISM: I\n nternational inventory  of musical sources. In \nSeries A/II Music manuscripts after 1600 ，K. G. \nSaur Verlag, 1997． \n[15] J. Riley, C . Hunter, C . Colvard, and A . Berry:  \nDefinition of a FRBR -based Metadata Model for the \nIndiana University Variations3 Project ，\nhttp://www.dlib.indiana.edu/projects/variations3/doc\ns/v3FRBRreport.pdf ，2007.  \n[16] M. Goto: Development of the RWC Music Database,  \nProceedings of the 18th International Congress on \nAcoustics  (ICA 2004), pp.  I-553– 556, April 2004.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n329[\n17] M. Goto, H . Hashiguchi, T . Nishimura, and R . Oka: \nRWC Music Database: Popular, Classical, and Jazz \nMusic Databases , Proceedings of the 3rd \nInternational Conference on Music Information \nRetrieval  (ISMIR 2002), pp.  287–288, October 2002.  \n[18] T. Eerola  and, P. Toiviainen: The Digital Archive of \nFinnish Folk Tunes , University of Jyvaskyla, \nAvailable  online  at: http://wwwjyufi/musica/sks , \n2004.  \n[19] C. McKay, D. McEnnis , and I. Fujinaga : A large \npublicly accessible prototype audio database for \nmusic research , Proceedings of the 7th  International \nConference on Music Information Retrieval  (ISMIR \n2006 ), pp.  160–164, October 2006. \n[20] N. Carlos, L. Alessandro, and A. Celso: The Latin \nMusic Database, Proceedings of the 9 th \nInternational Conference on Music Information \nRetrieval  (ISMIR200 8), pp.  451–456, September \n2008. \n[21] E. Acotto: Toward a formalization of musical \nrelevance,  in B. Kokinov, A. Karmiloff -Smith, and \nJ. Nersessian  (Eds.), European Perspectives on \nCognitive Science, New Bulgarian Unive rsity Press, \n2011.  [22] MakeMusic Inc.: Finale, Available online at:  \nhttp://www.finalemusic.com/, 20 14. \n[23] M. Hamanaka, K . Hirata, and S. Tojo: FATTA: Full \nAutomatic Time -span Tree Analyzer, Proceedings \nof the 2007 International Computer Music \nConference (ICMC2007), Vol.  1, pp.  153–156, \nAugust 2007. \n[24] Y. Miura, M. Hamanaka, K . Hirata, and S. Tojo: \nUse of Decision Tree to Detect GTTM Group Boundaries, Proceedings of the 2009 International \nComputer Music Conference  (ICMC2009), pp.  125–\n128, August 2009.  \n[25] K. Kanamori  and M . Hamanaka: Method to Detect \nGTTM Local Grouping Boundaries based on Clustering and Statistical Learning , Proceedings of \n2014 International Computer Music Conference \n(ICMC 2014)  joint with the 11th Sound & Music \nComputing Conference (SMC2014) , September  \n2014 ( accepted ). \n[26] M. Hamanaka, K . Hirata, and S. Tojo: Time -Span \nTree Analyzer for Polyphonic Music, 10th \nInternational Symposium on Computer Music \nMultidisciplinary Research  (CMM R2013), October \n2013.  1 2  3  4 5  67  8   9   10   11  12  13 14 15 16 17 18 19 20 21 22 23 24 25 26  27 28 29 30 31 32   33   34  351 2  3  4 5  67  8   9   10   11  12  13 14 15 16 17 18 19 20 21 22 23 24 25 26  27 28 29 30 31 32   33   34  35(a) Analysis result in GT TMdatabase\n(b)Analysis result by the musicologist  \n  \nFigure 5 . Time -span trees of \"Fuga C dur\" composed by Johann Pachelbel. \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n330"
    },
    {
        "title": "Hierarchical Approach to Detect Common Mistakes of Beginner Flute Players.",
        "author": [
            "Yoonchang Han",
            "Kyogu Lee"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415878",
        "url": "https://doi.org/10.5281/zenodo.1415878",
        "ee": "https://zenodo.org/records/1415878/files/HanL14.pdf",
        "abstract": "Music lessons are a repetitive process of giving feedback on a student’s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instru- ment. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical ap- proach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incor- rect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The sys- tem is structured hierarchically, as mis-fingering detec- tion requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Exper- imental results show that it is possible to identify differ- ent mistakes in flute performance using our proposed al- gorithms.",
        "zenodo_id": 1415878,
        "dblp_key": "conf/ismir/HanL14",
        "keywords": [
            "repetitive process",
            "feedback on performance",
            "improvement of performance skills",
            "unique characteristics of instruments",
            "common mistakes of beginner flute players",
            "hierarchical approach",
            "detection of common mistakes",
            "structured algorithms",
            "quantifying quality of flute sound",
            "structured hierarchy"
        ],
        "content": "HIERARCHICAL APPROACH TO DETECT COMMON \nMISTAKES OF BEGINNER FLUTE PLAYERS  \n Yoonchang Han, Kyogu Lee  \n Music and Audio Research Group \nSeoul National University, Seoul, Republic of Korea \n{yoonchanghan,kglee}@snu. ac.kr   \nABSTRACT  \nMusic lesson s are a repetitive process of giving feedback \non a student’s performance techniques. The manner in \nwhich  performance skills are improved depends on the \nparticular instrument, and therefore , it is important to \nconsider  the unique characteristics of the target instru-\nment. In this paper, we investigate the common mistakes \nof beginner flute players and propose a hierarchical ap-\nproach to detect such mistakes. We first examine the \nstructure and mechanism of the flute,  and define se veral \ntypes of common mistakes that can be caused by incor-\nrect assembl y, poor blowing skills,  or mis-fingering . We \npropose tailored algorithms for detecting each case by \ncombin ing deterministic  signal processing and deep \nlearning , to quantify the quality of a flute sound. The sy s-\ntem is structured hierarchically, as mis- fingering dete c-\ntion requires the input sound to be correctly assembled \nand blown to discriminate  minor sound difference. Expe r-\nimental results show that it is possible to identify differ-\nent mistakes in flute performance using our proposed al-\ngorithms. \n1. INTRODUCTION  \nThe most important part of  a music lesson is giving a stu-\ndent feedback on  his or her performance,  posture, and \nplaying skills so that the student can  play the sound  cor-\nrectly . Music lesson method s vary depending on the in-\nstrument  being learned; t herefore,  audio  signal processing \nfor music education should make extensive use of prior \nknowledge regarding playing style , common mistake s, \nunique characteristics , and con straints of the target in-\nstrument . However,  most existing music signal analysis \ntechniques use a general -purpose model , and relatively \nlittle attention is paid to an instrument- speci fic approach. \nA general -purpose model is advantageous  because it can \nbe applied to various types of instruments . However, this \nmodel lacks the capability to  captur e instrument- specific \nsound characteristic s. There are always common mistakes \nthat beginners make , but little is known about how to d e-\ntect these  automatically.       T he goal of this paper is to  investigate common be-\nginner’s mistake s when  playing  a specific instrument —\nthe flute, in this case —and to analyze the spectral chara c-\nteristic of each case to give the student  appropriate feed-\nback on his or her performance. Because the sound of a \nmusical instrument is affected by numer ous factors, in \nour work, we first divide the factors that usually lead be-\nginners  to play the wrong sound into three parts: incorrect \nflute assembly , blowing skill , and fingering.  \n     The rest of the paper is organized as follows: We \nbriefly present existing works related to our proposed idea. Then , we investigate  possible mistakes in flute pe r-\nformance by examining  the structure and mechanism of \nthe flute , and  several ty pes of common mistakes and the \nresulting sounds are explained . Next, we present an over-\nall system structure to distinguish each mistake,  along  \nwith a detail explanation  of each proposed algorithm . We \nthen present the experimental results to demonstrate the \nfe \nasibility of the proposed system, followed by our con-\nclusion and directions for future work.  \n2. RELATED WORK \nThe characteristics of musical instruments depend on \ntheir sound production mechanism . The characteristics of \none instrument can greatly differ fro m those of others, \nand each instrument’s characteristics  may not be cap-\ntured equally well as another even when  using the same \ncomputational model  [2]. However, there has been mi n-\nimal research regarding  an instrument- specific model. \nSome examples of instrument- specific approach es in-\nvolve the use of a violin [8, 14-16] , guitar  [1], bells [ 9], \nand tabla [ 5]. For instance, the violin transcription sys-\ntem in [ 8] makes use of characteristics such as highest \nand lowest pitch, possible play style  (e.g.,  upper octave \nduophony) , vibrato , and loudness.  The training system in \n[14] uses a common envelope style of violin sound for \nnote segmentation prior to real -time pitch detection , and  \n[9] uses the acoustic characteristics of a church bell , as \nwell as the rules of a bell charming performance , for \ntranscription and estimating the number of bells . In add i-\ntion, a chord transcription system designed for guitar in \n[1] outperforms the non- guitar -specific method.   \n     As shown above, using prior knowledge of the char-\nacteristics of a target instrument  creates new possibilities  \nin music signal pro cessing, and can also  improve the per- © First author, Second author, Third author.  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  First author, Second author, Third \nauthor.  “Paper Template For ISMIR 2014”, 15th International Society \nfor Music Information Retrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n77  \n \nformance of the system.  However, there are still many \ninstruments to be studied, and the flute is one of them.  \n3. COMMON MISTAKE S OF A FLUTE PLAY ER \n3.1 Assembling  the Flute  \nLike most woodwind instruments, the flute needs to be \nassembled before it is played . The f lute consists of a head \njoint, body joint, and foot joint , as shown in Figure 1. The \nconnecting part between the bod y and foot joint  is very \nshort , while  the connecting  part between the head joint \nand body joint is a few centimeters long. This intentiona l-\nly designed adjustable part is called the tuning slide, and \nit can be used for changing the total length of the flute to \nvarious sizes , which affects the overall pitch of the flute. \nFor instance,  if the head joint is placed very deep in to the \ntuning slide  of the body, the pitch will be increased for \nevery note. By contrast , if the head joint is pulled out too \nfar, the  overall pitch will drop owing to the longer wav e-\nlength.  \n \nFigure 1. Flute consists of head jo int, body joint , and \nfoot joint (modified after [11]). \n \n     Another method of pitch tuning is adjusting the cork \npart of the head joint , as shown in Figure 2. This can be \nadjusted by a screw . Pushing the cork will raise the pitch \nof all notes.  However, this is beyond the scope of th is pa-\nper, as this screw is normally not adjusted  by flute pe r-\nformers but by flute technicians . \n \n \n     Figure  2. Schematic of a flute head joint [1 3]. \n     Trained performers use this variable tuning slide  for \npitch tuning . The pitch of the  flute is sensitive to the co n-\nditions of the surrounding environment, such as humidity \nand temperature. However , novice flutists are not sens i-\ntive to minor pitch s hifting, and they may play the flute in \nthe wrong overall pitch without recognizing it.  \n3.2 Blowing  Embouchure  \nThe f lute generates sound by blowing a rapid air jet \nacross the embouchure1 hole, as shown in Figure 3 . \nHence, the quality of the generated sound is highly d e-\npendent on the blowing skil l of the performer . Blowing \nskill involves  lip position and the thickness /stability  of \nthe air jet . Clear  tone production  is challenging for begi n-\nners because  the method of tone production for the flute \n                                                             \n1 Mouthpiece of a musical instrument. is not supported by mechanical parts ; rather, it depends \nonly on the player’ s blowing skill [4]. \n \nFigure 3. Airstream o scillation of the flute embouchure \nhole. The labels indicate the phase angles of the acoustic \ncurrent at the hole [3]. \n     T one quality and octave of the sound are related to \nblowing skill. The flute has a range of three octaves , \nstarting from middle C  (C4), with several  less-used notes \nin octave s 3 and 7. The blowing pressure determines the \noctave of the sound , as shown in Figure 4. Greater blo w-\ning pressure can be achieved by blowing a narrower and \nstronger air jet. To generate  a stable and clean sound , it is \nimportant to  keep this blowing pressure reasonably \nsteady. Failure to do this will result in fluctuating sound \nand noise, which is highly unpleasant  and typically  the \nfirst hurdle  for beginners to overcome in  their training.   \n   \nFigure 4. Air jet blowing pressure has a roughly linear \nrelation ship to fundamental frequency. A, B, C , and D are \ndifferent performers, and different shapes represent di f-\nferent dynamics [1 2]. \n3.3 Fingering  \nNovice flutists frequently make  mistakes in fingering ow-\ning to their lack of familiarity with the irregular fingering \nrules of the flute. High -octave fingering is comparatively \nmore complex than low-octave  fingering  [4], which is the \nreason why flute lesson s usually start with the lowest oc-\ntave and move step-by-step to higher octaves. Hence, we \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n78  \n \nfocus on oc taves 4 and 5, which are the octaves that be-\nginner flute player s initially study.  \n     Most of the octave 5 fingerings are identical to  those \nof octave 4, as shown in Figure 3. H owever, the fingering \nfor C and D, as well as the sharps of these notes , require  \ndifferent fingering s than those of octave 4 . These notes \ncan be played with octave 4 fingering using a faster and \nsharper air jet, but this result s in a slightly airy  timbre , \ncompared to the sound when the flute is correct ly fin-\ngered. As this airy  timbre is not significantly noticeable, \nand most of the notes in octaves 4 and 5 share the same \nfingering, many beginners do not notice that they used \noctave 4 fingerings to play octave 5, unless the instructor  \nspots it. \n \nFigure 5. Fingering of octave 4 and 5  flute notes . Note \nthat C, D, and sharp s of these require  different  fingering , \nunlike E, F, G, A , and B . \n     Another fingering -related problem is the proper posi-\ntioning  of the finger s. The open- hole flute requires that \nthe flutist use his or her finger s to block the hole s in the \nkeys. Most professional flutists prefer the open- hole flute \nowing to its advantage s in tone productio n and intonation \nadjustment [4 ]. However,  this is  not considered  in our \nsystem  because beginners who have  trouble with block-\ning open- hole keys can avoid  this problem by  putting \nplastic plug s in the hole s until they get used to playing \nthe open- hole flute. \n4. PROPOSED SYSTEM & ME TRICS  \nThe overall system comprises several steps. In the first step, the system determines whether the flute is asse m-\nbled correctly  using entire input audio . Next, once the \nflute sound is detected as coming from a correctly asse m-\nbled flute, the system measures if sound of the each note \nis a clear, correctly blown sound or an airy -timbered \nsound. Finally, the properly blown sound is identified as \nsound generated from either correct fingering or incorrect \nfingering. The system is hierarchical ly structured , be-\ncause mis- fingering detection does not work  well for \nfluctuated sound or  head joint pushed/pulled sound as it \nrequires d iscriminating minor  sound differ ence.  The i nput \naudio is resampled to 16  kHz first, and he system arch i-\ntecture is shown in Figure 6.  \n4.1 Assembling Error Detection  \nSome mistake s can cause modifications to the overall \npitch, and some mistakes result in poor timbre.  The as-\nsembling error  affects only the overall pitch of the gene r-\nated sound.  As mentioned in 3.1, the distance  the head joint is pushed in or pulled out from the tuning slide of \nthe body joint determin es the overall pitch.  To this end, a \nquantized chromagram from Harte and Sandler is used to detect the tuning center [6].  \n \nFigure 6. Flow diagram of the overall system. The bold \nbox indicates where the system sends  feedback to the us-\ner. \n \nFigure 7. HPCP peaks histogram  within a semitone for \ncorrectly (up) and loosely assembled  (down)  cas\ne. \n \n     T o determine the tuning center, a spectrum of linear \nfrequency spectra is Constant -Q transformed and \nsummed across octaves to produce a harmonic pitch class \nprofile (HPCP). A 36-bin quantized chromagram is used \nto determine the semitone center, and three bins were al-\nAudio input\nMis-ﬁngering detectionFluctuated\nCorrectCorrectly\nassembledHead joint \npushedHead joint \npulled\nCorrectly\nblownFluctuated sound detectionFind tuning center (0-3)\nMis-ﬁngering\n0 0.5 1 1.5 2 2.5 30510152025303540Distribution of HPCP peaks\nHPCP peaksCount\n0 0.5 1 1.5 2 2.5 3051015202530Distribution of HPCP peaks\nHPCP peaksCount\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n79  \n \nlocated for each semitone.  By observing the distribution \nof peak positions across the width of a semitone , as \nshown in Figure 7, it is possible to determine the tuning \ncenter of the instrument. Because three bins are allocated \nfor each semitone, the tuning center of a perfectly tuned \nsound would ideally be 1.5. Therefore, the system will \nconsider the input sou nd to be correctly tuned when the \ntuning center value is approximately 1.5. If the detected \ntuning center is  too low  (less than 1), the system sends  \nfeedback to the user that the head joint is too loosely a s-\nsembled. Conversely , the system tells the user that the \nhead joint is assembled more tightly than necessary  when \nthe tuning center  is high  (greater than 1).  \n4.2 Fluctuated S ound Detection  \nIncorrect lip position on the embouchure , along with an \nirregular stream  of blown wind , results in a highly un-\npleasant  and fluctuating tone. This sound contains many  \ninharmonic partials in a spectrum , and it is clearly visible \non a spectrogram.  Performing binary masking on a spec-\ntrogram makes these  inharmonic partials more obvious , \nas shown in  the second row of Figure 8.  \n \n \nFigure 8. Log spectrogram,  binary masked spectrogram , \nand sum of bins fo r each frame for D, E, F, G, A, and B \nof octave  5. Up to 6 second is correctly blown sound and \nfrom 6 to 12 second is fluctuated sound.  \nBinary masking is performed as follows: \n \n                                                                                                       (1) \n where X is the log spectrum , Xb is the binary masked \nspectrum , and θ is the thresh old constant. Empirically, a \nvalue between -20 and -30 works well for θ, depends on \nrecording environment.  Note that these values are ob-\ntained  when  natural log multiplied by 20  is used  for the \nlog spectrum . Using this binary masked spectrogram, the \nsum of the number of positive valued bins of each spec-\ntrum can be used as a measure ment  for determining how \nthe sound fluctuates  owing to poor blowing skill . This \ncan be expressed as follows: \n                                                                                                            \n                                                                                                       (2) \n \nwhere F is  the amount of fluctuation.  The third row of \nFigure 8 is F value obtained from (2) with 1 second m e-\ndian filtering, and it is possible to observe the value is \nmuch higher for fluctuated sound than correctly blown \nsound.  \n4.3 Mis-fingering detection \nAs mentioned in  3.3, for C5, C #5, D5, and D#5, using \noctave 4 fingering with a faster and sharper air jet still \ngenerates octave 5 pitches even without correct fingering, \nalthough the timbre is slightly airy. To detect this timbral \ndifference, we decided to use both the Mel-frequency \ncepstral coefficient (MFCC )—a widely used , hand-\ndesigned feature— and sparse filtering  (SF) [ 10]— a deep-\nlayered , unsupervised feature learning  method . SF works \nby optimizing the sparsity of feature distribution, and it \nworks well on a range of data modalities without specific \ntuning. Both single - and double- layered sparse filtering \nwere  used with 200 units for each layer. The o btained \nfeature was classified into two classes (correct/incorrect) \nusing a random forest (RF) classifier, which exhibits be t-\nter performance than a support vector machine or back-\npro\npagation neural network  in a variety of cases [7]. The \nflow diagram for mis- fingering detection is shown below.  \n \nFigure 9. Flow diagram for mis -fingering detection.  \nLog spectrogram\nTime(sec)Frequency(Hz)\n1 2 3 4 5 6 7 8 9 10 110\n2000\n4000\n6000\n8000\nBinary masked spectrogram\nTime(sec)Frequency(Hz)\n  \n1 2 3 4 5 6 7 8 9 10 110\n2000\n4000\n6000\n8000\n0 2 4 6 8 10 12050100150200Sum of binary spectrum\nTime(sec)Bin count\nSF\nCorrect Mis-ﬁngeringCorrectly blown sound\nSpectrogram\nClassiﬁerMFCC Feature\nRandom ForestSF + MFCC\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n80  \n \n5. EXPERIMENT \n5.1 Objective & Procedure  \nThe goal of our experiment  was to explore whether the \nproposed system and algorithms work well for detecting \nthe mistakes of  beginner  flutists . Flute sound samples \nwere obtained from two intermediates (who have played \nthe flute for one to two  years) and one expert ( who holds \nan exam score of  Grade 8 with a Distinction).  Flutes used \nfor the experi ment were a B foot joint with open hole s, \nand a silver head with nickel body and foot . The c orrect \nflute sound, fluctuating  sound, head joint pulled , and  \nhead joint pushed sound were recorded  for octaves b e-\ntween 4 and 5 . The l ength of the collected audio  was 30 \nseconds  for each semi tone. The case of correct and incor-\nrect fingering for C5, C#5, D5, and D#5  was recorded for \n10 minutes  each to obtain sufficient training  data. The \ninput audio was recorded at 44.1 kHz mono and \ndownsampled to 18 kHz. Tuning center  was calculated \nfrom whole target audio as it is not time -varying charac-\nteristics . Meanwhile, fluctu ating  and mis- fingering dete c-\ntion was performed framewise.  Different window and \nhop size were used for each experiment, as each mistake \ndetection algorithm requires different spectral resolution.  \n5.2 Results  \nThe e xperiment al result s show that the system successful-\nly distinguishes each mistake. To find tuning center, a 74 \nms window and 18 ms hop size were used. As shown in \nTable 1, the tuning center of a correctly played sample is \nclose to 1.5, which is the exact center.  Also , tuning  center \nvalues for the head joint when it is pushed and pulled  fell \ninto the expected range, which were (0–1) and (2 –3), re-\nspectively.  \nMistake cases  Tuning center value (0 to 3) \nPlayer 1 Player 2 Player 3 \nCorrect  1.68 1.59 1.62 \nHead joint pushed  2.55 2.49 2.43 \nHead joint pulled 0.48 0.45 0.75 \nTable 1 . Tuning center values of correct, head joint \npushed, and head joint pulled flute sound of three diffe r-\nent flut ists. \n     Next, Figure 10 is a framewise distribution  of fluctua-\ntion measure (1) for correct  and fluctuated flute sound.  A \n64 ms window and 32 ms hop size were used, with θ val-\nue of -25dB.  The m edian value of the correct flute sound \nis 50, and most of the value s fall between 63 and 3 7. The \nfluctuating  flute sound has a median value of 167, and \nmost of the value s fall between 150 and 178 . This means \nthat these cases are clearly distinguishable using the pro-\nposed metric.       Finally, Table 2 shows the ten -fold cross -validation \nresults of the proposed mis -fingering classification using \nsingle- layer SF, double- layer SF, and MFCC as a feature, \nand RF as a classifier. A 16 ms window and 10 ms hop \nsize were used, and SF was used wi th 200 units per layer.  \n \nFigure 10. Box plot of f luctuation measure ments . The \ncentral marks indicate  the median, and the edges are the \nfirst and third quartiles.  \n \nMethod  Accuracy  (%) \nSpectrogram + SF (s ingle) 90.24 \nSpectrogram + SF ( double)  90.02 \nMFCC  90.89 \nMFCC + SF  (single)  90.33 \nMFCC + SF (double)  91.35 \nTable 2 . Mis-fingering classification ten-fold cross-\nvalidation result using SF/ MFCC as a feature and  RF as \na classifier.  \nThe result shows that the combination of the MFCC and  \ndouble -layered  SF performs the best ; however , all of the \napproaches perform  reasonably well within a not very \nmeaningful margin.  The result indicates that the MFCC, a \nhandcrafted feature, is still useful in separat ing the tim-\nbral difference s of the flute. Further , although SF is not \ndesigned for the purpose of timbre analysis, it works \nquite well without fine -tuning , as mentioned in [ 10]. In \nthe experiment, single -layered SF worked better when the \ninput is  a spectrogram, but double -layered SF showed \nbetter performance when the input is MFCC.  \n6. CONCLUSION  & FUTURE WORK  \nThe objective of our  work is to use audio signal analysis \nto give a student  feedback on his or her flute performance \nto help fix mistakes , as a lesson teacher would  do. T o \nachieve this goal, we examined the mechanism and stru c-\nture of the flute. We  also investigated the common mi s-\ntakes of be ginner  flute players . We determined  several  \ntypes of common mistakes and developed a  hierarchical \nsystem to detect such cases by observing  the tuning ce n-20406080100120140160180\ncorrect fluctuatedFluctuation measureF value\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n81  \n \nter, fluctuation metric , and a mis-fingering detection a l-\ngorithm . As a result, we have successfully identified \ncommon mistake cases fr om input audio, which can be \nused as  feedback that would be provided by a  lesson \nteacher-. Head -joint assembling errors were detected by \ndetermining the tuning center of the flute sound . Fluctuat-\ning sound caused by poor blowing skill s was separated \nfrom the correct flute sound by measuring the amount of \nnoisy harmonic contents.  Finally, mis -fingering case s \nwere detected by analyzing their timbre using MFCC and \nSF with an RF classifier . \n     There remain some problems to be tackled in this mis-\ntake detection algorithm for real -world user application s. \nFirst, the mis-fingering dete ction algorithm may be af-\nfected by the material  or maker of the flute because  the \nalgorithm detects very minor change s in timbre . In the \nexperiment,  only two types of flute ( silver head with \nnickel body, and f oot) were used. However, the flute can \nbe made of various types of metal , such as silver, gold, \nand platinum . Moreover , various flute makers have their \nown timbral characteristics, which may influence the \nclassification results . Second, the experiment was do ne \non the frame level, but the user perceive s the score based \non the note level. Hence, the system should be used along \nwith appropriate onset -offset detection to give more user -\nfriendly feedback.  \n     We believe that this type of timbre- related and user -\nbehavior -oriented feedback is highly important for the \nnext-generation music transcription system s, especially \nthose used for educational purpose s. Playing the instru-\nment with correct onset and pitch is not a very difficult \npart of  being a good player, but making a beautiful timbre \nis what  really takes time.  This paper focuses only on the \nflute; however , our overall approach , including  analyzing \nmistake cases and determining customized solution s, can \nbe applied to various instruments in a similar way.  \n7. ACKNOWLEDGEMENTS  \nThis research was supported by the MSIP  (Ministry of \nScience, ICT & Future Planning), Korea, under the ITRC  \n(Information Technology Research Center) support pr o-\ngram supervised by the NIPA  (National IT Industry Pr o-\nmotion Agency\" (NIPA -2013- H0301 -13-4005).  \n8. REFERENCES  \n[1] A. M. Barbancho, A. Klapuri, L.  J. Tardon, and I.  \nBarbancho : “Automatic Transcription of Guitar \nChords and Fingering from Audio, ” IEEE TASLP, \n20(3): 915– 921, 2012. \n[2] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchoff, \nand A. Klapuri : “Automatic Music Transcription: \nBreaking the Glass Ceiling ,” In ISMIR , 2012 . \n[3] J. W. Coltman: “ Sounding Mechanism of the Flute \nand Organ Pipe.” The Journal of the Acoustical S o-\nciety of America  44, 1968. [4] C. Delaney, Teacher’s Guide for the Flute. Rev. \n11/98, Selmer, 1969.  \n[5] O. Gillet and G. Richard : “Automatic Labelling of \nTabla Signals ,” In ISMIR , 2003. \n[6] C. Harte and M. Sandler : “Automatic Chord Ident i-\nfication using a Quantised Chromagram. ” In Audio \nEngineering Society Convention 118 . 2005. \n[7] M. Liu, M. Wang, J. Wang , and D. Li : “Comparison \nof Random Forest, Support Vector Machine and \nBack Propagation Neural Network for Electronic \nTongue Data Classification: Application to the \nRecognition of Orange Beverage and Chinese Vin e-\ngar,” Elsevier Sensors and Actuators, pp. 970– 980, \nVol. 1 77, 2013.  \n[8] A. Loscos, Y. Wang, and W. J. Boo : “Low Level \nDescriptors for Automatic Violin Transcription .” In \nISMIR , 2006. \n[9] M. Marolt : “Automatic Transcription of Bell Chi m-\ning Recordings. ” In IEEE TASLP , 20(3): pp. 844–\n853, 2012.  \n[10] J. Ngiam, P. W. Koh, Z. Chen, S. Bhaskar and A. Y. Ng., “Sparse Filtering ,” In NIPS, 2011.  \n[11] H. Pinks terboer, Tipbook Flute and Piccolo: The \nComplete Guide, Hal Leonard, 2009 . \n[12] T. D. Rossing, F. Richard Moore, and P. A. \nWheeler : The Science of Sound. Vol. 2. Massach u-\nsetts. Addison -Wesley, 1990.  \n[13] J. Smith, J. Wolfe, and M. Green : “Head Joint , \nEmbouchure Hole and Filtering Effects on the Input \nImpedance of Flutes.” In Proc. of the Stockholm \nMusic Acoustics Conference , pp. 295– 298. 2003.  \n[14] J. Wang, S. Wang, W. Chen, K. Chang, and H. \nChen : “Real-Time Pitch Training System for Violin \nLearners ,” Multimedia and Expo Workshops \n(ICMEW), IEEE , 2012 . \n[15] R. S. Wilson : “First Steps Towards Violin Perfo r-\nmance Extraction using Genetic Programming ,” In \nJohn R. Koza , editor , Genetic Algorithms and Ge-\nnetic programming , pp. 253– 262, 2002.  \n[16] J. Yin, Y. Wang, and D. Hsu : “Digital Violin Tutor: \nAn Integrated System for Beginning Violin Lear n-\ners,” ACM Multimedia, Hilton, Singapore, 2005.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n82"
    },
    {
        "title": "Predicting Expressive Dynamics in Piano Performances using Neural Networks.",
        "author": [
            "Sam van Herwaarden",
            "Maarten Grachten",
            "W. Bas de Haas"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416678",
        "url": "https://doi.org/10.5281/zenodo.1416678",
        "ee": "https://zenodo.org/records/1416678/files/HerwaardenGH14.pdf",
        "abstract": "This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these fea- tures are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a B¨osendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by differ- ent pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach dif- fers from earlier work in a number of ways: (1) an ad- ditional input representation based on a local history of velocity values is used, (2) the RBMs are trained to re- sult in a network with sparse activations, (3) network con- nectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance.",
        "zenodo_id": 1416678,
        "dblp_key": "conf/ismir/HerwaardenGH14",
        "keywords": [
            "predicting",
            "expressive",
            "accentuation",
            "piano",
            "performances",
            "neural",
            "networks",
            "Restricted",
            "Bosendorfer",
            "piano"
        ],
        "content": "PREDICTING EXPRESSIVE DYNAMICS IN PIANO PERFORMANCES\nUSING NEURAL NETWORKS\nSam van Herwaarden\nAustrian Research Institute for AI\nsamvherwaarden@gmail.comMaarten Grachten\nAustrian Research Institute for AI\nhttp://www.ofai.at/˜maarten.grachtenW. Bas de Haas\nUtrecht University\nw.b.dehaas@uu.nl\nABSTRACT\nThis paper presents a model for predicting expressive\naccentuation in piano performances with neural networks.\nUsing Restricted Boltzmann Machines (RBMs), features\nare learned from performance data, after which these fea-\ntures are used to predict performed loudness. During\nfeature learning, data describing more than 6000 musical\npieces is used; when training for prediction, two datasets\nare used, both recorded on a B ¨osendorfer piano (accurately\nmeasuring note on- and offset times and velocity values),\nbut describing different compositions performed by differ-\nent pianists. The resulting model is tested by predicting\nnote velocity for unseen performances. Our approach dif-\nfers from earlier work in a number of ways: (1) an ad-\nditional input representation based on a local history of\nvelocity values is used, (2) the RBMs are trained to re-\nsult in a network with sparse activations, (3) network con-\nnectivity is increased by adding skip-connections, and (4)\nmore data is used for training. These modiﬁcations result\nin a network performing better than the state-of-the-art on\nthe same data and more descriptive features, which can be\nused for rendering performances, or for gaining insight into\nwhich aspects of a musical piece inﬂuence its performance.\n1. INTRODUCTION\nMusic is not performed exactly the way it is described in\nscore: a performance in which notes occur on a regular\ntemporal grid and all notes are played equally loud is often\nconsidered dull. Depending on the instrument, perform-\ners have different parameters they use for modulating ex-\npression in their music [14]: time (timing, tempo), pitch,\nloudness and timbre. For some of these parameters com-\nposers add annotations to musical score describing how\nthey should be varied, but for a large part performers are\nexpected render the score according to tacit knowledge,\nand personal judgment. This allows performers to imbue\non a performance their personal style, but this is not to say\nthat music performance is arbitrary—it is often clear which\nc\rS. van Herwaarden, M. Grachten, W.B. de Haas.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Sam van Herwaarden, Maarten\nGrachten, W. Bas de Haas. “Predicting expressive dynamics in piano per-\nformances using neural networks”, 15th International Society for Music\nInformation Retrieval Conference, 2014.interpretations are (not) musically appropriate.\nThis article describes a number of modiﬁcations to the\nmethod for modeling expressive dynamics proposed by\nGrachten & Krebs [7], and is based on the MSc thesis\nwork described in [17]. We show that, with an additional\ninput representation and a different set-up of the machine\nlearning approach, we achieve a statistically signiﬁcant im-\nprovement on the prediction accuracy achieved in [7], with\nmore descriptive features. Our achieved performance is\nalso comparable with the work in [8]. In the following sec-\ntions we ﬁrst summarize previous work in this area, fol-\nlowed by an overview of the used machine learning archi-\ntecture. We then describe the experiments, the results and\nthe relevance of the ﬁndings.\n2. PREVIOUS WORK\nTwo important aspects of music that affect the way it is to\nbe performed are the musical structure, and the emotion\nthat the performance should convey [13]. The last decades\ndifferent methods for analyzing the structural properties of\na piece of music have been proposed (e.g. [12, 15]), where\nthe analysis tends to stress the relationship between struc-\nture on a local level (elements of pitch and rhythm) and\ntheir effect on the melodic expectancy of a listener. Emo-\ntional charge conveyed by a piece is more abstract and vari-\nable: trained musicians can play the same piece conveying\ndifferent emotions, and in fact these emotions can be iden-\ntiﬁed by listeners [5].\nBecause musical structure can be studied through in-\nspection of the musical score, computational models of\nmusical expression tend to focus on this. A number of\ndifferent computational models of expression have been\ndeveloped earlier, studying different expressive parame-\nters (e.g. [1, 4]). Many models are rule-based, where the\nrules describing how expression should be applied are of-\nten hand-designed. Other models still focus on rules, but\nautomatically extract them from performance data (e.g. [11,\n18]). A performance model can also be based on the score\nannotations for the relevant parameter provided by the com-\nposer, as in [8] which uses information on note pitch, loud-\nness annotations and other hand-crafted features.\nSome recent studies model regularities in musical se-\nquences using unsupervised techniques [2, 16], in the con-\ntext of musical sequence prediction. Grachten & Krebs [7]\napply unsupervised learning techniques to learn features\nfrom a simple input representation based on a piano roll\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n47representation of the symbolic score, in the context of pre-\ndicting musical expression. The resulting learned features\nthen describe common patterns occurring in the input data,\nwhich can be related to concepts from music theory and\nused for prediction of expressive dynamics. By using a\nsimple input representation and network, the model re-\nmains relatively transparent with regard to its inner work-\nings. It is shown that Restricted Boltzmann Machines\n(RBMs) learn the most effective model, and in this paper,\nwe build on that approach.\nAn RBM is a type of artiﬁcial neural network, particu-\nlarly suitable for unsupervised learning from binary input\ndata. During training it learns a set of features that can ef-\nﬁciently encode the input data. The features are used to\ntransform the input data non-linearly, which can be useful\nfor further (supervised) learning. For a detailed explana-\ntion of RBMs the reader is referred to for example [9, 10].\n3. ARCHITECTURE\nFigure 1 illustrates the setup of the network we use. As\ninput the network sees the music data in two different\nrepresentations: the score-based note-centered represen-\ntation ﬁrst developed by [7] and the new loudness-based\nvelocity-history representation. The input data is trans-\nformed through a series of hidden unit activations (RBM\nfeature activations) in L1,L2andL3. These feature ac-\ntivations are then used to estimate the output (normalized\nvelocity). As is typical with neural networks, the model\nis blind to the meaningful ordering of the input nodes (we\ncould change the ordering without affecting the results).\nThe set-up is different from that in [7] in a number of\nways: (1) an additional input representation based on a lo-\ncal history of velocity values is used, (2) the RBMs are\ntrained for sparse activations, (3) network connectivity is\nincreased with skip-connections (i.e. w1andw2in Figure\n1 can be used simultaneously), and (4) more data is used\nfor training. The following sections cover these changes in\nmore detail. First, we describe the data available for devel-\noping the model. We then describe the way these data are\npresented to our model as input and output, and ﬁnally the\nprocess of training and evaluating our model.\n3.1 Available data\nData from a number of sources is used for the exper-\niments in this paper. We have score data, which de-\nscribes musical score in a piano-roll fashion, and we have\nperformance data, based on recordings from a computer-\ncontrolled B ¨osendorfer piano. For the performance data,\naccurate note on- and offsets are available as well as ve-\nlocity values, and these values have been linked to corre-\nsponding score data. For all available performance data,\nscore data is also available, the converse does not hold.\nA number of (MIDI) score datasets is used: the\nJSB Chorales,1some MuseScore pieces,2the Mutopia\n1www.jsbchorales.net\n2www.musescore.orgvelocity-history\nnote-centeredL3\nL1L2vt\nw1w2 w3\nFigure 1: The used architecture. The rounded squares\ncorrespond to in- and outputs, the circles to layers of hid-\nden units trained as Restricted Boltzmann Machines. w1\nthrough w3are the weights used to predict vtbased on the\nhidden unit activations in hidden layers L1through L3.\nw1through w3are determined with a least-squares ﬁt.\ndatabase,3the Nottingham database,4the Piano-midi\narchive5and the V oluntocracy dataset6. These datasets\nare used during unsupervised learning with the note-\ncentered representation only. The performance datasets\nwe use have been developed at the Austrian Research In-\nstitute for AI (OFAI). One dataset contains performance\ndata of all Chopin’s piano music played by Nikita Maga-\nloff [3] (\u0018 300:000 notes in 155 pieces), the other contains\nall Mozart piano sonatas, performed by Roland Batik [18]\n(\u0018100:000 notes in 128 pieces). These datasets have been\nused both for unsupervised and supervised learning.\n3.2 Note-centered representation\nScore data is input into the network in one form in the\nnote-centered representation, which is based on a piano-\nroll representation. For every note in a musical score, an\ninput sample is generated with this note in the center, as\nillustrated in Figure 2b. The horizontal axis corresponds\nto score time and covers a span of 3 beats before the onset\nof the central note to 3 beats after the onset. Each beat is\nfurther divided into 8 equal units of time (effectively each\ncolumn in the input corresponds to a 32ndnote), and longer\nnotes are wider. The vertical axis corresponds to relative\npitch compared to the central note, and covers a span of\n\u000055 to+55 semi-tones. To allow the representation to dis-\ntinguish between separate notes of the same pitch played\nconsecutively, and a single long note at that pitch, note du-\nrations are represented as their score duration minus 32nd\nnote duration (this was also done in [7]).\nThis approach is the same as the duration coding ap-\nproach used in [7] with two exceptions: they experimented\nwith time-spans of 1, 2 and 4 beats (with very small dif-\n3www.mutopiaproject.org\n4www.chezfred.org.uk/University/music/\ndatabase.htm\n5www.piano-midi.de\n6www.voluntocracy.org\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n48(a)\n(b)\n(c)\nFigure 2: A short piece of score, and resulting network input for the note indicated by the arrow: (a) shows the score,\nwhere the annotations should be interpreted as performed loudness, not as annotated loudness directives, (b) shows the\nnote-centered representation and (c) the velocity-history representation.\nferences in results between the 2 and 4 beats experiments),\nand used a pitch range of \u000087 to+87 semi-tones (so that\nalways the entire piano keyboard is covered). In practice,\nthe large pitch range is likely unnecessary and only in-\ncreases the length of the network input vector (note com-\nbinations with such intervals are very rare and do not no-\nticeably affect the learned features).\nThis choice of representation makes our system insen-\nsitive to absolute pitch: if all input notes are transposed by\na few semi-tones in the same direction, the generated in-\nput samples will be identical. This also allows the system\nto learn about harmony based on relative pitch: for exam-\nple certain chords will typically be represented in the same\nway regardless of their root tone. No additional informa-\ntion on absolute note pitch was included, to keep the model\nsimple.\n3.3 Velocity-history representation\nWhen analyzing expressive parameters in existing perfor-\nmances, it is interesting to not only take into account direct\nharmonic and rhythmic structure around a note as is done\nwith the note-centered representation, but also effects in\ncontinuity of musical phrases: for example, in many cases\nnote loudness increases or decreases gradually over a num-\nber of notes. The precise accentuation of a note is than\naffected by the accentuation of preceding notes.\nOur velocity-history representation is designed to en-\ncode this kind of information. Figure 2c illustrates this rep-\nresentation. Conceptually, it is similar to the note-centered\nrepresentation, with a few differences: the vertical axis\nnow represents relative velocity (normalized with respect\nto the mean \u0016and standard deviation \u001bof the velocity in\na piece, where the range from \u0016\u00002\u001bto\u0016+ 2\u001b is quan-\ntized into 12 discrete values), and the horizontal axis cor-\nresponds to the time preceding the current note (ranging\nfrom note onset\u00003beats to note onset +0).\nThe velocity-history representation uses information\nfrom an actual performance during prediction. In a sense,\nthe system is asked to predict the continuation of a musical\nphrase: given that the last notes were played in a certain\nway, how will the next note be played? When using this\nrepresentation, experiments with our model aim to explainhow a note is performed in an existing performance, rather\nthan predict it for a new piece of bare score (an actual per-\nformance needs to be available).\n3.4 Velocity normalization\nSince we use semi-supervised learning, at some point we\nneed target values accompanying our input representations.\nWe have exactly one sample for each note, and we are\nstudying dynamics, so the logical parameter to base these\ntarget values on is note velocity. However, the different\npieces described in our data have fairly diverse character-\nistics when it comes to dynamics. Some pieces are per-\nformed louder on average, or have stronger variations in\ndynamics. In this study we have chosen to focus on lo-\ncal effects within a single piece, and not so much on dif-\nferences between pieces. For this reason we normalize\nour velocity target values so they have zero-mean and unit\nstandard-deviation within a piece (we use these values both\nfor supervised learning and for generating the velocity-\nhistory representation). This is slightly different from the\nnormalization used in [7], where normalization was only\nused to obtain zero-mean within a piece.\n3.5 Training and evaluation\nThe process of developing and testing the network can be\nseparated into three phases: unsupervised learning, super-\nvised learning and performance evaluation. We will now\ndescribe these in more detail.\n3.5.1 Unsupervised learning\nDuring unsupervised learning, we train only hidden layers\nL1through L3. The layers are trained as RBMs on the full\nset of score data in the note-centered and velocity-history\nrepresentations, where L1andL3are trained on the input\nrepresentations directly, and L2is trained on the feature\nactivations in L1.\nIn the note-centered representation samples consist of\n5280 binary input values. L1is trained with 512 hidden\nunits (ensuring a signiﬁcant bottleneck in the network),\nandL2contains fewer hidden units again: 200 units. In\nthe velocity-history representation samples consist of 288\ninput values, these are encoded in 120 hidden units in L3.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n49We enforce sparse coding in the network, using the\nmethod proposed in [6], which allows us to not only con-\ntrol the average activation of hidden units in the network,\nbut also the actual distribution of activations: we can force\nthe RBM to represent each sample as a number of highly\nactive features, improving inspectability.\n3.5.2 Supervised learning\nFor supervised learning we use a simple approach: given\nthe transformation of an input sample by L1toL3, we\nﬁt the hidden unit activations in these layers to the corre-\nspondingvt(normalized velocity) values using least-squares.\nExploratory experiments suggested that more advanced tech-\nniques do not yield much better results. Thus, w1through\nw3simply deﬁne a linear transformation from the features\nactivations to a prediction of the normalized velocity.\n3.5.3 Performance evaluation\nTo evaluate the performance of our model we use a leave-\none-out approach: we cycle through all the pieces in the\nperformance data, where every time a particular piece is\nleft out during supervised learning, after which the trained\nnetwork is used to predict the expressive dynamics of the\nleft-out piece. The quality of the prediction is then quan-\ntiﬁed using the R2measure (coefﬁcient of determination).\nAs mentioned before, the full set of data is used during\nunsupervised learning – because the objective function op-\ntimized during this phase has no relation to the velocity\ntargets, we believe that this is an acceptable approach. As\nthe ﬁnal score after cycling through the whole dataset in\nthis fashion, we use the weighted average R2, where the\nnumber of notes in a piece is used as its weight.\n4. EXPERIMENTS\nIn our experiments we vary two parameters: network con-\nnectivity, and training/testing datasets. Other experiments\nwere also done but are not described in this paper, for these\nthe interested reader is referred to [17].\n4.1 Network connectivity\nDifferent parts of our model describe information con-\ncerning different aspects of the input data. The note-\ncentered representation corresponds to rhythmic and har-\nmonic structure of the score surrounding a note, while the\nvelocity-history representation relates more closely to ex-\npressive phrases. This distinction continues through the\nlayers of feature activations. To get an impression of how\nstrongly the expressive variation in velocity data corre-\nsponds to these different aspects, we experimented with\nthe different layers in isolation and together. We will re-\nfer to the network conﬁgurations by the layers that were\nused during training and prediction, i.e. L1;2means both of\nthe layers on top of the note-centered representation were\nused, and L3was not. Another way to see this would be\nthatw3is constrained to be a matrix of only 0’s.no vel. inf. with vel. inf.\nL1L1;2 L2L3L1;2;3\nM.!M. .202 .207 .191 .315 .470\nB.!B. .366 .376 .357 .236 .532\nB.!M. .132 .126 .125 .286 .386\nM.!B. .291 .295 .283 .209 .457\nAll!M. .198 .203 .186 .313 .466\nAll!B. .341 .350 .329 .222 .503\nTable 1:\u0016R2scores obtained on the test data. X!Y\nindicates the model was trained on Xand tested on Y,\nwhere M.is the Magaloff and B.the Batik dataset. Experi-\nments with velocity information (vel. inf.) use the velocity-\nhistory representation as input. We use the underlined re-\nsult for comparison with previous work ( [7] and [8]).\n4.2 Training datasets\nExperimenting with different sets of training data is inter-\nesting for several reasons. One is that from a musicological\nperspective, the structure of music of different styles can be\nquite different. As an extreme example, a system trained\non Jazz music would not be expected to reliably predict\nperformances of piano music by Bach. Another reason is\nthat we can use combinations of datasets to test the valid-\nity of our model: if a model trained on music from one set\nof recordings, still performs well on another set of record-\nings, this can give us some conﬁdence that our model has\nlearned something about music in a general sense, and not\njust about the particular dataset.\nAs mentioned before, we use two datasets: one describ-\ning performances of Chopin music and the other Mozart\nmusic. In all cases, during testing we kept the datasets\nseparate. However, we varied the set of data used for train-\ning: we trained on the same dataset as used for testing,\nwe trained on one dataset and tested on the other, and we\ntested a model trained on all data.\n5. RESULTS\nTable 1 lists the results obtained with our model. The\nmodel is more successful explaining the variance in the\nBatik (Mozart) data than in the Magaloff (Chopin) data –\none possible explanation for this is that Chopin’s music\n(from the Romantic period) has much more extreme varia-\ntions in expression than Mozart’s music (from the Classical\nperiod). It seems reasonable that a performance with more\ndynamic variation is harder to predict.\nWhen comparing the different architectures, most in-\nformation used by our model is encoded in L1andL3.\nL2has less predictive value than L1, and the score only\nimproves by a little bit when these two layers are used to-\ngether (suggesting there is a large amount of overlap in\nthe information they encode). L3, which is based on the\nvelocity-history representation (which was not used in [7])\nclearly contains a lot of information.\nInterestingly, L3contains most relevant information for\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n50(a)\n (b)\n (c)\n (d)\n (e)\nFigure 3: Some hand-selected features from L1that are representative for the types of patterns learned from the note-\ncentered representation (see Figure 2b). Dark values correspond to negative weights, light values to positive weights.\nthe Magaloff data, and L1for the Batik data. This could be\ndue to the difference between music from the Romantic pe-\nriod and that from the Classical period: L1contains more\ninformation about harmony, whereas L3contains more in-\nformation about the expressive ‘ﬂow’ of the piece.\nTraining on a single dataset has a positive effect on\nthe prediction scores. This is likely due to the fact that\nthe datasets are of a different nature in terms of musical\nstyle, and if we would want to predict performance param-\neters for a Mozart piece, training on Chopin music will\nnot provide our model with the relevant ‘know-how’. This\nis also illustrated by the cross-training experiments, where\nwe trained on one dataset and tested on the other: a drop in\nperformance of around 0.08 in all cases is observed. Still,\nalso a relatively large amount of the predictive capability\nremains, providing some conﬁdence that our model gener-\nalizes over different datasets to some extent.\nBecause the velocity-history representation requires de-\ntailed performance data for predictions, we use the results\nfrom our L1;2experiments when comparing our results to\nearlier work which does not use performance data. In [7]\nthe best obtained \u0016R2score on the Magaloff data is :139,\nusing a single dense RBM layer with 1000 hidden units\n(similar to our L1model). Our L1;2model achieved an\n\u0016R2of .207 on the same dataset. To keep statistical test-\ning simple, we tested the statistical signiﬁcance of the dif-\nference in unweighted averageR2of our model and the\nmodel in [7] using a Wilcoxon signed rank test. We chose\nthe Wilcoxon test because the underlying distribution of\ntheR2data is unknown. We found that the unweighted av-\nerageR2of .199 of our L1;2model is signiﬁcantly differ-\nent from the unweighted average R2of .121 of the model\nin [7] (W = 11111;p< 2:2\u000110\u000016). In [8], the maximal\nobtained prediction accuracy on the Magaloff dataset is an\n\u0016R2of .188. This model uses information our models have\nno access to, most importantly dynamic score annotations.\nNevertheless, with an \u0016R2of .207 our L1L2model again\nseems more successful even though it does not take such\nannotations into account.7When we do use performance\n7To perform the statistical test, detailed results from [7] were kindly\nprovided by the authors. For the work in [8] these results were unfor-\ntunately unavailable, meaning we could not perform the same statistical\nanalysis with this result.data, the difference becomes more pronounced: our L1;2;3\nmodel obtains an \u0016R2of .470 on the Magaloff data.\nSomething interesting to mention here is that in [17] we\nalso experimented with limiting training data to a particu-\nlar genre (i.e. training only on Nocturnes). These exper-\niments suggested that the velocity-history representation\nencodes some genre-speciﬁc information, however due to\nspace constraints we do not cover these results further here.\n6. DISCUSSION\nWe discuss two properties of our model: the features that\nwere learned from the musical data, and the performance\nachieved during prediction. Figure 3 illustrates a number\nof hand-selected features that have been learned from the\nnote-centered representation, which were chosen to give an\nimpression of the variety of learned features. Compared\nto the features learned by [7], there is a larger variety of\nfeatures, where features represent sharper patterns.\n6.1 Learned features\nFigure 3 illustrates some of the learned features. The dis-\nplayed features were selected so as to give the reader an\nimpression of the diversity of the learned features . From a\nmusicological perspective, it is interesting to see that there\nseem to be some remarkable patterns relating the features\nto music theory. The features learned from the velocity-\nhistory representation are harder to interpret musicologi-\ncally, these are not further discussed in this paper.\nFigure 3a shows clear horizontal banding, where inter-\nestingly the bands are exactly 12 rows apart – this corre-\nsponds to octaves. The feature in some locations displays a\nstrong contrast between pitches one semi-tone apart, which\nis related to dissonance.\nA common pattern is illustrated in Figure 3b, with a\ndark (inhibitive) band above or below a lighter region. This\ntype of feature is also described by Grachten & Krebs [7],\nwho argue this can be regarded as an accompaniment ver-\nsus melody detector: the illustrated feature is strongly in-\nhibited by notes in a sample that are below the central note,\nmeaning that the feature activates more readily for bass\nnotes. The opposite type of feature, with inhibitive regions\nabove and excitatory regions below the central note (not\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n51shown here), is active with a high probability for melody\nnotes, where surrounding notes have lower pitch.\nAnother common pattern is the vertical banding illus-\ntrated by Figure 3c. There is some variation in the offset\nof the vertical bands from the edges (their phase) and how\nclose they are together (their period). These features can\nconvey information on the pace in the current part of the\npiece (predominantly short or long notes) and the temporal\nposition of the note with respect to the beat.\nA few features also display diagonal banding as illus-\ntrated by Figure 3d, although these are relatively rare.\nStill, we hypothesize that with these our model can deduce\nwhether the central note is in an ascending or descending\nsequence.\nA ﬁnal common pattern is that in Figure 3e, with a sharp\nwhite band corresponding to a note at a certain relative\npitch and time from the central note. It seems reasonable\nto suggest that these can be related to particular melodic\nsteps – changes from one note to another with a particular\nrelative pitch and timing.\n6.2 Model performance\nThe performance of our model is an improvement com-\npared to earlier work, particularly when the goal is to ex-\nplain the structure of an existing performance rather than\npredict a performance for a new piece of score – in the\nformer situation the velocity-history representation can be\nused to good effect. Still, when considering a purely pre-\ndictive context (using no velocity information), an R2of\naround 0.2 leaves room for improvement. There is of course\na practical limit in terms of what score can be obtained:\neven the same pianist might not play a piece in exactly the\nsame way on different occasions, meaning that an R2close\nto 1.0 cannot be expected. A factor that limits our model is\nthat it considers score structure at a local level only – struc-\nture at larger timescales is not considered, nor are loudness\nannotations, which of course also convey a lot of infor-\nmation about how loudly a particular piece of score is to\nbe played. These omissions are opportunities for further\nwork: including these components could improve perfor-\nmance further, for example loudness annotations could be\nincluded similarly to what was done in [8].\n7. CONCLUSIONS\nWe showed that neural networks trained on relatively raw\nrepresentations of musical score and musical performances\ncan be used to predict expressive dynamics in piano per-\nformances. This was done before in [7], but we changed\nthe learning architecture (using sparse RBMs and skip-\nconnections), and developed a new input representation,\nresulting in better predictions and clearer features. We\nalso showed that our model generalizes well to datasets on\nwhich it was not trained.\n8. ACKNOWLEDGEMENTS\nThe research described in this article was sponsored by\nthe Austrian Science Fund (FWF) under project Z159(Wittgenstein Award), and by the European Commis-\nsion under the projects Lrn2Cre8 (grant agreement no.\n610859), and PHENICX (grant agreement no. 601166).\nThe research was part of an MSc project resulting in a the-\nsis [17], the contents of which overlap to some extent with\nthose in this paper. W. Bas de Haas is supported by the\nNetherlands Organization for Scientiﬁc Research, through\nthe NWO-VIDI-grant 276-35-001.\n9. REFERENCES\n[1] E. Bisesi and R. Parncutt. An accent-based approach to auto-\nmatic rendering of piano performance: Preliminary auditory\nevaluation. Archives of Acoustics, 36(2):283–296, 2010.\n[2] N. Boulanger-Lewandowski, Y . Bengio, and P. Vincent.\nModeling temporal dependencies in high-dimensional se-\nquences: Application to polyphonic music generation and\ntranscription. In Proceedings of the Twenty-nine Interna-\ntional Conference on Machine Learning. ACM, 2012.\n[3] S. Flossmann, W. Goebl, M. Grachten, B. Niedermayer, and\nG. Widmer. The magaloff project: An interim report. Journal\nof New Music Research, 39(4):363–377, 2010.\n[4] A. Friberg, L. Fryden, L. Bodin, and J. Sundberg. Perfor-\nmance rules for computer-controlled contemporary keyboard\nmusic. Computer Music Journal, 15(2):49–55, 1991.\n[5] A. Gabrielsson and P.N. Juslin. Emotional expression in\nmusic performance: Between the performer’s intention and\nthe listener’s experience. Psychology of music, 24(1):68–91,\n1996.\n[6] H. Goh, N. Thome, and M. Cord. Biasing restricted boltz-\nmann machines to manipulate latent selectivity and sparsity.\nInNIPS workshop on deep learning and unsupervised feature\nlearning, 2010.\n[7] M. Grachten and F. Krebs. An assessment of learned score\nfeatures for modeling expressive dynamics in music. Trans-\nactions on multimedia: Special issue on music data mining,\n16(5):1–8, 2014.\n[8] M. Grachten and G. Widmer. Explaining musical expression\nas a mixture of basis functions. In Proceedings of the 8th\nSound and Music Computing Conference (SMC 2011), 2011.\n[9] G.E. Hinton. A practical guide to training restricted boltz-\nmann machines. Momentum, 9(1):926, 2010.\n[10] G.E. Hinton and T.J. Sejnowski. Learning and relearning in\nboltzmann machines. MIT Press, Cambridge, Mass, 1:282–\n317, 1986.\n[11] H. Katayose and S. Inokuchi. Learning performance rules in\na music interpretation system. Computers and the Humani-\nties, 27(1):31–40, 1993.\n[12] F. Lerdahl and R.S. Jackendoff. A generative theory of tonal\nmusic. The MIT Press, 1983.\n[13] C. Palmer. Music performance. Annual review of psychology,\n48(1):115–138, 1997.\n[14] R. Parncutt. Accents and expression in piano performance.\nPerspektiven und Methoden einer Systemischen Musikwis-\nsenschaft, pages 163–185, 2003.\n[15] H. Schenker. Five graphic music analyses, 1969.\n[16] A. Spiliopoulou and A. Storkey. Comparing probabilistic\nmodels for melodic sequences. In Proceedings of the 2011\nEuropean conference on Machine learning and knowledge\ndiscovery in databases - Volume Part III, ECML PKDD’11,\npages 289–304, Berlin, Heidelberg, 2011. Springer-Verlag.\n[17] S. van Herwaarden. Teaching neural networks to play the pi-\nano. Master’s thesis, Utrecht University, 2014.\n[18] G. Widmer. Large-scale induction of expressive performance\nrules: First quantitative results. In Proceedings of the Inter-\nnational Computer Music Conference (ICMC’2000), pages\n344–347, 2000.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n52"
    },
    {
        "title": "Tracking the &quot;Odd&quot;: Meter Inference in a Culturally Diverse Music Corpus.",
        "author": [
            "Andre Holzapfel",
            "Florian Krebs",
            "Ajay Srinivasamurthy"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415000",
        "url": "https://doi.org/10.5281/zenodo.1415000",
        "ee": "https://zenodo.org/records/1415000/files/HolzapfelKS14.pdf",
        "abstract": "In this paper, we approach the tasks of beat tracking, down- beat recognition and rhythmic style classification in non- Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evalua- tion, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and down- beats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with com- parable results, providing a novel method for inferring the metrical structure in culturally diverse datasets.",
        "zenodo_id": 1415000,
        "dblp_key": "conf/ismir/HolzapfelKS14",
        "keywords": [
            "Bayesian model",
            "tempo inference",
            "downbeats recognition",
            "rhythmic style classification",
            "non-Western music",
            "adaptation to styles",
            "time signatures",
            "music corpus",
            "odd meter types",
            "state of the art"
        ],
        "content": "TRACKING THE “ODD”: METER INFERENCE IN A CULTURALLY\nDIVERSE MUSIC CORPUS\nAndre Holzapfel\nNew York University Abu Dhabi\nandre@rhythmos.orgFlorian Krebs\nJohannes Kepler University\nFlorian.Krebs@jku.atAjay Srinivasamurthy\nUniversitat Pompeu Fabra\najays.murthy@upf.edu\nABSTRACT\nIn this paper, we approach the tasks of beat tracking, down-\nbeat recognition and rhythmic style classiﬁcation in non-\nWestern music. Our approach is based on a Bayesian\nmodel, which infers tempo, downbeats and rhythmic style,\nfrom an audio signal. The model can be automatically\nadapted to rhythmic styles and time signatures. For evalua-\ntion, we compiled and annotated a music corpus consisting\nof eight rhythmic styles from three cultures, containing a\nvariety of meter types. We demonstrate that by adapting\nthe model to speciﬁc styles, we can track beats and down-\nbeats in odd meter types like 9=8or7=8with an accuracy\nsigniﬁcantly improved over the state of the art. Even if the\nrhythmic style is not known in advance, a uniﬁed model is\nable to recognize the meter and track the beat with com-\nparable results, providing a novel method for inferring the\nmetrical structure in culturally diverse datasets.\n1. INTRODUCTION\nMusical rhythm subordinated to a meter is a common fea-\nture in many music cultures around the world. Meter pro-\nvides a hierarchical time structure for the rendition and rep-\netition of rhythmic patterns. Though these metrical struc-\ntures vary considerably across cultures, metrical hierar-\nchies can often be stratiﬁed into levels of differing time\nspans. Two of these levels are, in terminology of Euroge-\nnetic music, referred to as beats, and measures. The beats\nare the pulsation at the perceptually most salient metrical\nlevel, and are further grouped into measures. The ﬁrst beat\nof each measure is called the downbeat. Determining the\ntype of the underlying meter, and the alignment between\nthe pulsations at the levels of its hierarchy with music per-\nformance recordings – a process we refer to as meter in-\nference – is fundamental to computational rhythm analysis\nand supports many further tasks, such as music transcrip-\ntion, structural analysis, or similarity estimation.\nThe automatic annotation of music with different as-\npects of rhythm is at the focus of numerous studies in Mu-\nsic Information Retrieval (MIR). M ¨uller et al [5] discussed\nc\rAndre Holzapfel, Florian Krebs, Ajay Srinivasamurthy.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Andre Holzapfel, Florian Krebs, Ajay\nSrinivasamurthy. “Tracking the “odd”: meter inference in a culturally di-\nverse music corpus”, 15th International Society for Music Information\nRetrieval Conference, 2014.the estimation of the beat (called beat tracking), and the\nestimation of higher-level metrical structures such as the\nmeasure length. Approaches such as the one presented by\nKlapuri et al [3] aim at estimating structures at several met-\nrical levels, while being able to differentiate between cer-\ntain time signatures. In [7] beats and downbeats are esti-\nmated simultaneously, given information about the tempo\nand the meter of a piece. Most of these approaches assume\nthe presence of a regular metrical grid, and work reason-\nably well for Eurogenetic popular music. However, their\nadaptation to different rhythmic styles and metrical struc-\ntures is not straight-forward.\nRecently, a Bayesian approach referred to as bar pointer\nmodel has been presented [11]. It aims at the joint estima-\ntion of rhythmic pattern, the tempo, and the exact position\nin a metrical cycle, by expressing them as hidden variables\nin a Hidden Markov Model (HMM) [8]. Krebs et al. [4]\napplied the model to music signals and showed that ex-\nplicitely modelling rhythmic patterns is useful for meter\ninference for a dataset of Ballroom dance music.\nIn this paper, we adapt the observation model of the ap-\nproach presented in [4] to a collection of music from dif-\nferent cultures: Makam music from Turkey, Cretan music\nfrom Greece, and Carnatic music from the south of In-\ndia. The adaption of observation models was shown to\nbe of advantage in [4, 6], however restricted to the con-\ntext of Ballroom dance music. Here, we extract rhythmic\npatterns from culturally more diverse data, and investigate\nif their inclusion into the model improves the performance\nof meter inference. Furthermore, we investigate if a uni-\nﬁed model can be derived that covers all rhythmic styles\nand time signatures that are present in the training data.\n2. MOTIVATION\nThe music cultures considered in this paper are based on\ntraditions that can be traced back for centuries until the\npresent, and were documented by research in ethnomusi-\ncology for decades. Rhythm in two of these cultures, Car-\nnatic and Turkish Makam music, is organized based on po-\ntentially long metrical cycles. All three make use of rhyth-\nmic styles that deviate audibly from the stylistic paradigms\nof Eurogenetic popular music. Previous studies on music\ncollections of these styles have shown that the current state\nof the art performs poorly in beat tracking [2, 9] and the\nrecognition of rhythm class [9]. As suggested in [9], we\nexplore a uniﬁed approach for meter inference that can rec-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n425ognize the rhythmic style of the piece and track the meter\nat the same time.\nThe bar pointer model, as described in Section 4, can be\nadapted to rhythmic styles by extracting possible patterns\nusing small representative downbeat annotated datasets.\nThis way, we can obtain an adapted system for a speciﬁc\nstyle without recoding and parameter tweaking. We be-\nlieve that this is an important characteristic for algorithms\napplied in music discovery and distribution systems for a\nlarge and global audience. Through this study, we aim to\nanswer crucial questions: Do we need to differentiate be-\ntween rhythmic styles in order to track the meter, or is a\nuniversal approach sufﬁcient? For instance, can we track a\nrhythmic style in Indian music using rhythmic patterns de-\nrived from Turkish music? Do we need to learn patterns\nat all? If a particular style description for each style is\nneeded, this has some serious consequences for the scala-\nbility of rhythmic similarity and meter inference methods;\nwhile we should ideally aim at music discovery systems\nwithout an ethnocentric bias, the needed universal analysis\nmethods might come at a high cost given the high diversity\nin the musics of the world.\n3. MUSIC CORPORA\nIn this paper we use a collection of three music corpora\nwhich are described in the following.\nThe corpus of Cretan music consists of 42 full length\npieces of Cretan leaping dances. While there are several\ndances that differ in terms of their steps, the differences in\nthe sound are most noticeable in the melodic content, and\nwe consider all pieces to belong to one rhythmic style. All\nthese dances are usually notated using a 2=4time signa-\nture, and the accompanying rhythmical patterns are usually\nplayed on a Cretan lute. While a variety of rhythmic pat-\nterns exist, they do not relate to a speciﬁc dance and can be\nassumed to occur in all of the 42 songs in this corpus.\nThe Turkish corpus is an extended version of the anno-\ntated data used in [9]. It includes 82 excerpts of one minute\nlength each, and each piece belongs to one of three rhythm\nclasses that are referred to as usul in Turkish Art music. 32\npieces are in the 9=8-usul Aksak, 20 pieces in the 10=8-usul\nCurcuna, and 30 samples in the 8=8-usul D¨uyek.\nThe Carnatic music corpus is a subset of the annotated\ndataset used in [10]. It includes 118 two minute long ex-\ncerpts spanning four t ¯alas (the rhythmic framework of Car-\nnatic music, consisting of time cycles). There are 30 ex-\namples in each of ¯adit¯ala (8 beats/cycle), r¯upaka t¯ala (3\nbeats/cycle) and mishra ch ¯aput¯ala (7 beats/cycle), and 28\nexamples in khanda ch ¯aput¯ala (5 beats/cycle).\nAll excerpts described above were manually annotated\nwith beats and downbeats. Note that for both Indian and\nTurkish music the cultural deﬁnition of the rhythms con-\ntain irregular beats. Since the irregular beat sequence is a\nsubset of the (annotated) equidistant pulses, it can be de-\nrived easily from the result of a correct meter inference.\nFor further details on meter in Carnatic and Turkish makam\nmusic, please refer to [9].4. METER INFERENCE METHOD\n4.1 Model description\nTo infer the metrical structure from an audio signal we use\nthebar pointer model, originally proposed in [11] and re-\nﬁned in [4]. In this model we assume that a bar pointer\ntraverses a bar and describe the state of this bar pointer\nat each audio frame kby three (hidden) variables: tempo,\nrhythmic pattern, and position inside a bar. These hidden\nvariables can be inferred from an (observed ) audio signal\nby using an HMM. An HMM is deﬁned by three quan-\ntities: A transition model which describes the transitions\nbetween the hidden variables, an observation model which\ndescribes the relation between the hidden states and the\nobservations (i.e., the audio signal), and an initial distribu-\ntion which represents our prior knowledge about the hid-\nden states.\n4.1.1 Hidden states\nThe three hidden variables of the bar pointer model are:\n\u000fRhythm pattern index rk2fr1;r2;:::;r Rg, whereRis\nthe number of different rhythmic patterns that we con-\nsider to be present in our data. Further, we denote the\ntime signature of each rhythmic pattern by \u0012(rk)(e.g.,\n9=8forAksak patterns). In this paper, we assume that\neach rhythmic pattern belongs to a rhythmic class, and\na rhythm class (e.g., Aksak, Duyek ) can hold several\nrhythmic patterns. We investigate the optimal number\nof rhythmic patterns per rhythm class in Section 5.\n\u000fPosition within a bar mk2f1;2;:::;M (rk)g:\nWe subdivide a whole note duration into 1600 discrete,\nequidistant bar positions and compute the number of po-\nsitions within a bar with rhythm rkbyM(rk) = 1600\u0001\n\u0012(rk)(e.g., a bar with 9=8meter has 1600\u00019=8 = 1800\nbar positions).\n\u000fTemponk2fnmin(rk);:::;n max(rk)g: The tempo can\ntake on positive integer values, and quantiﬁes the num-\nber of bar positions per audio frame. Since we use an au-\ndio frame length of 0:02s, this translates to a tempo res-\nolution of 7.5 (=60s\n1=4\u00011600\u00010:02s) beats per minute (BPM)\nat the quarter note level. We set the minimum tempo\nnmin(rk)and the maximum tempo nmax(rk)according\nto the rhythmic pattern rk.\n4.1.2 Transition model\nWe use the transition model proposed in [4, 11] with the\ndifference that we allow transitions between rhythmic pat-\ntern states within a song as shown in Equation 3. In the\nfollowing we list the transition probabilities for each of the\nthree variables:\n\u000fP(mkjmk\u00001;nk\u00001;rk\u00001): At time frame kthe bar\npointer moves from position mk\u00001tomkas deﬁned by\nmk= [(mk\u00001+nk\u00001\u00001)mod(M (rk\u00001))] + 1: (1)\nWhenever the bar pointer crosses a bar border it is reset\nto 1 (as modeled by the modulo operator).\n\u000fP(nkjnk\u00001;rk\u00001): If the tempo nk\u00001is inside the\nallowed tempo range fnmin(rk\u00001);:::;n max(rk\u00001)g,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n426there are three possible transitions: the bar pointer re-\nmains at the same tempo, accelerates, or decelerates:\nP(nkjnk\u00001) =8\n<\n:1\u0000pn; n k=nk\u00001\npn\n2; n k=nk\u00001+ 1\npn\n2; n k=nk\u00001\u00001(2)\nTransitions to tempi outside the allowed range are as-\nsigned a zero probability. pnis the probability of a\nchange in tempo per audio frame, and was set to pn=\n0:02, the tempo ranges (n min(r),nmax(r)) for each\nrhythmic pattern are learned from the data (Section 4.2).\n\u000fP(rkjrk\u00001): Finally, the rhythmic pattern state is as-\nsumed to change only at bar boundaries:\nP(rkjrk\u00001;mk<m k\u00001) =pr(rk\u00001;rk) (3)\npr(rk\u00001;rk)denotes the probability of a transition from\npatternrk\u00001to patternrkand will be learned from the\ntraining data as described in Section 4.2. In this paper\nwe allow transitions only between patterns of the same\nrhythm class, which will force the system to assign a\npiece of music to one of the learned rhythm classes.\n4.1.3 Observation model\nIn this paper, we use the observation model proposed\nin [4]. As summarized in Figure 1, a Spectral Flux-like\nonset feature, y, is extracted from the audio signal (sam-\npled with 44100 Hz) using the same parameters as in [4]. It\nsummarizes the energy changes that are likely to be related\nto instrument onsets in two dimensions related to two fre-\nquency bands, above and below 250 Hz. In contrast to [4]\nwe removed the normalizing step at the end of the feature\ncomputations, which we observed not to inﬂuence the re-\nsults.\nAudio signal\nSTFT\nFilterbank (82\nbands)\nLogarithm\nDifference\nSum o\nver frequency\nbands (0..250Hz)Sum o\nver frequency\nbands (250..22500Hz)\nSubtract mv\navg\nOnset feature y\nFigure 1: Computing the onset feature yfrom the audio\nsignal\nAs described in [4], the observation probabilities\nP(ykjmk;nk;rk)are modeled by a set of Mixture of\nGaussian distributions (GMM). As it is infeasible to spec-\nify a GMM for each state (this would result in N\u0002M\u0002R\nGMMs), we make two assumptions: First, we assume\nthat the observation probabilities are independent of the\ntempo and second, we assume that the observation prob-\nabilities only change each 64th note (which corresponds to\n1600/64=25 bar positions). Hence, for each rhythmic pat-\ntern, we have to specify 64\u0002\u0012(r)GMMs.4.1.4 Initial distribution\nFor each rhythmic pattern, we assume a uniform state dis-\ntribution within the tempo limits and over all bar positions.\n4.2 Learning parameters\nThe parameters of the observation GMMs, the transition\nprobabilities of the rhythm pattern states, and the tempo\nranges for each rhythmic style are learned from the data\ndescribed in Section 3. In our experiments we perform a\ntwo-fold cross-validation, excluding those ﬁles from the\nevaluation that were used for parameter learning.\n4.2.1 Observation model\nThe parameters of the observation model consist of\nthe mean values, covariance matrix and the component\nweights of the GMM for each 64th note of a rhythmic pat-\ntern. We determine these as follows:\n1. The two-dimensional onset feature y(see Section 4.1.3)\nis computed from the training data.\n2. The features are grouped by bar and bar position within\nthe 64th note grid. If there are several feature values for\nthe same bar and 64th note grid point, we compute the\naverage, if there is no feature we interpolate between\nneighbors. E.g., for a rhythm class which spans a whole\nnote (e.g., D¨uyek (8/8 meter)) this yields a matrix of size\nB\u0002128, where Bis the number of bars with D¨uyek\nrhythm class in the dataset.\n3. Each dimension of the features is normalized to zero\nmean and unit variance.\n4. For each of the eight rhythm classes in the corpus de-\nscribed in Section 3, a k-means clustering algorithm as-\nsigns each bar of the dataset (represented by a point in\na 128-dimensional space) to one rhythmic pattern. The\ninﬂuence of the number of clusters kon the accuracy\nof the metrical inference will be evaluated in the exper-\niments.\n5. For each rhythmic pattern, at all 64th grid points, we\ncompute the parameters of the GMM by maximum like-\nlihood estimation.\n4.2.2 Tempo ranges and transition probabilities\nFor each rhythmic pattern, we compute the minimum and\nmaximum tempo of all bars of the training fold that were\nassigned to this pattern by the procedure described in Sec-\ntion 4.2.1. In the same way, we determine the transition\nprobabilities prbetween rhythmic patterns.\n4.3 Inference\nIn order to obtain beat-, downbeat-, and rhythmic class es-\ntimations, we compute the optimal state sequence fm\u0003\n1:K;\nn\u0003\n1:K;r\u0003\n1:Kgthat maximizes the posterior probability of the\nhidden states given the observations y1:Kand hence ﬁts\nbest to our model and the observations. This is done using\nthe well-known Viterbi algorithm [8].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4275. EXPERIMENTS\n5.1 Evaluation metrics\nA variety of measures for evaluating beat and downbeat\ntracking performance are available (see [1] for a detailed\noverview and descriptions of the metrics listed below)1.\nWe chose ﬁve metrics that are characterized by a set of di-\nverse properties and are widely used in beat tracking eval-\nuation.\nFmeas (F-measure): The F-measure is computed from\ncorrectly detected beats within a window of \u000670 ms by\nF-measure =2pr\np+r(4)\nwherep(precision) denotes the ratio between correctly de-\ntected beats and all detected beats, and r(recall ) denotes\nthe ratio between correctly detected beats and the total\nnumber of annotated beats. The range of this measure is\nfrom0%to100%.\nAMLt (Allowed Metrical Levels with no continuity re-\nquired): In this method an estimated beat is counted as\ncorrect, if it lies within a small tolerance window around an\nannotated pulse, and the previous estimated beat lies within\nthe tolerance window around the previous annotated beat.\nThe value of this measure is then the ratio between the\nnumber of correctly estimated beats divided by the number\nof annotated beats (as percentage between 0%and100%).\nBeat sequences are also considered as correct if the beats\noccur on the off-beat, or are double or half of the annotated\ntempo.\nCMLt (Correct Metrical Level with no continuity re-\nquired): The same as AMLt, without the tolerance for off-\nbeat, or doubling/halving errors.\ninfGain (Information Gain): Timing errors are calcu-\nlated between an annotation and all beat estimations within\na one-beat length window around the annotation. Then, a\nbeat error histogram is formed from the resulting timing\nerror sequence. A numerical score is derived by measuring\nthe K-L divergence between the observed error histogram\nand the uniform case. This method gives a measure of how\nmuch information the beats provide about the annotations.\nThe range of values for the Information Gain is 0 bits to\napproximately 5.3 bits in the applied default settings.\nDb-Fmeas (Downbeat F-measure): For measuring the\ndownbeat tracking performance, we use the same F-\nmeasure as deﬁned for beat tracking (using a \u000670 ms tol-\nerance window).\n5.2 Results\nIn Experiment 1, we learned the observation model de-\nscribed in Section 4.2 for various numbers of clusters,\nseparately for each of the eight rhythm classes. Then,\nwe inferred the meter using the HMM described in Sec-\ntion 4.1, again separately for each rhythm class. The re-\nsults of this experiment indicate how many rhythmic pat-\nterns are needed for each class in order to achieve an opti-\nmal beat and downbeat tracking with the proposed model.\n1We used the MATLAB code available at http://code.\nsoundsoftware.ac.uk/projects/beat-evaluation/ with\nstandard settings.Tables (1a) to (1h) show the performance with all the eval-\nuation measures for each of the eight styles separately. For\nExperiment 1 (Ex-1), all signiﬁcant increases compared\nto the previous row are emphasized using bold numbers\n(according to paired-sample t-tests with 5% signiﬁcance\nlevel). In our experiments, increasing the number Rof\nconsidered patterns from one to two leads to a statistically\nsigniﬁcant increase in most cases. Therefore, we can con-\nclude that for tracking these individual styles, more than\none pattern is always needed. Further increase to three\npatterns leads to signiﬁcant improvement only in the ex-\nceptional case of ¯Adi t ¯ala, where measure cycles with long\ndurations and rich rhythmic improvisation apparently de-\nmand higher number of patterns and cause the system to\nperform worse than for other classes. Higher numbers than\nR= 3 patterns never increased any of the metrics signiﬁ-\ncantly. It is important to point out again that a test song was\nnever used to train the rhythmic patterns in the observation\nmodel in Experiment 1.\nThe interesting question we address in Experiment 2 is\nif the rhythm class of a test song is a necessary informa-\ntion for an accurate meter inference. To this end, we per-\nformed meter inference for a test song combining all the\ndetermined rhythmic patterns for all classes in one large\nHMM. This means that in this experiment the HMM can be\nused to determine the rhythm class of a song, as well as for\nthe tracking of beats and downbeats. We use two patterns\nfrom each rhythm class (except ¯adi t¯ala), the optimally per-\nforming number of patterns in Experiment 1, to construct\nthe HMM. For ¯adi t¯ala, we use three patterns since using\n3 patterns improved performance in Experiment 1, to give\na total ofR= 17 different patterns for the large HMM.\nThe results of Experiment 2 are depicted in the rows la-\nbeled Ex-2 in Tables (1a) to (1h), signiﬁcant change over\nthe optimal setting in Experiment 1 are emphasized using\nbold numbers. The general conclusion is that the system is\ncapable of a combined task of classiﬁcation into a rhythm\nclass and the inference of the metrical structure of the sig-\nnal. The largest and, with the exception of ¯adi t ¯ala, only\nsigniﬁcant decrease between the Experiment 1 and Experi-\nment 2 can be observed for the downbeat recognition (Db-\nFmeas). The reason for this is that a confusion of a test\nsong into a wrong class may still lead to a proper track-\ning of the beat level, but the tracking of the higher metrical\nlevel of the downbeat will suffer severely from assigning a\npiece to a class with a different length of the meter than the\ntest piece.\nAs described in Section 4.1, we do not allow transi-\ntions between different rhythm classes. Therefore, we can\nclassify a piece of music into a rhythm class by evaluat-\ning to which rhythmic pattern states rkthe piece was as-\nsigned. The confusion matrix is depicted in Table 2, and\nit shows that the highest confusion can be observed within\ncertain classes of Carnatic music, while the Cretan leaping\ndances and the Turkish classes are generally recognized\nwith higher recall rate. The accent patterns in mishra ch ¯apu\nand khanda ch ¯apu can be indeﬁnite, non-characteristic and\nnon-indicative in some songs, and hence there is a possi-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n428RFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 65.9 45.0 57.6 0.89 46.6\n291.0 76.6 90.0 1.62 88.6\n3 90.6 77.2 91.1 1.59 86.5\nEx-2 17 85.7 68.7 89.3 1.57 65.1\nKL 69.38 41.24 64.60 1.46 -\n(a) Turkish Music: Aksak (9/8)RFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 71.4 47.8 50.3 0.68 38.9\n289.1 75.6 75.6 1.04 48.6\n3 87.7 73.0 73.0 0.99 54.4\nEx-2 17 89.3 74.8 77.5 1.16 41.1\nKL 52.77 5.90 59.04 0.77 -\n(b) Turkish Music: Curcuna (10/8)\nRFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 57.2 33.5 42.2 0.68 37.3\n285.2 70.1 82.7 1.51 75.4\n3 83.4 63.3 81.9 1.45 73.7\nEx-2 17 86.6 75.8 87.2 1.64 72.6\nKL 70.25 49.52 71.79 1.53 -\n(c) Turkish Music: D ¨uyek (8/8)RFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 68.1 60.7 60.8 1.33 59.1\n293.0 91.3 91.3 2.25 86.2\n3 92.9 91.0 91.0 2.25 85.8\nEx-2 17 88.8 74.3 92.5 2.24 72.2\nKL 35.87 34.42 72.07 1.57 -\n(d) Cretan leaping dances (2/4)\nRFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 49.6 38.9 47.0 0.93 16.5\n2 56.7 44.0 59.5 1.21 32.5\n3 61.6 49.5 65.9 1.40 32.8\nEx-2 17 62.4 40.6 76.7 1.73 21.4\nKL 59.42 45.90 64.91 1.53 -\n(e) Carnatic music: ¯Adi (8/8)RFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 68.2 65.8 71.4 2.04 60.8\n282.8 82.5 90.2 2.77 81.9\n3 83.0 82.9 89.5 2.73 80.5\nEx-2 17 77.2 60.6 88.9 2.39 62.0\nKL 53.42 29.17 60.37 1.30 -\n(f) Carnatic music: R ¯upaka (3/4)\nRFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 84.1 79.0 79.0 1.54 71.0\n293.7 92.2 92.2 2.00 86.4\n3 93.4 91.6 91.6 1.99 89.9\nEx-2 17 90.0 81.6 86.3 1.83 55.0\nKL 74.61 44.99 68.71 1.25 -\n(g) Carnatic music: Mishra ch ¯apu (7/8)RFmeas CMLt AMLt infGain Db-Fmeas\nEx-11 58.9 38.0 41.5 0.70 27.7\n294.3 88.9 94.9 2.00 77.3\n3 93.7 88.1 94.3 1.95 78.2\nEx-2 17 90.3 76.0 93.2 2.01 70.6\nKL 76.16 57.76 66.34 1.18 -\n(h) Carnatic music: Khanda ch ¯apu (5/8)\nTable 1: Evaluation results for each rhythm class, for Experiment 1 (separate evaluation per style, shown as Ex-1), and\nExperiment 2 (combined evaluation using one large HMM, shown as Ex-2). The last row in each Table, with row header as\nKL, shows the beat tracking performance using Klapuri beat tracker. For Ex-1, bold numbers indicate signiﬁcant change\ncompared to the row above, for Ex-2, bold numbers indicate signiﬁcant change over the best parameter setting in Ex-1\n(bold R parameter), and for KL the only differences to Ex-2 that are not statistically signiﬁcant are underlined.\nbility of confusion between the two styles. Confusion be-\ntween the three cultures, especially between Turkish and\nCarnatic is extremely rare, which makes sense due to dif-\nferences in meter types, performance styles, instrumental\ntimbres, and other aspects which inﬂuence the observation\nmodel. The recall rates of the rhythm class averaged for\neach culture are 69:6% for Turkish music, 69:1% for the\nCretan music, and 61:02% for Carnatic music. While the\ndatasets are not exactly the same, these numbers represent\na clear improvement over the cycle length recognition re-\nsults depicted in [9] for Carnatic and Turkish music.\nFinally, we would like to put the beat tracking accura-\ncies achieved with our model into relation with results ob-\ntained with state of the art approaches that do not include\nan adaption to the rhythm classes. In Table 1, results of the\nalgorithm proposed in [3], which performed generally best\namong several other approaches, are depicted in the last\nrows (KL) of each subtable. We underline those results that\ndo not differ signiﬁcantly from those obtained in Experi-\nment 2. In all other cases the proposed bar pointer modelperforms signiﬁcantly better. The only rhythm class, for\nwhich our approach does not achieve an improvement in\nmost metrics is the ¯adi t¯ala. As mentioned earlier, this can\nbe attributed to the large variety of patterns and the long\ncycle durations in ¯adi t¯ala.\n6. CONCLUSIONS\nIn this paper we adapted the observation model of a\nBayesian approach for the inference of meter in music of\ncultures in Greece, India, and Turkey. It combines the task\nof determining the type of meter with the alignment of the\ndownbeats and beats to the audio signal. The model is ca-\npable of performing the meter recognition with an accu-\nracy that improves over the state of the art, and is at the\nsame time able to achieve for the ﬁrst time high beat and\ndownbeat tracking accuracies in additive meters like the\nTurkish Aksak and Carnatic mishra ch ¯apu.\nOur results show that increasing the diversity of a corpus\nmeans increasing the number of the patterns, i.e.a larger\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n429Turkish Greek Carnatic\nAksak D¨uyek Curcuna Cretan ¯Adi R¯upaka M.ch ¯apu K.ch ¯apu Recall\nAksak 21 7 2 2 66\nD¨uyek 23 2 5 77\nCurcuna 1 3 13 2 1 65\nCretan 3 5 29 3 2 69\n¯Adi 14 8 1 7 47\nR¯upaka 3 19 1 7 63\nM.ch ¯apu 2 1 16 11 53\nK.ch ¯apu 4 1 23 82\nPrecision 84 61 76 76 64 56 84 47\nTable 2: Confusion matrix of the style classiﬁcation of the large HMM (Ex-2). The rows refer to the true style and the\ncolumns to the predicted style. The empty blocks are zeros (omitted for clarity of presentation).\namount of model parameters. In the context of the HMM\ninference scheme applied in this paper this implies an in-\ncreasingly large hidden-parameter state-space. However,\nwe believe that this large parameter space can be handled\nby using more efﬁcient inference schemes such as Monte\nCarlo methods.\nFinally, we believe that the adaptability of a music pro-\ncessing system to new, unseen material is an important de-\nsign aspect. Our results imply that in order to extend meter\ninference to new styles, at least some amount of human\nannotation is needed. If there exist music styles where\nadaptation can be achieved without human input remains\nan important point for future discussions.\nAcknowledgments\nThis work is supported by the Austrian Science Fund\n(FWF) project Z159, by a Marie Curie Intra-European Fel-\nlowship (grant number 328379), and by the European Re-\nsearch Council (grant number 267583).\n7. REFERENCES\n[1] M. Davies, N. Degara, and M. D. Plumbley. Evaluation\nmethods for musical audio beat tracking algorithms.\nQueen Mary University of London, Tech. Rep. C4DM-\n09-06, 2009.\n[2] A. Holzapfel and Y . Stylianou. Beat tracking using\ngroup delay based onset detection. In Proceedings of\nISMIR - International Conference on Music Informa-\ntion Retrieval, pages 653–658, 2008.\n[3] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the Meter of Acoustic Musical Signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n14(1):342–355, 2006.\n[4] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern\nmodeling for beat- and downbeat tracking in musi-\ncal audio. In Proc. of the 14th International Society\nfor Music Information Retrieval Conference (ISMIR-\n2013), Curitiba, Brazil, nov 2013.[5] M. M ¨uller, D. P. W. Ellis, A. Klapuri, G. Richard, and\nS. Sagayama. Introduction to the Special Issue on Mu-\nsic Signal Processing. IEEE Journal of Selected Topics\nin Signal Processing, 5(6):1085–1087, 2011.\n[6] G. Peeters. Template-based estimation of tempo: us-\ning unsupervised or supervised learning to create better\nspectral templates. In Proc. of the 13th International\nConference on Digital Audio Effects (DAFX 2010),\nGraz, Austria, 2010.\n[7] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: Theory and large-scale evaluation. IEEE Trans-\nactions on Audio, Speech and Language Processing,\n19(6):1754–1769, 2011.\n[8] L. R. Rabiner. A tutorial on hidden markov models\nand selected applications in speech recognition. In Pro-\nceedings of the IEEE, pages 257–286, 1989.\n[9] A. Srinivasamurthy, A. Holzapfel, and X. Serra. In\nsearch of automatic rhythm analysis methods for Turk-\nish and Indian art music. Journal for New Music Re-\nsearch, 43(1):94–114, 2014.\n[10] A. Srinivasamurthy and X. Serra. A supervised ap-\nproach to hierarchical metrical cycle tracking from au-\ndio music recordings. In Proc. of the 39th IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP-2014), pages 5237–5241, Flo-\nrence, Italy, May 2014.\n[11] N. Whiteley, A. Cemgil, and S. Godsill. Bayesian mod-\nelling of temporal structure in musical audio. In Proc.\nof the 7th International Conference on Music Informa-\ntion Retrieval (ISMIR-2006), Victoria, 2006.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n430"
    },
    {
        "title": "Detection of Motor Changes in Violin Playing by EMG Signals.",
        "author": [
            "Ling-Chi Hsu",
            "Yu-Lin Wang",
            "Yi-Ju Lin",
            "Cheryl D. Metcalf",
            "Alvin W. Y. Su"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416452",
        "url": "https://doi.org/10.5281/zenodo.1416452",
        "ee": "https://zenodo.org/records/1416452/files/HsuWLMS14.pdf",
        "abstract": "Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the infor- mation from audio signal is not enough to understand the sensorimotor programming in players. Recently, the in- vestigation of muscular activities of players during per- formance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultane- ously and also develop algorithms to analyze the music performance and discover its relation to player’s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of vio- lin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular ac- tivities during violin playing. With the additional infor- mation of the audio signal, the proposed work could effi- ciently extract the period and detect the direction of mo- tor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance.",
        "zenodo_id": 1416452,
        "dblp_key": "conf/ismir/HsuWLMS14",
        "keywords": [
            "harmonious body movements",
            "motor sequences",
            "muscular activities",
            "EMG signal",
            "music performance",
            "sensorimotor programming",
            "violin bowing",
            "audio sounds",
            "features",
            "muscular activities"
        ],
        "content": "DETECTION OF MOTOR CHANGES IN VIOLIN PLAYING \nBY EMG SIGNALS \nLing -Chi Hsu, Yu-Lin Wang , Yi-Ju Lin,  Alvin \nW.Y. Su  Cheryl D. Metcalf  \nDepartme nt of CSIE, National Cheng -Kung   \nUniversity,  Taiwan  \nt19897843@gmail.com ;  \ndaphne.yl.wang@gmail.com ; \nlyjca.cs96@g2.nctu.edu.tw ;  \nalvinsu@mail.ncku.edu.tw  \n Faculty of Health Sciences, University of \nSouthampton , United Kingdom  \nc.d.metcalf@soton.ac.uk  \nABSTRACT \nPlaying a music instrument relies on the harmonious \nbody movements. Motor sequences are trained to achieve \nthe perfect performances in musicians. Thus, the info r-\nmation from audio signal is not enough to understand the \nsensorimotor programming in players. Recently, the in-\nvestigation of muscular activities of players during per-\nformance has attracted our interests. In this work, we \npropose a multi-channel system that records the audio \nsounds and electromyography (EMG) signal simultane-\nously and also develop algorithms to analyze the music \nperformance and discover its relation to player ’s motor \nsequenc es. The movement segment was first identified by \nthe information of audio sounds, and the direction of vio-\nlin bowing was detected by the EMG signal. Six features \nwere introduced to reveal the variations of muscular ac-\ntivities during violin playing. With the additional infor-\nmation of the audio signal, the proposed work could effi-\nciently extract the period and detect the direction of mo-\ntor changes in violin bowing. Therefore, the proposed \nwork could provide a better understanding of how players \nactivate the muscles to organize the multi-joint movement \nduring violin performance. \n1. INTRODUCTION \nFor musicians, the ir motor skills must be honed by many \nhours of daily practice to maintain the performing quali ty. \nMotor sequences are trained to achieve the perfect per-\nformances. Playing a music al instrument relies on the \nharmonious coordination of body movements, arm and \nfingers. This is fundamental to understanding the neuro-\nphysiological mechanisms that underpin learning. It \ntherefore becomes important to understand the sen-\nsorimotor programming in players. In the late 20th centu-\nry, Harding et al. [1] directly measured the force between \nplayer’s finger s and piano keys with different skill levels. \nEngel et al. [2 ] found there is an anticipatory change of \nsequential hand movements in pianists. Parlitz et al. [3 ] explored the dynamic pressure s to analyze how pianists \ndepressed the piano keys and hold them down during \nplaying. The pressure measurement advances the evalua-\ntion of the keystroke in piano playing [4-5]. The use of \nmuscle activity via electromyography (EMG) signals al-\nlows further investigation into the motor control sequenc-\nes that produce the music. EMG is a technique which \nevaluates the electrical activity of the muscle by rec-\nording the electrical potentials when muscles generate an \nelectrical voltage during activation, which results in a \nmovement or coordinated action. \nEMG is generally recorded in two protocols; invasive \nelectromyography (IEMG) and surface electromyography \n(SEMG). IEMG is used to measure deep muscles and \ndiscrete positions using a fine-wire needle; however, it is \nnot a preferable model for subjects due to the invasive-\nness and being less repetitive . Compar ed to IEMG, \nSEMG has the following characteristics : (1) it is non-\ninvasive; (2) it provides global information; (3) it is com-\nparatively simple and inexpensive ; (4) it is applicable by \nnon-medical personnel; and (5) it can be used over a \nlonger time during work and sport activities [6]. There-\nfore, the SEMG is suitable for use within biomechanics \nand movement analysis, and was used in this paper. \nFor the analysis of musical performance, EMG  has \nbeen used to evaluate behavioral changes of the fingers \n[7-8], upper limbs [9-10] shoulder [11-12] and wrist [13] \nin piano, violin, cello and drum players. The EMG meth-\nod allows for differentiating the variations and reproduci-\nbility of muscular activities in individual players. Com-\nparing the EMG activity between expert pianists and nov-\nice players [7- 14] has also been studied. \nThere have been many approaches developed for seg-\nmentation of EMG signals [15]. Prior EMG segmentation \ntechniques were mainly used to detect the time period for \na certain muscle contraction, but we found that the poten-\ntial variations from various muscles maybe different dur-\ning a movement. It causes the conventional EMG seg-\nmentation to fail to extract the accurate timing of move-\nment in instrument playing .  \nIn this paper, the timing activation of the muscle group \nis assessed, and the changes in motor control of players \nduring performance are investigated.  We propose a sys-\ntem with the function of concurrently recording the audio \nsignal and behavioral changes (EMG) while playing an \ninstrument. This work is particularly focused on violin \nplaying, which is considered difficult to segment with the  © L.C. Hsu, Y.J. Lin, Y.L. Wang, A. W.Y. Su , C.D. Metcalf . \nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution:  L.C. Hsu, Y.J. Lin, Y.L . Wang, \nA.W.Y. Su, C.D. Metcalf . “Detection Of Motor Changes In Violin Play-\ning By EMG  Signals ”, 15th International Society for Music I nformation \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n495soft onsets of the notes . The segment with body move-\nments was first identified by the information of audio \nsounds. It is believed that if there is an audio signal, then \nthere is a corresponding movement. Six  features were \nthen introduced to EMG signals to discover the variation \nof movements. This work identif ies the individual \nmovement segments, i.e. up-bowing and down-bowing, \nduring violin playing. Thus, how motor systems operat ed \nin musicians and affected during performance could be \nexplored using this methodology. \nThis paper is organized as follows. The multi-channel \nsignal recording system and its experimental protocol are \nshown in section 2. In section 3, we introduce the pro-\nposed algorithms for segmenting the EMG signal with \nadditional audio information . The experimental results \nare shown in section 4 and the conclusion and future \nwork are given in section 5. \n2. AUDIO SOUNDS AND BIOSIGNAL \nRECOR DING SYSTEM \nThis work proposed a multi-channel signal recording \nsystem capable of recording audio and EMG signals \nconcurrently. The system is illustrated in Figure 1 and \ncomprises: (a) a signal pre-amplifier acquisition board, \n(b) an analog to digital signal processing unit, and (c) a \nhost-system. \nFigure  1. The proposed multi -channel recording system \nfor recording audio signal and EMG concurrently.  \n \nThe violin signal was recorded in a chamber and the \nmicrophone was placed 30cm from the player with a \nsampling rate of 44100Hz. With this real violin recording, \nthe sound is supposedly embedded with the noise and the \nartifacts.  \nFurthermore, there is three subjects in the experiment \ndatabase. The violinist play music and be recorded.  Each \nparticipant was requested to press one string during play-\ning. This experiment included two tasks for performance \nevaluation, and each task contain ed 10 movements. The \nmovements for task#1 and task#2 are defined as follows. \nMovements for task#1:  \n(1) Player presses the 2nd string then is idle for 2s \n(begin the bow at the frog).   \n(2) Pulls the bow from the frog to the tip for 4s \n(whole bow down). \n(3) Pulls the whole bow up for 4s. \n \nMovements for task#2: \n(1) Player presses the 3rd string then is idle for 2s \n(begin the bow at the tip).   \n(2) Pulls the whole bow up for 4s. \n(3) Pulls the whole bow down for 4s. Two seconds resting time was given between the two \nconsecutive movements. \nThe EMG sampling rate was 1000Hz. The electrode s \nattached on the surface of the player’s skin as shown Fig-\nure 2. In this study, the direction of violin bowing, i.e. up-\nbowing and down-bowing, is detected by the correspond-\ning muscle activity (EMG signal). The total of 8 muscles \nin the upper limb and body is measured in our system. \nFigure 3 shows the 8-channel EMG signals of up-bowing \nmovement, and potential variations were shown in all \nchannels when bowing. Three types of variations were \nobserved and grouped: \n \n(1) Channel#1 to Channel#6 : it is seen that the trend of \nsix channels is similar; additionally, the average \nnoise floor between channel #3 and channel#6 are \nlower than others; finally, we choose channel#6 be-\ncause the position is convenient to place the elec-\ntrode. \n(2) Channel#7: the channel involving the most noise. \n(3) Channel#8: although it has more noise than Chan-\nnel#1 to Channel#6, it is the important part when we \nhave a whole-bowing movement. \n \nFigure 2 . The placement of the electrodes attached on \nthe player ’s skin [16, 17]. \n \nFigure 3. The 8-channel EMG signals of up-down bow-\ning movements. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n496To reduce the computation and retain the variety of \nfeatures, only channel#6 and channel#8 were thereafter \nused for further analysis. Figure 4 shows the EMG sig-\nnals of channel#6 and channel#8 while during down-\nbowing. \n \nFigure 4.  The EMG signals of triceps (channel#6) and \npectoralis (channel#8) during down-bowing movements. \n3. METHOD \nThe following section will introduce the proposed algo-\nrithm for detecting the bowing states during violin play-\ning. The proposed system is capable of recording audio \nand EMG signals concurrently, and in this study a bow-\ning state detection algorithm was developed, which was \nimplemented the embedded system. The flowchart of the \nproposed method is shown in Figure 5. \n \n \nFigure 5.  Flowchart of the proposed system. \n \nThe EMG signals were segmented according to the vi-\nolin sounds. Then, six features were identified to detect \nthe direction of bowing movements. For analyzing the \naudio signal, the window size of a frame is 2048 samples \nand the hop size 256 samples. \n3.1 Onset/Transition/Offset detection \nThis section elaborates on the state detection of audio \nsounds. The states of audio sounds are defined as Onset , \nTransition  and Offset  in this study. The O nset is the be-\nginning of bowing; the Transition  is the timing when the \nnext bowing movement occurred; the Offset  is the end of the bowing; the Sustain  is the duration of the note seg-\nment. Both frequency and spatial features were calculated \nand used as the inputs to our developed finite state ma-\nchine (FSM). The diagram of our proposed FSM is illus-\ntrated in Figure 6. The output of FSM identifies the result \nof note detection and further used for EMG segmentation. \n \nFigure 6. The state diagram of audio sounds. \n \nThe violin signal was analyzed both in frequency and \ntime domains. For frequency analysis, the violin signal \nwas first transformed by short time Fourier transform . \nThe inverse correlation (IC) was then applied to calculate \nthe possible note onset peri od. The inverse correlation (IC) \ncoefficients are computed from the correlation coeffi-\ncients of two consecutive discrete Fourier transform  spec-\ntra [18]. A support vector machine ( SVM ), denoted  as \nSVM ic (1), was applied for detecting the accurate timing \nof onset. SVM is a popular methodology , with high speed \nand simple implementation , for classification and regres-\nsion analysis [ 19].  \n𝑆𝑉𝑀 𝑖𝑐= {0 , 𝑛𝑜𝑛 − 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛\n1 , 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛      (1) \nFor spatial analysis, the amplitude envelop (AE) was \nused to detect the segment of the sound data. AE is eval-\nuated as the maximum value of a frame . There are two \nsimilar classifiers, called SVM ae1 (2) and SVM ae2 (3). \nSVM ae1 is used to identify the possible onset s and SVM ae2 \nis used to identify the possible offset s. \n𝑆𝑉𝑀 𝑎𝑒1= {0 , 𝑛𝑜𝑛 − 𝑜𝑛𝑠𝑒𝑡\n1 , 𝑜𝑛𝑠𝑒𝑡           (2) \n \n𝑆𝑉𝑀 𝑎𝑒2= {0 , 𝑛𝑜𝑛 − 𝑜𝑓𝑓𝑠𝑒𝑡\n1 , 𝑜𝑓𝑓𝑠𝑒𝑡           (3) \n \nFigure 7 shows (a) a segment of audio sounds with one \nsequence of down-bowing and up-bowing, while Figure \n7(b) and (c) display the results of IC and AE, respectively. \nDuring the bowing state, the IC value is extremely \nsmall when compared to the results of the non-bowing \nstate. IC seems to be a good index to identify the state of \nwhether the violin is being played, or not . However, it \ncan be seen that a time deviation  is introduced if the sys-\ntem simply applies a hard threshold, e.g. 0. 3. Alternative-\nly, the AE value becomes  large r at \nthe playing  state. But \nthe issue of time deviation  is also present  in this feature, \nif a hard threshold is applied.  \nAfter calculating the IC and AE values, their variation \nis considered as one set of input data for SVM. The time \nperiod of each data is 100ms. Therefore, SVM ic, SVM ae1 \nand SVM ae2 are designed to detect the most plausible tim-\ning of onset, transition and offset. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n497 \nFigure 7.  (a) The audio sounds of down-bowing and up-\nbowing; (b) the results of IC; (c) the results of AE \n3.2 Detection of bowing direction \nIn each movement, there are one onset, one offset, and \nseveral transitions. However, the total number of transi-\ntions will differ from the number of notes.  After detection \nof the bowing state is completed, the duration between \nonset and offset is applied for segmenting the EMG sig-\nnal of triceps  (channel#6) and pectroalis (channel#8).  For \neach note duration, there are three cases:  \n(1)The duration from the onset to the first transition. \n(2)The duration from the current transition to the next \ntransition. \n(3)The duration from the last transition of the offset.  \nThis note duration extracted from the audio sound is \ncalled an active frame and the active frames are variant \nlengths from each other. The segment extracted by the \naudio sounds is called an active  frame  and the active \nframes are variant lengths from each other. \nFor each active frame , six features in [20] were applied \nto calculate the variations of EMG signal while bowing. \nThe features are:  \n Mean absolute value (MAV) \n Mean absolute value slope (MAVS) \n Zero crossings (ZC) \n Slope sign changes (SSC) \n Waveform length (WL) \n Correlation variation (CV) \nHere, the active frame is experimentally divided into \n20 segments for calculating MAV and WL, thus each ac-\ntive frame has 20 values of MAV and WL . For CV, we \ncalculate the auto-correlation and cross-correlation of \nchannel#6 and channel#8, and therefore there are 3 values of CV for each active frame. Table 1 lists the number of \neach feature for each channel. \nTable 1.  The number of each feature per channel \nFeature  MAV  MAVS  ZC SSC WL \nNumber  20 19 1 1 20 \nA more detailed description of those applied features \ncould be found in [ 20]. Figure 8 displays the triceps EMG \nsignal of one active frame (8s ~ 16s) and the results cal-\nculated by MAV, MAVS, ZC, SSC and WL.  It can be \nseen that variations are exhibited  for 6 features in violin \nplaying with a down- up bowing movement. \nThe detection of bowing direction is also determined \nby a SVM classifier which is denoted as SVM dir (3). For \nSVM dir, a total of 125 inputs are used (61 inputs for chan-\nnel#6 and channel#8 each, plus 3 values of CV) and it \nidentifies whether the active EMG frame is in the up-\nbowing or down-bowing state. \n𝑆𝑉𝑀 𝑑𝑖𝑟= {0 , 𝑈𝑝 − 𝑏𝑜𝑤𝑖𝑛𝑔\n1 , 𝐷𝑜𝑤𝑛 − 𝑏𝑜𝑤𝑖𝑛𝑔               (3) \n \nFigure 8.  One down-up bowing movement and its six \nfeatures :  (a) the down-bowing movement, (b) the up-\nbowing movement. \n3.3 Performance evaluation \nIn our experiment , 10-fold cross -validation is used  for \nSVM ic, SVM ae and SVM dir, and the performance evalua-\ntion calculates the accuracy ( 4), precision ( 5), recall ( 6) \nand F -score ( 7) of each detecting function.  \nAccuracy =𝑻𝒓𝒖𝒆  𝑷𝒐𝒔𝒊𝒕𝒊𝒗𝒆 +𝑻𝒓𝒖𝒆  𝑁𝑒𝑔𝑎 𝒕𝒊𝒗𝒆\n𝑷𝒐𝒔𝒊𝒕𝒊𝒗𝒆 +𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒;           ( 4) \n \nPrecision =𝑇𝑟𝑢𝑒  𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑧𝑒\n𝑇𝑟𝑢𝑒  𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒+𝐹𝑎𝑙𝑠𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒;           (5) \n \nRecall =𝑇𝑟𝑢𝑒  𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒\n𝑇𝑟𝑢𝑒  𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒+𝐹𝑎𝑙𝑠𝑒 𝑁e𝑔𝑎𝑡𝑖𝑣𝑒;           (6) \n \nF-score =2⋅𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛⋅𝑅𝑒𝑐𝑎𝑙𝑙\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙;            (7) \n \nThe true positive means it correctly detected the \nmovement; the false positive is a falsely detected move-\nment; and the false negative is a missed detection. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4984. EXPERIMENTAL RESULTS \nIn this section, the efficiency of the proposed SVMs is \nobserved . An example of the proposed EMG segmenta-\ntion is then compared to the prior work [ 15]. Finally, the \naveraged and overall simulation results are given.  \n4.1 The performance of SVM classifications \nTo illustrate both the proposed IC and AE effectively \nidentify the sound states of onset and offset, respectively, \nFigure 9 shows the trend of IC and AE values in one \ndown-up bowing movement by using the classification \nresults for SVM ic and SVM ae1 and SVM ae2. Table 2 shows \nthat, with the given FSM, the detection rate of onsets, \ntransitions and offsets are 90%, 100%, 100%, respective-\nly.  \n \nFigure 9.  The results of 3 classifiers: (a) onsets, (b) tran-\nsitions, (c) offsets. \nTable 2. The detection results of the bowing states with \nthe given FSM. \n Onset  Transition  Offset  \nAccuracy  90.00%  100%  100%  \nPrecision  90.00%  100%  100%  \nRecall  90.00%  100%  100%  \nF-score  90.00%  100%  100%  \nFigure 10 shows the distribution of active EMG frames \nduring up-bowing and down-bowing states, and it dis-\nplays the distribution of MAV , MAVS and WL. The \nSVM dir classifies the data with 85% accuracy. \n \nFigure 10. (a) The original distribution of up-bowing and \ndown-bowing EMG frames; (b) the results of SVM dir \nclassification. \n4.2 EMG segmentation \nThe results of EMG segmentation and its comparison to \n[15] are both illustrated in Figure 11. Figure 11 shows the violin signal of task#1 with three movements. Figure 11 \n(b) and (c) are the EMG segmentations  of our proposed \nmethod and [ 15], respectively . Channel#6 is used in th is \nexample  to illustrate a sample output . It is believed that if \nthere is an audio signal, then there is a corresponding \nmovement . It can be seen that the results segmented by \n[15], without the  additional  information of  the audio sig-\nnal, could not precisely  identify the segment  of move-\nments during bowing. However, t he proposed method is \nbased on the information from  audio  signal s and clearly \nidentifies  the segment of behavioral changes during  violin \nplaying.  \n \nFigure 11.  (a) The violin signal ; (b) the proposed EMG \nsegmentations ; (c) the EMG segmentations of  [15].  \n4.3 The simulation results \nThe detection result of violin bowing direction w as given \nin Table 3 where accuracy, precision, recall and F-score \nare presented. \n \nTable 3.  The detection results of the bowing direction: (1) \nthe detection results of ground truths of active frames; (2) \nthe detection results of extracted active frames. \n (1) (2) \nAccuracy  85% 87.5%  \nPrecision  76.92%  82.61%  \nRecall  100%  95% \nF-score 86.96%  88.37%  \nThe average detection results were shown to have excel-\nlent performance with an accuracy of 85%~87 .5%. The \nresults show that the proposed method efficiently identi-\nfies the bowing direction in violin playing. \n5. CONCLUSION AND FUTURE WORK \nThe proposed biomechanical system for recording the au-\ndio sounds and EMG signals during playing an instru-\nment was developed . The proposed method not only ex-\ntracts the segment during movement and detects the mov-\ning direction of bowing, but with the additional infor-\nmation of violin sounds, changes in muscle activity as an \nelement of motor control, could be efficiently detected \nwhen compared to the prior EMG segmentation (without \nany sound information). To the authors ’ knowledge, this \nis the first study which proposes such concept.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n499Future work will improve the detection rate of onset, \ntransition and offset to extract the period of an active \nframe more precisely. The detection of the bowing direc-\ntion will be also improved. Furthermore, the relationship \nbetween the mu sical sounds and the muscular activities  of \nplayers in musical performance  will be observed and ana-\nlyzed . By m easur ing the music and the player ’s muscular \nactivit y, better insight s can be made  into the neurophysio-\nlogical control during music al performances and may \neven prevent players from the  injuries  as greater insights \ninto these mechanisms are made .  \n6. REFERENCES \n[1] DC. Harding, KD. Brandt, and BM. Hillberry: \n“Minimization of finger joint forces and tendon \ntensions in pianists, ” Med. Probl. Perform Art  \npp.103-104, 1989. \n[2] KC. Engel, M. Flanders, and JF. Soechting: \n“Anticipatory and sequential motor control in piano \nplaying, ” Exp Brain Res . pp. 189- 199, 1997. \n[3] D. Parlitz, T. Peschel, and E. Altenmuller: \n“Assessment of dynamic finger forces in pianists: \nEffects of training and expertise, ” J. Biomech. \npp.1063-1067, 1998. \n[4] H. Kinoshita , S. Furuya , T. Aoki, and E. \nAltenmüller E.:  “Loudness control in pianists as \nexempliﬁed in  keystroke force measurements on \ndifferent touches, ” J Acoust Soc Am.  pp. 2959- 69, \n2007.  \n[5] AE. Minetti, LP. Ardigò, and T. McKee: “Keystroke \ndynamics and timing: Accuracy, precision and \ndifference between hands in pianist's performance, ” \nJ Biomech . pp. 3738- 43, 2007. \n[6] R. Merletti and P. Parker: Electromyography: physi-\nology, engineering, and noninvasive applications, \nWiley-IEEE Press , 2004. \n[7] C.-J. Lai, R.-C. Chan, and T.-F. Yang et al.: “EMG \nchanges during graded isometric exercise in pianists: \ncomparison with non-musicians, ” Journal of the \nChinese Medical Association,  vol. 71, no. 11, pp. \n571-575, 2008. \n[8] M. Candidi, L. M. Sacheli, and I. Mega et al.: “So-\nmatotopic mapping of piano fingering errors in sen-\nsorimotor experts: TMS studies in pianists and visu-\nally trained musically naïves,” Cerebral Cortex,  vol. \n24, no. 2, pp. 435-443, 2014. \n[9] S. Furuya, T. Aoki, and H. Nakahara et al .: “Indi-\nvidual differences in the biomechanical effect of \nloudness and tempo on upper-limb movements dur-\ning repetitive piano keystrokes,” Human movement \nscience,  vol. 31, no. 1, pp. 26-39, 2012. \n[10] D. L. Rickert and M. Halaki, K. A. Ginn et al.: “The \nuse of fine-wire EMG to investigate shoulder muscle \nrecruitment patterns during cello bowing: The re-\nsults of a pilot study, ” Journal of Electromyography \nand Kinesiology , vol. 23, no. 6, pp. 1261-1268, 2013. [11] A. Fjellman-Wiklund and H. Grip, J. S. Karlsson et \nal.: “EMG trapezius muscle activity pattern in string \nplayers: Part I —is there variability in the playing \ntechnique?,” International journal of industrial er-\ngonomics,  vol. 33, no. 4, pp. 347-356, 2004. \n[12] J. G. Bloemsaat, R. G. Meulenbroek, and G. P. Van \nGalen: “Differential effects of mental load on prox-\nimal and distal arm muscle activity,” Experimental \nbrain research,  vol. 167, no. 4, pp. 622-634, 2005. \n[13] S. Fujii and T. Moritani: “Spike shape analysis of \nsurface electromyographic activity in wrist flexor \nand extensor muscles of the world's fastest drummer,” \nNeuroscience letters,  vol. 514, no. 2, pp. 185-188, \n2012.  \n[14] S. Furuya and H. Kinoshita: “Organization of the \nupper limb movement for piano key-depression \ndiffers between expert pianists and novice players,” \nExperimental brain research,  vol. 185, no. 4, pp. \n581-593, 2008. \n[15] P. Mazurkiewicz, “Automatic Segmentation of EMG \nSignals Based on Wavelet Representation,” Advanc-\nes in Soft Computing Volume 45, 2007, pp 589-595 \n[16] Bodybuilding is lifestyle!  \"Chest - Bodybuilding is \nlifstyle!\"http://www.bodybuildingislifestyle.com/che\nst/. \n[17] Bodybuilding is lifestyle!  \"Chest - Bodybuilding is \nlifestyle!\" \nhttp://www.bodybuildingislifestyle.com/hamstrings/. \n[18] WJJ. Boo, Y. Wang, and A. Loscos, “A violin music \ntranscriber for personalized learning,” pp 2081- 2084 , \nIEEE International Conference on Multimedia and \nExpo,  2006. \n[19] BE. Boser, IM. Guyon and VN. Vapnik : “A training \nalgorithm for optimal margin classiﬁers,” In Fifth \nAnnual Workshop on Computational Learning \nTheory, ACM 1992.  \n[20] AJ. Andrews: “Finger movement classification using \nforearm EMG signals,” M. Sc. dissertation, Queen's \nUniversity , Kingston, ON, Canada, 2008. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n500"
    },
    {
        "title": "A Cross-Cultural Study on the Mood of K-POP Songs.",
        "author": [
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Kahyun Choi",
            "J. Stephen Downie"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417993",
        "url": "https://doi.org/10.5281/zenodo.1417993",
        "ee": "https://zenodo.org/records/1417993/files/HuLCD14.pdf",
        "abstract": "Prior research suggests that music mood is one of the most important criteria when people look for music-but the perception of mood may be subjective and can be in- fluenced by many factors including the listeners' cultural background. In recent years, the number of studies of mu- sic mood perceptions by various cultural groups and of automated mood classification of music from different cultures has been increasing. However, there has yet to be a well-established testbed for evaluating cross-cultural tasks in Music Information Retrieval (MIR). Moreover, most existing datasets in MIR consist mainly of Western music and the cultural backgrounds of the annotators were mostly not taken into consideration or were limited to one cultural group. In this study, we built a collection of 1,892 K-pop (Korean Pop) songs with mood annota- tions collected from both Korean and American listeners, based on three different mood models. We analyze the differences and similarities between the mood judgments of the two listener groups, and propose potential MIR tasks that can be evaluated on this dataset.",
        "zenodo_id": 1417993,
        "dblp_key": "conf/ismir/HuLCD14",
        "keywords": [
            "music mood",
            "subjective perception",
            "cultural background",
            "increasing studies",
            "MIR tasks",
            "cross-cultural evaluations",
            "K-pop songs",
            "mood annotations",
            "listener groups",
            "MIR tasks"
        ],
        "content": "P\nm\nt\nf\nb\ns\na\nc\na\nt\nm\nm\nw\nt\no\nt\nb\nd\no\nt\nT\nt\nR\ni\nm\ns\nc\nm\nf\ns\nh\no\nb\nD\nM\nt\np\na\ne\nt\nT\nbP\nm\nth\nfl\nba\nsi\nau\ncu\na \nta\nm\nm\nw\nto\nof\nti\nba\ndi\nof\nta\nT\nto\nR\nin\nm\nst\nca\nm\nfo\nse\nha\non\nbe\nD\nM\nte\npo\nan\neX\nti\nT\nbu\n \nL\nL\nC\nS\nC\nU\nri\nmo\nhe\nlu\nac\nic\nut\nul\nw\nas\nmo\nmu\nwe\no \nf \no\nas\nif\nf \nas\nTh\nor\nRe\nng\nmo\ntu\nat\nma\nor\ner\nav\nn \nel\nDe\nMI\nes\nos\nnd\nX\no\nTh\nui\nLic\nLic\nCh\nSo\nCo\nU\nio\nos\ne p\nue\nck\nc m\nto\nltu\nw\nsk\nos\nus\nere\no\n1\nn\nse\nffe\nt\nsk\nhe\nrs \nec\ng \noo\nud\nte\nay\nrt \nrv\nve\nc\nle\nes\nIR\nstb\nse\nd \nXc\nn\nhis\nil\nce\nce\nho\non\non\nUn\nor \nst \np\nn\nkg\nm\nom\nur\nwe\nks \nst \nsic\ne \non\n1,8\nns \ned\nfer\nth\nks \n m\nb\no\nn\nod\ndie\ned\ny \nt\nve\ne \ncl\ned\np\nR \nbe\ned\nc\nch\nn v\ns \nd\nen\nen\noi, \ngs\nnfe\nni\nr\ni\ner\nnc\ngr\nmo\nm\nre\nell\ni\ne\nc \nm\nne\n89\nc\nd \nre\nhe \nth\nm\nbe\ng\nnu\nd \nes\nd \np\nto\ne u\nd\nla\nd \nit\n(\ned\nd \nco\nha\nve\ns\ndin\nnse\nnse\nJ\ns”\nere\nv\nx\nre\nm\nrc\ned\nro\noo\nat\nes\nl-\nin\nex\na\nm\n c\n92\nco\no\nen\nt\nh\nmo\neh\ngn\num\nd\ns \nth\ner\now\nus\nde\nas\nb\nte\n([7\nd \nb\nom\nan\nen\nstu\nng\ned\ne \nJ. \n”, \nen\nve\nxi\nes\nmp\nce\nd \nou\nod\nte\ns h\ne\nn \nxi\nan\nmo\ncu\n2 \noll\non\nnc\ntw\nat\noo\nhi\nniz\nm\nda\nin\nha\nrc\nwa\nse\nev\nsi\nby\n t\n7]\nf\nby\nm\nng\nnu\nud\ng \nd \n(C\nSt\n1\nnc\ner\ni\nse\npo\nep\nb\nun\nd \ned\nha\nst\nM\nst\nnd\nst\nul\nK\nle\nn \nce\nw\nt \nod\nn\nzi\nmb\nat\nn \nat\nce\nar\ner\nve\nif\ny \nth\n] \nfor\ny i\nmp\nge \nue\ndy\na\nu\nCC\nte\n5t\nce,\nrs\na\nea\nor\npti\nby\nnd\np\nd \nas\nta\nMu\ntin\nd \ntly\nltu\nK\nec\nth\nes \no \nca\nd \nnd\nin\ner\na \nM\nt \nei\nrd\nrs \nelo\nfy\nli\nhe\n[\nr \nin\npa\n(\ne, \ny \na d\n ©\nund\nC \neph\nth\n, 2X\nit\nao\narc\nrta\nio\ny \nd. \npe\nm\ns b\nab\nu\nng\nt\ny \nur\nK-p\ncte\nhr\na\nl\nan\no\nd p\nng\nr \nt\nM\np\nv\nd \nf\nop\nyin\nis\ne \n[1\nc\nnte\nare\n(M\nh\na\nda\n© \nde\nB\nhe\nh \n20X\nty\nox\nch\nan\non\nm\nI\ner\nmo\nb\nbli\nsi\ng \nth\nn\nra\npo\ned\nre\nan\nli\nn \nf \npe\ng t\no\nto\nMu\npe\ne \ne\nfr\npe\nng\nste\ng\n4\ncro\ner\ned\nM\nha\nai\nat\nX\ner \nBY\nen\nIn\n01A\nXi\ny \nxh\nh \nnt\nn \nma\nIn\nce\noo\nee\nis\nic\nd\nhe\nno\nal \nop\nd \nee\nnd\nst\nb\nm\neo\nth\nof\no \nus\neo\nm\nes\nom\ned\ng \nen\ngr\n4])\no\nre\nd.\nMIR\nas\nim\nta\nXia\na\nY \nn D\nnte\n4A\nia\no\nhu\ns\nt c\no\nan\nn r\nep\nod\nen\nsh\nc \nda\ne \not\ng\np \nfr\ne \nd \nte\nbe\nmu\nop\nhe\nf \ni\nic\nop\nm\nta\nm\nd \nm\nne\nro\n), \nss\nes\n. \nR\ns y\nms\nase\nao\na \n4\nD\ner\n. A \nao\nof\nu\nsu\ncr\nof \nny\nre\npt\nd \nn \nhe\nIn\nat\nc\nt t\ngr\n(\nfro\nd\nsi\nen\ne e\nu\npl\ne \ns\nim\nc \npl\nmu\nab\nm \na\nm\ner\nw\nw\ns-\nste\nM\nRE\nye\ns \net\no H\nC\n4.\now\nrnaC\no \nf H\n@\nug\nri\nfm\ny \nec\ntio\nc\nin\nd \nnf\nas\ncu\nta\nro\n(K\nom\ndif\nim\nne\nev\n1\nsi\nle\nim\ntu\nmp\nIn\ne \nus\nbl\nd\nan\nmu\nrs \nwi\nw\n-c\ned\nMu\nEX\net\nt\nt1\nHu\nCr\n0)\nwn\natC\nH\nH\nh\ngg\nte\nm\nfa\nce\non\ncl\nnc\nfo\nse\nult\nak\nou\nKo\nm\nff\nm\ner \nva\n. \nic\n’s\nm\nud\npr\nnf\nic\nis\ndif\nnd\nus\nn\nwe\ncu\nd \nu\nX)\nt t\nto\n c\nu, \nrea\n). \nni\ntioCR\nH\nHo\nhk\nge\ner\no\nfac\nn\nns\nla\ncr\nte\nor\net\ntu\nke\nup\nor\nm b\nfer\nil\ng\nal\nI\nc i\ns \nmp\ndi\nro\nfo\nfr\nc \nsh\nff\nd \nic\nfr\nng\ne \nult\nre\nsi\n),\nto\n w\nco\nJ\nat\nA\nie\nonR\nu\non\nku\nA\nes\nria\no\nct\nnt \ns \nas\nre\nes\nrm\nts \nur\nen\np. \nre\nbo\nre\nla\ngr\nlu\nIN\nis\nm\npo\nie\nov\nor\nro\nm\nhin\nfe\ne\nc \nro\ng \nst\ntu\nes\nic\n, w\no a\nw\non\nJin\niv\nAt\n. “\nnalRO\nu \nn\nu\nA\nts\na \nd\nto\ny\nb\nsi\nea\nstb\nma\ni\nra\nn i\nI\nea\not\nen\nri\nro\nuat\nNT\ns a\nmo\nort\ns \nve\nrm\nom\nmo\nng\nre\nev\nf\nom\nin\nti\nura\nse\nc \nw\nad\nwo\nns\nn H\nve \nttr\n“A\nl O\nng\n.\nAB\ns \nw\nd m\nor\nye\nby\nif\nas\nbe\nat\nin\nl \nin\nIn\nan\nth\nnt\niti\nou\nte\nT\nar\not\nta\nh\ne \nma\nm \noo\ng \nen\nva\nfro\nm \nnt\nll\nal\nea\nIn\nwh\ndd\nor\nsis\nHa\nC\nrib\nA \nSO\ng K\nh\nBS\nth\nwh\nm\ns \nar\ny \nfic\nin\ned\ntio\nn M\nb\nnt\nn t\nn \nh \nt m\nie\nup\ned\nTR\nrg\ntiv\nan\nha\nu\nat\nod\na\nnt\nalu\nom\nd\nte\nl \nl \nar\nnf\nhi\nd \nrk\nst\na L\nC\nbu\nC\nocS\nK\nhk\nST\nha\nhe\nma\nin\nrs\nv\nca\nng\nd \non\nM\nba\nto\nth\nP\nK\nm\nes\nps\nd o\nRO\ngu\nva\nnc\nav\nus\ntio\nd\nd \na \nt p\nua\nm\ndi\nre\nd\nM\nrch\nfo\nch\na\nk \ntin\nL\nom\nut\nCro\nciSS\nKo\nk \nT\nat\nen\nay\nnc\ns, \nva\nati\ng.\nf\nn \nMI\nac\n c\nhi\nPo\nKo\nmo\n b\ns, \non\nO\nua\nat\nce\nve\ner\non\ndif\nd\ng\npa\nat\nm \nif\nes\no \nMI\nhe\nor\nh \na c\nto\nng\nee\nm\ntio\nos\netS\non\nTR\nt \nn \ny b\ncl\nth\nari\nio\n H\nfo\nR\nIR\nck\nco\ns \nop\nor\noo\nbe\na\nn \nOD\nab\ntio\ne o\ne \nrs\nn \nff\ndi\ngl\nar\nte\nd\nffe\nst\nn\nIR\ner\nrm\ni\ncr\now\ng \ne, \nmm\non\nss-\nty -\nn\nRA\nm\np\nbe\nlu\nh\nio\non\nH\nor\nR\nR \nkg\non\ns\np) \nre\nod\net\nan\nt\nDU\nbly\non\no\nb\ns’\nR\nfer\nff\nlo\nrt\nd\ndif\nfer\ns \nno\nR \nrs\nma\nis\nro\nw\no\nK\nmo\nn: \n-C\nfC\nng\nA\nmu\npe\ne \nud\ne \nou\nn \nHo\nr \net\nc\ngr\nns\nstu\ns\nan\nd \ntw\nnd\nth\nU\ny \nn \nf \nbe\n \nRe\nre\nfe\nob\ns \nd a\nff\nre\no\not\nt\ns c\nat\n t\nos\nwa\nof\nKa\non\nX\nCu\nfoC\ng \nAC\nus\neo\ns\ndin\nn\nus\no\now\ne\ntr\nco\nro\nsid\nud\nso\nn \nm\nwe\nd \nhis\nC\no\no\nm\nee\na\net\nen\nere\nba\no\nal\nfe\nen\non\nt \nta\nca\ntio\nth\ns-\nard\nf 1\nahy\ns \nXi\nult\nor U\nCT\nsi\nop\nsu\nng\nnu\ns \nof\nwe\nev\nrie\non\nou\nd\ndy\non\na\nm\nee\np\ns d\nCT\non\nof\nmu\nen\ncc\ntr\nnt\nen\nal \nof\nlg\nere\nnt \nn \nh\nask\nan\non\nhe\n-c\nd \n1,\nyu\nA\nia\ntu\nMU\nT \nic\nple\nubj\ng \num\nc\nf \nev\nva\nev\nns\nun\ner\ny,\nng\nan\nmo\nen\npr\nda\nTI\nne\nf l\nu\nn \nce\nrie\nt \nnt\nM\nf t\ngo\nen\nc\nc\nha\nks\nn \nn \ne \ncu\nf\n,8\nun\nAt\nao \nura\nMuL\nU\nc \ne \nbje\nt\nmb\nu\nm\nve\nalu\nva\nis\nnd\nra\n, w\ngs\nnd\nd\nn \nro\nat\nIO\ne \nlis\nsi\ne\nes\nev\nc\ntly\nM\nth\nor\nnt\nco\ncu\nav\ns \nb\nR\np\nult\nfil\n89\nn C\nttr\nH\nal \nusL\nUn\nm\nlo\nec\nth\nb\nult\nmu\ner\nua\nal \nst \nds \nat\nw\ns \nd \ndel\nth\nop\nta\nO\no\nst\nic\nex\nss\nva\ncu\ny \nMI\nhe\nit\nt \nou\nul\nve\nw\nbe\nR\npri\ntu\nll\n92\nCh\nrib\nHu\nS\nsicT\nn\nmo\noo\nct\nhe\ner\ntu\nus\nr, \nat\n(\nm\nio\nwe\nw\nA\nls\nhe\npo\nas\nON\nof \nte\nc m\nxp\ns \nal \nult\n(\nIR\ne w\nth\nc\nun\ntu\ne \nwh\ne p\nRe\nim\nur\nin\n2 s\nho\nbu\nu, \nStu\nc TU\nniv\nj\noo\nok\ntiv\n l\nr \nura\nsi\nth\ntin\n(M\nm\no\non\ne b\nwi\nAm\ns. \ne \nos\net\nN \nft\nen\nm\nplo\nto\n(\ntu\n([\nR \nw\nhm\ncu\nnt\nur\na \nhe\npr\ntr\nma\nra\nng\nso\noi,\nuti\n J\nud\nInU\nv\nj\nod\nk \nve\nlis\no\nal\nic \nhe\nng\nM\nma\nof \nn \nbu\nth\nm\nW\nm\nse\nt. \nth\nnin\nmo\nor\no \n(M\nura\n2\ns\nwo\nms\nult\ntr\nra\nw\ner\nro\nrie\nar\nal \ng \non\n, J\nion\nJin\ndy\nnfUR\ne\ni\nd \nf\ne \nst\nf \nl \ner\ng \nMI\nin\nt\no\nu\nh \nmer\nW\nmo\n p\nhe\nng\noo\nri\nm\nM\nal\n] \nsy\nor\ns \ntu\nrie\nal \nw\nre\nop\nev\nry\ne\nt\nng\nJ. \nn \nn \ny o\nfoR\nrs\nn\ni\nfo\na\nte\ns\ng\nfr\nre\nc\nR\nnl\nth\nor \nil\nm\nri\nWe\noo\np\n s\ng \nod\nin\nm\nMIR\nl \n[\nys\nrld\nth\nure\nes\ni\nwe\ne m\npe\nva\ny \nv\nth\ngs\nS\n4\nH\nof \nrmRA\nJ\nsi\nnh\nis \nor \nn\nn\ntu\ngro\nro\ne h\ncr\nR)\nly\nhe\nw\nlt \nm\nic\ne \nod\no\nst\nto\nd,\nng\nmu\nR\nb\n9\nste\nd,\nha\nes\ns \nin\nell\nm\ner\nal\nM\nva\nhis\ns \nte\n4.0\nHa\nfM\nmaA\nJi\nit\nha\no\nm\nd\nner\nud\nou\nom\nha\nro\n. \ny o\ne \nw\na\no\nca\na\nd \nte\ntro\no \n a\ng \nus\nR) \nba\n])\nem\n r\nat\ns \n(\nnfl\nl-e\nme\nly\nl \nM\nlu\ns \nfr\neph\n0 \na \nMo\natAL\nn\nty\nal\non\nmu\nd c\nrs\ndi\nup\nm \nas\nos\nM\nof\na\ner\na c\no\nn\nan\nju\nen\non\nm\nan\nth\nsic\nh\nac\n). \nm\nre\nt \na\n[1\nflu\nes\neth\ny \nE\nMIR\nua\ng\nro\nhe\nI\nL\noo\ntioL\nn H\ny \nl\nne\nu\nca\ns’\nie\nps\nd\ns y\nss\nM\nf \nan\nre\nco\nd \n l\nna\nu\nnt\nng\nm\nn \nhe\nc.\nha\nck\nI\nm \nes\nc\nan\n17\nue\nst\nh\ne\nEv\nR\nat\nga\nom\nen\nInt\nLee\nod\nonL \nH\no\ne\ne \nsi\nan\n c\nes \ns \ndi\nye\n-c\nMo\nW\nnn\ne \nol\na\nlis\naly\nd\ntia\nge\nmu\ni\ne \n. \nav\nkg\nIn\nth\nse\nan\nnd\n7]\nen\nta\no\nv\nva\nR e\nio\nap\nm\nn D\nte\ne, \nd i\nn RS\nHa\nof \ne\no\nic\nn \ncu\no\na\nif\net\ncu\nore\nW\nno\nli\nlle\nan\nst\nyz\ndg\nal\nes\nusi\nn\nu\nR\nve\ngr\nn \nha\nar\nn \nd/\n] \nnc\nb\nd\nal\nalu\nev\non\np \nm a\nDo\nern\nK\nin\nRS\na \nfW\ne@\nof\nc—\nb\nul\nof\nan\nffe\nt t\nul\neo\ne\nota\nim\nec\nnn\nten\nze\nm\nl \nst\nic\nncr\nus\nRe\ne \nro\nan\nat\nrc\nw\n/o\n[\nce\nli\nds \nlu\nua\nva\nn \nb\na \now\nna\nKa\nn K\netT\nL\nW\n@u\nf \n—\nbe \nltu\nf m\nnd\nfer\nto\nltu\nov\nst\nat\nmi\nct\nno\nn\ne \nme\nM\nt f\nc \nre\nse\nec\nin\nu\nn \nt \nch\nw\nr \n1\nes \nish\np\nua\nat\nal\nta\nby\nn\nwn\nati\nah\nK-\ntriT\nL\nWa\nuw\nth\n—b\ni\nur\nm\nd \nre\no b\nur\nve\nte\nto\nit\ntio\not\ner\nth\nen\nMI\nfa\n[4\nea\ne \nce\nnd\nun\ne\nc\nhe\nwo\nl\n4\no\nh\npr\nat\ntio\nlu\nas\ny \nno\nni\nion\nhy\n-P\nievU\nLe\na\nw\nh\nbu\nin\nra\nmu\no\nen\nb\nra\ner\nern\nor\ned\non\nta\nrs\nh\nnt\nIR\nac\n4]\nas\no\nen\ndi\nnd\nef\nan\ner\nork\nla\n])\non\ned\nro\ned\non\nua\nsk\n1\non\nie.\nna\nyu\nPo\nvaU\nee\nsh\n.\ne \nut \nn-\nal \nu-\nof \nnt \ne \nal \nr, \nn \nrs \nd \nn \na-\ns, \ne \nts \nR \nc-\n]. \ns-\nof \nnt \ni-\nds \nf-\nn \nrs \nk \na-\n). \nn \nd \no-\nd \nn \na-\nk. \n) \nn-\n. \nal\nun\np\nalD\ne \nh\ne\nf\nfD\nhin\nedY\nn\nduY\ng\nu Y O\ngto\nW\nb\nf\nr\ntl\np\nu\nM\nc\nd\np\nL\nW\nr\ni\nH\nv\ns\nl\nb\nn\nm\nf\na\nt\nK\np\nu\nb\nt\nA\ne\nK\nt\np\nh\na\ns\nc\ng\nc\nb\n  \n1\nr\nw\n2\npO\no\nW\nba\nfro\nre\nw\nis\npr\nus\nM\nco\nde\npa\nLe\nW\nre\nn\nHu\nvi\nso\nar\nbe\nno\nm\nfro\nan\nio\nK-\npe\nun\nbe\nh\nAs\nen\nKo\no\npe\nho\nac\nsic\nco\ngr\nco\nbu\n  \n T\nrie\nwh\npoO\nn\nWe\nas\nom\nan\nwe\nste\nro\nsin\nMu\nou\nes\nay\nee\nWe\nan\nfl\nu \nd\non\nrg\nels\not\nu\nom\nnn\nK\non\n-p\neo\nni\nee\ne \nsi\nnc\nor\n K\nec\now\ncc\nC\nc \non\nro\non\nui\n  \nTh\nety\nhe\nopF\nn \nes\nse\nm\nn)\nee\nen\nop\nng\nus\nun\nsig\nyin\ne \nes\nn \nlu\n[\nded\nng\nge\ns \nt b\nusi\nm\nno\nK\nna\npo\nop\nq\nen\n1\nia\nce\nre\nK\nct \nw \nce\nC\nm\nnta\nou\nntr\nlt\n  \nhe\ny \nen\nh\np-hF \nte\nd\nm t\n);\nen\nne\npo\ng \nic\nntr\ngn\nng\na\nte\nl\nue\n[1\nd \ngs\ner\nw\nbe\nic\nm \nota\nK-p\nal\nop\nple\nqu\nn \n19\na. \nes \nea\nK-p\no\nm\nss\nCu\nm\nai\nun\nro\nt i\n  \ne t\no\nn n\nhtt\nhoM\ner\nd o\ntw\n; \nn \ner\nosi\nth\nc \nri\nne\ng \nan\ner\nlis\nen\n17\nb\n p\nr (\nwe\ne \nc m\nA\nat\npo\n a\np \ne \nue \nh\n95\nA\ni\nan\npo\nof\nm\nsi\nurr\nmo\nin\nnd\nol\nin\n  \nta\nof \nno\ntp\notM\nrn\non\nw\n2\nm\nrs\nin\nhi\nis\nie\ned\nm\nnd\nrn\nst\nnc\n7]\nb\npr\n(n\ner\nd\nm\nAm\nti\nop\nau\nH\nf\nc\nhe\n50\nA \nn\nn \nop\nf \nmo\nin\nre\noo\nn \nd o\nlle\nn t\n \nag\nfM\not\np:/\nt-1M\nn \nn \no \n2) \nmo\ns \nng\nis\ns \ns \nd \nm\nd \nn s\nten\ne\n c\ny\nro\nne\nre\ndi\nmo\nm\non\np \nud\nHo\nfr\nch\nav\n0s\nre\nn t\nli\np \nK\noo\nng\nen\nod\np\no\ned\nth\n  \ng d\nM\nt n\n//w\n10MO\nK\n{\ncu\nth\nd\na\noo\no\ng \ns d\na\na\nt\nmo\nH\nso\nn\nd \nco\ny \nov\nea\ne \nir\noo\nme\nn\ni\ndi\not\nro\nha\nvi\ns, \nec\nth\nis\nm\nK-\nod\ng m\nntl\nd r\npri\nf \nd \nhi\n  \nda\nMIR\nne\nw\n00O\nK\nc\nul\nhr\ndi\nan\nod\nn\nc\nda\na \nan\nto\nor\nHu\non\ner\np\nom\nW\nvi\nar\nap\nec\nod\nri\ns \nis \nie\nt \nom\nar\nil\ny\nce\nhe\nte\nm\n-p\nd c\nm\nly\nre\nim\na\n[\nis \n  \nata\nR\nee\nw\n0-OO\nKa\nck\nlt\nre\nist\nna\nd \n t\nro\nat\nm\nnd\no \nre\nu \nng\nrs\npe\nm\nW\nid\nly\npp\nct\nd \nic\nd\ni\nn\n1\nm \nra\ny\nye\nen\n p\nen\nmu\npo\nca\nmu\ny \nec\nm\nan\n5\ns\n  \na \nR e\ned\nww\nlaO\nah\nka\ntu\nee\nti\nal\nl\nth\nos\nta\nme\nd c\ns\n \n[\ngs\ns \neo\nmp\nWe\nde\ny \npl\ntly\np\nca\ndir\nin\nnce\n0\nd\nct\ny i\net \nnt\npe\nne\nusi\nop\nan\nus\nth\nco\nmar\nnn\n].\nstu\n  \nw\nev\nded\nw.\nauOD\nh\nah\nure\ne m\nin\nly\nla\nhe\nss\nase\ned\ncu\ner\nat\n[9\ns \nan\nop\npa\nst\nd\n5\nli\ny \npe\nan\nre\nnc\nes\n0\ndi\nte\nin\ni\nt s\ner\ner\nic\np, \nn \nic\nhe\nog\nri\nno\n. T\nud\n  \nwi\nva\nd \nb\nunD\ny\nh\ne \nm\nnct\nyz\nab\ne \ns-\net\ndi\nul\nrv\ntt\n9] \np\nnd\npl\nare\nte\nd b\n50\ned\nc\nrc\nn a\nec\ncre\ns,\n c\nif\ner\nnf\ns \nstu\nrc\nrs \nc. \na\nb\nc \ner\ngn\nily\nota\nT\ndy\n  \nll\nalu\nfo\nil\nnchD \nyu\ny\n(\nmu\nt \nzin\nbe\nsa\ncu\nt. \n2\niu\nltu\nve\nen\nc\nro\nd \ne\ned\nern\nby\n00\nd \nco\nce\nan\nctl\nea\n, a\nch\nffe\nris\nflu\nd\nu\ncep\nb\nI\nai\nbe \nb\nre\nni\ny \nat\nTo\ny \n  \nl b\nua\nor\nlb\nheI\nun\nU\nyu\n(i.\nus\nc\nng\nels\nam\nul\n1 \n2. \num\nur\ne \nnt\nco\nov\nf\n’s\nd \nn \ny \n0 s\nt\nom\nep\nnd\nly\nas\nas\nha\ner\nsti\nue\nde\ndy\npt\nba\nIn\nim\nu\ny\ne e\nti\nW\nto\no t\ni\n  \nbe\nat\nr M\nbo\nesIN\nn \nU\nu2\n.e\nsic\nu\ng \ns \nm\nlt\n  \nR\nm \nre\ng\nti\nom\nvi\nfo\ns p\nm\nC\nso\nto\nm\npt\nd \ny c\nsin\ns \nar\nre\nic\nen\nee\ny \nti\nas\nn t\nmi\nus\ny u\nex\nio\nW\nor\nth\nis \n  \ne i\nio\nM\noa\ns-N\nC\nUn\n2,\ne.,\nc \nult\nt\np\nme\ntu\n  \nR\nb\nes\ngl\no\nm\nid\nou\npe\nm\nli\nCh\non\no t\nmp\ntio\nK\nco\nn\nev\nrt \nen\ncs\nnc\nep\nb\nio\nse\nth\nin\nse\nus\nxi\non\nWe\nrs \nhe\nt\n  \nin\non\nMI\nard\nsiN \nC\nni\n,j\n K\nm\ntu\nth\npr\ne \nura\nRE\nbe\ns. \nlo\nn\nmp\nde\nun\ner\nmo\nist\nh\nng\ntw\npa\non\nK\nom\ng\nvi\ni\nnt \ns d\nce\nply\nby\non\ned\nhi\nng\ned\nse\nist\nn \nes\nw\ne b\nth\n  \nnc\nns\nR\nd.\nistK\nCh\niv\nj\nK\nmo\nura\nhe\nro\nse\nal\nEL\ney\nA\nb\nn t\npa\ned\nnd\nrc\noo\nte\nin\ngs\nwo\nara\nns\nKo\nm\nly\nid\nn\nc\ndu\ned\ny \ny \nn o\nd o\ns \ng \nd a\ner\nt \n([\nste\nw\nb\nhe \n  \nor\n a\nRE\nco\ntaK\nho\nve\nd\nK-\noo\nal\ne \nov\net\nl M\nLA\nyo\nAs\nba\nto\nare\nd b\nd t\nce\nod\nen\nne\ns)\no \nab\ns \nre\nmp\ny \nde\n 2\ncu\nue\nd \nr\nL\nof\non\ns\nto\nas\ns \nse\n[3\ner\na\nes\nf\n  \nrp\nan\nEX\nom\nar-K-\noi\ner\ndo\np\nod\nl g\nd\nvid\nt \nM\nA\non\ns \nl \no \ned\nby\nth\nep\nd l\nne\nes\n) i\ns\nbl\no\nea\npa\nb\nen\n20\nul\ne \nb\nro\nLe\nf K\nn \nstu\no \ns \nfr\nev\n3]\nrn\ns \nst\nfir\n  \npo\nnd\nX.\nm\n-i-P\ni \nrs\now\npo\nd \ngr\ndif\nde\no\nMI\nAT\nnd\nm\nu\nc\nd \ny \nha\npti\nla\ners\nse\nin\nse\nle\nof \nan\nar\nbe\nnc\n01\nltu\nit\nby\no\nee\nK\nh\nud\ni\na\nfro\nve\n \nn \ne\nt o\nrs\n  \nor\nd r\n \nm/a\ns-P\nsi\nwn\nop\nm\nro\nff\ned\nof \nIR\nTE\nd \nma\nus\ncr\nm\nA\nat \nio\nab\ns \ne l\nn \nep\ne. \nt\nn \nab\nec\nce\n11\nur\nts\ny \nte\ne e\nK-\nho\ndy\nim\na d\nom\ner\n[5\nm\neit\nof\nst \n  \nat\nre\nar\n-nO\nity\nni\np \nmo\nou\nfer\nd \nf1\nR \nE\nth\nan\nse\nro\nm\nAm\nc\non\nbe\nt\nli\nth\npa\nI\nh\nli\nbl\no\ned\n12\nra\ns h\nA\ned\net\np\now\ny,\nmp\nde\nm\nra\n5]\nmu\nth\nf \no\n  \nte\nele\nrti\nnoO\ny\ni\n \nor\nod\nup\nre\nb\n1,\nta\nD\nhe\nny\ner\nss\nmo\nm\ncu\nn o\nels\nto\nst\nhe\nara\nIn\ne \nis\nle\nm\nd b\n2, \nal \nh\nAm\nd \nt a\no\nw \n, w\npr\nes\nm d\nal \n])\nus\nhe\no\nof\n  \ned \nea\nic\no-1OP\ny o\ne\nr \nde\nps\nen\nby\n,8\nas\nD W\ne \ny \ns,\ns-\noo\nme\nul\no\ns \no \nte\nei\nat\nn \ns\nste\ne. \nmi\nby\na\nb\nis\nm\nin\nal\np\nt\nw\nro\nsc\ndi\nin\n. \nsi\ner\nou\nf i\n  \ni\nas\nle\n1-P \nJ\nof\ne}\nK\nel\ns \nnc\ny \n89\nsk\nW\nb\nM\n, \n-c\nod\ner\ntu\nf \no\nl\nen\nir \nte\nth\nsa\nen\n  \nin\ny \nan\nba\nsto\nme\nn \nl. \n g\nth\nwe\nov\ncri\niff\nnf\nH\nic \nr n\nur \nit\n  \nnt\nse\nes\n-oS\nJ\nf \n}@\nK\nls\n(i\nce\nA\n92\nks\nW\nbo\nM\nre\ncu\nd \nric\nur\nm\non\nab\nne\ns\ne d\nhi\nam\nne\n \nng\nth\nnd\nac\nor\nri\nth\n[7\nge\nhe\ne \nve\nip\nffe\nfl\nHo\na\nno\nk\ns \n  \nto\nd \n/n\nonS\n. \nI\n@i\nor\ns a\ni.e\nes\nA\n2 \ns t\nO\nou\nIR\nes\nult\nla\nca\nra\nmu\nn \nbe\ners\nstu\nda\nis\nme\ner\ng p\nh\nd \nck\nry\nic\nhe\n7]\nen\ney\nfo\ne \npt\ner\nlu\now\nan\not\nkn\nk\n  \no M\ni\nne\n-nSO\nS\nIll\ni\nre\nan\ne.\n \nAm\nK\nth\nOR\nun\nR\nse\ntu\nab\nan\nal \nus\nU\nel\ns.\nud\nat\n p\ne \nrs\np\ne \nis\nkg\ny;\nca\ne \n] \nnr\ny a\noc\nth\nto\nre\nuen\nw\nnd\nt \nno\nkin\nM\nin\new\nneO\nSt\nli\nl\nea\nnn\n, \nan\nme\nK-\nha\nR\nnd\nR s\nea\nur\nbe\nn, \nb\nsi\nU.\nls\n T\ndy\nta\npa\ns\n, \nop\nl\ns \ngr\n; K\nan\nlo\nd\nre\nap\ncu\nhe\nr \nn\nnt\nwe\nd \nsp\now\nnd\nMI\nncr\nws\newON\nte\nin\nl\nan\nno\nA\nnd\ner\n-p\nat\nRK\nda\nsy\nar\nal\nel\nC\nba\nic \nS\ns \nT\ny,\nas\nap\nse\nm\npu\nlau\na\nro\nK\nn p\non\nis\nes \npp\nus\ne \nf\nnt \nti\nev\nt\np\nw\nd \nIR\nre\ns/4\nw-N\nep\nno\nli\nn \not\nAm\nd \nric\npo\nt c\nK \nar\nys\nrch\nl \ns \nCh\nac\nm\nS. \no\nTh\n, \net\npe\nt \nma\nu\nun\nac\nou\nKo\npo\nng\nsc\nb\npl\ns \nu\nfor\ncu\nia\nve\nth\nec\nwle\nt\nRE\nem\n46\n-kNG\nph\noi\nin\nP\nta\nm\ns\nca\nop\nca\nry\nste\nh\ni\no\nh\nck\nm\np\non\nhe\nal\nts\ner\no\nak\nla\nn\nti\nun\nor\nop\ng \ncu\nby\nlie\non\nun\nr \nu\nl \nr,\nhe\nci\ned\nth\nEX\nme\n67\nkoG\nh\nis\nno\nPo\nate\nme\nsi\nan\np \nan\ny o\nem\ner\nss\non\nin\nkg\nmo\npo\nn \ne d\nlth\ns a\nr, \nof \nki\nar\nch\niv\nnd\nea\np \nh\nus\ny A\ned\nn \nnd\no\nlt\nd\n, \n c\nif\ndg\nha\nX \nen\n77\nreGS\nhe\ns \no\nop\ned\neri\nim\nn \nso\nn \nof\nm\nrs\nsu\nn \nne\ngro\noo\nop\nC\nda\nh\nan\nw\nfK\nin\nr w\nh \nve\nds\nan\nc\nhi\nse\nA\nd \nt\nde\norg\ntu\nda\nm\ncu\nfie\nge\nat \nfo\nnta\n76\neaS\nen\ni\np)\nd \nic\nmi\na\non\nb\nf \nms \ns \nue\na\nes\nou\nod\np \nCh\nat\no\nnd\nw\nK\nng\nw\no\nly\ns. \nn \ncu\nst\ned\nAm\ng\nth\ner\ng\nure\nta\nmo\nul\ned\ne, \nis\nfor\nal\n64\na-cS \nn \ns\n a\nb\nca\nila\nan\nng\nbe\nl\nn\nh\nes\na \nse\nun\nd. \ns\nhi\nta\nu\nd \nwe\nK-p\ng \nwit\nof\ny \nK\nc\nul\nto\nd \nm\nge\nhe\nrst\nan\nes\nas\nos\nltu\nd \nt\ns \nr u\nlly\n4/b\nchD\ns.\nan\nby\nan\nar\nnd\ngs\n e\nan\nne\nha\ns \ns\ne, \nnd\nY\no\nin\nas\nug\nt\ne \npo\nth\nth\nf B\ns\nK\ncu\nltu\nor\nth\ner\nen\ne m\nta\nni\ns. \nse\nst \nur\n[3\nth\nc\nus\ny \nbi\nhaDo\n.e\nnd\ny l\nn a\nri\nd \ns;\nev\nng\nee\nav\nin\net\na\nd \nYa\nn\nne\net\nh\nth\nco\nop\nhe\nh \nB\nso\nK-p\nult\nur\nry\nhe\nri\nnr\nm\nan\niz\n \nets\no\nra\n3]\nhe\nco\nse\no\nill\naro\ned\nd \nlis\nan\nti\nK\n; \nva\ngu\ned\nve\nn \nt \nan\ni\nan\nng\nes\nts\nh t\nhu\nom\np \ne \nin\nil\nou\npo\ntu\nre\ny o\ne \nic\nre \nmo\nnd\nzin\ns \nof\nal\n] \ne d\nom\ne i\nov\nlb\nrt ow\nd\nl\nst\nnd\nie\nKo\nan\nal\nua\nd t\ne \nM\no\nnd\nin\nng\ngs \ne \ns \nth\nus \nm\ns\nm\nnt\nllb\nug\nop\nur\ne \nof\nd\nan\nl\noo\ndin\nng\nin\nf \nl b\no\nda\nmp\nin\nver\nown\nu\nla\nte\nd \ns \nor\nnd\nu\nag\nto\nb\nM\nof\nd K\nnd\ng \np\np\nw\nhe\nm\nmp\nso\nmo\nte\nbo\ngh\np \nre\ns\nf E\ndif\nn \nla\nod\nng\ng \nn \nth\nba\nor\nat\npo\nn a\nr \narn\nu \nab\nn\nK\nb\nre\nd \nuat\nge\no \nbe\nMI\nf \nK\nde\nan\npr\np\nwe\ne l\nm\npa\non\no\nern\noa\nht \nh\n h\nin\nE\nff\na\nab\nd \ng \na\nm\nh\nac\nr n\nta\nos\na \nti\nrdnie\nbe\nner\nKo\nbe\nea\n3\nte\nes\nb\nen\nIR\n3\nKo\ne\nn\nro\nop\ner\nla\nmay\nar\nng\no\nna\nar\nb\nha\nha\nnc\nEa\nfe\nan\nbe\na\no\nan\nmu\nem\nck\nno\nas\nse\nv\nim\nd-ke \nls\nrs\no-\ne-\nan\n3)\ned\ns, \ne \nn \nR. \n0 \no-\nd \nd \no-\np \nre \na-\ny \nre \ngs \nd \na-\nrd\nby\nas\nas\nce\nast\nr-\nnd\nls\ns-\nof\nnd\nu-\nm\nk-\not\net\ned\na-\nme\nk-s \ns \n-\n-\nn \n) \nd \n-\nd \ny \ns \ns \ne \nt \n-\nd \ns \n-\nf \nd \n-\nm \n-\nt \nt \nd \n-\ne \n- \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n385  \n \nof a significant amount of non-Western music, annotated \nby listeners from two distinct cultures, and labeled based on three music mood models. In MIREX, there have been \ntwo mood-related (sub)-tasks: Audio Mood Classification \n(AMC) starting from 2007 and the mood tag subtask in Audio Tag Classification (ATC) starting from 2009\n1. \nBoth tasks consist of Western songs labeled by listeners from unspecified cultural backgrounds [3]. This new da-taset will enable evaluation tasks that explore the cross-\ncultural generalizability of automated music mood recog-\nnition systems [17].  \n3. STUDY DESIGN \n3.1 The K-Pop Music Dataset \nThe dataset consists of 1,892 K-pop songs across seven \ndominant music genres in K-pop, namely Ballad, \nDance/Electronic, Folk, Hip-hop/Rap, Rock, R&B/Soul, and Trot [7]. 30 second music clips were extracted from each song and presented to the listeners for mood annota-tion. This was to mitigate the cognitive load of annotators and to minimize the effect of possible mood changes dur-\ning the entire duration of some songs (which can happen \nfor some songs but is beyond the scope of this study). \n3.2 Music Mood Models \nIn representing music mood, there are primarily two \nkinds of models: categorical and dimensional [5]. In cate-\ngorical models, music mood is represented as a set of dis-crete mood categories (e.g., happy, sad, calm, angry, etc.) and each song is assigned to one or more categories. This study adopted two categorical models used in MIREX: 1) \nthe five mood clusters (Table 1) used in the Audio Mood \nClassification task [3] where each song is labeled with one mood cluster exclusively; and 2) the 18 mood groups (Figure 2) used in the mood tag subtask in Audio Tag Classification where each song is labeled with up to six groups. Besides being used in MIREX, these two models \nwere chosen due to the fact that they were developed \nfrom empirical data of user judgments and in a way that is completely independent from any dimensional models, and thus they can provide a contrast to the latter.   \nUnlike categorical models, dimensional models repre-\nsent a “mood space” using a number of dimensions with continuous values. The most influential dimensional \nmodel in MIR is Russell’s 2-dimensional model [11], \nwhere the mood of each song is represented as a pair of numerical values indicating its degree in the Valence  (i.e., \nlevel of pleasure) and Arousal  (i.e., level of energy) di-\nmensions. Both categorical and dimensional models have their advantages and disadvantages. The former uses nat-\nural language terms and thus is considered more intuitive \nfor human users, whereas the latter can represent the de-gree of mood(s) a song may have (e.g., a little sad). Therefore, we used both kinds of models when annotating the mood of our K-pop song set. In addition to the 5 mood clusters and 18 mood groups, the K-pop songs \nwere also annotated with the Valence-Arousal 2-\ndimensional model. \n                                                           \n1 http://www.music-ir.org/mirex/wiki/MIREX_HOME Table 1.  Five mood clusters in the MIREX AMC task. \n3.3 Annotation Process \nFor a cross-cultural comparison, a number of American \nand Korean listeners were recruited to annotate the mood \nof the songs. The American listeners were recruited via a \nwell-known crowdsourcing platform, Amazon Mechani-cal Turk (MTurk), where workers complete tasks requir-ing human intelligence for a small fee. MTurk has been recognized as a quick and cost-effective way of collecting human opinions and has been used successfully in previ-\nous MIR studies (e.g., [6], [8]). In total, 134 listeners who \nidentified themselves as American participated in the an-notations based on the three mood models.  \nFor the five-mood cluster model, each “HIT” (Human \nIntelligence Task, the name for a task in MTurk) con-tained 22 clips with two duplicates for a consistency \ncheck. Answers were only accepted if the annotations on \nthe duplicate clips were the same. Participants were paid $2.00 for successfully completing each HIT. For the 18-group model, we paid $1.00 for each HIT, which con-tained 11 clips with one duplicate song for consistency check. There were fewer clips in each HIT of this model \nas the cognitive load was heavier: it asked for multiple \n(up to six) mood labels out of 18. For the Valence-Arousal (V-A) dimensional model we designed an inter-face with two slide scales in the range of [-10.0, 10.0] (Figure 1). We paid $1.00 for each HIT, which contained 11 clips with one duplicate song for a consistency check. \nConsistency was defined such that the difference between \nthe two annotations of the duplicate clips in either dimen-sion should be smaller than 2.0. The threshold was based on the findings in [16] where a number of listeners gave V-A values to the same songs in two different occasions and the differences never exceeded 10% of the entire \nrange. For each of the three mood representation models, \nthree annotations were collected for each music clip. The total cost was approximately $1800.  \nAs there was no known crowdsourcing platform for \nKorean people, the nine Korean listeners who participat-ed in the annotation were recruited through professional \nand personal networks of the authors. The annotation was \ndone with our in-house annotation systems, which are similar to those in MTurk. All instructions and mood la-bels/dimensions were translated into Korean to minimize possible misunderstanding of the terminology. Similarly, each song received three annotations in each mood mod-\nel. The payments to annotators were also comparable to \nthose in MTurk. Although the total number of annotators in the two cultural groups differs, each song had exactly Cluster1 \n(C_1) passionate, rousing, confident, boisterous,  \nrowdy \nCluster2 \n(C_2) rollicking, cheerful, fun, sweet,  \namiable/good natured \nCluster3 \n(C_3) literate, poignant, wistful, bittersweet,  \nautumnal, brooding \nCluster4 \n(C_4) humorous, silly, campy, quirky, whimsi-\ncal, witty, wry \nCluster5 \n(C_5) aggressive, fiery, tense/anxious, intense,  volatile, visceral \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n386  \n \nsix independent annotations on which the following anal-\nysis and comparisons are based. \n \nFigure 1 . Annotation interface of the VA model (hori-\nzontal dimension is Valence, vertical is Arousal). \n4. RESULTS \nThe annotations by American and Korean listeners are \ncompared in terms of judgment distribution, agreement \nlevels, and confusion between the two cultural groups. The Chi-square independence test is applied to estimate whether certain distributions were independent with lis-teners’ cultural background.  \n4.1 Distribution of Mood Judgment \nTable 2 shows the distribution of mood judgment of lis-\nteners from both cultural groups across five mood clus-ters. A Chi-square independence test indicates that the \ndistribution does depend on cultural group ( p < 0.001, df  \n= 4, /g548\n2=396.90). American listeners chose C_1 ( passion-\nate) and C_5 ( aggressive) more often while Korean lis-\nteners chose C_2 (cheerful ), C_3  (bittersweet ) and C_4 \n(silly/quirky ) more often. It is noteworthy that both \ngroups chose C_3 ( bittersweet ) most often among all five \nclusters. This is different from [9] where both American \nand Korean listeners chose C_2 ( cheerful ) most often for \nAmerican Pop songs. This difference may indicate that K-pop songs are generally more likely to express C_3 \nmoods than American Pop songs.  \n C_1 C_2 C_3 C_4 C_5 Total \nAmerican 1768 897 2225 311 475 5676 \nKorean 959 1321 2598 453 345 5676 \nTable 2. Judgment distributions across 5 mood clusters. \nWith the 18-mood group model, a listener may label a \nsong with up to six mood groups. The American listeners \nchose 13,521 groups in total, resulting in an average of \n2.38 groups per song. The Korean listeners chose 7,465 groups in total, which resulted in 1.32 groups per song. The fact that American listeners assigned almost twice as many groups to each song as Korean listeners did may be related to the individualism/collectivism dichotomy \nfound in psychology and cultural studies [13]; Americans \ntend to be individualistic and are more flexible in accept-ing a range of ideas (mood groups in this case) than peo-ple from collectivistic cultures (often represented by East \nAsian cultures). Future studies employing more qualita-\ntive approaches are warranted to verify this speculation. Figure 2 shows the distribution of judgments across the 18 mood groups. A chi-square test verified that the distri-bution is statistically significantly dependent on cultural \nbackgrounds ( p < 0.001, df = 17, /g548\n2=1664.49). Americans \nused “gleeful”, “romantic”, “brooding”, “earnest”, “hope-\nful”, and “dreamy” more often than Koreans, while Kore-ans applied “sad” more frequently than Americans. Both groups used “angry” and “anxious” very rarely, probably due to the nature of K-pop songs. Similar observations \nwere made in [17], where mood labels applied to Chinese \nand Western Pop songs were compared and radical moods such as “aggressive” and “anxious” were applied much more infrequently to Chinese songs than to West-ern songs. This may indicate a cultural difference in mu-sic: Chinese and Korean cultures tend to restrain and/or \ncensor the expression of radical or destructive feelings \nwhereas in Western cultures people are willing and free to expression of all kinds of feelings [10].  \n \nFigure 2 . Judgment distributions across 18 mood groups \n(each group is represented by one representative term). \n \nFigure 3 . Boxplot of Valence and Arousal values. \nFigure 3 shows the boxplot of the annotations based on \nthe VA dimensional space given by the two groups of lis-\nteners. The V-A scores given by Americans are more \nscattered than those by Koreans, suggesting that Ameri-cans were more willing to choose extreme values. In ad-dition, the means and medians indicate that Americans rated the songs with lower arousal values but higher va-lence values than Koreans ( p < 0.001 in non-paired t-test \nfor both cases). In other words, Americans tended to con-sider the songs to be less intense and more positive than did Koreans. This may also reflect the cultural difference \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n387  \n \nthat individuals from Western cultures tend to experience \nand/or express more positive emotions than those from Eastern cultures [12], and Asians present themselves as \nless aroused compared to Americans and Europeans [1].   \n4.2 Agreements Within and Across Cultural Groups \nIn order to find out whether listeners from the same cul-\ntural background agree more with each other than with \nthose from another cultural group, we examined the agreement among annotations provided by listeners in each cultural group as well as across cultural groups. The agreement measures used are the Sokal-Michener coeffi-cient and intra-class correlation (ICC). The former is ap-\npropriate for categorical data while the latter is used for \nnumerical data in the V-A space.  \n4.2.1 Sokal-Michener coefficient \nThe Sokal-Michener (S-M) coefficient is the ratio of the \nnumber of pairs with the same values and the total num-\nber of variables [2][9], and therefore a higher value indi-cates a higher agreement. For instance, if two listeners i \nand j had the same mood judgments on 189 of the 1892 \nsongs, the S-M coefficient between them is approximate-ly 0.1. Table 3 shows the average S-M coefficient aggre-\ngated across all pairs of annotators within and across cul-\ntural groups on the five-cluster annotations. It is not sur-prising that Koreans reached a higher agreement than Americans since they are annotating songs originating from their own culture. This is consistent with the find-ings in [2] and [9], where American listeners reached a \nhigher agreement on the mood of American Pop songs \nthan did Korean and Chinese listeners. The agreement level was the lowest when annotations from American and Korean listeners (cross-cultural) were paired up. The distribution of agreed vs. disagreed judgments is signifi-cantly dependent on whether the listeners are from the \nsame cultural group or not, evidenced by the Chi-square \ntest results (Table 3). Listeners from the same cultural group tend to agree more with each other than with those \nfrom a different culture.      \n American  Korean  /g548\n2 df P \nAmerican  0.47 0.43 25.35 1 <0.001 \nKorean 0.43 0.56 249.71 1 <0.001 \n Table 3.  S-M coefficients of the five-cluster annotation \nwithin and across cultural groups \nThe analysis is more complex for the 18 group annota-\ntion, as each judgment can associate multiple labels with \na song. To measure the agreement, we paired up labels applied to a song by any two annotators, and then calcu-\nlated the S-M coefficient as the proportion of matched \npairs among all pairs. For example, if annotator_1 la-belled a song S with g1, g2, g3 and annotator_2 labelled it with g1, g4, then there were six annotation pairs and only one of them matched (i.e., g1 matched g1). The S-M coefficient in this case is 1/6 = 0.17. Although the de-\nnominator increases when more labels are chosen, the \nchances they get matched also increase. All annotations from all listeners within each cultural group and across cultural groups were paired up in this way, and the result-ant S-M coefficients are shown in Table 4. Again, the agreement level within Koreans was higher than that within Americans and also across cultural groups. How-\never, the agreement within Americans was at the same \nlevel as the cross-cultural agreement, which is further ev-idenced by the statistically insignificant result of the Chi-\nsquare test.      \n American  Korean  /g548\n2 df p \nAmerican  0.11 0.11 3.72 1 0.054 \nKorean 0.11 0.15   156.88 1 <0.001 \nTable 4.  S-M coefficient of the 18-group annotation with-\nin and across cultural groups \n4.2.2 Intra-Class Correlation \nThe intra-class correlation (ICC) is a measure of agree-\nment when ratings are given based on a continuous scale [15]. In the case of V-A annotation in this study, there is a different set of raters (listeners) for each item (song), \nand thus the one-way random model is used to calculate \nICC within each group (3 raters) and across both groups (6 raters), for the valence and arousal dimensions. As shown in Table 5, cross-cultural agreement on valence is lower than within-cultural ones. Unlike five mood cluster annotation, both groups showed similar level of agree-\nment on both dimensions. It is also noteworthy that the \nagreement on arousal annotation is much higher than va-lence annotation within- and cross-culturally. This is con-\nsistent with earlier MIR literature where valence has been \nrecognized as more subjective than arousal [5]. \n American  Korean  Cross-Cultural \nValence  0.27 0.28 0.23 \nArousal 0.55 0.54 0.54 \nTable 5.  ICC of Valence Arousal annotations within and \nacross cultural groups \n4.3 Confusion Between Cultural Groups \nTo further our understanding on the difference and simi-\nlarity of mood perceptions between the two cultural groups, we also examined the disagreement between lis-teners in the two groups in each of the three types of an-\nnotations. For the 5-cluster annotation, Table 6 shows the \nconfusion matrix of the 1,438 songs with agreed labels by at least two listeners in each cultural group. Each cell shows the number of songs labeled as one mood cluster by Koreans (column) and another by Americans (row). The cells on the (highlighted) diagonal are numbers of \nsongs agreed by the two groups, while other cells repre-\nsent the disagreement between the two groups. The ma-trix shows that both groups agreed more on C_3 ( bitter-\nsweet ) within themselves (661 and 842 songs respectively \nas shown by the “Total” cells). The bold numbers indi-cate major disagreements between the two groups. There \nare 268 songs Korean listeners judged as C_3 ( bitter-\nsweet ) that Americans judged as C_1 ( passionate). The \ntwo groups only agreed on C_5 ( aggressive ) on 18 songs, \nwhereas 49 songs judged as C_5 ( aggressive) by Ameri-\ncans were judged by the Koreans as C_1 ( passionate ).  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n388  \n \nTable 7 shows the confusion matrix of the seven mood \ngroups (due to space limit) with the most agreed songs by \nmajority vote among the Korean listeners. The biggest \nconfusion/discrepancy is between “exciting” and “glee-\nful”: 135 songs perceived as “gleeful” by Americans were perceived as “exciting” by Koreans. Other major confusions are between “exciting” and “cheerful”, and “sad” and “mournful.” These moods have similar seman-tics in terms of valence (both “sad” and “mournful” have \nlow valence values) and arousal (both “exciting” and \n“gleeful” have high arousal values), which may explain the confusion between these terms. Similarly, there are few songs with disagreement between mood labels with very distinct semantics, such as “exciting” vs. “sad/calm/mournful”; “calm” vs. “cheerful/gleeful”; and \n“gleeful” vs. “mournful”.  \n           KO  \nAM C_1 C_2 C_3 C_4 C_5 Total \nC_1 70\u0003 79\u0003 268\u0003 18\u0003 22\u0003 457\u0003\nC_2 41\u0003 126\u0003 10\u0003 11\u0003 2\u0003 190\u0003\nC_3 19\u0003 53\u0003 558\u0003 22\u0003 9\u0003 661\u0003\nC_4 10\u0003 6\u0003 5\u0003 22\u0003 1\u0003 44\u0003\nC_5 49\u0003 10\u0003 1\u0003 8\u0003 18\u0003 86\u0003\nTotal 189 \u0003274\u0003842\u0003 81\u0003 52\u00031438\u0003\nTable 6.  Cross-tabulation between 5-cluster annotations \nacross cultural groups \nIt is interesting to see that a number of songs perceived \nas “romantic” by Americans were seen as “sad” (31 songs) \nand “calm” (30 songs) by Koreans. On the other hand, 18 songs perceived as “romantic” by Koreans were viewed as “calm” by Americans. “Romantic” was seldom con-fused with other high arousal moods such as “exciting” or \n“cheerful” by either Koreans or Americans, suggesting \nthat both cultures tend to associate “romantic” with low arousal music. \n        KO \nAM exci-\nting sad\u0003chee-\nrful\u0003calm\u0003mour-\nnful\u0003glee-\nful\u0003roma-\ntic\u0003Total \nexciting 71 2 35 2 2 28 3 143 \nsad 0 32 0 13 13 0 4 62 \ncheerful 35 3 32 1 3 7 2 83 \ncalm 0 10 0 25 4 0 18 57 \nmournful 0 48 0 23 27 0 6 104 \ngleeful 135 4 98 2 2 55 4 300 \nromantic 4 31 3 30 18 3 27 116 \ntotal 245 130 168 96 69 93 64 865 \nTable 7.  Cross-tabulation between 18-group annotations \nacross cultural groups \nFor the 2-D annotation, we show the disagreement be-\ntween the two groups in the four quadrants of the 2-D space (Table 8). Both groups agreed more with listeners from their own cultural group on the first quadrant \n(+A+V) and the third quadrant (-A-V) (as shown by the \n“Total” cells). The largest discrepancy was observed be-tween –A+V and –A-V: 116 songs were perceived as having negative arousal and positive valence (-A+V) by Americans but negative valence (-A-V) by Koreans. Sim-ilarly, for the songs perceived as having positive arousal \nby both groups, 118 of them were again perceived as hav-\ning positive valence (+A+V) by Americans but negative valence (+A-V) by Koreans. This is consistent with our finding that Korean listeners are more likely to label neg-\native moods than Americans (Section 4.1). \n      KO \nAM +A+V +A-V -A+V -A-V Total \n+A+V 495 118 25 34 672 \n+A-V 8 30 1 17 56 \n-A+V 51 24 84 116 275 \n-A-V 10 19 80 178 287 \nTotal 565 191 190 346 1290 \nTable 8.  Cross-tabulation among the four quadrants in 2-\nD annotations across cultural groups \n5. DISCUSSIONS \n5.1 Differences and Similarities Between Groups \nThe results show that mood judgments and the level of \nagreement are dependent on the cultural background of the listeners. A number of differences were found be-\ntween the annotations of the two groups. First, Americans \nassigned a larger number of labels to each song, and ap-\nplied more extreme valence and arousal values than Ko-\nreans (Figure 3). We speculate that perhaps this is related to the fact that the Western culture tends to encourage in-dividualism and divergent thinking more than the Eastern culture [13]. The difference in the number of annotators is another possible explanation. Both of these factors will \nbe further explored in future work. Second, compared to \nAmericans, Koreans were more likely to label songs with negative moods such as “bittersweet”, “sad,” and “mournful” (Table 2, Figure 2), give lower valence val-ues (Figure 3), and agree with each other more often on songs with negative valence (Table 9). These observa-\ntions were consistent with and supported by findings in \nprevious cultural studies that people from Western cul-tures tend to experience and/or express more positive emotions than those from Eastern cultures [12]. The fact that Americans in this study could not understand the lyr-ics of the songs may also have contributed to these results. \nSometimes song lyrics and melody may express different \nmoods to invoke complex emotions (e.g., dark humor). In particular, a recent trend among K-pop artists to use fast-er tempo in Ballad songs may make the melody sound positive or neutral, although the lyrics are sad or melan-choly as is the convention for Ballad songs.    \nIt is also found that agreements of within-cultural \ngroups are higher than that of cross-cultural groups based on the comparison of S-M coefficient, and ICC values (on valence only). For within-cultural group agreement, Koreans reached a higher agreement than Americans on 5-cluster annotation, which may be explained by the fact \nthat Koreans were more familiar with the K-pop songs \nused in this study than Americans. Prior familiarity with \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n389  \n \nsongs was also identified as a factor affecting the agree-\nment level of mood perception in previous studies [2].  \nSome similarities were also found between the annota-\ntions of the two groups: 1) both groups applied and agreed on C_3 ( bittersweet ) more often than other mood \nclusters (Tables 2 and 8); 2) both groups seldom applied radical mood labels such as “aggressive”, “angry”, “anx-ious” (Table 2 and Figure 2); and 3) both groups agreed more on songs with +A+V and –A-V values (Table 9). \nThese similarities can potentially be attributed to the na-\nture of the K-pop songs. A previous study comparing mood labels on Western and Chinese Pop songs also found that there were significantly fewer radical mood labels assigned to Chinese Pop songs than to Western songs [17]. This may reflect Eastern Asian preferences \nfor non-aggressive music, perhaps due to their tradition of \nbeing more conservative and limiting the expression of feelings [10]. Another likely explanation would be the censorship and regulation\n1 that still heavily affects the \npopular music culture in countries like South Korea and China.  \n5.2 Proposed MIR Evaluation Tasks \nOne of the main contributions of this study is to build a \nlarge cross-cultural dataset for MIR research. The unique \ncharacteristics of the dataset built for this study make it \nsuitable for various evaluation tasks involving cross-cultural components. Specifically, for each of the three annotation sets (i.e., 5-clusters, 18-groups, and 2-dimenions), both within- and cross-cultural evaluations can be performed. For the former, both training and test data can be extracted from the datasets with annotations \nby listeners from the same cultural group (by cross-\nvalidation, for example); for the latter, models can be trained by the dataset annotated by listeners in one culture and applied to the dataset annotated by listeners in anoth-er culture. These tasks will be able to evaluate whether mood recognition models often used in Western music \ncan be equally applied to 1) non-Western music, specifi-\ncally K-Pop songs; 2) K-Pop songs annotated by Ameri-can and/or Korean listeners; and 3) cross-cultural music mood recognition, for both categorical mood classifica-tion [17] and dimensional mood regression [5].         \n6. CONCLUSIONS AND FUTURE WORK \nThis study analyzed music mood annotations on a large set of K-Pop songs provided by listeners from two dis-tinct cultural groups, Americans and Koreans, using three \nmood annotation models. By comparing annotations from \nthe two cultural groups, differences and similarities were identified and discussed. The unique characteristics of the dataset built in this study will allow it to be used in future MIR evaluation tasks with an emphasis on cross-cultural applicability of mood recognition algorithms and sys-\ntems. Future work will include detailed and qualitative \ninvestigation on the reasons behind the differences be-tween mood judgments of these two user groups as well as listeners from other cultural groups.    \n                                                           \n1 http://freemuse.org/archives/7294  7. ACKNOWLEGEMENT \nFunding support: Korean Ministry of Trade, Industry and \nEnergy (Grant #100372) & the A.W. Mellon Foundation. \n8. REFERENCES \n[1] E. Y. Bann and J. J. Bryson: “Measuring cultural \nrelativity of emotional valence and arousal using semantic clustering and twitter,” Proc. of the Annual \nConf. of the Cognitive Sci. Soc.,  pp.1809-1814, 2013. \n[2] X. Hu & J. H. Lee: “A cross-cultural study of music mood perception between American and Chinese listeners,” ISMIR, pp.535-540. 2012. \n[3] X. Hu, J. S. Downie, C. Laurier, M., Bay and A. Ehmann: “The 2007 MIREX audio mood classification task: Lessons learned,” ISMIR , 2008. \n[4] P. N. Juslin and P. Laukka, P.: “Expression, perception, \nand induction of musical emotions: a review and a questionnaire study of everyday listening,” J. New \nMusic Research , vol. 33, no. 3, pp. 217–238, 2004. \n[5] Y. E. Kim, E. M. Schmidt, R. Migenco, B. G. Morton, \nP. Richardson, J. J. Scott, J. A. Speck, and D. Turnbull: “Music emotion recognition: A state of the art review,”  ISMIR , pp. 255–266, 2010. \n[6] J. H. Lee: “Crowdsourcing music similarity judgments \nusing Mechanical Turk,” ISMIR , 2010. \n[7] J. H. Lee, K. Choi, X. Hu and J. S. Downie: “K-Pop \ngenres: A cross-cultural exploration,” ISMIR , 2013. \n[8] J. H. Lee and X. Hu: “Generating ground truth for \nmood classification in Music Digital Libraries Using Mechanical Turk,” IEEE-ACM  JCDL , 2012. \n[9] J. H. Lee and X. Hu: “Cross-cultural similarities and differences in music mood perception,” Proceedings of \nthe iConference, 2014. \n[10] R. R. McCrae, P. T. Costa, and M. Yik: “Universal aspects of Chinese personality structure,” in M. H. Bond (Ed.) The Handbook of Chinese Psychology , pp. \n189–207. Hong Kong: Oxford University Press, 1996. \n[11] J. A. Russell: “A circumspect model of affect,” Journal \nof Psychology and Soc.\n Psychology , vol. 39, no. 6, 1980 \n[12] Y. Miyamoto and X. Ma: “Dampening or savoring positive emotions: A dialectical cultural script guides emotion regulation.” Emotion  11.6, pp.1346-347, 2011. \n[13] J. J. Schmidt, T. Facca and J. C. Soper: “International \nvariations in divergent creativity and the impact on \nteaching entrepreneurship,” Journal of Higher Ed. \nTheory & Practice . Vol. 13 Issue 2, pp. 101-109. 2013. \n[14] X. Serra: “A multicultural approach in music information research,” ISMIR, 2011. \n[15] P. E. Shrout and J. L. Fleiss: “Intraclass correlations: \nUses in assessing rater reliability,” Psychological \nBulletin, Vol. 86, pp. 420-3428, 1979. \n[16] J. A. Speck, E. M. Schmidt, B. G. Morton and Y. E. \nKim: “A comparative study of collaborative vs. traditional annotation methods,” ISMIR, 2011. \n[17] Y.-H. Yang and X. Hu: “Cross-cultural music mood \nclassification: A comparison on English and Chinese \nsongs,” ISMIR, 2012. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n390"
    },
    {
        "title": "Music Information Behaviors and System Preferences of University Students in Hong Kong.",
        "author": [
            "Xiao Hu 0001",
            "Jin Ha Lee 0001",
            "Leanne Ka Yan Wong"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414778",
        "url": "https://doi.org/10.5281/zenodo.1414778",
        "ee": "https://zenodo.org/records/1414778/files/HuLW14.pdf",
        "abstract": "This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supple- mental interviews were conducted in order to investigate users’ music information related activities. We found that university students in Hong Kong listened to music fre- quently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple",
        "zenodo_id": 1414778,
        "dblp_key": "conf/ismir/HuLW14",
        "keywords": [
            "quantitative",
            "qualitative",
            "user study",
            "music information needs",
            "university students",
            "Hong Kong",
            "music information related activities",
            "listening to music",
            "entertainment",
            "stress reduction"
        ],
        "content": "MUSIC INFORMATION BEHAVIORS AND SYSTEM PREFERENCES \nOF\n UNIVERSITY STUDENTS IN HONG KONG \nXiao Hu Ji n Ha Lee Leanne Ka Yan Wong \nUniversity of Hong Kong \nxiaoxhu@hku.hk  U\nniversity of Washington \njinhalee@uw.edu  U\nniversity of Hong Kong \nwkayan@connect.hku.hk  \nABSTRACT \nT\nhis paper presents a user study on music information \nneeds and behaviors of university students in Hong Kong. \nA mix of quantitative and qualitative methods was used. \nA survey was completed by 101 participants and supple-\nmental interviews were conducted in order to investigate \nusers’ music information related activities. We found that \nuniversity students in Hong Kong listened to music fre-\nquently and mainly for the purposes of entertainment, \nsinging and playing instruments, and stress reduction. \nThis user group often searches for music with multiple \nmethods, but common access points like genre and time \nperiod were rarely used. Sharing music with people in \ntheir online social networks such as Facebook and Weibo \nwas a common activity. Furthermore, the popularity of \nsmartphones prompted the need for streaming music and \nmobile music applications. We also examined users’ \npreferences on music services available in Hong Kong \nsuch as YouTube and KKBox, as well as the characteris-\ntics liked and disliked by the users. The results not only \noffer insights into non-Western users’ music behaviors \nbut also for designing online music services for young \nmusic listeners in Hong Kong. \n1. INTRODUCTION AND RELATED WORK \nSeeking music and music information is prevalent in our \neveryday life as music is an indispensable element for \nmany people [1]. People in Hong Kong are not an excep-\ntion. Hong Kong has the second highest penetration rate \nof broadband Internet access in Asia, following South \nKorea1. Consequently, Hong Kongers are increasingly \nu\nsing various online music information services to seek \nand listen to music, including iTunes, YouTube, Kugou, \nSogou and Baidu2. However, our current understanding \no\nf their music information needs and behaviors are still \nlacking, as few studies explored user populations in Hong \nKong, or in any non-Western cultures.  \nHong Kong is a unique location that merges the West-ern and Eastern cultures. Before the handover to the Chi-\nnese government in 1997, Hong Kong had been ruled by \nthe British government for 100 years. This had resulted in \na heavy influence of Western culture, although much of \nthe Chinese cultural heritage has also been preserved well \nin Hong Kong. The cultural influences of Hong Kong to \nthe neighboring regions in Asia were significant, espe-\ncially in the pre-handover era. In fact, in the 80s and \nthroughout the 90s, Cantopop (Cantonese popular music, \nsometimes referred to as HK-pop) was widely popular \nacross many Asian countries, and produced many influ-\nential artists such as Leslie Cheung, Anita Mui, Andy \nLau, and so on [2]. In the post-handover era, there has \nbeen an influx of cultural products from mainland China \nwhich is significantly affecting the popular culture of \nHong Kong [8]. The cultural history and influences of \nHong Kong, especially paired with the significance of \nCantopop, makes it an interesting candidate to explore \namong many non-Western cultures.  \nOf the populations in Hong Kong, we specifically \nwanted to investigate young adults on their music infor-\nmation needs and behaviors. They represent a vibrant \npopulation who are not only heavily exposed to and fast \nadopters of new ideas, but also represent the future work-\nforce and consumers. University students in Hong Kong \nare mostly digital natives (i.e., grew up with access to \ncomputers and the Internet from an early age) with rich \nexperience of seeking and listening to digital music. Ad-\nditionally the fact that they are influenced by both West-\nern and Eastern cultures, and exposed to both global and \nlocal music make them worthy of exploring as a particu-\nlar group of music users.1   2  \nT\nhere have been a few related studies which investi-\ngated music information users in Hong Kong. Lai and \nChan [5] surveyed information needs of users in an aca-\ndemic music library setting. They found that the frequen-\ncies of using score and multimedia were higher than us-\ning electronic journal databases, books, and online jour-\nnals. Nettamo et al. [9] compared users in New York City \nand those in Hong Kong in using their mobile devices for \nmusic-related tasks. Their results showed that users’ envi-\n                                                           \n1 http://www.itu.int/ITU-D/ICTEYE/Reporting/Dynamic ReportWiz-\na\nrd.aspx \n2 http://hk.epochtimes.com/b5/11/10/20/145162.htm   © Xiao Hu, Jin Ha Lee, Leanne Ka Yan Wong. \nLicensed under a Creative Commons Attribution 4.0 Internationa l \nLicense (CC BY 4.0). Attribution:  Xiao Hu, Jin Ha Lee, Leanne Ka \nYan Wong. “Music Information Behaviors and System Preferences of \nUniversity Students in Hong Kong”, 15th International Society for \nMusic Information Retrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n579  \n \nr\nonment and context greatly influenced their behaviors, \nand there were cultural differences in consuming and \nmanaging mobile music between the two user groups. \nOur study investigates everyday music information be-\nhaviors of university students in Hong Kong, and thus the \nscope is broader than these studies.  \nI\nn addition to music information needs and behaviors, \nthis study also examines the characteristics of popular \nmusic services adopted by university students in Hong \nKong, in order to investigate their strengths and weak-\nnesses. Recommendations for designing music services \nare proposed based on the results. This study will im-\nprove our understanding on music information behaviors \nof the target population and contribute to the design of \nmusic services that can better serve target users. \n2. METHODS  \nA mix of quantitative and qualitative methods was used \nin order to triangulate our results. We conducted a survey \nin order to collect general information about target users’ \nmusic information needs, seeking behaviors, and opinions \non commonly used music services. Afterwards, follow-up \nface-to-face interviews of a smaller user group were con-\nducted to collect in-depth explanations on the themes and \npatterns discovered in the survey results. Prior to the for-\nmal survey and interviews, pilot tests were carried out \nwith a smaller group of university students to ensure that \nthe questions were well-constructed and students were \nable to understand and answer them without major issues. \n2.1 Survey \nThe survey was conducted as an online questionnaire. \nThe questionnaire instrument was adapted from the one \nused in [6] and [7], with modifications to fit the multilin-\ngual and multicultural environment. Seventeen questions \nabout the use of popular music services were added to the \nquestionnaire. The survey was implemented with Lime-\nSurvey, an open-source survey application, and consisted \nof five parts: demographic information, music preference, \nmusic seeking behaviors, music collection management, \nand opinions on preferred music services. Completing the \nsurvey took approximately 30 minutes, and each partici-\npant was offered a chance to enter his/her name for a raf-\nfle to win one of the three supermarket gift coupons of \nHKD50, if they wished.  \nThe target population was students (both undergradu-\nate and graduate) from the eight universities sponsored by \nthe government of Hong Kong Special Administrative \nRegion. The sample was recruited using Facebook due to \nits popularity among university students in Hong Kong. \nSurvey invitations were posted on Facebook, initially \nthrough the list of friends of the authors, and then further \ndisseminated by chain-referrals.  \n2.2 Interviews \nSemi-structured interviews were conducted after the sur-\nvey data were collected and analyzed, in order to seek in-depth explanations to support the survey findings. Face-\nt\no-face interviews were carried out individually with five \nparticipants from three different universities. The inter-\nviews were conducted in Cantonese, the mother tongue of \nthe interviewees, and were later transcribed and translated \nto English. Each interview lasted up to approximately 20 \nminutes.  \n3. SURVEY DATA ANALYSIS \nOf the 167 survey responses collected, 101 complete re-\nsponses were analyzed in this study. All the survey par-\nticipants were university students in Hong Kong. Among \nthem, 58.4% of were female and 41.6% of them were \nmale. They were all born between 1988 and 1994, and \nmost of them (88.1%) were born between 1989 and 1992. \nTherefore, they were in their early 20s when the survey \nwas taken in 2013. Nearly all of them (98.0%) were un-\ndergraduates majoring Science/Engineering (43.6%), So-\ncial Sciences/Humanities (54.0%) and Other (2.0%). \n3.1 Music Preferences  \nIn order to find out participants’ preferred music genres, \nthey were asked to select and rank up to five of their fa-\nvorite music genres from a list of 25 genres covering \nmost Western music genres. To ensure that the partici-\npants understand the different genres, titles and artist \nnames of example songs representative of each genre \nwere provided. The results are shown in Table 1 where \neach cell represents the number of times each genre was \nmentioned with the rank corresponding in the column. \nPop was the most preferred genre among the participants, \nfollowed by R&B/Soul and Rock. We also aggregated the \nresults by assigning reversely proportional weights to the \nranks (1st: 5 points, and 5th: 1 point). The most popular \nmusic genres among the participants were Pop (311 pts), \nR&B/Soul (204 pts), Rock (109 pts), Gospel (88 pts) and \nJazz (86 pts).  \n 1st 2nd 3rd 4th 5th Total Total (%)  \nPop 43 14 9 4 5 75 74.2% \nR&B 7 29 11 7 6 60 59.4% \nRock 6 9 10 3 7 35 34.7% \nGospel 9 6 2 4 5 26 25.7% \nJazz 6 8 3 5 5 27 26.7% \nTable 1. Preferences on music genres  \nMoreover, as both Chinese and English are official \nlanguages of Hong Kong, participants were also asked to \nrank their preferences on languages of lyrics. The five \noptions were English, Cantonese, Mandarin, Japanese and \nKorean. The last three were included due to popularity of \nsongs from nearby countries/regions in Hong Kong, in-\ncluding mainland China and Taiwan (Mandarin), Japan \n(Japanese), and Korea (Korean). As shown in Table 2, \nEnglish was in fact highly preferred, followed by Canton-\nese. Mandarin was mostly ranked at the second or third \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n580  \n \np\nlace, while Korean and Japanese were ranked lower. We \nalso aggregated the answers and found that the most pop-\nular languages in songs are English (394 points), Canton-\nese (296 points), and Mandarin (223 points).  \n 1st 2nd 3rd 4th 5th Total Total (%)  \nEnglish 46 27 16 3 2 94 93.1% \nCantonese 31 20 15 7 2 75 74.3% \nMandarin 13 23 20 2 2 60 59.4% \nKorean 6 15 6 16 14 57 56.4% \nJapanese 5 5 10 16 16 52 51.5% \nTable 2. Preferences on languages of song lyrics \n3.2 Music Seeking Behaviors \nWhen asked about the type of music information they \nhave ever searched, most participants indicated prefer-\nences on audio: MP3s and music videos (98.0%), music \nrecordings (e.g., CDs, vinyl records, tapes) (94.1%), and \nmusic multimedia in other formats (e.g., Blue-ray, DVD, \nVHS) (88.1%). Written forms of music information were \nsought by fewer respondents: books on music (73.2%), \nmusic magazines (69.3%), and academic music journals \n(63.4%). Approximately one out of three participants \neven responded that they have never sought music maga-\nzines (30.7%) or academic music journals (36.6%).  \nAs for the frequency of search, 41.6% of respondents \nindicated that they sought MP3s and music videos at least \na few times a week, compared to only 18.8% for music \nrecordings (e.g., CDs, vinyl records, tapes) and 24.8% for \nmusic multimedia in other formats (e.g., Blue-ray, DVD).  \nMoreover, 98.0% of participants responded that they \nhad searched for music information on the Internet. \nAmong them, almost all (99.0%) answered that they had \ndownloaded free music online, and 95.0% responded that \nthey had listened to streaming music or online radio. This \nclearly indicates that participants sought digital music \nmore often through online channels than offline or physi-\ncal materials. However, even though 77.8% of respond-\nents had visited online music store, only 69.7% of them \nhad purchased any electronic music files or albums. Not \nsurprisingly, participants preferred free music resources. \nMusic was certainly a popular element of entertain-\nment in the lives of the participants. When asked why \nthey sought music, all participants included entertainment \nin their answers. Also, a large proportion (83.0%) indi-\ncated that they sought music for entertainment at least a \nfew times a week. Furthermore, 97.0% of respondents \nsearch for music information for singing or playing a mu-\nsical instrument for fun. This proportion is significantly \nhigher than the results from the previous survey of uni-\nversity population in the United States (32.8% for singing \nand 31.9% for playing a musical instrument) [6]. In addi-\ntion, 78.2% of our respondents do this at least two or \nthree times a month. We conjecture that this is most like-\nly due to the popularity of karaoke in Hong Kong. Known-item search was the most common type of music \ninformation seeking; nearly all respondents (95.1%) \nsought music information for the identifica-\ntion/verification of musical works, artist and lyrics, and \nabout half of them do so at least a few times a week. Ob-\ntaining background information was also a strong reason; \nover 90% of the participants sought music to learn more \nabout music artists (97.0%) as well as music (94.1%), and \napproximately half of them (53.5% and 40.6%, respec-\ntively) sought this kind of music information at least two \nor three times a month.  \nWhen asked which sources stimulated or influenced \ntheir music information needs, all 101 participants \nacknowledged online video clips (e.g. YouTube) and TV \nshows/movies. This suggests that the influence of other \nmedia using music is quite significant which echoes the \nfinding that associative metadata in music seeking was \nimportant for the university population in the United \nStates [6]. Also over 70% of the participants’ music \nneeds were influenced by music heard in public places, \nadvertisement/commercial, radio show, or family mem-\nbers’/friends’ home.  \nAs for the metadata used in searching for music, per-\nformer was the most popular access point with 80.2% of \npositive responses, followed by the title of work(s) \n(65.3%) and some words of lyrics (62.4%). Other com-\nmon types of metadata such as genre and time period \nwere only used by a few respondents (33.7% and 29.7%, \nrespectively). Particularly for genre, the proportion is sig-\nnificantly lower than 62.7% as found in the prior survey \nof university population in the United States [6]. This is \nperhaps related to the exposure to different music genres \nin Hong Kong, and the phenomenon that Hong Kongers \nmusic listeners tend to emphasize an affinity with friends \nwhile Americans (New Yorkers) are more likely to use \nmusic to highlight their individual personalities [9]. \nMoreover, participants responded that they would also \nseek music based on other users’ opinions: 57.4% by rec-\nommendations from other people and 52.5% by populari-\nty. The proportion for popularity is also fairly larger than \nthe 31% in [6]. This shows that the social aspect is a cru-\ncial factor affecting participants’ music seeking behaviors. \nOf the different types of people, friends and family \nmembers (91.1%) and people on their social network \nwebsites (e.g. Facebook, Weibo) (89.1%) were the ones \nwhom they most likely ask for help when searching for \nmusic. In addition, they turned to the Internet more fre-\nquently than friends and family members. Thirty-nine \npercent of them sought help on social network websites at \nleast a few times a week while only 23.8% turned to \nfriends/family members at least a few times a week. \nOn the other hand, when asked which physical places \nthey go to in order to search for music or music infor-\nmation, 82.18% said that they would find music in family \nmembers’ or friends’ home, which was higher than going \nto record stores (75.3%), libraries (70.3%), and academic \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n581  \n \ni\nnstitutions (64.4%). Overall, these data show that users’ \nsocial networks, and especially online networks are im-\nportant for their music searching process.  \n3.3 Music Collection Management  \nMore participants were managing a digital collection \n(40.6%) than a physical one (25.7%). On average, each \nrespondent estimated that he/she managed 900 pieces of \ndigital music and 94 pieces of music in physical formats. \nThis shows that managing digital music is more popular \namong participants, although the units that they typically \nassociate with digital versus physical items might differ \n(e.g., digital file vs. physical album).  \n We also found that students tended to manage their \nmusic collections with simple methods. Over half of the \nrespondents (50.0% for music in physical formats and \n56.1% for digital music) manage their music collection \nby artist name. Participants sometimes also organized \ntheir digital collections by album title (17.7%), but rarely \nby format type (3.9%) and never by record label. More \nparticipants indicated they did not organize their music at \nall for their physical music collection (19.2%) than their \ndigital music collection (2.4%). When they did organize \ntheir physical music collection, they would use album ti-\ntle (11.5%) and genre (11.5%). Overall, organizing the \ncollection did not seem to be one of the users’ primary \nactivities related to music information.  \n3.4  Preferred Music Services  \nRespondents gave a variety of responses regarding their \nmost frequently visited music services: YouTube (51.5%), \nKKBox (26.7%), and iTunes (14.9%) were the most pop-\nular ones. KKBox is a large cloud-based music service \nprovider founded in Taiwan, very popular in the region \nand sometimes referred to as “Asian Spotify.” YouTube, \nwhich provides free online streaming music video, was \nalmost twice as popular as the second most favored music \nservice, KKBox. The popularity of YouTube was also \nobserved in Lee and Waterman’s survey of 520 music \nusers in 2012 [7]. Their respondents ranked Pandora as \nthe most preferred service, followed by YouTube as the \nsecond.  \nThe participants were also asked to evaluate their fa-\nvorite music services. Specifically, they were asked to \nindicate their level of satisfaction using a 5-point Likert \nscale on 15 different aspects on search function, search \nresults and system utility. Table 3 shows the percentage \nof positive (aggregation of “somewhat satisfied” and \n“very satisfied) and negative (aggregation of “somewhat \nunsatisfied” and “very unsatisfied”) ratings among users \nwho chose each of the three services as their most fa-\nvored one. \nFor those who selected YouTube as their most fre-\nquently used service, they indicated that they were espe-\ncially satisfied with its keyword search function (74.5%), \nrecommendation of keywords (70.6%), variety of availa-\nble music information (60.8%) and attractive interface (56.9%). Only a few respondents (9.8%) were unsatisfied \nwith certain features of YouTube such as advanced \nsearch, relevance of search results, and navigation. It is \nsurprising to see that five respondents rated YouTube \nnegatively on the aspect of price. We suspect they might \nhave associated this aspect with the price of purchasing \ndigital music from certain music channels on YouTube, \nor the indirect cost of having to watch ads. However, we \ndid not have the means to identify these respondents to \nverify the reasons behind their ratings.    \n Yo uTube KKBox iTunes \nP N P N P N search \nfunction keyword search 74.5  7.8 29.6 7.4 13.3 0.0 \nadvanced search 54.9  9.8 44.4 18.5 46.7 6.7 \ncontent-based search 51.0  7.8 44.4 29.6 66.7 13.3 \nauto-correction 49.0  7.8 29.6 29.6 20.0 33.3 \nkeywords suggestion 70.6  3.9 40.7 25.9 20.0 53.3 search results number of results 52.9  7.8 40.7 22.2 6.7 33.3 \nrelevance 47.1 9.8 48.1 18.5 13.3 33.3 \naccuracy  49.0 7.8 44.4 18.5 33.3 26.7 utility price of the service 39.2  9.8 25.9 25.9 33.3 20.0 \naccessibility 52.9  7.8 22.2 37.0 26.7 20.0 \nnavigation 52.9 9.8 18.5 29.6 6.7 20.0 \nvariety of available \nmusic information 60.8 7.8 22.2 22.2 26.7 13.3 \nmusic recommendation  52.9 7.8 33.3 22.2 53.3 20.0 \ninterface attractiveness  56.9 3.9 33.3 7.4 40.0 20.0 \nmusic sharing 47.1  3.9 40.7 7.4 40.0 20.0 \nTable 3 User ratings of three most preferred music \nservices (“P”: positive; “N”: negative, in percentage)  \nThe level of satisfaction for KKBox was lower than \nthat of YouTube. Nearly half of the participants who use \nKKBox were satisfied with its relevance of results \n(48.1%), advanced search function (44.4%) and content-\nbased search function (44.4%). The aspects of KKBox \nthat participants did not like included the lack of accessi-\nbility (37.0%), content-based search function (29.6%), \nand auto-correction (29.6%). Interestingly, the content-\nbased search function in KKBox was controversial \namong the participants. Some participants liked it proba-\nbly because it was a novel feature that few music services \nhad; while others were not satisfied with it, perhaps due \nto fact that current performance of audio content-based \ntechnologies have yet to meet users’ expectation.  \nOnly 15 participants rated iTunes as their most fre-\nquently used music service. Their opinions on iTunes \nwere mixed. Its content-based search function and music \nrecommendations were valued by 66.7% and 53.3% of \nthe 15 participants, respectively. The data seem to sug-\ngest that audio content-based technologies in iTunes per-\nformed better than KKBox, but this must be verified with \na larger sample in future work. On the other hand, over \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n582  \n \nh\nalf of the respondents gave negative response to the \nkeyword suggestion function in iTunes. Moreover, the \nauto-correction, number of search results, and relevance \nof search results also received negative responses by one \nthird of the respondents. These functions are related to \nthe content of music collection in iTunes, and thus we \nsuspect that the coverage of iTunes perhaps did not meet \nthe expectations of young listeners in Hong Kong, as \nmuch as the other two services did.  \n4. THEMES/TRENDS FROM INTERVIEWS \n4.1 Multiple Music Information Searching Strategies \nInterviewees searched for music using not only music \nservices like YouTube or KKBox, but also general-\npurpose search engines, such as Google and Yahoo!. \nMost often, a simple keyword search with the song title \nor artist name was conducted when locating music in \nthese music services. However, more complicated \nsearches such as those using lyrics and the name of com-\nposer are not supported by most existing music services. \nIn this case, search engines had to be used. For example, \nif the desired song title and artist name are unknown or \ninaccurate, interviewees would search for them on \nGoogle or Yahoo! with any information they know about \nthe song. The search often directed them to the right piece \nof metadata which then allowed them to conduct a search \nin YouTube or other music services. As expected, this \ndoes not always lead to successful results; one participant \nsaid “when I did not know the song title or artist name, I \ntried singing the song to Google voice search, but the re-\nsult was not satisfactory.”   \n4.2 Use of Online Social Networks \nOnline social network services are increasingly popular \namong people in Hong Kong. According to an online \nsurvey conducted with 387 Hong Kong residents in \nMarch 20113, the majority of the respondents visited Fa-\nc\nebook (92%), read blogs (77%) and even wrote blog \nposts (52%). Social media provides a convenient way for \npeople to connect with in Hong Kong where maintaining \na work-life balance can be quite challenging.  \nUniversity students in Hong Kong are also avid social \nmedia users. They prefer communicating and sharing in-\nformation with others using online social networks for the \nefficiency and flexibility. Naturally, it also serves as a \nconvenient channel for sharing music recommendations \nand discussing music-related topics. Relying on others \nwas considered an important way to search for music: \n“Normally, I will consider others’ opinions first. There \nare just way too many songs, so it helps find good music \nmuch more easily.”,  “I love other people’s comments, es-\npecially when they have the same view as me!” \n                                                           \n3 Hong Kong social media use higher than United States: \nh\nttp://travel.cnn.com/hong-kong/life/hong-kong-social-media-use-\nhigher-united-states-520745.  4.3 2 4/7 Online Music Listening \nParticipants in this study preferred listening to or watch-\ning streaming music services rather than downloading \nmusic. Downloading an mp3 file of a song usually takes \nabout a half minute with a broadband connection and \nslightly longer with a wireless connection. Interviewees \ncommented that downloading just added an extra step \nwhich was inconvenient to them. \nApart from the web, smart mobile devices are becom-\ning ubiquitous which is also affecting people’s mode of \nmusic listening. According to Mobilezine4, 87% of Hong \nK\nongers aged between 15 and 64 own a smart device. \nAccording to Phneah [10], 55% of Hong Kong youths \nthink that the use of smartphones dominates their lives as \nthey are unable to stop using smartphones even in re-\nstrooms, and many sleep next to it. As expected, universi-\nty students in Hong Kong are accustomed to having 24/7 \naccess to streaming music on their smartphones.  \n5. IMPLICATIONS FOR MUSIC SERVICES \n5.1 Advanced Search  \nA simple keyword search may not be sufficient to ac-\ncommodate users who want to search for music with var-\nious metadata, not only with song titles, but also per-\nformer’s names, lyrics, and so on. For example, if a user \nwants to locate songs with the word “lotus” in the lyrics, \nthey would simply use “lotus” as the search keyword. \nHowever, the search functions in various music services \ngenerally are not intelligent enough to understand the se-\nmantic differences among the band named Lotus and the \nword “lotus” in lyrics, not to mention which role the band \nLotus might have played (e.g., performer, composer, or \nboth). As a result, users have to conduct preliminary \nsearches in web search engines as an extra step when at-\ntempting to locate the desired song. Many users will ap-\npreciate having an advanced search function with specific \nfields in music services that allow them to conduct lyric \nsearch with “lotus” rather than a general keyword search.  \n5.2 Mood Search \nParticipants showed great interests in the feeling or emo-\ntion in music, as they perceived the meaning of songs \nwere mostly about particular emotions. Terms such as \n“positive”, “optimistic”, and “touching” were used to de-\nscribe the meaning of music during the interviews . There-\nfo\nre, music services that can support searching by mood \nterms may be useful.  \nMusic emotion or mood has been recognized as an im-\nportant access point for music [3]. A cross-cultural study \nby Hu and Lee [4] points out that listeners from different \ncultural backgrounds have different music mood judg-\nments and they tend to agree more with users from the \n                                                           \n4 Hong Kong has the second highest smartphone penetration in the \nw\norld: http://mobilezine.asia/2013/01/hong-kong-has-the-second-\nhighest-smartphone-penetration-in-the-world/.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n583  \n \ns\name cultural background than users from other cultures. \nThis cultural difference must be taken into account when \nestablishing mood metadata for music services.  \n5.3 Connection with Social Media \nSocial media play a significant role in sharing and dis-\ncussing music among university students in Hong Kong. \nYouTube makes it easy for people to share videos in var-\nious online social communities such as Facebook, Twitter \nand Google Plus. Furthermore, users can view the shared \nYouTube videos directly on Facebook which makes it \neven more convenient. This is one of the key reasons our \nparticipants preferred YouTube. However, music services \nlike iTunes have yet to adopt this strategy. For our study \npopulation, linking social network to music services \nwould certainly enhance user experience and help pro-\nmote music as well.  \n5.4 Smartphone Application \nMany participants are listening to streaming music with \ntheir smartphones, and thus naturally, offering music apps \nfor smart devices will be critical for music services. Both \nYouTube and iTunes offer smartphone apps. Moreover, \ninstant messaging applications, such as WhatsApp, is \nfound as the most common reason for using smartphones \namong Hong Kongers [10]. To further improve the user \nexperience, music-related smartphone apps may consider \nincorporating online instant messaging capabilities.   \n6. CONCLUSION \nMusic is essential for many university students in Hong \nKong. They listen to music frequently for the purpose of \nentertainment and relaxation, to help reduce stress in their \nextremely tense daily lives. Currently, there does not ex-\nist a single music service that can fulfill all or most of \ntheir music information needs, and thus they often use \nmultiple tools for specific searches. Furthermore, sharing \nand acquiring music from friends and acquaintances was \na key activity, mainly done on online social networks. \nComparing our findings to those of previous studies re-\nvealed some cultural differences between Hong Kongers \nand Americans, such as Hong Kongers relying more on \npopularity and significantly less on genres in music \nsearch.  \nWith the prevalence of smartphones, students are in-\ncreasingly becoming “demanding” as they get accus-\ntomed to accessing music anytime and anywhere. Stream-\ning music and music apps for smartphones are becoming \nincreasingly common. The most popular music service \namong university students in Hong Kong was YouTube \ndue to its convenience, user-friendly interface, and requir-\ning no payment to use their service. In order to further \nimprove the design of music services, we recommended \nproviding an advanced search function, emotion/mood-\nbased search, social network connection, smartphone \napps as well as access to high quality digital music which \nwill help fulfill users’ needs. 7. ACKNOWLEDGEMENT \nThe study was partially supported by a seed basic re-\nsearch project in University of Hong Kong. The authors \nextend special thanks to Patrick Ho Ming Chan for assist-\ning in data collection.  \n8. REFERENCES  \n[1] M. A. Casey, R. Veltkamp, M. Goto, M. Leman, C.  \nRhodes, and M. Slaney: “Content-Based Music \nInformation Retrieval: Current Directions and Future \nChallenges,” Proceedings of the IEEE,  96 (4), pp.  \n668-696, 2008.  \n[2] S. Y. Chow: “Before and after the Fall: Mapping \nHong Kong Cantopop in the Global Era,” LEWI \nWorking Paper Series , 63, 2007. \n[3] X. Hu: “Music and mood: Where theory and reality \nmeet,” Proceedings of iConference . 2010. \n[4] X. Hu and J. H. Lee: “A Cross-cultural Study of \nMusic Mood Perception between American and \nChinese Listeners,”  Proceedings  of the ISMIR, \npp.535-540, 2012. \n[\n5] K. Lai and K. Chan: “Do you know your music \nusers' needs? A library user survey that helps \nenhance a user-centered music collection.” The \nJournal of Academic Librarianship , 36(1), pp.63-69, \n2010. \n[6] J. H. Lee and S. J. Downie: “Survey of music \ninformation needs, uses, and seeking behaviours: \nPreliminary findings,” Proceedings of the ISMIR , pp. \n441-446, 2004. \n[7] J. H. Lee and M. N. Waterman: “Understanding user \nrequirements for music information services,” \nProceedings of the ISMIR , pp. 253-258, 2012. \n[8] B. T. McIntyre, C. C. W. Sum, and Z. Weiyu: \n“Cantopop: The voice of Hong Kong,” Journal of \nAsian Pacific Communication , 12 (2), pp. 217-243, \n2002. \n[9] E. Nettamo, M. Norhamo, and J. Häkkilä: “A cross-\ncultural study of mobile music: Retrieval, \nmanagement and consumption,” Proceedings of \nOzCHI 2006 , pp. 87-94, 2006. \n[10] J. Phneah: “Worrying signals as smartphone \naddiction soars,” The Standard.  Retrieved from \nhttp://www. \nthestandard.com.hk/news_detail.asp?pp_cat=30&art\n_id=132763&sid=39444767&con_type=1, 2013.  \n[11] V. M. Steelman: “Intraoperative music therapy: \nEffects on anxiety, blood pressure,” Association of \nOperating Room Nurses Journal,  52(5), pp. 1026-\n1034, 1990. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n584"
    },
    {
        "title": "Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks.",
        "author": [
            "Po-Sen Huang",
            "Minje Kim",
            "Mark Hasegawa-Johnson",
            "Paris Smaragdis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415678",
        "url": "https://doi.org/10.5281/zenodo.1415678",
        "ee": "https://zenodo.org/records/1415678/files/HuangKHS14.pdf",
        "abstract": "Monaural source separation is important for many real world applications. It is challenging since only single chan- nel information is available. In this paper, we explore us- ing deep recurrent neural networks for singing voice sep- aration from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal con- nections are explored. We propose jointly optimizing the networks for multiple source signals by including the sepa- ration step as a nonlinear operation in the last layer. Differ- ent discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30∼2.48 dB GNSDR gain and 4.32∼5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset.",
        "zenodo_id": 1415678,
        "dblp_key": "conf/ismir/HuangKHS14",
        "keywords": [
            "monaural source separation",
            "real world applications",
            "challenging",
            "deep recurrent neural networks",
            "supervised setting",
            "singing voice separation",
            "single channel information",
            "joint optimization",
            "source to interference ratio",
            "state-of-the-art performance"
        ],
        "content": "SINGING-VOICE SEPARATION FROM MONAURAL RECORDINGS\nUSING DEEP RECURRENT NEURAL NETWORKS\nPo-Sen Huangy, Minje Kimz, Mark Hasegawa-Johnsony, Paris Smaragdisyzx\nyDepartment of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA\nzDepartment of Computer Science, University of Illinois at Urbana-Champaign, USA\nxAdobe Research, USA\nfhuang146, minje, jhasegaw, parisg@illinois.edu\nABSTRACT\nMonaural source separation is important for many real\nworld applications. It is challenging since only single chan-\nnel information is available. In this paper, we explore us-\ning deep recurrent neural networks for singing voice sep-\naration from monaural recordings in a supervised setting.\nDeep recurrent neural networks with different temporal con-\nnections are explored. We propose jointly optimizing the\nnetworks for multiple source signals by including the sepa-\nration step as a nonlinear operation in the last layer. Differ-\nent discriminative training objectives are further explored\nto enhance the source to interference ratio. Our proposed\nsystem achieves the state-of-the-art performance, 2.30\u00182.48\ndB GNSDR gain and 4.32\u00185.42 dB GSIR gain compared\nto previous models, on the MIR-1K dataset.\n1. INTRODUCTION\nMonaural source separation is important for several real-\nworld applications. For example, the accuracy of auto-\nmatic speech recognition (ASR) can be improved by sep-\narating noise from speech signals [10]. The accuracy of\nchord recognition and pitch estimation can be improved by\nseparating singing voice from music [7]. However, current\nstate-of-the-art results are still far behind human capabil-\nity. The problem of monaural source separation is even\nmore challenging since only single channel information is\navailable.\nIn this paper, we focus on singing voice separation from\nmonaural recordings. Recently, several approaches have\nbeen proposed to utilize the assumption of the low rank\nand sparsity of the music and speech signals, respectively\n[7, 13, 16, 17]. However, this strong assumption may not\nalways be true. For example, the drum sounds may lie in\nthe sparse subspace instead of being low rank. In addition,\nall these models can be viewed as linear transformations in\nthe spectral domain.\nc\rPo-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nParis Smaragdis.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Po-Sen Huang, Minje Kim, Mark\nHasegawa-Johnson, Paris Smaragdis. “Singing-V oice Separation From\nMonaural Recordings Using Deep Recurrent Neural Networks”, 15th In-\nternational Society for Music Information Retrieval Conference, 2014.\nSTFTMixture \nSignalDNN /DRNNMagnitude \nSpectra\nPhase \nSpectra\nDiscriminative \nTrainingEvaluation ISTFTTime Frequency \nMaskingJoint Discriminative Training\nEstimated \nMagnitude SpectraFigure 1. Proposed framework.\nWith the recent development of deep learning, with-\nout imposing additional constraints, we can further extend\nthe model expressibility by using multiple nonlinear layers\nand learn the optimal hidden representations from data. In\nthis paper, we explore the use of deep recurrent neural net-\nworks for singing voice separation from monaural record-\nings in a supervised setting. We explore different deep re-\ncurrent neural network architectures along with the joint\noptimization of the network and a soft masking function.\nMoreover, different training objectives are explored to op-\ntimize the networks. The proposed framework is shown in\nFigure 1.\nThe organization of this paper is as follows: Section 2\ndiscusses the relation to previous work. Section 3 intro-\nduces the proposed methods, including the deep recurrent\nneural networks, joint optimization of deep learning mod-\nels and a soft time-frequency masking function, and differ-\nent training objectives. Section 4 presents the experimental\nsetting and results using the MIR-1K dateset. We conclude\nthe paper in Section 5.\n2. RELATION TO PREVIOUS WORK\nSeveral previous approaches utilize the constraints of low\nrank and sparsity of the music and speech signals, respec-\ntively, for singing voice separation tasks [7, 13, 16, 17].\nSuch strong assumption for the signals might not always\nbe true. Furthermore, in the separation stage, these models\ncan be viewed as a single-layer linear network, predicting\nthe clean spectra via a linear transform. To further improve\nthe expressibility of these linear models, in this paper, we\nuse deep learning models to learn the representations from\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n477timeL-layer DRNN\n...\n...\n...\n...\n......\n...\n...\n...\n...\ntime1-layer RNN\ntimeL-layer sRNN\n...\n...\n...\n...\n...\n1lL\n12L\n1Figure 2. Deep Recurrent Neural Networks (DRNNs) architectures: Arrows represent connection matrices. Black, white,\nand grey circles represent input frames, hidden states, and output frames, respectively. (Left): standard recurrent neural\nnetworks; (Middle): Lintermediate layer DRNN with recurrent connection at the l-th layer. (Right): Lintermediate layer\nDRNN with recurrent connections at all levels (called stacked RNN).\ndata, without enforcing low rank and sparsity constraints.\nBy exploring deep architectures, deep learning approaches\nare able to discover the hidden structures and features at\ndifferent levels of abstraction from data [5]. Deep learn-\ning methods have been applied to a variety of applications\nand yielded many state of the art results [2,4,8]. Recently,\ndeep learning techniques have been applied to related tasks\nsuch as speech enhancement and ideal binary mask estima-\ntion [1, 9–11, 15].\nIn the ideal binary mask estimation task, Narayanan and\nWang [11] and Wang and Wang [15] proposed a two-stage\nframework using deep neural networks. In the ﬁrst stage,\nthe authors use dneural networks to predict each output\ndimension separately, where dis the target feature dimen-\nsion; in the second stage, a classiﬁer (one layer perceptron\nor an SVM) is used for reﬁning the prediction given the\noutput from the ﬁrst stage. However, the proposed frame-\nwork is not scalable when the output dimension is high.\nFor example, if we want to use spectra as targets, we would\nhave 513 dimensions for a 1024-point FFT. It is less de-\nsirable to train such large number of neural networks. In\naddition, there are many redundancies between the neural\nnetworks in neighboring frequencies. In our approach, we\npropose a general framework that can jointly predict all\nfeature dimensions at the same time using one neural net-\nwork. Furthermore, since the outputs of the prediction are\noften smoothed out by time-frequency masking functions,\nwe explore jointly training the masking function with the\nnetworks.\nMaas et al. proposed using a deep RNN for robust auto-\nmatic speech recognition tasks [10]. Given a noisy signal\nx, the authors apply a DRNN to learn the clean speech y.\nIn the source separation scenario, we found that modeling\none target source in the denoising framework is subopti-\nmal compared to the framework that models all sources. In\naddition, we can use the information and constraints from\ndifferent prediction outputs to further perform masking and\ndiscriminative training.3. PROPOSED METHODS\n3.1 Deep Recurrent Neural Networks\nTo capture the contextual information among audio sig-\nnals, one way is to concatenate neighboring features to-\ngether as input features to the deep neural network. How-\never, the number of parameters increases rapidly according\nto the input dimension. Hence, the size of the concatenat-\ning window is limited. A recurrent neural network (RNN)\ncan be considered as a DNN with indeﬁnitely many lay-\ners, which introduce the memory from previous time steps.\nThe potential weakness for RNNs is that RNNs lack hier-\narchical processing of the input at the current time step. To\nfurther provide the hierarchical information through multi-\nple time scales, deep recurrent neural networks (DRNNs)\nare explored [3, 12]. DRNNs can be explored in different\nschemes as shown in Figure 2. The left of Figure 2 is a\nstandard RNN, folded out in time. The middle of Figure\n2 is anLintermediate layer DRNN with temporal connec-\ntion at thel-th layer. The right of Figure 2 is an Linterme-\ndiate layer DRNN with full temporal connections (called\nstacked RNN (sRNN) in [12]).\nFormally, we can deﬁne different schemes of DRNNs as\nfollows. Suppose there is an Lintermediate layer DRNN\nwith the recurrent connection at the l-th layer, the l-th hid-\nden activation at time tis deﬁned as:\nhl\nt=fh(xt;hl\nt\u00001)\n=\u001el\u0000\nUlhl\nt\u00001+Wl\u001el\u00001\u0000\nWl\u00001\u0000\n:::\u001e 1\u0000\nW1xt\u0001\u0001\u0001\u0001\n;\n(1)\nand the output, yt, can be deﬁned as:\nyt=fo(hl\nt)\n=WL\u001eL\u00001\u0000\nWL\u00001\u0000\n:::\u001e l\u0000\nWlhl\nt\u0001\u0001\u0001\n; (2)\nwhere xtis the input to the network at time t,\u001elis an\nelement-wise nonlinear function, Wlis the weight matrix\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n478for thel-th layer, and Ulis the weight matrix for the re-\ncurrent connection at the l-th layer. The output layer is a\nlinear layer.\nThe stacked RNNs have multiple levels of transition\nfunctions, deﬁned as:\nhl\nt=fh(hl\u00001\nt;hl\nt\u00001)\n=\u001el(Ulhl\nt\u00001+Wlhl\u00001\nt); (3)\nwhere hl\ntis the hidden state of the l-th layer at time t.Ul\nandWlare the weight matrices for the hidden activation at\ntimet\u00001and the lower level activation hl\u00001\nt, respectively.\nWhenl= 1, the hidden activation is computed using h0\nt=\nxt.\nFunction\u001el(\u0001)is a nonlinear function, and we empir-\nically found that using the rectiﬁed linear unit f(x) =\nmax(0; x)[2] performs better compared to using a sig-\nmoid or tanh function. For a DNN, the temporal weight\nmatrix Ulis a zero matrix.\n3.2 Model Architecture\nAt timet, the training input, xt, of the network is the con-\ncatenation of features from a mixture within a window. We\nuse magnitude spectra as features in this paper. The out-\nput targets, y1tandy2t, and output predictions, ^ y1tand\n^ y2t, of the network are the magnitude spectra of different\nsources.\nSince our goal is to separate one of the sources from a\nmixture, instead of learning one of the sources as the tar-\nget, we adapt the framework from [9] to model all different\nsources simultaneously. Figure 3 shows an example of the\narchitecture.\nMoreover, we ﬁnd it useful to further smooth the source\nseparation results with a time-frequency masking technique,\nfor example, binary time-frequency masking or soft time-\nfrequency masking [7, 9]. The time-frequency masking\nfunction enforces the constraint that the sum of the pre-\ndiction results is equal to the original mixture.\nGiven the input features, xt, from the mixture, we ob-\ntain the output predictions ^ y1tand^ y2tthrough the net-\nwork. The soft time-frequency mask mtis deﬁned as fol-\nlows:\nmt(f) =j^ y1t(f)j\nj^ y1t(f)j+j^ y2t(f)j; (4)\nwheref2f1;:::;Fgrepresents different frequencies.\nOnce a time-frequency mask mtis computed, it is ap-\nplied to the magnitude spectra ztof the mixture signals to\nobtain the estimated separation spectra ^ s1tand^ s2t, which\ncorrespond to sources 1 and 2, as follows:\n^ s1t(f) =mt(f)zt(f)\n^ s2t(f) = (1\u0000mt(f))zt(f);(5)\nwheref2f1;:::;Fgrepresents different frequencies.\nThe time-frequency masking function can be viewed as\na layer in the neural network as well. Instead of training the\nnetwork and applying the time-frequency masking to the\nresults separately, we can jointly train the deep learning\nmodels with the time-frequency masking functions. We\nInput LayerHidden LayersSource 1 Source 2\nOutput\nxtht1y1t\nht3y1t y2t\nht+1zt zt\nht2\nht-1y2tFigure 3. Proposed neural network architecture.\nadd an extra layer to the original output of the neural net-\nwork as follows:\n~ y1t=j^ y1tj\nj^ y1tj+j^ y2tj\fzt\n~ y2t=j^ y2tj\nj^ y1tj+j^ y2tj\fzt;(6)\nwhere the operator \fis the element-wise multiplication\n(Hadamard product). In this way, we can integrate the\nconstraints to the network and optimize the network with\nthe masking function jointly. Note that although this extra\nlayer is a deterministic layer, the network weights are op-\ntimized for the error metric between and among ~ y1t,~ y2t\nandy1t,y2t, using back-propagation. To further smooth\nthe predictions, we can apply masking functions to ~ y1tand\n~ y2t, as in Eqs. (4) and (5), to get the estimated separation\nspectra ~ s1tand~ s2t. The time domain signals are recon-\nstructed based on the inverse short time Fourier transform\n(ISTFT) of the estimated magnitude spectra along with the\noriginal mixture phase spectra.\n3.3 Training Objectives\nGiven the output predictions ^ y1tand^ y2t(or~ y1tand~ y2t)\nof the original sources y1tandy2t, we explore optimizing\nneural network parameters by minimizing the squared er-\nror and the generalized Kullback-Leibler (KL) divergence\ncriteria, as follows:\nJMSE =jj^ y1t\u0000y1tjj2\n2+jj^ y2t\u0000y2tjj2\n2 (7)\nand\nJKL=D(y1tjj^ y1t) +D(y2tjj^ y2t); (8)\nwhere the measure D(AjjB )is deﬁned as:\nD(AjjB ) =X\ni\u0012\nAilogAi\nBi\u0000Ai+Bi\u0013\n: (9)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n479D(\u0001k\u0001) reduces to the KL divergence whenP\niAi=P\niBi=\n1;so thatAandBcan be regarded as probability distribu-\ntions.\nFurthermore, minimizing Eqs. (7) and (8) is for increas-\ning the similarity between the predictions and the targets.\nSince one of the goals in source separation problems is to\nhave high signal to interference ratio (SIR), we explore dis-\ncriminative objective functions that not only increase the\nsimilarity between the prediction and its target, but also\ndecrease the similarity between the prediction and the tar-\ngets of other sources, as follows:\njj^ y1t\u0000y1tjj2\n2\u0000\rjj^ y1t\u0000y2tjj2\n2+jj^ y2t\u0000y2tjj2\n2\u0000\rjj^ y2t\u0000y1tjj2\n2\n(10)\nand\nD(y1tjj^ y1t)\u0000\rD (y1tjj^ y2t)+D(y2tjj^ y2t)\u0000\rD (y2tjj^ y1t);\n(11)\nwhere\ris a constant chosen by the performance on the\ndevelopment set.\n4. EXPERIMENTS\n4.1 Setting\nOur system is evaluated using the MIR-1K dataset [6].1A\nthousand song clips are encoded with a sample rate of 16\nKHz, with durations from 4 to 13 seconds. The clips were\nextracted from 110 Chinese karaoke songs performed by\nboth male and female amateurs. There are manual annota-\ntions of the pitch contours, lyrics, indices and types for un-\nvoiced frames, and the indices of the vocal and non-vocal\nframes. Note that each clip contains the singing voice\nand the background music in different channels. Only the\nsinging voice and background music are used in our exper-\niments.\nFollowing the evaluation framework in [13, 17], we use\n175 clips sung by one male and one female singer (‘ab-\njones’ and ‘amy’) as the training and development set.2\nThe remaining 825 clips of 17 singers are used for testing.\nFor each clip, we mixed the singing voice and the back-\nground music with equal energy (i.e. 0 dB SNR). The goal\nis to separate the singing voice from the background music.\nTo quantitatively evaluate source separation results, we\nuse Source to Interference Ratio (SIR), Source to Arti-\nfacts Ratio (SAR), and Source to Distortion Ratio (SDR)\nby BSS-EV AL 3.0 metrics [14]. The Normalized SDR\n(NSDR) is deﬁned as:\nNSDR( ^v;v;x) = SDR( ^v;v)\u0000SDR(x; v); (12)\nwhere ^vis the resynthesized singing voice, vis the orig-\ninal clean singing voice, and xis the mixture. NSDR is\nfor estimating the improvement of the SDR between the\npreprocessed mixture xand the separated singing voice\n^v. We report the overall performance via Global NSDR\n1https://sites.google.com/site/unvoicedsoundseparation/mir-1k\n2Four clips, abjones 508, abjones 509, amy 908, amy 909, are\nused as the development set for adjusting hyper-parameters.(GNSDR), Global SIR (GSIR), and Global SAR (GSAR),\nwhich are the weighted means of the NSDRs, SIRs, SARs,\nrespectively, over all test clips weighted by their length.\nHigher values of SDR, SAR, and SIR represent better sep-\naration quality. The suppression of the interfering source is\nreﬂected in SIR. The artifacts introduced by the separation\nprocess are reﬂected in SAR. The overall performance is\nreﬂected in SDR.\nFor training the network, in order to increase the va-\nriety of training samples, we circularly shift (in the time\ndomain) the singing voice signals and mix them with the\nbackground music.\nIn the experiments, we use magnitude spectra as input\nfeatures to the neural network. The spectral representation\nis extracted using a 1024-point short time Fourier trans-\nform (STFT) with 50% overlap. Empirically, we found\nthat using log-mel ﬁlterbank features or log power spec-\ntrum provide worse performance.\nFor our proposed neural networks, we optimize our mod-\nels by back-propagating the gradients with respect to the\ntraining objectives. The limited-memory Broyden-Fletcher-\nGoldfarb-Shanno (L-BFGS) algorithm is used to train the\nmodels from random initialization. We set the maximum\nepoch to 400 and select the best model according to the\ndevelopment set. The sound examples and more details of\nthis work are available online.3\n4.2 Experimental Results\nIn this section, we compare different deep learning models\nfrom several aspects, including the effect of different in-\nput context sizes, the effect of different circular shift steps,\nthe effect of different output formats, the effect of different\ndeep recurrent neural network structures, and the effect of\nthe discriminative training objectives.\nFor simplicity, unless mentioned explicitly, we report\nthe results using 3 hidden layers of 1000 hidden units neu-\nral networks with the mean squared error criterion, joint\nmasking training, and 10K samples as the circular shift\nstep size using features with a context window size of 3\nframes. We denote the DRNN-k as the DRNN with the re-\ncurrent connection at the k-th hidden layer. We select the\nmodels based on the GNSDR results on the development\nset.\nFirst, we explore the case of using single frame features,\nand the cases of concatenating neighboring 1 and 2 frames\nas features (context window sizes 1, 3, and 5, respectively).\nTable 1 reports the results using DNNs with context win-\ndow sizes 1;3, and 5. We can observe that concatenating\nneighboring 1 frame provides better results compared with\nthe other cases. Hence, we ﬁx the context window size to\nbe 3 in the following experiments.\nTable 2 shows the difference between different circular\nshift step sizes for deep neural networks. We explore the\ncases without circular shift and the circular shift with a step\nsize off50K, 25K, 10Kg samples. We can observe that\nthe separation performance improves when the number of\ntraining samples increases (i.e. the step size of circular\n3https://sites.google.com/site/deeplearningsourceseparation/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n480Model (context window size) GNSDR GSIR GSAR\nDNN (1) 6.63 10.81 9.77\nDNN (3) 6.93 10.99 10.15\nDNN (5) 6.84 10.80 10.18\nTable 1. Results with input features concatenated from\ndifferent context window sizes.\nModelGNSDR GSIR GSAR(circular shift step size)\nDNN (no shift) 6.30 9.97 9.99\nDNN (50,000) 6.62 10.46 10.07\nDNN (25,000) 6.86 11.01 10.00\nDNN (10,000) 6.93 10.99 10.15\nTable 2. Results with different circular shift step sizes.\nModel (num. of outputGNSDR GSIR GSARsources, joint mask)\nDNN (1, no) 5.64 8.87 9.73\nDNN (2, no) 6.44 9.08 11.26\nDNN (2, yes) 6.93 10.99 10.15\nTable 3. Deep neural network output layer comparison\nusing single source as a target and using two sources as\ntargets (with and without joint mask training). In the “joint\nmask” training, the network training objective is computed\nafter time-frequency masking.\nshift decreases). Since the improvement is relatively small\nwhen we further increase the number of training samples,\nwe ﬁx the circular shift size to be 10K samples.\nTable 3 presents the results with different output layer\nformats. We compare using single source as a target (row\n1) and using two sources as targets in the output layer (row\n2 and row 3). We observe that modeling two sources simul-\ntaneously provides better performance. Comparing row 2\nand row 3 in Table 3, we observe that using the joint mask\ntraining further improves the results.\nTable 4 presents the results of different deep recurrent\nneural network architectures (DNN, DRNN with different\nrecurrent connections, and sRNN) and the results of dif-\nferent objective functions. We can observe that the models\nwith the generalized KL divergence provide higher GSARs,\nbut lower GSIRs, compared to the models with the mean\nsquared error objective. Both objective functions provide\nsimilar GNSDRs. For different network architectures, we\ncan observe that DRNN with recurrent connection at the\nsecond hidden layer provides the best results. In addition,\nall the DRNN models achieve better results compared to\nDNN models by utilizing temporal information.\nTable 5 presents the results of different deep recurrent\nneural network architectures (DNN, DRNN with differ-\nent recurrent connections, and sRNN) with and without\ndiscriminative training. We can observe that discrimina-\ntive training improves GSIR, but decreases GSAR. Over-\nall, GNSDR is slightly improved.Model (objective) GNSDR GSIR GSAR\nDNN (MSE) 6.93 10.99 10.15\nDRNN-1 (MSE) 7.11 11.74 9.93\nDRNN-2 (MSE) 7.27 11.98 9.99\nDRNN-3 (MSE) 7.14 11.48 10.15\nsRNN (MSE) 7.09 11.72 9.88\nDNN (KL) 7.06 11.34 10.07\nDRNN-1 (KL) 7.09 11.48 10.05\nDRNN-2 (KL) 7.27 11.35 10.47\nDRNN-3 (KL) 7.10 11.14 10.34\nsRNN (KL) 7.16 11.50 10.11\nTable 4. The results of different architectures and different\nobjective functions. The “MSE” denotes the mean squared\nerror and the “KL” denotes the generalized KL divergence\ncriterion.\nModel GNSDR GSIR GSAR\nDNN 6.93 10.99 10.15\nDRNN-1 7.11 11.74 9.93\nDRNN-2 7.27 11.98 9.99\nDRNN-3 7.14 11.48 10.15\nsRNN 7.09 11.72 9.88\nDNN + discrim 7.09 12.11 9.67\nDRNN-1 + discrim 7.21 12.76 9.56\nDRNN-2 + discrim 7.45 13.08 9.68\nDRNN-3 + discrim 7.09 11.69 10.00\nsRNN + discrim 7.15 12.79 9.39\nTable 5. The comparison for the effect of discriminative\ntraining using different architectures. The “discrim” de-\nnotes the models with discriminative training.\nFinally, we compare our best results with other previous\nwork under the same setting. Table 6 shows the results\nwith unsupervised and supervised settings. Our proposed\nmodels achieve 2.30\u00182.48 dB GNSDR gain, 4.32\u00185.42\ndB GSIR gain with similar GSAR performance, compared\nwith the RNMF model [13]. An example of the separation\nresults is shown in Figure 4.\n5. CONCLUSION AND FUTURE WORK\nIn this paper, we explore using deep learning models for\nsinging voice separation from monaural recordings. Specif-\nically, we explore different deep learning architectures, in-\ncluding deep neural networks and deep recurrent neural\nnetworks. We further enhance the results by jointly op-\ntimizing a soft mask function with the networks and ex-\nploring the discriminative training criteria. Overall, our\nproposed models achieve 2.30\u00182.48 dB GNSDR gain and\n4.32\u00185.42 dB GSIR gain, compared to the previous pro-\nposed methods, while maintaining similar GSARs. Our\nproposed models can also be applied to many other appli-\ncations such as main melody extraction.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n481(a) Mixutre\n (b) Clean vocal\n (c) Recovered vocal\n (d) Clean music\n (e) Recovered music\nFigure 4. (a) The mixture (singing voice and music accompaniment) magnitude spectrogram (in log scale) for the clip\nAni101 in MIR-1K; (b) (d) The groundtruth spectrograms for the two sources; (c) (e) The separation results from our\nproposed model (DRNN-2 + discrim).\nUnsupervised\nModel GNSDR GSIR GSAR\nRPCA [7] 3.15 4.43 11.09\nRPCAh [16] 3.25 4.52 11.10\nRPCAh + FASST [16] 3.84 6.22 9.19\nSupervised\nModel GNSDR GSIR GSAR\nMLRR [17] 3.85 5.63 10.70\nRNMF [13] 4.97 7.66 10.03\nDRNN-2 7.27 11.98 9.99\nDRNN-2 + discrim 7.45 13.08 9.68\nTable 6. Comparison between our models and previous\nproposed approaches. The “discrim” denotes the models\nwith discriminative training.\n6. ACKNOWLEDGEMENT\nWe thank the authors in [13] for providing their trained\nmodel for comparison. This research was supported by\nU.S. ARL and ARO under grant number W911NF-09-1-\n0383. This work used the Extreme Science and Engineer-\ning Discovery Environment (XSEDE), which is supported\nby National Science Foundation grant number ACI-1053575.\n7. REFERENCES\n[1] N. Boulanger-Lewandowski, G. Mysore, and M. Hoffman.\nExploiting long-term temporal dependencies in NMF using\nrecurrent neural networks with application to source separa-\ntion. In IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2014.\n[2] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rectiﬁer\nneural networks. In JMLR W&CP: Proceedings of the Four-\nteenth International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS 2011), 2011.\n[3] M. Hermans and B. Schrauwen. Training and analysing deep\nrecurrent neural networks. In Advances in Neural Information\nProcessing Systems, pages 190–198, 2013.\n[4] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly,\nA. Senior, V . Vanhoucke, P. Nguyen, T. Sainath, and\nB. Kingsbury. Deep neural networks for acoustic modeling\nin speech recognition. IEEE Signal Processing Magazine,\n29:82–97, Nov. 2012.\n[5] G. Hinton and R. Salakhutdinov. Reducing the dimensional-\nity of data with neural networks. Science, 313(5786):504 –\n507, 2006.[6] C.-L. Hsu and J.-S.R. Jang. On the improvement of singing\nvoice separation for monaural recordings using the MIR-1K\ndataset. IEEE Transactions on Audio, Speech, and Language\nProcessing, 18(2):310 –319, Feb. 2010.\n[7] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-\nJohnson. Singing-voice separation from monaural recordings\nusing robust principal component analysis. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 57–60, 2012.\n[8] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck.\nLearning deep structured semantic models for web search us-\ning clickthrough data. In ACM International Conference on\nInformation and Knowledge Management (CIKM), 2013.\n[9] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and\nP. Smaragdis. Deep learning for monaural speech sepa-\nration. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2014.\n[10] A. L. Maas, Q. V Le, T. M O’Neil, O. Vinyals, P. Nguyen,\nand A. Y . Ng. Recurrent neural networks for noise reduction\nin robust ASR. In INTERSPEECH, 2012.\n[11] A. Narayanan and D. Wang. Ideal ratio mask estimation using\ndeep neural networks for robust speech recognition. In Pro-\nceedings of the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing. IEEE, 2013.\n[12] R. Pascanu, C. Gulcehre, K. Cho, and Y . Bengio. How to con-\nstruct deep recurrent neural networks. In International Con-\nference on Learning Representations, 2014.\n[13] P. Sprechmann, A. Bronstein, and G. Sapiro. Real-time on-\nline singing voice separation from monaural recordings using\nrobust low-rank modeling. In Proceedings of the 13th Inter-\nnational Society for Music Information Retrieval Conference,\n2012.\n[14] E. Vincent, R. Gribonval, and C. Fevotte. Performance mea-\nsurement in blind audio source separation. Audio, Speech,\nand Language Processing, IEEE Transactions on, 14(4):1462\n–1469, July 2006.\n[15] Y . Wang and D. Wang. Towards scaling up classiﬁcation-\nbased speech separation. IEEE Transactions on Audio,\nSpeech, and Language Processing, 21(7):1381–1390, 2013.\n[16] Y .-H. Yang. On sparse and low-rank matrix decomposition\nfor singing voice separation. In ACM Multimedia, 2012.\n[17] Y .-H. Yang. Low-rank representation of both singing voice\nand music accompaniment via learned dictionaries. In Pro-\nceedings of the 14th International Society for Music Informa-\ntion Retrieval Conference, November 4-8 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n482"
    },
    {
        "title": "JAMS: A JSON Annotated Music Specification for Reproducible MIR Research.",
        "author": [
            "Eric J. Humphrey",
            "Justin Salamon",
            "Oriol Nieto",
            "Jon Forsyth",
            "Rachel M. Bittner",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415924",
        "url": "https://doi.org/10.5281/zenodo.1415924",
        "ee": "https://zenodo.org/records/1415924/files/HumphreySNFBB14.pdf",
        "abstract": "The continued growth of MIR is motivating more com- plex annotation data, consisting of richer information, mul- tiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of address- ing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, com- prehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we pro- vide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and dis- cuss how now is a crucial time to make a concerted effort toward sustainable annotation standards.",
        "zenodo_id": 1415924,
        "dblp_key": "conf/ismir/HumphreySNFBB14",
        "keywords": [
            "MIR",
            "annotation data",
            "richer information",
            "multiple annotations",
            "music signal",
            "JSON-based format",
            "simpler",
            "structure",
            "sustainability",
            "future research"
        ],
        "content": "JAMS: A JSON ANNOTATED MUSIC SPECIFICATION FOR\nREPRODUCIBLE MIR RESEARCH\nEric J. Humphrey1,*, Justin Salamon1,2, Oriol Nieto1, Jon Forsyth1,\nRachel M. Bittner1, and Juan P. Bello1\n1Music and Audio Research Lab, New York University, New York\n2Center for Urban Science and Progress, New York University, New York\nABSTRACT\nThe continued growth of MIR is motivating more com-\nplex annotation data, consisting of richer information, mul-\ntiple annotations for a given task, and multiple tasks for\na given music signal. In this work, we propose JAMS, a\nJSON-based music annotation format capable of address-\ning the evolving research requirements of the community,\nbased on the three core principles of simplicity, structure\nand sustainability. It is designed to support existing data\nwhile encouraging the transition to more consistent, com-\nprehensive, well-documented annotations that are poised\nto be at the crux of future MIR research. Finally, we pro-\nvide a formal schema, software tools, and popular datasets\nin the proposed format to lower barriers to entry, and dis-\ncuss how now is a crucial time to make a concerted effort\ntoward sustainable annotation standards.\n1. INTRODUCTION\nMusic annotations —the collection of observations made\nby one or more agents about an acoustic music signal—\nare an integral component of content-based Music Infor-\nmation Retrieval (MIR) methodology, and are necessary\nfor designing, evaluating, and comparing computational\nsystems. For clarity, we deﬁne the scope of an annota-\ntion as corresponding to time scales at or below the level\nof a complete song, such as semantic descriptors (tags) or\ntime-aligned chords labels. Traditionally, the community\nhas relied on plain text and custom conventions to serialize\nthis data to a ﬁle for the purposes of storage and dissem-\nination, collectively referred to as “lab-ﬁles”. Despite a\nlack of formal standards, lab-ﬁles have been, and continue\nto be, the preferred ﬁle format for a variety of MIR tasks,\nsuch as beat or onset estimation, chord estimation, or seg-\nmentation.\n\u0003Please direct correspondence to ejhumphrey@nyu.edu\nc\rEric J. Humphrey, Justin Salamon, Oriol Nieto, Jon\nForsyth, Rachel M. Bittner, Juan P. Bello.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Eric J. Humphrey, Justin Salamon,\nOriol Nieto, Jon Forsyth, Rachel M. Bittner, Juan P. Bello. “JAMS: A\nJSON Annotated Music Speciﬁcation for Reproducible MIR Research”,\n15th International Society for Music Information Retrieval Conference,\n2014.Meanwhile, the interests and requirements of the com-\nmunity are continually evolving, thus testing the practical\nlimitations of lab-ﬁles. By our count, there are three un-\nfolding research trends that are demanding more of a given\nannotation format:\n\u000fComprehensive annotation data: Rich annotations,\nlike the Billboard dataset [2], require new, content-\nspeciﬁc conventions, increasing the complexity of\nthe software necessary to decode it and the burden\non the researcher to use it; such annotations can be\nso complex, in fact, it becomes necessary to docu-\nment how to understand and parse the format [5].\n\u000fMultiple annotations for a given task: The expe-\nrience of music can be highly subjective, at which\npoint the notion of “ground truth” becomes tenu-\nous. Recent work in automatic chord estimation [8]\nshows that multiple reference annotations should be\nembraced, as they can provide important insight into\nsystem evaluation, as well as into the task itself.\n\u000fMultiple concepts for a given signal: Although sys-\ntems are classically developed to accomplish a sin-\ngle task, there is ongoing discussion toward inte-\ngrating information across various musical concepts\n[12]. This has already yielded measurable beneﬁts\nfor the joint estimation of chords and downbeats [9]\nor chords and segments [6], where leveraging mul-\ntiple information sources for the same input signal\ncan lead to improved performance.\nIt has long been acknowledged that lab-ﬁles cannot be used\nto these ends, and various formats and technologies have\nbeen previously proposed to alleviate these issues, such\nas RDF [3], HDF5 [1], or XML [7]. However, none of\nthese formats have been widely embraced by the commu-\nnity. We contend that the weak adoption of any alternative\nformat is due to the combination of several factors. For ex-\nample, new tools can be difﬁcult, if not impossible, to in-\ntegrate into a research workﬂow because of compatibility\nissues with a preferred development platform or program-\nming environment. Additionally, it is a common criticism\nthat the syntax or data model of these alternative formats\nis non-obvious, verbose, or otherwise confusing. This is\nespecially problematic when researchers must handle for-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n591mat conversions. Taken together, the apparent beneﬁts to\nconversion are outweighed by the tangible costs.\nIn this paper, we propose a JSON Annotated Music Spec-\niﬁcation (JAMS) to meet the changing needs of the MIR\ncommunity, based on three core design tenets: simplicity,\nstructure, and sustainability. This is achieved by combin-\ning the advantages of lab-ﬁles with lessons learned from\npreviously proposed formats. The resulting JAMS ﬁles\nare human-readable, easy to drop into existing workﬂows,\nand provide solutions to the research trends outlined previ-\nously. We further address classical barriers to adoption by\nproviding tools for easy use with Python and MATLAB,\nand by offering an array of popular datasets as JAMS ﬁles\nonline. The remainder of this paper is organized as fol-\nlows: Section 2 identiﬁes three valuable components of an\nannotation format by considering prior technologies; Sec-\ntion 3 formally introduces JAMS, detailing how it meets\nthese design criteria and describing the proposed speci-\nﬁcation by example; Section 4 addresses practical issues\nand concerns in an informal FAQ-style, touching on usage\ntools, provided datasets, and some practical shortcomings;\nand lastly, we close with a discussion of next steps and\nperspectives for the future in Section 5.\n2. CORE DESIGN PRINCIPLES\nIn order to craft an annotation format that might serve the\ncommunity into the foreseeable future, it is worthwhile to\nconsolidate the lessons learned from both the relative suc-\ncess of lab-ﬁles and the challenges faced by alternative for-\nmats into a set of principles that might guide our design.\nWith this in mind, we offer that usability, and thus the like-\nlihood of adoption, is a function of three criteria:\n2.1 Simplicity\nThe value of simplicity is demonstrated by lab-ﬁles in two\nspeciﬁc ways. First, the contents are represented in a for-\nmat that is intuitive, such that the document model clearly\nmatches the data structure and is human-readable, i.e. uses\na lightweight syntax. This is a particular criticism of RDF\nand XML, which can be verbose compared to plain text.\nSecond, lab-ﬁles are conceptually easy to incorporate into\nresearch workﬂows. The choice of an alternative ﬁle for-\nmat can be a signiﬁcant hurdle if it is not widely supported,\nas is the case with RDF, or the data model of the document\ndoes not match the data model of the programming lan-\nguage, as with XML.\n2.2 Structure\nIt is important to recognize that lab-ﬁles developed as a\nway to serialize tabular data (i.e. arrays) in a language-\nindependent manner. Though lab-ﬁles excel at this par-\nticular use case, they lack the structure required to en-\ncode complex data such as hierarchies or mix different data\ntypes, such as scalars, strings, multidimensional arrays,\netc. This is a known limitation, and the community has\ndevised a variety of ad hoc strategies to cope with it: folder\ntrees and naming conventions, such as “fXg/fYg/fZg.lab”,where X, Y , and Z correspond to “artist”, “album”, and\n“title”, respectively1; parsing rules, such as “lines begin-\nning with ‘#’ are to be ignored as comments”; auxiliary\nwebsites or articles, decoupled from the annotations them-\nselves, to provide critical information such as syntax, con-\nventions, or methodology. Alternative representations are\nable to manage more complex data via standardized markup\nand named entities, such as ﬁelds in the case of RDF or\nJSON, or IDs, attributes and tags for XML.\n2.3 Sustainability\nRecently in MIR, a more concerted effort has been made\ntoward sustainable research methods, which we see posi-\ntively impacting annotations in two ways. First, there is\nconsiderable value to encoding methodology and metadata\ndirectly in an annotation, as doing so makes it easier to\nboth support and maintain the annotation while also en-\nabling direct analyses of this additional information. Ad-\nditionally, it is unnecessary for the MIR community to de-\nvelop every tool and utility ourselves; we should instead\nleverage well-supported technologies from larger commu-\nnities when possible.\n3. INTRODUCING JAMS\nSo far, we have identiﬁed several goals for a music anno-\ntation format: a data structure that matches the document\nmodel; a lightweight markup syntax; support for multiple\nannotations, multiple tasks, and rich metadata; easy work-\nﬂow integration; cross-language compliance; and the use\nof pre-existing technologies for stability. To ﬁnd our an-\nswer, we need only to look to the web development com-\nmunity, who have already identiﬁed a technology that meets\nthese requirements. JavaScript Object Notation (JSON)2\nhas emerged as theserialization format of the Internet, now\nﬁnding native support in almost every modern program-\nming language. Notably, it was designed to be maximally\nefﬁcient and human readable, and is capable of represent-\ning complex data structures with little overhead.\nJSON is, however, only a syntax, and it is necessary\nto deﬁne formal standards outlining how it should be used\nfor a given purpose. To this end, we deﬁne a speciﬁca-\ntion on top of JSON (JAMS), tailored to the needs of MIR\nresearchers.\n3.1 A Walk-through Example\nPerhaps the clearest way to introduce the JAMS speciﬁ-\ncation is by example. Figure 1 provides the contents of a\nhypothetical JAMS ﬁle, consisting of nearly valid3JSON\nsyntax and color-coded by concept. JSON syntax will be\nfamiliar to those with a background in C-style languages,\nas it uses square brackets (“[ ] ”) to denote arrays (alterna-\ntively, lists or vectors), and curly brackets (“fg”) to denote\n1http://www.isophonics.net/content/\nreference-annotations\n2http://www.json.org/\n3The sole exception is the use of ellipses (“:::”) as continuation charac-\nters, indicating that more information could be included.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n592                                                      ,{'tag':\n 'beat':                                        \n 'chord':                                        \n 'melody':                                                                                          ,                                                      ,                                                     ,\n                                                    , 'file_metadata':                                        \n 'sandbox':                       }{'foo': \"bar\", ... }                 , ...]                                                                                                    ,[                                        {'data':                                             \n                                                        ,                                            , 'annotation_metadata':       \n 'sandbox':          }                                 ,  , ...][                                 {'value': \"good for running\",  'confidence': 0.871, 'secondary_value': \"use-case\"} 'secondary_value': \"use-case\"}{'corpus': \"User-Generated Tags\",\n                                                  , 'version': \"0.0.1\", 'annotation_rules': \"Annotators were provided ...\", 'annotation_tools': \"Sonic Visualizer, ...\", 'validation': \"Data were checked by ...\", 'data_source': \"Manual Annotation\", 'curator':                                    'annotator':{'unique_id': \"61a4418c841\", 'skill_level': \"novice\", 'principal_instrument': \"voice\", 'primary_role': \"composer\", ... }{\"name\": \"Jane Doe\", \"email\": \"j.doe@xyz.edu\"}\n{ ... }{'data': ... }                                          \n                 , ...]                                                                                          ,[                                        {'data':                                              'annotation_metadata':         ,                                              , 'sandbox':         }       { ... }{ ... }                                ,                 , ...][                                 {'time':                        , 'label':                       }{'value': 0.237, ... }{'value': \"1\", ... }{'time': ... }{'data': ... }                                          \n                 , ...]                                                                                          ,[                                        {'data':                                              'annotation_metadata':         ,                                              , 'sandbox':         }       { ... }{ ... }                                   ,                 , ...][                                 {'start':                       , 'label':                       } 'end':                      ,{'value': 0.237, ... }{'value': \"1\", ... }{'value': \"Eb\", ... }{'time': ... }{'data': ... }                                          \n                 , ...]                                                                                 ,                                                    ,[                                        {'data':                                             \n                                                , 'annotation_metadata':         ,                                                , 'sandbox':       { ... }{ ... }                                    ,                                             ,                  , ...][                                 {'value': [ 205.340, 204.836, 205.561, ... ], 'confidence': [ 0.966, 0.884, 0.896, ... ], 'label':                           } 'time': [ 10.160, 10.538, 10.712, ... ],{'value': \"vocals\", ... }{'value': ... }{'data': ... }                                          \n                                           ,{'version': \"0.0.1\",                              'identifiers':                            'artist': \"The Beatles\",                           'title': \"With a Little Help from My Friends\", 'release': \"Sgt. Pepper's Lonely Hearts Club Band\", 'duration': 159.11 }{'echonest_song_id': \"SOVBDYA13D4615308E\",                                     'youtube_id': \"jBDF04fQKtQ”, ... }{'value': \"rock\", ... }EFGH\nI\nJ\nKLDCBA\nMFigure 1. Diagram illustrating the structure of the JAMS\nspeciﬁcation.objects (alternatively, dictionaries, structs, or hash maps).\nDeﬁning some further conventions for the purpose of illus-\ntration, we use single quotes to indicate ﬁeld names, italics\nwhen referring to concepts, and consistent colors for the\nsame data structures. Using this diagram, we will now step\nthrough the hierarchy, referring back to relevant compo-\nnents as concepts are introduced.\n3.1.1 The JAMS Object\nA JAMS ﬁle consists of one top-level object, indicated\nby the outermost bounding box. This is the primary con-\ntainer for all information corresponding to a music sig-\nnal, consisting of several task-array pairs, an object for\nfile metadata, and an object for sandbox. A task-\narray is a list of annotations corresponding to a given task\nname, and may contain zero, one, or many annotations for\nthat task. The format of each array is speciﬁc to the kind\nof annotations it will contain; we will address this in more\ndetail in Section 3.1.2.\nThefile metadata object (K) is a dictionary con-\ntaining basic information about the music signal, or ﬁle,\nthat was annotated. In addition to the ﬁelds given in the di-\nagram, we also include an unconstrained identifiers\nobject (L), for storing unique identiﬁers in various names-\npaces, such as the EchoNest or YouTube. Note that we\npurposely do not store information about the recording’s\naudio encoding, as a JAMS ﬁle is format-agnostic. In other\nwords, we assume that any sample rate or perceptual codec\nconversions will have no effect on the annotation, within a\npractical tolerance.\nLastly, the JAMS object also contains a sandbox, an\nunconstrained object to be used as needed. In this way, the\nspeciﬁcation carves out such space for any unforeseen or\notherwise relevant data; however, as the name implies, no\nguarantee is made as to the existence or consistency of this\ninformation. We do this in the hope that the speciﬁcation\nwill not be unnecessarily restrictive, and that commonly\n“sandboxed” information might become part of the speci-\nﬁcation in the future.\n3.1.2 Annotations\nAnannotation (B) consists of all the information that is\nprovided by a single annotator about a single task for a\nsingle music signal. Independent of the task, an annotation\ncomprises three sub-components: an array of objects for\ndata (C), an annotation metadata object (E), and\nan annotation-level sandbox. For clarity, a task-array (A)\nmay contain multiple annotations (B).\nImportantly, a data array contains the primary anno-\ntation information, such as its chord sequence, beat loca-\ntions, etc., and is the information that would normally be\nstored in a lab-ﬁle. Though all data containers are func-\ntionally equivalent, each may consist of only one object\ntype, speciﬁc to the given task. Considering the different\ntypes of musical attributes annotated for MIR research, we\ndivide them into four fundamental categories:\n1. Attributes that exist as a single observation for the\nentire music signal, e.g. tags.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5932. Attributes that consist of sparse events occurring at\nspeciﬁc times, e.g. beats or onsets.\n3. Attributes that span a certain time range, such as\nchords or sections.\n4. Attributes that comprise a dense time series, such\nas discrete-time fundamental frequency values for\nmelody extraction.\nThese four types form the most atomic data structures, and\nwill be revisited in greater detail in Section 3.1.3. The im-\nportant takeaway here, however, is that data arrays are not\nallowed to mix fundamental types.\nFollowing [10], an annotation metadata object\nis deﬁned to encode information about what has been an-\nnotated, who created the annotations, with what tools, etc.\nSpeciﬁcally, corpus provides the name of the dataset to\nwhich the annotation belongs; version tracks the version of\nthis particular annotation; annotation rules describes the\nprotocol followed during the annotation process; annota-\ntiontools describes the tools used to create the annota-\ntion; validation speciﬁes to what extent the annotation was\nveriﬁed and is reliable; data source details how the anno-\ntation was obtained, such as manual annotations, online\naggregation, game with a purpose, etc.; curator (F) is it-\nself an object with two subﬁelds, name andemail, for the\ncontact person responsible for the annotation; and annota-\ntor(G) is another unconstrained object, which is intended\nto capture information about the source of the annotation.\nWhile complete metadata are strongly encouraged in prac-\ntice, currently only version andcurator are mandatory in\nthe speciﬁcation.\n3.1.3 Datatypes\nHaving progressed through the JAMS hierarchy, we now\nintroduce the four atomic data structures, out of which an\nannotation can be constructed: observation, event, range\nandtime series. For clarity, the data array (A) of a tag\nannotation is a list of observation objects; the data array of\nabeat annotation is a list of event objects; the data array\nof achord annotation is a list of range objects; and the\ndata array of a melody annotation is a list of time series\nobjects. The current space of supported tasks is provided\nin Table 1.\nOf the four types, an observation (D) is the most atomic,\nand used to construct the other three. It is an object that\nhas one primary ﬁeld, value, and two optional ﬁelds,\nconfidence andsecondary value. The value and\nsecondary value ﬁelds may take any simple primi-\ntive, such as a string, numerical value, or boolean, whereas\ntheconfidence ﬁeld stores a numerical conﬁdence es-\ntimate for the observation. A secondary value ﬁeld is pro-\nvided for ﬂexibility in the event that an observation re-\nquires an additional level of speciﬁcity, as is the case in\nhierarchical segmentation [11].\nAnevent (H) is useful for representing musical attributes\nthat occur at sparse moments in time, such as beats or on-\nsets. It is a container that holds two observations, time\nandlabel. Referring to the ﬁrst beat annotation in theobservation event range time series\ntag beat chord melody\ngenre onset segment pitch\nmood key pattern\nnote\nsource\nTable 1. Currently supported tasks and types in JAMS.\ndiagram, the value oftime is a scalar quantity (0.237),\nwhereas the value oflabel is a string (‘1’), indicating\nmetrical position.\nArange (I) is useful for representing musical attributes\nthat span an interval of time, such as chords or song seg-\nments (e.g. intro, verse, chorus). It is an object that consists\nof three observations: start, end, and label.\nThetime series (J) atomic type is useful for represent-\ning musical attributes that are continuous in nature, such\nas fundamental frequency over time. It is an object com-\nposed of four elements: value, time, confidence\nandlabel. The ﬁrst three ﬁelds are arrays of numerical\nvalues, while label is an observation.\n3.2 The JAMS Schema\nThe description in the previous sections provides a high-\nlevel understanding of the proposed speciﬁcation, but the\nonly way to describe it without ambiguity is through for-\nmal representation. To accomplish this, we provide a JSON\nschema4, a speciﬁcation itself written in JSON that uses\na set of reserved keywords to deﬁne valid data structures.\nIn addition to the expected contents of the JSON ﬁle, the\nschema can specify which ﬁelds are required, which are\noptional, and the type of each ﬁeld (e.g. numeric, string,\nboolean, array or object). A JSON schema is concise, pre-\ncise, and human readable.\nHaving deﬁned a proper JSON schema, an added bene-\nﬁt of JAMS is that a validator can verify whether or not a\npiece of JSON complies with a given schema. In this way,\nresearchers working with JAMS ﬁles can easily and conﬁ-\ndently test the integrity of a dataset. There are a number of\nJSON schema validator implementations freely available\nonline in a variety of languages, including Python, Java, C,\nJavaScript, Perl, and more. The JAMS schema is included\nin the public software repository (cf. Section 4), which also\nprovides a static URL to facilitate directly accessing the\nschema from the web within a workﬂow.\n4. JAMS IN PRACTICE\nWhile we contend that the use and continued development\nof JAMS holds great potential for the many reasons out-\nlined previously, we acknowledge that speciﬁcations and\nstandards are myriad, and it can be difﬁcult to ascertain\nthe beneﬁts or shortcomings of one’s options. In the in-\nterest of encouraging adoption and the larger discussion of\n4http://json-schema.org/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n594standards in the ﬁeld, we would like to address practical\nconcerns directly.\n4.1 How is this any different than X?\nThe biggest advantage of JAMS is found in its capacity\nto consistently represent rich information with no addi-\ntional effort from the parser and minimal markup over-\nhead. Compared to XML or RDF, JSON parsers are ex-\ntremely fast, which has contributed in no small part to its\nwidespread adoption. These efﬁciency gains are coupled\nwith the fact that JAMS makes it easier to manage large\ndata collections by keeping all annotations for a given song\nin the same place.\n4.2 What kinds of things can I do with JAMS that I\ncan’t already do with Y?\nJAMS can enable much richer evaluation by including mul-\ntiple, possibly conﬂicting, reference annotations and di-\nrectly embedding information about an annotation’s ori-\ngin. A perfect example of this is found in the Rock Corpus\nDataset [4], consisting of annotations by two expert musi-\ncians: one, a guitarist, and the other, a pianist. Sources of\ndisagreement in the transcriptions often stem from differ-\nences of opinion resulting from familiarity with their prin-\ncipal instrument, where the voicing of a chord that makes\nsense on piano is impossible for a guitarist, and vice versa.\nSimilarly, it is also easier to develop versatile MIR systems\nthat combine information across tasks, as that information\nis naturally kept together.\nAnother notable beneﬁt of JAMS is that it can serve as\na data representation for algorithm outputs for a variety of\ntasks. For example, JAMS could simplify MIREX submis-\nsions by keeping all machine predictions for a given team\ntogether as a single submission, streamlining evaluations,\nwhere the annotation sandbox and annotator metadata can\nbe used to keep track of algorithm parameterizations. This\nenables the comparison of many references against many\nalgorithmic outputs, potentially leading to a deeper insight\ninto a system’s performance.\n4.3 So how would this interface with my workﬂow?\nThanks to the widespread adoption of JSON, the vast ma-\njority of languages already offer native JSON support. In\nmost cases, this means it is possible to go from a JSON\nﬁle to a programmatic data structure in your language of\nchoice in a single line of code using tools you didn’t have\nto write. To make this experience even simpler, we ad-\nditionally provide two software libraries, for Python and\nMATLAB. In both instances, a lightweight software wrap-\nper is provided to enable a seamless experience with JAMS,\nallowing IDEs and interpreters to make use of autocom-\nplete and syntax checking. Notably, this allows us to pro-\nvide convenience functionality for creating, populating, and\nsaving JAMS objects, for which examples and sample code\nare provided with the software library5.\n5https://github.com/urinieto/jams4.4 What datasets are already JAMS-compliant?\nTo further lower the barrier to entry and simplify the pro-\ncess of integrating JAMS into a pre-existing workﬂow, we\nhave collected some of the more popular datasets in the\ncommunity and converted them to the JAMS format, linked\nvia the public repository. The following is a partial list of\nconverted datasets: Isophonics (beat, chord, key, segment);\nBillboard (chord); SALAMI (segment, pattern); RockCor-\npus (chord, key); tmc323 (chords); Cal500 (tag); Cal10k\n(tag); ADC04 (melody); and MIREX05 (melody).\n4.5 Okay, but mydata is in a different format – now\nwhat?\nWe realize that it is impractical to convert every dataset\nto JAMS, and provide a collection of Python scripts that\ncan be used to convert lab-ﬁles to JAMS. In lieu of direct\ninterfaces, alternative formats can ﬁrst be converted to lab-\nﬁles and translated to JAMS thusly.\n4.6 My MIR task doesn’t really ﬁt with JAMS.\nThat’s not a question, but it is a valid point and one worth\ndiscussing. While this ﬁrst iteration of JAMS was designed\nto be maximally useful across a variety of tasks, there are\ntwo broad reasons why JAMS might not work for a given\ndataset or task. One, a JAMS annotation only considers\ninformation at the temporal granularity of a single audio\nﬁle and smaller, independently of all other audio ﬁles in\nthe world. Therefore, extrinsic relationships, such as cover\nsongs or music similarity, won’t directly map to the speci-\nﬁcation because the concept is out of scope.\nThe other, more interesting, scenario is that a given use\ncase requires functionality we didn’t plan for and, as a\nresult, JAMS doesn’t yet support. To be perfectly clear,\nthe proposed speciﬁcation is exactly that –a proposal– and\none under active development. Born out of an internal\nneed, this initial release focuses on tasks with which the\nauthors are familiar, and we realize the difﬁculty in solving\na global problem in a single iteration. As will be discussed\nin greater detail in the ﬁnal section, the next phase on our\nroadmap is to solicit feedback and input from the commu-\nnity at large to assess and improve upon the speciﬁcation.\nIf you run into an issue, we would love to hear about your\nexperience.\n4.7 This sounds promising, but nothing’s perfect.\nThere must be shortcomings.\nIndeed, there are two practical limits that should be men-\ntioned. Firstly, JAMS is not designed for features or signal\nlevel statistics. That said, JSON is still a fantastic, cross-\nlanguage syntax for serializing data, and may further serve\na given workﬂow. As for practical concerns, it is a known\nissue that parsing large JSON objects can be slow in MAT-\nLAB. We’ve worked to make this no worse than reading\ncurrent lab-ﬁles, but speed and efﬁciency are not touted\nbeneﬁts of MATLAB. This may become a bigger issue as\nJAMS ﬁles become more complete over time, but we are\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n595actively exploring various engineering solutions to address\nthis concern.\n5. DISCUSSION AND FUTURE PERSPECTIVES\nIn this paper, we have proposed a JSON format for mu-\nsic annotations to address the evolving needs of the MIR\ncommunity by keeping multiple annotations for multiple\ntasks alongside rich metadata in the same ﬁle. We do so in\nthe hopes that the community can begin to easily leverage\nthis depth of information, and take advantage of ubiqui-\ntous serialization technology (JSON) in a consistent man-\nner across MIR. The format is designed to be intuitive and\neasy to integrate into existing workﬂows, and we provide\nsoftware libraries and pre-converted datasets to lower bar-\nriers to entry.\nBeyond practical considerations, JAMS has potential to\ntransform the way researchers approach and use music an-\nnotations. One of the more pressing issues facing the com-\nmunity at present is that of dataset curation and access. It is\nour hope that by associating multiple annotations for multi-\nple tasks to an audio signal with retraceable metadata, such\nas identiﬁers or URLs, it might be easier to create freely\navailable datasets with better coverage across tasks. Anno-\ntation tools could serve music content found freely on the\nInternet and upload this information to a common repos-\nitory, ideally becoming something like a Freebase6for\nMIR. Furthermore, JAMS provides a mechanism to han-\ndle multiple concurrent perspectives, rather than forcing\nthe notion of an objective truth.\nFinally, we recognize that any speciﬁcation proposal\nis incomplete without an honest discussion of feasibility\nand adoption. The fact remains that JAMS arose from the\ncombination of needs within our group and an observation\nof wider applicability. We have endeavored to make the\nspeciﬁcation maximally useful with minimal overhead, but\nappreciate that community standards require iteration and\nfeedback. This current version is not intended to be the\ndeﬁnitive answer, but rather a starting point from which\nthe community can work toward a solution as a collective.\nOther professional communities, such as the IEEE, con-\nvene to discuss standards, and perhaps a similar process\ncould become part of the ISMIR tradition as we continue\nto embrace the pursuit of reproducible research practices.\n6. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProc. of the 12th International Society for Music In-\nformation Retrieval Conference, pages 591–596, 2011.\n[2] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fuji-\nnaga. An expert ground truth set for audio chord recog-\nnition and music analysis. In Proc. of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 633–638, 2011.\n6http://www.freebase.com[3] Chris Cannam, Christian Landone, Mark B Sandler,\nand Juan Pablo Bello. The sonic visualiser: A visuali-\nsation platform for semantic descriptors from musical\nsignals. In Proc. of the 7th International Society for\nMusic Information Retrieval Conference, pages 324–\n327, 2006.\n[4] Trevor De Clercq and David Temperley. A corpus anal-\nysis of rock harmony. Popular Music, 30(1):47–70,\n2011.\n[5] W Bas de Haas and John Ashley Burgoyne. Parsing the\nbillboard chord transcriptions. University of Utrecht,\nTech. Rep, 2012.\n[6] Matthias Mauch, Katy Noland, and Simon Dixon. Us-\ning musical structure to enhance automatic chord tran-\nscription. In Proc. of the 10th International Society for\nMusic Information Retrieval Conference, pages 231–\n236, 2009.\n[7] Cory McKay, Rebecca Fiebrink, Daniel McEnnis,\nBeinan Li, and Ichiro Fujinaga. Ace: A framework\nfor optimizing music classiﬁcation. In Proc. of the 6th\nInternational Society for Music Information Retrieval\nConference, pages 42–49, 2005.\n[8] Yizhao Ni, Matthew McVicar, Raul Santos-Rodriguez,\nand Tijl De Bie. Understanding effects of subjectiv-\nity in measuring chord estimation accuracy. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 21(12):2607–2615, 2013.\n[9] H ´el`ene Papadopoulos and Geoffroy Peeters. Joint esti-\nmation of chords and downbeats from an audio signal.\nAudio, Speech, and Language Processing, IEEE Trans-\nactions on, 19(1):138–152, 2011.\n[10] G. Peeters and K. Fort. Towards a (better) deﬁnition of\nannotated MIR corpora. In Proc. of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 25–30, Porto, Portugal, Oct. 2012.\n[11] Jordan Bennett Louis Smith, John Ashley Burgoyne,\nIchiro Fujinaga, David De Roure, and J Stephen\nDownie. Design and creation of a large-scale database\nof structural annotations. In Proc. of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 555–560, 2011.\n[12] Emmanuel Vincent, Stanislaw A Raczynski, Nobutaka\nOno, Shigeki Sagayama, et al. A roadmap towards ver-\nsatile mir. In Proc. of the 11th International Society for\nMusic Information Retrieval Conference, pages 662–\n664, 2010.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n596"
    },
    {
        "title": "Geographical Region Mapping Scheme Based on Musical Preferences.",
        "author": [
            "Sanghoon Jun",
            "Seungmin Rho",
            "Eenjun Hwang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418011",
        "url": "https://doi.org/10.5281/zenodo.1418011",
        "ee": "https://zenodo.org/records/1418011/files/JunRH14.pdf",
        "abstract": "Many countries and cities in the world tend to have dif- ferent types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize ge- ographical proximity for evaluating the similarity of mu- sic preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as cul- ture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their popula- tions. That is, countries or cities in a traditional map are rearranged in the music map such that regions with simi- lar musical preferences are close to one another. To do this, we collect users’ music play history and extract pop- ular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results.",
        "zenodo_id": 1418011,
        "dblp_key": "conf/ismir/JunRH14",
        "keywords": [
            "countries",
            "cities",
            "different types of preferred music",
            "geographical proximity",
            "music preferences",
            "musical preferences",
            "popularity",
            "musical preferences",
            "iterative algorithm",
            "regions"
        ],
        "content": "GEOGRAPHICAL REGION MAPPING SCHEME BASED ON \nMUSICAL PREFERENCES  \nSanghoon Jun  Seungmin Rho  Eenjun Hwang  \nKorea University  \nysbhjun@korea.ac.kr  Sungkyul University  \nsmrho@sungkyul.edu  Korea  University  \nehwang04@ korea.ac.kr  \nABSTRACT  \nMany  countries and  cities in the world tend to have dif-\nferent types of  preferred or popular music, such as pop, \nK-pop, and reggae. Music -related applications utilize g e-\nographical proximity for evaluating the similarity of m u-\nsic preferences between two regions. Sometimes, this can \nlead to incorrect results due to other factor s such as cu l-\nture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which \nregions are positioned close to one another depending on \nthe similarity of the musical preferences of their popul a-\ntions. That is, countries or cities in a traditional map are \nrearranged in the music map such that regions with sim i-\nlar musical preferences are close to one another. To do \nthis, we co llect users’ music play history and extract po p-\nular artists and tag information from the collected d ata. \nSimilarities among regions are calculated using the tags \nand their fr equencies. And then, an iterative algorithm for \nrearranging the regions into a music map is applied. We \npresent a method for constructing the music map along \nwith some experimental r esults.  \n1. INTRODUCTION  \nTo recommend suitable music pieces to users, various methods have been proposed and one of them is the joint \nconsideration of music and location information. In ge n-\neral, users in the same place tend to listen to similar kinds of music and this is shown by the statistics of music li s-\ntening history. Context -aware computing utilizes this \nhuman tendency to recommend son gs to a user.  \nHowever, the current approach of exploring geograp h-\nical proximity for obtaining a user’s music preferences might have several limitations due to various factors such \nas region scale, culture, religion, and language. That is, \nneighboring regions can show significant differences in \nmusic listening statistics and vice versa.  \nIn fact, the geographical distance between two regions \nis not always proportional to the degree of difference in music preferences. For instance, assume that there are \ntwo ne ighboring countries having different music prefer-ences. In the case of two regions near the border of the \ntwo countries, the people might show very different m u-\nsic preferences from those living in a region far from the border but in the same country. The d egree of preference \ndifferences can be varied because of the difference in the sizes of the countries. Furthermore, the water bodies that cover 71% of the Earth’s surface can lead to a disjunction \nof the differences.  \nMusic from countries that have a high cultural infl u-\nence might gain global popularity. For instance, pop m u-\nsic from the United States is very popular all over the \nworld. Countries that have a common cultural bac k-\nground might have similar musical preferences irrespec-tive of the geographical distance between them. La n-\nguage is another important factor that can lead to differ-ent countries, such as the US and the UK, having similar \npopular music charts.  \nFor these reasons, predicting musical preferences on \nthe basis of geographical proximity can lead to incorrect results. In this paper, we present a scheme for construc t-\ning a music map where regions are positioned close to one another depending on the musical preferences of their \npopulations. That is, regions such as cities in a traditional map are rear ranged in the music map such that regions \nwith similar musical preferences are close to one another. As a result, regions with similar musical preferences are \nconcentrated in the music map and regions with distinct \nmusical preferences are far away from the group.  \nThe rest of this paper is organized as follows: In Se c-\ntion 2, we present a brief overview of the related works. Section 3 presents the scheme for mapping a geograp h-\nical region to a new music space. Section 4 describes the experiments that we performed and some of the results. \nIn the last section, we conclude the paper with directions \nfor future work.  \n2. RELATED WORK  \nMany studies have tried to utilize location information for various music -related applications such as music \nsearch and recommendation. Kaminskas et al. presented a context -aware music recommender system that su g-\ngests music items on the basis of the users’ contextual \nconditions, such as the users’ mood or location [1]. They \ndefined the term “place of interest (POI)” and considered \nthe selection of suitable music tracks on the basis of the \nPOI. In [2], Schedl et al. presented a music recommend a-\n © Sanghoon Jun, Seungmin Rho, Eenjun Hwang . \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution: Sanghoon Jun, Seungmin Rho, \nEenjun Hwang . “Geographical Region Mapping Scheme Based On \nMusical Preferences ”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n519t\nion algorithm that combines information on the music \ncontent, music context, and user context by using a data \nset of geo -located music listing activities. In [3], Schedl \net al derived and analyzed culture- specific music liste n-\ning patterns by collecting music listening patterns of dif-\nferent countries (cities). They utilized social microblog such as Twitter and its tags in order to collect music -\nrelated information and measure the similarities between \nartists. Jun et al. presented a music recommender that \nconsiders personal and general musical predilections on \nthe basis of time and location [4]. They analyzed massive \nsocial network streams from twitter an d extracted the \nmusic listening histories. On the basis of a statistical \nanalysis of the time and location, a collection of songs is \nselected and blended using automatic mixing techniques. \nThese location -aware methods show a reasonable music \nsearch and recommendation performance when the range of the place of interest is small. However, the aforeme n-\ntioned problems might occur when the location range \nincreases. Furthermore, these methods do not consider \nthe case where remote regions have similar music prefer-\nences, which is often the case.  \nOn the basis of these observations, in this paper, we \npropose a new data structure called a “music map ”, \nwhere regions with similar musical preferences are locat-\ned close to one another. Some pioneering studies to re p-\nresent music by using visualization techniques have been \nreported. Lamere et al. presented an application for ex-\nploring and discovering new music by using a three -dimensional (3D) visualization model [5]. Using the m u-\nsic similarity model, they provided new tools f or explor-\ning and interacting with a music collection. In [6], Knees \net al. presented a user interface that creates a virtual landscape for music collection. By extracting features \nfrom audio signals and clustering the music pieces, they \ncreated a 3D island  landscape. In [7], Pampalk et al. pr e-\nsented a system that facilitates the exploration of music libraries. By estimating the perceived sound similarities, \nmusic pieces are organized on a two -dimensional (2D) \nmap so that similar pieces are located close to one anot h-\ner. In [8], Rauber et al. proposed an approach to automa t-\nically create an organization of music collection  based on \nsound similarities. A 3D visualization of music colle c-\ntion offers an interface for an interactive exploration of \nlarge music reposi tories.  \n3. GEOGRAPHICAL REGION MAPPING  \nIn this paper, we propose a scheme for geographical r e-\ngion mapping on the basis of the musical preferences of \nthe people residing in these regions. The proposed \nscheme consists of three parts as shown in Figure 1. Firs t-\nly, the music listening history and the related location d a-\nta are collected from Twitter. After defining regions, the collected data are refined to tag the statistics per region \nby querying popular artists and their popularities from last.fm. Similarities between the defined regions are cal-\nculated and stored in the similarity matrix. The similarity \nmatrix is represented into a 2D space by using an iterative \nalgorithm. Then, a Gaussian mixture model (GMM) is \ngenerated for constructing the music map on the basis of \nthe relative location of the regions . \n3.1 Music Listen History and Location Collection  \nBy analyzing the music listening history a nd location data, \nwe can find out the music type that is popular in a certain city or country. In order to construct a music map, we \nneed to collect the music listening history and location \ninformation on a global scale. To do this, we utilize \nlast.fm, which is a popular music database. However, last.fm has several limitations related to the coverage of \nthe global music listening history. The most critical one is \nthat the database provides the listening data of a partic u-\nlar country only. In other words, we cannot obtain the d a-\nta for a detailed region. Users in some countries (not all \ncountries) use last.fm, and it does not contain sufficient \ndata to cover the preferences of all the regions of these countries. Because of this, we observed that popular m u-\nsic i n the real world does not always match with the \nlast.fm data.  \nOn the other hand, an explosive number of messages \nare generated all over the world through Twitter. Twitter is one of the most popular social network services. In this \nstudy, we use Twitter for collecting a massive amount of \nmusic listening history data. By filtering music -related \nmessages from Twitter, we can collect various types of  \nFigure 1. Overall scheme  \n \n \nFigure 2. Collected data from twitter  \nExtract music listening \ndata and location\nDefine region and \ngroup data\nExtract popular artists \nfrom groups\nExtract tag statistics from \npopular artists\nGenerate similarity matrixMapping regions to \n2-dimensional space\nGenerate Gaussian \nmixture model\nGenerate map\nCalculate similarities of \nregion pairs\nAFAX\nAL\nDZAS\nADAO\nAI\nAQ\nAGARAM\nAWAU\nATAZ\nBSBH\nBDBB\nBYBEBZBJ\nBMBTBOBQ\nBABW\nBVBRIO\nBN\nBGBF\nBIKHCM\nCA\nCVKY\nCFTD\nCLCN\nCXCCCOKM\nCG\nCDCKCRCIHR\nCUCW\nCYCZDKDJDMDO\nECEGSV\nGQ\nEREEETF\nK\nFO\nFJFI\nFRGF\nPFTF\nGA\nGMGE DEGH\nGI\nGR\nGLGDGP\nGU\nGT\nGGGN\nGWGY\nHTHM\nVAHNHK\nHUIS\nIN\nID\nIRIQ\nIE IM\nILITJM\nJPJEJOKZ\nKE\nKIKP\nKRKW\nKGLA\nLV\nLBLS\nLRLY\nLILTLUMO\nMK\nMGMW\nMY\nMVMLMT\nMHMQMR\nMUYTMX\nFMMD\nMC\nMNMEMS\nMAMZ\nMM NA\nNR\nNPNL\nNCNZNI\nNENG\nNU\nNFMP\nNOOM\nPKPWPS\nPA\nPGPYPEPH\nPN\nPLPT\nPRQARE\nRO\nRU\nRWBL\nSHKN LCMFPM\nVCWS\nSMSTSA\nSNRSSCSL\nSGSX\nSK\nSISB\nSOZAGS\nSSES\nLKSD\nSRSJSZ\nSECH\nSYTWTJ\nTZTHTLTG\nTKTO\nTT\nTNTR\nTMTC\nTVUG\nUAAE\nGBUS\nUM UY\nUZVUVEVN\nVG\nVI\nWFEHYE\nZM\nZWSpace RepresentationSpace Mapping\nSimilarity MeasurementData Collection\n-Messages\n-GPS information\n-Profile location\n- Geo.TopArtist\n- Artist.TopTags\n- Google maps\n-Geocoder\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n520mu\nsic-related information, such as artist name, song title, \nand the published location. Figure 2 shows the distrib u-\ntion of the collected music -related tweets from around the \nworld.  \nWe used the Tweet Stream provided through a Twitter \napplication processing interface (API) for collecting \ntweets. In order to select only the music -related tweets, \nwe used music -related hashtags. Hashtags are very useful \nfor searching the relevant t weets or for grouping tweets \non the basis of topics. As shown in Table 1, we used the \nmusic -related hashtag lists that have been defined in [ 4]. \nMusic -related tweet messages contain musical infor-\nmation such as song title and artist name. These textual \ndata are represented in various forms. In particular, we \nconsidered the patterns shown in Table 2 for finding the \nartist names and the song titles. We employed a local \nMusicBrainz [ 9] server to validate the artist names.  \nFor collecting location information, we  gathered global \npositioning system (GPS) data that are included in tweet \nmessages. However, we observed that the number of tweets that contain GPS data is quite small considering \nthe total number of tweets. To solve this, we collected the \nprofile location  of the user who published a tweet me s-\nsage. Profile location contains the text address of the country or the city of the user. We employed the Google \nGeocoding API [ 10] for validating the location name and \nconverting the address to GPS coordinates.  \n \n3.2 Region Definition and Tag Representation  \nUsing the collected GPS information, we created a set of regions on the basis of the city or country. For grouping \ndata by city name or country name, the collected GPS i n-\nformation is converted into its corresponding city or \ncountry name. In this study, we got 1327 cities or 198 \ncountries from the music listening history collected \nthrough Twitter .  \nFor each region, we collect  two sets A\nr and AC r of re-\nferred artist names  and their play counts , respectively:   \n \n                            𝐴𝑟= {𝑎1, … ,𝑎𝑛}                             (1) \n                          𝐴𝐶𝑟= {𝑎𝑐1, … ,𝑎𝑐𝑛}                           (2) \n \nwhere n is the number of referred artists. Also, u sing an \nartist name , we can collect his/her  tag list. For a region r, \nwe construct a set T r of top tags by querying top tags  to \nlast.fm  using the artist names  of the region r  as follows:  \n \n 𝑇𝑟={𝑔𝑒𝑡𝑇𝑜𝑝𝑇𝑎𝑔𝑠 (𝑎1)∪…∪𝑔𝑒𝑡𝑇𝑜𝑝𝑇𝑎𝑔𝑠 (𝑎𝑛)| 𝑎𝑖∈ 𝐴𝑟}  \n      = {𝑡1, … 𝑡𝑚}                                                                        (3) \n \nwhere getTopTags (a) returns  a list of top tags of  artist a \nand m is the number of collected tags for the region r . We \ndefine a function RTC(r, t) that calculates  the total count \nof tag t in region r using the following equati on: \n \n           𝑅𝑇𝐶 (𝑟,𝑡)=∑𝑎𝑐𝑖×𝑔𝑒𝑡𝑇𝑎𝑔𝐶𝑜𝑢𝑛𝑡 (𝑎𝑖,𝑡)𝑎𝑖∈𝐴𝑟          (4) \n Her\ne, getTagCount (a, t) returns the count  of tag  t for the \nartist a in last.fm. In the same vein, RTC can return a set \nof tag counts when the second argument is a tag set T. \n \n          𝑅𝑇𝐶 (𝑟,𝑻)= {𝑅𝑇𝐶 (𝑟,𝑡1), … ,𝑅𝑇𝐶 (𝑟,𝑡𝑚)|𝑡𝑖∈ 𝑻}           ( 5) \n \n3.3 Similarity Measurement  \nTo construct a music map of regions, we need a mea s-\nurement for estimating musical similarity . In this paper, \nwe assume that music proximity  between regions  is \nclosely related to the artists and their tags because the \nmusical characteristics of a region can be explained by \nthe artists’  tags of the region.  In particular, in order to \nmeasure the similarity among the regions represented by the tag groups, we employed a cosine similarity meas-urement as shown in the following equation:  \n \n      𝑇𝑆𝑀 (𝑟1,𝑟2)=𝑅𝑇𝐶 (𝑟1,𝑻𝑢)×𝑅𝑇𝐶 (𝑟2,𝑻𝑢)\n�𝑅𝑇𝐶�𝑟1,𝑻𝑟1��×�𝑅𝑇𝐶�𝑟2,𝑻𝑟2��                 (6) #nowpla ying #np #music  \n#soundcloud  #musicfans  #listenlive  \n#hiphop  #musicmondays  #pandora  \n#mp3  #itunes  #newmusic  \n  \nTable 1. Music -related  hashtags.  \n \n<Phrase A > by <  Phrase B > \n< Phrase A > - < Phrase B  > \n< Phrase A  > / < Phrase B  > \n“< Phrase A  >” - < Phrase B  > \n \nTable 2. Typical syntax for parsing  song title and artist  \n \nFigure 3. Tag similarity matrix of 34 countries  \n  \nAUATBECACLCZDKEEFIFRDEGRHUISIEILITJPKRLUMXNLNZNOPLPTSKSIESSECHTRUKUSAU\nAT\nBE\nCA\nCL\nCZ\nDK\nEE\nFI\nFR\nDE\nGR\nHU\nIS\nIE\nIL\nIT\nJP\nKR\nLU\nMX\nNL\nNZ\nNO\nPL\nPT\nSK\nSI\nES\nSE\nCH\nTR\nUK\nUS\n0.50.60.70.80.91\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n521         \n                    𝑻𝑢=𝑻𝑟1∩𝑻𝑟2                                   (7) \n \nThe cosine similarities of all possible pairs of regions \nwere calculated and stored in the tag similarity matrix \nTSM. Hence, if there were m regions in the collection, we obtained a TSM of m  × m. A sample TSM for 34 cou n-\ntries is shown in Figure 3.  \n3.4 2D Space M apping  \nOn the basis of the TSM, we generated a 2D space for a music map  by converting tag similarities between regions \ninto proper metric for 2D space mapping. In this paper, \nthis conversion is done approximately  using an iterative \nalgorithm . The proposed algorithm is based on the co m-\nputational model such as a self -organizing map and an \nartificial neural network algorithm. By using an iterative phase, the algorithm gradually separates the regions in \ninverse proportion to the tag similarity.  \n3.4.1 Initialization  \nIn the initialization phase, 2D space is generated where X-axis and Y -axis of the space have ranges from 0 to 1. \nEach region is randomly placed on the 2D space.  We o b-\nserved that our random initialization does not provide de-\nterministic result of the 2D space mapping.  \n3.4.2 Iterations  \nIn each iteration, a region in the 2D space is randomly \nselected and the tag distance TD between the selected r e-\ngion r\ns and any other region r i is computed using the \nsimilarity matrix.  \n \n                   𝑇𝐷(𝑟𝑠,𝑟𝑖)= 1−𝑇𝑆𝑀 (𝑟𝑠,𝑟𝑖)                   (8) \n \nSubsequently, Euclidean distances ED  between the s e-\nlected region r s and other region r i is computed using the \nfollowing equation   \n𝐸𝐷(𝑟𝑠,𝑟𝑖)=��𝑥(𝑟𝑠)−𝑥(𝑟𝑖)�2+(𝑦(𝑟𝑠)−𝑦(𝑟𝑖))2   (9) \n \nwhere x(ri) and y (ri) returns x and y positions of the r e-\ngion r i in 2D space, respectively. In order for TD  and ED \nto have same value  as much as possible, the following \nequation is applied  \n \n𝑥(𝑟𝑖)=𝑥(𝑟𝑖)+  𝜆(𝑡)(𝐸𝐷(𝑟𝑠,𝑟𝑖)−𝑇𝐷(𝑟𝑠,𝑟𝑖))(𝑥(𝑟𝑠)−𝑥(𝑟𝑖))\n𝐸𝐷(𝑟𝑠,𝑟𝑖)  \n(10) \n \n𝑦(𝑟𝑖)=𝑦(𝑟𝑖)+  𝜆(𝑡)(𝐸𝐷(𝑟𝑠,𝑟𝑖)−𝑇𝐷(𝑟𝑠,𝑟𝑖))(𝑦(𝑟𝑠)−𝑦(𝑟𝑖))\n𝐸𝐷(𝑟𝑠,𝑟𝑖) \n(11) \n \nHere, λ(t) is a learning rate in t -th iteration. The learning \nrate is monotonically decreased during iteration accor d-\ning to the following equation  \n \n𝜆(𝑡) =𝜆0exp (−𝑡/𝑇)                           (12) \n   \n(a) iteration = 1  (b) iteration = 100  \n  \n(c) iteration = 400  (d) iteration = 1000  \n \nFigure 4. Example of mapped space in iterations   \nFigure 5. Gaussian mixture model  of 34 countries  \n \nFigure 6. Music map of 34 countries  \n0 0.2 0.4 0.6 0.8 100.20.40.60.81\nAustraliaAustria\nBelgium\nCanadaCh\nileCzech republic\nDenmarkEstoniaFinland\nFranceGermanyGreece\nHungary\nIcelandIrelandIsrael\nItalyJapan\nKorea, republic of\nLuxembourg\nMexicoNetherlandsNew zealand\nNorwayPoland\nPortugalSlovakiaSlovenia\nSpain\nSwedenSwitzerlandTurkey\nUnited kingdom\nUnited states\n  \n \n   \n-0.2 0 0.2 0.4 0.6 0.8 1 1.2-0.200.20.40.60.81\nAUAT\nBE\nCACLCZ\nDKEEFI\nFRDEGR\nHU\nISIEIL\nITJP\nKR\nLU\nMXNLNZ\nNOPL\nPTSKSI\nES\nSECHT\nR\nUK\nUS\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n522λ0 denotes the initial learning rate, and T represents the \ntotal number of iterations. After each iteration, regions \nhaving higher TD are located far away from the selected \nregion and regions having lower TD are located closer. \nFigure 4 shows examples of the mapped space after iter a-\ntions.  \n3.5 Space Representation  \nAfter 2D space mapping, the regions are mapped such \nthat regions having similar music preferences are placed \nclose to one another. As a result, they form distinct \ncrowds in the 2D space. In contrast, regions having unique preferences are placed apart from the crowds. To \nrepresent them as a map, a 2D distribution on the space is \nnot sufficient. In this paper, in order to represent the \ninformation like a real world map, we employed the \nGMM. The Gaussian  with diagonal matrix  is constructed \nusing the following equations:  \n \n                              𝜇(i)= {𝑥(𝑟\n𝑖),𝑦(𝑟𝑖)}                       (1 3) \n𝜎(i)=�18𝑛� 0\n018𝑛��                      (1 4) \n                                      𝑝(i)=1\n𝑛𝑛(𝑟𝑖)                           (1 5)   \n \nHere, n is total number of regions and nn( ri) returns the \nnumber of neighboring regions of region r i in the 2D \nspace. To model the GMM  in the crowded area of 2D \nspace, mixing proportion p (i) is adjusted based on the \nnumber of neighbors nn(ri). In other words, nn(ri) has a \nhigher value when p (i) is crowded and it reduces the pr o-\nportion of i -th Gaussian. It helps to prevent Gaussian  \nfrom over -height. An example of generated GMM is \nshown in Figure 5.  \nTo generate a music map using the GMM, the prob a-\nbilistic density function (pdf) of the GMM is simplified \nby applying a threshold. By projecting the GMM on the 2D plane after applying the threshold to the pdf, the \nboundaries of the GMM are created. We empirically \nfound that the threshold value 0 gives an appropriate \nboun dary. A boundary represents regions as a continent or a small island on the basis of their distribution. As a result, the mapped result is visualized as a music map \nhaving an appearance similar to that of a real world map. \nAn example of a music map for 34 countries is shown in \nFigure 6 . Although the generated music ma p contains less \ninformation than the  contour graph of GMM , it could  be \nmore intuitive to the  casual users to understand the rel a-\ntions between regions in terms of music preferences . \n4. EXPERIMENT  \n4.1 Experiment Setup  \nTo collect the music -related tweets, we gathered the tweet \nstreams from the Twitter server in real time in order to collect the music information of Twitter users. During \none week, we collected 4.57 million tweets that had the \nhashtags listed in Table 1. A fter filtering the tweets \nthrough regular expressions, 1.56 million music listening history records were collected. We got 1327 cities or 198  \nFigure  7. Average difference of distances in it erations  \n \nFigure 8. Music map of 239 countries  \n \nFigure 9. Top tags of music map.  0 100 200 300 400 50000.10.20.30.40.50.6\nIterationsAverage |ED-TD|\nAFAXAL\nDZ\nASAD\nAOAI\nAQ\nAGAR\nAM\nAWAUAT\nAZBS\nBHBDBB\nBY\nBEBZBJBM\nBT BOBQ\nBABW\nBVBR\nIO BNBG\nBFBIKH\nCMCA\nCVKYCFTD\nCLCN\nCX CCCO\nKMCG\nCD\nCKCR CIHR\nCUCWCYCZDK\nDJDM\nDOECEG\nSVGQ\nEREEETFK\nFOFJFI\nFR\nGFPFTF\nGA\nGMGE\nDE\nGHGIGR GLGD\nGPGUGT\nGGGN\nGW GYHT\nHMVA\nHNHK\nHUISIN\nID\nIRIQ\nIE IMIL\nIT\nJM\nJP\nJEJOKZKEKI\nKPKRKW\nKG\nLALV\nLB\nLS\nLRLY\nLILTLU\nMOMK\nMGMW\nMYMV\nML\nMTMH\nMQMRMUYT\nMX\nFMMD\nMCMN\nME MSMAMZ\nMMNA\nNR\nNPNLNC\nNZ\nNINE\nNGNUNF\nMPNO\nOM\nPK\nPWPS\nPAPG\nPYPEPH\nPN\nPL\nPTPRQARERO\nRURWBLSH KN\nLCMF\nPM\nVCWS\nSM\nST\nSA\nSNRS\nSCSL\nSG SXSKSISB\nSOZAGSSS\nES\nLKSD\nSRSJ\nSZSE\nCH\nSYTW\nTJ\nTZ THTL\nTGTK\nTO\nTTTNTR\nTMTC\nTVUG\nUA\nAEGB\nUSUMUYUZ\nVUVE\nVNVG\nVI WFEH\nYEZMZW\n  \nHhHhRo\nIn\nHhPo\nPoPo\nPo\nRoRo\nRo\nElPoRo\nRo Hh\nPoRoIn\nRo\nPoElRoNu\nElElEl\nRoEl\nPo Po\nPo PoRo\nRoRo Ro\nPoPo\nPoPoHhEl\nPoIn\nPo PoRo\nInEl\nPo\nPoRoRoRo\nRoRoRoRo Po\nRoIn\nPoRoRo\nPoJp\nRoPoPoEl\nPoElRo\nEl\nPuRoPo\nRo\nRoRoRo\nHhPoRo RoPo\nFvPoRo\nRoHh\nPoPoEl\nPoRo\nPoIn\nPoPoRo\nRo\nRoIn\nPoPoRo\nRo\nRo\nJp\nJpRoRoRoHh\nRoInIn\nRo\nPoPo\nRo\nRo\nHhIn\nInRoRo\nElEl\nRoRo\nPoRo\nPo\nRoPo\nElElRoHh\nPo\nPoPo\nRoRoPo PoPoEl\nPoPo\nRo\nRoPoEl\nRo\nPoRo\nHhPoEl\nRoEl\nRo\nPo\nInIn\nInRo\nPoRoPoPo\nRo\nPoPoPoPoRo\nRoJzJzJz El\nPoPo\nHh\nPoDb\nRo\nRo\nPo\nPoRo\nPoFv\nPoPoRoRoEl\nPo RoRoRo\nPo\nRoRo\nInRo\nElPo\nPo\nRoRo\nRo\nHh InEl\nInPo\nHh\nPoElRo\nRoEl\nPoHh\nRo\nPoPo\nPoPoRoRo\nPoPo\nInPo\nPo RoRo\nPoPoRo\n  \nPo(Pop), El(electronic), Hh(Hip-Hop), Ro(rock), Jz(jazz), In(Indie), Jp(japanese), \nFv(female vocalists), Db(Drum and bass), Pu(punk), Nu(Nu Metal)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n523c\nountries from the music listening history collected \nthrough Twitter. We collected the lists of the top artists \nfor 249 countries from last.fm. For these countries, 2735 \nartists and their top tags were collected from last.fm.  \n4.2 Differences of ED and TD  \nIn the proposed scheme, the iterative algorithm gradually \nreduces the difference between ED and TD, as mentioned \nabove.  In order to show that the algorithm reduces the \ndifference and moves the regions appropriately, the aver-age difference between ED and TD is measured in each \niteration. Figure 7 shows the average distances during 500 iterations. The early phases in the com putation show \nhigh average distance differences due to the random in i-\ntialization. As the iteration proceeds, the average distance \ndifferences are gradually reduced and converged.   \n4.3 Map Generation for 249 C ountries  \nIn order to evaluate the effectiveness of t he proposed \nscheme, we defined a region group that contained 249 \ncountries. After collecting the music listening history \nfrom Twitter and last.fm, we generated a music map by \nusing the proposed scheme. Figure 8 shows the resulting music map. We observed that the map consisted of a big \nisland (continent) and a few small islands. In the center of \nthe big island, countries that had a high musical influence, \nsuch as the US and the UK, were located. On the other \nhand, countries having unique music preferences such as \nJapan and Hong Kong were formed as small islands and \nlocated far away from the big island.   \n4.4 Top Tag Representation  \nA music map is based on the musical preferences b e-\ntween regions, and these preferences were calculated on \nthe basis of the similarities  of the musical tags.  In the last \nexperiment, we first find out the top tag of each country  \nand show the distribution of the top tag s in the music map . \nFigure 9 shows the top tags of the map in Figure 8. In the \nmap, “Rock” and “Pop”, which are the most popular tags \nin the collected data, are located in the center and occu-\npies a significant portion of the big island. On the north \nside of the big island, “ Electronic ” tag is located and in \nthe south , “Indie” tag is placed. The “Pop” tag, which is \npopular in  almost every country, is located throughout the \nmap.  \n5. CONCLUSION  \nIn this paper, we proposed a scheme for constructing a music map in which regions such as cities and countries are located close to one another depending on the musical \npreferences of the people residing in them. To do this, we \ncollected the music play history and extracted the popular \nartists and tag information from Twitter and last.fm. A \nsimilarity matrix for each region pair was calculated by using the tags and their frequencies. By applying an iter a-\ntive algorithm and GMM, we reorganized the regions into a music map according to the tag similarities. The possi-ble application domains of the proposed scheme span a \nbroad range —from music collection, browsing services, \nand music marketing tool s, to a worldwide music trend \nanalysis.  \n6. ACKNOWLEDGEMENT   \nThis research was supported by Basic Science Research Program through the National Research Foundation of \nKorea  (NRF) funded by the Ministry of Education (NRF -\n2013R1A1A2012627) and the MSIP  (Ministry  of Science, \nICT & Future Planning), Korea, under the ITRC  (Infor-\nmation Technology Research Center) support program \n(NIPA -2014- H0301- 14-1001) supervised by the NIPA  \n(National IT Industry Promotion Agency)  \n7. REFERENCES  \n[1] M. Kaminskas and F. Ricci, “Location- Adapted \nMusic Recommendation Using Tags,” in User \nModeling, Adaption and Personalization, Springer \nBerlin Heidelberg, 2011, pp. 183– 194. \n[2] M. Schedl and D. Schnitzer, “Location -Aware \nMusic Artist Recommendation,” in MultiMedia \nModeling, Springer International P ublishing, 2014, \npp. 205– 213. \n[3] M. Schedl and D. Hauger, “Mining Microblogs to \nInfer Music Artist Similarity and Cultural Listening \nPatterns,” in Proceedings of the 21st International Conference Companion on World Wide Web, New York, USA, 2012, pp. 877 –886. \n[4] S. Jun, D. Kim, M. Jeon, S. Rho, and E. Hwang, “Social mix: automatic music recommendation and \nmixing scheme based on social network analysis,” \nJournal of Supercomputing, pp. 1 –22, Apr. 2014.  \n[5] P. Lamere and D. Eck, “Using 3d visualizations to explore and di scover music,” in in Int. Conference \non Music Information Retrieval , 2007.  \n[6] P. Knees, M. Schedl, T. Pohle, and G. Widmer, \n“Exploring Music Collections in Virtual \nLandscapes,” IEEE MultiMedia , vol. 14, no. 3, pp. \n46–54, Jul. 2007.   \n[7] E. Pampalk, A. Rauber, and  D. Merkl, “Content -\nbased Organization and Visualization of Music \nArchives,” in Proceedings of the Tenth ACM \nInternational Conference on Multimedia, New York, NY, USA, 2002, pp. 570 –579. \n[8] A. Rauber, E. Pampalk, and D. Merkl, “The SOM-enhanced JukeBox: Organization and Visualization of Music Collections Based on Perceptual Models,” \nJournal of New Music Research , vol. 32, no. 2, pp. \n193– 210, 2003.   \n[9] “MusicBrainz  - The Open Music Encyclopedia.” \n[Online]. Available: http://musicbrainz.org/. \n[Accessed: 03 -May-2014].  \n[10] “The Google Geocoding API” [Online]. Available: \nhttps://developers.google.com/maps/documentation/geocoding/. [Accessed: 03 -May-2014]  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n524"
    },
    {
        "title": "A Data Set for Computational Studies of Schenkerian Analysis.",
        "author": [
            "Phillip B. Kirlin"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417833",
        "url": "https://doi.org/10.5281/zenodo.1417833",
        "ee": "https://zenodo.org/records/1417833/files/Kirlin14.pdf",
        "abstract": "Schenkerian analysis, a kind of hierarchical music anal- ysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computa- tional studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those composi- tions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously eval- uate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first pub- licly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical se- lections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software.",
        "zenodo_id": 1417833,
        "dblp_key": "conf/ismir/Kirlin14",
        "keywords": [
            "Schenkerian analysis",
            "music analysis",
            "computational studies",
            "data sets",
            "ground-truth analyses",
            "publicly available",
            "large-scale data",
            "musical selections",
            "encoding",
            "software"
        ],
        "content": "A DATA SET FOR COMPUTATIONAL STUDIES OF\nSCHENKERIAN ANALYSIS\nPhillip B. Kirlin\nDepartment of Mathematics and Computer Science, Rhodes College\nkirlinp@rhodes.edu\nABSTRACT\nSchenkerian analysis, a kind of hierarchical music anal-\nysis, is widely used by music theorists. Though it is part of\nthe standard repertoire of analytical techniques, computa-\ntional studies of Schenkerian analysis have been hindered\nby the lack of available data sets containing both musical\ncompositions and ground-truth analyses of those composi-\ntions. Without such data sets, it is difﬁcult to empirically\nstudy the patterns that arise in analyses or rigorously eval-\nuate the performance of intelligent systems for this kind\nof analysis. To combat this, we introduce the ﬁrst pub-\nlicly available large-scale data set of computer-processable\nSchenkerian analyses. We discuss the choice of musical se-\nlections in the data set, the encoding of the music and the\ncorresponding ground-truth analyses, and the possible uses\nof these data. As an example of the utility of the data set,\nwe present an algorithm that transforms the Schenkerian\nanalyses into hierarchically-organized data structures that\nare easily manipulated in software.\n1. CORPUS-DRIVEN RESEARCH\nCorpus-driven research is now commonplace in the music\ninformatics community. With the wealth of raw musical\ninformation now available in digital form, in many cases, it\nis straightforward to construct and use data sets containing\nnumerous musical compositions. However, the problem\nof collecting ground-truth metadata about the content of\nthe music still exists, especially where high-level features\nare concerned. This is a problem that effects researchers\nworking with music in audio or symbolic formats.\nGround-truth data sets that include features speciﬁ-\ncally relating to music theory or music analysis are par-\nticularly labor-intensive to construct. Information about\nthe high-level harmonic or melodic structure of composi-\ntions is often only found scattered throughout textbooks\nor individual research publications, and so there are few\npublicly-available corpora containing such information in\na computer-processable format. Some data sets are cre-\nated only for speciﬁc research projects and then discarded,\nc\rPhillip B. Kirlin.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Phillip B. Kirlin. “A Data Set For\nComputational Studies of Schenkerian Analysis”, 15th International So-\nciety for Music Information Retrieval Conference, 2014.are not in an easy-to-use format, or are simply never made\nwidely available.\nThe lack of varied ground-truth musical metadata relat-\ning to theory and analysis — especially data sets speciﬁ-\ncally designed to align with symbolic music data — hin-\nders corpus-driven research studies because time must be\nspent collecting data. Sometimes the researchers must per-\nform the music analysis themselves, possibly inadvertently\nintroducing biases into the data. Without widely available\ncomprehensive data sets, it is extremely difﬁcult to con-\nduct large-scale experiments on the structure of musical\ncompositions in symbolic form, or quantitatively evaluate\nthe performance of computational systems that emulate a\nmusic analysis process.\nThere is a particular dearth of empirical data available\nin the realm of Schenkerian analysis, a widely used analyt-\nical system that illustrates a hierarchical structure among\nthe notes of a composition. Though Schenkerian anal-\nysis is one of the most comprehensive methods for mu-\nsic analysis that we have available today [1], there are no\nlarge-scale digital repositories of analyses available to re-\nsearchers. In addition to the reasons stated above for the\nlack of corpora, Schenkerian analysis presents a number\nof unique challenges to creating a useful data set. First, a\nSchenkerian analysis for a composition is illustrated using\nthe musical score of the composition itself, and commonly\nrequires multiple staves to show the hierarchical structure\nuncovered. This requires substantial space on the printed\npage and thus is a deterrent to retaining large sets of analy-\nses. Second, there is no established computer-interpretable\nformat for Schenkerian analysis storage, and third, even if\nthere were a format, it would take a great deal of effort\nto encode a number of analyses into processable computer\nﬁles.\nThe lack of data has kept the number of computational\nstudies of Schenkerian analysis requiring ground-truth data\nto a bare minimum; some examples include studies using\ncorpora with six [7] or eight [6] pieces. Though these stud-\nies are useful, the results would likely carry more weight if\nthe data sets used were larger.\nWith all of these ideas in mind, in this paper we intro-\nduce the ﬁrst large-scale data set of musical compositions\nalong with corresponding ground-truth Schenkerian analy-\nses, called S CHENKER 411. The 41 musical selections in-\ncluded constitute the largest-known corpus of Schenkerian\nanalyses in a machine-readable format. The musical selec-\n1Available at www.cs.rhodes.edu/ \u0018kirlinp/schenker41.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n213tions are standardized in mode, length, and instrumenta-\ntion, and the analyses are stored in a novel text-based rep-\nresentation designed to be easily processed by a computer.\nWe created these data with the hope that they would be\nuseful to researchers (a) studying the Schenkerian analysis\nprocess itself from a quantitative standpoint (for instance,\ndetecting patterns in the way analysis is done), (b) need-\ning a data set of analyses for use with supervised machine\nlearning techniques, and (c) performing any sort of quanti-\ntative evaluation requiring ground-truth hierarchical music\nanalyses.\n2. THE DATA SET\n2.1 Creation and Content\nIn order to create a data set of musical compositions\nand corresponding ground-truth Schenkerian analyses that\nwould be useful to researchers with a wide variety of goals,\nwe restricted ourselves to music from the common prac-\ntice period of European art music, and selected 41 excerpts\nfrom works by J. S. Bach, G. F. Handel, Joseph Haydn,\nM. Clementi, W. A. Mozart, L. van Beethoven, F. Schu-\nbert, and F. Chopin. All of the compositions were either\nfor a solo keyboard instrument (or arranged for such an in-\nstrument) or for voice with keyboard accompaniment. All\nwere in major keys and did not modulate.\nThe musical excerpts were also selected for the ease of\nlocating a Schenkerian analysis for each excerpt done by\nan outside expert. Analyses for the 41 excerpts chosen\ncame from four places: Forte and Gilbert’s textbook In-\ntroduction to Schenkerian Analysis [4] and the correspond-\ning instructor’s manual [3], Cadwallader and Gagn ´e’s text-\nbook Analysis of Tonal Music [2], Pankhurst’s handbook\nSchenkerGUIDE [9], and a professor of music theory who\nteaches a Schenkerian analysis class. These four sources\nare denoted by the labels F&G, C&G, SG, and Expert in\nTable 1, which lists the excerpts in the corpus.\nFrom a Schenkerian standpoint, we also chose excerpts\nsuch that the analyses of the excerpts would all share some\ncommonalities. All the analyses contained a single linear\nprogression as the fundamental background structure: ei-\nther an instance of the Ursatz or a rising linear progression.\nSome excerpts contained an Ursatz with an interruption: a\nSchenkerian construct that occurs when a musical phrase\nends with an incomplete instance of the Ursatz, then re-\npeats with a complete version.\nWe put these restrictions on the musical content in place\nbecause we expected that if S CHENKER 41 were to be used\nfor supervised machine learning, such algorithms would be\nable to better model a corpus with less variability among\nthe pieces.\nOverall, S CHENKER 41 contains 253 measures of music\nand 907 notes. The lengths of individual excerpts ranged\nfrom 6 to 53 notes.\n2.2 Encoding\nWith our selected musical excerpts and our corresponding\nanalyses in hand, we needed to translate the musical in-formation into machine-readable form. Musical data has\nmany established encoding schemes; we used MusicXML,\na format that preserves more information from the original\nscore than say, MIDI.\nTranslating the Schenkerian analyses proved harder be-\ncause there is no current standard for storing such analy-\nses in a format that a computer could easily process and\nmanipulate. Therefore, we devised a text-based encoding\nscheme to represent the various notations found in a Schen-\nkerian analysis. Each analysis is stored in a single text ﬁle\nthat is linked to a speciﬁc MusicXML ﬁle containing the\nmusical excerpt being analyzed.\nSchenkerian analyses are primarily based on the con-\ncept of a prolongation, a situation where an analyst deter-\nmines that a group of notes is elaborating a group of more\nstructurally fundamental notes. Consider the descending\nmelodic pattern D–C–B–F]–G all occurring over G major\nharmony, as is shown in Figure 1. We could imagine that\nan analyst would determine that this passage outlines a de-\nscending G-major triad (D–B–G), with the second note C\n(a passing tone) serving to melodically connect the preced-\ning D to the following B. We would say the note C prolongs\nthe motion from D to B. Similarly, the F] prolongs the mo-\ntion from B to G. Schenkerian analysis hypothesizes that\nany tonal composition is structured as a nested collection\nof prolongations; identifying them is a important compo-\nnent of the analysis procedure.\nEvery prolongation identiﬁed in an analysis is encoded\nin the analysis text ﬁle using the syntax X(Y)Z, where\nXandZare individual notes in the score and Yis a non-\nempty list of notes. Such a statement means that the notes\ninYprolong the motion from note Xto note Z. Addi-\ntionally, we permit incomplete prolongations in the text\nﬁle representation: one of XorZmay be omitted. The\nnotes of X,YandZare transcribed in the text ﬁle as is\nshown in Figure 1, with a measure number, followed by a\npitch and octave, followed by a integer to distinguish be-\ntween repeated notes in the same measure. Figure 2 shows\nhow the prolongations of Figure 1 would be encoded. Note\nthat the prolongation involving the F] is encoded with no\nXcomponent; this tell us that there is no strong melodic\nconnection from the B to the F], only from the F] to the G.\nFigure 1. A melodic sequence with note names.\n1d5-1 (1c5-1) 1b4-1\n(1f#4-1) 2g4-1\n1d5-1 (1b4-1) 2g4-1\nFigure 2. An encoding of the prolongations present.\nThis text format easily supports encoding prolongations\nat differing hierarchical levels in the music. We can see\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n214how Figure 2 encodes both the “surface-level” prolonga-\ntions D–C–B and F]–G, but also the deeper prolongation\nD–B–G which outlines the ﬁfth relationship in the G-major\nchord.\nAside from prolongations, the encoding system supports\ndescribing repetitions of notes that may be omitted in the\nanalysis on the printed page; any linear progressions, in-\ncluding instances of the Ursatz; and the harmonic context\npresent at any point in the analysis.\n2.3 Compromises\nAn additional challenge not previously mentioned in creat-\ning the S CHENKER 41 analyses is choosing an appropriate\nlevel of detail of the material to encode. Because the main\nobjects in analyses are prolongations, it is natural to at-\ntempt to group them into categories like “neighbor tone”\nand “passing tone.” However, not all prolongations iden-\ntiﬁed in analyses are easily categorized, and so category\nlabels are often omitted in analyses not found in an educa-\ntional context. This raises the question of whether or not to\nattempt to encode the category of prolongations in this cor-\npus. To avoid the risk of incorrectly interpreting analyses,\nwe have chosen to encode only what is directly observ-\nable on the printed page — the hierarchical relationship\nbetween groups of notes — and not categorize the prolon-\ngations found in the analyses. We recognize that this is a\ncompromise between staying true to the data and encoding\nall potentially useful information.\n3. USAGE OF THE DATA\nThe S CHENKER 41 data set enables the undertaking of a\nwide variety of tasks and studies. In addition to the already-\ndiscussed endeavors of using the corpus for supervised ma-\nchine learning or for quantitative evaluation, we theorize\nthat with these data it could be possible to address the fol-\nlowing questions:\n\u000fDo analysts identify certain types of prolongations\nmore often than others under certain circumstances?\nThese circumstances may involve the composer, mu-\nsical genre, or even the analysis source.\n\u000fDoes Schenkerian analysis align well with other\nforms of music analysis, such as Narmour’s\nimplication-realization model of melodic expecta-\ntion [8]?\n\u000fHow well do Schenkerian analyses align with ex-\npressive performances of the music [10]? Do fea-\ntures of a performance such as phrasing, volume,\nor other quantiﬁable measures of musicality corre-\nspond to various Schenkerian annotations in an anal-\nysis?\nBesides answering questions about Schenkerian analy-\nsis itself, we hope that that the availability of S CHENKER 41\nwill spur others to study the utility of Schenkerian analy-\nsis in other areas of music informatics. For instance, we\nsuspect hierarchical analyses could prove useful in con-\nstructing musical similarity metrics, because Schenkeriananalyses may highlight a common melodic pattern residing\nunder the surface in two different musical excerpts.\nThough the S CHENKER 41 analyses can be directly pro-\ncessed by software, the nature of the ﬂat text ﬁle format in\nwhich the data are encoded makes it difﬁcult to see hierar-\nchical relationships between notes not directly related by a\nsingle prolongation. Therefore, in this section we describe\nan algorithm to translate the analysis text ﬁles into hierar-\nchical graph structures known as MOPs. It is possible to\nuse the S CHENKER 41 data in MOP form to automatically\nlearn characteristics of Schenkerian analysis [5].\n3.1 Maximal Outerplanar Graphs\nMaximal outerplanar graphs, or MOPs, were ﬁrst proposed\nby Yust [11] as elegant structures for representing a set of\nmusical prolongations in a Schenkerian-style hierarchy. A\nMOP represents a hierarchy of melodic intervals located\nin a monophonic sequence of notes, though Yust proposed\nsome extensions for polyphony. For example, the prolon-\ngations mentioned in Figures 1 and 2 are represented by\nthe MOP shown in Figure 3.\nDGBCF#\nFigure 3. A MOP representation of the music in Figure 1.\nFormally, a MOP is a complete triangulation of a poly-\ngon, where the vertices of the polygon are notes and the\nouter perimeter of the polygon consists of the melodic in-\ntervals between consecutive notes of the original music,\nexcept for the edge connecting the ﬁrst note to the last,\nwhich is called the root edge. Each triangle in the poly-\ngon speciﬁes a prolongation. For instance, in Figure 3, the\npresence of triangle D–C–B means that the melodic mo-\ntion from D to B is prolonged by the C. By expressing the\nhierarchy in this fashion, each edge (x; y )carries the in-\nterpretation that notes xandyare “consecutive” at some\nlevel of abstraction of the music. Edges closer to the root\nedge express more abstract relationships than edges farther\naway.\nOuterplanarity is a property of a graph that can be drawn\nsuch that all the vertices are on the perimeter of the graph.\nSuch a condition is necessary for us to enforce the strict hi-\nerarchy among the prolongations. A maximal outerplanar\ngraph cannot have any additional edges added to it with-\nout destroying the outerplanarity; such graphs are neces-\nsarily polygon triangulations, and under this interpretation,\nall prolongations must occur over triples of notes.\nThere are three representational issues with MOPs we\nmust address before discussing the algorithm to convert an\nanalysis text ﬁle into MOPs. First, Schenkerian analyses\nas commonly encountered often include prolongations in-\nvolving more than three notes. The analysis sources used\nin S CHENKER 41 are no exception. For this reason, we re-\nlax the “maximal” qualiﬁer for MOPs and permit prolon-\ngations involving any number of notes in our MOP repre-\nsentation. A prolongation involving more than three notes\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n215will be translated into a polygon with more than three edges\nin the MOP representation.\nSecond, MOPs do not have a direct way to represent\na prolongation with only a single “parent” note. Because\nMOPs model prolongations as a way of moving from one\nmusical event toanother event, every prolongation must\nhave two parent notes. Music sometimes presents situa-\ntions, however, that an analyst would model with a one-\nparent prolongation, such as an incomplete neighbor tone\n(we encountered this situation in Figure 2). Yust interprets\nsuch prolongations as having a “missing” origin or goal\nnote that has been elided with a nearby structural note,\nwhich substitutes in the MOP for the missing note.\nThe third representational issue stems from trying to\nrepresent prolongations involving the ﬁrst or last note in\nthe music. Prolongations necessarily take place over time,\nand in a MOP, we interpret the temporally middle notes\nas prolonging the motion from the earliest note (the left\nparent) to the latest (the right parent). Following this tem-\nporal logic, we can infer that the root edge of a MOP must\ntherefore necessarily be between the ﬁrst note of the music\nand the last, implying these are the two most structurally\nimportant notes of a composition. As this is not always\ntrue in compositions, we add two pseudo-events to every\nMOP: an initiation event that is located temporally before\nthe ﬁrst note of the music, and a termination event, which\nis temporally positioned after the last note. The root edge\nof a MOP is ﬁxed to always connect the initiation event\nand the termination event. These extra events allow for\nany melodic interval — and therefore any pair of notes in\nthe music — to be represented as the most structural event\nin the composition. For instance, in Figure 4, which shows\nthe D–C–B–F]–G pattern with initiation and termination\nevents (labeled S TART and F INISH ), the analyst has indi-\ncated that the G is the most structurally signiﬁcant note in\nthe passage, as this note prolongs the motion along the root\nedge.\nDGBCF#STARTFINISH\nFigure 4. A MOP containing initiation and termination\nevents.\n3.2 Converting the Corpus to MOPs\nWe now present an algorithm to convert a text ﬁle anal-\nysis like those in S CHENKER 41 to a collection of MOPs.\nBecause a single MOP only represents a monophonic se-\nquence of notes, we may need multiple MOPs to store all\nof the prolongations in a single text ﬁle analysis. Most of\nthe analyses in S CHENKER 41 contain at least two MOPs,\none representing the structure of the main melody, and one\nrepresenting structure of the bass line.\nThe algorithm operates in three phases. In the ﬁrst phase,\nwe make a pass through the analysis text ﬁle to identify\nwhich notes will belong to which MOPs. We do this by\ncreating a temporary graph structure consisting of all thenotes present in the analysis and initially no edges. For\neach prolongation in the analysis ﬁle X(Y)Z, we add the\nedges (X; Z )and (i; Z ) for each note iin the set of notes\nY. After processing every prolongation, every connected\ncomponent in the graph will correspond to a single MOP.\nPhase two adds edges to the MOP for all two-parent\nprolongations. For each MOP graph identiﬁed in phase\none, we ﬁrst remove all the edges, then create a “skeleton”\nMOP structure consisting of edges connecting only con-\nsecutive notes in the music, plus the additional edges in-\nvolving the S TART and F INISH vertices. Figure 5(a) illus-\ntrates this skeletal structure for the prolongations described\nin Figure 2. We then create edges in the MOP correspond-\ning to all prolongations in the analysis text ﬁle that have\ntwo parent notes. Adding appropriate edges is straightfor-\nward: for a prolongation X(Y)Z, we add an edge from\nnoteXto the ﬁrst note of the set of notes Y, an edge from\nthe last note of Yto note Z, and an edge from XtoZ.\nIf the consecutive notes of Yare not already connected to\neach other by edges, we also add such edges. At the end of\nphase two, we would have a structure like in Figure 5(b).\nDGBCF#STARTFINISHDGBCF#STARTFINISH(a)(b)\nFigure 5. The (a) beginning and (b) end of phase two of\ncreating a MOP.\nPhase three involves adding edges in the MOP for one-\nparent prolongations, i.e., prolongations in the analysis text\nﬁle of the form X(Y)or(Y)Z. We begin by adding\nedges between consecutive notes of Yas in phase two.\nThe next step is identifying any additional edges neces-\nsary to enforce that the notes of Yshould be lower in the\nhierarchy than XorZ, whichever parent note is present.\nFortunately, it is guaranteed that every one-parent prolon-\ngation will fall into one of the six categories described be-\nlow, each of which we handle separately. We brieﬂy de-\nscribe the six categories here, and their processing steps\nare fully described in the pseudocode of Algorithm 1. The\ncode refers to the “smallest interior polygon” for a one-\nparent prolongation p, which is the smallest polygon in the\nMOP containing all the notes of p(the parent note and all\nof the child notes). This interior polygon will always exist\nin a MOP because MOPs express a strict hierarchy among\nthe notes, and therefore all the notes of a prolongation will\nbe found within a single polygon.\nCategory 1 corresponds to a one-parent prolongation\nmissing a right parent, where the MOP already contains\nan edge connecting the left parent Xto the ﬁrst note of\nY, and the edge in question already implies a hierarchical\nrelationship between XandY. In this situation, there are\nno extra edges to add because the necessary hierarchical\nrelationship already exists. Category 2 corresponds to the\nsame situation as Category 1, but reversed for a missing\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n216Algorithm 1\n1:procedure PROCESS -ONE-PARENT -PROLONGATIONS\n2: LetSbe the set of one-parent prolongations.\n3: while S6=;do\n4: p shortest length prolongation in S\n5: I identify smallest interior polygon containing all notes of p\n6: Assume vertices of Iare numbered 0...m \u00001\n7: ifleftParent(p) =I[0]and ﬁrstChildNote(p) =I[1]then .Category 1\n8: S S\u0000fpg .No additional edges needed; p’s children are already lower in the hierarchy\n9: else if rightParent(p) =I[m\u00001]and lastChildNote(p) =I[m\u00002]then .Category 2\n10: S S\u0000fpg .No additional edges needed; p’s children are already lower in the hierarchy\n11: else if leftParent(p) =I[0]then .Category 3\n12: Add edge (leftParent(p), ﬁrstChildNote(p)) to MOP; S S\u0000fpg\n13: else if rightParent(p) =I[m\u00001]then .Category 4\n14: Add edge (rightParent(p), lastChildNote(p)) to MOP; S S\u0000fpg\n15: else if rightParent(p) is missing then .Category 5\n16: newRight earliest I[x]such that I[x]is later than all of p’s children\n17: ifchoice of newRight increases length of prolongation pthen\n18: Update p’s length in S; defer processing\n19: else\n20: Add edge (leftParent(p), newRight) to MOP; S S\u0000fpg\n21: else if leftParent(p) is missing then .Category 6\n22: newLeft latest I[x]such that I[x]is earlier than all of p’s children\n23: ifchoice of newLeft increases length of prolongation then\n24: Update p’s length in S; defer processing\n25: else\n26: Add edge (newLeft, rightParent(p)) to MOP; S S\u0000fpg\nleft parent note.\nCategory 3 corresponds to a one-parent prolongation\nmissing a right parent, where the the MOP does notcon-\ntain an edge connecting the left parent Xto the ﬁrst note\nofY, but other nearby edges already imply a hierarchical\nrelationship between XandY. Here, we only need to add\nan edge from Xto the ﬁrst child note of Y. Category 4 cor-\nresponds to the same situation as Category 3, but reversed\nfor a missing left parent.\nCategory 5 corresponds to a one-parent prolongation\nmissing a right parent, where the the MOP does notcon-\ntain an edge connecting the left parent Xto the ﬁrst note\nofY, and no other edges in the MOP already imply a hi-\nerarchical relationship between XandY. In this situation\nwe must explicitly ﬁnd a suitable right parent note, which\nwe choose to be the temporally earliest note on the interior\npolygon that is later than all the notes of Y. Category 6 cor-\nresponds to the same situation as Category 5, but reversed\nfor a missing left parent.\n4. CONCLUSIONS\nIn this paper, we presented S CHENKER 41, the ﬁrst large-\nscale data set of musical compositions and corresponding\nSchenkerian analyses in a computer-processable format.\nWe anticipate that with the rise of corpus-driven research\nin music informatics, this data set will be of value to re-\nsearchers investigating various characteristics of Schenker-\nian analysis, using machine learning techniques to study\nthe analytical procedure, or harnessing the analyses for use\nin other music informatics tasks. We also presented an\nalgorithm for translating the analyses into MOPs, which\nserve as useful data structures for representing the hierar-\nchical organization of the analyses.5. REFERENCES\n[1] Matthew Brown. Explaining Tonality. University of\nRochester Press, 2005.\n[2] Allen Cadwallader and David Gagn ´e.Analysis of Tonal\nMusic: A Schenkerian Approach. Oxford University\nPress, Oxford, 1998.\n[3] Allen Forte and Steven E. Gilbert. Instructor’s Manual\nforIntroduction to Schenkerian Analysis. W. W. Nor-\nton and Company, New York, 1982.\n[4] Allen Forte and Steven E. Gilbert. Introduction to\nSchenkerian Analysis. W. W. Norton and Company,\nNew York, 1982.\n[5] Phillip B. Kirlin. A Probabilistic Model of Hierarchi-\ncal Music Analysis. PhD thesis, University of Mas-\nsachusetts Amherst, 2014.\n[6] Phillip B. Kirlin and David D. Jensen. Probabilistic\nmodeling of hierarchical music analysis. In Proceed-\nings of the 12th International Society for Music Infor-\nmation Retrieval Conference, pages 393–398, 2011.\n[7] Alan Marsden. Schenkerian analysis by computer: A\nproof of concept. Journal of New Music Research,\n39(3):269–289, 2010.\n[8] Eugene Narmour. Beyond Schenkerism: The Need for\nAlternatives in Music Analysis. University of Chicago\nPress, 1977.\n[9] Tom Pankhurst. SchenkerGUIDE: A Brief Handbook\nand Website for Schenkerian Analysis. Routledge, New\nYork, 2008.\n[10] Christopher Raphael. Symbolic and structural repre-\nsentation of melodic expression. In Proceedings of the\n10th International Society for Music Information Re-\ntrieval Conference, pages 555–560, 2009.\n[11] Jason Yust. Formal Models of Prolongation. PhD the-\nsis, University of Washington, 2006.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n217Composer Excerpt name Analysis source\nBach Minuet in G major, BWV Anh. 114, mm. 1–16 Expert\nBach Chorale 233, Werde munter, mein Gemute, mm. 1–4 Expert\nBach Chorale 317 (BWV 156), Herr, wie du willt, so schicks mit mir, mm. 1–5 F&G manual\nBeethoven Seven Variations on a Theme by P. Winter, WoO 75,\nVariation 1, mm.1–8 C&G\nBeethoven Seven Variations on a Theme by P. Winter, WoO 75,\nTheme, mm. 1–8 C&G\nBeethoven Ninth Symphony, Ode to Joy theme from ﬁnale (8 measures) SG\nBeethoven Piano Sonata in F minor, Op. 2, No. 1, Trio, mm. 1–4 SG\nBeethoven Seven Variations on God Save the King, Theme, mm. 1–6 SG\nChopin Mazurka, Op. 17, No. 1, mm. 1–4 SG\nChopin Grande Valse Brilliante, Op. 18, mm. 5–12 SG\nClementi Sonatina for Piano, Op. 38, No. 1, mm. 1–2 SG\nHandel Trio Sonata in B-ﬂat major, Gavotte, mm. 1–4 Expert\nHaydn Divertimento in B-ﬂat major, Hob. 11/46, II, mm. 1–8 F&G\nHaydn Piano Sonata in C major, Hob. XVI/35, I, mm. 1–8 F&G\nHaydn Twelve Minuets, Hob. IX/11, Minuet No. 3, mm. 1–8 SG\nHaydn Piano Sonata in G major, Hob. XVI/39, I, mm. 1–2 SG\nHaydn Hob. XVII/3, Variation I, mm. 19–20 SG\nHaydn Hob. I/85, Trio, mm. 39–42 SG\nHaydn Hob. I/85, Menuetto, mm. 1–8 SG\nMozart Piano Sonata 11 in A major, K. 331, I, mm. 1–8 F&G\nMozart Piano Sonata 13 in B-ﬂat major, K. 333, III, mm. 1–8 F&G manual\nMozart Piano Sonata 16 in C major, K. 545, III, mm. 1–8 F&G manual\nMozart Six Variations on an Allegretto, K. Anh. 137, mm. 1–8 F&G manual\nMozart Piano Sonata 7 in C major, K. 309, I, mm. 1–8 C&G\nMozart Piano Sonata 13 in B-ﬂat major, K. 333, I, mm. 1–4 F&G\nMozart 7 Variations in D major on “Willem van Nassau,” K. 25,\nmm. 1–6 SG\nMozart Twelve Variations on “Ah vous dirai-je, Maman,” K. 265,\nVar. 1, mm. 23–32 SG, C&G\nMozart 12 Variations in E-ﬂat major on “La belle Franc ¸oise,” K. 353,\nTheme, mm. 1–3 SG\nMozart Minuet in F for Keyboard, K. 5, mm. 1–4 SG\nMozart 8 Minuets, K. 315, No. 1, Trio, mm. 1–8 SG\nMozart 12 Minuets, K. 103, No. 4, Trio, mm. 15–16 SG\nMozart 12 Minuets, K. 103, No. 3, Trio mm. 7–8, SG\nMozart Untitled from the London Sketchbook, K. 15a, No. 1, mm. 12–14 SG\nMozart 9 Variations in C major on “Lison dormait,” K. 264,\nTheme, mm. 5–8 SG\nMozart 12 Minuets, K. 103, No. 12, Trio, mm. 13–16 SG\nMozart 12 Minuets, K. 103, No. 1, Trio, mm. 1–8 SG\nMozart Piece in F for Keyboard, K. 33B, mm. 7–12 SG\nSchubert Impromptu in B-ﬂat major, Op. 142, No. 3, mm. 1–8 F&G manual\nSchubert Impromptu in G-ﬂat major, Op. 90, No. 3, mm. 1–8 F&G manual\nSchubert Impromptu in A-ﬂat major, Op. 142, No. 2, mm. 1–8 C&G\nSchubert Wanderer’s Nachtlied, Op. 4, No. 3, mm. 1–3 SG\nTable 1. The musical excerpts contained in S CHENKER 41.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n218"
    },
    {
        "title": "Automated Detection of Single- and Multi-Note Ornaments in Irish Traditional Flute Playing.",
        "author": [
            "Münevver Köküer",
            "Peter Jancovic",
            "Islah Ali-MacLachlan",
            "Cham Athwal"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415266",
        "url": "https://doi.org/10.5281/zenodo.1415266",
        "ee": "https://zenodo.org/records/1415266/files/KokuerJAA14.pdf",
        "abstract": "This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented orna- ment detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamenta- tion. We employed onset detection methods based on sig- nal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by ad- jacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evalua- tions are performed on monophonic flute recordings from Grey Larsen’s CD, which was manually annotated by an experienced flute player. The onset and single- and multi- note ornament detection performance is presented in terms of the precision, recall and F-measure.",
        "zenodo_id": 1415266,
        "dblp_key": "conf/ismir/KokuerJAA14",
        "keywords": [
            "automatic system",
            "single- and multi-note ornaments",
            "Irish traditional flute playing",
            "onset detection",
            "fundamental frequency",
            "musical ornamentation",
            "duration",
            "pitch",
            "experimental evaluations",
            "monophonic flute recordings"
        ],
        "content": "AUTOMATED DETECTION OF SINGLE- AND MULTI-NOTE\nORNAMENTS IN IRISH TRADITIONAL FLUTE PLAYING\nM¨unevver K ¨ok¨uer1;2, Peter Jan ˇcoviˇc2, Islah Ali-MacLachlan1, Cham Athwal1\n1DMT Lab, Birmingham City University, UK\n2School of Electronic, Electrical & Systems Engineering, University of Birmingham, UK\nfmunevver.kokuer, islah.ali-maclachlan, cham.athwalg@bcu.ac.uk\np.jancovic@bham.ac.uk\nABSTRACT\nThis paper presents an automatic system for the detection\nof single- and multi-note ornaments in Irish traditional ﬂute\nplaying. This is a challenging problem because ornaments\nare notes of a very short duration. The presented orna-\nment detection system is based on ﬁrst detecting onsets\nand then exploiting the knowledge of musical ornamenta-\ntion. We employed onset detection methods based on sig-\nnal envelope and fundamental frequency and customised\ntheir parameters to the detection of soft onsets of possibly\nshort duration. Single-note ornaments are detected based\non the duration and pitch of segments, determined by ad-\njacent onsets. Multi-note ornaments are detected based on\nanalysing the sequence of segments. Experimental evalua-\ntions are performed on monophonic ﬂute recordings from\nGrey Larsen’s CD, which was manually annotated by an\nexperienced ﬂute player. The onset and single- and multi-\nnote ornament detection performance is presented in terms\nof the precision, recall and F-measure.\n1. INTRODUCTION\nWithin Irish traditional music, ornaments are used exten-\nsively by all melody instruments. They are central to the\nstyle of the music, adding to its liveliness and expression.\nAmongst traditional players, the melody is merely a frame-\nwork [3, 4] – dynamics, ornamentation and context will be\nadded in real time. This is often different from classical\nmusic where a standard notation for each piece of music\nusually includes ornaments as written by the composer.\nOrnaments are notes of a very short duration. They can\nbe categorised into single-note and multi-note ornaments.\nSingle-note ornaments are amongst the most common in\nIrish traditional music. Multi-note ornaments consist of a\nspeciﬁc sequence of note and single-note ornaments.\nc\rM¨unevver K ¨ok¨uer1;2, Peter Jan ˇcoviˇc2, Islah Ali-\nMacLachlan1, Cham Athwal1.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: M¨unevver K ¨ok¨uer1;2, Pe-\nter Jan ˇcoviˇc2, Islah Ali-MacLachlan1, Cham Athwal1. “AUTOMATED\nDETECTION OF SINGLE- AND MULTI-NOTE ORNAMENTS IN\nIRISH TRADITIONAL FLUTE PLAYING”, 15th International Society\nfor Music Information Retrieval Conference, 2014.Methods for ornament detection are typically based on\ndetection of note onsets. Note onsets may be categorised\nas hard or soft. A hard onset, typical in percussive instru-\nments, is characterised by a sudden change in energy. A\nsoft onset shows a more gradual change in energy and it\noccurs in wind instruments, like ﬂute. A variety of meth-\nods have been proposed for the detection of note onsets\nin music recordings, e.g., [1, 8, 11, 13, 17]. The methods\ntypically exploit the change in the energy of the signal,\nwhich may be estimated in temporal or spectral domain.\nThe use of phase has also been investigated, e.g., [1, 11],\nand combined with the fundamental frequency in [11]. It\nhas been reported that reliable note onset detection for non-\npercussive instruments is more difﬁcult to obtain due to the\nsoft nature of the onsets [11].\nAn automated detection of ornaments is a challenging\nproblem. This is because ornaments are of very short du-\nrations, which may cause them being easily omitted or\nfalsely detected. Unlike note onset detection, this research\narea has received relatively little attention. An automatic\nlocation of ornaments for ﬂute recordings based on MPEG-\n7 features was investigated in [5]. Transcription of baroque\nornaments in two piano recordings by analysing rhythmic\ngroupings and expressive timing was studied in [2]. This\nwork used onset values from manually edited time-tagged\naudio. Several works employed spectral-domain energy-\nbased onset detection, e.g., [9, 10, 16]. The work in [16]\nanalysed ornamentation from Bassoon recordings. The work\nof a group from Dublin Institute of Technology, summarised\nin [9], is the only study on the detection of ornaments in\nIrish traditional ﬂute music. This provided only some ini-\ntial results and on a considerably smaller dataset.\nIn this paper, we extend our recent work presented in [14]\nand investigate automatic detection of single- and multi-\nnote ornaments in ﬂute playing. The presented ornament\ndetection system is based on ﬁrst detecting onsets and then\nexploiting knowledge of musical ornamentation. We ex-\nplore the use of several different methods for onset detec-\ntion and customisation of their parameters to detection of\nsoft onsets of notes which may be also of very short du-\nration. The detected onsets provide segmentation of the\nsignal, where a segment is deﬁned by the adjacent detected\nonsets. This segmentation, together with the musical knowl-\nedge of ornamentation is then used for the detection of\nsingle- and multi-note ornaments. Experimental evalua-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n15tions are performed using recordings of Irish traditional\ntunes played by ﬂute from Grey Larsen’s CD [15]. Re-\nsults of ornament detection are presented in terms of the\nprecision, recall and F-measure. The average F-measure\nperformance for single- and multi-note ornaments is over\n76% and 67%, respectively.\n2. SINGLE- AND MULTI-NOTE ORNAMENTS IN\nIRISH TRADITIONAL FLUTE PLAYING\nOrnaments are used as embellishments in Irish traditional\nmusic [15]. They are notes of a very short duration, created\nthrough the use of special ﬁngered articulations.\nSingle-note ornaments, namely ‘cut’ and ‘strike’, are\npitch articulations. The ‘cut’ involves quickly lifting and\nreplacing a ﬁnger from a tonehole, and corresponds to a\nhigher note than the ornamented note. The ‘strike’ is per-\nformed by momentarily closing an open hole, and corre-\nsponds to a lower note than the ornamented note.\nMulti-note ornaments are successive use of single-note\nornaments. To simplify the description, we refer to the\nornamented note as the base note throughout the rest of\nthis paper. The ‘roll’ consists of the base note, a ‘cut’,\nbase note, a ‘strike’ and then returning to the base note. A\nshorter version of the roll, referred to as short-roll, omits\nthe starting base note. The ‘crann’ consists of the base note\nthat is cut three times in rapid succession and then return-\ning to the base note. The short-crann omits the starting\nbase note. The ‘shake’ commences with a ‘cut’, followed\nby a base note and a second ‘cut’ and then returning to the\nbase note.\nA schematic visualisation of the single- and multi-note\nornaments is given in Figure 1. In the multi-note orna-\nments ﬁgure, the proportions of the length of the individ-\nual parts aim to approximately indicate the typical duration\nproportions. For instance, in theory, a roll would be split\nequally into three parts by the cut and the strike but in real-\nity different players will time this differently according to\nthe ‘swing’ of the tune, their muscle control and a host of\nother attributes that make up their personal style.\n3. AUTOMATIC DETECTION OF ORNAMENTS\nThis section presents the developed automatic ornament\ndetection system. We ﬁrst give a brief description of the\nonset detection methods we employed and then describe\nhow the detected onsets are used for the detection of single-\nand multi-note ornaments.\n3.1 Methods for detection of onsets\nHere we brieﬂy describe three onset detection methods we\nemployed. Two of the methods exploit the change of the\nsignal amplitude over time, with processing performed in\nthe temporal and spectral domain [1, 8]. The third method\nis based on the fundamental frequency [6, 11]. Each of\nthe method requires several parameters to be set and their\nvalues are explored during experimental evaluations and\npresented later in Section 4.3. The implementation of the\nFrequency\nTimeFrequency\nTime‘Cut’\n‘Strike’\n‘Crann’\n‘Roll’\n‘Shake’\n(b)(a)Figure 1. A schematic representation of single-note (a)\nand multi-note (b) ornaments.\ntemporal domain energy-based method used in parts some\nfunctions from the MIRtoolbox.\n3.1.1 Signal energy: spectral domain\nThis method, also sometimes referred to as spectral-ﬂux\nmethod, performs onset detection in the spectral domain.\nThe signal is segmented into overlapping frames. Each\nsignal frame is multiplied by Hamming window. The win-\ndowed frames are then zero padded and the Fourier trans-\nform is applied to provide the short-term Fourier spectrum.\nFor each frequency bin, the differences between the short-\nterm magnitude spectra of successive signal frames is com-\nputed. This is then half-wave rectiﬁed and the L2norm is\ncalculated to provide the value of the detection function\nat the current frame. The peaks of the detection function,\nwhose amplitude is above a threshold are used as the de-\ntected onsets. We explored the use of a ﬁxed threshold\nvalue as well as computing the value adaptively based on\nthe median of the detection function values around the cur-\nrent frame. Finally, if two consecutive peaks are found\nwithin a given time distance, only the ﬁrst peak is used.\n3.1.2 Signal energy: temporal domain\nAnother method we employed performs the detection in\ntemporal domain. The signal is passed through a bank of\nfourteen band pass ﬁlters, each tuned to a speciﬁc note on\nthe ﬂute in the range from D4toB5. The ﬁlters have\nnon-overlapping bands, with the lower and the upper fre-\nquency being half way between the adjacent note frequen-\ncies. These fourteen notes are readily playable on an un-\nkeyed concert ﬂute. The signal in each band is full-wave\nrectiﬁed and then smoothed, resulting in amplitude enve-\nlope. The time derivative of the amplitude envelope is cal-\nculated in each band and this is smoothed by convolving\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n16it with a half-Hanning window. We explored several ways\nof making decision about detected onsets. The information\nfrom all bands can be combined by summing together their\nsmoothed derivative signals. Alternatively, a single band\ncan be chosen as a representative at each time based on as-\nsessment of amplitudes of peaks around that time across\nall bands. Onsets are obtained by comparing the values of\npeaks to a threshold, which may be ﬁxed or adaptive over\ntime.\n3.1.3 Fundamental frequency\nIn addition to methods exploiting the signal envelope, we\nalso explore the use of the fundamental frequency (F 0).\nThis has been reported to be beneﬁcial for soft onset de-\ntection in [11]. Among a large variety of existing F0esti-\nmation algorithms, we employed the YIN algorithm [7] in\nthis work. The F0estimation may result in so called dou-\nbling / halving errors. To help dealing with these errors, the\nF0estimates are postprocessed using a median ﬁlter. The\nlength of this ﬁlter needs to be set sensitively – a longer ﬁl-\nter may be preferable to deal with the F0estimation errors\nbut this may also cause ﬁltering out ornaments, which are\ncharacterised by their short duration.\nThe detection function at the frame time n, denoted as\nRn, is based on calculating the change of F0over time.\nThis can be performed by taking the difference between\ntheF0estimate at the frame (n+ \u0002) and(n\u0000\u0002). The on-\nset is detected as the ﬁrst frame for which abs(R n)>\u000bF0,\nwhere the value of the threshold \u000bF0relates to the differ-\nence between frequencies of the closest possible notes.\n3.2 Ornament detection\nThe detected onsets, as obtained using the methods de-\nscribed in Section 3.1, provide a segmentation of the sig-\nnal, where each segment is formed based on the adjacent\ndetected onsets.\nWe characterise each detected segment by some fea-\ntures, speciﬁcally, here we use the duration of the segment\nand its segmental fundamental frequency. For a given seg-\nment, its duration, denoted by Dseg, is obtained based on\nthe detected onsets and its fundamental frequency, denoted\nbyFseg\n0, is calculated as the median value of the F0s corre-\nsponding to all signal frames assigned to that segment. Fi-\nnally, these segment features are used to determine whether\nthe detected segment corresponds to a note or a single-\nnote ornament and whether the sequence of segments cor-\nresponds to a multi-note ornament, and if single- or multi-\nnote ornament is detected, then to determine its type.\n3.2.1 Single-note ornament detection\nAs single-note ornaments are expected to be of a shorter\nduration than notes, we examined whether the duration of\nthe detected segments can be used to discriminate these\nornaments from notes. We conducted statistical analysis\nof the duration of notes and single-note ornaments in our\nrecordings. This was performed using the manual onset\nannotations. The obtained distributions of the durations\nare depicted in Figure 2 – these indicate that the durationcan indeed provide a good discrimination between notes\nand ornaments. Based on these results, we consider that\na segment is classiﬁed as a single-note ornament when its\nduration is below 90 ms, otherwise it is classiﬁed as a note.\n0 0.02 0.04 0.06 0.08 0.100.050.10.150.2\nDuration (sec)Proportion\n0 0.2 0.4 0.6 0.800.050.10.150.20.25\nDuration (sec)Proportion\n(a) (b)\nFigure 2. The distribution of the duration of single-note\nornaments (a) and notes (b) obtained using the develop-\nment set.\nThe decision whether the detected single-note ornament\nis a ‘cut’ or ‘strike’ can be made based on comparing the\nvalues of the Fseg\n0of the current and the following seg-\nment. This reﬂects the musical knowledge of ornamenta-\ntion. IfFseg\n0of the segment detected as ornament is higher\nthanFseg\n0of the following segment, the ornament is clas-\nsiﬁed as ‘cut’ and as ‘strike’ otherwise.\n3.2.2 Multi-note ornament detection\nThe detection of multi-note ornaments, namely ‘crann’,\n‘roll’ and ‘shake’, is based on analysing the features of a\nsequence of detected consecutive segments. We used a set\nof rules to determine whether the sequence corresponds to\none of the multi-note ornament types or not. These rules\nreﬂect the deﬁnition of the multi-note ornaments as pre-\nsented in Section 2 and for each ornament type are de-\nscribed below. Let us consider that rdenotes the index of\nthe ﬁrst segment in the sequence of detected segments we\nare currently analysing. Let us denote by \u0001Fseg\n0(j;j+ 2)\nthe difference between the Fseg\n0for the segment (r+j)\nandFseg\n0for the segment (r+j+ 2) , wherejis an index\nto be set.\n‘Crann’ is detected if the following is fulﬁlled: i) the\nsequence of Fseg\n0follows the pattern ‘BHBHBHB’, where\n‘B’ stands for a base note and ‘H ’ for a note higher than the\nbase note; ii) the segmental Fseg\n0is similar for segments\ncorresponding to the base note, i.e., the \u0001Fseg\n0(j;j+ 2) is\nwithin the given tolerance range \fF0whenjis individually\nset to 0, 2, and 4; and iii) the segment duration Dsegis\nbelow\fDfor segments given by setting jfrom 1 to 5 and\nis above\fDforjset to 0 and 6. The ‘Short-Crann’ is using\nthe same rules but taking into account that the starting base\nnote is omitted.\n‘Roll ’ is detected if the following is fulﬁlled: i) the se-\nquence ofFseg\n0follows the pattern ‘BHBLB’, where ‘L ’\nstands for a note lower than the base note; ii) the value of\n\u0001Fseg\n0(j;j+ 2) is within the tolerance range \fF0forjset\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n17to 0 and 2; and iii) the segment duration Dsegis above\fD\nforjbeing 0 and 2 and is below \fDwhenjis 1 and 3.\nAgain, the ‘Short-Roll ’ is using the same rules but taking\ninto account that the starting base note is omitted.\n‘Shake’ is detected if the following is fulﬁlled: i) the\nsequence of notes follows the pattern ‘HBHB’; ii) the value\nof\u0001Fseg\n0(j;j+ 2) is within a given tolerance range \fF0\nforjset to 1; and iii) the segment duration Dsegis below\n\fDwhenjis 1 and 2, and is above \fDwhenjis 3.\nThe parameters \fF0and\fDwere set to 20 Hz (except\nfor ‘crann’ when 30 Hz was used) and 90 ms, respectively.\n4. EXPERIMENTAL RESULTS\n4.1 Data description\nEvaluations are performed using recordings of Irish tra-\nditional tunes and training exercises played by ﬂute from\nGrey Larsen’s CD which accompanied his book “Essential\nGuide to Irish Flute and Tin Whistle” [15]. The tunes are\nbetween 20 sec and 1 min 11 sec long. All recordings are\nmonophonic and are sampled at 44.1 kHz sampling fre-\nquency. Manual annotation of the recordings to indicate\nthe times of onsets and offsets and the identity of notes\nand ornaments was performed by the third author of this\npaper, who is a highly experienced musician with over 10\nyears of ﬂute playing. The manual annotation is used as\nthe ground truth in evaluations. The data was split into\nseparate development and evaluation sets. The develop-\nment set, consisting of 6 tunes (namely ‘Study5’, ‘Study6’,\n‘Study17’, ‘Lady on the Island’, ‘The Lonesome Jig’, ‘The\nDrunken Landlady’), was used for ﬁnding the best param-\neter values of onset detection methods. The evaluation set,\nconsisting of 13 tunes, was used to obtain the presented\nresults. The list of the tunes from the evaluation set, with\nthe number of notes and ornaments, is given in Table 1. In\ntotal, this set contains 3025 onsets, including notes and or-\nnaments. Out of these there are 301 single-note ornaments,\nconsisting of 257 cuts and 44 strikes, and 152 multi-note\nornaments, consisting of 117 rolls (including short-rolls),\n19 cranns (including short-cranns), and 16 shakes.\n4.2 Evaluation measures\nPerformance of the onset and ornament detection is evalu-\nated in terms of the precision (P ), recall (R ) andF-measure.\nThe deﬁnition of these measures is the same as used in\nMIREX onset detection evaluations, speciﬁcally,\nP=Ntp\nNtp+Nfp;R=Ntp\nNtp+Nfn;F=2PR\nP+R\nwhereNtpis the number of correctly detected onsets / or-\nnaments and NfpandNfnis the number of inserted and\ndeleted onsets / ornaments, respectively. The onset de-\ntection is considered as correct when it is within \u000650 ms\naround the onset annotation.\nThe single-note and multi-note ornaments are consid-\nered to be detected correctly when the onsets, correspond-\ning to the start and to the end of the ornament are within\n\u000650 ms and\u0006100 ms range, respectively.Tune Title Number of Time\nNotes Ornaments (sec.)\n(C-S-Ro-Cr-Sh)\nStudy 11 76 20–0–0–0–0 26\nStudy 22 127 0–28–0–0–0 47\nMaids of Ardagh 98 23–0–5–0–0 32\nHardiman the .. 112 12–0–7–1–0 28\nThe Whinny Hills .. 117 15–1–5–2–4 30\nThe Frost is All .. 151 27–2–12–0–0 41\nThe Humours of .. 289 59–7–12–14–0 82\nThe Rose in the .. 152 22–2–11–0–0 39\nScotsman over .. 153 18–0–9–2–0 38\nA Fig for a Kiss 105 17–3–6–0–2 28\nRoaring Mary 176 15–1–21–0–3 44\nThe Mountain Road 105 8–0–6–0–3 25\nThe Shaskeen 181 21–0–23–0–4 42\nTable 1. The list of tunes contained in the evaluation set,\nwith the number of onsets and ornaments and duration of\neach tune. The notation ‘C’, ‘S’, ‘Ro’, ‘Cr’ and ‘Sh’ stands\nfor ‘cut’, ‘strike’, ‘roll’, ‘crann’ and ‘shake’, respectively.\n4.3 Results of onset detection\nWe have performed extensive evaluations on the develop-\nment set with different parameter values for each of the\nonset detection method. The best values of parameters for\neach of the method are given in Table 2. The achieved\nperformance on the evaluation set using these parameters\nfor each method is presented in Table 3. Note that these\nresults include the onsets corresponding to both notes and\nornaments. Performance difference of less than 1% was\nobserved when the parameters were tuned speciﬁcally for\nthe evaluation set. It can be seen that all methods pro-\nvide good onset detection performance, with the F0-based\nmethod being slightly better than the energy-based meth-\nods. A method based on F0was shown to perform best for\nwind instruments also in [11], where it was also shown that\nits combination with other methods provided only slight\nimprovement at similar PandRvalues. As such, in the\nfollowing, we use only the F0-based method for evaluat-\ning the ornament detection performance. An example of a\nsignal extract from one of the tune and the corresponding\nF0estimate and the detection function, with indicated true\nlabel and detected onsets, are depicted in Figure 3.\n4.4 Results of single-note ornament detection\nThe results of single-note ornament detection are presented\nin Table 4 separately for ‘cut’ and ‘strike’. The achieved\ndetection performance is signiﬁcantly higher than that pre-\nsented in previous ﬂute studies using similar data [9]. The\nperformance for ‘cut’ is close to the overall onset detection\nperformance as presented in Table 3. The performance for\n‘strike’ is considerably lower than for ‘cut’. This has also\nbeen observed in previous research and may be due to the\nnature the ‘strike’ is created. There was 5 substitutions of\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n18Onset detection method with best values of the parameters\nsig-energy (spectral):\n– frame length of 1024 samples (23.2 ms)\n– frame shift of 896 samples (20.3 ms)\n– threshold set as ﬁxed at 2% of the maximum of the\nnormalised detection function\n– minimum distance between peaks set to 10 ms\nsig-energy (temporal):\n– half-Hanning window of 35 ms\n– threshold set as ﬁxed at 15% of the maximum of the\nnormalised detection function\n– minimum distance between peaks set to 20 ms\nF0:\n– frame length of 1024 samples (23.2 ms)\n– frame shift of 128 samples (2.9 ms)\n– median ﬁlter of length 9 frames\n– parameter \u0002set to 6 frames (17 ms)\n– parameter \u000bF0set to 10 Hz\nTable 2. Parameters of each onset detection method and\ntheir best values obtained based on the development set.\nAlgorithm Evaluation performance (%)\nPrecision Recall F-measure\nsig-energy (spectral) 94.9 85.0 89.7\nsig-energy (temporal) 87.9 88.6 88.3\nF0 89.1 92.9 91.0\nTable 3. Results of onset detection obtained by each of the\nemployed method.\ncut for strike and 1 substitution of strike for cut. These\nerrors were contributed by slight inaccuracies in onset de-\ntection andF0misestimation.\nSingle-note Ornament Detection\nPrecision (%) Recall (%) F-measure (%)\nCut 88.4 86.4 87.4\nStrike 63.8 68.2 65.9\nTable 4. Results of single-note ornament detection ob-\ntained by employing the F0-based onset detection method.\n4.5 Results of multi-note ornament detection\nExperiments for multi-note ornament detection were per-\nformed by analysing all the possible sequence patterns re-\nsulting from the detected segments – this consisted here\nof 3020 sequence pattern candidates. The results of multi-\nnote ornament detection are presented in Table 5 separately\nfor ‘roll’, ‘crann’ and ‘shake’. These results include also\nthe short versions for ‘roll’ and ‘crann’. It can be seen that\n0 0.5 1 1.5 2 2.5 3 3.5 4−101\nTime (sec)Amplitude\n0 200 400 600 800 1000 1200 140050010001500200025003000\nFrame−time indexFrequency (Hz)\n0 200 400 600 800 1000 1200 1400200400600800\nFrame−time indexFrequency (Hz)\n  \n0 200 400 600 800 1000 1200 1400050100150200\nFrame−time indexFrequency (Hz)F0\nF0 (filt)Figure 3. An extract from the tune ‘The Lonesome Jig’,\ndepicting (from top to bottom) the waveform, spectrogram,\nF0estimation (unﬁltered (red) and ﬁltered (dashed black))\nand the detection function with indicated detected onsets\n(blue2) and true label (magenta r).\nthe performance for ‘shake’ is considerably lower than that\nfor ‘roll’ and ‘crann’. This is due to the short sequence pat-\ntern of ‘shake’, consisting of only 4 parts, which makes it\nmore likely to be accidentally match with other note se-\nquence. We have also analysed the performance separately\nfor the short and normal versions of the ‘roll’ and ‘crann’\nornaments. This showed that the F-measure performance\nfor ‘roll’ was approximately 17% better than for ‘short-\nroll’. This trend was not observed for ‘short-crann’, which\nmay be due its longer note sequence.\n5. CONCLUSION AND FUTURE WORK\nIn this paper, we presented work on detection of single-\nand multi-note ornaments in Irish traditional ﬂute music.\nWe employed three different methods for onset detection\nand customised their parameter values to detecting soft on-\nsets of possibly very short notes. The method based on\nthe fundamental frequency (F 0) achieved around 91% on-\nset detection performance in terms of the F-measure and\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n19Multi-note Ornament Detection\nPrecision (%) Recall (%) F-measure (%)\nRoll 87.5 67.0 75.9\nCrann 86.7 68.4 76.5\nShake 50.0 50.0 50.0\nTable 5. Results of multi-note ornament detection ob-\ntained by employing the F0-based onset detection method.\noutperformed slightly the other two energy-based meth-\nods. TheF0-based method was then used for evaluating\nthe ornament detection performance. The discrimination\nbetween notes and single-note ornaments was based on\nthe duration of segments deﬁned by the adjacent detected\nonsets. The F0information of the current and the fol-\nlowing segment was used to distinguish between ‘cut’ and\n‘strike’ single-note ornaments. The achieved F-measure\nperformance for ‘cut’ was over 87%, while for ‘strike’ over\n65%. The multi-note ornament detection system was based\non analysing the properties of a sequence of detected seg-\nments. This included the sequential pattern of segmental\nF0’s, the duration of each segment, and the relationship\nof the segmental F0’s among the segments. The average\nF-measure performance over all types of multi-note orna-\nments was over 67%.\nThere are several points we are currently considering to\nextend this work. First, we plan to analyse the errors made\nby each of the onset detection methods and accordingly\nexplore whether their combination could lead to detection\nperformance improvements. This would also include ex-\nploration of the use of other onset detection methods, in-\ncluding other F0estimation algorithms and possible incor-\nporation of the sinusoidal detection method we presented\nin [12]. Second, we will explore a compensation for vari-\nations in tempo across the recordings. Finally, we plan to\nemploy probabilistic rules for detection of multi-note orna-\nments which should allow for better handling of the varia-\ntions due to player’s style.\nAcknowledgement\nThis work was supported by a project under the ‘Trans-\nforming Musicology’ programme funded by Arts and Hu-\nmanities Research Council (UK).\n6. REFERENCES\n[1] J. P. Bello, L. Daudet, S. Abdallah, Ch. Duxbury,\nM. Davies, and M. B. Sandler. A tutorial on onset de-\ntection in music signals. IEEE Trans. on Speech and\nAudio Processing, pages 1–13, 2005.\n[2] G. Boenn. Automated quantisation and transcription of\nmusical ornaments from audio recordings. In Proc. of\nthe Int. Computer Music Conf. (ICMC), pages 236–\n239, Copenhagen, Denmark, Aug. 2007.\n[3] B. Breathnach. Folk music and dances of Ireland . Os-\nsian, London, 1996.[4] C. Carson. Last night’s fun: in and out of time with\nIrish music. North Point Press, New York, 1997.\n[5] M. Casey and T. Crawford. Automatic location and\nmeasurement of ornaments in audio recording. In Proc.\nof the 5th Int. Conf. on Music Information Retrieval\n(ISMIR), pages 311–317, Barcelona, Spain, 2004.\n[6] N. Collins. Using a pitch detector for onset detection.\nInProc. of the 5th Int. Conf. on Music Information Re-\ntrieval (ISMIR), pages 100–106, Spain, 2005.\n[7] A. de Cheveigne and H. Kawahara. Yin, a fundamental\nfrequency estimator for speech and music. Journal of\nthe Acoustical Society of America, 111(4):1917–1930,\nApril 2002.\n[8] S. Dixon. Onset detection revisited. In Proc. of the 9th\nInt. Conf. on Digital Audio Effects (DAFx), pages 133–\n137, Montreal, Canada, Sep. 2006.\n[9] M. Gainza and E. Coyle. Automating ornamentation\ntranscription. In IEEE Int. Conf. on Acoustics, Speech\nand Signal Processing, Honolulu, Hawaii, 2007.\n[10] M. Gainza, E. Coyle, and B. Lawlor. Single-note or-\nnaments transcription for the Irish tin whistle based on\nonset detection. In Proc. of the Digital Audio Effects\n(DAFX), Naples, Italy, 2004.\n[11] A. Holzapfel, Y . Stylianou, A. C. Gedik, and\nB. Bozkurt. Three dimensions of pitched instrument\nonset detection. IEEE Trans. on Audio, Speech, and\nLanguage Processing, 18(6):1517–1527, Aug. 2010.\n[12] P. Jan ˇcoviˇc and M. K ¨ok¨uer. Detection of sinusoidal sig-\nnals in noise by probabilistic modelling of the spectral\nmagnitude shape and phase continuity. In Proc. of the\nIEEE Int. Conf. on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP), pages 517–520, Prague, Czech Re-\npublic, May 2011.\n[13] A. Klapuri. Sound onset detection by applying psy-\nchoacoustic knowledge. In Proc. of the IEEE Int.\nConf. on Acoustics, Speech, and Signal Processing\n(ICASSP), pages 3089–3092, March 1999.\n[14] M. K ¨ok¨uer, I. Ali-MacLachlan, P. Jan ˇcoviˇc, and\nC. Athwal. Automated detection of single-note orna-\nments in Irish traditional ﬂute playing. In Int. Workshop\non Folk Music Analysis, Istanbul, Turkey, June 2014.\n[15] G. Larsen. The Essential Guide to Irish Flute and\nTin Whistle. Mel Bay Publications, Paciﬁc, Missouri,\nUSA, 2003.\n[16] M. Puiggros, E. G ´omez, R. Ramirez, X. Serra, and\nR. Bresin. Automatic characterization of ornamenta-\ntion from bassoon recordings for expressive synthesis.\nIn9th Int. Conf. on Music Perception and Cognition,\nBologna, Italy, 2006.\n[17] E. D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. Journal of the Acoustical Society of\nAmerica, 103(1):588–601, 1998.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n20"
    },
    {
        "title": "Probabilistic Extraction of Beat Positions from a Beat Activation Function.",
        "author": [
            "Filip Korzeniowski",
            "Sebastian Böck",
            "Gerhard Widmer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415118",
        "url": "https://doi.org/10.5281/zenodo.1415118",
        "ee": "https://zenodo.org/records/1415118/files/KorzeniowskiBW14.pdf",
        "abstract": "We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker cur- rently uses for this purpose. Our experiments show im- provement upon the current method for a variety of data sets and quality measures, as well as better results com- pared to other state-of-the-art algorithms.",
        "zenodo_id": 1415118,
        "dblp_key": "conf/ismir/KorzeniowskiBW14",
        "keywords": [
            "probabilistic",
            "beat positions",
            "neural network",
            "greedy search",
            "replacement",
            "data sets",
            "quality measures",
            "improvement",
            "state-of-the-art",
            "algorithms"
        ],
        "content": "PROBABILISTIC EXTRACTION OF BEAT POSITIONS FROM A BEAT\nACTIVATION FUNCTION\nFilip Korzeniowski, Sebastian B ¨ock, and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nfilip.korzeniowski@jku.at\nABSTRACT\nWe present a probabilistic way to extract beat positions\nfrom the output (activations) of the neural network that is at\nthe heart of an existing beat tracker. The method can serve\nas a replacement for the greedy search the beat tracker cur-\nrently uses for this purpose. Our experiments show im-\nprovement upon the current method for a variety of data\nsets and quality measures, as well as better results com-\npared to other state-of-the-art algorithms.\n1. INTRODUCTION\nRhythm and pulse lay the foundation of the vast major-\nity of musical works. Percussive instruments like rattles,\nstampers and slit drums have been used for thousands of\nyears to accompany and enhance rhythmic movements or\ndances. Maybe this deep connection between movement\nand sound enables humans to easily tap to the pulse of a\nmusical piece, accenting its beats. The computer, however,\nhas difﬁculties determining the position of the beats in an\naudio stream, lacking the intuition humans developed over\nthousands of years.\nBeat tracking is the task of locating beats within an au-\ndio stream of music. Literature on beat tracking suggests\nmany possible applications: practical ones such as auto-\nmatic time-stretching or correction of recorded audio, but\nalso as a support for further music analysis like segmenta-\ntion or pattern discovery [4]. Several musical aspects hin-\nder tracking beats reliably: syncopation, triplets and off-\nbeat rhythms create rhythmical ambiguousness that is dif-\nﬁcult to resolve; varying tempo increases musical expres-\nsivity, but impedes ﬁnding the correct beat times. The mul-\ntitude of existing beat tracking algorithms work reasonably\nwell for a subset of musical works, but often fail for pieces\nthat are difﬁcult to handle, as [11] showed.\nIn this paper, we further improve upon the beat tracker\npresented in [2]. The existing algorithm uses a neural net-\nwork to detect beats in the audio. The output of this neural\nnetwork, called activations, indicates the likelihood of a\nc\rFilip Korzeniowski, Sebastian B ¨ock, Gerhard Widmer.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Filip Korzeniowski, Sebastian B ¨ock,\nGerhard Widmer. “Probabilistic Extraction of Beat Positions from a Beat\nActivation Function”, 15th International Society for Music Information\nRetrieval Conference, 2014.beat at each audio position. A post-processing step selects\nfrom these activations positions to be reported as beats.\nHowever, this method struggles to ﬁnd the correct beats\nwhen confronted with ambiguous activations.\nWe contribute a new, probabilistic method for this pur-\npose. Although we designed the method for audio with a\nsteady pulse, we show that using the proposed method the\nbeat tracker achieves better results even for datasets con-\ntaining music with varying tempo.\nThe remainder of the paper is organised as follows: Sec-\ntion 2 reviews the beat tracker our method is based on. In\nSection 3 we present our approach, describe the structure\nof our model and show how we infer beat positions. Sec-\ntion 4 describes the setup of our experiments, while we\nshow their results in Section 5. Finally, we conclude our\nwork in Section 6.\n2. BASE METHOD\nIn this section, we will brieﬂy review the approach pre-\nsented in [2]. For a detailed discourse we refer the reader\nto the respective publication. First, we will outline how\nthe algorithm processes the signal to emphasise onsets. We\nwill then focus on the neural network used in the beat\ntracker and its output in Section 2.2. After this, Section 3\nwill introduce the probabilistic method we propose to ﬁnd\nbeats in the output activations of the neural network.\n2.1 Signal Processing\nThe algorithm derives from the signal three logarithmi-\ncally ﬁltered power spectrograms with window sizes W\nof 1024, 2048 and 4096 samples each. The windows are\nplaced 441 samples apart, which results in a frame rate of\nfr= 100 frames per second for audio sampled at 44:1kHz.\nWe transform the spectra using a logarithmic function to\nbetter match the human perception of loudness, and ﬁlter\nthem using 3 overlapping triangular ﬁlters per octave.\nAdditionally, we compute the ﬁrst order difference for\neach of the spectra in order to emphasise onsets. Since\nlonger frame windows tend to smear spectral magnitude\nvalues in time, we compute the difference to the last, sec-\nond to last, and third to last frame, depending on the win-\ndow sizeW. Finally, we discard all negative values.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5130 2 4 6 8 10\nTime [s]0.00.10.20.30.40.50.6Activation(a) Activations of a piece from the Ballroom dataset\n0 2 4 6 8 10\nTime [s]0.000.010.020.030.040.050.06Activation (b) Activations of a piece from the SMC dataset\nFigure 1. Activations of pieces from two different datasets. The activations are shown in blue, with green, dotted lines\nshowing the ground truth beat annotations. On the left, distinct peaks indicate the presence of beats. The prominent\nrhythmical structure of ballroom music enables the neural network to easily discern frames that contain beats from those\nthat do not. On the right, many peaks in the activations do not correspond to beats, while some beats lack distinguished\npeaks in the activations. In this piece, a single woodwind instrument is playing a solo melody. Its soft onsets and lack of\npercussive instruments make detecting beats difﬁcult.\n2.2 Neural Network\nOur classiﬁer consists of a bidirectional recurrent neural\nnetwork of Long Short-Term Memory (LSTM) units, called\nbidirectional Long Short-Term Memory (BLSTM) recur-\nrent neural network [10]. The input units are fed with the\nlog-ﬁltered power spectra and their corresponding positive\nﬁrst order differences. We use three fully connected hidden\nlayers of 25 LSTM units each. The output layer consists of\na single sigmoid neuron. Its value remains within [0;1],\nwith higher values indicating the presence of a beat at the\ngiven frame.\nAfter we initialise the network weights randomly, the\ntraining process adapts them using standard gradient de-\nscent with back propagation and early stopping. We obtain\ntraining data using 8-fold cross validation, and randomly\nchoose 15% of the training data to create a validation set.\nIf the learning process does not improve classiﬁcation on\nthis validation set for 20 training epochs, we stop it and\nchoose the best performing neural network as ﬁnal model.\nFor more details on the network and the learning process,\nwe refer the reader to [2].\nThe neural network’s output layer yields activations for\nevery feature frame of an audio signal. We will formally\nrepresent this computation as mathematical function. Let\nNbe the number of feature frames for a piece, and N\u0014N=\nf1;2;:::;Ngthe set of all frame indices. Furthermore,\nlet\u001dnbe the feature vector (the log-ﬁltered power spectra\nand corresponding differences) of the nthaudio frame, and\n\u0007 = (\u001d1;\u001d2;:::;\u001dN)denote all feature vectors computed\nfor a piece. We represent the neural network as a function\n\t :N\u0014N![0;1]; (1)\nsuch that \t(n; \u0007) is the activation value for the nthframe\nwhen the network processes the feature vectors \u0007. We will\ncall this function “activations” in the following.\nDepending on the type of music the audio contains, the\nactivations show clear (or, less clear) peaks at beat posi-\ntions. Figure 1 depicts the ﬁrst 10 seconds of activationsfor two different songs, together with ground truth beat an-\nnotations. In Fig. 1a, the peaks in the activations clearly\ncorrespond to beats. For such simple cases, thresholding\nshould sufﬁce to extract beat positions. However, we often\nhave to deal with activations as those in Fig. 1b, with many\nspurious and/or missing peaks. In the following section,\nwe will propose a new method for extracting beat positions\nfrom such activations.\n3. PROBABILISTIC EXTRACTION OF BEAT\nPOSITIONS\nFigure 1b shows the difﬁculty in deriving the position of\nbeats from the output of the neural network. A greedy local\nsearch, as used in the original system, runs into problems\nwhen facing ambiguous activations. It struggles to correct\nprevious beat position estimates even if the ambiguity re-\nsolves later in the piece. We therefore tackle this problem\nusing a probabilistic model that allows us to globally opti-\nmise the beat sequence.\nProbabilistic models are a frequently used to process\ntime-series data, and are therefore popular in beat track-\ning (e.g. [3, 9, 12, 13, 14]). Most systems favour generative\ntime-series models like hidden Markov models (HMMs),\nKalman ﬁlters, or particle ﬁlters as natural choices for this\nproblem. For a more complete overview of available beat\ntrackers using various methodologies and their results on a\nchallenging dataset we refer the reader to [11].\nIn this paper, we use a different approach: our model\nrepresents each beat with its own random variable. We\nmodel time as dimension in the sample space of our ran-\ndom variables as opposed to a concept of time driving a\nrandom process in discrete steps. Therefore, all activations\nare available at any time, instead of one at a time when\nthinking of time-series data.\nFor each musical piece we create a model that differs\nfrom those of other pieces. Different pieces have different\nlengths, so the random variables are deﬁned over differ-\nent sample spaces. Each piece contains a different number\nof beats, which is why each model consists of a different\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n514Y\nX1 X2\u0001\u0001\u0001 XK\nFigure 2. The model depicted as Bayesian network. Each\nXkcorresponds to a beat and models its position. Yrep-\nresents the feature vectors of a signal.\nnumber of random variables.\nThe idea to model beat positions directly as random\nvariables is similar to the HMM-based method presented in\n[14]. However, we formulate our model as a Bayesian net-\nwork with the observations as topmost node. This allows\nus to directly utilise the whole observation sequence for\neach beat variable, without potentially violating assump-\ntions that need to hold for HMMs (especially those re-\ngarding the observation sequence). Also, our model uses\nonly a single factor to determine potential beat positions in\nthe audio – the output of a neural network – whereas [14]\nutilises multiple features on different levels to detect beats\nand downbeats.\n3.1 Model Structure\nAs mentioned earlier, we create individual models for each\npiece, following the common structure described in this\nsection. Figure 2 gives an overview of our system, depicted\nas Bayesian network.\nEachXkis a random variable modelling the position of\nthekthbeat. Its domain are all positions within the length\nof a piece. By position we mean the frame index of the ac-\ntivation function – since we extract features with a frame\nrate offr= 100 frames per second, we discretise the con-\ntinuous time space to 100 positions per second.\nFormally, the number of possible positions per piece is\ndetermined by N, the number of frames. Each Xkis then\ndeﬁned as random variable with domain N\u0014N, the natural\nnumbers smaller or equal to N:\nXk2N\u0014N with1\u0014k\u0014K; (2)\nwhereKis the number of beats in the piece. We estimate\nthis quantity by detecting the dominant interval \u001cof a piece\nusing an autocorrelation-based method on the smoothed\nactivation function of the neural network (see [2] for de-\ntails). Here, we restrict the possible intervals to a range\n[\u001cl::\u001cu], with both bounds learned from data. Assuming a\nsteady tempo and a continuous beat throughout the piece,\nwe simply compute K=N=\u001c.\nYmodels the features extracted from the input audio.\nIf we divide the signal into Nframes,Yis a sequence of\nvectors:\nY2f(y1;:::;yN)g; (3)where each ynis in the domain deﬁned by the input fea-\ntures. Although Yis formally a random variable with a\ndistribution P(Y), its value is always given by the con-\ncrete features extracted from the audio.\nThe model’s structure requires us to deﬁne dependen-\ncies between the variables as conditional probabilities. As-\nsuming these dependencies are the same for each beat but\nthe ﬁrst, we need to deﬁne\nP (X1jY)and\nP (XkjXk\u00001;Y):\nIf we wanted to compute the joint probability of the model,\nwe would also need to deﬁne P(Y)– an impossible task.\nSince, as we will elaborate later, we are only interested in\nP(X1:KjY)1, andYis always given, we can leave this\naside.\n3.2 Probability Functions\nExcept forX1, two random variables inﬂuence each Xk:\nthe previous beat Xk\u00001and the features Y. Intuitively, the\nformer speciﬁes the spacing between beats and thus the\nrough position of the beat compared to the previous one.\nThe latter indicates to what extent the features conﬁrm the\npresence of a beat at this position. We will deﬁne both as\nindividual factors that together determine the conditional\nprobabilities.\n3.2.1 Beat Spacing\nThe pulse of a musical piece spaces its beats evenly in\ntime. Here, we assume a steady pulse throughout the piece\nand model the relationship between beats as factor favour-\ning their regular placement according to this pulse. Fu-\nture work will relax this assumption and allow for varying\npulses.\nEven when governed by a steady pulse, the position of\nbeats is far from rigid: slight modulations in tempo add\nmusical expressivity and are mostly artistic elements in-\ntended by performers. We therefore allow a certain devi-\nation from the pulse. As [3] suggests, tempo changes are\nperceived relatively rather than absolutely, i.e. halving the\ntempo should be equally probable as doubling it. Hence,\nwe use the logarithm to base 2 to deﬁne the intermediate\nfactor ~\band factor \b, our beat spacing model. Let xand\nx0be consecutive beat positions and x>x0, we deﬁne\n~\b (x;x0) =\u001e\u0000\nlog2(x\u0000x0) ; log2(\u001c);\u001b2\n\u001c\u0001\n; (4)\n\b (x;x0) =(~\b (x;x0)if0<x\u0000x0<2\u001c\n0 else;(5)\nwhere\u001e\u0000\nx;\u0016;\u001b2\u0001\nis the probability density function of a\nGaussian distribution with mean \u0016and variance \u001b2,\u001cis\nthe dominant inter-beat interval of the piece, and \u001b2\n\u001crepre-\nsents the allowed tempo variance. Note how we restrict the\nnon-zero range of \b: on one hand, to prevent computing\nthe logarithm of negative values, and on the other hand, to\nreduce the number of computations.\n1We use Xm:n to denote all Xkwith indices mton\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n515The factor yields high values when xandx0are spaced\napproximately \u001capart. It thus favours beat positions that\ncorrespond to the detected dominant interval, allowing for\nminor variations.\nHaving deﬁned the beat spacing factor, we will now\nelaborate on the activation vector that connects the model\nto the audio signal.\n3.2.2 Beat Activations\nThe neural network’s activations \tindicate how likely2\neach framen2N\u0014Nis a beat position. We directly use\nthis factor in the deﬁnition of the conditional probability\ndistributions.\nWith both factors in place we can continue to deﬁne\nthe conditional probability distributions that complete our\nprobabilistic model.\n3.2.3 Conditional Probabilities\nThe conditional probability distribution P(XkjXk\u00001;Y)\ncombines both factors presented in the previous sections. It\nfollows the intuition we outlined at the beginning of Sec-\ntion 3.2 and molds it into the formal framework as\nP (XkjXk\u00001;Y) =\t (Xk;Y)\u0001\b (Xk;Xk\u00001)P\nXk\t (Xk;Y)\u0001\b (Xk;Xk\u00001):\n(6)\nThe case ofX1, the ﬁrst beat, is slightly different. There\nis no previous beat to determine its rough position using\nthe beat spacing factor. But, since we assume that there\nis a steady and continuous pulse throughout the audio, we\ncan conclude that its position lies within the ﬁrst interval\nfrom the beginning of the audio. This corresponds to a\nuniform distribution in the range [0;\u001c], which we deﬁne as\nbeat position factor for the ﬁrst beat as\n\b1(x) =(\n1=\u001cif0\u0014x<\u001c;\n0 else: (7)\nThe conditional probability for X1is then\nP (X1jY) =\t (X1;Y)\u0001\b1(X1)P\nX1\t (X1;Y)\u0001\b1(X1): (8)\nThe conditional probability functions fully deﬁne our\nprobabilistic model. In the following section, we show\nhow we can use this model to infer the position of beats\npresent in a piece of music.\n3.3 Inference\nWe want to infer values x\u0003\n1:KforX1:Kthat maximise the\nprobability of the beat sequence given Y= \u0007, that is\nx\u0003\n1:K= argmax\nx1:KP(X1:Kj\u0007): (9)\nEachx\u0003\nkcorresponds to the position of the kthbeat. \u0007are\nthe feature vectors computed for a speciﬁc piece. We use\n2technically, it is not a likelihood in the probabilistic sense – it just\nyields higher values if the network thinks that the frame contains a beat\nthan if nota dynamic programming method similar to the well known\nViterbi algorithm [15] to obtain the values of interest.\nWe adapt the standard Viterbi algorithm to ﬁt the struc-\nture of model by changing the deﬁnition of the “Viterbi\nvariables”\u000eto\n\u000e1(x) = P (X1=xj\u0007) and\n\u000ek(x) = max\nx0P (Xk=xjXk\u00001=x0;\u0007)\u0001\u000ek\u00001(x0);\nwherex;x02N\u0014N. The backtracking pointers are set\naccordingly.\nP(x\u0003\n1:Kj\u0007)gives us the probability of the beat se-\nquence given the data. We use this to determine how\nwell the deducted beat structure ﬁts the features and in\nconsequence the activations. However, we cannot directly\ncompare the probabilities of beat sequences with different\nnumbers of beats: the more random variables a model has,\nthe smaller the probability of a particular value conﬁgura-\ntion, since there are more possible conﬁgurations. We thus\nnormalise the probability by dividing by K, the number of\nbeats.\nWith this in mind, we try different values for the domi-\nnant interval \u001cto obtain multiple beat sequences, and\nchoose the one with the highest normalised probability.\nSpeciﬁcally, we run our method with multiples of \u001c(1=2,\n2=3, 1, 3=2, 2) to compensate for errors when detecting the\ndominant interval.\n4. EXPERIMENTS\nIn this section we will describe the setup of our experi-\nments: which data we trained and tested the system on,\nand which evaluation metrics we chose to quantify how\nwell our beat tracker performs.\n4.1 Data\nWe ensure the comparability of our method by using three\nfreely available data sets for beat tracking: the Ballroom\ndataset [8,13]; the Hainsworth dataset [9]; the SMC dataset\n[11]. The order of this listing indicates the difﬁculty asso-\nciated with each of the datasets. The Ballroom dataset con-\nsists of dance music with strong and steady rhythmic pat-\nterns. The Hainsworth dataset includes of a variety of mu-\nsical genres, some considered easier to track (like pop/rock,\ndance), others more difﬁcult (classical, jazz). The pieces\nin the SMC dataset were speciﬁcally selected to challenge\nexisting beat tracking algorithms.\nWe evaluate our beat tracker using 8-fold cross vali-\ndation, and balance the splits according to dataset. This\nmeans that each split consists of roughly the same relative\nnumber of pieces from each dataset. This way we ensure\nthat all training and test splits represent the same distribu-\ntion of data.\nAll training and testing phases use the same splits. The\nsame training sets are used to learn the neural network and\nto set parameters of the probabilistic model (lower and up-\nper bounds\u001cland\u001cufor dominant interval estimation and\n\u001b\u001c). The test phase feeds the resulting tracker with data\nfrom the corresponding test split. After detecting the beats\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n516for all pieces, we group the results according to the original\ndatasets in order to present comparable results.\n4.2 Evaluation Metrics\nA multitude of evaluation metrics exist for beat tracking al-\ngorithms. Some accent different aspects of a beat tracker’s\nperformance, some capture similar properties. For a com-\nprehensive review and a detailed elaboration on each of\nthe metrics, we refer the reader to [5]. Here, we restrict\nourselves to the following four quantities, but will publish\nfurther results on our website3.\nF-measure The standard measure often used in informa-\ntion retrieval tasks. Beats count as correct if detected\nwithin\u000670ms of the annotation.\nCemgil Measure that uses a Gaussian error window with\n\u001b= 40ms instead of a binary decision based on a\ntolerance window. It also incorporates false posi-\ntives and false negatives.\nCMLt The percentage of correctly detected beats at the\ncorrect metrical level. The tolerance window is set\nto 17.5% of the current inter-beat interval.\nAMLt Similar to CMLt, but allows for different metrical\nlevels like double tempo, half tempo, and off-beat.\nIn contrast to common practice4, we do not skip the\nﬁrst 5 seconds of each audio signal for evaluation. Al-\nthough skipping might make sense for on-line algorithms,\nit does not for off-line beat trackers.\n5. RESULTS\nTable 1 shows the results of our experiments. We obtained\nthe raw beat detections on the Ballroom dataset for [6, 12,\n13] from the authors of [13] and evaluated them using our\nframework. The results are thus directly comparable to\nthose of our method. For the Hainsworth dataset, we col-\nlected results for [6,7,12] from [7], who does skip the ﬁrst\n5 seconds of each piece in the evaluation. In our experi-\nence, this increases the numbers obtained for each metric\nby about 0:01.\nThe approaches of [6, 7] do not require any training.\nIn [12], some parameters are set up based on a separate\ndataset consisting of pieces from a variety of genres. [13]\nis a system that is specialised for and thus only trained on\nthe Ballroom dataset.\nWe did not include results of other algorithms for the\nSMC dataset, although available in [11]. This dataset did\nnot exist at the time most beat trackers were crafted, so the\nauthors could not train or adapt their algorithms in order to\ncope with such difﬁcult data.\nOur method improves upon the original algorithm [1,\n2] for each of the datasets and for all evaluation metrics.\nWhile F-Measure and Cemgil metric rises only marginally\n(except for the SMC dataset), CMLt and AMLt improves\n3http://www.cp.jku.at/people/korzeniowski/ismir2014\n4As implemented in the MatLab toolbox for the evaluation of beat\ntrackers presented in [5]SMC F Cg CMLt AMLt\nProposed 0.545 0.436 0.442 0.580\nB¨ock [1, 2] 0.497 0.402 0.360 0.431\nHainsworth F Cg CMLt AMLt\nProposed 0.840 0.718 0.784 0.875\nB¨ock [1, 2] 0.837 0.717 0.763 0.811\nDegara*[7] - - 0.629 0.815\nKlapuri*[12] - - 0.620 0.793\nDavies*[6] - - 0.609 0.763\nBallroom F Cg CMLt AMLt\nProposed 0.903 0.864 0.833 0.910\nB¨ock [1, 2] 0.889 0.857 0.796 0.831\nKrebs [13] 0.855 0.772 0.786 0.865\nKlapuri [12] 0.728 0.651 0.539 0.817\nDavies [6] 0.764 0.696 0.574 0.864\nTable 1. Beat tracking results for the three datasets. F\nstands for F-measure and Cgfor the Cemgil metric. Re-\nsults marked with a star skip the ﬁrst ﬁve seconds of each\npiece and are thus better by about 0.01 for each metric, in\nour experience.\nconsiderably. Our beat tracker also performs better than\nthe other algorithms, where metrics were available.\nThe proposed model assumes a stable tempo throughout\na piece. This assumption holds for certain kinds of music\n(like most of pop, rock and dance), but does not for others\n(like jazz or classical). We estimated the variability of the\ntempo of a piece using the standard deviation of the local\nbeat tempo. We computed the local beat tempo based on\nthe inter-beat interval derived from the ground truth an-\nnotations. The results indicate that most pieces have a\nsteady pulse: 90% show a standard deviation lower than\n8.61 bpm. This, of course, depends on the dataset, with\n97% of the ballroom pieces having a deviation below 8.61\nbpm, 89% of the Hainsworth dataset but only 67.7% of the\nSMC data.\nWe expect our approach to yield inferior results for\npieces with higher tempo variability than for those with\na more constant pulse. To test this, we computed Pear-\nson’s correlation coefﬁcient between tempo variability and\nAMLt value. The obtained value of \u001a= -0.46 indicates that\nour expectation holds, although the relationship is not lin-\near, as a detailed examination showed. Obviously, multiple\nother factors also inﬂuence the results. Note, however, that\nalthough the tempo of pieces from the SMC dataset varies\nmost, it is this dataset where we observed the strongest im-\nprovement compared to the original approach.\nFigure 3 compares the beat detections obtained with the\nproposed method to those computed by the original ap-\nproach. It exempliﬁes the advantage of a globally opti-\nmised beat sequence compared to a greedy local search.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5170 5 10 15 20\nTime [s]0.000.010.020.030.040.050.06ActivationFigure 3. Beat detections for the same piece as shown in Fig. 1b obtained using the proposed method (red, up arrows)\ncompared to those computed by the original approach (purple, down arrows). The activation function is plotted solid\nblue, ground truth annotations are represented by vertical dashed green lines. Note how the original method is not able to\ncorrectly align the ﬁrst 10 seconds, although it does so for the remaining piece. Globally optimising the beat sequence via\nback-tracking allows us to infer the correct beat times, even if the peaks in the activation function are ambiguous at the\nbeginning.\n6. CONCLUSION AND FUTURE WORK\nWe proposed a probabilistic method to extract beat posi-\ntions from the activations of a neural network trained for\nbeat tracking. Our method improves upon the simple ap-\nproach used in the original algorithm for this purpose, as\nour experiments showed.\nIn this work we assumed close to constant tempo\nthroughout a piece of music. This assumption holds for\nmost of the available data. Our method also performs rea-\nsonably well on difﬁcult datasets containing tempo chang-\nes, such as the SMC dataset. Nevertheless we believe that\nextending the presented method in a way that enables track-\ning pieces with varying tempo will further improve the sys-\ntem’s performance.\nACKNOWLEDGEMENTS\nThis work is supported by the European Union Seventh\nFramework Programme FP7 / 2007-2013 through the Gi-\nantSteps project (grant agreement no. 610591).\n7. REFERENCES\n[1] MIREX 2013 beat tracking results. http://nema.lis.\nillinois.edu/nema_out/mirex2013/results/\nabt/, 2013.\n[2] S. B ¨ock and M. Schedl. Enhanced Beat Tracking With\nContext-Aware Neural Networks. In Proceedings of the 14th\nInternational Conference on Digital Audio Effects (DAFx-\n11), Paris, France, Sept. 2011.\n[3] A. T. Cemgil, B. Kappen, P. Desain, and H. Honing. On\ntempo tracking: Tempogram Representation and Kalman ﬁl-\ntering. Journal of New Music Research, 28:4:259–273, 2001.\n[4] T. Collins, S. B ¨ock, F. Krebs, and G. Widmer. Bridging the\nAudio-Symbolic Gap: The Discovery of Repeated Note Con-\ntent Directly from Polyphonic Music Audio. In Proceedings\nof the Audio Engineering Society’s 53rd Conference on Se-\nmantic Audio, London, 2014.\n[5] M. E. P. Davies, N. Degara, and M. D. Plumbley. Evaluation\nmethods for musical audio beat tracking algorithms. QueenMary University of London, Centre for Digital Music, Tech.\nRep. C4DM-TR-09-06, 2009.\n[6] M. E. P. Davies and M. D. Plumbley. Context-Dependent\nBeat Tracking of Musical Audio. IEEE Transactions on Au-\ndio, Speech and Language Processing, 15(3):1009–1020,\nMar. 2007.\n[7] N. Degara, E. A. Rua, A. Pena, S. Torres-Guijarro, M. E. P.\nDavies, and M. D. Plumbley. Reliability-Informed Beat\nTracking of Musical Signals. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 20(1):290–301, Jan.\n2012.\n[8] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis,\nC. Uhle, and P. Cano. An experimental comparison of au-\ndio tempo induction algorithms. IEEE Transactions on Audio,\nSpeech, and Language Processing, 14(5):1832–1844, 2006.\n[9] S. W. Hainsworth and M. D. Macleod. Particle Filtering\nApplied to Musical Tempo Tracking. EURASIP Journal on\nAdvances in Signal Processing, 2004(15):2385–2395, Nov.\n2004.\n[10] S. Hochreiter and J. Schmidhuber. Long Short-Term Mem-\nory.Neural Computing, 9(8):1735–1780, Nov. 1997.\n[11] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. a. L. Oliveira,\nand F. Gouyon. Selective Sampling for Beat Tracking Eval-\nuation. IEEE Transactions on Audio, Speech, and Language\nProcessing, 20(9):2539–2548, Nov. 2012.\n[12] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis of the\nmeter of acoustic musical signals. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 14(1):342–355, 2006.\n[13] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic Pattern Mod-\neling for Beat and Downbeat Tracking in Musical Audio. In\nProc. of the 14th International Conference on Music Infor-\nmation Retrieval (ISMIR), 2013.\n[14] G. Peeters and H. Papadopoulos. Simultaneous beat and\ndownbeat-tracking using a probabilistic framework: theory\nand large-scale evaluation. IEEE Transactions on Audio,\nSpeech, and Language Processing, 19(6):1754–1769, 2011.\n[15] L. R. Rabiner. A tutorial on hidden Markov models and se-\nlected applications in speech recognition. Proceedings of the\nIEEE, 77(2):257–286, 1989.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n518"
    },
    {
        "title": "Cadence Detection in Western Traditional Stanzaic Songs using Melodic and Textual Features.",
        "author": [
            "Peter van Kranenburg",
            "Folgert Karsdorp"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416172",
        "url": "https://doi.org/10.5281/zenodo.1416172",
        "ee": "https://zenodo.org/records/1416172/files/KranenburgK14.pdf",
        "abstract": "Many Western songs are hierarchically structured in stan- zas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdi- vided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour [11]. We employ a random forest classification algorithm. The pre- cision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An abla- tion study shows that none of the kinds of features is suffi- cient to account for good classification, while some of the groups perform moderately well on their own.",
        "zenodo_id": 1416172,
        "dblp_key": "conf/ismir/KranenburgK14",
        "keywords": [
            "cadential",
            "melodic",
            "lyrics",
            "stanza",
            "phrase",
            "symbolically",
            "folk",
            "trigram",
            "classification",
            "random forest"
        ],
        "content": "CADENCE DETECTION IN WESTERN TRADITIONAL STANZAIC\nSONGS USING MELODIC AND TEXTUAL FEATURES\nPeter van Kranenburg, Folgert Karsdorp\nMeertens Institute, Amsterdam, Netherlands\nfpeter.van.kranenburg,folgert.karsdorpg@meertens.knaw.nl\nABSTRACT\nMany Western songs are hierarchically structured in stan-\nzas and phrases. The melody of the song is repeated for\neach stanza, while the lyrics vary. Each stanza is subdi-\nvided into phrases. It is to be expected that melodic and\ntextual formulas at the end of the phrases offer intrinsic\nclues of closure to a listener or singer. In the current paper\nwe aim at a method to detect such cadences in symbolically\nencoded folk songs. We take a trigram approach in which\nwe classify trigrams of notes and pitches as cadential or\nas non-cadential. We use pitch, contour, rhythmic, textual,\nand contextual features, and a group of features based on\nthe conditions of closure as stated by Narmour [11]. We\nemploy a random forest classiﬁcation algorithm. The pre-\ncision of the classiﬁer is considerably improved by taking\nthe class labels of adjacent trigrams into account. An abla-\ntion study shows that none of the kinds of features is sufﬁ-\ncient to account for good classiﬁcation, while some of the\ngroups perform moderately well on their own.\n1. INTRODUCTION\nThis paper presents both a method to detect cadences in\nWestern folk-songs, particularly in folk songs from Dutch\noral tradition, and a study to the importance of various mu-\nsical parameters for cadence detection.\nThere are various reasons to focus speciﬁcally on ca-\ndence patterns. The concept of cadence has played a major\nrole in the study of Western folk songs. In several of the\nmost important folks song classiﬁcation systems, cadence\ntones are among the primary features that are used to put\nthe melodies into a linear ordering. In one of the earli-\nest classiﬁcation systems, devised by Ilmari Krohn [10],\nmelodies are ﬁrstly ordered according to the number of\nphrases, and secondly according to the sequence of ca-\ndence tones. This method was adapted for Hungarian mel-\nodies by B ´artok and Kod ´aly [16], and later on for German\nfolk songs by Suppan and Stief [17] in their monumental\nMelodietypen des Deutschen Volksgesanges. Bronson [3]\nintroduced a number of features for the study of Anglo-\nAmerican folk song melodies, of which ﬁnal cadence and\nc\rPeter van Kranenburg, Folgert Karsdorp.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Peter van Kranenburg, Folgert Kars-\ndorp. “Cadence Detection in Western Traditional Stanzaic Songs using\nMelodic and Textual Features”, 15th International Society for Music In-\nformation Retrieval Conference, 2014.mid-cadence are the most prominent ones. One of the\nunderlying assumptions is that the sequence of cadence\ntones is relatively stable in the process of oral transmis-\nsion. Thus, variants of the same melody are expected to\nend up near to each other in the resulting ordering.\nFrom a cognitive point of view, the perception of clo-\nsure is of fundamental importance for a listener or singer\nto understand a melody. In terms of expectation [8, 11],\na ﬁnal cadence implies no continuation at all. It is to be\nexpected that speciﬁc features of the songs that are related\nto closure show different values for cadential patterns as\ncompared to non-cadential patterns. We include a subset\nof features that are based on the conditions of closure as\nstated by Narmour [11, p.11].\nCadence detection is related to the problem of segmen-\ntation, which is relevant for Music Information Retrieval\n[21]. Most segmentation methods for symbolically repre-\nsented melodies are either based on pre-deﬁned rules [4,\n18] or on statistical learning [1,9,12]. In the current paper,\nwe focus on the musical properties of cadence formulas\nrather than on the task of segmentation as such.\nTaking Dutch folk songs as case study, we investigate\nwhether it is possible to derive a general model of the mel-\nodic patterns or formulas that speciﬁcally indicate melodic\ncadences using both melodic and textual features. To ad-\ndress this question, we take a computational approach by\nemploying a random forest classiﬁer (Sections 5 and 6).\nTo investigate which musical parameters are of impor-\ntance for cadence detection, we perform an ablation study\nin which we subsequently remove certain types of features\nin order to evaluate the importance of the various kinds of\nfeatures (Section 7).\n2. DATA\nWe perform all our experiments on the folk song collec-\ntion from the Meertens Tune Collections (MTC-FS, ver-\nsion 1.0), which is a set of 4,120 symbolically encoded\nDutch folk songs.1Roughly half of it consists of tran-\nscriptions from ﬁeld recordings that were made in the Nether-\nlands during the 20th century. The other half is taken from\nsong books that contain repertoire that is directly related\nto the recordings. Thus, we have a coherent collection of\nsongs that reﬂects Dutch everyday song culture in the early\n20th century. Virtually all of these songs have a stanzaic\nstructure. Each stanza repeats the melody, and each stanza\n1Available from: http://www.liederenbank.nl/mtc.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n391consists of a number of phrases. Both in the transcrip-\ntions and in the song books, phrase endings are indicated.\nFigure 1 shows a typical song from the collection. The\nlanguage of the songs is standard Dutch with occasionally\nsome dialect words or nonsense syllables. All songs were\ndigitally encoded by hand at the Meertens Institute (Ams-\nterdam) and are available in Humdrum **kern format. The\nphrase endings were encoded as well and are available for\ncomputational analysis and modeling.\n3. OUR APPROACH\nOur general approach is to isolate trigrams from the melo-\ndies and to label those as either cadential or non-cadential.\nA cadential trigram is the last trigram in a phrase. We com-\npare two kinds of trigrams: trigrams of successive notes\n(note-trigrams), and trigrams of successive pitches (pitch-\ntrigrams), considering repeated pitches as one event. In the\ncase of pitch-trigrams, a cadence pattern always consists of\nthe three last unique pitches of the phrase. There are two\nreasons for including pitch-trigrams. First, pitch repetition\nis often caused by the need to place the right number of\nsyllables to the melody. It occurs that a quarter note in one\nstanza corresponds to two eighth notes in another stanza\nbecause there is an extra syllable at that spot in the song\ntext. Second, in models of closure in melody [11, 15] suc-\ncessions of pitches are of primary importance.\nFigure 1 depicts all pitch-trigrams in the presented mel-\nody. The trigram that ends on the ﬁnal note of a phrase\nis a cadential trigram. These are indicated in bold. Some\ncadential trigrams cross a phrase boundary when the next\nphrase starts with the same pitch.\nFrom each trigram we extract a number of feature val-\nues that reﬂect both melodic and textual properties. We\nthen perform a classiﬁcation experiment using a Random\nForest Classiﬁer [2]. This approach can be regarded a ‘bag-\nof-trigrams’ approach, where each prediction is done inde-\npendently of the others, i.e. all sequential information is\nlost. Therefore, as a next step we take the labels of the\ndirect neighboring trigrams into account as well. The ﬁ-\nnal classiﬁcation is then based on a majority vote of the\npredicted labels of adjacent trigrams. These steps will be\nexplained in detail in the next sections.\n \n/clefs.G\nwou/noteheads.s2\nrij en/noteheads.s2\ngaan./noteheads.s286/flags.d3/scripts.rcomma\nJan berts/noteheads.s2\ndie/noteheads.s2 /noteheads.s2\nAl/noteheads.s2/flags.d3\nhet/noteheads.s2/noteheads.s2 /noteheads.s2\nuit\nmeis je/noteheads.s2 /noteheads.s2/noteheads.s2\nmooi al/noteheads.s2\nstaan./noteheads.s2\nzag/noteheads.s2/clefs.G/flags.d3\nDie/noteheads.s2 /scripts.rcomma\nver/noteheads.s2\neen/noteheads.s2\ndaar/noteheads.s2\nvan/noteheads.s2\neen/noteheads.s2\n/flags.d3\nmooi/noteheads.s2/flags.u3\nvan/noteheads.s2/dots.dot\nver/noteheads.s2\nje/noteheads.s2\nmeis/noteheads.s2/flags.d3/noteheads.s2 /dots.dot\nstaan./noteheads.s2/noteheads.s2 /clefs.G/flags.d3\nal/noteheads.s2 /dots.dot\nzag daar/noteheads.s2/flags.u3\n/noteheads.s2\nDie/noteheads.s2\nFigure 1. Examples of pitch-trigrams. The cadential tri-\ngrams are indicated in bold.4. FEATURES\nWe represent each trigram as a vector of feature values. We\nmeasure several basic properties of the individual pitches\nand of the pattern as a whole. The code to automatically\nextract the feature values was written in Python, using the\nmusic21 toolbox [5]. The features are divided into groups\nthat are related to distinct properties of the songs. Some\nfeatures occur in more than one group. The following\noverview shows all features and in parentheses the value\nfor the ﬁrst trigram in Figure 1. Detailed explanations are\nprovided in sections 4.1 and 4.2.\nPitch Features\nScale degree Scale degrees of the ﬁrst, second, and third item (5, 1, 3).\nRange Difference between highest and lowest pitch (4).\nHas contrast third Whether there are both even and odd scale degrees in\nthe trigram (False).\nContour Features\nContains leap Whether there is a leap in the trigram (True).\nIs ascending Whether the ﬁrst and second intervals, and both are ascend-\ning (False, True, False).\nIs descending Whether the ﬁrst and second intervals, and both are de-\nscending (True, False, False).\nLarge-small Whether the ﬁrst interval is large and the second is small\n(True).\nRegistral change Whether there is a change in direction between the ﬁrst\nand the second interval (True).\nRhythmic Features\nBeat strength The metric weights of the ﬁrst, second and third item (0.25,\n1.0, 0.25).\nMin beat strength The smallest metric weight (0.25).\nNext is rest Whether a rest follows the ﬁrst, second and third item (False,\nFalse, False).\nShort-long Whether the second item is longer than the ﬁrst, and the third\nis longer than the second (False, False).\nMeter The meter at the beginning of the trigram (“6/8”).\nTextual Features\nRhymes Whether a rhyme word ends at the ﬁrst, second and third item\n(False, False, False).\nWord stress Whether a stressed syllable is at the ﬁrst, second and third\nitem (True, True, True).\nDistance to last rhyme Number of notes between the last the ﬁrst, second\nand third item and the last rhyme word or beginning of the melody\n(0, 1, 2).\nNarmour Closure Features\nBeat strength The metric weights of the ﬁrst, second and third item (0.25,\n1.0, 0.25).\nNext is rest Whether a rest follows the ﬁrst, second and third item (False,\nFalse, False).\nShort-long Whether the second item is longer than the ﬁrst, and the third\nis longer than the second (False, False).\nLarge-small Whether the ﬁrst interval is large (\u0015 ﬁfth) and the second is\nsmall (\u0014 third) (True).\nRegistral change Whether there is a change in direction between the ﬁrst\nand the second interval (True).\nContextual Features\nNext is rest third Whether a rest or end of melody follows the third item\n(False).\nDistance to last rhyme Number of notes between the last the ﬁrst, second\nand third item and the last rhyme word or beginning of the melody\n(0, 1, 2).\n4.1 Melodic Features\nSeveral of the features need some explanation. In this sec-\ntion we describe the melodic features, while in the next\nsection, we explain how we extracted the textual features.\nHasContrastThird is based on the theory of Jos Smits-\nVan Waesberghe [15], the core idea of which is that a mel-\nody gets its tension and interest by alternating between\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n392pitches with even and uneven scale degrees, which are two\ncontrasting series of thirds.\nThe metric weight in the Rhythmic features is the beat-\nstrength as implemented in music21’s meter model.\nThe Narmour features are based on the six (preliminary)\nconditions of closure that Narmour states at the beginning\nof his ﬁrst book on the Implication-Realisation theory [11,\np.11]: “[...] melodic closure on some level occurs when\n1. a rest, an onset of another structure, or a repetition in-\nterrupts an implied pattern; 2. metric emphasis is strong;\n3. consonance resolves dissonance; 4. duration moves cu-\nmulatively (short note to long note); 5. intervallic motion\nmoves from large interval to small interval; 6. registral di-\nrection changes (up to down, down to up, lateral to up, lat-\neral to down, up to lateral, or down to lateral). Of course,\nthese six may appear in any combination.” Because the\nmelodies are monophonic, condition 3 has no counterpart\nin our feature set.\nThe contextual features are not features of the trigram in\nisolation, but are related to the position in the melody. In an\ninitial experiment we found that the distance between the\nﬁrst note of the trigram and the last cadence is an important\npredictor for the next cadence. Since this is based on the\nground-truth label, we cannot include it directly into our\nfeature set. Since we expect rhyme in the text to have a\nstrong relation with cadence in the melody, we include the\ndistance to the last rhyme word in number of notes.\nmooi/noteheads.s2\nFalseAlAl/clefs.G/accidentals.sharp/noteheads.s2/flags.d3/noteheads.s2\nFalsevErver/noteheads.s2/noteheads.s2\nFalsemoj/noteheads.s2\nFalseb@rtsberts/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s286/noteheads.s2\nFalsej@en\nFalse/noteheads.s2\nFalsewA+wwou\njAn/noteheads.s2/scripts.ufermata\nTrue/flags.d3/noteheads.s2\nFalserE+rij/noteheads.s2\nFalsezAxzag/noteheads.s2/noteheads.s2\nJan/flags.d3\ngaan.\nxan\n/noteheads.s2\nFalsemE+smeis/flags.d3/noteheads.s2\nFalsej@je/noteheads.s2/noteheads.s2\nFalsemojmooi/flags.d3/noteheads.s2/noteheads.s2\n/flags.d3/noteheads.s2\nTruestanstaan./dots.dot/noteheads.s2/scripts.ufermata\nmE+smeis/noteheads.s2\nFalsej@je/noteheads.s2/noteheads.s2/scripts.ufermata\nTruestanstaan.4\n/clefs.G/accidentals.sharp/noteheads.s2\nFalse/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2\nFalsevErver/dots.dot /flags.u3/noteheads.s2\nFalsezAxzag/dots.dot/noteheads.s2\nFigure 2. Rhyme as detected by our method. The ﬁrst line\nshows the original text after removing non-content words.\nThe second line shows the phonological representations\nof the words (in SAMPA notation). The third line shows\nwhether rhyme is detected (‘True’ if a rhyme word ends at\nthe corresponding note).\n4.2 Textual Features\nIn many poetical texts, phrase boundaries are determined\nby sequences of rhyme. These establish a structure in a\ntext, both for aesthetics pleasure and memory aid [14]. In\nfolk music, phrasal boundaries established by sequences of\nrhyme are likely to relate to phrases in the melody.\nWe developed a rhyme detection system which allows\nus to extract these sequences of rhyming lyrics. Because\nof orthographical ambiguities (e.g. cruise, where /u:/ is\nrepresented by uiwhereas in muse it is represented by u),\nit is not as straightforward to perform rhyme detection on\northographical representations of words. Therefore, we\ntransform each word into its phonological representation\n(e.g. cruise becomes /kru:z/ andbike /baIk/).\n__c_rcuriusiruiuisisese_e________crucruise'kru:0z0Figure 3. Example sliding window for phoneme classiﬁ-\ncation.\nWe approach the problem of phonemicization as a su-\npervised classiﬁcation task, where we try to predict for\neach character in a given word its corresponding phoneme.\nWe take a sliding window-based approach where for each\nfocus character (i.e. the character for which we want to pre-\ndict its phonemic representation) we extract as features n\ncharacters to the left of the focus character, ncharacters to\nthe right, and the focus character itself. Figure 3 provides\na graphical representation of the feature vectors extracted\nfor the word cruise. The fourth column represents the fo-\ncus character with a context of three characters before and\nthree after the focus character. The last column represents\nthe target phonemes which we would like to predict. Note\nthat the ﬁrst target phoneme in Figure 3 is preceded by an\napostrophe (’k), which represents the stress position on the\nﬁrst (and only) syllable in cruise. This symbolic notation\nof stress in combination with phonology allows us to si-\nmultaneously extract a phonological representation of the\ninput words as well as their stress patterns. For all words\nin the lyrics in the dataset we apply our sliding window\napproach with n= 5, which serves as input for the su-\npervised classiﬁer. In this paper we make use of a k= 1\nNearest Neighbor Classiﬁer as implemented by [6] using\ndefault settings, which was trained on the data of the e-\nLex database2. In the running text of our lyrics, 89.5% of\nthe words has a direct hit in the instance base, and for the\nremaining words in many cases suitable nearest neighbors\nwere found. Therefore, we consider the phonemicization\nsufﬁciently reliable.\nWe assume that only content words (nouns, adjectives,\nverbs and adverbials) are possible candidate rhyme words.\nThis assumption follows linguistic knowledge as phrases\ntypically do not end with function words such as determin-\ners, prepositions, etcetera. Function words are part of a\nclosed category in Dutch. We extract all function words\nfrom the lexical database e-Lex and mark for each word in\neach lyric whether it is a function word.\nWe implemented rhyme detection according to the rules\nfor Dutch rhyme as stated in [19]. The algorithm is straight-\nforward. We compare the phoneme-representations of two\nwords backwards, starting at the last phoneme, until we\nreach the ﬁrst vowel, excluding schwas. If all phonemes\n2http://tst-centrale.org/en/producten/lexica/\ne-lex/7-25\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n393Class pr rec F1\u001bF1support\nnote-trigrams\ncadence 0.84 0.72 0.78 0.01 23,925\nnocadence 0.96 0.98 0.97 0.01 183,780\npitch-trigrams\ncadence 0.85 0.69 0.76 0.01 23,838\nnocadence 0.95 0.98 0.96 0.00 130,992\nTable 1. Results for single labels.\nand the vowel are exactly the same, the two words rhyme.\nAs an example we take kinderen (‘children’) and hin-\nderen (‘to hinder’). The phoneme representations as pro-\nduced by our method are /kInd@r@/ and /hInd@r@/. The\nﬁrst vowel starting from the back of the word, exclud-\ning the schwas (/@/), is /I/. Starting from this vowel,\nthe phoneme representations of both words are identical\n(/Ind@r@/). Therefore these words rhyme.\nWe also consider literal repetition of a word as ‘rhyme’,\nbut not if a sequence of words is repeated literally, such\nas in the example in Figure 1. Such repetition of entire\nphrases occurs in many songs. Labeling all words as rhyme\nwords would weaken the relation with cadence or ‘end-of-\nsentence’. We only label the last word of repeated phrases\nas a rhyme word. Figure 2 shows an example.\n5. CLASSIFICATION WITH SINGLE LABELS\nAs a ﬁrst approach we consider the trigrams independently.\nA melody is represented as ‘bag-of-trigrams’. Each tri-\ngram has a ground-truth label that is either ‘cadence’ or\n‘no cadence’, as depicted in Figure 1 for pitch-trigrams’\nWe employ a Random Forest classiﬁer [2] as imple-\nmented in the Python library scikit-learn [13]. This classi-\nﬁer combines ndecision trees (predictors) that are trained\non random samples extracted from the data (with replace-\nment). The ﬁnal classiﬁcation is a majority vote of the pre-\ndictions of the individual trees. This procedure has proven\nto perform more robustly than a single decision tree and\nis less prone to over-ﬁtting the data. Given the relatively\nlarge size of our data set, we set the number of predictors\nto 50 instead of the default 10. For the other parameters,\nwe keep the default values.\nThe evaluation is performed by 10-fold cross-validation.\nOne non-trivial aspect of our procedure is that we construct\nthe folds at the level of the songs, rather than at that of indi-\nvidual trigrams. Since it is quite common for folk songs to\nhave phrases that are literally repeated, folding at the level\nof trigrams could result in identical trigrams in the train\nand test subsets, which could lead to an overﬁtted classi-\nﬁer. By ensuring that all trigrams from a song are either in\nthe test or in the train subset, we expect better generaliza-\ntion. This procedure is applied throughout this paper.\nThe results are shown in Table 1. For both classes aver-\nages of the values for the precision, the recall and the F1-\nmeasure over the folds are included, as well as the standard\ndeviation of the F1measure, which indicates the variation\nover the folds. The number of items in both classes (sup-Class pr rec F1\u001bF1support\nnote-trigrams\ncadence 0.89 0.72 0.80 0.01 23,925\nnocadence 0.96 0.99 0.98 0.00 183,780\npitch-trigrams\ncadence 0.89 0.71 0.79 0.01 23,838\nnocadence 0.95 0.98 0.97 0.01 130,992\nTable 2. Results for classiﬁcation with label trigrams.\nport) shows that cadences are clearly a minority class.\nWe observe that the note-trigrams lead to slightly better\ncadence-detection as compared to pitch-trigrams. Appar-\nently, the repetition of pitches does not harm the discrim-\ninability. Furthermore, there is an unbalance between the\nprecision and the recall of the cadence-trigrams. The pre-\ncision is rather high, while the recall is moderate.\n6. CLASSIFICATION WITH LABEL TRIGRAMS\nWhen our cadence detection system predicts the class of a\nnew trigram, it is oblivious of the decisions made for earlier\npredictions. One particularly negative effect of this near-\nsightedness is that the classiﬁer frequently predicts two (or\neven more) cadences in a row, which, given our our train-\ning material, is extremely unlikely. We attempt to circum-\nvent this ‘defect’ using a method, developed by [20] that\npredicts trigrams of class labels instead of single, binary\nlabels. Figure 4 depicts the standard single class classi-\nﬁcation setting, where each trigram is predicted indepen-\ndent of all other predictions. In the label trigram setting\n(see Figure 5), the original class labels are replaced with\nthe class label of the previous trigram, the class label of\nthe current trigram and the label of the next trigram. The\nlearning problem is transformed into a sequential learn-\ning problem with two stages. In the ﬁrst stage we predict\nfor each trigram a label trigram y(t)= (y 1;y2;y3)where\ny2 f0;1g. To arrive at the ﬁnal single class predictions\n(i.e. is it a cadence or not), in the second stage we take\nthe majority vote over the predictions of the focus trigram\nand those of its immediate left and right neighboring tri-\ngrams. Take t4in Figure 5 as an example. It predicts that\nthe current trigram is a cadence. The next trigram and the\nprevious trigram also predict it to be a cadence and based\non this majority vote, the ﬁnal prediction is that t4is a\ncadence. Should t3andt5both have predicted the zero\nclass (e.g.y(t3)= (0; 0;0)andy(t5)= (0; 1;0)), the ma-\njority vote would be 0. The advantage of this method is\nthat given the negligible number of neighboring cadences\nin our training data, we can virtually rule out the possibility\nto erroneously predict two or more cadences in a row.\nTable 2 shows the performance of the label-trigram clas-\nsiﬁer for both classes and both for pitch and note trigrams.\nThe values show an important improvement for the preci-\nsion of cadence-detection and a slight improvement of the\nrecall. The lower number of false positives is what we ex-\npected by observing the classiﬁcation of adjacent trigrams\nas ‘cadence’ in the case of the single-label classiﬁer.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n39400010t1t2t3t4t5Figure 4. Short example sequence of trigrams. Each tri-\ngramtihas a binary label indicating whether the trigram is\ncadential (1) or non-cadential (0).\nt1t2t3t4t5000000001100010\nFigure 5. Label-trigrams for the same sequence as in Fig-\nure 1, where t4has label 1 and the other trigrams have label\n0. Each trigram tigets a compound label consisting of its\nown label and the labels of the direct neighboring trigrams.\n7. ABLATION STUDY\nTo study the importance of the various kinds of features,\nwe perform an ablation study. We successively remove\neach of the groups of features as deﬁned in section 4 from\nthe full set and do a classiﬁcation experiment with the re-\nmaining features. Subsequently, we perform a similar se-\nries of classiﬁcation experiments, but now with each single\ngroup of features. The ﬁrst series shows the importance\nof the individual groups of features, and the second series\nshows the predictive power for each of the groups. Because\nthe groups are assembled according to distinct properties\nof music and text, this will give insight in the importance\nof various musical and textual parameters for cadence de-\ntection. We use the label-trigram classiﬁer with the note-\ntrigrams, which performed best on the full set.\nWe expect occurrence of rests to be a very strong predic-\ntor, because according to our deﬁnition a ‘rest’ always fol-\nlows after the ﬁnal cadence, and we know that in our cor-\npus rests almost exclusively occur between phrases. There-\nfore, we also take the three features that indicate whether a\nrest occurs in the trigram or directly after it, as a separate\ngroup. The performance when leaving these three features\nout will show whether they are crucial for cadence detec-\ntion.\nTable 3 shows the evaluation measures for each of the\nfeature subsets. Precision, recall and F1for class ‘cadence’\nare reported. Again, the values are averaged over 10 folds.\nWe see that none of the single groups of features is cru-\ncial for the performance that was achieved with the com-\nplete set of features. The basic melodic features (F pitch,\nFcontour , andFrhyhmic ) all perform very bad on their own,\nshowing low to extremely low recall values. The contour\nfeatures even do not contribute at all. Only the rhythmic\nfeatures yield some performance. The features on rest areSubset pr rec F1\u001bF1\nFall 0.89 0.72 0.80 0.01\nFallnFpitch 0.88 0.72 0.79 0.01\nFpitch 0.84 0.04 0.08 0.01\nFallnFcontour 0.88 0.73 0.80 0.01\nFcontour 0.00 0.00 0.00 0.00\nFallnFrhythmic 0.79 0.49 0.60 0.01\nFrhythmic 0.90 0.35 0.50 0.01\nFallnFtextual 0.85 0.58 0.69 0.02\nFtextual 0.70 0.40 0.51 0.01\nFallnFnarmour 0.83 0.55 0.66 0.01\nFnarmour 0.95 0.30 0.45 0.01\nFallnFcontextual 0.87 0.67 0.76 0.01\nFcontextual 0.71 0.45 0.56 0.01\nFallnFrest 0.87 0.67 0.76 0.01\nFrest 0.97 0.27 0.43 0.02\nTable 3. Results for various feature subsets for class ‘ca-\ndence’.\nincluded in the set of rhythmic features. The classiﬁca-\ntion with just the features on rest, Frestshows very high\nprecision and low recall. Still, the recall with all rhythmic\nfeatures is higher than only using the rest-features. Since\nrests are so tightly related to cadences in our corpus, the\nhigh precision for Frestis what we expected. If we exclude\nthe rest-features, the precision stays at the same level as for\nthe entire feature set and the recall drops with 0:06, which\nshows that only a minority of the cadences exclusively re-\nlies on rest-features to be detected.\nThe set of features that is based on the conditions of clo-\nsure as formulated by Narmour shows high precision and\nlow recall. Especially the high precision is interesting, be-\ncause this conﬁrms Narmour’s conditions of closure. Ap-\nparently, most patterns that are classiﬁed as cadence based\non this subset of features, are cadences indeed. Still, the\nlow recall indicates that there are many cadences that are\nleft undetected. One cause could be that the set of condi-\ntions as stated by Narmour is not complete, another cause\ncould be the discrepancy between our features and Nar-\nmour’s conditions. Further investigation would be neces-\nsary to shed light on this. Removing the Narmour-based\nfeatures from the full feature set does not have a big im-\npact. The other features have enough predictive power.\nThe textual features on their own show moderate pre-\ncision and very moderate recall. They are able to discern\ncertain kinds of cadences to a certain extent, while miss-\ning most of the other cadences. The drop of 0:14 in recall\nforFallnFtextual as compared to the full set shows that\ntext features are crucial for a considerable number of ca-\ndences to be detected. The same applies to a somewhat\nlesser extent to contextual features. Removing the contex-\ntual features from the full set causes a drop of 0:05 in the\nrecall, which is considerable but not extreme. It appears\nthat the group of cadence trigrams for which the contex-\ntual features are crucial is not very big.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3958. CONCLUSION AND FUTURE WORK\nIn this paper we developed a system to detect cadences in\nWestern folk songs. The system makes use of a Random\nForest Classiﬁer that on the basis of a number of hand-\ncrafted features (both musical and textual) is able to accu-\nrately locate cadences in running melodies. In a follow-\nup experiment we employ a method, originally developed\nfor textual sequences, that predicts label-trigrams instead\nof the binary labels ‘cadence’ or ‘non-cadence’. We show\nthat incorporating the predictions of neighboring instances\ninto the ﬁnal prediction, has a strong positive effect on pre-\ncision without a loss in recall.\nIn the ablation study we found that all groups of fea-\ntures, except for the contour features, contribute to the over-\nall classiﬁcation, while none of the groups is crucial for\nthe majority of the cadences to be detected. This indicates\nthat cadence detection is a multi-dimensional problem for\nwhich various properties of melody and text are necessary.\nThe current results give rise to various follow-up stud-\nies. A deeper study to the kinds of errors of our system\nwill lead to improved features and increased knowledge\nabout cadences. Those that were detected exclusively by\ntextual features form a particular interesting case, possibly\ngiving rise to new melodic features. Next, n-grams other\nthan trigrams as well as skip-grams [7] could be used, we\nwill compare the performance of our method with existing\nsymbolic segmentation algorithms, and we want to make\nuse of other features of the text such as correspondence\nbetween syntactic units in the text and melodic units in the\nmelody.\n9. REFERENCES\n[1] Rens Bod. Probabilistic grammers for music. In Pro-\nceedings of BNAIC 2001, 2001.\n[2] L. Breiman. Random forests. Machine Learning,\n45(1):5–32, 2001.\n[3] Bertrand H Bronson. Some observations about melodic\nvariation in british-american folk tunes. Journal of the\nAmerican Musicological Society, 3:120–134, 1950.\n[4] Emilios Cambouropoulos. The local boundary detec-\ntion model (lbdm) and its application in the study of\nexpressive timing. In Proc. of the Intl. Computer Mu-\nsic Conf, 2001.\n[5] Michael Scott Cuthbert and Christopher Ariza. Mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. In Proceedings of the 11th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR 2010), pages 637–642, 2010.\n[6] Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,\nand Antal Van den Bosch. TiMBL: Tilburg Memory\nBased Learner, version 6.3, Reference Guide, 2010.\n[7] David Guthrie, Ben Allison, W. Liu, Louise Guthrie,\nand Yorick Wilks. A closer look at skip-gram mod-\nelling. In Proceedings of the Fifth international Con-ference on Language Resources and Evaluation LREC-\n2006, 2006.\n[8] David Huron. Sweet Anticipation. MIT Press, Cam-\nbridge, Mass., 2006.\n[9] Zolt ´an Juh ´asz. Segmentation of hungarian folk songs\nusing an entropy-based learning system. Journal of\nNew Music Research, 33(1):5–15, 2004.\n[10] Ilmari Krohn. Welche ist die beste Methode, um\nVolks- und volksm ¨assige Lieder nach ihrer melodis-\nchen (nicht textlichen) Beschaffenheit lexikalisch zu\nordnen? Sammelb ¨ande der internationalen Musikge-\nsellschaft, 4(4):643–60, 1903.\n[11] Eugene Narmour. The Analysis and Cognition of Ba-\nsic Melodic Structures - The Implication-Realization\nModel. The University of Chicago Press, Chicago and\nLonden, 1990.\n[12] Marcus Pearce, Daniel M ¨ullensiefen, and Geraint Wig-\ngins. The role of expectation and probabilistic learning\nin auditory boundary perception: A model comparison.\nPerception, 39(10):1365–1389, 2010.\n[13] F. Pedregosa et al. Scikit-learn: Machine learning\nin Python. Journal of Machine Learning Research,\n12:2825–2830, 2011.\n[14] David C. Rubin. Memory in Oral Traditions. Oxford\nUniversity Press, New York, 1995.\n[15] Jos Smits van Waesberghe. A Textbook of Melody: A\ncourse in functional melodic analysis. American Insti-\ntute of Musicology, 1955.\n[16] B. Suchoff. Preface, pages ix–lv. State University of\nNew York Press, Albany, 1981.\n[17] W. Suppan and W. Stief, editors. Melodietypen des\nDeutschen Volksgesanges. Hans Schneider, Tutzing,\n1976.\n[18] David Temperley. A probabilistic model of melody per-\nception. In Proceedings of the 7th International Con-\nference on Music Information Retrieval (ISMIR 2006),\nVictoria, BC, 2006.\n[19] Erica van Boven and Gillis Dorleijn. Literair\nMechaniek. Coutinho, Bussum, 2003.\n[20] Antal Van den Bosch and Walter Daelemans. Improv-\ning sequence segmentation learning by predicting tri-\ngrams. In Proceedings of the Ninth Conference on Nat-\nural Language Learning, CoNLL-2005, pages 80–87,\nAnn Arbor, MI, 2005.\n[21] Frans Wiering and Hermi J.M. Tabachneck-Schijf.\nCognition-based segmentation for music information\nretrieval systems. Journal of New Music Research,\n38(2):137–154, 2009.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n396"
    },
    {
        "title": "Computational Models for Perceived Melodic Similarity in A Cappella Flamenco Singing.",
        "author": [
            "Nadine Kroher",
            "Emilia Gómez",
            "Catherine Guastavino",
            "Francisco Gómez 0001",
            "Jordi Bonada"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416306",
        "url": "https://doi.org/10.5281/zenodo.1416306",
        "ee": "https://zenodo.org/records/1416306/files/KroherGGGB14.pdf",
        "abstract": "The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a com- mon melodic skeleton, which is subject to spontaneous im- provisation containing strong prolongations and ornamen- tations. For our research we collected human similarity judgements from na¨ıve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We further- more calculated distances from manually extracted high- level descriptors defined by flamenco experts. The suitabi- lity of a set of computational melodic similarity measures was evaluated by analyzing the correlation between com- puted similarity and human ratings. We observed signifi- cant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between hu- man ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and orna- mentation are better suited to model perceived similarity for this material than models based on chroma distance.",
        "zenodo_id": 1416306,
        "dblp_key": "conf/ismir/KroherGGGB14",
        "keywords": [
            "melodic similarity",
            "cappella flamenco singing",
            "flamenco songs",
            "spontaneous improvisation",
            "high-level descriptors",
            "computational melodic similarity measures",
            "correlation between human ratings",
            "flamenco experts",
            "temporal deviation",
            "dynamics"
        ],
        "content": "COMPUTATIONAL MODELS FOR PERCEIVED MELODIC SIMILARITY\nIN A CAPPELLA FLAMENCO SINGING\nN. Kroher, E. G ´omez\nUniversitat Pompeu\nFabra\nemilia.gomez\n@upf.edu,\nnadine.kroher\n@upf.eduC. Guastavino\nMcGill University\n& CIRMMT\ncatherine.guastavino\n@mcgill.caF. G´omez\nTechnical University\nof Madrid\nfmartin\n@eui.upm.esJ. Bonada\nUniversitat Pompeu\nFabra\njordi.bonada\n@upf.edu\nABSTRACT\nThe present study investigates the mechanisms involved\nin the perception of melodic similarity in the context of a\ncappella ﬂamenco singing performances. Flamenco songs\nbelonging to the same style are characterized by a com-\nmon melodic skeleton, which is subject to spontaneous im-\nprovisation containing strong prolongations and ornamen-\ntations. For our research we collected human similarity\njudgements from na ¨ıve and expert listeners who listened\nto audio recordings of a cappella ﬂamenco performances as\nwell as synthesized versions of the same songs. We further-\nmore calculated distances from manually extracted high-\nlevel descriptors deﬁned by ﬂamenco experts. The suitabi-\nlity of a set of computational melodic similarity measures\nwas evaluated by analyzing the correlation between com-\nputed similarity and human ratings. We observed signiﬁ-\ncant differences between listener groups and stimuli types.\nFurthermore, we observed a high correlation between hu-\nman ratings and similarities computed from features from\nﬂamenco experts. We also observed that computational\nmodels based on temporal deviation, dynamics and orna-\nmentation are better suited to model perceived similarity\nfor this material than models based on chroma distance.\n1. INTRODUCTION\nThe task of modeling perceived melodic similarity among\nmusic pieces is a multi-dimensional task whose complex-\nity increases when human judgements are inﬂuenced by\nimplicit knowledge about genre-speciﬁc musicological as-\npects and contextual information. Nevertheless, such com-\nputational models are of utmost importance for automatic\nsimilarity retrieval and recommendation systems in large\nmusic databases. Furthermore, analysis of melodic sim-\nc\rN. Kroher, E. G ´omez, C. Guastavino, F. G ´omez, J.\nBonada.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: N. Kroher, E. G ´omez, C. Guastavino,\nF. G ´omez, J. Bonada. “Computational Models for Perceived Melodic\nSimilarity in A Cappella Flamenco Singing”, 15th International Society\nfor Music Information Retrieval Conference, 2014.ilarity among large amounts of data can provide impor-\ntant clues for musicological studies regarding style clas-\nsiﬁcation, similarity and evolution. In the past, numer-\nous approaches have focused on melodic similarity mea-\nsures, mainly computed from automatically aligned score-\nlike representations. For a complete review of symbolic\nnote similarity measures we refer the reader to [1]. Sev-\neral previous studies have related computational measures\nto human ratings. In an extensive study in [14], expert\nratings of similarity between western pop songs and gen-\nerated variants were compared to 34 computational mea-\nsures. The best correlation was observed for a hybrid method\ncombining various weighted distance measures, which is\nsuccessfully used to automatically retrieve variants of a\ngiven melody from a folk song database. In similar studies,\nhuman similarity ratings were compared to transportation\ndistances [16] and statistical descriptors related to tone, in-\nterval and note duration distribution [17]. In order to gain a\ndeeper insight into the perception process of melodic sim-\nilarity, V olk and van Kranenburg studied the relationship\nbetween musical features and human similarity-based cate-\ngorization, where a large collection of folk songs was man-\nually categorized into tune families [15]. Furthermore, hu-\nman similarity judgement based on various musical facets\nwere gathered. Results indicate that songs perceived as\nsimilar tend to show strong similarities in rhythm, pitch\ncontour and contained melodic motifs, whereas the indi-\nvidual importance of these criteria varies among the data.\nWhen dealing with audio recordings for which no score\nis available, it seems natural to focus on the alignment\nand comparison of the time-frequency representation of the\nmelodic contour. In the context of singing voice assess-\nment, Molina et al. used dynamic time warping to align\nfundamental frequency contours and calculate melodic and\nrhythmic deviations between them [2].\nDespite the growing interest in non-Western music tra-\nditions, most algorithms are designed and evaluated on West-\nern commercial music. In a ﬁrst genre-speciﬁc approach to\nmelodic similarity in ﬂamenco music, Cabrera et al. com-\nputed melodic similarity among a cappella singing perfor-\nmances from automatic descriptions [3]. The two stan-\ndard distance measures implemented, the editdistance and\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n65the correlation between pitch and interval histograms, ob-\ntained rather poor results when compared to expert judge-\nments. As proposed by Mora et al., better results for intra-\nand inter-style similarity can be obtained for a similarity\nmeasure based on manually extracted high-level features\n(i.e., the direction of melodic movement in a speciﬁc part\nof the performance) [4]. Such studies elucidate the need\nfor exploration of particular characteristics of non-Western\nmusic genres and the adaptation of existing music infor-\nmation retrieval systems to such styles.\nThe present study addresses perceived melodic similar-\nity in a cappella ﬂamenco singing from different stand-\npoints: with the aim of gaining insight into the mecha-\nnisms involved in perceiving melodies as more or less sim-\nilar, we gathered similarity ratings among performances of\nthe same style from na ¨ıve listeners as well as ﬂamenco\nexperts and analyzed them in terms of intra-subject and\nintra-group agreement. In order to isolate the melody from\nother variables such as lyrics, expression and dynamics,\nwe gathered the same ratings for synthesized melodic con-\ntours. We furthermore evaluated three computational mod-\nels for melodic similarity by analyzing the correlation be-\ntween computed similarity and human ratings. We com-\npared the results to distances computed from manually ex-\ntracted high-level features deﬁned by experts in the ﬁeld.\nThe rest of the paper is organized as follows. In Sec-\ntion 2 we provide background information on ﬂamenco\nmusic and the martinete style, which is the focus of this\nstudy. We give a detailed description of the database used\nin the present experiment in Section 3. Section 4 sum-\nmarizes the methodology of the listening experiments, the\nextracted high-level features and the implemented compu-\ntational similarity models. We give the results of the cor-\nrelation analysis in Section 5 and conclude our study in\nSection 6.\n2. BACKGROUND\nFlamenco is an oral tradition whose roots are as diverse as\nthe cultural inﬂuences of its area of origin, Andalusia, a\nregion in southern Spain. Its characteristics are inﬂuen-\nced by music traditions of a variety of immigrants and\ncolonizations that settled in the area throughout the past\ncenturies, among them Visigoths, Arabs, Jews and to a\nlarge extend gipsies, who decisively contributed to shape\nthe genre as we know it today. For a comprehensive and\ncomplete study on history and style, we refer to [5–7]. Fla-\nmenco germinated and nourished mainly from the singing\ntradition and until now the singing voice represents its cen-\ntral element, usually accompanied by the guitar and rhyth-\nmic hand-clapping. In the ﬂamenco jargon, songs, but also\nstyles, are referred to as cantes.\n2.1 The ﬂamenco singing voice\nFlamenco singing performances are usually spontaneous\nand highly improvisational. Songs are passed from gen-\neration to generation and only rarely manually transcribed.\nEven though there is no distinct ideal for timbre and several\n(a) Performance by Antonio Mairena\n(b) Performance by Chano Lobato\nFigure 1. Manual transcriptions of performances a debla\n“En el barrio de Triana”; Transcription: Joaquin Mora\nvoice types can be identiﬁed, the ﬂamenco singing voice\ncan be generally characterized as matt, breathy, and con-\ntaining few high frequency harmonics. Moreover, singers\nusually lack the singer’s formant [13]. Melodic movements\nappear mainly in conjunct degrees within a small pitch\nrange ( tessitura) of a major sixth interval and are char-\nacterized by insistence on recitative notes. Furthermore,\nsingers use a large amount melisma, microtonal ornamen-\ntation and pitch glides during note attacks [4].\n2.2 The ﬂamenco martinete\nMartinete is considered one of the oldest styles and forms\npart of the sub-genre of the ton´as, a group of unaccompa-\nnied singing styles, or cantes. As in other cantes, songs\nbelonging to martinete style are characterized by a com-\nmon melodic skeleton, which is subject to strong sponta-\nneous ornamentation and expressive prolongations. The\nuntrained listener might perceive two performances of the\nsame cante as very different and the fact that they belong to\nthe same style is not obvious at all. To illustrate this prin-\nciple, Figure 1 shows the transcription of two a cappella\nperformances in Western music notation, both belonging\nto the same style (debla) [4].\nFurthermore, the martinete is characterized by a solemn\nperformance in slow tempo with free rhythmic interpreta-\ntion. Traditionally, the voice is accompanied by hammer\nstrokes on an anvil. The tonality corresponds mainly to the\nmajor mode, whereas the third scale degree may be low-\nered occasionally, converting the scale to the minor mode.\n3. MUSIC COLLECTION\nIn consultation with ﬂamenco experts, we gathered 12 re-\ncordings of martinete performances, covering the most re-\npresentative singers of this style. This dataset represents\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n66Singer Percussion\nAntonio Mairena No\nChano Lobato No\nChocolate Yes\nJacinto Almad ´en No\nJesus Heredia No\nManuel Sim ´on Yes\nMiguel Vargas No\nNaranjito No\nPaco de Lucia No\nTalegon de C ´ordoba Yes\nTom´as Pav ´on No\nTurronero No\nTable 1. Dataset containing 12 martinete performances.\na subset of the ton´as1dataset, which contains a total of\n56martinete recordings. The average duration of the ex-\ntracted excerpts containing the ﬁrst verse is approximately\n20 seconds. We limited our study to such a small set,\nmainly due to the duration of the listening experiment. As\nan additional stimuli for the listening experiments, we fur-\nthermore created synthesized versions of all excerpts. We\nused the method described in [8] to extract fundamental\nfrequency and energy envelopes and re-synthesize with a\nsinusoid.\nWe selected the ﬁrst verse of each recording, containing\nthe characteristic exposition of the melodic skeleton. Al-\nthough some martinete recordings contain additional ac-\ncompaniment (guitar, bowing string or wind instruments),\nwe limited our selection to a cappella recordings without\nrhythmic accompaniment or with very sparse one, as it is\nfound traditionally. We intentionally incorporated a wide\nrange of interpretation characteristics, regarding richness\nin ornamentation, tempo, articulation and lyrics. Among\nthe singers listed in Table 1, Tom´as Pav ´onis to be men-\ntioned as the most inﬂuential artist in the a cappella singing\nstyles, performing the martinete in an exemplar way. Fur-\nthermore, Antonio Mairena andChocolate are thought to\nbe the main references for their singing abilities and knowl-\nedge of the singing styles. Chano Lobato omits some of the\nbasic notes during the melodic exposition and the perfor-\nmance has been included as an example of strong deviation\nin the melodic interpretation.\n4. METHODOLOGY\n4.1 Human similarity ratings\nIn order to obtain a ground truth for perceived melodic\nsimilarity among the selected excerpts, we conduct a lis-\ntening experiment in Montreal (Canada) with 24 na ¨ıve lis-\nteners with little or no previous exposure to ﬂamenco and\nin Sevilla (Spain) with 3 experts, as described in [9]. After\nevaluating various experiment designs (i.e. pair-wise com-\nparison), we decided to collect the similarity ratings in a\n1http://mtg.upf.edu/download/datasets/tonasfree sorting task [19]. Using the sonic mapper2software,\nsubjects were asked to create groups of similar interpre-\ntations, leaving the number of groups open. The partic-\nipants were explicitly instructed to focus on the melody\nonly, neglecting differences in voice timbre, lyrics, percus-\nsion accompaniment and sound quality. Nevertheless, in\norder to isolate the melodic line as a similarity criterion, the\nexperiment had also been conducted with the synthesized\nversions of the excerpts described above. For each ex-\ncerpt we extracted the fundamental frequency as described\nin [8] with a window length of 33 ms and a hop size of\n0.72 ms. The pitch contour was synthesized with a sin-\ngle sine wave. A similarity matrix was computed based\non the number of times a pair of performances had been\ngrouped together. We compared individual participants’\nsimilarity matrices using Mantel tests. The Mantel test can\nbe considered as the most widely used method to account\nfor distance correlations [12]. We used zt, a simple tool for\nMantel test, developed by Bonnet and Vande Peer [18].,\nand measured the correlation between participant matri-\nces. We observed that the average correlation for novices\nis\u0016= 0:0824, with a \u001b= 0:2109 and the average p-value:\n\u0016= 0:3391,\u001b= 0:2139 (min=0.002). This indicates\na very low agreement among them, and indicates differ-\nences in perception of melodic similarity depending on the\nlistener’s background. Although we should take these re-\nsults with caution given the small number of experts, we\nfound higher correlation values among them, with an av-\nerage correlation \u0016= 0:1891, and \u001b= 0:1170. For a\ndetailed description of the procedure and the analysis, we\nrefer to [9].\n4.2 Manually extracted high-level features\nWe manually extracted six high-level features deﬁned by\nexperts in the ﬁeld. As illustrated above, two cantes hav-\ning the same main notes and different ornamentation would\nbe perceived as the same cante by a ﬂamenco aﬁcionado.\nThis fact makes the automatic computation of the features\nunfeasible. Because of that, we had to rely on manual ex-\ntraction.\nThe high-level features were the following.\n1. Repetition of the ﬁrst hemistich. A hemistich is half-\nline of a verse; the presence of this repetition is im-\nportant in these cantes.\n2. Clivis/ﬂexa at the end of the ﬁrst hemistich. A clivis\nis a descending melodic movement. Here it refers to\na descending melodic contour between main notes.\nAgain, the ornamentation is not taken into account\nwhen detecting the presence of the clivis.\n3. Highest scale degree in the two ﬁrst hemistichs. The\nhighest scale degree reached during the cante is an\nimportant feature.\n4. Frequency of the highest degree in the second hemistich.\nHow many times that highest degree is reached is\nalso signiﬁcant.\n2http://www.music.mcgill.ca/ gary/mapper/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n675. Final note of the second hemistich.\n6. Duration (fast / regular / slow).\nA distance matrix was obtained by calculating the Eu-\nclidean distance among the feature vectors. The feature\nvectors were mostly composed of categorical data and we\nused a standardized Euclidean distance. For a detailed ex-\nplanation of the descriptors and their musicological back-\nground, the reader is referred to [4].\n4.3 Computational similarity measures\nWe implemented three computational measures based on\nfundamental frequency envelopes and automatic transcrip-\ntions and evaluated their suitability for modeling the per-\nceived melodic similarity by analyzing the correlation be-\ntween computed distance matrices and human judgements.\nThe fundamental frequency contours as well as the au-\ntomatically generated symbolic note representations were\nobtained using the system described in [8].\n4.3.1 Dynamic time warping alignment\nSimilar to [2] we used a dynamic time warping algorithm\nto align melodies and estimate their rhythmic and pitch\nsimilarity. Since vocal vibrato and microtonal ornamen-\ntations strongly inﬂuence the cost matrix, we instead align\ncontinuous contours of quantized pitch values obtained with\nthe automatic transcription described in [8]. The cost ma-\ntrixMdescribes the squared frequency deviation between\nall possible combinations of time frames between the two\nanalyzed contours f01andf02, where\u000bis a constraint\nlimiting the maximum cost:\nMi;j=min((f 01[i]\u0000f02[j])2;\u000b) (1)\nThe dynamic time warping algorithm determines the\noptimal path among the matrix Mfrom ﬁrst to last frame.\nThe deviation of the slope of the path pwith lengthNfrom\nthe diagonal path gives a measure for temporal deviation\n(DTWtemporal ),\n\u0001temp =PN\ni=1(p[i]\u0000pdiag[i])2\nN(2)\nwhile the average over its elements deﬁnes the pitch devi-\nation (DTWpitch):\n\u0001pitch =PN\ni=1p[i]\nN: (3)\nWe used a MATLAB implementation3, which extends\nthe algorithm with several restrictions in order to obtain a\nmusically meaningful temporal alignment. Figure 2 shows\nthe cost matrix and Figure 3 the unaligned and aligned pitch\nsequences.\n3http://www.ee.columbia.edu/ dpwe/resources/matlab/dtw/\ntime song1 [samples]time song2 [samples]Dynamic time warping\n500 1000 1500 2000 2500500\n1000\n1500\n2000\n2500Figure 2. Dynamic time warping: Cost matrix and optimal\npath.\nFigure 3. Unaligned (top) and aligned (bottom) melodic\ncontours.\n4.3.2 Global performance descriptors\nAs described in [10], we extracted a total of 13 global\ndescriptors from automatic transcriptions and computed a\nsimilarity matrix based on the Euclidean distance among\nfeature vectors. In order to determine the most suitable de-\nscriptors for this task, we analyzed the phylogenetric tree\n(Figure 4) computed from the distance matrix of expert\nsimilarity ratings. Here, we identify two main clusters, at\nlarge distance from each other.\nUsing these two clusters as classes in a classiﬁcation\ntask, we perform a support vector machine (SVM) subset\nselection in order to identify the descriptors that are best\nsuited to distinguish the two clusters. We accordingly ex-\ntracted the six best ranked descriptors for all songs and\ncomputed the similarity matrix from the Euclidean dis-\ntances among feature vectors. The extracted descriptors\nare summarized below:\n1.Amount of silence: Percentage of silent frames.\nClust er 1\nClust er 2\nFigure 4. Phylogenetic tree generated from expert similar-\nity judgements.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n68Figure 5. Harmonic pitch class proﬁle for a sung phrase\nwith a resolution of 12 bins per semitone.\n2.Average note duration in seconds.\n3.Note duration ﬂuctuation: Standard deviation of\nthe note duration in seconds.\n4.Average volume of the notes relative to the normal-\nized maximum.\n5.Volume ﬂuctuation: standard deviation of the note\nvolume relative to normalized maximum.\n6.Amount of ornamentation: Average per-frame dis-\ntance in [Hz] between the quantized note value and\nthe fundamental frequency contour.\n4.3.3 Chroma similarity\nWe implemented a similarity measure presented in [11]\nin the context of cover identiﬁcation: First, the harmonic\npitch class proﬁles (HPCP) are extracted on a global and a\nframe basis. The resulting pitch class histogram describes\nthe relative strength of the 12 pitch classes of the equal-\ntempered scale. HPCPs are robust to detuning as well as\nvariation in timbre and dynamics. After adjusting the key\nof one sequence to the other, a binary similarity matrix\nis computed based on the frame-wise extracted HPCPs.\nAgain, dynamic time warping was used to ﬁnd the best\npossible path among the similarity matrix. For a detailed\ndescription of the algorithm, we refer the reader to [11].\n4.4 Evaluation\nWe evaluated the suitability of the computational models\nfor this task by analyzing the correlation between com-\nputed similarity and human ratings. A common method\nto evaluate a possible relation between two distance ma-\ntrices is the Mantel test [12]: ﬁrst, the linear correlation\nbetween two matrices is measured with the Pearson corre-\nlation, which gives a value rbetween -1 and 1. A strong\ncorrelation is indicated by a value signiﬁcantly different\nfrom zero. To verify that a relation exists, the value is com-\npared to correlations to permuted versions of the matri-\nces. Here, 10000 random permutations are performed. The\nconﬁdence value pcorresponds to the proportion of permu-\ntations giving a higher correlation than the original matrix.Consequently, a conﬁdence value close to zero conﬁrms an\nexisting correlation.\n5. RESULTS\nFigure 6 shows the comparison of the computed similarity\nmeasures by means of correlation rand conﬁdence value\npfor the different participant groups and stimuli types. We\nﬁrst note that the distance measure obtained from manu-\nally extracted high-level descriptors seems to reﬂect best\nthe perceived melodic similarity for both, expert and na ¨ıve\nlisteners. Even though the computed similarity correlates\nstrongly with the expert ratings, the also strong relation\nwith the non-expert similarity judgments is still surpris-\ning, given the fact that the descriptors are based on rather\nabstract musicological concepts. We furthermore ﬁnd a\nweaker, but still signiﬁcant correlation between human rat-\nings and the temporal deviation measure of the dynamic\ntime warping algorithm as well as the vector distance among\nperformance descriptors. On the other hand, we ﬁnd no re-\nlation between human ratings and the pitch deviation from\nthe dynamically aligned sequences, nor the chroma sim-\nilarity measure. Given the fact that the selected perfor-\nmance descriptors are related to dynamic and temporal be-\nhavior and ornamentation and the temporal deviation mea-\nsure does not consider the absolute pitch difference of the\naligned sequences, we can speculate that for the given ma-\nterial these factors inﬂuence perceived similarity stronger\nthan differences in the pitch progression. Martinete presents\na particularly interesting case, since the skeleton of the\nmelodic contour and at least its outer envelope is preserved\nthroughout the performances. Notice also that in all cases\nthe found correlation with the similarity ratings of real record-\nings is stronger than for the synthesized versions. Since\nnone of the computational methods take voice timbre or\nlyrics into account, we can preclude that these factors in-\nﬂuenced human judgement. It is however possible that it\nwas more difﬁcult for the listener to internalize these syn-\nthesized sequences compared to real recordings given their\nartiﬁcial nature and consequently judging similarity was\nmore difﬁcult and less precise.\n6. CONCLUSIONS\nThe present study investigates the mechanisms involved\nin the perception of melodic similarity for the particular\ncase of a cappella ﬂamenco singing. We compared hu-\nman judgements from experts and na ¨ıve listeners for au-\ndio recordings and synthesized melodic contours. Com-\nputational models are furthermore used to create distance\nmatrices and evaluated based on their correlation with hu-\nman ratings. We observed a signiﬁcantly higher agreement\namong experts and a stronger correlation among compu-\ntational models and the ratings based on real recordings\nthan when comparing to ratings for synthesized melodies.\nFurthermore, we discover that models based on descriptors\nrelated to rhythm, dynamics and ornamentation are better\nsuited to recreate similarity judgements than models based\non absolute pitch distance. We obtained the highest corre-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n69Figure 6. Correlation between computed similarity and\nhuman ratings. Statistically signiﬁcant results are marked\ngrey.\nlation for both expert and non-expert ratings for a similar-\nity measure computed from manually extracted high-level\nfeatures. The problem of how to compute the high-level\nfeatures automatically is still open. This problem is equiv-\nalent to that of automatically detecting ornamentation and\nmain notes in a ﬂamenco cante.\nAcknowledgements\nThe authors would like to thank Joaquin Mora for pro-\nviding the manual transcriptions and Joan Serr ´a for com-\nputing the chroma similarity measures. This research is\npartly funded by the COFLA (Proyectos de Excelencia de\nla Junta de Andalucia, P12-TIC-1362) and SIGMUS (Span-\nish Ministry of Economy and Competitiveness, TIN2012-\n36650) research projects as well as the PhD fellowship pro-\ngram of the Department of Information and Communica-\ntion Technologies, Universitat Pompeu Fabra.\n7. REFERENCES\n[1] A. Marsden: “Interrogating Melodic Similarity: A\nDeﬁnitive Phenomenon or the Product of Interpreta-\ntion?” Journal of New Music Research, V ol. 4, No. 44,\npp. 323–335, 2012.\n[2] E. Molina, I. Barbancho, E. G ´omez, A. M. Barbanco,\nand L. J. Tard ´on: “Fundamental frequency alignment\nvs. note-based melodic similarity for singing voice as-\nsessment,” Proceedings of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), 2013.\n[3] J. J. Cabrera, J. M. D ´ıaz-B ´a˜nez, F. J. Escobar,\nE. G ´omez, F. G ´omez, and J. Mora: “Comparative\nMelodic Analysis of A Cappella Flamenco Cantes,”\nProceedings of the Conference on Interdisciplinary\nMusicology, 2008.\n[4] J. Mora, F. G ´omez, E. G ´omez, F. J. Escobar, and J. M.\nD´ıaz-B ´a˜nez: “Melodic Characterization and Similarity\nin A Cappella Flamenco Cantes,” Proceedings of the\n11th International Society for Music Information Re-\ntrieval Conference (ISMIR), 2010.[5] J. Blas Vega, and M. R ´ıos Ruiz: Diccionario enci-\nclopdico ilustrado del ﬂamenco, Cinterco,1988.\n[6] J. L. Navarro ,and M. Ropero: Historia del ﬂamenco ,\nTartessos, 1995.\n[7] J. M. Gamboa: Una historia del ﬂamenco, Espasa-\nCalpe, 2005.\n[8] E. G ´omez, and J. Bonada: “Towards Computer-\nAssisted Flamenco Transcription: An Experimental\nComparison of Automatic Transcription Algorithms\nas Applied to A Cappella Singing,” Computer Music\nJournal, V ol. 37, No. 2, pp. 73-90, 2013.\n[9] E. G ´omez, C. Guastavino, F. G ´omez, and J. Bonada:\n“Analyzing Melodic Similarity Judgements in Fla-\nmenco A Cappella Singing,” Proceedings of the Inter-\nnational Conference on Music Perception and Cogni-\ntion, 2012.\n[10] N. Kroher: The Flamenco Cante: Automatic Char-\nacterization of Flamenco Singing by Analyzing Audio\nRecordings, Master Thesis, Universitat Pompeu Fabra,\n2013.\n[11] J. Serra, E. G ´omez, P Herrera, and X. Serra: “Chroma\nBinary Similarity and Local Alignment Applied to\nCover Song Identiﬁcation,” IEEE Transactions on Au-\ndio, Speech and Language Processing, V ol. 16, No. 6,\npp. 1138-1151, 2008..\n[12] N. Mantel, and R. S. Valand: “A technique of non-\nparametric multivariate analysis,” Biometrics, V ol. 26,\npp. 547-558, 1970.\n[13] J.Sundberg: “The acoustics of the singing voice,” Sci-\nentiﬁc American, V ol. 236 (3), pp.104-116, 1977.\n[14] D. Muellensiefen, and K. Frieler: “Modelling experts’\nnotions of melodic similarity,” Musicae Scientiae, Dis-\ncussion Forum 4A, pp.183-210, 2007.\n[15] A. V olk, and P. van Kranenburg: “Melodic similarity\namong folk songs: An annotation study on similarity-\nbased categorization in music,” Musicae Scientiae, 16\n(3) pp.317-339, 2012.\n[16] R. Typke, and F. Wiering: “Transportation distances\nand human perception of melodic similarity,” Musicae\nScientiae, Discussion Forum 4A, pp.153-181, 2007.\n[17] T. Eerola, T Jaervinen, J. Louhivuori, and P. Toivia-\nnen, : “Statistical Features and Perceived Similarity of\nFolk Melodies,” Music Perception, 18 (3), pp.275-296,\n2001.\n[18] E. Bonnet, and Y . Van de Peer : “zt: a software tool for\nsimple and partial Mantel tests,” Journal of Statistical\nsoftware, 7 (10), pp.1-12, 2002.\n[19] B. Giordano, C. Guastavino, E. Murphy, M. Ogg, and\nB.K. Smith: ”Comparison of Dissimilarity Estimation\nMethods”. Multivariate Behavioral Research, 46, 1-\n33, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n70"
    },
    {
        "title": "Keyword Spotting in A-capella Singing.",
        "author": [
            "Anna M. Kruspe"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416870",
        "url": "https://doi.org/10.5281/zenodo.1416870",
        "ee": "https://zenodo.org/records/1416870/files/Kruspe14.pdf",
        "abstract": "Keyword spotting (or spoken term detection) is an inter- esting task in Music Information Retrieval that can be ap- plied to a number of problems. Its purposes include topi- cal search and improvements for genre classification. Key- word spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher vari- ations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel- Frequency Cepstral Coefficents (MFCCs), Perceptual Lin- ear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we aver- age the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classi- fier.",
        "zenodo_id": 1416870,
        "dblp_key": "conf/ismir/Kruspe14",
        "keywords": [
            "Keyword Spotting",
            "A-Capella Singing",
            "Spoken Term Detection",
            "Hidden Markov Models (HMMs)",
            "Mel-Frequency Cepstral Coefficients (MFCCs)",
            "Perceptual Linear Predictive Features (PLPs)",
            "Temporal Patterns (TRAPs)",
            "Multilayer Perceptrons (MLPs)",
            "Phoneme Posteriors",
            "Speech Recognition"
        ],
        "content": "KEYWORD SPOTTING IN A-CAPELLA SINGING\nAnna M. Kruspe\nFraunhofer IDMT, Ilmenau, Germany\nJohns Hopkins University, Baltimore, MD, USA\nkpe@idmt.fhg.de\nABSTRACT\nKeyword spotting (or spoken term detection) is an inter-\nesting task in Music Information Retrieval that can be ap-\nplied to a number of problems. Its purposes include topi-\ncal search and improvements for genre classiﬁcation. Key-\nword spotting is a well-researched task on pure speech, but\nstate-of-the-art approaches cannot be easily transferred to\nsinging because phoneme durations have much higher vari-\nations in singing. To our knowledge, no keyword spotting\nsystem for singing has been presented yet.\nWe present a keyword spotting approach based on\nkeyword-ﬁller Hidden Markov Models (HMMs) and test\nit on a-capella singing and spoken lyrics. We test Mel-\nFrequency Cepstral Coefﬁcents (MFCCs), Perceptual Lin-\near Predictive Features (PLPs), and Temporal Patterns\n(TRAPs) as front ends. These features are then used to\ngenerate phoneme posteriors using Multilayer Perceptrons\n(MLPs) trained on speech data. The phoneme posteriors\nare then used as the system input. Our approach produces\nuseful results on a-capella singing, but depend heavily on\nthe chosen keyword. We show that results can be further\nimproved by training the MLP on a-capella data.\nWe also test two post-processing methods on our phoneme\nposteriors before the keyword spotting step. First, we aver-\nage the posteriors of all three feature sets. Second, we run\nthe three concatenated posteriors through a fusion classi-\nﬁer.\n1. INTRODUCTION\nKeyword spotting is the task of searching for certain words\nor phrases (spoken term detection) in acoustic data. In con-\ntrast to text data, we cannot directly search for these words,\nbut have to rely on the output of speech recognition sys-\ntems in some way.\nIn speech, this problem has been a topic of research since\nthe 1970’s [1] and has since seen a lot of development and\nimprovement [11]. For singing, however, we are not aware\nof any fully functional keyword spotting systems.\nMusic collections of both professional distributors and pri-\nvate users have grown exponentially since the switch to a\ndigital format. For these large collections, efﬁcient search\nc\rAnna M. Kruspe.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Anna M. Kruspe. “Keyword spotting\nin a-capella singing”, 15th International Society for Music Information\nRetrieval Conference, 2014.methods are necessary. Keyword spotting in music col-\nlections has beneﬁcial applications for both user groups.\nUsing keyword spotting, users are able to search their col-\nlections for songs with lyrics about certain topics. As an\nexample, professional users might use this in the context\nof synch licensing [4] (e.g., “I need a song containing\nthe word ’freedom’ for a car commercial”.) Private users\ncould, for example, use keyword spotting for playlist gen-\neration (“Generate a playlist with songs that contain the\nword ‘party’.”)\nIn this paper, we present our approach to a keyword spot-\nting system for a-capella singing. We will ﬁrst look at the\ncurrent state of the art in section 2. We then present our\ndata set in section 3. In section 4, we describe our own\nkeyword spotting system. A number of experiments on this\nsystem and their results are presented in section 5. Finally,\nwe draw conclusions in section 6 and give an outlook on\nfuture work in section 7.\n2. STATE OF THE ART\n2.1 Keyword spotting principles\nAs described in [13], there are three basic principles that\nhave been developed over the years for keyword spotting\nin speech:\nLVCSR-based keyword spotting For this approach, full\nLarge V ocabulary Continues Speech Recognition\n(LVCSR) is performed on the utterances. This re-\nsults in a complete text transcription, which can then\nbe searched for the required keywords. LVCSR-\nbased systems lack tolerance for description errors -\ni.e., if a keyword is not correctly transcribed from the\nstart, it cannot be found later. Additionally, LVCSR\nsystems are complex and expensive to implement.\nAcoustic keyword spotting As in LVCSR-based key-\nword spotting, acoustic keyword spotting employs\nViterbi search to ﬁnd the requested keyword in a\ngiven utterance. In this approach, however, the sys-\ntem does not attempt to transcribe each word, but\nonly searches for the speciﬁc keyword. Everything\nelse is treated as “ﬁller”. This search can be per-\nformed directly on the audio features using an acous-\ntic example, or on phoneme posteriorgrams gener-\nated by an acoustic model. In the second case, the\nalgorithm searches for the word’s phonemes.\nThis approach is easy to implement and provides\nsome pronunciation tolerance. Its disadvantage is\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n271the lack of integration of a-priori language knowl-\nedge (i.e. knowledge about plausible phoneme and\nword sequences) that could improve performance.\nPhonetic search keyword spotting Phonetic search key-\nword spotting starts out just like LVCSR-based key-\nword spotting, but does not generate a word tran-\nscription of the utterance. Instead, phoneme lattices\nare saved. Phonetic search for the keyword is then\nperformed on these lattices. This approach combines\nthe advantages of LVCSR-based keyword spotting\n(a-priori knowledge in the shape of language mod-\nels) and acoustic keyword spotting (ﬂexibility and\nrobustness).\n2.2 Keyword spotting in singing\nThe described keyword spotting principles cannot easily\nbe transferred to music. Singing, in contrast to speech,\npresents a number of additional challenges, such as larger\npitch ﬂuctuation, more pronunciation variation, and differ-\nent vocabulary (which means existing models cannot easily\nbe transferred).\nAnother big difference is the higher variation of phoneme\ndurations in singing. Both LVCSR-based keyword spot-\nting and Phonetic search keyword spotting depend heavily\non predictable phoneme durations (within certain limits).\nWhen a certain word is pronounced, its phonemes will usu-\nally have approximately the same duration across speak-\ners. The language model employed in both approaches will\ntake this information into account.\nWe compared phoneme durations in the TIMIT speech\ndatabase [7] and our own a-capella singing database (see\nsection 3). The average standard deviations for vowels and\nconsonants are shown in ﬁgure 1. It becomes clear that\nthe phoneme durations taken from TIMIT do not vary a\nlot, whereas some the a-capella phonemes show huge vari-\nations. It becomes clear that this especially concerns vow-\nels (AA, AW, EH, IY , AE, AH, AO, EY , AY , ER, UW, OW,\nUH, IH, OY). This observation has a foundation in music\ntheory: Drawn-out notes are usually sung on vowels.\nFor this reason, acoustic keyword spotting appears to be\nthe most feasible approach to keyword spotting in singing.\nTo our knowledge, no full keyword spotting system for\nsinging has been presented yet. In [2], an approach based\non sub-sequence Dynamic Time Warping (DTW) is sug-\ngested. This is similar to the acoustic approach, but does\nnot involve a full acoustic model. Instead, example utter-\nances of the keyword are used to ﬁnd similar sequences in\nthe tested utterance.\nIn [5], a phoneme recognition system for singing is pre-\nsented. It extracts Mel-Frequency Cepstral Coefﬁcients\n(MFCCs) and Temporal Patterns (TRAPs) which are then\nused as inputs to a Multilayer Perceptron (MLP). The pho-\nnetic output of such a system could serve as an input to a\nkeyword spotting system.\nThere are also some publications where similar principles\nare applied to lyrics alignment and Query by Humming\n[12] [3].\nFigure 1: Average standard deviations for vowels and con-\nsonants in the TIMIT speech databases (blue) and our a-\ncapella singing data set (green).\n3. DATA SET\nOur data set is the one presented in [5]. It consists of the\nvocal tracks of 19 commercial pop songs. They are studio\nquality with some post-processing applied (EQ, compres-\nsion, reverb). Some of them contain choir singing. These\n19 songs are split up into clips that roughly represent lines\nin the song lyrics.\nTwelve of the songs were annotated with time-aligned\nphonemes. The phoneme set is the one used in CMU\nSphinx1and TIMIT [7] and contains 39 phonemes. All\nof the songs were annotated with word transcriptions. For\ncomparison, recordings of spoken recitations of all song\nlyrics were also made. These were all performed by the\nsame speaker.\nWe selected 51 keywords for testing our system. Most of\nthem were among the most frequent words in the provided\nlyrics. A few were selected because they had a compara-\ntively large number of phonemes. An overview is given in\ntable 1.\n4. PROPOSED SYSTEM\nFigure 2 presents an overview of our system.\n1. Feature extraction We extract Mel-Frequency Cep-\nstral Coefﬁcients (MFCCs), Perceptual Linear Pre-\ndictive features (PLPs), and Temporal Patterns\n(TRAPs) [6]. We keep 20 MFCC coefﬁcients and 39\nPLP coefﬁcients (13 direct coefﬁcients plus deltas\nand double-deltas). For the TRAPs, we use 8 lin-\nearly spaced spectral bands and a temporal context\nof 20 frames and keep 8 DCT coefﬁcients.\n2. MLP training and phoneme recognition Using each\nfeature data set, we train Multi-Layer Perceptrons\n(MLPs). MLPs are commonly used to train acoustic\nmodels for the purpose of phoneme recognition. We\nchose a structure with two hidden layers and tested\nthree different dimension settings: 50, 200, and 1000\ndimensions per layer. MLPs were trained solely on\nTIMIT data ﬁrst, then on a mix of TIMIT and a-\ncapella in a second experiment. The resulting MLPs\nare then used to recognize phonemes in our a-capella\ndataset, thus generating phoneme posteriorgrams.\n1http://cmusphinx.sourceforge.net/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n272Number of Phonemes Keywords\n2 way, eyes\n3 love, girl, away, time, over, home, sing, kiss, play, other\n4 hello, trick, never, hand, baby, times, under, things, world, think, heart, tears, lights\n5 always, inside, drink, nothing, rehab, forever, rolling, feeling, waiting, alright, tonight\n6 something, denial, together, morning, friends, leaving, sunrise\n7 umbrella, afternoon, stranger, somebody, entertain, everyone\n8 beautiful, suicidal\nTable 1: All 51 tested keywords, ordered by number of phonemes.\nFigure 2: Overview of our keyword spotting system. Variable parameters are shown in italics.\nThe following two points described optional post-\nprocessing steps on the phoneme posteriorgrams.\n3a. Posteriorgram merging For this post-processing\nstep, we take the phoneme posterior results that\nwere obtained using different feature sets and\naverage them. We tested both the combinations of\nPLP+MFCC, PLP+TRAP, and PLP+MFCC+TRAP.\n3b. Fusion MLP classiﬁer As a second post-processing\noption, we concatenate phoneme posteriors obtained\nby using different feature sets and run them through\na fusion MLP classiﬁer to create better posteri-\nors. We again tested the combinations PLP+MFCC,\nPLP+TRAP, and PLP+MFCC+TRAP.\n4. Keyword spotting The resulting phoneme posterior-\ngrams are then used to perform the actual keyword\nspotting. As mentioned above, we employ an acous-\ntic approach. It is based on keyword-ﬁller Hidden\nMarkov Models (HMMs) and has been described\nin [14] and [8].\nIn general, two separate HMMs are created: One for\nthe requested keyword, and one for all non-keyword\nregions (=ﬁller). The keyword HMM is generated\nusing a simple left-to-right topology with one state\nper keyword phoneme, while the ﬁller HMM is a\nfully connected loop of states for all phonemes.\nThese two HMMs are then joined. Using this com-\nposite HMM, a Viterbi decode is performed on the\nphoneme posteriorgrams. Whenever the Viterbi path\npasses through the keyword HMM, the keyword is\ndetected. The likelihood of this path can then be\ncompared to an alternative path through the ﬁller\nHMM, resulting in a detection score. A threshold\nFigure 3: Keyword-ﬁller HMM for the keyword “greasy”\nwith ﬁller path on the left hand side and two possible key-\nword pronunciation paths on the right hand side. The pa-\nrameter\fdetermines the transition probability between\nthe ﬁller HMM and the keyword HMM. [8]\ncan be employed to only return highly scored occur-\nrences. Additionally, the parameter \fcan be tuned\nto adjust the model. It determines the likelihood of\ntransitioning from the ﬁller HMM to the keyword\nHMM. The whole process is illustrated in ﬁgure 3.\nWe use theF1measure for evaluation. Results are consid-\nered to be true positives when a keyword is spotted some-\nwhere in an expected utterance. Since most utterances con-\ntain one to ten words, we consider this to be sufﬁciently ex-\nact. Additionally, we evaluate the precision of the results.\nFor the use cases described in section 1, users will usually\nonly require a number of correct results, but not necessarily\nall the occurrences of the keyword in the whole database.\nWe consider a result to be correct when the keyword is\nfound as part of another word with the same pronuncia-\ntion. The reasoning behind this is that a user who searched\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n273Figure 4:F1measures for a-capella data (left) and speech\n(right) when using PLP, MFCC, or TRAP features. The\nMLPs for phoneme recognition had two hidden layers with\n50, 200, or 1000 nodes each.\nfor the keyword “time” might also accept occurrences of\nthe word “times” as correct.\n5. EXPERIMENTS\n5.1 Experiment 1: Oracle search\nAs a precursor to the following experiments, we ﬁrst tested\nour keyword spotting approach on oracle posteriorgrams\nfor the a-capella data. This was done to test the general\nfeasibility of the algorithm for keyword spotting on singing\ndata with its highly variable phoneme durations.\nThe oracle posteriorgrams were generated by converting\nthe phoneme annotations to posteriorgram format by set-\nting the likelihoods of the annotated phonemes to 1during\nthe corresponding time segment and everything else to 0.\nA keyword search on these posteriorgrams resulted in F1\nmeasures of 1for almost all keywords. In cases where\nthe result was not 1, we narrowed the reasons down to an-\nnotation errors and pronunciation variants that we did not\naccount for. We conclude that our keyword-ﬁller approach\nis generally useful for keyword spotting on a-capella data,\nand our focus in the following experiments is on obtaining\ngood posteriorgrams from the audio data.\n5.2 Experiment 2: A-Capella vs. Speech\nFor our ﬁrst experiment, we run our keyword spotting sys-\ntem on the a-capella singing data, and on the same utter-\nances spoken by a single speaker. We evaluate all three fea-\nture datasets (MFCC, PLP, TRAP) separately. The recog-\nnition MLP is trained on TIMIT speech data only. We also\ntest three different sizes for the two hidden MLP layers:\n50 nodes, 200 nodes, and 1000 nodes in each layer. The\nresults are shown in ﬁgure 4.\nAs described in section 2.2, we expected keyword spotting\non singing to be more difﬁcult than on pure speech because\nof a larger pitch range, more pronunciation variations, etc.\nOur results support this assumption: In speech, keywords\nare recognized with an average F1measure of 33% using\nonly PLP features, while the same system results in an av-\nerageF1of only 10% on a-capella singing.\nFor both data sets, an MLP with 200 nodes in the hidden\nlayers shows a notable improvement over one with just 50.\nWhen using 1000 nodes, the result still improves by a few\npercent in most cases.\nWhen looking at the features, PLP features seem to work\nFigure 5:F1measures for a-capella data (left) and speech\n(right) when the recognition is trained only on TIMIT\nspeech data (blue) or on a mix of TIMIT and a-capella data\n(green).\nbest by a large margin, with TRAPs coming in second. It\nis notable, however, that some keywords can be detected\nmuch better when using MFCCs or TRAPs than PLPs (e.g.\n“sing”, “other”, “hand”, “world”, “tears”, “alright”). As\ndescribed in [5] and [10], certain feature sets represent\nsome phonemes better than others and can therefore bal-\nance each other out. A combination of the features might\ntherefore improve the whole system.\nEvaluation of the average precision (instead of F1mea-\nsure) shows the same general trend. The best results are\nagain obtained when using PLP features and the largest\nMLP. The average precision in this conﬁguration is 16%\nfor a-capella singing and 37% for speech. (While the dif-\nference is obvious, the result is still far from perfect for\nspeech. This demonstrates the difﬁculty of the recognition\nprocess without a-priori knowledge.)\n5.3 Experiment 3: Training including a-capella data\nAs a measure to improve the phoneme posteriorgrams for\na-capella singing, we next train our recognition MLP with\nboth TIMIT and a part of the a-capella data. We mix in\nabout 50% of the a-capella clips with the TIMIT data.\nThey make up about 10% of the TIMIT speech data. The\nresults are shown in ﬁgure 5 (only the results for the largest\nMLP are shown).\nThis step improves the keyword recognition on a-capella\ndata massively in all feature and MLP conﬁgurations. The\nbest result still comes from the biggest MLP when using\nPLP features and is now an average F1of24%. This step\nmakes the recognition MLP less speciﬁc to the properties\nof pure speech and therefore does not improve the results\nfor the speech data very much. It actually degrades the best\nresult somewhat.\nThe effect on the average precision is even greater. The a-\ncapella results are improved by 10to15percentage points\nfor each feature set. On speech data, the PLP precision\ndecreases by 7percentage points.\n5.4 Experiment 4: Posterior merging\nAs mentioned in experiment 2, certain feature sets seem to\nrepresent some keywords better than others. We therefore\nconcluded that combining the results for all features could\nimprove the recognition result.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n274Figure 6:F1measures for a-capella data (left) and speech\n(right) when posteriorgrams for two or three features are\nmerged. The conﬁgurations PLP+MFCC, PLP+TRAP, and\nPLP+MFCC+TRAP are shown and compared to the PLP\nonly result.\nFigure 7: F1measures for a-capella data (left) and\nspeech (right) when posteriorgrams for two or three fea-\ntures are fed into a fusion classiﬁer. The conﬁgura-\ntions PLP+MFCC, PLP+TRAP, and PLP+MFCC+TRAP\nare shown and compared to the PLP only result.\nTo this end, we tested merging the phoneme posterior-\ngrams between the MLP phoneme recognition step and the\nHMM keyword spotting step. In order to do this, we sim-\nply calculated the average values across the posteriors ob-\ntained using the three different feature data sets. This was\ndone for all phonemes and time frames. Keyword spotting\nwas then performed on the merged posteriorgrams. We\ntested the conﬁgurations PLP+MFCC, PLP+TRAP, and\nPLP+MFCC+TRAP. The results are shown in ﬁgure 6.\nPosterior merging seems to improve the results for a-\ncapella singing somewhat and works best when all three\nfeature sets are used. The F1measure on a-capella singing\nimproves from 24% (PLP) to 27%. It does not improve the\nspeech result, where PLP remains the best feature set.\n5.5 Experiment 5: Fusion classiﬁer\nAfter the posterior merging, we tested a second method\nof combining the feature-wise posteriorgrams. In this\nsecond method, we concatenated the posteriorgrams ob-\ntained from two or all three of the feature-wise MLP rec-\nognizers and ran them through a second MLP classiﬁer.\nThis fusion MLP was trained on a subset of the a-capella\ndata. This fusion classiﬁer generates new, hopefully im-\nproved phoneme posteriorgrams. HMM keyword spot-\nting is then performed on these new posteriorgrams. We\nagain tested the conﬁgurations PLP+MFCC, PLP+TRAP,\nand PLP+MFCC+TRAP. The results are shown in ﬁgure 7.\nThe fusion classiﬁer improves the F1measure for a-capella\nsinging by 5percentage points. The best result of 29% is\nobtained when all three feature sets are used. Precisionimproves from 24% to31%. However, the fusion classiﬁer\nmakes the system less speciﬁc towards speech and there-\nfore decreases the performance on speech data.\n5.6 Variation across keywords\nThe various results we presented in the previous exper-\niments varies widely across the 51 keywords. This is\na common phenomenon in keyword spotting. In many\napproaches, longer keywords are recognized better than\nshorter ones because the Viterbi path becomes more re-\nliable with each additional phoneme. This general trend\ncan also be seen in our results, but even keywords with the\nsame number of phonemes vary a lot. The precisions vary\nsimilarly, ranging between 2%and100%.\nWhen taking just the 50% of the keywords that can be rec-\nognized best, the average F1measure for the best approach\n(fusion MLP) jumps from 29% to44%. Its precision in-\ncreases from 31% to46%. We believe the extremely bad\nperformance of some keywords is in part due to the small\nsize of our data set. Some keywords occurred in just one\nof the 19 songs and were, for example, not recognized be-\ncause the singer used an unusual pronunciation in each oc-\ncurrence or had an accent that the phoneme recognition\nMLP was not trained with. We therefore believe these re-\nsults could improve massively when more training data is\nused.\n6. CONCLUSION\nIn this paper, we demonstrated a ﬁrst keyword spotting ap-\nproach for a-capella singing. We ran experiments for 51\nkeywords on a database of 19 a-capella pop songs and\nrecordings of the spoken lyrics. As our approach, we\nselected acoustic keyword spotting using keyword-ﬁller\nHMMs. Other keyword spotting approaches depend on\nlearning average phoneme durations, which vary a lot more\nin a-capella singing than in speech. These approaches\ntherefore cannot directly be transferred.\nAs a ﬁrst experiment, we tested our approach on oracle\nphoneme posteriorgrams and obtained almost perfect re-\nsults. We then produced “real world” posteriorgrams using\nMLPs with two hidden layers which had been trained on\nTIMIT speech data. We tested PLP, MFCC, and TRAP\nfeatures. The training yielded MLPs with 50, 200, and\n1000 nodes per hidden layer. We observed that the 200\nnode MLP produced signiﬁcantly better results than the 50\nnode MLPs in all cases (p< 0:0027), while the 1000 node\nMLPs only improved upon this result somewhat. PLP fea-\ntures performed signiﬁcantly better than the two other fea-\nture sets. Finally, keywords were detected much better in\nspeech than in a-capella singing. We expected this result\ndue to the speciﬁc characteristics of singing data (higher\nvariance of frequencies, more pronunciation variants).\nWe then tried training the MLPs with a mixture of TIMIT\nspeech data and a portion of our a-capella data. This im-\nproved the results for a-capella singing greatly.\nWe noticed that some keywords were recognized better\nwhen MFCCs or TRAPs were used instead of PLPs. We\ntherefore tried two approaches to combine the results for\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n275all three features: Posterior merging and fusion classiﬁers.\nBoth approaches improved the results on the a-capella\ndata. The best overall result for a-capella data was pro-\nduced by a fusion classiﬁer that combined all three features\n(29%).\nAs expected, keyword spotting on a-capella singing proved\nto be a harder task than on speech. The results varied\nwidely between keywords. Some of the very low results\narise because the keyword in question only occurred in one\nsong where the singer used an unusual pronunciation or\nhad an accent. The small size of our data set also poses a\nproblem when considering the limited number of singers.\nThe acoustic model trained on speech data and a part of the\na-capella data might be subject to overﬁtting to the singers’\nvocal characteristics.\nIn contrast, the recognition worked almost perfectly for\nkeywords with more training data. Keyword length also\nplayed a role. When using only the 50% best keywords,\nthe averageF1measure increased by 15percentage points.\nFinally, there are many applications where precision plays\na greater role than recall, as described in section 4. Our\nsystem can be tuned to achieve higher precisions than F1\nmeasures and is therefore also useful for these applications.\nWe believe that the key to better keyword spotting results\nlies in better phoneme posteriorgrams. A larger a-capella\ndata set would therefore be very useful for further tests and\nwould provide more consistent results.\n7. FUTURE WORK\nAs mentioned in section 2, more sophisticated keyword\nspotting systems for speech incorporate knowledge about\nplausible phoneme durations (e.g. [9]). In section 2.2, we\nshowed why this approach is not directly transferable to\nsinging: The vowel durations vary too much. However,\nconsonants are not affected. We would therefore like to\nstart integrating knowledge about average consonant dura-\ntions in order to improve our keyword spotting system. In\nthis way, we hope to improve the results for the keywords\nthat were not recognized well by our system.\nFollowing this line of thought, we could include even more\nlanguage-speciﬁc knowledge in the shape of a language\nmodel that also contains phonotactic information, word\nfrequencies, and phrase frequencies. We could thus move\nfrom a purely acoustic approach to a phonetic (lattice-\nbased) approach.\nWe will also start applying our approaches to polyphonic\nmusic instead of a-capella singing. To achieve good results\non polyphonic data, pre-processing will be necessary (e.g.\nvocal activity detection and source separation).\n8. REFERENCES\n[1] J. S. Bridle. An efﬁcient elastic-template method for\ndetecting given words in running speech. In Brit.\nAcoust. Soc. Meeting, pages 1 – 4, 1973.\n[2] C. Dittmar, P. Mercado, H. Grossmann, and E. Cano.\nTowards lyrics spotting in the SyncGlobal project. In3rd International Workshop on Cognitive Information\nProcessing (CIP), 2012.\n[3] H. Fujihara and M. Goto. Three techniques for im-\nproving automatic synchronization between music and\nlyrics: Fricative detection, ﬁller model, and novel fea-\nture vectors for vocal activity detection. In IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 69–72, Las Vegas, NV ,\nUSA, 2008.\n[4] H. Grossmann, A. Kruspe, J. Abesser, and H. Luka-\nshevich. Towards cross-modal search and synchroniza-\ntion of music and video. In International Congress on\nComputer Science Information Systems and Technolo-\ngies (CSIST), Minsk, Belarus, 2011.\n[5] J. K. Hansen. Recognition of phonemes in a-cappella\nrecordings using temporal patterns and mel frequency\ncepstral coefﬁcients. In 9th Sound and Music Comput-\ning Conference (SMC), pages 494–499, Copenhagen,\nDenmark, 2012.\n[6] H. Hermansky and S. Sharma. Traps – classiﬁers of\ntemporal patterns. In Proceedings of the 5th Inter-\nnational Conference on Spoken Language Processing\n(ICSLP), pages 1003–1006, Sydney, Australia, 1998.\n[7] J. S. Garofolo et al. TIMIT Acoustic-Phonetic Contin-\nuous Speech Corpus. Technical report, Linguistic Data\nConsortium, Philadelphia, 1993.\n[8] A. Jansen and P. Niyogi. An experimental evaluation\nof keyword-ﬁller hidden markov models. Technical re-\nport, Department of Computer Science, University of\nChicago, 2009.\n[9] K. Kintzley, A. Jansen, K. Church, and H. Hermansky.\nInverting the point process model for fast phonetic key-\nword search. In INTERSPEECH. ISCA, 2012.\n[10] A. M. Kruspe, J. Abesser, and C. Dittmar. A GMM ap-\nproach to singing language identiﬁcation. In 53rd AES\nConference on Semantic Audio, London, UK, 2014.\n[11] A. Mandal, K. R. P. Kumar, and P. Mitra. Recent devel-\nopments in spoken term detection: a survey. Interna-\ntional Journal of Speech Technology, 17(2):183–198,\nJune 2014.\n[12] A. Mesaros and T. Virtanen. Automatic recognition of\nlyrics in singing. EURASIP Journal on Audio, Speech,\nand Music Processing, 2010(4), January 2010.\n[13] A. Moyal, V . Aharonson, E. Tetariy, and M. Gishri.\nPhonetic Search Methods for Large Speech Databases,\nchapter 2: Keyword spotting methods. Springer, 2013.\n[14] I. Szoeke, P. Schwarz, P. Matejka, L. Burget,\nM. Karaﬁat, and J. Cernocky. Phoneme based acous-\ntics keyword spotting in informal continuous speech.\nIn V . Matousek, P. Mautner, and T. Pavelka, editors,\nTSD, volume 3658 of Lecture Notes in Computer Sci-\nence, pages 302–309. Springer, 2005.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n276"
    },
    {
        "title": "Automatic Melody Transcription based on Chord Transcription.",
        "author": [
            "Antti Laaksonen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415218",
        "url": "https://doi.org/10.5281/zenodo.1415218",
        "ee": "https://zenodo.org/records/1415218/files/Laaksonen14.pdf",
        "abstract": "This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord tran- scription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To eval- uate the algorithm, we present a new ground truth dataset that consists of 1,5 hours of audio excerpts together with hand-made melody and chord transcriptions.",
        "zenodo_id": 1415218,
        "dblp_key": "conf/ismir/Laaksonen14",
        "keywords": [
            "automatic",
            "melody",
            "transcription",
            "chord",
            "transcription",
            "audio",
            "segments",
            "melody",
            "patterns",
            "harmony"
        ],
        "content": "AUTOMATIC MELODY TRANSCRIPTION BASED ON CHORD\nTRANSCRIPTION\nAntti Laaksonen\nDepartment of Computer Science\nUniversity of Helsinki\nahslaaks@cs.helsinki.fi\nABSTRACT\nThis paper focuses on automatic melody transcription in a\nsituation where a chord transcription is already available.\nGiven an excerpt of music in audio form and a chord tran-\nscription in symbolic form, the task is to create a symbolic\nmelody transcription that consists of note onset times and\npitches. We present an algorithm that divides the audio\ninto segments based on the chord transcription, and then\nmatches potential melody patterns to each segment. The\nalgorithm uses chord information to favor melody patterns\nthat are probable in the given harmony context. To eval-\nuate the algorithm, we present a new ground truth dataset\nthat consists of 1,5 hours of audio excerpts together with\nhand-made melody and chord transcriptions.\n1. INTRODUCTION\nMelody and chords have a strong connection in Western\nmusic. The purpose of this paper is to exploit this con-\nnection in automatic melody transcription. Given a chord\ntranscription, we can use it in melody transcription to con-\nstrain the set of possible melodies. Both the rhythm and\nthe pitches of the melody should match the chords in a suc-\ncessful melody transcription.\nFor example, let us consider the melody in Figure 1.\nThe melody consists of 16 bars, each annotated with a\nchord symbol. The ﬁrst observation is that chord bound-\naries divide the melody into segments of approximately\nequal length. Each of the segments has a simple rhythmical\nstructure. In this case the chord boundaries exactly match\nthe bar lines, and each segment contains up to three melody\nnotes. Of course, many melodies are more complex than\nthis, but the underlying segmentation is still usually appar-\nent.\nLet us now consider the pitches of the melody. The key\nof the melody mainly determines what pitches typically oc-\ncur in the melody. In this example the key of the melody\nis C major, and almost all melody pitches belong to the C\nmajor scale. However, there are two exceptions: the G#\nc\rAntti Laaksonen.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Antti Laaksonen. “AUTO-\nMATIC MELODY TRANSCRIPTION BASED ON CHORD TRAN-\nSCRIPTION”, 15th International Society for Music Information Retrieval\nConference, 2014.\n \nG\n/noteheads.s2/noteheads.s2/accidentals.sharpDm\n/noteheads.s1/clefs.G43 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharpF\n/noteheads.s1C\n/noteheads.s1/dots.dotE\n/noteheads.s1Dm\n/noteheads.s1/dots.dotA\n/noteheads.s1/noteheads.s2A\n/noteheads.s1/dots.dot\n/dots.dot /noteheads.s1/noteheads.s2/noteheads.s2Dm/noteheads.s2/noteheads.s2D/accidentals.sharpo\n/noteheads.s2\n/clefs.GG\n/rests.1/dots.dotD/accidentals.sharpo\n/noteheads.s2/noteheads.s2/noteheads.s2C/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2C/noteheads.s2Dm/noteheads.s1/dots.dotG\n/rests.2Figure 1: A melody from Disney’s Snow White and the\nSeven Dwarfs. The chord transcription consists of 16\nchords, and the melody transcription consists of 30 notes.\nnote in the second bar and the C# note in the sixth bar.\nThus, although the melody follows the C major scale, the\nindividual chords also have an effect on the pitches. In this\ncase the major thirds of E major and A major chords are so\npredominant that the melody adapts to the harmony.\nHuman transcribers routinely use this kind of musical\nknowledge in music transcription. If the melody does not\nmatch the chords, or the chords do not match the melody,\nthe transcription cannot be correct. However, in automatic\nmusic transcription, chord extraction and melody extrac-\ntion have been studied separately for the most part.\nCurrently, the best automatic systems for chord tran-\nscription produce promising results, while melody tran-\nscription seems to be a more challenging problem. For this\nreason, we approach automatic melody transcription with\nthe assumption that a chord transcription has already been\ndone. We present an algorithm that divides the audio data\ninto segments based on the chord boundaries. After this,\nthe algorithm assigns each segment a melody pattern that\nmatches both the audio data and the chord information.\n1.1 Problem statement\nGiven an excerpt of music in audio form and a chord tran-\nscription in symbolic form, the task is to produce a melody\ntranscription in symbolic form. We concentrate on typical\nWestern music, and assume that the pitches of the notes are\ngiven in semitones.\nWe assume that the audio data Ais divided into nA\nframes of equal length using some preprocessing method.\nFor each audio frame k(1\u0014k\u0014nA), we are given values\nA[k]:begin andA[k]:end that are time values in seconds\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n119when the frame begins and ends. In addition, for each pos-\nsible melody note qwe are given a real number A[k][q]in\nthe range [0;1]. This value estimates the strength of the\nnoteqin framek.\nThe chord transcription Cconsists ofnCchord changes.\nFor each chord change k(1\u0014k\u0014nC), we are given a\nvalueC[k]:time that is the time when the chord changes.\nIn addition, we are given a value C[k]:chord that is the\nname of the chord. We restrict ourselves to triads (major,\nminor, diminished and augmented chords that consist of\nthree notes), which results in a total of 48 possible chords.\nFinally, the outcome of the algorithm should be a melody\ntranscription Mthat consists of nMmelody notes. For\neach melody note k(1\u0014k\u0014nM), the algorithm should\nproduce values M[k]:time andM[k]:pitch that denote the\nonset time of the note and the pitch of the note.\nThroughout the paper, we use MIDI note numbers to\nrefer to the pitches. Thus, every pitch has a unique integer\nvalue and the interval of pitches aandbisja\u0000bjsemitones.\nPitch C4 (261.6 Hz) is associated with MIDI value 60.\n1.2 Related work\nAutomatic melody transcription has been studied actively\nduring the last decade. Detailed reviews of the proposed\nmethods can be found in [16] and [20].\nThe usual ﬁrst step in automatic melody transcription\nis to detect potential melody notes in the audio signal. The\nmost popular method for this is to calculate a salience func-\ntion for the audio frames using the discrete Fourier trans-\nform or a similar technique (e.g. [2, 6, 15, 19]). Other pro-\nposed approaches for audio data processing include signal\nsource separation [3] and audio frame classiﬁcation [5].\nAfter this, the ﬁnal melody is selected according to some\ncriterion. One technique for this is to construct a hidden\nMarkov model (HMM) for note transitions and use the\nViterbi algorithm for tracking the most probable melodic\nline [3, 5, 19]. An alternative to this is to use a set of local\nrules that describe typical properties of melody notes and\noutlier notes [7, 15, 20]. In addition, some systems [2, 6]\nfeature agents that follow potential melody lines.\nThe idea of providing additional information to facili-\ntate the melody transcription has also been considered in\nprevious studies. A usual approach for this is to gather in-\nformation from users. For example, users can determine\nwhich instruments are present [8], help in the source sep-\naration process [4] or create an initial version of the tran-\nscription [9]. The drawback of these systems is, of course,\nthat the transcription is not fully automatic.\nThere are also some previous studies that combine key,\nchord and pitch estimation. In [18], the key and the chords\nof the piece are estimated simultaneously. Multiple pitch\ntranscription systems that exploit key and chord informa-\ntion in pitch estimation include [1] and [17].\nMost of the previous work on automatic melody tran-\nscription focuses on a slightly different problem than the\ntopic of this paper, namely how to determine the melody\nfrequency in the audio signal frame-by-frame. In [19] and\n[22], the output of the algorithm is similar to ours.2. ALGORITHM\nIn this section we present our melody transcription algo-\nrithm that uses a chord transcription as a starting point for\nthe transcription. The algorithm ﬁrst divides the audio data\ninto segments so that the boundaries of the segments cor-\nrespond to the boundaries of the chords in the chord tran-\nscription. After this, the key of the piece is estimated using\nthe chord transcription. Finally, the algorithm assigns each\nsegment a pattern of notes that matches both the audio data\nand the chord transcription.\nThe input and the output of the algorithm are as de-\nscribed in Section 1.1. Thus, the algorithm is given nA\naudio frames in array Aand a chord transcription of nC\nchord changes in array C, and the algorithm produces a\nmelody transcription of nMnotes in array M.\n2.1 Segmentation\nThe ﬁrst step in the algorithm is to divide the audio data\ninto segments. The segments will be processed separately\nin a later phase in the algorithm. The idea is to choose the\nboundaries of the segments so that the harmony in each\nsegment is stable. This is accomplished using the chord\nboundaries in the chord transcription.\nLet\nl(k) =C[k+ 1]:time\u0000C[k]:time\nfor eachkwhere 1\u0014k\u0014nC\u00001and\nf(k;x) = (l(k)=x)=bl(k)=xc\nwherexis a real value. Thus, l(k)is the length of the\nsegment between chord changes kandk+1, andf(k;x)is\nan estimate how evenly xdivides that segment into smaller\nsegments. Finally, let\ng(k;x) =\u001a1iff(k;x)\u00141 +\u000f\n0otherwise\nand\ns(x) =nC\u00001X\ni=1g(i;x):\nIff(k;x)\u00141+\u000ffor some small \u000f, our interpretation is\nthatxdivides the segment evenly. Thus, g(k;x)indicates\nif the segment is divided evenly, and s(x) is the number of\nsegments that are divided evenly if xwas chosen. In this\npaper we use value \u000f= 0:1.\nThe algorithm chooses a value of xsuch thatxis in the\nrange [min x;max x]ands(x) is as large as possible. The\nvaluexwill be used as a unit length in the segmentation.\nThe valuesminxandmax xdenote the minimum and max-\nimum unit length; in this paper we use values minx= 0:5\nandmax x= 3.\nFinally, the algorithm produces a segment division Sof\nnSsegments by dividing each chord segment kintol(k)=x\nsmaller segments of equal length (the number of segments\nis rounded to the nearest integer). For each new segment u\n(1\u0014u\u0014nS) the algorithm assigns the values S[u]:begin\nandS[u]:end as described above, and S[u]:chord denotes\nthe name of the chord in the segment.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1202.2 Key estimation\nAfter determining the segments, the algorithm estimates\nthe key of the piece. The estimated key will be used later\nin the algorithm to favor melody notes that agree with the\nkey. The key estimation is done using a simple method that\nis based on the chord information.\nThe algorithm goes through all segments in Sand main-\ntains a counter for each pitch class (a total of 12 counters).\nInitially, all the counters are zero. For each segment k,\nthe algorithm increases the value of each counter that cor-\nresponds to S[k]:chord . For example, if S[k]:chord is G\nmajor, the algorithm increases counters that correspond to\nnotes G, B and D.\nFinally, the algorithm determines the key using the coun-\nters as follows. There are 24 possible keys, 12 major keys\nand 12 minor keys. For each key, the algorithm calculates\nthe sum of counters that correspond to tonic, mediant and\ndominant in that key. The key whose sum is the highest is\nselected as the key of the piece.\nThis method for key estimation is more simple than\nmethods used in previous studies involving chord and key\nestimation from music audio [11,18]. However, this method\nproduces results that are considered accurate enough for\nthis purpose.\n2.3 Pattern matching\nFor each segment, the algorithm chooses a melody pattern\nthat matches both the audio data within the segment and\nthe chord and key information. Each segment is processed\nindependently, and the ﬁnal melody transcription consists\nof all melody patterns in the segments.\nThe algorithm divides each segment into dnote slots\nwheredis a preselected constant for all segments. Each\nnote slot can contain either one melody note or rest in the\nmelody pattern. The idea is to select dso that most rhythms\ncan be represented using dnote slots, but at the same time\ndis small enough to restrict the number of melody notes.\nIn practice, small integers that are divisible by 2 and/or 3\nshould be good choices for d.\nAn optimal melody pattern is selected according to a\nscoring function. The scoring function should give high\nscores for melody patterns that are probable choices for\nthe segment. Depending on the scoring function, there are\nthree ways to construct the optimal melody pattern:\n\u000fGreedy construction: If the melody slots are inde-\npendent of one another, we can select the optimal\nmelody note for each slot and combine the results to\nget the optimal melody pattern.\n\u000fDynamic programming: If the melody slots are not\nindependent but the score of a slot only depends on\nthe previous slot, we can use dynamic programming\nto construct the optimal pattern.\n\u000fComplete search: If the score of a melody pattern\ncannot be calculated before all melody notes are se-\nlected, we have to go through all possible note pat-\nterns and select the optimal one.The methods involving greedy construction and dynamic\nprogramming are efﬁcient in all practical situations. How-\never, in complete search we need to check qdmelody pat-\nterns whereqis the number of possible choices for a melody\nslot. In practice, q\u001950, so complete search can be used\nonly whend\u00144to keep the algorithm efﬁcient.\n2.4 Scoring function\nThe scoring function that we use in this paper is primar-\nily based on the key information and favors melody notes\nthat match the estimated key of the excerpt. In addition,\nthe notes that belong to the chord of the segment have an\nincreased probability to be selected to the melody. Thus, if\nan E major chord occurs in the C major key, the note G# is\na strong candidate for the melody note even if it does not\nbelong to the C major scale.\nLets(k;a;q )denote the score for an event where a’th\nnote slot of segment kcontains note q. We calculate the\nscore using the formula\ns(k;a;q ) =z\u0001b(k;a;q )\u0000x\u0001c(k;a;q )\nwhereb(k;a;q )is a base score calculated from the audio\ndata,c(k;a;q )is a penalty for selecting a note that does\nnot appear in the audio data, and zandxare parameters\nthat control the balance of the base score and the penalty.\nLetIbe a set that contains the indices of all audio frames\nthat are inside the current note slot. Now we deﬁne\nb(k;a;q ) =X\ni2IA[i][q]\nand\nc(k;a;q ) =X\ni2Ie(i;q)\nwhere\ne(i;q) =\u001a1ifA[i][q] = 0\n0otherwise.\nThe parameter zfavors melody notes that match the\nchord transcription, and it should depend on the key of the\nexcerpt and the chord in segment k. We setz= 2ifqbe-\nlongs to the current chord, z= 1ifqbelongs to the key of\nthe excerpt and otherwise z= 0. The parameter xcontrols\nthe effect of adding a note to the melody that does not ap-\npear strongly in the audio data, and we study the effect of\nthat constant in Section 3.\nFinally, we select the note pattern greedily so that each\nnote slot will be assigned the note that maximizes the score\nfor that slot. If no note produces a score greater than 0, we\nleave that slot empty.\nWe also experimented with dynamic programming scor-\ning functions that favor small intervals between consec-\nutive notes, but the results remained almost unchanged.\nConsequently, we chose the more simple greedy construc-\ntion approach.\n3. EVALUATION\nIn this section we present results concerning the accuracy\nof the transcriptions produced by our algorithm, using real-\nworld inputs. We evaluated our algorithm using a dataset\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n121of Western popular music. We used both hand-made and\nautomatic chord transcriptions as additional input for the\nalgorithm.\nAudio Melody Extraction task is an established part of\nthe MIREX evaluation [13]. However, in the MIREX eval-\nuation each audio frame is assigned a melody note fre-\nquency, and those results cannot be compared with our\nmelody transcriptions that consist of note onset times and\npitches in semitones.\n3.1 Dataset\nThe evaluation dataset consists of 1,5 hours of audio ex-\ncerpts from Western popular music. The length of each\nexcerpt in the dataset is between 20 and 60 seconds. For\neach excerpt, we manually created time-aligned melody\nand chord transcriptions. We chose the excerpts so that the\ncontent of each excerpt is unique i.e. repetitions of verses\nand choruses are not included in the dataset.\nThe dataset can be found on our web site1, and is avail-\nable for free for use in research. For each excerpt, the\ndataset includes an audio ﬁle in WA V format, and chord\nand melody transcriptions in text format. Each chord tran-\nscription is a list of chord change times and chord symbols,\nand each melody transcription is a list of note onset times\nand pitches. Thus, the transcriptions in the dataset corre-\nspond to the deﬁnitions in Section 1.1.\n3.2 Evaluation method\nTo evaluate a melody transcription, we calculate two val-\nues: the precision and the recall. Precision is the ratio of\nthe number of correct notes in the transcription and the to-\ntal number of notes in the transcription. Recall is the ratio\nof the number of correct notes in the transcription and the\ntotal number of notes in the ground truth.\nLetXbe a melody transcription of nXnotes created\nby the algorithm, and let Gbe the corresponding melody\ntranscription of nGnotes in the ground truth. Both tran-\nscriptions consists of a list of melody note onset times and\npitches as described in Section 1.1.\nTo evaluate the precision and the recall of X, we ﬁrst\nalign the transcriptions. Let nDbe the maximum integer\nvalue such that we can create lists DXandDGas follows.\nListDXconsists ofnDnote indices in X, and listDG\nconsists ofnDnote indices in G. In addition, for each k\n(1\u0014k\u0014nD)X[DX[k]]:pitch =G[D G[k]]:pitch and\njX[DX[k]]:time\u0000G[D G[k]]:timej \u0014 \u000bwhere\u000bis a\nsmall contant. In other words, we require that lists DX\nandDGalign a set of notes where all pitches match each\nother and the onset times of the notes do not differ more\nthan\u000bfrom each other.\nFinally, let\nprecision(X;G ) =nD=nX\nand\nrecall(X;G) = nD=nG:\n1http://cs.helsinki.fi/u/ahslaaks/fpds/In practice, we calculate the value nDefﬁciently using\ndynamic programming. The technique is similar to calcu-\nlating the Levenshtein distance between two strings [14].\nThis evaluation method corresponds with that used in\n[19] and [22], however, the previous papers do not specify\nhow the notes in the two transcriptions are aligned.\n3.3 Experiments\nWe implemented our algorithm as described in Section 2.\nFor calculating array Awe used an algorithm by Salamon\nand G ´omez that estimates potential melody contours in the\naudio signal. We used the Vamp plugin implementation of\nthe algorithm (”all pitch contours”). Note that this algo-\nrithm already restricts the set of possible melodies consid-\nerably. We converted each pitch frequency to a MIDI note\nnumber assuming that the frequency of A4 is 440 Hz.\nWe used four chord transcriptions in the evaluation:\n\u000fA random chord transcription where the time be-\ntween two chord changes is a random real number\nin the range [0:5;2]and each chord is randomly se-\nlected from the set of 48 possible triads.\n\u000fA simple automatic chord transcription created by\nour own algorithm. We used the standard technique\nof constructing a hidden Markov model and tracking\nthe optimal path using the Viterbi algorithm [21].\n\u000fAn advanced automatic chord transcription created\nusing the Chordino tool [12].\n\u000fThe chord transcription in the ground truth.\nRandom chord transcriptions were used in an effort to\nunderstand the actual role of the chord information and\nhow the algorithm works if the chord information does not\nmake sense at all.\nFinally, there are three parameters that we varied during\nthe evaluation:\n\u000fd: the number of note slots in a segment as described\nin Section 2.3 (default value: 6),\n\u000fx: the cost for assigning a note to a frame with-\nout a note as described in Section 2.4 (default value:\n1.00),\n\u000f\u000b: the maximum onset time difference in the evalua-\ntion as described in Section 3.2 (default value: 0.25).\nThe default values were chosen so that they produce\ngood results on the evaluation dataset.\nIn each experiment in the evaluation, we varied one pa-\nrameter and kept the remaining parameters unchanged. We\nused the ground truth chord transcription as the default\nchord transcription. We created melody transcriptions for\nall excerpts in the dataset and calculated average precision\nand recall values.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n122 0 0.2 0.4 0.6 0.8 1\nrandom simple advanced ground truthprecisionrecall(a)The results using random chord transcription, simple\nautomatic transcription, advanced automatic transcrip-\ntion, and ground truth transcription.\n 0 0.2 0.4 0.6 0.8 1\n1 2 3 4 6 8 10precisionrecall(b)The results varying the parameter d: the number of\navailable note slots in a segment.\n 0 0.2 0.4 0.6 0.8 1\n0.1 0.5 1 2 5precisionrecall\n(c)The results varying the parameter x: the cost for\nassigning a note to a frame without a note.\n 0 0.2 0.4 0.6 0.8 1\n0.1 0.2 0.3 0.4 0.5precisionrecall(d)The results varying the parameter \u000b: the maximum\nonset time difference in seconds in the evaluation.\nFigure 2: The results of the experiment.\n3.4 Results\nIn the ﬁrst experiment (Figure 2a) we studied how the qual-\nity of the chord transcription affects the results. As ex-\npected, the better the chord transcription, the better the\nprecision of the melody transcription. However, recall was\nhighest when using automatic chord transcriptions. One\npossible reason for this is that there were more chord changes\nin automatic transcriptions than in the ground truth tran-\nscription. Therefore more melody notes were selected us-\ning the automatic chord transcriptions.\nIn the second experiment (Figure 2b) we varied the pa-\nrameterd: the number of note slots in a segment. Our\nﬁndings suggest that 6 note slots is a good trade-off be-\ntween the precision and the recall. This can be explained\nby the fact that 6 is divisible by both 2 and 3, and thus seg-\nments of 6 note slots are suitable for both 3/4 time and 4/4\ntime music. Interestingly, when d\u00146the precision of the\ntranscription remained nearly unchanged.\nIn the third experiment (Figure 2c) we varied the pa-\nrameterx: the cost for assigning a note to a frame without\na note. This was an important parameter, and the resultswere as expected. Increasing the parameter ximproves the\nprecision because melody notes are only selected if they\nappear strongly in the audio data. At the same time, this\ndecreases the recall because fewer uncertain notes are in-\ncluded in the melody transcription.\nFinally, in the fourth experiment (Figure 2d) we varied\nthe parameter \u000b: the maximum note onset time difference\nin the evaluation. Of course, the greater the parameter \u000b,\nthe better the results. Interestingly, after reaching a value\nof approximately 0.25, increasing \u000bdid not affect the re-\nsults considerably. The probable reason for this is that if\nthe melody note pitches in the transcription are not correct,\nthe situation cannot be rescued by allowing more error for\nthe onset times.\nPrevious studies also present some results about the pre-\ncision and the accuracy of the algorithms. However, ﬁnd-\nings from these studies cannot be directly compared with\nthe new results because the evaluation dataset is different\nin each study. In [19] precision 0.49 and recall 0.61 was\nreported using a database of 84 popular songs. In [22] the\nmelody transcription was evaluated using a small set of 11\nsongs with precision 0.68 and recall 0.63.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1234. CONCLUSIONS\nIn this paper we presented an automatic melody transcrip-\ntion algorithm that uses a chord transcription for select-\ning melodies that match the harmony of the music. We\nevaluated the algorithm using a collection of popular mu-\nsic excerpts, and the results of the evaluation suggest that\nthe chord information can be successfully used in melody\ntranscription of real-world inputs.\nOur new evaluation dataset consists of 1,5 hours of au-\ndio excerpts of popular music together with melody and\nchord annotations. The dataset can be used at no cost\nfor research purposes, for example as evaluation material\nfor other chord transcription and melody transcription sys-\ntems.\nOur future work aims to use the harmony information\nprovided by the chord transcription more extensively in\nmelody transcription. Currently our algorithm uses only\ninformation about chord notes to constrain the pitches of\nmelody notes, but using more advanced musical knowl-\nedge should yield better results.\n5. ACKNOWLEDGEMENTS\nThis work has been supported by the Helsinki Doctoral\nProgramme in Computer Science and the Academy of Fin-\nland (grant number 118653).\n6. REFERENCES\n[1] E. Benetos, A. Jansson and T. Weyde: ”Improving automatic\nmusic transcription through key detection,” AES 53rd Inter-\nnational Conference on Semantic Audio, 2014.\n[2] K. Dressler: “An auditory streaming approach for melody ex-\ntraction from polyphonic music,” 12th International Society\nfor Music Information Retrieval Conference, 19–24, 2011.\n[3] J.-L. Durrieu et al: ”Source/ﬁlter model for unsupervised\nmain melody extraction from polyphonic audio signals,”\nIEEE Transactions on Audio, Speech, and Language Pro-\ncessing, 18(3), 564–575, 2010.\n[4] J.-L. Durrieu and J.-P. Thiran: ”Musical audio source sep-\naration based on user-selected F0 track,” 10th International\nConference on Latent Variable Analysis and Signal Separa-\ntion, 2012.\n[5] D. Ellis and G. Poliner: “Classiﬁcation-based melody tran-\nscription,” Machine Learning, 65(2–3), 439–456, 2006.\n[6] M. Goto: ”A real-time music scene description system:\npredominant-F0 estimation for detecting melody and bass\nlines in real-world audio signals,” Speech Communication,\n43(4), 311–329, 2004.\n[7] S. Joo, S. Park, S. Jo and C. Yoo: “Melody extraction based\non harmonic coded structure,” 12th International Society for\nMusic Information Retrieval Conference, 227–232, 2011.\n[8] H. Kirchhoff, S. Dixon and A. Klapuri: ”Shift-variant non-\nnegative matrix deconvolution for music transcription,” 37th\nInternational Conference on Acoustics, Speech and Signal\nProcessing, 2012.\n[9] A. Laaksonen: ”Semi-automatic melody extraction using\nnote onset time and pitch information from users,” SMC\nSound and Music Computing Conference, 689–694, 2013.[10] M. Lagrange et al: ”Normalized cuts for predominant\nmelodic source separation,” IEEE Transactions on Audio,\nSpeech, and Language Processing, 16(2), 278–290, 2008.\n[11] K. Lee and M. Slaney: ”A uniﬁed system for chord tran-\nscription and key extraction using hidden Markov models,”\n8th International Conference on Music Information Retri-\navel, 245–250, 2007\n[12] M. Mauch and S. Dixon: ”Approximate note transcription for\nthe improved identiﬁcation of difﬁcult chords,” 11th Interna-\ntional Society for Music Information Retrieval Conference,\n135–140, 2010\n[13] MIREX Wiki: Audio Melody Extraction task,\nhttp://www.music-ir.org/mirex/wiki/\n[14] G. Navarro: ”A guided tour to approximate string matching,”\nACM Computing Surveys, 33(1): 31–88, 2001\n[15] R. Paiva, T. Mendes and A. Cardoso: ”Melody detection in\npolyphonic musical signals: exploiting perceptual rules, note\nsalience, and melodic smoothness,” Computer Music Jour-\nnal, 30(4), 80–98, 2006.\n[16] G. Poliner et al: ”Melody transcription from music audio:\napproaches and evaluation,” IEEE Transactions on Audio,\nSpeech, and Language Processing, 15(4), 1247–1256, 2007.\n[17] S. Raczy ´nski, E. Vincent, F. Bimbot and S. Sagayama:\n”Multiple pitch transcription using DBN-based musicologi-\ncal models,” 11th International Society for Music Informa-\ntion Retrieval Conference, 363–368, 2010\n[18] T. Rocher et al: ”Concurrent estimation of chords and keys\nfrom audio,” 11th International Society for Music Informa-\ntion Retrieval Conference, 141–146, 2010\n[19] M. Ryyn ¨anen and A. Klapuri: ”Automatic transcription of\nmelody, bass line, and chords in polyphonic music,” Com-\nputer Music Journal, 32(3), 72–86, 2008.\n[20] J. Salamon and E. G ´omez: ”Melody extraction from poly-\nphonic music signals using pitch contour characteristics,”\nIEEE Transactions on Audio, Speech, and Language Pro-\ncessing, 20(6), 1759–1770, 2012.\n[21] A. Sheh and D. Ellis: “Chord segmentation and recognition\nusing EM-trained hidden Markov models,” 4th International\nConference on Music Information Retrieval, 183–189, 2003\n[22] J. Weil et al: ”Automatic generation of lead sheets from poly-\nphonic music signals,” 10th International Society for Music\nInformation Retrieval Conference, 603–608, 2009\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n124"
    },
    {
        "title": "Automatic Key Partition Based on Tonal Organization Information of Classical Music.",
        "author": [
            "Wang-Kong Lam",
            "Tan Lee 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414748",
        "url": "https://doi.org/10.5281/zenodo.1414748",
        "ee": "https://zenodo.org/records/1414748/files/LamL14.pdf",
        "abstract": "Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we de- scribe a generative account of chord progression consist- ing of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is com- pared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental re- sult shows that our system is better in terms of key detec- tion accuracy.",
        "zenodo_id": 1414748,
        "dblp_key": "conf/ismir/LamL14",
        "keywords": [
            "key information",
            "tonal music analysis",
            "chord progressions",
            "specific structures and rules",
            "phrase-structure grammar rules",
            "modulation",
            "musically sensible key partition",
            "classical music excerpts",
            "Kostka-Payne corpus",
            "key detection accuracy"
        ],
        "content": "AUTOMATIC KEY PARTITION BASED ON TONAL ORGANIZATION\nINFORMATION OF CLASSICAL MUSIC\nLam Wang Kong, Tan Lee\nDepartment of Electronic Engineering\nThe Chinese University of Hong Kong Hong Kong SAR, China\nfwklam,tanleeg@ee.cuhk.edu.hk\nABSTRACT\nKey information is a useful information for tonal music\nanalysis. It is related to chord progressions, which follows\nsome speciﬁc structures and rules. In this paper, we de-\nscribe a generative account of chord progression consist-\ning of phrase-structure grammar rules proposed by Martin\nRohrmeier. With some modiﬁcations, these rules can be\nused to partition a chord symbol sequence into different\nkey areas, if modulation occurs. Exploiting tonal grammar\nrules, the most musically sensible key partition of chord\nsequence is derived. Some examples of classical music\nexcerpts are evaluated. This rule-based system is com-\npared against another system which is based on dynamic\nprogramming of harmonic-hierarchy information. Using\nKostka-Payne corpus as testing data, the experimental re-\nsult shows that our system is better in terms of key detec-\ntion accuracy.\n1. INTRODUCTION\nChord progression is the foundation of harmony in tonal\nmusic and it can determine the key. The keyinvolves cer-\ntain melodic tendencies and harmonic relations that main-\ntain the tonic as the centre of attention [4]. Key is an in-\ndicator of the musical style or character. For example, the\nkey C major is related to innocence and pureness, whereas\nF minor is related to depression or funereal lament [16].\nKey detection is useful for music analysis. A classical mu-\nsic piece may have several modulations (key changes). A\nchange of key means a change of tonal center, the adop-\ntion of a different tone to which all the other tones are to\nbe related [10]. Key change allows tonal music to convey\na sense of long-range motion and drama [17].\nKeys and chord labels are interdependent. Even if\nthe chord labels are free from errors, obtaining the key\npath is often a non-trivial task. For example, if a mu-\nsic excerpt has been analyzed with the chord sequence\n[B[;F;Gmin;Amin;G;C ], how would you analyze its\nkey? Is it a phrase entirely in B[ major or C major, as\nc\rLam Wang Kong, Tan Lee.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Lam Wang Kong, Tan Lee. “Auto-\nmatic key partition based on Tonal Organization Information of Classical\nMusic”, 15th International Society for Music Information Retrieval Con-\nference, 2014.they are the beginning or ending chords? Seems it is not,\nas B[major chord is normally not a member chord of C\nmajor and vice versa. It seems that there must be a key\nchange in the middle. But how would you ﬁnd out the\npoint of key change, and how does the key change? With\nthe help of the tonal grammar tree analysis in x2.1, a good\nestimate of the key path can be obtained. To start with, we\nassume that the excerpt consists of harmonically complete\nphrase(s) and the chord labels are free from errors.\nThere are some existing algorithms to estimate the key\nbased on chord progression. These algorithms can be clas-\nsiﬁed into two categories: statistical-based andrule-based\napproach. Hidden Markov model is very often used in the\nstatistical approach. Lee & Stanley [7] extracted key in-\nformation by performing harmonic analysis on symbolic\ntraining data and estimated the model parameters from\nthem. They built 24 key-speciﬁc HMMs (all major and mi-\nnor keys) for recognizing a single global key which has the\nhighest likelihood. Raphael & Stoddard [11] performed\nharmonic analysis on pitch and rhythm. They divided the\nmusic into a ﬁxed musical period, usually a measure, and\nassociate a key and chord to each of period. They per-\nformed functional analysis of chord progression to deter-\nmine the key. Unlabeled MIDI ﬁles were used to train the\ntransition and output distributions of HMM. Instead of rec-\nognizing the global key, it can track the local key. Catteau\net al. [2] described a probabilistic framework for simulta-\nneous chord and key recognition. Instead of using training\ndata, Lerdahl’s representation of tonal space [8] were used\nas a distance metric to model the key and chord transition\nprobabilities. Shenoy et al. [15] proposed a rule-based ap-\nproach for determining the key from chord sequence. They\ncreated a reference vector for each of the 12 major and\nminor keys, including the possible chords within the key.\nHigher weights were assigned to primary chords (tonic,\nsubdominant and dominant chords). The chord vector ob-\ntained from audio data were compared against the refer-\nence vector using weighted cosine similarity. The pattern\nwith the highest rank is chosen as the selected global key.\nThis paper uses a rule-based approach to model tonal\nharmony. A context-free dependency structure is used to\nexhaust all the possible combinations of key paths, and\nthe best one is selected according to music knowledge.\nThe main objective of this research is to exploit this tonal\ncontext-free dependency structure in order to partition an\nexcerpt of classical music into several key sections.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n501Functional level Scale degree level\nTR!DR T T!I\nDR!SR D T!I IV I\nTR!TR DR S!IV\nXR!XR XR D!Vjvii\nphrase !TR T!vijIII\nD!V II (minor)\nAdded rules for scale degree level: S!ii(major)\nS!ii(minor) S!V IjbII(minor)\nT!I IV V I jV I IV I jI bII I X!D(X)X\nD!I V, after SorD(V) D(X)!V=Xjvii=X\nTR tonic region S predominant function\nDR dominant region X any speciﬁc function\nSR predominant region D(\u0005) secondary dominant\nXR any speciﬁc region X / Y X of Y chord\nT tonic function I, III... major chords\nD dominant function ii, vi... minor chords\nTable 1. Rules (top) and labels (bottom) used in our system\n2. TONAL THEORY OF CLASSICAL MUSIC\n2.1 Schenkerian analysis and formalization\nTo interpret the structure of the tonal music, Schenkerian\nanalysis [14] is used. The input is assumed to be classical\nmusic with one or more tonal centre (tonal region). Each\ntonal centre can be elaborated into tonic – dominant – tonic\nregions [1]. The dominant region can be further elaborated\ninto predominant-dominant regions. Each region can be\nrecursively elaborated to form a tonal grammar tree. We\ncan derive the key information by referring to the top of the\ntree, which groups the chord sequence into a tonal region.\nContext-free grammar can be used to formalize this\ntree structure. A list of generative syntax is proposed by\nRohrmeier [13] in the form of V!w.Vis a single non-\nterminal symbol, while wis a string of terminals and/or\nnon-terminals. Chord symbols (eg. IV) are represented\nby terminals. They are the leaves of the grammar tree.\nTonal functions (eg. Tfor tonic) or regions (eg. TRfor\ntonic region) are represented by non-terminals. They can\nbe the internal nodes or the root of the grammar tree. For\ninstance, the rule D!Vjviiindicates that the Vorvii\nchord can be represented by the dominant function. The\nruleS!ii(major) indicates that iichord can be repre-\nsented by the predominant function only when the current\nkey is major. Originally Rohrmeier has proposed 28 rules.\nSome of them were modiﬁed to suit classical music and\nwere listed in Table 1.\nBased on this set of rules, Cocke–Younger–Kasami\nparsing algorithm [18] is used to construct a tonal grammar\ntree. If a music input is harmonically valid, a single tonal\ngrammar tree can be built like in Figure 1. Else some scat-\ntered tree branches are resulted and cannot be connected to\none single root.\nFigure 1. Example of a tonal grammar tree (single key)\nFigure 3. Flow diagram of our key partitioning system\n2.2 Modulation\nIn Rohrmeier’s generative syntax of tonal harmony, modu-\nlation is formalized as a new local tonic [13]. Each func-\ntional region (new key section) is grouped as a single non-\ntonic chord in the original passage, and they may relate this\n(elaborated) chord to the neighbouring chords.\nIn this research we have a more general view of mod-\nulation. As a music theorist, Reger had published a book\nModulation, showing how to modulate from C major / mi-\nnor to every other key [12]. Modulation to every other key\nis possible, but modulation to harmonically closer keys is\nmore common [10]. For instance, if the music is origi-\nnally in C major, it is more probable to modulate to G ma-\njor instead of B major. Lerdahl’s chordal distance [8] is\nused to measure the distance between different keys. Here\nRohrmeier’s modulation rules in [13] are not used. Instead,\na tonal grammar tree is built for each new key section,\nand the key path with the best score is chosen. Any key\nchanges explainable by tonicization (temporary borrowing\nof chords from other keys), such as the chords [IV/V V\nI], is not considered as a modulation. Figure 2 shows an\nexample of tonal grammar tree with modulation, from E\nminor to D] minor. It is presented by two disjunct trees.\n3. SYSTEM BUILDING BLOCKS\n3.1 Overview\nThe proposed key partitioning system is shown as in Figure\n3. This system takes a sequence of chord labels (e.g. A\nminor, E major) and outputs the best key path. The path\nmay consist of only one key, or several keys. For example,\n[F F F F F F] or [Am Am Am C C C] (m indicates minor\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n502Figure 2. Example of a tonal grammar tree with modulation\nchords, other chords are major) are both valid key paths.\nTheTonal Grammar Tree mentioned inx2.1 is the main\ntool used in this system.\n3.2 Algorithm for key partitioning\nEach key section is assumed to have at least one tonic\nchord. The top of each grammar tree must be TR (tonic re-\ngion), so the key section is a complete tonal grammar tree\nby itself. Furthermore, the minimum length of each key\nsection is assumed to be 3 chords. However, if no valid\npaths can be found, key sections with only 2 chords are\nalso considered.\nThe algorithm is as follows:\n1. In a chord sequence, hypothesize any of the chord\nlabel as the tonic of a key. Derive the tonal grammar\ntree of each key.\n2. Find if there is any key that can build a single com-\nplete tree for the entire sequence. If yes, limit the\nvalid paths to these single-key paths and go to step\n7. This phrase is assumed to have a single key only.\nElse go to next step.\n3. For each chord label in the sequence, ﬁnd the max-\nimum possible accumulated chord sequence length\nof each key section (up to that label). Determine\nif this sequence is breakable at that label (The sec-\nondary dominant chord is dependent on the subse-\nquent chord. For example, the tonicization segment\nV/V V cannot be broken in the middle, as V/V is\ndependent on V chord).\n4. Find out all possible key sections with at least 3\nchords including at least one tonic chord.\n5. Find out all valid paths traversing all the possible\nkey sections, from beginning to end, in a brute-force\nmanner.Path no. Key paths\n1 Gm Gm Gm Am Am Am\n2 Gm Gm Gm C C C\n3 B[ B[ B[ Am Am Am\n4 B[ B[ B[ C C C\nTable 2. All valid key paths in the example\n6. If no valid paths can be found, go back to step 4 and\nchange the requirement to “at least 2 chords”. Else\nproceed to step 7.\n7. Evaluate the path score of all valid paths and select\nthe one with the highest score to be the best key path.\nA simple example is used to illustrate this process. The\ninput chord sequence is [B[ F Gm Am G C]. Incomplete\ntrees with the keys (B[, F, Gm, Am, G, C) are built. As all\nthe trees are incomplete, proceed to step 3 and the accu-\nmulated length is calculated. The B[ major tree is shown\nin Figure 4 as an example. Other ﬁve trees (F, Gm, Am,\nG, C) were built in the same fashion. Either key sections\n1-3 or 1-4 of B[major are valid key sections as they can\nall be grouped into a single TR and they have at least 3\nchords. Then all the valid key paths were found and they\nare listed in Table 2. All the path scores were evaluated by\nthe equation (1) of the next section.\n3.3 Formulation\nWe have several criteria for choosing the best key path. A\ngood choice of a key section should be rich in tonic and\ndominant chords, as they are the most important chords to\ndeﬁne and establish a key [10]. It is more preferable if the\nkey section starts and ends with the tonic chord, and with\nless tonicizations as a simpler explanation is better than a\ncomplicated one. In a music excerpt, less modulations and\nmodulations to closer keys are preferred. We formulate\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n503Figure 4. The incomplete B[ major Tree\nthese criteria with equation (1):\nStotal=aStd\u0000bSton\u0000cScost+dSstend\u0000eSsect (1)\nwhereStdis the no. of tonic and dominant chords, Ston\nis the total number of tonicization steps. For example, in\nchord progression V/V/ii V/ii ii, the ﬁrst chord has two\nsteps, while the second chord has one step. Ston= 2+1+\n0 = 3.Scostis the total modulation cost: the total tonal\ndistance of each modulation measured by Lerdahl’s dis-\ntance deﬁned in [8]. Sstend indicates whether the excerpt\nstarts and ends with tonic or not. Ssectis the total number\nof key sections. If a key section has only 2 chords, it is\ncounted as 3 in Ssectas a penalty. These parameters con-\ntrol how well chords ﬁt in a key section against how often\nthe modulation occurs. Std,StonandSstend maximizes\nﬁtness of the chord sequence to a key section. Scostand\nSsectinduce penalty whenever modulation occurs. The pa-\nrametersStd,Ston,Scost,Sstend andSsectare normalized\nso that their mean and standard deviation are 0 and 1 re-\nspectively. All the coefﬁcients, namely a;b;c;d;e, are de-\ntermined experimentally, although a slightly different set\nof values does not have a large effect on the key partition-\ning results. They are set at [a;b;c;d;e] = [1 ;0:4;2;2;0:4].\nKey structure is generally thought to be hierarchical. An\nexcerpt may have one level of large-scale key changes and\nanother level of tonicizations [17], and the boundary is not\nwell-deﬁned. So it seemed fair to adjust these parameters\nin order to match the level of key changes labeled by the\nground truth. The key path with the highest Stotal is cho-\nsen as the best path.\n4. EXPERIMENTS\n4.1 Settings\nTo test the system, we have chosen the Kostka-Payne cor-\npus, which contains classical music excerpts in a theory\nbook [5]. This selection has 46 excerpts, covering compo-\nsitions of many famous composers. They serve as repre-\nsentative examples of classical music in common practice\nperiod (around 1650-1900). All of the excerpts were exam-\nined. This corpus has ground truth key information labeled\nby David Temperley1. The mode (major or minor) of the\nkey was labeled by an experienced musician. The chord\nlabels are also available from the website, with the mode\n1http://www.theory.esm.rochester.edu/temperley/kp-stats/added by the experienced musician2. All the chord types\nhave been mapped to their roots: major or minor. There\nare 25 excerpts with a single key and 21 excerpts with key\nchanges (one to four key changes). The longest excerpt\nhas 47 chords whereas the shortest excerpt has 8 chords.\nThe instrumentation ranges from solo piano to orchestral.\nAs we assume the input chord sequence to be harmoni-\ncally complete, the last chord of excerpts 9, 14 and 15 were\ntruncated as they are the starting chord of another phrase.\nThere are 866 chords in total. For every excerpt, the parti-\ntioning algorithm in x3.2 is used to obtain the best path.\n4.2 Baseline system\nTo the best of author’s knowledge, there is currently no\nkey partitioning algorithm directly use chord labels as in-\nput. To compare the performance of our key partitioning\nsystem, another system based on Krumhansl’s harmonic-\nhierarchy information and dynamic programming were\nset up. Krumhansl’s key proﬁle has been used in many\nnote-based key tracking systems such as [3, 9]. Here\nKrumhansl’s harmonic-hierarchy ratings (listed in Chap-\nter 7 of [6]) are used to obtain the perceptual closeness of\na chord in a particular key. A higher rating corresponds\nto a higher tendency to be part of the key. As a fair com-\nparison, the number of chords in a key section is restricted\nto be at least three, which is the same in our system. To\nprevent ﬂuctuations of the key, a penalty term D(x;y)is\nimposed on key changes. The multiplicative constant of\npenalty term \u000bis determined experimentally to give the\nbest result. The best key path is found iteratively by the\ndynamic programming technique presented by equations\n(2) and (3):\nAx[1] =Hx[1]8x2K (2)\nAx[n] = max\u001aAx[n\u00001] +Hx[n];\nAy[n\u00001] +Hx[n]\u0000\u000bD(x;y)\u001b\n8x;y2K; wherey6=x\n(3)\nHx[n]is the harmonic-hierarchy rating of the nthchord\nwith the key x.Ax[n]is the accumulated key strength of\nthenthchord when the current key is x.Kis the set of all\npossible keys. D(x;y)is the distance between keys x;y\nbased on the key distance in [6] derived from multidimen-\nsional scaling. The best path can be found by obtaining\nthe largestAxof the last chord and tracking all the way\nback toAx[1]. The same Kostka-Payne corpus chord la-\nbels were used to test this baseline system. The best result\nwas obtained by setting \u000b= 4:5.\n4.3 Results\nThe key partitioning result of our proposed system and the\nbaseline system were compared against the ground truth\nprovided by Temperley. Four kinds of result metrics were\nused. The average matching score is shown in Figure 5.\n2All the chord and key labels can be found here:\nhttps://drive.google.com/ﬁle/d/0B0Td6LwTUL-\nvMVJ6MFcyYWsxVzQ/edit?usp=sharing\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n504Figure 5. Key partitioning result, with 95% conﬁdence\ninterval\nExact indicates the exact matches between the obtained\nkey path and the ground truth. As modulation is a grad-\nual process, the exact location of key changes may not be\ndeﬁnitive. It is more meaningful to consider Inexact. For\ninexact, the obtained key is also considered as correct if\nit matches the key of the previous or next chord. MIREX\nrefers to the MIREX 2014 Audio key detection evaluation\nstandard3. Harmonically close keys will be given a par-\ntial point. Perfect ﬁfth is awarded with 0.5 points, rela-\ntive minor/ major 0.3 points, whereas parallel major/ mi-\nnor 0.2 points. This is useful as sometimes a chord pro-\ngression may be explainable by two different related keys.\nMIREX inrefers to the MIREX standard, but with the ad-\ndition that the points of previous or next chord will also be\nconsidered and the maximum point will be chosen as the\nmatching score of that chord.\nThe proposed system outperforms the baseline system\nby about 18% for exact or inexact matching and 0.1 points\nfor MIREX-related scores. It shows that our knowledge-\nbased tonal grammar tree system is better than the base-\nline system which is based on perceptual closeness. Tonal\nstructural information is exploited, so we have a better un-\nderstanding of the chord progression and modulations.\n4.4 Error analysis\nThe ground truth key information are compared against the\nkey labels generated by the proposed algorithm. 17 bound-\nary errors were detected, ie. the key label of the previous\nor next chord was recognized instead. In classical music,\nmodulation is usually not a sudden event. It occurs gradu-\nally through several pivot chords (chords common to both\nkeys) [10]. Therefore it is sometimes subjective to deter-\nmine the boundary between two key sections. It may not\nbe a wrong labeling if the boundary is different from the\nground truth. Other types of error are listed in Table 3.\nThe most common error is the misclassiﬁcation as dom-\ninant key, which is the closest related key [10]. It shares\nmany common chords with the tonic key. From Table 4,\nthe same chord sequence can be analyzed by two keys that\nare dominantly-related. Although the B[ major analysis\ncontains more tonicizations, the resultant score disadvan-\ntage may be outweighed by the cost of key changes, if it is\nfollowed by a B[ major section.\n3http://www.music-ir.org/mirex/wiki/2014:Audio Key DetectionKey relation Semitone difference total no. %\nDominant 7 35 32.7\nSupertonic 2 32 29.9\nRelative 3 11 10.3\nParallel 0 11 10.3\nMinor 3rd3 9 8.4\nMajor 3rd4 8 7.5\nLeading tone 1 3 2.8\nTritone 6 2 1.9\nTable 3. Eight categories of the 107 error labels\nchord symbols Gm C F B[ Gm C F\nF major ii V I IV ii V I\nB[major vi V/V V I vi V/V V\nTable 4. Analysis with two different keys\nModulations between keys that are supertonically-\nrelated (differs by 2 semitones) or relative major / minor\nhave a similar problem as the dominant key modulation.\nMany common chords are shared among both keys, so it\nis easy to confuse these two keys. It is worth to mention\nthat nine of the supertonically-related errors came from ex-\ncerpt 45. In Temperley’s key labels, the whole excerpt is\nlabeled as C major with measures 10-12 considered as a\npassage of tonicization. However, in [5], it was written that\n“Measures 10-12 can be analyzed in terms of secondary\nfunctions or as a modulation”. If the measures 10-12 are\nconsidered as a modulation to D minor, then the analysis\nof these nine chords is correct.\nThe parallel key modulation, for example from C major\nto C minor, has a different problem. Sometimes composers\ntend to start the phrase with a new mode (major or minor)\nwithout much preparation, as the tonic is the same. Fluctu-\nation between major and minor of the same key has always\nbeen common [10]. When the phrase information is ab-\nsent, the exact position of modulation cannot be found by\nthe proposed system.\nIn another way, there may exist some ornament notes\nthat obscure the real identity of a chord, so that the chord\nsymbol analyzed acoustically is different from the chord\nsymbol analyzed structurally or grammatically. For exam-\nple, in Figure 6, the ﬁrst two bars should be analyzed as\nIV6-vii\u001e7-Iprogression in A major. However, the C] of\ntheIchord is delayed to the next chord. The appoggiatura\nB]made theIchord sound as a ichord, the tonic minor\nchord instead. Similarly, the last two bars should be ana-\nlyzed asIV6=5-viio7-iin F] minor. However, the passing\nnote A] made theichord sound as a Ichord, the original\nA is delayed to the next chord. In these two cases, the key\nderived by the last chord in the progression is in conﬂict\nwith the other chords. Hence the key will be recognized\nwrongly if the acoustic chord symbol is provided instead\nof the structural chord symbol.\n5. DIFFICULTIES\nThe biggest problem of this research is lack of labeled data.\nTo the best of our knowledge, large chord label database\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n505Figure 6. Excerpt from Mozart’s Piano Concerto no. 23,\n2ndmovement\nfor classical music is absent. The largest database we could\nﬁnd is the Kostka-Payne corpus used in this paper. In the\nfuture, we may consider manually label more music pieces\nto check if the system works generally well in classical\nmusic.\nMoreover, key partitioning is sometimes subjective to\nlistener’s perception. In some cases, there are several pivot\nchords to establish the new key center. “Ground truth”\nboundaries of key sections are sometimes set arbitrarily.\nOr there are several sets of acceptable and sensible parti-\ntions of key sections. This problem is yet to be studied. In-\nconsistency between acoustic and structural chord symbols\nmentioned inx4.4 is also yet to be solved. For any rule-\nbased systems, exceptions may occur. Composers may de-\nliberately break some traditions in the creative process. It\nis not possible to handle all these exceptional cases.\n6. FUTURE WORK AND CONCLUSION\nWe have only considered major and minor chords in this\npaper. As dominant 7thand diminished chords are com-\nmon in classical music, we may consider expanding the\nchord type selection to make chord labels more accurate.\nThe current system assumes chord labels to be free of er-\nrors. We plan to study the method of key tracking in the\npresence of chord label errors. Then we may incorporate\nthis system to the chord classiﬁcation system for audio key\ndetection, as the key and chord progression is interdepen-\ndent. Currently the input phrases must be complete in or-\nder to make this tree building process work. We plan to ﬁnd\nthe key partition method for incomplete input phrases. A\nmore efﬁcient algorithm for tree building process, instead\nof brute-force, is yet to be discovered. Then less trees are\nrequired to be built.\nIn this paper, we have discussed the uses of tonal gram-\nmar to partition key sections of classical music. The\nproposed system outperforms the baseline system which\nuses dynamic programming on Krumhansl’s harmonic-\nhierarchy ratings. This tonal grammar is useful for tonal\nclassical music information retrieval and hopefully more\nuses can be found.\n7. REFERENCES\n[1] A. Cadwallader and D. Gagn ´e.Analysis of Tonal Mu-\nsic: A Schenkerian Approach. Oxford University Press,\nOxford, 1998.\n[2] B. Catteau, J. Martens, and M. Leman. A probabilistic\nframework for audio-based tonal key and chord recog-\nnition. Advances in Data Analysis, (2005):1–8, 2007.[3] E. G ´omez and P. Herrera. Estimating The Tonality Of\nPolyphonic Audio Files: Cognitive Versus Machine\nLearning Modelling Strategies. In ISMIR, pages 1–4,\n2004.\n[4] B. Hyer. Key (i). In S. Sadie, editor, The New Grove\nDictionary of Music and Musicians. Macmillan Pub-\nlishers, London, 1980.\n[5] S. M. Kostka and D. Payne. Workbook for tonal har-\nmony, with an introduction to twentieth-century music.\nMcGraw-Hill, New York, 3rd ed. edition, 1995.\n[6] C. L. Krumhansl. Cognitive Foundations of Musical\nPitch. Oxford University Press, New York, 1990.\n[7] K. Lee and M. Slaney. Acoustic Chord Transcription\nand Key Extraction From Audio Using Key-Dependent\nHMMs Trained on Synthesized Audio. In Array, edi-\ntor,Ieee Transactions On Audio Speech And Language\nProcessing, volume 16, pages 291–301. Ieee, 2008.\n[8] F. Lerdahl. Tonal pitch space. Oxford University Press,\nOxford, 2001.\n[9] H. Papadopoulos and G. Peeters. Local Key Esti-\nmation From an Audio Signal Relying on Harmonic\nand Metrical Structures. IEEE Transactions on Audio,\nSpeech, and Language Processing, 20(4):1297–1312,\nMay 2012.\n[10] W. Piston. Harmony. W. W. Norton, New York, rev. ed.\nedition, 1948.\n[11] C. Raphael and J. Stoddard. Functional harmonic anal-\nysis using probabilistic models. Computer Music Jour-\nnal, pages 45–52, 2004.\n[12] M. Reger. Modulation. Dover Publications, Mineola,\nN.Y ., dover ed. edition, 2007.\n[13] M. Rohrmeier. Towards a generative syntax of tonal\nharmony. Journal of Mathematics and Music, 5(1):35–\n53, Mar. 2011.\n[14] H. Schenker. Free Composition. Longman, New York,\nLondon, 1979.\n[15] A. Shenoy and R. Mohapatra. Key determination of\nacoustic musical signals. 2004 IEEE International\nConference on Multimedia and Expo (ICME) (IEEE\nCat. No.04TH8763), pages 1771–1774, 2004.\n[16] R. Steblin. A history of key characteristics in the eigh-\nteenth and early nineteenth centuries. University of\nRochester Press, Rochester, NY , 2nd edition, 2002.\n[17] D. Temperley. The cognition of basic musical struc-\ntures. MIT Press, Cambridge, Mass., 2001.\n[18] D. H. Younger. Recognition and parsing of context-\nfree languages in time n3.Information and Control,\n10(2):189–208, 1967.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n506"
    },
    {
        "title": "Improving Music Recommender Systems: What Can We Learn from Research on Music Tastes?",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417797",
        "url": "https://doi.org/10.5281/zenodo.1417797",
        "ee": "https://zenodo.org/records/1417797/files/Laplante14.pdf",
        "abstract": "The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such pre- dictions are difficult to make accurately due to the com- plex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and so- ciology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations be- tween music preferences and a wide variety of sociodem- ographic and individual characteristics, including person- ality traits, values, ethnicity, gender, social class, and po- litical orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review.",
        "zenodo_id": 1417797,
        "dblp_key": "conf/ismir/Laplante14",
        "keywords": [
            "music tastes",
            "pre- dictions",
            "complex nature",
            "correlates",
            "music preferences",
            "sociodemographic",
            "individual characteristics",
            "social influences",
            "personal and social identities",
            "music recommender systems"
        ],
        "content": "IMPROVING MUSIC RECO MMENDER SYSTEMS: WHA T \nCAN WE LEARN FROM RE SEARCH ON MUSIC TASTES? \n  Audrey Laplante \n École de bibliothéconomie et des sciences de l’information, Université de Montréal  \naudrey.laplante@umontreal.ca  \nABSTRACT  \nThe success of a music recommender system depends on \nits ability to predict how much a particular user will like or dislike each item in its catalogue. However, such pre-\ndictions are difficult to make accurately due to the co m-\nplex nature of music tastes. In this paper, we review the  \nliterature on music tastes from social psychology and s o-\nciology of music to identify the correlates of music tastes and to understand how music tastes are formed and \nevolve through time. Research shows associations be-\ntween music preferences and a wide variety of sociode m-\nographic and individual characteristics, including person-ality traits, values, ethnicity, gender, social class, and p o-\nlitical orientation. It also reveals the importance of social \ninfluences on music tastes, more specifically from family \nand peers, as well as the central role of music tastes in the \nconstruction of personal and social identities. Suggestions for the design of music recommender systems are made \nbased on this literature review.  \n1. INTRODUCTION  \nThe success of a music recommender s ystem (RS) d e-\npends on its ability to propose the right music, to the right \nuser, at the right moment. This, however, is an extremely \ncomplex task. A wide variety of factors influence the de-\nvelopment of music preferences, thus making it difficult for system s to predict how likely a particular user is to \nlike or dislike a piece of music. This probably explains why music RS are often based on collaborative filtering (CF): it allows systems to uncover complex patterns in preferences that would be difficult to model based on mu-\nsical attributes [1]. However, in order to make those  pre-\ndictions as accurate as possible, these systems need to collect a considerable amount of information about the music preferences of each user. To do so, they elicit e x-\nplicit feedback from users, inviting them to rate, ban, or love songs, albums, or artists. They also collect implicit feedback, most often in the form of purchase or  listening \nhistory data (including songs skipped) of individual users. These pieces of information are combined to form the u s-\ner’s music taste profile, which allows the system s to ide n-\ntify like -minded users and to recommend music based on the taste profiles of these users. One of the pri ncipal lim i-\ntations of RS based on CF is that, before they could gat h-\ner sufficient information about the preferences of a user, they perform po orly. This corr esponds to the well -\ndocumented new user cold -start prob lem.  \nOne way to ease this problem would be to try to enrich \nthe taste profile of a new user by relying on other types of \ninformation that are known to be correlated with music preferenc es. More recently, it has become increasingly \ncommon for music RS to encourage users to create a pe r-\nsonal profile, or to allow them to connect to the system with a general social network  site account (for i nstance, \nDeezer  users can connect with their Facebook or their \nGoogle+ account). Music RS thus have access to a wider \narray of i nformation regarding new users.  \nResearch on music tastes can provide insights into how \nto take advantage of this information. More than a decade ago, similar reasoning led Ui tdenbogerd and Schyndel [2] \nto review the literature on the subject to identify the fa c-\ntors affecting music tastes. In 2003 , however, a paper \npublished by Rentfrow and Gosling [3] on the relation-\nship between music and personality generated a renewed interest for music tastes among researchers , which tran s-\nlated into a sharp increase i n research on this topic.  \nIn this paper, we propose to review the recent literature \non music preferences  from social psychology and socio l-\nogy of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve \nthrough time. We first explain the process by which we \nidentified and selected the articles and books reviewed. We then present the structure and the correlates of music preferences based on the literature review. We conclude with a brief discussion on the imp lications of these fin d-\nings for music RS design.    \n2. METHODS  \nWe used two databases to identify the literature on music \npreferences, one in psychology, PsycINFO  (Ovid), and \none in sociology, Sociological Abstracts (ProQuest). We \nused the thesaurus of each da tabase to find the d e-\nscriptors that were used to represent the two concepts of interest (i.e., music, p references), which led to the  que-\nries presented in Table 1.   \nPsycINFO  music AND preferences  \nSociological  \nAbstracts:  (music OR \"music/musical\") AND (\" pref-\nerence/preferences\" OR preferences)  \nTable 1. Queries used to retrieve articles in databases  \n © Audrey Laplante  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Audrey Laplante . “Improving \nMusic Recommend er Systems: What Can We Learn from Research on \nMusic Tastes?”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n451  \n \nBoth searches were limited to the subject heading field. We also limited the search to peer -reviewed publications \nand to articles published in 1999 or later  to focus on the \narticles pub lished during  the last 15 years . This yiel ded \n155 a rticles in PsycINFO and 38 a rticles in Soc iologi cal \nAbstracts. A dditional art icles and books were ident ified \nthrough chai ning (i.e., by fo llowing cit ations in r etrieved \narticles), which allowed us to add a few i mportant doc u-\nments that had been pu blished before 1999. Consi dering \nthe limited space and the large number of doc uments on \nmusic tastes, further sele ction was nee ded. A fter ensuring \nthat all a spects were cov ered, we r ejected a rticles with a \nnarrow focus (e.g.,  articles f ocusing on a specific music \ngenre or personality trait). For topics on which there were several public ations, we re tained a rticles with the highest \nnumber to cit ations based on Google Scholar. We also \ndecided to ex clude articles on the rela tions hip between \nmusic prefe rences and the fun ctions of music to conce n-\ntrate on ind ividual characteris tics.  \n3. REVIEW OF LITERATURE  ON MUSIC \nTASTES  \nResearch shows that people, especially adolescents, use \ntheir music tastes as a social badge through which they \nconvey who they are, or rather how they would like to be \nperceived [4, 5]. This indicates that people consider that \nmusic preferences reflect personality, values, and beliefs. In the same line, people often make inferences about the personality of others based on their music preferences, as revealed  by a study in which music was found to be the \nmain topic of conversation between two young adults who are given the task of getting to know each other [6]. \nThe same study showed tha t these inferences are often \naccurate: people can correctly infer several psychological characteristics based on one’s music preferences, which \nsuggests that they have an intuitive knowledge of the r e-\nlationships that exist between music preferences and pe r-\nsonality. Several researchers have studied these relatio n-\nships systematically to identify the correlates of music \npreferences that pertain to personality and demographic characteristics, values and beliefs, and social influences and stratification.  \n3.1 Dimens ions of music tastes  \nThere are numerous music genres and subgenres. Ho w-\never, as mentioned in [7], attitudes toward genres are not \nisolated from one another: there are genres that seem to \ngo together while others seem to oppose. Therefore, to \nreduce the number of variables, prior to attempting to \nidentify the correlates of music preferences, most r e-\nsearchers start by examining the nature of music prefe r-\nences to identify the principal dimensions. The approach of Rentfrow and Gosling [3] is representative of the \nwork of several researchers. To un cover the underlying \nstructure of music preferences, they first asked 1,704 \nstudents from an American university to indicate their \nliking of 14 different music genres using a 7 -point Likert \nscale. This questionnaire was called the Short Test Of \nMusic Prefe rences (STOMP). They then performed fa c-\ntor analysis by means of principal -components analysis with varimax rotation on participants’ ratings. This a l-\nlowed them to uncover a factor structure of music prefer-ences, composed of four dimensions, which they labe led \nReflective and Complex , Intense and Rebellious , Upbeat \nand Conventional , and Energetic and Rhythmic . Table 2  \nshows the genres most strongly associated with each d i-\nmension. To verify the generalizability of this structure \nacross samples, they replicated  the study with 1,384 stu-\ndents of the same university, and examined the music libraries of individual users in a peer -to-peer music se r-\nvice. This allowed them to confirm the robustness of the model.  \n \nMusic -preference d i-\nmension  Genres most strongly \nassociat ed \nReflective and Complex  Blues, Jazz, Classical, Folk  \nIntense and Rebellious  \n Rock, Alternative, Heavy \nmetal  \nUpbeat and Conventional  Country, Sound tracks, Re-\nligious, Pop  \nEnergetic and Rhythmic  \n Rap/hip -hop, Soul/funk, \nElectronica/dance  \nTable 2. Music-preference dimensions of Rentfrow and \nGosling (2003) . \nSeveral other researchers replicated Rentfrow and \nGosling’s study with other populations  and slightly di f-\nferent methodologies. To name a few, [8] surveyed 2,334 \nDutch adolescents aged 12 –19; [9] surveyed 268 Jap a-\nnese college students;  [10, 11] surveyed 422 and 170 \nGerman students, respectively; and [12] surveyed 358 \nCanadian students. Although there is a considerable de-\ngree of similarity in the results acros s these studies, there \nalso appears to be a few inconsistencies. Firstly, the number of factors varies: while 4 studies revealed a 4 -\nfactor structure [3, 8-10], one found 5 factors [11], and \nanother, 9 factors\n1 [12]. These differences c ould poten-\ntially be explained by the fact that researchers used di f-\nferent music preference tests: the selection of the genres to include in these tests depends on the listening habits of the target population and thus needs to be adapted. The \ngrouping of genres also varies. In the 4 above- mentioned \nstudies in which a 4 -factor structure was found, rock and \nmetal music were consistently grouped together. Howe v-\ner, techno/electronic was not always grouped with the same genres: while it was grouped with rap, hip -hop, and \nsoul music in 3 studies, it was grouped with popular m u-\nsic in the study with the Dutch sample [8]. Similarly, r e-\nligious music was paired with popular music in Rentfrow and Gosling’s study , but was paired with classical and \njazz music in the 3 other studies. These discrepancies could come from the fact that some music genres might \nhave different connotations in different cultures. It can \nalso be added that music genres are problematic  in th em-\nselves: they are broad, inconsistent, and ill -defined. To \n                                                             \n1 For this study, the researchers started with 30 genres, as opposed to \nothers who used between 11 and 21 genres.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n452  \n \nsolve these problems, Rentfrow and colleagues [13, 14] \nreplicated the study yet again, but used 52 music excerpts representing 26 different genres to measure music prefer-ences instead of a list of music genres. The resulting \nstructure was slightly different. It was composed of 5 fa c-\ntors labeled Mellow, Unpretentious , Sophisticated , In-\ntense , and Contemporary  (MUSIC). This approach also \nallowed them to examine the ties between the factors and \nthe musical attributes. To do so, they asked non -experts \nto rate each music excerpt according to various attrib utes \n(i.e., auditory features, affect, energy level, perceived complexity) and used this information to identify the m u-\nsical attributes that were more strongly associated with each factor.  \n3.2 Personality traits  \nSeveral researchers have examined the relationsh ip be-\ntween music preferences and personality traits [3, 8-10] \nusing the 5 -factor model of personality, commo nly called \nthe “Big Five” dimensions of personality (i.e., Extrave r-\nsion, Emotional Stability, Agreeableness, Conscientious-\nness, and Openness to Experience). Rentfrow and Go s-\nling [3] were the first to conduct a la rge-scale study f o-\ncusing on this aspect, involving more than 3,000 partic i-\npants. In addition to taking the STOMP test for measu r-\ning their music preferences, participants had to complete \n6 personality tests, including the Big Five Inventory. The \nanalysis of the results revealed associations between \nsome personality traits and the 4 dimensions of music \npreferences. For instance, they found that liking Refle c-\ntive and Complex music (e.g., classical, jazz) or Intense \nand Rebellious music (e.g., rock, metal) was positively \nrelated to Openness to Experience; and liking Upbeat and \nConventional  music (e.g., popular music) or Energetic \nand Rhythmic  music (e.g., rap, hip -hop) was positively \ncorrelated with extraversion . Emotional Stability was the \nonly personality dimension that had no significant corr e-\nlation with any of the music- preference dimensions. \nOpenness and Extraversion  were the best predictors of \nmusic preferences.   \nAs mentioned previously , since researchers use diffe r-\nent genres and thus find different music -preference di-\nmensions, comparing results from various studies is pro b-\nlematic. Nonetheless, subsequent studies seem to confirm \nmost of Rentfrow and Gosling’s findings. Delsing et al. [8] studied Dut ch adolescents and found a similar pattern \nof associations between personality and music prefe r-\nences dimensions . Only two correlations did not match. \nHowever, it should be noted that the correlations were \ngenerally lower, a disparity the authors attribute to the \nage difference between the two samples (college student \nvs. adolescents): adolescents being more influenced than young adults by their peers, personality might have a lesser effect on their music preferences. Brown [9] found \nfewer significant correlations when studying Japanese university students. The strongest correlations concerned Openne ss, which was positively associated with liking \nReflective and Complex  music (e.g., classical, jazz) and \nnegatively related to liking Energetic and Rhythmic m u-\nsic (e.g., hip -hop/rap). The positive correlation between Energetic and Rhythmic  music and Extrav ersion , which \nwas found in most other studies [3, 8, 10], was not found \nwith the Japanese sample.  \n3.3 Values and Beliefs  \nFewer recent studies have focused on the relationship b e-\ntween music preferences and values or beliefs compared to personality. Nevertheless, several correlates of music preferences were found in this area, from political orien-\ntation to religion to vegetarianism [15].  \n3.3.1 Political Orientation  \nIn the 1980s, Peterson and Christenson [16] surveyed 259 \nAmerican university students on their music preferences and political orientatio n. They found that liberalism was \npositively associated with liking jazz, reggae, soul, or hardcore punk, whereas linking 70s rock or 80s rock was negatively related to liberalism. They also uncovered a relationship between heavy metal and political aliena tion: \nheavy metal fans were significantly more likely than oth-ers to check off the “Don’t know/don’t care” box in re-sponse to the question about their political orientation. More recently, Rentfrow and Gosling [3] found that poli t-\nical conservatism was positively associated with liking \nUpbeat & Conventional  music (e.g., popular music), \nwhereas political liberalism was positively associated \nwith liking Energetic and Rhythmic (e.g., rap, hip -hop) or \nReflective and Co mplex (e.g., classical, jazz)  music, al t-\nhough the last two correlations were weak. North and Hargreaves [15], who surveyed 2,532 British individuals, \nand Gardikiotis and Baltzis [17], who surveyed 606 \nGreek college students, also found that people who liked classical music, opera, and blues were more likely to have liberal, pro -social beliefs (e.g., public health care, prote c-\ntion of the environment, taking care of the most vulner a-\nble). In contrast, fans of hip -hop, dance, and DJ -based \nmusic were found to be amon g the least likely groups to \nhold liberal beliefs (e.g., increased taxation to pay for public services, public health care) [15]. As we can see, \nliking jazz and classical music was consistently associat-ed with liberalism, but no such clear patterns of associa-tions emerge d for other music genres, which suggest s that \nfurther research is needed.   \n3.3.2 Religious Beli efs \nThere are very few studies that examined the link b e-\ntween music preferences and religion. The only recent one we could find was the study by North and Hargreaves previously mentioned [15]. Their analysis revealed that \nfans of western, classical music, disco, and musicals were the most likely to be religious; whereas fans of dance, i n-\ndie, or DJ -based music  were least likely to be religious. \nThey also found a significant relation between music preferences and the religion affiliation of people. Fans of \nrock, musicals, or adult pop were more likely to be Protestant; fans of opera or country/western were more likely to be Catholic; and fans of R&B and hip -hop/rap \nwere more likely to adhere to other religions. Another older study used the 1993 General Social Survey to ex-amine the attitude of American adults towards heavy \nmetal and rap music and found that people  who attended \nreligious services were more likely to dislike heavy metal \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n453  \n \n(no such association was found with rap music) [18]. \nConsidering that religious belief s vary across cultures, \nfurther studies are needed to discern a clear pattern of a s-\nsociations between music preferences and religion.  \n3.4 Demographic Variables  \n3.4.1 Gender  \nSeveral studies have revealed associations between ge n-\nder and music tastes. It was found tha t women were more \nlikely to be fans of chart pop or other types of easy liste n-\ning music (e.g., country) [7, 12, 15, 19, 20], whereas men \nwere more likely to prefer rock and heavy metal [12, 15, \n19, 20]. This is not to say that women do not like rock : in \nColley’s study [19], which focused on gender differences \nin music tastes, rock was the second most highly rated music genre among women: the average rating for wo m-\nen was 4.1 (on an 8 -point scale from 0 to 7) vs. 4.8 for \nmen. There was, however, a much greater gap in the att i-\ntudes towards popular music between men and women, \nwho attributed 3.17 and 4.62 on average, respectively. This was the genre for which gender difference was the most pronounced. Lastly, it is worth mentioning that most studies did not find any significant gender difference for \nrap [7, 12, 19] , which indicates that music in this category \nappeals to both sexes. This is a surprising result conside r-\ning the misogynistic  message con veyed  by many rap \nsongs. Christenson and Roberts [7] speculated that this \ncould be due to the fact that men appreciate rap for its \nsubversive lyrics while women appreciate it for its danc e-\nability.  \n3.4.2 Race and Ethnicity  \nVery few studies have examined the ties between music \npreferences and race and ethnicity. In the 1970s, a survey \nof 919 American college students revealed that, among \nthe demographic characteristics, race was the strongest predictor of music preferences [21]. In a book published \nin 1998 [7], Christenson and Roberts affirmed that racial \nand ethnic origins of fans of a music genre mirror those of its musicians. To support their affirmation, they r e-\nported the results of a survey of adolescents conducted in the 1990s by  Carol Dykers in which 7% of black adole s-\ncents reported rap as their favourite music genre, co m-\npared with 13% of white adolescents. On the other hand, 25% of white adolescents indicated either rock or heavy \nmetal as their favourite genre, whereas these two  genres \nhad been only mentioned by a very small number of \nblack adolescents (less than 5% for heavy metal). North \n& Hargreaves [15] also found a significant relationship \nbetween ethnic background and music pref erences. This \nstudy was conducted more recently (in 2007), with Bri t-\nish adults, and with a more diversified sample in terms of ethnic origins. Interest ingly, they found that a high pr o-\nportion of the respondents who were from an Asian back-ground liked R&B, dance, and hip -hop/rap, which seems \nto challenge Christenson and Roberts’ affirmation. [22] \nwho studied  3,393 Canadian adolescents, performed a \ncluster analysis to group respondents according to their music preferences. They then examined the correlates of each music -taste cluster. The analysis revealed a diffe rent \nethnic composition for different clusters. For instance, the Black Styl ists cluster was composed of fans of hip -hop \nand reggae who were largely black,  with some South \nAsian represe ntation. By contrast, the Hard Rockers , who \nlike heavy metal and altern ative music, were almost e x-\nclusively white.  \n3.4.3 Age \nMost researchers who study music preferences draw their participants from the student population of the university where they work. As a result, samples are mostly homo g-\nenous in terms of age, which explains the small number \nof studies that focused on the relationship between age and music preferences. Age was found to be significantly associated with music preferences. For instance, [23] \ncompared the music preferences of different age groups and found that there were only two genres —rock and \ncountry —that appeared in the five most highly rated ge n-\nres of both the 18- 24 year olds and the 55 -64 year olds. \nWhile the favourite genres of younger adults were rap, \nmetal, rock, country, and blues; older adults preferred gospel, country, mood/easy listening, rock, and class i-\ncal/chamber  music. [15] also found a correlation between \nage and preferences for certain music genres. Unsurpri s-\ningly, t heir analysis revealed that people who l iked what \ncould be considered trendy music genres (e.g., hip -\nhop/rap, DJ -based music, dance, indie, chart pop) were \nmore likely to be young, whereas people who liked more conventional music genres (e.g., classical music, sixties pop, musicals, country) wer e more likely to be older. [24] \nconducted a study involving more than 250,000 partic i-\npants and found that the interest for music genres assoc i-\nated with the Intense  (e.g., rock, heavy metal, punk) and \nthe Contemporary  (e.g., rap, funk, reggae) music -\npreference dimensions  decreases with age, whereas the interest for music genres associated with the Unprete n-\ntious  (e.g., pop, country) and the Sophisticated  (e.g., clas-\nsical, folk, jazz) dimensions increases.  \nSome researchers have also looked at the trajectory of \nmusic tast es. Studies on the music preferences of children \nand adolescents revealed that as they get older, adole s-\ncents tend to move away from mainstream rock and pop, although these genres remain popular throughout adole s-\ncence [7]. Research has also demonstrated that music \ntastes are already fairly stable in early adolescence and further crysta llize in late adolescence or early adulthood \n[25, 26]. Using data from the American national Survey \nof Public Participation in the Arts  (SPPA) of 1982, 1992, \nand 2002, [23] examined the relationship between age \nand music tastes, with a focus on older age. They looked \nat the number of genres liked per age group and found \nthat in young adulthood, people had fairly narrow tastes. Their tastes expand into middle age (i.e., 55 year old), to then narrow again, suggesting that people disengage from music in older age. They also found that although music genres that are popular among younger adults change from generation to generation; they remain much more \nstable among older people.  \n3.4.4 Education  \nEducation was also found to be significantly correlated to music preferences. [15] found that individuals who held a \nmaster’s degree or a Ph .D. were most likely to like opera, \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n454  \n \njazz, classical music, or blues; whereas fans of country, musicals, or 1960s pop were most likel y to have a lower \nlevel of education.  [27] studied 325 adolescents and their \nparents and also found an association between  higher ed-\nucation and a taste for classical and jazz music. Parents with lower education were more likely to like popular music and to dislike classical and jazz music.  \n3.5 Social influences  \nAs mentioned before, research established that people use \ntheir music preferences as a social badge that conveys \ninformation about their personality, values, and beliefs. \nBut music does not only play a role in the construction of personal identity. It is also important to social identity. \nMusic preferences can also act as a social badge that i n-\ndicates membership in a social group or a social class.  \n3.5.1 Peers and Parents  \nConsidering the importance adolescents ascribe to both \nfriendship and music, it is not surprising to learn that s o-\ncial groups often identify with music subcultures during \nadolescence [4]. Therefore, it seems le gitimate to posit \nthat in the process of forming their social identity, ad o-\nlescents may adopt music preferences similar to that of \nother members of the social group to which they belong or they aspire to belong. This hypothesis seems to be con-firmed by rec ent studies. [28] examined the music prefe r-\nences of 566 Dutch adolescents who formed 283 same -\nsex friendship dyads  and found a high degree of similari-\nty in the music preferences of mutual friends. Since they surveyed the same  participants one year after the initial \nsurvey, they could also examine the role of music prefe r-\nences in the formation of new friendships and found that adolescents who had similar music preferences were more likely to become friends, as long as their mus ic \npreferences were not associated with the most main-stream dimensions. In the same line, Boer and colleagues [29] conducted three studies (two laboratory experime nts \ninvolving German participants  and one field study in-\nvolving Hong Kong university students) to examine the \nrelationship between similarity in music preferences and social attraction. They found that people  were more likely \nto be attracted to others who shared their music tastes b e-\ncause it suggests that they might also share the same va l-\nues.   \nAdolescents were also found to be influenced by the \nmusic tastes of their parents. ter Bogt and colleagues [27] \nstudied the music tastes of 325 adolescents and their pa r-\nents. Their analysis revealed some significant correlations. \nThe adolescents whose parents liked classical or jazz m u-\nsic were also more likely to appreciate these music genres. Parents’ preferences for popular music were associated \nwith a preference for popular and dance music in their  \nadolescent children. Parents were also found to pass on \ntheir liking of rock music to their adolescent daughters \nbut not to their son s. One possible explanation for the i n-\nfluence of parents on their children’s music tastes is that since family members liv e under the same roof, children \nare almost inevitably exposed to the favourite music of their parents.  3.5.2 Social Class \nIn La Distinction [ 30], Bourdieu proposed a social strati-\nfication of tastes and cultural practices according to which a taste for highbrow music or other cultural prod-ucts (and a disdain for lo wbrow culture) is considered the \nexpression of a high status. Recent research, however, suggests that a profound transformation in the tastes of the elite has occurred. In an article published in 1996, P e-\nterson and Kern [31] reported the results of a study of the \nmusical tastes of American s based on data from the Sur-\nvey of Public Participation in the Arts  of 1982 and 1992. \nTheir analysis revealed that far from being snob bish in \ntheir tastes, individuals with a high occupational status \nhad eclectic tastes which spanned across the lo w-\nbrow/highbrow spectrum. In fact, people of high status \nwere found to be more omnivorous than others , and their \nlevel of omnivorousness has increased over time. This \nhighly cited study has motivated several other researchers to study the link between social class and music prefe r-\nences. Similar studies were conducted in other countries, notably in France [32], Spain [33], and the Netherlands \n[34], and yielded similar results.  \n4. IMPLICATION FOR MUSI C RECOMMENDER \nSYSTEM DESIGN  \nA review of the literature on music tastes revealed many \ninteresting findings that could be used to improve music RS. Firstly, we saw that researchers had been able to un-cover the underlying structure of music preferences, which is composed of 4 or 5 factors. The main advantage \nfor music RS is that these factors are fairly stable across populations and time, as opposed to genres, which are \ninconsistent and ill -defined. As suggested by Rentfrow, \nGoldberg, and Levitin themselves [13], music RS could \ncharacterize the music preferences of their users  by calcu-\nlating a score for each dimension.  \nSecondly, some personality dimensions were found to \nbe correlated to music preferences. In most studies, \nOpenness to experience was the strongest predictor of music tastes. It was positively related to liking Reflective \nand Complex  music (e.g., jazz and classical) and, to a \nlesser extent, to Intense and Rebellious  music (e.g., rock, \nheavy metal). This could indicate that users who like th e-\nse music genres are more open to new music than other \nusers. RS could take that into account and adapt the no v-\nelty level accordingly.  \nFinally, the demographic correlates of music prefe r-\nences (e.g., age, gender, education, race), as well as rel i-\ngion and political orientation, could help ease the new \nuser cold -start problem. As me ntioned in the introdu c-\ntion, many music RS invite new users to create a profile and/or allow them to connect with a social networking site account, in which they have a profile. These profiles contain various types of information about users. Music RS coul d combine such information to make inferences \nabout the music preferences of new users. In the same line, information about the education and the occupation of a user could be used to identify potential high -status, \nomnivore users.   \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n455  \n \n5. CONCLUSION  \nThe abundant research on music tastes in sociology and \nsocial psychology has been mostly overlooked by music \nRS developers. This review of selected literature on the topic allowed us to present  the patterns of associations \nbetween music preferences and demographic ch aracteri s-\ntics, personality traits, values and beliefs. It also revealed the importance of social influences on music tastes and \nthe role music plays in the construction of individual and \nsocial identities.  \n6. REFERENCES  \n[1] Y. Koren: “Fac tor in the neighbors: Scalable and accurate \ncollaborative filtering,” ACM Transactions on Knowledge \nDiscovery Data,  Vol. 4, No. 1, pp. 1 -24, 2010.  \n[2] A. Uitdenbogerd and R. V. Schyndel: \"A review of factors \naffecting music recommender success,\" ISMIR 2002: \nProceedings of the Third International Conference on Music Information Retrieval , M. Fingerhut, ed., pp. 204 -\n208, Paris, France: IRCAM - Centre Pompidou, 2002.  \n[3] P. J. Rentfrow and S. D. Gosling: “The do re mi's of everyday life: The structure and pers onality correlates of \nmusic preferences.,” Journal of Personality and Social \nPsychology,  Vol. 84, No. 6, pp. 1236 -1256, 2003.  \n[4] S. Frith: Sound effects : youth, leisure, and the politics of \nrock'n'roll , New York: Pantheon Books, 1981.  \n[5] A. C. North and  D. J. Hargreaves: “Music and adolescent \nidentity,” Music Education Research, Vol. 1, No. 1, pp. \n75-92, 1999.  \n[6] P. J. Rentfrow and S. D. Gosling: “Message in a ballad: the role of music preferences in interpersonal perception,” Psychological Science,  Vol. 17, No. 3, pp. 236 -242, 2006.  \n[7] P. G. Christenson and D. F. Roberts: It's not only rock & \nroll : popular music in the lives of adolescents , Cresskill: \nHampton Press, 1998.  \n[8] M. J. M. H. Delsing, T. F. M. ter Bogt, R. C. M. E. \nEngels, and W. H. J. Mee us: “Adolescents' music \npreferences and personality characteristics,” European \nJournal of Personality,  Vol. 22, No. 2, pp. 109 -130, 2008.  \n[9] R. Brown: “Music preferences and personality among \nJapanese university students,” International Journal of \nPsychol ogy, Vol. 47, No. 4, pp. 259 -268, 2012.  \n[10] A. Langmeyer, A. Guglhor -Rudan, and C. Tarnai: “What \ndo music preferences reveal about personality? A cross -\ncultural replication using self -ratings and ratings of music \nsamples,” Journal of Individual Difference s, Vol. 33, No. \n2, pp. 119 -130, 2012.  \n[11] T. Schäfer and P. Sedlmeier: “From the functions of music to music preference,” Psychology of Music,  Vol. \n37, No. 3, pp. 279 -300, 2009.  \n[12] D. George, K. Stickle, R. Faith, and A. Wopnford: “The association between types of music enjoyed and cognitive, behavioral, and personality factors of those who listen,” \nPsychomusicology,  Vol. 19, No. 2, pp. 32-56, 2007.  \n[13] P. J. Rentfrow, L. R. Goldberg, and D. J. Levitin: “The \nstructure of musical preferences: A five -factor model,” \nJournal of Personality and Social Psychology,  Vol. 100, \nNo. 6, pp. 1139 -1157, 2011.  \n[14] P. J. Rentfrow, L. R. Goldberg, D. J. Stillwell, M. Kosinski, S. D. Gosling, and D. J. Levitin: “The song remains the same: A replication and extension of t he \nmusic model,” Music Perception,  Vol. 30, No. 2, pp. 161 -\n185, 2012.  \n[15] A. C. North and D. J. Hargreaves, Jr.: “Lifestyle \ncorrelates of musical preference: 1. Relationships, living arrangements, beliefs, and crime , ” Psychology of Music , \nVol 35m No. 1,  pp. 58- 87, 2007.  \n[16] J. B. Peterson and P. G. Christenson: “Political orientation and music preference in the 1980s,” Popular Music and \nSociety,  Vol. 11, No. 4, pp. 1 -17, 1987.  \n[17] A. Gardikiotis and A. Baltzis: “'Rock music for myself \nand justice to the  world!': Musical identity, values, and \nmusic preferences,” Psychology of Music,  Vol. 40, No. 2, \npp. 143 -163, 2012.  \n[18] J. Lynxwiler and D. Gay: “Moral boundaries and deviant \nmusic: public attitudes toward heavy metal and rap,” Deviant Behavior,  Vol. 21, No. 1, pp. 63 -85, 2000.  \n[19] A. Colley: “Young people's musical taste: relationship with gender and gender -related traits,” Journal of Applied \nSocial Psychology,  Vol. 38, No. 8, pp. 2039 -2055, 2008.  \n[20] P. G. Christenson and J. B. Peterson: “Genre and gen der \nin the structure of music preferences,” Communication \nResearch,  Vol. 15, No. 3, pp. 282 -301, 1988.  \n[21] R. S. Denisoff and M. H. Levine: “Youth and popular music: A test of the taste culture hypothesis,” Youth & \nSociety,  Vol. 4, No. 2, pp. 237 -255, 1972.  \n[22] J. Tanner, M. Asbridge, and S. Wortley: “Our favourite melodies: musical consumption and teenage lifestyles,” \nBritish Journal of Sociology,  Vol. 59, No. 1, pp. 117 -144, \n2008.  \n[23] J. Harrison and J. Ryan: “Musical taste and ageing,” \nAgeing & Societ y, Vol. 30, No. 4, pp. 649 -669, 2010.  \n[24] A. Bonneville -Roussy, P. J. Re ntfrow, M. K. Xu, and J. \nPotter: “ Music through the ages: Trends in musical \nengagement and preferences from adolescence through middle adulthood, ”  American Psychological Association , \npp. 703 -717, 2013.  \n[25] M. B. Holbrook and R. M. Schindler: “Some exploratory findings on the development of musical tastes,” Journal of \nConsumer Research,  Vol. 16, No. 1, pp. 119 -124, 1989.  \n[26] J. Hemming: “Is there a peak in popular music preference at a certain song- specific age? A replication of Holbrook \n& Schindler’s 1989 study,” Musicae Scientiae,  Vol. 17, \nNo. 3, pp. 293 -304, 2013.  \n[27] T. F. M. ter Bogt, M. J. M. H. Delsing, M. van Zalk, P. G. Christenson, and W. H. J. Meeus: “Intergenerational Continuity of Taste: Parental and Adolescent Music \nPreferences,” Social Forces,  Vol. 90, No. 1, pp. 297 -319, \n2011.  \n[28] M. H. W. Selfhout, S. J. T. Branje, T. F. M. ter Bogt, and W. H. J. Meeus: “The role of music preferences in early adolescents’ friendship formation and stability,” Journal \nof Adolescence,  Vol. 32, No. 1, pp. 95 -107,\n  2009.  \n[29] D. Boer, R. Fischer, M. Strack, M. H. Bond, E. Lo, and J. Lam: “How shared preferences in music create bonds between people: Values as the missing link,” Personality \nand Social Psychology Bulletin,  Vol. 37, No. 9, pp. 1159 -\n1171,  2011.  \n[30] P. Bourdieu: La distinction: critique sociale du jugement , \nParis: Éditions de minuit, 1979.  \n[31] R. A. Peterson and R. M. Kern: “Changing Highbrow \nTaste: From Snob to Omnivore,” Ameri can Sociological \nReview,  Vol. 61, No. 5, pp. 900 -907, 1996.  \n[32] P. Coulangeon and Y. Lemel: “Is ‘distinction’ really \noutdated? Questioning the meaning of the omnivorization of musical taste in contemporary France,” Poetics,  Vol. \n35, No. 2 -3, pp. 93 -111, 2007. \n[33] J. López -Sintas, M. E. Garcia -Alvarez, and N. Filimon: \n“Scale and periodicities of recorded music consumption: \nreconciling Bourdieu's theory of taste with facts,” The \nSociological Review,  Vol. 56, No. 1, pp. 78 -101, 2008.  \n[34] K. van Eijck: “Soci al Differentiation in Musical Taste \nPatterns,” Social Forces,  Vol. 79, No. 3, pp. 1163 -1185, \n2001.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n456"
    },
    {
        "title": "In-depth Motivic Analysis based on Multiparametric Closed Pattern and Cyclic Sequence Mining.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418345",
        "url": "https://doi.org/10.5281/zenodo.1418345",
        "ee": "https://zenodo.org/records/1418345/files/Lartillot14.pdf",
        "abstract": "The paper describes a computational system for exhaus- tive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclic- ity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for auto- mated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground- truth motives are detected, while additional relevant infor- mation completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source frame- work for audio and music analysis.",
        "zenodo_id": 1418345,
        "dblp_key": "conf/ismir/Lartillot14",
        "keywords": [
            "computational system",
            "exhaustive but compact description",
            "motivic patterns",
            "symbolic representations",
            "closed heterogeneous pattern mining",
            "multiparametric space",
            "control of pattern cyclic-ity",
            "new method for auto-automated bundling",
            "ground-truth motives",
            "additional relevant information"
        ],
        "content": "IN-DEPTH MOTIVIC ANALYSIS BASED ON MULTIPARAMETRIC\nCLOSED PATTERN AND CYCLIC SEQUENCE MINING\nOlivier Lartillot\nAalborg University, Department of Architecture, Design and Media Technology, Denmark\nolartillot@gmail.com\nABSTRACT\nThe paper describes a computational system for exhaus-\ntive but compact description of repeated motivic patterns in\nsymbolic representations of music. The approach follows\na method based on closed heterogeneous pattern mining\nin multiparametrical space with control of pattern cyclic-\nity. This paper presents a much simpler description and\njustiﬁcation of this general strategy, as well as signiﬁcant\nsimpliﬁcations of the model, in particular concerning the\nmanagement of pattern cyclicity. A new method for auto-\nmated bundling of patterns belonging to same motivic or\nthematic classes is also presented.\nThe good performance of the method is shown through\nthe analysis of a piece from the JKUPDD database. Ground-\ntruth motives are detected, while additional relevant infor-\nmation completes the ground-truth musicological analysis.\nThe system, implemented in Matlab, is made publicly\navailable as part of MiningSuite, a new open-source frame-\nwork for audio and music analysis.\n1. INTRODUCTION\nThe detection of repetitions of sequential representations in\nsymbolic music is a problem of high importance in music\nanalysis. It enables the detection of repeated motifs and\nthemes1, and of structural repetition of musical passages.\n1.1 Limitation of previous approaches\nFinding these patterns without knowing in advance their\nactual description is a difﬁcult problem. Previous approa-\nches have shown the difﬁculty of the problem related to the\ncombinatorial explosion of possible candidate patterns [2].\nSome approaches tackle this issue by generating a large set\nof candidate patterns and applying simple global heuris-\ntics, such as ﬁnding longest or most frequent patterns [3,8].\nSimilarly, other approaches base the search for patterns on\n1Here motif and theme are considered as different musicological in-\nterpretations of a same pattern conﬁguration: motifs are usually shorter\nthan themes.\nc\rOlivier Lartillot.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Olivier Lartillot. “IN-DEPTH MO-\nTIVIC ANALYSIS BASED ON MULTIPARAMETRIC CLOSED PAT-\nTERN AND CYCLIC SEQUENCE MINING”, 15th International Soci-\nety for Music Information Retrieval Conference, 2014.general statistical characteristics [5]. The problem is that\nthere is no guarantee that this global ﬁltering leads to a se-\nlection of patterns corresponding to those selected by mu-\nsicologists and perceived by listeners.\n1.2 Exhaustive mining of closed and cyclic patterns\nIn our research, we endeavour to reveal the factors under-\nlying this structural explosion of possible patterns and to\nformalise heuristics describing how listeners are able to\nconsensually perceive clear pattern structures out of this\napparent maze. We found that pattern redundancy is based\non two core issues [6]:\n\u000fclosed pattern mining: When a pattern is repeated,\nall underlying pattern representations it encompasses\nare repeated as well. In simple string representation,\nstudied in section 22, these more general patterns\ncorrespond to preﬁxes, sufﬁxes and preﬁxes of suf-\nﬁxes. The proliferation of general patterns, as shown\nin Figure 1, leads to combinatorial explosion. Re-\nstricting the search to the most speciﬁc (or “maxi-\nmal”) patterns is excessively selective as it ﬁlters out\npotentially interesting patterns (such as CDE in Fig-\nure 1), and would solely focus on large sequence rep-\netitions. By restricting the search to closed patterns\n– i.e., patterns that have more occurrences than their\nmore speciﬁc patterns –, all pattern redundancy is ﬁl-\ntered out without loss of information. [6] introduces\na method for exhaustive closed pattern mining.\n\u000fpattern cyclicity: When repetitions of a pattern are\nimmediately successive, another combinatorial set\nof possible sequential repetitions can be logically in-\nferred [2], as shown in Figure 2. This redundancy\ncan be avoided by explicitly modelling the cyclic\nloop in the pattern representation, and by general-\nising the notion of closed pattern accordingly.\nBy carefully controlling these factors of combinatorial\nredundancy without damaging the non-redundant pattern\ninformation, the proposed approach in [6] enables to out-\nput an exhaustive description of pattern repetitions. Pre-\nvious approaches did not consider those issues and per-\nformed instead global ﬁltering techniques that broadly miss\nthe rich pattern structure.\n2The more complex multiparametric general/speciﬁc transformations\nare studied in section 3.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n361ABCDECDEABCDECDEFigure 1. Patterns found in a sequence of symbols. Below\nthe sequence, each row represents a different pattern class\nwith the occurrences aligned to the sequence. Thick black\nlines correspond to closed patterns (the upper one is the\nmaximal pattern), grey lines to preﬁxes of closed patterns,\nand thin lines to non-closed patterns.\nA B C A B C A B C A ……\nFigure 2. Closed patterns found in a cyclic sequence of\nsymbols. The occurrences of the pattern shown in thick\nlines do not overlap, whereas those shown in thin lines do.\n1.3 New approach\nIn this paper, we propose a simpliﬁed description and mod-\nelling of this exhaustive pattern mining approach. In sec-\ntion 2, we present the problem of closed pattern mining on\nthe simple case of monoparametric string analysis, intro-\nduce a simpliﬁed algorithmic implementation, and present\na new way to simply justify the interest of the approach. In\nsection 3, the approach is generalised to the multidimen-\nsionality of the musical parametric space. Section 4 dis-\ncusses pattern cyclicity and presents a new simple model\nthat solves this issue. In section 5, the interest of the method\nis shown through the analysis of a piece of music from the\nJKUPDD database.\n2. CORE PRINCIPLES OF THE MODEL\n2.1 Advantages of incremental one-pass approach\nAs explained in the previous section, testing the closed-\nness of a pattern requires comparing its number of occur-\nrences with those of all the more speciﬁc patterns. Previous\ncomputer science researches in closed pattern mining (one\nrecent being [9]) incrementally construct the closed pat-\nterns dictionary while considering the whole document to\nbe analysed (in our case, the piece of music). This requires\nthe design of complex algorithms to estimate the number\nof occurrences of each possible pattern candidate.\nWe introduced in [6] a simpler approach based on an in-\ncremental single pass throughout the document (i.e., from\nthe beginning to the end of the piece of music), during\nwhich the closed pattern dictionary is incrementally con-\nstructed: for each successive note nin the sequence, allpatterns in the subsequence ending to that note nare ex-\nhaustively searched for. The main advantage of the incre-\nmental approach is based on the following property.\nLemma 2.1 (Closed pattern characterisation). When fol-\nlowing the incremental approach, for any closed pattern\nP, there exists a particular moment in the piece of music\nwhere an occurrence OofPcan be inferred while no oc-\ncurrence of any more speciﬁc pattern can be inferred.\nProof. There are three alternative conditions concerning\nthe patterns more speciﬁc than P:\n\u000fThere is no pattern more speciﬁc than P. In this\ncase, the observation is evident.\n\u000fThere is only one pattern Smore speciﬁc than P.\nFor instance, in Figure 3, S=ABCD is more spe-\nciﬁc than P=CD. Since Pis closed, it has more\noccurrences than S, so there exists an occurrence of\nPthat is not occurrence of S.\n\u000fThere are several patterns S1; : : : ; S nmore speciﬁc\nthanP. For instance, in Figure 1, S1=ABCDE\nandS2=ABCDECDE are both more speciﬁc than\nP=CDE. As soon as two different more speciﬁc\npatterns S1(one or several time) and S2(ﬁrst time)\nhave appeared in the sequence, pattern Pcan be de-\ntected, since it is repeated in S1andS2, butS2is not\ndetected yet, since it has not been repeated yet.\nAs soon as we detect a new pattern repetition, such that\nfor that particular occurrence where the repetition is de-\ntected, there is no more speciﬁc pattern repetition, we can\nbe sure that the discovered pattern is closed.\nWhen considering a given pattern candidate at a given\npoint in the piece of music, we need to be already informed\nabout the eventual existence of more speciﬁc pattern occur-\nrences at the same place. Hence, for a given note, patterns\nneed to be extended in decreasing order of speciﬁcity.\nTo details further the approach, let’s consider in a ﬁrst\nsimple case the monoparametric contiguous string case,\nwhere the main document is a sequence of symbols, and\nwhere pattern occurrences are made of contiguous sub-\nstrings. In this case, ‘more general than’ simple means ‘is\na subsequence of’. In other words, a more general pattern\nis apreﬁx or/of a sufﬁx of a more speciﬁc pattern. Let’s\nconsider these two aspects separately:\n\u000fSince the approach is incremental, patterns are con-\nstructed by incrementally extending their preﬁxes (in\ngrey in Figure 1). Patterns are therefore represented\nas chains of preﬁxes, and the pattern dictionary is\nrepresented as a preﬁx tree. In this paradigm, if a\ngiven pattern Pis a preﬁx of a closed pattern S, and\nif both have same number of occurrences, the preﬁx\nPcan still be considered as a closed pattern, in the\nsense that it is an intermediary state to the constitu-\ntion of the closed pattern S.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n362CDABCDEABCDEFigure 3. Closed patterns found in a sequence of sym-\nbols. The occurrence during which a pattern is discovered\nis shown in black. Dashed extensions indicate two possible\npattern extensions when integrating the last note.\n\u000fThe closedness of a pattern depends hence solely on\nthe patterns to which it is a sufﬁx. Thanks to the\nincremental one-pass approach, these more speciﬁc\npatterns are already inferred. The only constraint to\nbe added is that when a given note is considered,\nthe candidate patterns should be considered in de-\ncreasing order of speciﬁcity, i.e. from the longest to\nthe shortest (which are sufﬁxes of the longer ones).\nFor instance, in Figure 3, when analysing the last\nnote, E, there are two candidate patterns for exten-\nsion, ABCD and CD. Since we ﬁrst extend the most\nspeciﬁc pattern ABCDE, when considering then the\nmore general pattern CD, extension CDE is found as\nnon-closed and thus not inferred.\n2.2 Algorithmic details\nFollowing these principles, the main routine of the algo-\nrithm simply scans the musical sequence chronologically,\nfrom the ﬁrst to the last note. Integrating a new note con-\nsists in checking:\n\u000fwhether pattern occurrence(s) ending at the previous\nnote can be extended with the new note,\n\u000fwhether the new note initiates the start of a new pat-\ntern occurrence.\nThe extension of a pattern occurrence results from two al-\nternative mechanisms:\nRecognition the new note is recognised as a known exten-\nsion of the pattern.\nDiscovery the new note continues the occurrence in the\nsame way that a previous note continued an older\noccurrence of the pattern: the pattern is extended\nwith this new common description, and the two oc-\ncurrences are extended as well.\nConcerning the discovery mechanism, the identiﬁcation\nof new notes continuing older contexts can be implemented\nusing a simple associative array, storing the note following\neach occurrence according to its description. This will be\ncalled a continuation memory . Before actually extending\nthe pattern, we should make sure that the extended pattern\nis closed.\n2.3 Speciﬁc Pattern Class\nSearching for all closed patterns in a sequence, instead of\nall possible patterns, enables an exhaustive pattern analysis\nwithout combinatorial explosion: all non-closed patternscan be deduced from the closed pattern analysis. Yet, the\nset of closed patterns can remain quite large and the ex-\nhaustive collection of their occurrences can become cum-\nbersome. [6] proposes to limit the analysis, without any\nloss of information, to closed patterns’ speciﬁc classes,\nwhich correspond to pattern occurrences that are not in-\ncluded in occurrences of more speciﬁc patterns. For in-\nstance, in Figure 3, the speciﬁc class of CD contains only\nits ﬁrst occurrence, because the two other ones are super-\nposed to occurrences of the more speciﬁc pattern ABCDE.\nWe propose a simpler model for the determination of\nspeciﬁc class of closed patterns. Non-speciﬁc occurrences\nare regenerated whenever necessary. Because occurrences\nof a given pattern are not all represented, the notes follow-\ning these occurrences are not memorised, although they\ncould generate new pattern extensions. To circumvent this\nissue, the extension memory related to any given pattern\ncontains the extensions not only of that pattern but also of\nany more speciﬁc pattern.\n3. MULTIPARAMETRIC PATTERN MINING\nThe model presented in the previous section searches for\nsequential patterns on monoparametric sequences, com-\nposed of a succession of symbols taken from a given al-\nphabet. Music cannot be reduced to unidimensional para-\nmetric description.\n3.1 Parametric space\nThe problem needs to be generalised by taking into account\nthree main aspects:\n\u000fNotes are deﬁned by a hierarchically structured com-\nbination of parameters (diatonic and chromatic pitch\nand pitch class, metrical position, etc.).\n\u000fNotes are deﬁned not only in terms of their absolute\nposition on ﬁxed scales, but also relatively to a given\nlocal context, and in particular with respect to the\nprevious notes (deﬁning pitch interval, gross con-\ntour, rhythmic values, etc.). These interval represen-\ntations are also hierarchically structured. Gross con-\ntour, for instance, is a simple description of the inter-\npitch interval between successive notes as “increas-\ning”, “decreasing” or ”unison”. Matching along gross\ncontour enables to track intervallic augmentation and\ndiminution. For instance, in the example in section\n5, the ﬁrst interval of the fugue subject is either a\ndecreasing third or a decreasing second. The actual\ndiatonic pitch interval representation differs, but the\ngross contour remains constantly “decreasing”.\n\u000fA large part of melodic transformations can be un-\nderstood as repetitions of sequential patterns that do\nnot follow strictly all the parametric descriptions, but\nonly a subset. For instance, a rhythmical variation\nof a melodic motif consists in repeating the pitch se-\nquence, while developing the rhythmical part more\nfreely.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n363[6] proposes to integrate both absolute note position\nand relative note interval into a single parametric space.\nThis enables to deﬁne a motive and any occurrence as a\nsimple succession of parametric descriptions. [6] also shows\nthe importance of heterogeneous patterns, which are made\nof a succession of parameters that can each be deﬁned on\ndifferent parametric dimensions. For instance, the subject\nof the fugue analysed in section 5 is heterogeneous, as it\nstarts with a gross contour interval followed by more spe-\nciﬁc descriptions. In the multiparametric paradigm, a pat-\nternGis more general than a pattern Sif it is a sufﬁx of\nSand/or the successive parametric descriptions of the pat-\nterns are equal or more general than the related parametric\ndescriptions in pattern P.\n3.2 Motivic/thematic class as “paradigmatic sheaf”\nExtending the exhaustive method developed in the previ-\nous section to this heterogeneous pattern paradigm enables\nto describe all possible sequential repetitions along all para-\nmetric dimensions. This leads to very detailed pattern char-\nacterisation, describing in details the common sequential\ndescriptions between any pair of similar motif. However, a\nmore synthetic analysis requires structuring the set of dis-\ncovered patterns into motivic or thematic classes. Manual\nmotivic taxonomy of these discovered patterns has been\nshown in [7].\nWe have conceived a method for the collection of all\npatterns belonging to a same motivic or thematic class.\nStarting from one pattern seed, the method collects all other\npatterns that can be partially aligned to the seed, as well as\nthose that can be aligned to any pattern thus collected. Pat-\nterns are searched along the following transformations:\n\u000fMore general patterns of same length\n\u000fMore speciﬁc patterns: only the sufﬁx that have same\nlength that the pattern seed is selected.\n\u000fPreﬁxes of pattern seed can be used as pattern seeds\ntoo: they might contain additional sets of more gen-\neral and more speciﬁc patterns of interest.\n\u000fPattern extensions, leading to a forking of the mo-\ntivic or thematic class into several possible continu-\nations\nAll the patterns contained in the bundle remain informative\nin the way they show particular commonalities between\nsubset of the motivic/thematic class, as shown in the anal-\nysis in section 5.\n3.3 Heterogeneous pattern mining\nA parametric description of a given note in the musical\nsequence instantiates values to all ﬁelds in the paramet-\nric space. Values in the more general ﬁelds are automat-\nically computed from their more speciﬁc ﬁelds. A para-\nmetric description of a note in a pattern instantiates values\nto some ﬁelds in the space, the other indeterminate ﬁelds\ncorresponding to undeﬁned parameters. Values can be as-\nsigned to more general ﬁelds, even if no value is assignedto their corresponding more speciﬁc ﬁelds. Methods have\nbeen implemented that enable to compare two parametric\ndescriptions, in order to see if they are equal, or if one is\nsubsumed into the other, and if not, to compute the inter-\nsection of the two descriptions.\nThe multiparametric description is integrated in the two\ncore mechanisms of the incremental pattern mining model\nas follows:\nRecognition As before, the observed parametric descrip-\ntion of the new note is compared to the descriptions\nof the patterns’ extensions. If the pattern extension’s\ndescription ﬁts only partially, a new more general\npattern extension is created (if not existing yet) re-\nlated to the common description.\nDiscovery The continuation memory is structured in the\nsame way as the parametric space: for each possi-\nble parametric ﬁeld, an associative memory stores\npattern continuations according to their values along\nthat particular parametric ﬁeld. As soon as a stored\npattern continuation is identiﬁed with the current note\nalong a particular parametric ﬁeld, the complete para-\nmetric description common to these two contexts is\ncomputed, and the pattern extension is attempted along\nthat common parametric description. As before, a\npattern is extended only if the extended pattern is\nclosed.\n4. PATTERN CYCLICITY\nA solution to the problem of cyclicity introduced in sec-\ntion 1.2 was proposed in [6] through the formalisation of\ncyclic patterns, where the last state of the chain represent-\ning the pattern is connected back to its ﬁrst state, formal-\nising this compelling expectation of the return of the pe-\nriodic pattern. One limitation of the approach is that it\nrequired the explicit construction of cyclic pattern, which\ndemanded contrived algorithmic formalisations. The prob-\nlem gets even more difﬁcult when dealing with multipara-\nmetric space, in particular when the pattern is only partially\nextended, i.e., when the expected parametric description is\nreplaced by a less speciﬁc parametric matching, such as\nin the musical example shown in Figure 4. In this case, a\nmore general pattern cyclic needs to be constructed, lead-\ning to the inference of a complex network of pattern cycles\nparticularly difﬁcult to conceptualise and implement.\nWe propose a simpler approach: instead of formalising\ncyclic patterns, pattern cyclicity is represented on the pat-\nternoccurrences directly. Once a successive repetition of a\npattern has been detected, such as the 3-note pattern start-\ning the musical example in Figure 4, the two occurrences\nare fused into one single chain of notes, and all the subse-\nquent notes in the cyclic sequence are progressively added\nto that chain. This cyclic chain is ﬁrst used to track the de-\nvelopment of the new cycle (i.e., the third cycle, since there\nwere already two cycles). The tracking of each new cy-\ncle is guided by a model describing the expected sequence\nof musical parameters. Initially, for the third cycle, this\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n364cycle 3 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s289/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.18.2—www.lilypond.orgGCEb+3+2m-5mGCEbG+3+2m-5m8th8th8th8th8th8thCEb+3+2m-8th8th8th--4CEb+3+2m-48th8th8th14710131415\ncycle 1Abcycle 2cycle 4cycle 5Figure 4. Two successive repetitions of a pattern, at the be-\nginning of the musical sequence, characterised by a pitch\nsequence (G, C, Eb, and back to G), a pitch interval se-\nquence (ascending perfect fourth (+3), ascending minor\nthird (+2m) and descending minor sixth (-5m)), and a\nrhythmical sequence made of a succession of 8th notes.\nThis successive repetition leads to the inference of a cyclic\nchain, indicated at the bottom of the ﬁgure. When this cy-\ncle is initially inferred, at note 7, the model of the cycle,\nrepresented above “cycle 3”, corresponds to the initial pat-\ntern description. At note 10, some descriptions expected\nby the model (indicated in bold italics) are not fulﬁlled, but\na more general description is inferred (descending gross\ncontour (-)). Consequently, the next cycle (4)’s model is\ngeneralised accordingly. At note 13, a new regularity is\ndetected, due to the repetition of pitch Ab and of descend-\ning perfect ﬁfth (-4). Consequently, the next cycle (5)’s\nmodel is specialised accordingly.\nmodel corresponds to the pattern that was repeated twice\nin the two ﬁrst cycles.\n\u000fIf the new cycle scrupulously follows the model, this\nsame model will be used to guide the development\nof the subsequent cycle.\n\u000fIf the new cycle partially follows the model (such as\nthe modiﬁcation, at the beginning of bar 2 in Fig-\nure 4, of the decreasing sixth interval, replaced by a\nmore general decreasing contour), the model is up-\ndated accordingly by replacing the parameters that\nhave not been matched with more general parame-\nters.\n\u000fIf the new cycle shows any new pattern identiﬁca-\ntion with the previous cycle (such as the repetition\nof pitch Ab at the beginning of cycles 4 and 5 in Fig-\nure 4), the corresponding descriptions are added to\nthe model.\n\u000fIf at some point, the new note does not match at\nall the corresponding description in the model, the\ncyclic sequence is terminated.\nThis simple method enables to track the cyclic develop-\nment of repeated patterns, while avoiding the combinato-\nrial explosion inherent to this structural conﬁguration.\n5. TESTS\nThe model described in this paper is applied to the anal-\nysis of the Johannes Kepler University Patterns Develop-ment Database (JKUPDD-Aug2013), which is the train-\ning set part of the MIREX task on Discovery of Repeated\nThemes & Sections initiated in 2013, and made publicly\navailable, both symbolic representation of the scores and\nground-truth musicological analyses [4].\nThis section details the analysis of one particular piece\nof music included in the JKUPDD, the 20th Fugue in the\nSecond Book of Johann Sebastian Bach’s Well-Tempered\nClavier. The ground truth consists of the two ﬁrst bars\nof the third entry in the exposition part along the three\nvoices that constitute this fugue [1]. The third entry is\nchosen because it is the ﬁrst entry where the subject and\nthe two countersubjects are exposed altogether. To each\nof these three ground-truth patterns (the subject and the\ntwo countersubjects in this two-bar entry), the ground-truth\ndata speciﬁes a list of occurrences in the score.\nFigure 5 shows the thematic class related to ground-\ntruth pattern #1, i.e., the fugue’s subject. This is detected\nby the model as one single motivic/thematic class, i.e., one\ncomplete paradigmatic sheaf, resulting from the bundling\nmethod presented in section 3.2. All occurrences indicated\nin the ground truth are retrieved. The patterns forming\nthis thematic class are longer than the two-bar motif indi-\ncated in the ground truth. The limitation of all subjects and\ncounter-subjects in the musicological analysis to two bars\nstems from a theoretical understanding of fugue structure\nthat cannot be automatically inferred from a direct analysis\nof the score.\nThe analysis offered by the computational model of-\nfers much richer information than simply listing the occur-\nrences of the subjects and countersubjects. It shows what\nmusical descriptions characterise them, and details partic-\nular commonalities shared by occurrences of these subjects\nand countersubjects. For instance entries M1 and U1 be-\nlong to a same more speciﬁc pattern that describes their\nparticular development. L1, U1 and U3 start all with a de-\ncreasing third interval, and so on.\nThe model presented in this paper does not yet inte-\ngrate mechanisms for the reduction of ornamentation, as\ndiscussed in the next section. The only melodic ornamen-\ntation appearing in pattern #1 is the addition of a passing\nnote after the ﬁrst note of occurrences L2 and L3. This\nleads to a small error in the model’s results, where the ﬁrst\nactual note is not detected.\nThe thematic class related to ground-truth pattern #2,\nwhich is the ﬁrst countersubject, is extracted in the same\nway, forming a paradigmatic sheaf. The pattern class given\nby the model corresponds mostly to the ground truth. Here\nagain, some occurrences present similar extensions that are\ninventoried by the model, although they are ignored in the\nground truth. The last occurrence, which is a sufﬁx of the\npattern, is also detected accordingly. On the other hand,\nthe second last occurrence is not properly detected, once\nagain due to the addition of passing notes.\nPattern #3, which is the second countersubject, is more\nproblematic, because it is only 7 notes long. Several other\nlonger patterns are found by the model, and the speciﬁcity\nof pattern #3 is not grounded on characteristics purely re-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n365 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2L1/timesig.C44/clefs.F/noteheads.s2/rests.3/rests.2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/rests.2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/clefs.G/timesig.C44M1/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/rests.2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/clefs.G/timesig.C44U1/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/rests.3/flags.d3/dots.dot/noteheads.s2/flags.d3/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2L2/timesig.C44/rests.2/noteheads.s2/clefs.F/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/rests.2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/rests.2/clefs.G/timesig.C44U2/noteheads.s2/accidentals.sharp/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/rests.3/noteheads.s2M2/timesig.C44/clefs.G/rests.2/noteheads.s2/rests.3/rests.2/accidentals.sharp/flags.d3/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flat/noteheads.s2/noteheads.s2/noteheads.s2U3/timesig.C44/clefs.G/rests.2/accidentals.sharp/noteheads.s2/noteheads.s2/rests.3/rests.2/flags.u5/noteheads.s2/flags.u5/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/rests.3/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/clefs.F/timesig.C44L3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.18.2—www.lilypond.orgFigure 5. Entries of the subject in Bach’s Fugue, as found\nby the model. The fugue has three voices: upper (U), mid-\ndle (M) and lower (L). In each entry is slurred the part\nactually indicated in the ground-truth description of the\nsubject. The model proposes a longer description of the\nsubject, that is particularly developed in M1 and U1.\nlated to pattern repetition. As aforementioned, the ground-\ntruth selection of these three patterns are based on prin-\nciples related to fugue rules, namely the synchronised it-\neration of the three patterns along the separate voices. It\nseems questionable to expect a general pattern mining al-\ngorithm non-specialised to a particular type of music to be\nable to infer this type of conﬁguration.\n6. CONCLUSION\nThe approach is incremental, progressively analysing the\nmusical sequence through one single pass. This enables\nto control the structural complexity in a way similar to the\nway listeners perceive music.\nGross contour needs to be constrained by factors re-\nlated to local saliency and short-term memory. The integra-\ntion of more complex melodic transformation such as or-\nnamentation and reduction is currently under investigation.\nMotivic repetition with local ornamentation is detected by\nreconstructing, on top of “surface-level” monodic voices,\nlonger-term relations between non-adjacent notes related\nto deeper structures, and by tracking motives on the result-\ning syntagmatic network. More generally, the analysis ofpolyphony is under study, as well as the application of the\npattern mining approach to metrical analysis. The system,\nimplemented in Matlab, is made publicly available as part\nofMiningSuite3, a new open-source framework for audio\nand music analysis.\n7. ACKNOWLEDGMENTS\nThis work was funded by an Academy of Finland research\nfellowship at the Finnish Centre of Excellence in Interdis-\nciplinary Music Research at the University of Jyv ¨askyl ¨a.\nThe research is continued in the context of the European\nproject Learning to Create (Lrn2Cre8), which acknowl-\nedges the ﬁnancial support of the Future and Emerging\nTechnologies (FET) programme within the Seventh Frame-\nwork Programme for Research of the European Commis-\nsion, under FET grant number 610859.\n8. REFERENCES\n[1] S. Bruhn. J.S. Bach’s Well-Tempered Clavier: in-\ndepth analysis and interpretation, Mainer Interna-\ntional, Hong Kong, 1993.\n[2] E. Cambouropoulos. Towards a General Computa-\ntional Theory of Musical Structure, PhD thesis, Uni-\nversity of Edinburgh, 1998.\n[3] E. Cambouropoulos. “Musical parallelism and melodic\nsegmentation: A computational approach,” Music Per-\nception, 23(3), pp. 249–268, 2006.\n[4] T. Collins. MIREX 2013: Discovery of Repeated\nThemes and Sections, 2013. http://www.music-\nir.org/mirex/wiki/2013:Discovery ofRepeated Themes\n&Sections Accessed on 14 August 2014.\n[5] D. Conklin, and C. Anagnostopoulou. “Representation\nand Discovery of Multiple Viewpoint Patterns,” Pro-\nceedings of the International Computer Music Confer-\nence, 2001.\n[6] O. Lartillot. “Efﬁcient Extraction of Closed Motivic\nPatterns in Multi-Dimensional Symbolic Representa-\ntions of Music,” Proceedings of the International Sym-\nposium on Music Information Retrieval, 2005.\n[7] O. Lartillot. “Taxonomic categorisation of motivic\npatterns,” Musicae Scientiae, Discussion Forum 4B,\npp. 25–46, 2009.\n[8] D. Meredith, K., Lemstr ¨om, and G. Wiggins. “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Journal of\nNew Music Research, 31(4), pp. 321–345, 2002.\n[9] J. Wang, J. Han, and C. Li. “Frequent closed sequence\nmining without candidate maintenance,” IEEE Trans-\nactions on Knowledge and Data Engineering, 19:8,\npp. 1042–1056, 2007.\n3Available at http://code.google.com/p/miningsuite/.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n366"
    },
    {
        "title": "Codebook-based Scalable Music Tagging with Poisson Matrix Factorization.",
        "author": [
            "Dawen Liang",
            "John W. Paisley",
            "Dan Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416120",
        "url": "https://doi.org/10.5281/zenodo.1416120",
        "ee": "https://zenodo.org/records/1416120/files/LiangPE14.pdf",
        "abstract": "Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a “bag-of-tags” representation. This ap- proach exploits the shared latent structure between seman- tic tags and acoustic codewords. Leveraging the recently- developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used.",
        "zenodo_id": 1416120,
        "dblp_key": "conf/ismir/LiangPE14",
        "keywords": [
            "Automatic",
            "music",
            "tagging",
            "MIR",
            "matrix",
            "completion",
            "Poisson",
            "factorization",
            "latent",
            "structure"
        ],
        "content": "CODEBOOK-BASED SCALABLE MUSIC TAGGING WITH\nPOISSON MATRIX FACTORIZATION\nDawen Liang, John Paisley, Daniel P. W. Ellis\nDepartment of Electrical Engineering\nColumbia University\nfdliang, dpweg@ee.columbia.edu, jpaisley@columbia.edu\nABSTRACT\nAutomatic music tagging is an important but challenging\nproblem within MIR. In this paper, we treat music tagging\nas a matrix completion problem. We apply the Poisson\nmatrix factorization model jointly on the vector-quantized\naudio features and a “bag-of-tags” representation. This ap-\nproach exploits the shared latent structure between seman-\ntic tags and acoustic codewords. Leveraging the recently-\ndeveloped technique of stochastic variational inference, the\nmodel can tractably analyze massive music collections. We\npresent experimental results on the CAL500 dataset and\nthe Million Song Dataset for both annotation and retrieval\ntasks, illustrating the steady improvement in performance\nas more data is used.\n1. INTRODUCTION\nAutomatic music tagging is the task of analyzing the audio\ncontent (waveform) of a music recording and assigning to\nit human-relevant semantic tags [16] – which may relate to\nstyle, genre, instrumentation, or more subtle aspects of the\nmusic, such as those contributed by users on social media\nsites. Such “autotagging” [5] relies on labelled training\nexamples for each tag, and performance typically improves\nwith the number of training examples consumed, although\ntraining schemes also take longer to complete. In the era\nof “Big Data”, it is necessary to develop models which can\nrapidly handle massive amount of data; a starting point for\nmusic data is the Million Song Dataset [2], which includes\nuser tags from Last.fm [1].\nIn this paper, we treat the automatic music tagging as\na matrix completion problem, and use the techniques of\nstochastic variational inference to be able to learn from\nlarge amounts of data presented in an online fashion [9].\nThe “matrix completion” problem treats each track as a\nrow in a matrix, where the elements describe both the acous-\ntic properties (represented, for instance, as a histogram of\noccurrences of vector-quantized acoustic features) and the\nrelevance of a large vocabulary of tags: We can regard the\nc\rDawen Liang, John Paisley, Daniel P. W. Ellis.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Dawen Liang, John Paisley, Daniel\nP. W. Ellis. “Codebook-based scalable music tagging with\nPoisson matrix factorization”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.tag information as incomplete or missing for some of the\nrows, and seek to “complete” these rows based on infor-\nmation inferred from the complete, present rows.\n1.1 Related work\nThere have been a large number of papers on automatic\ntagging of music audio in recent years. In addition to the\npapers mentioned above, work particularly relevant to this\npaper includes the Codeword Bernoulli Average (CBA) ap-\nproach of Hoffman et al. [7], which uses a similar VQ his-\ntogram representation of the audio to build a simple but\neffective probabilistic model for each tag in a discrimina-\ntive fashion. Xie et al. [17] directly ﬁts a regularized logis-\ntic regression model to the normalized acoustic codeword\nhistograms to predict each tag and achieves state-of-the-art\nresults, and Ellis et al. [6] further improves tagging accu-\nracy by employing multiple generative models that capture\ndifferent characteristics of a music piece, which are com-\nbined in an optimized “bag-of-systems”.\nMuch of the previous work has been performed on the\nCAL500 dataset [16] of 502 Western popular music tracks\nthat were carefully labelled by at least three human an-\nnotators with their relevance to 149 distinct labels span-\nning instrumentation, genre, emotions, vocal characteris-\ntics, and use cases. This small dataset tends to reward ap-\nproaches that can maximize the information extracted from\nthe sparse data regardless of the computational cost. A rel-\natively larger dataset in this domain is CAL10k [15] with\nover 10,000 tracks described by over 500 tags, mined from\nPandora’s website1. However, neither of these datasets\ncan be considered industrial scale, which implies handling\nmillions of tracks and tens of thousands of tags.\nMatrix factorization techniques, in particular, nonnega-\ntive matrix factorization (NMF), have been widely used to\nanalyze music signals [8, 11] in the context of source sep-\naration. Paisley et al. [12] derived scalable Bayesian NMF\nfor topic modeling, which we develop here. To our knowl-\nedge, this is the ﬁrst application of matrix factorization to\nVQ acoustic features for automatic music tagging.\n2. DATA REPRESENTATION\nFor our automatic tagging system, the data comes from\ntwo sources: vector-quantized audio features and a “bag-\n1http://www.pandora.com/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n167of-tags” representation.\n\u000fVector-quantized audio features Instead of directly\nworking with audio features, we vector quantize all\nthe features following the standard procedure: We\nrun the K-means algorithm on a subset of randomly\nselected training data to learn Jcluster centroids\n(codewords). Then for each song, we assign each\nframe to the cluster with the smallest Euclidean dis-\ntance to the centroid. We form the VQ feature yVQ2\nNJby counting the number of assignments to each\ncluster across the entire song.\n\u000fBag-of-tags Similar to the bag-of-words represen-\ntation, which is commonly used to represent docu-\nments, we represent the tagging information (whether\nor not the tag applies to a song) with a binary bag-\nof-tags vector yBoT2f0; 1gjVj, whereVis the set\nof all tags.\nFor each song, we will simply concatenate the VQ fea-\ntureyVQand the bag-of-tags vector yBoT, thus the dimen-\nsion of the data is D=J+jVj. When we apply the matrix\nfactorization model to the data, the latent factors we learn\nwill exploit the shared latent structure between semantic\ntags and acoustic codewords. Therefore, we can utilize the\nshared latent structure to predict tags when only given the\naudio features.\n3. POISSON MATRIX FACTORIZATION\nWe adopt the notational convention that bold letters (e.g.\ny;\u0012;\f) denote matrices. i2f1;\u0001\u0001\u0001;Igis used to index\nsongs.d2f1;\u0001\u0001\u0001;Dgis used to index feature dimen-\nsions.k2f1;\u0001\u0001\u0001;Kgis used to index latent factors from\nthe matrix factorization model. Given the data y2NI\u0002D\nas described in Section 2, the Poisson matrix factorization\nmodel is formulated as follows:\n\u0012ik\u0018Gamma(a;ac);\n\fkd\u0018Gamma(b;b);\nyid\u0018Poisson(PK\nk=1\u0012ik\fkd);(1)\nwhere\fk2RD\n+denote thekth latent factors and \u0012i2RK\n+\ndenote the weights for song i.aandbare model hyper-\nparameters. cis a scalar on the weights that we tune to\nmaximize the likelihood.\nThere are a couple of reasons to choose a Poisson model\nover a more traditional Gaussian model [14]. First, the\nPoisson distribution is a more natural choice to model count\ndata. Secondly, real-world tagging data is extremely noisy\nand sparse. If a tag is not associated with a song in the\ndata, it could be either because that tag does not apply to\nthe song, or simply because no one has labelled the song\nwith the tag yet. The Poisson matrix factorization model\nhas the desirable property that it does not penalize values\nof0as strongly as the Gaussian distribution [12]. There-\nfore, even weakly labelled data can be used to learn the\nPoisson model.4. VARIATIONAL INFERENCE\nTo learn the latent factors \fand the corresponding decom-\nposition weights \u0012from the training data y, we need to\ncompute the posterior distribution p(\u0012;\fjy). However, no\nclosed-form expression exists for this hierarchical model.\nWe therefore employ mean-ﬁeld variational inference to\napproximate this posterior [10].\nThe basic idea behind mean-ﬁeld variational inference\nis to choose a factorized family of variational distributions,\nq(\u0012;\f) =KY\nk=1\u0010IY\ni=1q(\u0012ik)\u0011\u0010DY\nd=1q(\fkd)\u0011\n; (2)\nto approximate the posterior p(\u0012;\fjy), so that the Kullback-\nLeibler (KL) divergence between the variational distribu-\ntion and the true posterior is minimized. Following a fur-\nther approximation discussed in the next section, the fac-\ntorized distribution allows for a closed-form expression of\nthis variational objective, and thus tractable inference. Here\nwe choose variational distributions from the same family\nas the prior:\nq(\u0012ik) =Gamma(\u0012 ik;\rik;\u001fik);\nq(\fkd) =Gamma(\f kd;\u0017kd;\u0015kd):(3)\nMinimizing the KL divergence is equivalent to maximizing\nthe following variational objective:\nL=Eq[lnp(y;\u0012;\f)] +H(q); (4)\nwhereH(q)is the entropy of the variational distribution\nq. We can optimize the variational objective using coor-\ndinate ascent via two approaches: batch inference, which\nrequires processing of the entire dataset for every iteration;\nor stochastic inference, which only needs a small batch of\ndata for each iteration and can be potentially scale to much\nlarger datasets where batch inference is no longer compu-\ntationally feasible.\n4.1 Batch inference\nAlthough the model in Equation (1) is not conditionally\nconjugate by itself, as demonstrated in [4] we can intro-\nduce latent random variables zidk\u0018Poisson(\u0012 ik\fkd)with\nthe variational distribution being q(zidk) =Multi(zid;\u001eid),\nwherezid2NK,\u001eidk\u00150andP\nk\u001eidk= 1. This\nmakes the model conditionally conjugate, which means\nthat closed-form coordinate ascent updates are available.\nFollowing the standard results of variational inference\nfor conditionally conjugate model (e.g. [9]), we can obtain\nthe updates for \u0012ik:\n\rik=a+DX\nd=1yid\u001eidk;\n\u001fik=ac+DX\nd=1Eq[\fkd]:(5)\nThe scalecis updated as c\u00001=1\nIKP\ni;kEq[\u0012ik].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n168Similarly, we can obtain the updates for \fkd:\n\u0017kd=b+IX\ni=1yid\u001eidk;\n\u0015kd=b+IX\ni=1Eq[\u0012ik]:(6)\nFinally, for the latent variables zidk, the following update\nis applied:\n\u001eidk/expfEq[ln\u0012ik\fkd]g: (7)\nThe necessary expectations for \u0012ikare:\nEq[\u0012ik] =\rik=\u001fik;\nEq[ln\u0012ik] = (\rik)\u0000ln\u001fik;(8)\nwhere (\u0001)is the digamma function. The expectations for\n\fkdhave the same form, but use \u0017kdand\u0015kd.\n4.2 Stochastic inference\nBatch inference will alternate between updating \u0012and\f\nusing the entire data at each iteration until convergence to\na local optimum, which could be computationally inten-\nsive for large datasets. We can instead adopt stochastic\noptimization by selecting a subset (mini-batch) of the data\nat iterationt, indexed by Bt\u001af1;\u0001\u0001\u0001;Ig, and optimizing\nover a noisy version of the variational objective L:\nLt=I\njBtjX\ni2B tEq[lnp(yi;\u0012ij\f)] +Eq[lnp(\f)] +H(q):\n(9)\nBy optimizingLt, we are optimizing Lin expectation.\nThe updates for weights \u0012ikand latent variables zidkare\nessentially the same as batch inference, except that now we\nare only inferring weights for the mini-batch of data for\ni2Bt. The optimal scale cis updated accordingly:\nc\u00001=1\njBtjKX\ni2B t;kEq[\u0012ik]: (10)\nAfter alternating between updating weights \u0012ikand la-\ntent variables zidkuntil convergence, we can take a gradi-\nent step, preconditioned by the inverse Fisher information\nmatrix of variational distribution q(\fkd), to optimize \fkd\n(see [9] for more technical details),\n\u0017(t)\nkd= (1\u0000\u001at)\u0017(t\u00001)\nkd+\u001at\u0012\nb+I\njBtjX\ni2B tyid\u001eidk\u0013\n;\n\u0015(t)\nkd= (1\u0000\u001at)\u0015(t\u00001)\nkd+\u001at\u0012\nb+I\njBtjX\ni2B tEq[\u0012ik]\u0013\n;\n(11)\nwhere\u001at>0is a step size at iteration t. To ensure conver-\ngence [3], the following conditions must be satisﬁed:\nP1\nt=1\u001at=1;P1\nt=1\u001a2\nt<1: (12)\nOne possible choice of \u001atis\u001at= (t0+t)\u0000\u0014fort0>0\nand\u00142(0:5; 1]. It has been shown [9] that this update\ncorresponds to stochastic optimization with a natural gra-\ndient step, which better ﬁts the geometry of the parameter\nspace for probability distributions.4.3 Generalizing to new songs\nOnce the latent factor \f2RK\u0002D\n+ is inferred, we can natu-\nrally divide it into two blocks: the VQ part \fVQ2RK\u0002J\n+ ,\nand the bag-of-tags part \fBoT2RK\u0002jVj\n+ .\nGiven a new song, we can ﬁrst obtain the VQ feature\nyVQand ﬁt it with\fVQto compute posterior of the weights\np(\u0012jyVQ;\fVQ). We can approximate this posterior with the\nvariational inference algorithm in Section 4.1 with \fﬁxed.\nThen to predict tags, we can compute the expectation of\nthe dot product between the weights \u0012and\fBoTunder the\nvariational distribution:\n^yBoT=Eq[\u0012T\fBoT]: (13)\nSince for different songs the weights \u0012may be scaled dif-\nferently, before computing the dot product we normalize\nEq[\u0012]so that it lives on the probability simplex. To do au-\ntomatic tagging, we could annotate the song with top M\ntags according to ^yBoT. To compensate for a lack of diver-\nsity in the annotations, we adopt the same heuristic used\nin [7] by introducing a “diversity factor” d: For each pre-\ndicted score, we subtract dtimes the mean score for that\ntag. In our system, we set d= 3.\n5. EVALUATION\nWe evaluate the model’s performance on an annotation task\nand a retrieval task using CAL500 [16] and Million Song\nDataset (MSD) [2]. Unlike the CAL500 dataset where\ntracks are carefully-annotated, the Last.fm dataset [1] asso-\nciated with MSD comes from real-world user tagging, and\nthus contains only weakly labelled data with a tagging vo-\ncabulary that is much larger and more diverse. We compare\nour results on these tasks with two other sets of codebook-\nbased methods: Codeword Bernoulli Average (CBA) [7]\nand`2regularized logistic regression [17]. Like the Pois-\nson matrix factorization model, both methods are easy to\ntrain and can scale to relatively large dataset on a single\nmachine. However, since both methods perform optimiza-\ntion in a batch fashion, we will later refer to them as “batch\nalgorithms”, along with the Poisson model with batch in-\nference described in Section 4.1.\nFor the hyperparameters of the Poisson matrix factor-\nization model, we set a=b= 0:1, and the number of\nlatent factors K= 100 . To learn the latent factors \f, we\nfollowed the procedure in Section 4.1 for batch inference\nuntil the relative increase on the variational objective is less\nthan0:05%. For stochastic inference, we used a mini-batch\nsizejBtj= 1000 unless otherwise speciﬁed and took a full\npass of the randomly permuted data. As for the learning\nrate, we set t0= 1 and\u0014= 0:6. All the source code in\nPython is available online2.\n5.1 Annotation task\nThe purpose of annotation task is to automatically tag un-\nlabelled songs. To evaluate the model’s ability for anno-\ntation, we computed the average per-tag precision, recall,\n2http://github.com/dawenl/stochastic_PMF\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n169and F-score on a test set. Per-tag precision is deﬁned as\nthe average fraction of songs that the model annotates with\ntagvthat are actually labelled v. Per-tag recall is deﬁned\nas the average fraction of songs that are actually labelled\nvthat the model also annotates with tag v. F-score is the\nharmonic mean of precision and recall, and is one overall\nmetric for annotation performance.\n5.2 Retrieval task\nThe purpose of the retrieval task is, when given a query tag\nv, to provide a list of songs which are related to tag v. To\nevaluate the models’ retrieval performance, for each tag in\nthe vocabulary we ranked each song in the test set by the\npredicted score from Equation (13). We evaluated the area\nunder the receiver-operator curve (AROC) and mean aver-\nage precision (MAP) for each ranking. AROC is deﬁned\nas the area under the curve, which plots the true positive\nrate against the false positive rate, and MAP is deﬁned as\nthe mean of the average precision (AP) for each tag, which\nis the average of the precisions at each possible level of\nrecall.\n5.3 Results on CAL500\nFollowing the procedure similar to that described in [7,\n17], we performed a 5-fold cross-validation to evaluate\nthe annotation and retrieval performance on CAL500. We\nselected the top 78 tags, which are annotated more than\n50 times in the dataset, and learned a codebook of size\nJ= 2000. For the annotation task, we labelled each song\nwith the top 10 tags based on the predicted score. Since\nCAL500 is a relatively small dataset, we only performed\nbatch inference for Poisson matrix factorization model.\nThe results are reported in Table 1, which shows that\nthe Poisson model has comparable performance on the an-\nnotation task, and does slightly worse on the retrieval task.\nAs mentioned in Section 3, the Poisson matrix factoriza-\ntion model is particularly suitable for noisy and sparse data\nwhere 0’s are not necessarily interpreted as explicit obser-\nvations. However, this may not be the case for CAL500, as\nthe vocabulary was well-chosen and the data was collected\nfrom a survey where the tagging quality is understandably\nhigher than the actual tagging data in the real world, like\nthe one from Last.fm. Therefore, this task cannot fully ex-\nploit the advantage brought by the Poisson model. Mean-\nwhile, the amount of data in CAL500 is fairly small – the\ndatayﬁt to the model is simply a 502-by-2078 matrix.\nThis prevents us from adopting stochastic inference, which\nwill be shown being much more effective than batch infer-\nence even on a 10,000-song dataset in Section 5.4.\n5.4 Results on MSD\nTo demonstrate the scalability of the Poisson matrix factor-\nization model, we conducted experiments using MSD and\nthe associated Last.fm dataset. To our knowledge, there\nhas not been any previous work where music tagging re-\nsults are reported on the MSD.Model Prec Recall F-score AROC MAP\nCBA 0.41 0.24 0.29 0.69 0.47\n`2LogRegr 0.48 0.26 0.34 0.72 0.50\nPMF-Batch 0.42 0.23 0.30 0.67 0.45\nTable 1. Results for the top 78 popular tags on CAL500,\nfor Codeword Bernoulli Average (CBA), `2regularized lo-\ngistic regression (` 2LogRegr), and Poisson matrix factor-\nization with batch inference (PMF-Batch). The results for\nCBA and`2LogRegr are directly copied from [17].\nSince the Last.fm dataset contains 522,366 unique tags,\nit is not realistic to build the model with all of them. We\nﬁrst selected the tags with more than 1,000 appearances\nand removed those which do not carry discriminative in-\nformation (e.g. “my favorite”, “awesome”, “seen live”,\netc.). Then we ran the stemming algorithm implemented\ninNLTK3to further reduce the potential duplications and\ncorrect for alternate spellings (e.g. “pop-rock” v.s. “pop\nrock”, “love song” v.s. “love songs”), which gave us a vo-\ncabulary of 561 tags. Using the default train/test artist split\nfrom MSD, we ﬁltered out the songs which have been la-\nbelled with tags from the selected vocabulary. This gave us\n371,209 songs for training. For test set, we further selected\nthose which have at least 20 tags (otherwise, it is likely that\nthis song is very weakly labelled). This gave us a test set of\n2,757 songs. The feature we used is the Echo Nest’s timbre\nfeature, which is very similar to MFCC.\nWe randomly selected 10,000 songs as the data which\ncan ﬁt into the memory nicely for all the batch algorithms,\nand trained the following models with different codebook\nsizesJ2 f256; 512;1024; 2048g: Codeword Bernoulli\nAverage (CBA), `2regularized logistic regression (` 2Lo-\ngRegr), Poisson matrix factorization with batch inference\n(PMF-Batch) and stochastic inference by a single pass of\nthe data (PMF-Stoc-10K ). Here we used batch size jBtj=\n500 for PMF-Stoc-10K , as otherwise there will only be\n10 mini-batches from the subset. However, given enough\ndata, in general larger batch size will lead to relatively su-\nperior performance, since the variance of the noisy varia-\ntional objective in Equation (9) is smaller. To demonstrate\nthe effectiveness of the Poisson model on massive amount\nof data (exploiting the stochastic algorithm’s ability to run\nwithout loading the entire dataset into memory), we also\ntrained the model with the full training set with stochas-\ntic inference (PMF-Stoc-full). For the annotation task, we\nlabelled each song with the top 20 tags based on the pre-\ndicted score.\nThe results are reported in Table 2. In general, the\nperformance of Poisson matrix factorization is compara-\nbly better for smaller codebook size J. Speciﬁcally, for\nstochastic inference, even if the amount of training data is\nrelatively small, it is not only signiﬁcantly faster than batch\ninference, but can also help improve the performance by\nquite a large margin. Finally, not surprisingly, PMF-Stoc-\nfull dominates all the metrics, regardless of the size of the\ncodebook, because it is able to learn from more data.\n3http://www.nltk.org/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n170Codebook size Model Precision Recall F-score AROC MAP\nJ= 256CBA 0.112 (0.007) 0.121 (0.008) 0.116 0.695 (0.005) 0.112 (0.006)\n`2LogRegr 0.091 (0.008) 0.093 (0.006) 0.092 0.692 (0.005) 0.110 (0.006)\nPMF-Batch 0.113 (0.007) 0.105 (0.006) 0.109 0.647 (0.005) 0.094 (0.005)\nPMF-Stoc-10K 0.116 (0.007) 0.127 (0.007) 0.121 0.682 (0.005) 0.105 (0.006)\nPMF-Stoc-full 0.127 (0.008) 0.143 (0.008) 0.134 0.704 (0.005) 0.115 (0.006)\nJ= 512CBA 0.120 (0.007) 0.127 (0.008) 0.124 0.689 (0.005) 0.117 (0.006)\n`2LogRegr 0.096 (0.008) 0.108 (0.007) 0.101 0.693 (0.005) 0.113 (0.006)\nPMF-Batch 0.111 (0.007) 0.108 (0.006) 0.109 0.645 (0.005) 0.098 (0.005)\nPMF-Stoc-10K 0.112 (0.007) 0.128 (0.007) 0.120 0.687 (0.005) 0.110 (0.006)\nPMF-Stoc-full 0.130 (0.008) 0.154 (0.008) 0.141 0.715 (0.005) 0.122 (0.006)\nJ= 1024CBA 0.118 (0.007) 0.126 (0.007) 0.122 0.692 (0.005) 0.117 (0.006)\n`2LogRegr 0.113 (0.008) 0.129 (0.008) 0.120 0.698 (0.005) 0.115 (0.006\nPMF-Batch 0.112 (0.007) 0.109 (0.006) 0.111 0.635 (0.005) 0.098 (0.006)\nPMF-Stoc-10K 0.111 (0.007) 0.127 (0.007) 0.118 0.687 (0.005) 0.111 (0.006)\nPMF-Stoc-full 0.127 (0.008) 0.146 (0.008) 0.136 0.712 (0.005) 0.120 (0.006)\nJ= 2048CBA 0.124 (0.007) 0.129 (0.007) 0.127 0.689 (0.005) 0.117 (0.006)\n`2LogRegr 0.115 (0.008) 0.137 (0.008) 0.125 0.698 (0.005) 0.118 (0.006)\nPMF-Batch 0.109 (0.007) 0.110 (0.006) 0.110 0.637 (0.005) 0.098 (0.006)\nPMF-Stoc-10K 0.107 (0.007) 0.124 (0.007) 0.115 0.682 (0.005) 0.106 (0.006)\nPMF-Stoc-full 0.120 (0.007) 0.147 (0.008) 0.132 0.712 (0.005) 0.118 (0.006)\nTable 2. Annotation (evaluated using precision, recall, and F-score) and retrieval (evaluated using area under the receiver-\noperator curve (AROC) and mean average precision (MAP)) performance on the Million Song Dataset with various code-\nbook sizes, from Codeword Bernoulli Average (CBA), `2regularized logistic regression (` 2LogRegr), Poisson matrix\nfactorization with batch inference (PMF-Batch) and stochastic inference by a single pass of the subset (PMF-Stoc-10K )\nand full data (PMF-Stoc-full). One standard error is reported in the parenthesis.\n0 50 100 150 200 250 300 350 400\n# batches0.060.070.080.090.100.110.120.130.140.15F-Score\n0 50 100 150 200 250 300 350 400\n# batches0.560.580.600.620.640.660.680.700.72AROC\n0 50 100 150 200 250 300 350 400\n# batches0.070.080.090.100.110.120.13AP\nFigure 1. Improvement in performance with the number of mini-batches consumed for the PMF-Stoc-full system with\nJ= 512. Red lines indicate the performance of PMF-Batch which is trained on 10k examples; that system’s performance\nis exceeded after fewer than 5 mini-batches.\nFigure 1 illustrates how the metrics improve as more\ndata becomes available for the Poisson matrix factoriza-\ntion model, showing how the F-score, AROC, and MAP\nimprove with the number of (1000-element) mini-batches\nconsumed up to the entire 371k training set. We see that\ninitial growth is rapid, thanks to the natural gradient, with\nmuch of the beneﬁt obtained after just 50 batches. How-\never, we see continued improvement beyond this; it is rea-\nsonable to believe that if more data becomes available, the\nperformance can be further improved.\nTable 3 contains information on the qualitative perfor-\nmance of our model. The tagging model works by captur-\ning correlations between semantic tags and acoustic code-\nwords in each latent factor \fk. As discussed, when a new\nsong arrives with missing tag information, only the portion\nof\fkcorresponding to acoustic codewords is used, and the\nsemantic tag portion of \fkis used to make predictions of\nthe missing tags. Similar to related topic models [9], wecan therefore look at the highly probable tags for each \fk\nto understand what portion of the acoustic codeword space\nis being captured by that factor, and whether it is musically\ncoherent. We show an example of this in Table 3, where\nwe list the top 7 tags from 9 latent factors \fklearned by\nour model with J= 512. We sort the tags according to ex-\npected relevance under the variational distribution Eq[\fkd].\nThis shows which tags are considered to have high proba-\nbility for a song that has the given factor expressed. As is\nevident, each factor corresponds to a particular aspect of a\nmusic genre. We note that other factors contained similarly\ncoherent tag information.\n6. DISCUSSION AND FUTURE WORK\nWe present a codebook-based scalable music tagging model\nwith Poisson matrix factorization. The system learns the\njoint behavior of acoustic features and semantic tags, which\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n171“Pop” “Indie” “Jazz” “Classical” “Metal” “Reggae” “Electronic” “Experimental” “Country”\npop indie chillout piano metal reggae house instrumental country\nfemale vocal rock lounge instrumental death metal funk electro ambient classic country\ndance alternative chill ambient thrash metal funky electronic experimental male vocal\nelectronic indie rock downtempo classic brutal death metal dance dance electronic blues\nsexy post punk smooth jazz beautiful grindcore hip-hop electric house psychedelic folk\nlove psychedelic relax chillout heavy metal party techno progressive love songs\nsynth pop new wave ambient relax black metal sexy minimal rock americana\nTable 3. Top 7 tags from 9 latent factors for PMF-Stoc-full with J= 512. For each factor, we assign the closest music\ngenre on top. As is evident, each factor corresponds to a particular aspect of a music genre.\ncan be used to infer the most appropriate tags given the au-\ndio alone. The Poisson model is naturally less sensitive to\nzero values than some alternatives, making it a good match\nto “noisy” training examples derived from real users’ tag-\ngings, where the fact that no user has applied a tag does\nnot necessarily imply that the term is irrelevant. By learn-\ning this model using stochastic variational inference, we\nare able to efﬁciently exploit much larger training sets than\nare tractable using batch approaches, making it feasible to\nlearn from an entire set of over 370k tagged examples. Al-\nthough much of the improvement comes in the earlier it-\nerations, we see continued improvement implying this ap-\nproach can beneﬁt from much larger, effectively unlimited\nsources of tagged examples, as might be available on a\ncommercial music service with millions of users.\nThere are a few areas where our model can be easily de-\nveloped. For example, stochastic variational inference re-\nquires we set the learning rate parameters t0and\u0014, which\nis application-dependent. By using adaptive learning rates\nfor stochastic variational inference [13], model inference\ncan converge faster and to a better local optimal solution.\nFrom a modeling perspective, currently the hyperparam-\neters for weights \u0012are ﬁxed, indicating that the sparsity\nlevel of the weight for each song is assumed to be the\nsame a priori. Alternatively we could put song-dependent\nhyper-priors on the hyperparameters of \u0012to encode the in-\ntuition that some of the songs might have denser weights\nbecause more tagging information is available. This would\noffer more ﬂexibility to the current model.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Matthew Hoffman for help-\nful discussion. This work was supported in part by NSF\ngrant IIS-1117015.\n8. REFERENCES\n[1] Last.fm dataset, the ofﬁcial song tags and song similarity col-\nlection for the Million Song Dataset. http://labrosa.\nee.columbia.edu/millionsong/lastfm.\n[2] T. Bertin-Mahieux, D. Ellis, B. Whitman, and P. Lamere. The\nMillion Song Dataset. In Proceedings of the International\nSociety for Music Information Retrieval Conference, pages\n591–596, 2011.\n[3] L. Bottou. Online learning and stochastic approximations.\nOn-line learning in neural networks, 17(9), 1998.\n[4] A. T. Cemgil. Bayesian inference for nonnegative matrix\nfactorisation models. Computational Intelligence and Neuro-\nscience, 2009.[5] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Auto-\nmatic generation of social tags for music recommendation. In\nAdvances in Neural Information Processing Systems, pages\n385–392, 2007.\n[6] K. Ellis, E. Coviello, A. Chan, and G. Lanckriet. A bag\nof systems representation for music auto-tagging. Audio,\nSpeech, and Language Processing, IEEE Transactions on,\n21(12):2554–2569, 2013.\n[7] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple\nprobabilistic model for tagging music. In Proceedings of the\nInternational Society for Music Information Retrieval Con-\nference, pages 369–374, 2009.\n[8] M. Hoffman, D. Blei, and P. Cook. Bayesian nonparametric\nmatrix factorization for recorded music. In Proceedings of the\n27th Annual International Conference on Machine Learning,\npages 439–446, 2010.\n[9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic\nvariational inference. The Journal of Machine Learning Re-\nsearch, 14(1):1303–1347, 2013.\n[10] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul.\nAn introduction to variational methods for graphical models.\nMachine learning, 37(2):183–233, 1999.\n[11] D. Liang, M. Hoffman, and D. Ellis. Beta process sparse non-\nnegative matrix factorization for music. In Proceedings of the\nInternational Society for Music Information Retrieval Con-\nference, pages 375–380, 2013.\n[12] J. Paisley, D. Blei, and M.I. Jordan. Bayesian nonnegative\nmatrix factorization with stochastic variational inference. In\nE.M. Airoldi, D. Blei, E.A. Erosheva, and S.E. Fienberg, edi-\ntors, Handbook of Mixed Membership Models and Their Ap-\nplications. Chapman and Hall/CRC, 2015.\n[13] R. Ranganath, C. Wang, D. Blei, and E. Xing. An adaptive\nlearning rate for stochastic variational inference. In Proceed-\nings of The 30th International Conference on Machine Learn-\ning, pages 298–306, 2013.\n[14] R. Salakhutdinov and A. Mnih. Probabilistic matrix factoriza-\ntion. In Advances in Neural Information Processing Systems,\npages 1257–1264, 2008.\n[15] D. Tingle, Y .E. Kim, and D. Turnbull. Exploring automatic\nmusic annotation with acoustically-objective tags. In Pro-\nceedings of the international conference on Multimedia in-\nformation retrieval, pages 55–62. ACM, 2010.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Se-\nmantic annotation and retrieval of music and sound effects.\nIEEE Transactions on Audio, Speech and Language Process-\ning, 16(2):467–476, 2008.\n[17] B. Xie, W. Bian, D. Tao, and P. Chordia. Music tagging with\nregularized logistic regression. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Conference,\npages 711–716, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n172"
    },
    {
        "title": "Modeling Temporal Structure in Music for Emotion Prediction using Pairwise Comparisons.",
        "author": [
            "Jens Madsen",
            "Bjørn Sand Jensen",
            "Jan Larsen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416384",
        "url": "https://doi.org/10.5281/zenodo.1416384",
        "ee": "https://zenodo.org/records/1416384/files/MadsenJL14.pdf",
        "abstract": "The temporal structure of music is essential for the cogni- tive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of mu- sic such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal infor- mation is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel pro- cessing pipeline: 1) Extracting audio features for each track resulting in a multivariate ”feature time series”. 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autore- gressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis.",
        "zenodo_id": 1416384,
        "dblp_key": "conf/ismir/MadsenJL14",
        "keywords": [
            "temporal",
            "structure",
            "music",
            "cognitive",
            "processes",
            "emotions",
            "expressed",
            "music",
            "predicting",
            "higher-level"
        ],
        "content": "MODELING TEMPORAL STRUCTURE IN MUSIC FOR EMOTION\nPREDICTION USING PAIRWISE COMPARISONS\nJens Madsen, Bjørn Sand Jensen, Jan Larsen\nTechnical University of Denmark,\nDepartment of Applied Mathematics and Computer Science,\nRichard Petersens Plads, Building 321,\n2800 Kongens Lyngby, Denmark\nfjenma,bjje,janlag@dtu.dk\nABSTRACT\nThe temporal structure of music is essential for the cogni-\ntive processes related to the emotions expressed in music.\nHowever, such temporal information is often disregarded\nin typical Music Information Retrieval modeling tasks of\npredicting higher-level cognitive or semantic aspects of mu-\nsic such as emotions, genre, and similarity. This paper\naddresses the speciﬁc hypothesis whether temporal infor-\nmation is essential for predicting expressed emotions in\nmusic, as a prototypical example of a cognitive aspect of\nmusic. We propose to test this hypothesis using a novel pro-\ncessing pipeline: 1) Extracting audio features for each track\nresulting in a multivariate ”feature time series”. 2) Using\ngenerative models to represent these time series (acquiring\na complete track representation). Speciﬁcally, we explore\nthe Gaussian Mixture model, Vector Quantization, Autore-\ngressive model, Markov and Hidden Markov models. 3)\nUtilizing the generative models in a discriminative setting\nby selecting the Probability Product Kernel as the natural\nkernel for all considered track representations. We evaluate\nthe representations using a kernel based model speciﬁcally\nextended to support the robust two-alternative forced choice\nself-report paradigm, used for eliciting expressed emotions\nin music. The methods are evaluated using two data sets\nand show increased predictive performance using temporal\ninformation, thus supporting the overall hypothesis.\n1. INTRODUCTION\nThe ability of music to represent and evoke emotions is an\nattractive and yet a very complex quality. This is partly a\nresult of the dynamic temporal structures in music, which\nare a key aspect in understanding and creating predictive\nmodels of more complex cognitive aspects of music such\nas the emotions expressed in music. So far the approach\nc\rJens Madsen, Bjørn Sand Jensen, Jan Larsen.\nLicensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Jens Madsen, Bjørn Sand Jensen, Jan Larsen.\n“Modeling Temporal Structure in Music for Emotion Prediction using\nPairwise Comparisons”, 15th International Society for Music Information\nRetrieval Conference, 2014.of creating predictive models of emotions expressed in mu-\nsic has relied on three major aspects. First, self-reported\nannotations (rankings, ratings, comparisons, tags, etc.) for\nquantifying the emotions expressed in music. Secondly,\nﬁnding a suitable audio representation (using audio or lyri-\ncal features), and ﬁnally associating the two aspects using\nmachine learning methods with the aim to create predic-\ntive models of the annotations describing the emotions ex-\npressed in music. However the audio representation has\ntypically relied on classic audio-feature extraction, often\nneglecting how this audio representation is later used in the\npredictive models.\nWe propose to extend how the audio is represented by\nincluding feature representation as an additional aspect,\nwhich is illustrated on Figure 1. Speciﬁcally, we focus on\nincluding the temporal aspect of music using the added fea-\nture representation [10], which is often disregarded in the\nclassic audio-representation approaches. In Music Informa-\ntion Retrieval (MIR), audio streams are often represented\nwith frame-based features, where the signal is divided into\nframes of samples with various lengths depending on the\nmusical aspect which is to be analyzed. Feature extraction\nbased on the enframed signal results in multivariate time\nseries of feature values (often vectors). In order to use these\nfeatures in a discriminative setting (i.e. predicting tags, emo-\ntion, genre, etc.), they are often represented using the mean,\na single or mixtures of Gaussians (GMM). This can reduce\nthe time series to a single vector and make the features\neasy to use in traditional linear models or kernel machines\nsuch as the Support Vector Machine (SVM). The major\nproblem here is that this approach disregards all temporal\ninformation in the extracted features. The frames could be\nrandomized and would still have the same representation,\nhowever this randomization makes no sense musically.\nIn modeling the emotions expressed in music, the tempo-\nral aspect of emotion has been centered on how the labels\nare acquired and treated, not on how the musical content is\ntreated. E.g. in [5] they used a Conditional Random Field\n(CRF) model to essentially smooth the predicted labels of\nan SVM, thus still not providing temporal information re-\nThis work was supported in part by the Danish Council for Strategic\nResearch of the Danish Agency for Science Technology and Innovation\nunder the CoSound project, case number 11-115328.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n319Figure 1. Modeling pipeline.\ngarding the features. In [12] a step to include some temporal\ninformation regarding the audio features was made, by in-\ncluding some ﬁrst and second order Markov properties for\ntheir CRF model, however still averaging the features for\none second windows. Other approaches have ranged from\nsimple feature stacking in [13] to actually using a genera-\ntive temporal model to represent features in [17]. The latter\nshowed that using a Dynamical Texture Mixture model to\nrepresent the feature time series of MFCCs, taking tempo-\nral dynamics into account, carried a substantial amount of\ninformation about the emotional content. In the present\nwork, in contrast to prior work, we focus on creating a com-\nmon framework by using generative models to represent\nthe multivariate feature time series for the application of\nmodeling aspects related to the emotions expressed in mu-\nsic. Since very little work has been done within this ﬁeld,\nwe make a broad comparison of a multitude of generative\nmodels of time series data. We consider how the time se-\nries are modeled on two aspects: whether the observations\nare continuous or discrete, and whether temporal informa-\ntion should be taken into account or not. This results in\nfour different combinations, which we investigate: 1) a\ncontinuous, temporal, independent representation which\nincludes the mean, single Gaussian and GMM models; 2) a\ntemporal, dependent, continuous representation using Au-\ntoregressive models; 3) a discretized features representation\nusing vector quantization in a temporally independent Vec-\ntor Quantization (VQ) model; and ﬁnally 4) a representation\nincluding the temporal aspect ﬁtting Markov and Hidden\nMarkov Models (HMM) on the discretized data. A mul-\ntitude of these models have never been used in MIR as\na track-based representation in this speciﬁc setting. To\nuse these generative models in a discriminative setting, the\nProduct Probability Kernel (PPK) is selected as the natural\nkernel for all the feature representations considered. We\nextend a kernel-generalized linear model (kGLM) model\nspeciﬁcally for pairwise observations for use in predicting\nemotions expressed in music. We speciﬁcally focus on\nthe feature representation and the modeling pipeline and\ntherefore use simple, well-known, frequently used MFCC\nfeatures. In total, eighteen different models are investigated\non two datasets of pairwise comparisons evaluated on the\nvalence and arousal dimensions.2. FEATURE REPRESENTATION\nIn order to model higher order cognitive aspects of music,\nwe ﬁrst consider standard audio feature extraction which\nresults in a frame-based, vector space representation of the\nmusic track. Given Tframes, we obtain a collection of T\nvectors with each vector at time tdenoted by xt2RD,\nwhereDis the dimension of the feature space.The main\nconcern here is how to obtain a track-level representation\nof the sequence of feature vectors for use in subsequent\nmodelling steps. In the following, we will outline a number\nof different possibilities — and all these can be considered\nas probabilistic densities over either a single feature vector\nor a sequence of such (see also Table. 1).\nContinuous: When considering the original feature\nspace, i.e. the sequence of multivariate random variables,\na vast number of representations have been proposed de-\npending on whether the temporal aspects are ignored (i.e.\nconsidering each frame independently of all others) or mod-\neling the temporal dynamics by temporal models.\nIn the time-independent case, we consider the feature as\na bag-of-frames, and compute moments of the independent\nsamples; namely the mean. Including higher order moments\nwill naturally lead to the popular choice of representing the\ntime-collapsed time series by a multivariate Gaussian dis-\ntribution (or other continuous distributions). Generalizing\nthis leads to mixtures of distributions such as the GMM\n(or another universal mixture of other distributions) used in\nan abundance of papers on music modeling and similarity\n(e.g. [1, 7]).\nInstead of ignoring the temporal aspects, we can model\nthe sequence of multivariate feature frames using well-\nknown temporal models. The simplest models include AR\nmodels [10].\nDiscrete: In the discrete case, where features are natu-\nrally discrete or the original continuous feature space can\nbe quantized using VQ with a ﬁnite set of codewords re-\nsulting in a dictionary(found e.g. using K-means). Given\nthis dictionary each feature frame is subsequently assigned\na speciﬁc codeword in a 1-of-P encoding such that a frame\nat timetis deﬁned as vector ~xtwith one non-zero element.\nAt the track level and time-independent case, each frame\nis encoded as a Multinomial distribution with a single draw,\n~x\u0018Multinomial(\u0015; 1), where \u0015denotes the probability\nof occurrence for each codeword and is computed on the\nbasis of the histogram of codewords for the entire track.\nIn the time-dependent case, the sequence of codewords,\n~x0;~x1;:::;~xT, can be modeled by a relatively simple (ﬁrst\norder) Markov model, and by introducing hidden states this\nmay be extended to the (homogeneous) Hidden Markov\nmodel with Multinomial observations (HMM disc).\n2.1 Estimating the Representation\nThe probabilistic representations are all deﬁned in terms\nof parametric densities which in all cases are estimated\nusing standard maximum likelihood estimation (see e.g. [2]).\nModel selection, i.e. the number of mixture components,\nAR order, and number of hidden states, is performed using\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n320Obs. Time Representation Density Model \u0012 BaseContinuousIndp.Mean p(xj\u0012)\u0011\u000e(\u0016) \u0016;\u001b Gaussian\nGaussian p(xj\u0012) =N(xj\u0016;\u0006) \u0016;\u0006 Gaussian\nGMM p(xj\u0012) =LP\ni=1\u0015iN(xj\u0016i;\u0006i)f\u0015i;\u0016i;\u0006igi=1:LGaussian\nTemp. AR p(x0;x1;::;xPj\u0012) =N\u0010\n[x0;x1;::;xP]>jm;\u0006jA;C\u0011\nm;\u0006jA;C GaussianDiscreteIndp. VQ p(~x\nj\u0012) =\u0015 \u0015 Multinomial\nTemp.Markov p(~x0;~x1;::;~xTj\u0012) =\u0015~x0TQ\nt=1\u0003~xt;~xt\u00001 \u0015;\u0003 Multinomial\nHMM disc p(~x0;~x1;::;~xTj\u0012) =P\nz0:T\u0015z0TQ\nt=1\u0003zt;zt\u00001\bt \u0015;\u0003;\b Multinomial\nTable 1 . Continuous, features, x2RD,Lis the number of components in the GMM, Pindicates the order of the AR\nmodel, AandCare the coefﬁcients and noise covariance in the AR model respectively and Tindicates the length of the\nsequence. Discrete, VQ: ~x\u0018Multinomial (\u0015) ,\u0003zt;zt\u00001=p(ztjzt\u00001),\u0003~xt;~xt\u00001=p(~xtj~xt\u00001),\bt=p(~xtjzt). The\nbasic Mean representation is often used in the MIR ﬁeld in combination with a so-called squared exponential kernel [2],\nwhich is equivalent to formulating a PPK with a Gaussian with the given mean and a common, diagonal covariance matrix\ncorresponding to the length scale which can be found by cross-validation and speciﬁcally using q= 1in the PPK.\nBayesian Information Criterion (BIC, for GMM and HMM),\nor in the case of the AR model, CV was used.\n2.2 Kernel Function\nThe various track-level representations outlined above are\nall described in terms of a probability density as outlined in\nTable 1, for which a natural kernel function is the Probabil-\nity Product Kernel [6]. The PPK forms a common ground\nfor comparison and is deﬁned as,\nk\u0000\np(xj\u0012);p\u0000\nxj\u00120\u0001\u0001\n=Z\u0000\np(xj\u0012)p\u0000\nxj\u00120\u0001\u0001qdx;(1)\nwhereq>0is a free model parameter. The parameters of\nthe density model, \u0012, obviously depend on the particular\nrepresentation and are outlined in Tab.1. All the densities\ndiscussed previously result in (recursive) analytical compu-\ntations. [6, 11].1\n3. PAIRWISE KERNEL GLM\nThe pairwise paradigm is a robust elicitation method to the\nmore traditional direct scaling approach and is reviewed\nextensively in [8]. This paradigm requires a non-traditional\nmodeling approach for which we derive a relatively simple\nkernel version of the Bradley-Terry-Luce model [3] for\npairwise comparisons. The non-kernel version was used for\nthis particular task in [9].\nIn order to formulate the model, we will for now assume\na standard vector representation for each of Naudio ex-\ncerpts collected in the set X=fxiji= 1;:::;Ng, where\nxi2RD, denotes a standard, Ddimensional audio fea-\nture vector for excerpt i. In the pairwise paradigm, any\ntwo distinct excerpts with index uandv, where xu2X\nandxv2X, can be compared in terms of a given aspect\n1It should be noted that using the PPK does not require the same length\nTof the sequences (the musical excerpts). For latent variable models,\nsuch as the HMM, the number of latent states in the models can also be\ndifferent. The observation space, including the dimensionality D, is the\nonly thing that has to be the same.(such as arousal/valance). With Msuch comparisons we de-\nnote the output set as Y=f(ym;um;vm)jm= 1;:::;Mg,\nwhereym2f\u0000 1;+1g indicates which of the two excerpts\nhad the highest valence (or arousal). ym=\u00001means that\ntheum’th excerpt is picked over the vm’th and visa versa\nwhenym= 1.\nThe basic assumption is that the choice, ym, between the\ntwo distinct excerpts, uandv, can be modeled as the differ-\nence between two function values, f(xu)andf(xv). The\nfunctionf:X!Rhereby deﬁnes an internal, but latent,\nabsolute reference of valence (or arousal) as a function of\nthe excerpt (represented by the audio features, x).\nModeling such comparisons can be accomplished by the\nBradley-Terry-Luce model [3, 16], here referred to more\ngenerally as the (logistic) pairwise GLM model. The choice\nmodel assumes logistically distributed noise [16] on the\nindividual function value, and the likelihood of observing a\nparticular choice, ym, for a given comparison mtherefore\nbecomes\np(ymjfm)\u00111\n1 + e\u0000ym\u0001zm; (2)\nwithzm=f(xum)\u0000f(xvm)andfm= [f(xum);f(xvm)]T.\nThe main question is how the function, f(\u0001), is modeled. In\nthe following, we derive a kernel version of this model in the\nframework of kernel Generalized Linear Models (kGLM).\nWe start by assuming a linear and parametric model of\nthe form fi=xiw>and consider the likelihood deﬁned\nin Eq. (2). The argument, zm, is now redeﬁned such that\nzm=\u0000\nxumw>\u0000xvmw>\u0001\n. We assume that the model\nparameterized by wis the same for the ﬁrst and second in-\nput, i.e. xumandxvm. This results in a projection from the\naudio features xinto the dimensions of valence (or arousal)\ngiven by w, which is the same for all excerpts. Plugging\nthis into the likelihood function we obtain:\np(ymjxum;xvm;w) =1\n1 + e\u0000ym((xum\u0000xum)w>):(3)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n321Following a maximum likelihood approach, the effective\ncost function,  (\u0001), deﬁned as the negative log likelihood\nis:\n GLM (w) =\u0000XM\nm=1logp(ymjxum;xvm;w):(4)\nHere we assume that the likelihood factorizes over the ob-\nservations, i.e. p(Yjf) =QM\nm=1p(ymjfm). Furthermore,\na regularized version of the model is easily formulated as\n GLM\u0000L2(w) = GLM +\rkwk2\n2; (5)\nwhere the regularization parameter \ris to be found using\ncross-validation, for example, as adopted here. This cost is\nstill continuous and is solved with a L-BFGS method.\nThis basic pairwise GLM model has previously been\nused to model emotion in music [9]. In this work, the\npairwise GLM model is extended to a general regularized\nkernel formulation allowing for both linear and non-linear\nmodels. First, consider an unknown non-linear map of an\nelement x2X into a Hilbert space, H, i.e.,'(x) :X7!H .\nThus, the argument zmis now given as\nzm= ('(xum)\u0000'(xvm))wT(6)\nTherepresenter theorem [14] states that the weights, w—\ndespite the difference between mapped instances — can be\nwritten as a linear combination of the inputs such that\nw=XM\nl=1\u000bl('(xul)\u0000'(xvl)): (7)\nInserting this into Eq. (6) and applying the ”kernel trick” [2],\ni.e. exploiting that h'(x)'(x0)iH=k(x;x0), we obtain\nzm= ('(xum)\u0000'(xvm))MX\nl=1\u000bl('(x ul)\u0000'(xvl))\n=MX\nl=1\u000bl('(xum)'(xul)\u0000'(xum)'(xvl)\n\u0000'(xvm)'(xul) +'(xvm)'(xvl))\n=MX\nl=1\u000bl(k(xum;xul)\u0000k(xum;xvl)\n\u0000k(xvm;xul) +k(xvm;xvl))\n=MX\nl=1\u000blk(fxum;xvmg;fxul;xvlg): (8)\nThus, the pairwise kernel GLM formulation leads exactly to\nstandard kernel GLM like [19], where the only difference is\nthe kernel function which is now a (valid) kernel between\ntwo sets of pairwise comparisons2. If the kernel function\nis the linear kernel, we obtain the basic pairwise logistic\nregression presented in Eq. (3), but the the kernel formula-\ntion easily allows for non-vectorial inputs as provided by\nthe PPK. The general cost function for the kGLM model is\n2In the Gaussian Process setting this kernel is also known as the Pair-\nwise Judgment kernel [4], and can easily be applied for pairwise leaning\nusing other kernel machines such as support vector machinesdeﬁned as,\n kGLM\u0000L2(\u000b) =\u0000MX\nm=1logp(ymj\u000b;K) +\r\u000b>K\u000b;\ni.e., dependent on the kernel matrix, K, and parameters\n\u000b. It is of the same form as for the basic model and we\ncan apply standard optimization techniques. Predictions for\nunseen input pairs fxr;xsgare easily calculated as\n\u0001frs=f(xr)\u0000f(xs) (9)\n=XM\nm=1\u000bmk(fxum;xvmg;fxr;xsg): (10)\nThus, predictions exist only as delta predictions. However\nit is easy to obtain a “true” latent (arbitrary scale) function\nfor a single output by aggregating all the delta predictions.\n4. DATASET & EVALUATION APPROACH\nTo evaluate the different feature representations, two datasets\nare used. The ﬁrst dataset (IMM ) consists of NIMM= 20 ex-\ncerpts and is described in [8]. It comprises all MIMM= 190\nunique pairwise comparisons of 20 different 15-second\nexcerpts, chosen from the USPOP20023dataset. 13 par-\nticipants (3 female, 10 male) were compared on both the\ndimensions of valence and arousal. The second dataset\n(YANG) [18] consists of MYANG = 7752 pairwise compar-\nisons made by multiple annotators on different parts of the\nNYANG = 1240 different Chinese 30-second excerpts on\nthe dimension of valence. 20 MFCC features have been\nextracted for all excerpts by the MA toolbox4.\n4.1 Performance Evaluation\nIn order to evaluate the performance of the proposed repre-\nsentation of the multivariate feature time series we compute\nlearning curves. We use the so-called Leave-One-Excerpt-\nOut cross validation, which ensures that all comparisons\nwith a given excerpt are left out in each fold, differing from\nprevious work [9]. Each point on the learning curve is\nthe result of models trained on a fraction of all available\ncomparisons in the training set. To obtain robust learning\ncurves, an average of 10-20 repetitions is used. Further-\nmore a ’win’-based baseline ( Base low) as suggested in [8]\nis used. This baseline represents a model with no informa-\ntion from features. We use the McNemar paired test with\ntheNull hypothesis that two models are the same between\neach model and the baseline, if p<0:05 then the models\ncan be rejected as equal on a 5%signiﬁcance level.\n5. RESULTS\nWe consider the pairwise classiﬁcation error on the two out-\nlined datasets with the kGLM-L2 model, using the outlined\npairwise kernel function combined with the PPK kernel\n(q=1/2). For the YANG dataset a single regularization pa-\nrameter\rwas estimated using 20-fold cross validation used\n3http://labrosa.ee.columbia.edu/projects/\nmusicsim/uspop2002.html\n4http://www.pampalk.at/ma/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n322Obs. T\nime ModelsTraining set size\n1% 5% 10% 20% 40% 80% 100%ContinuousIndp.Mean 0.468\n0.386 0.347 0.310 0.277 0.260 0.252\nN(xj\u0016;\u001b ) 0.464 0.394 0.358 0.328 0.297 0.279 0.274\nN(xj\u0016;\u0006) 0.440 0.366 0.328 0.295 0.259 0.253 0.246\nGMM diag 0.458 0.378 0.341 0.304 0.274 0.258 0.254\nGMM full 0.441 0.362 0.329 0.297 0.269 0.255 0.252\nTemp.D\nARCV 0.447 0.360 0.316 0.283 0.251 0.235 0.228\nV AR CV 0.457 0.354 0.316 0.286 0.265 0.251 0.248DiscreteIndp.VQp=256 0.459 0.392\n0.353 0.327 0.297 0.280 0.279*\nVQp=512 0.459 0.394 0.353 0.322 0.290 0.272 0.269\nVQp=1024 0.463 0.396 0.355 0.320 0.289 0.273 0.271\nTemp.Mark\novp=8 0.454 0.372 0.333 0.297 0.269 0.254 0.244\nMarkov p=16 0.450 0.369 0.332 0.299 0.271 0.257 0.251\nMarkov p=24 0.455 0.371 0.330 0.297 0.270 0.254 0.248\nMarkov p=32 0.458 0.378 0.338 0.306 0.278 0.263 0.256\nHMM p=8 0.461 0.375\n0.335 0.297 0.267 0.250 0.246\nHMM p=16 0.451 0.370 0.328 0.291 0.256 0.235 0.228\nHMM p=24 0.441 0.366 0.328 0.293 0.263 0.245 0.240\nHMM p=32 0.460 0.373 0.337 0.299 0.268 0.251 0.247\nBaseline 0.485\n0.413 0.396 0.354 0.319 0.290 0.285\nTable 2 . Classiﬁcation error on the IMM dataset applying\nthe pairwise kGLM-L2 model on the valence dimension.\nResults are averages of 20 folds, 13 subjects and 20 rep-\netitions. McNemar paired tests between each model and\nbaseline all result in p\u001c0:001 except for results marked\nwith * which has p>0:05 with sample size of 4940.\nacross all folds in the CV . The quantization of the multi-\nvariate time series, is performed using a standard online\nK-means algorithm [15]. Due to the inherent difﬁculty of\nestimating the number of codewords, we choose a selection\nspeciﬁcally (8, 16, 24 and 32) for the Markov and HMM\nmodels and (256, 512 and 1024) for the VQ models. We\ncompare results between two major categories, namely with\ncontinuous or discretized observation space and whether\ntemporal information is included or not.\nThe results for the IMM dataset for valence are pre-\nsented in Table 2. For continuous observations we see a\nclear increase in performance between the Diagonal AR\n(DAR) model of up to 0.018 and 0.024, compared to tra-\nditional Multivariate Gaussian and mean models respec-\ntively. With discretized observations, an improvement of\nperformance when including temporal information is again\nobserved of 0.025 comparing the Markov and VQ mod-\nels. Increasing the complexity of the temporal represen-\ntation with latent states in the HMM model, an increase\nof performance is again obtained of 0.016. Predicting the\ndimension of arousal shown on Table 3, the DAR is again\nthe best performing model using all training data, outper-\nforming the traditional temporal-independent models with\n0.015. For discretized data the HMM is the best performing\nmodel where we again see that increasing the complex-\nity of the temporal representation increases the predictive\nperformance. Considering the YANG dataset, the results\nare shown in Table 4. Applying the Vector AR models\n(V AR), a performance gain is again observed compared to\nthe standard representations like e.g. Gaussian or GMM.\nFor discretized data, the temporal aspects again improve\nthe performance, although we do not see a clear picture that\nincreasing the complexity of the temporal representation\nincreases the performance; the selection of the number of\nhidden states could be an issue here.Obs. T\nime ModelsTraining set size\n1% 5% 10% 20% 40% 80% 100%ContinuousIndp.Mean 0.368 0.258\n0.230 0.215 0.202 0.190 0.190\nN(xj\u0016;\u001b ) 0.378 0.267 0.241 0.221 0.205 0.190 0.185\nN(xj\u0016;\u0006) 0.377 0.301 0.268 0.239 0.216 0.208 0.201\nGMM diag 0.390 0.328 0.301 0.277 0.257 0.243 0.236\nGMM full 0.367 0.303 0.279 0.249 0.226 0.216 0.215\nTemp.D\nARCV 0.411 0.288 0.243 0.216 0.197 0.181 0.170\nV AR CV 0.393 0.278 0.238 0.213 0.197 0.183 0.176Discr eteIndp.VQp=256 0.351 0.241\n0.221 0.208 0.197 0.186 0.183\nVQp=512 0.356 0.253 0.226 0.211 0.199 0.190 0.189\nVQp=1024 0.360 0.268 0.240 0.219 0.200 0.191 0.190\nTemp.Mark\novp=8 0.375 0.265 0.238 0.220 0.205 0.194 0.188\nMarkov p=16 0.371 0.259 0.230 0.210 0.197 0.185 0.182\nMarkov p=24 0.373 0.275 0.249 0.230 0.213 0.202 0.200\nMarkov p=32 0.374 0.278 0.249 0.229 0.212 0.198 0.192\nHMM p=8 0.410 0.310\n0.265 0.235 0.211 0.194 0.191\nHMM p=16 0.407 0.313 0.271 0.235 0.203 0.185 0.181\nHMM p=24 0.369 0.258 0.233 0.215 0.197 0.183 0.181\nHMM p=32 0.414 0.322 0.282 0.245 0.216 0.200 0.194\nBaseline 0.483\n0.417 0.401 0.355 0.303 0.278 0.269\nTable 3 . Classiﬁcation error on the IMM dataset applying\nthe pairwise kGLM-L2 model on the arousal dimension.\nResults are averages of 20 folds, 13 participants and 20\nrepetitions. McNemar paired tests between each model and\nbaseline all result in p\u001c0:001 with a sample size of 4940.\nObs. T\nime ModelsTraining set size\n1% 5% 10% 20% 40% 80% 100%ContinuousIndp.Mean 0.331\n0.300 0.283 0.266 0.248 0.235 0.233\nN(xj\u0016;\u001b ) 0.312 0.291 0.282 0.272 0.262 0.251 0.249\nN(xj\u0016;\u0006) 0.293 0.277 0.266 0.255 0.241 0.226 0.220\nGMM diag 0.302 0.281 0.268 0.255 0.239 0.224 0.219\nGMM full 0.293 0.276 0.263 0.249 0.233 0.218 0.214\nTemp.D\nARp=10 0.302 0.272 0.262 0.251 0.241 0.231 0.230\nV AR p=4 0.281 0.260 0.249 0.236 0.223 0.210 0.206Discr eteIndp.VQp=256 0.304 0.289\n0.280 0.274 0.268 0.264 0.224\nVQp=512 0.303 0.286 0.276 0.269 0.261 0.254 0.253\nVQp=1024 0.300 0.281 0.271 0.261 0.253 0.245 0.243\nTemp.Mark\novp=8 0.322 0.297 0.285 0.273 0.258 0.243 0.238\nMarkov p=16 0.317 0.287 0.272 0.257 0.239 0.224 0.219\nMarkov p=24 0.314 0.287 0.270 0.252 0.235 0.221 0.217\nMarkov p=32 0.317 0.292 0.275 0.255 0.238 0.223 0.217\nHMM p=8 0.359 0.320\n0.306 0.295 0.282 0.267 0.255\nHMM p=16 0.354 0.324 0.316 0.307 0.297 0.289 0.233\nHMM p=24 0.344 0.308 0.290 0.273 0.254 0.236 0.234\nHMM p=32 0.344 0.307 0.290 0.272 0.254 0.235 0.231\nBaseline 0.500\n0.502 0.502 0.502 0.503 0.502 0.499\nTable 4 . Classiﬁcation error on the YANG dataset applying\nthe pairwise kGLM-L2 model on the valence dimension.\nResults are averages of 1240 folds and 10 repetitions. Mc-\nNemar paired test between each model and baseline results\ninp\u001c0:001. Sample size of test was 7752.\n6. DISCUSSION\nIn essence we are looking for a way of representing an entire\ntrack based on the simple features extracted. That is, we are\ntrying to ﬁnd generative models that can capture meaningful\ninformation coded in the features speciﬁcally for coding\naspects related to the emotions expressed in music.\nResults showed that simplifying the observation space\nusing VQ is useful when predicting the arousal data. Intro-\nducing temporal coding of VQ features by simple Markov\nmodels already provides a signiﬁcant performance gain,\nand adding latent dimensions (i.e. complexity) a further\ngain is obtained. This performance gain can be attributed\nto the temporal changes in features and potentially hidden\nstructures in the features not coded in each frame of the fea-\ntures but, by their longer term temporal structures, captured\nby the models.\nWe see the same trend with the continuous observations,\ni.e. including temporal information signiﬁcantly increases\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n323predictive performance. These results are speciﬁc for the\nfeatures used, the complexity, and potentially the model\nchoice might differ if other features were utilized. Future\nwork will reveal if other structures can be found in features\nthat describe different aspects of music; structures that are\nrelevant for describing and predicting aspects regarding\nemotions expressed in music.\nAnother consideration when using the generative models\nis that the entire feature time series is replaced as such\nby the model, since the distances between tracks are now\nbetween the models trained on each of the tracks and not\ndirectly on the features5. These models still have to be\nestimated, which takes time, but this can be done ofﬂine\nand provide a substantial compression of the features used.\n7. CONCLUSION\nIn this work we presented a general approach for evaluat-\ning various track-level representations for music emotion\nprediction, focusing on the beneﬁt of modeling temporal as-\npects of music. Speciﬁcally, we considered datasets based\non robust, pairwise paradigms for which we extended a\nparticular kernel-based model forming a common ground\nfor comparing different track-level representations of mu-\nsic using the probability product kernel. A wide range\nof generative models for track-level representations was\nconsidered on two datasets, focusing on evaluating both\nusing continuous and discretized observations. Modeling\nboth the valence and arousal dimensions of expressed emo-\ntion showed a clear gain in applying temporal modeling\non both the datasets included in this work. In conclusion,\nwe have found evidence for the hypothesis that a statisti-\ncally signiﬁcant gain is obtained in predictive performance\nby representing the temporal aspect of music for emotion\nprediction using MFCC’s.\n8. REFERENCES\n[1]J-J. Aucouturier and F. Pachet. Music similarity mea-\nsures: What’s the use? In 3rd International Conference\non Music Information Retrieval (ISMIR), pages 157–\n163, 2002.\n[2]C.M. Bishop. Pattern Recognition and Machine Learn-\ning. Springer, 2006.\n[3]R. D. Bock and J. V . Jones. The measurement and pre-\ndiction of judgment and choice. Holden-day, 1968.\n[4]F. Huszar. A GP classiﬁcation approach to preference\nlearning. In NIPS Workshop on Choice Models and\nPreference Learning, pages 1–4, 2011.\n[5]V . Imbrasaite, T. Baltrusaitis, and P. Robinson. Emotion\ntracking in music using continuous conditional random\nﬁelds and relative feature representation. In ICME AAM\nWorkshop, 2013.\n[6]T. Jebara and A. Howard. Probability Product Kernels.\n5We do note that using a single model across an entire musical track\ncould potentially be over simplifying the representation, in our case only\nsmall 15-30-second excerpts were used and for entire tracks some segmen-\ntation would be appropriate.Journal of Machine Learning Research, 5:819–844,\n2004.\n[7]J. H. Jensen, D. P. W. Ellis, M. G. Christensen, and\nS. Holdt Jensen. Evaluation of distance measures be-\ntween gaussian mixture models of mfccs. In 8th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR), 2007.\n[8]J. Madsen, B. S. Jensen, and J. Larsen. Predictive model-\ning of expressed emotions in music using pairwise com-\nparisons. From Sounds to Music and Emotions, Springer\nBerlin Heidelberg, pages 253–277, Jan 2013.\n[9]J. Madsen, B. S. Jensen, J. Larsen, and J. B. Nielsen.\nTowards predicting expressed emotion in music from\npairwise comparisons. In 9th Sound and Music Comput-\ning Conference (SMC) Illusions, July 2012.\n[10] A. Meng, P. Ahrendt, J. Larsen, and L. K. Hansen. Tem-\nporal feature integration for music genre classiﬁcation.\nIEEE Transactions on Audio, Speech, and Language\nProcessing, 15(5):1654–1664, 2007.\n[11] A. Meng and J. Shawe-Taylor. An investigation of fea-\nture models for music genre classiﬁcation using the\nsupport vector classiﬁer. In International Conference\non Music Information Retrieval, pages 604–609, 2005.\n[12] E. M. Schmidt and Y . E. Kim. Modeling musical emo-\ntion dynamics with conditional random ﬁelds. In 12th\nInternational Conference on Music Information Re-\ntrieval (ISMIR), 2011.\n[13] E. M. Schmidt, J. Scott, and Y . E. Kim. Feature learn-\ning in dynamic environments: Modeling the acoustic\nstructure of musical emotion. In 13th International Con-\nference on Music Information Retrieval (ISMIR), 2012.\n[14] B. Sch ¨olkopf, R. Herbrich, and A. J. Smola. A gener-\nalized representer theorem. Computational Learning\nTheory, 2111:416–426, 2001.\n[15] D. Sculley. Web-scale k-means clustering. International\nWorld Wide Web Conference, pages 1177–1178, 2010.\n[16] K. Train. Discrete Choice Methods with Simulation.\nCambridge University Press, 2009.\n[17] Y . Vaizman, R. Y . Granot, and G. Lanckriet. Modeling\ndynamic patterns for emotional content in music. In\n12th International Conference on Music Information\nRetrieval (ISMIR), pages 747–752, 2011.\n[18] Y-H. Yang and H.H. Chen. Ranking-Based Emotion\nRecognition for Music Organization and Retrieval.\nIEEE Transactions on Audio, Speech, and Language\nProcessing, 19(4):762–774, May 2011.\n[19] J. Zhu and T. Hastie. Kernel logistic regression and the\nimport vector machine. In Journal of Computational\nand Graphical Statistics, pages 1081–1088. MIT Press,\n2001.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n324"
    },
    {
        "title": "Bayesian Audio Alignment based on a Unified Model of Music Composition and Performance.",
        "author": [
            "Akira Maezawa",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii",
            "Hiroshi G. Okuno"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415594",
        "url": "https://doi.org/10.5281/zenodo.1415594",
        "ee": "https://zenodo.org/records/1415594/files/MaezawaIYO14.pdf",
        "abstract": "This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Con- ventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic sim- ilarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each per- formance. To solve this problem, our model explicitly rep- resents two state sequences: a top-level sequence that de- termines the common structure inherent in the music it- self and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two se- quences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given perfor- mances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different perfor- mances of the same piece by analyzing the bottom-level se- quences. Experimental evaluation showed that our method outperformed the conventional methods.",
        "zenodo_id": 1415594,
        "dblp_key": "conf/ismir/MaezawaIYO14",
        "keywords": [
            "probabilistic",
            "model",
            "aligns",
            "multiple",
            "performances",
            "music",
            "dynamic",
            "time",
            "warping",
            "left-to-right"
        ],
        "content": "BAYESIAN AUDIO ALIGNMENT BASED ON A UNIFIED GENERATIVE\nMODEL OF MUSIC COMPOSITION AND PERFORMANCE\nAkira Maezawa1;2Katsutoshi Itoyama2Kazuyoshi Yoshii2Hiroshi G. Okuno3\n1Yamaha Corporation2Kyoto University3Waseda University\nfamaezaw1,itoyama,yoshiig@kuis.kyoto-u.ac.jp, okuno@aoni.waseda.jp\nABSTRACT\nThis paper presents a new probabilistic model that can align\nmultiple performances of a particular piece of music. Con-\nventionally, dynamic time warping (DTW) and left-to-right\nhidden Markov models (HMMs) have often been used for\naudio-to-audio alignment based on a shallow acoustic sim-\nilarity between performances. Those methods, however,\ncannot distinguish latent musical structures common to all\nperformances and temporal dynamics unique to each per-\nformance. To solve this problem, our model explicitly rep-\nresents two state sequences: a top-level sequence that de-\ntermines the common structure inherent in the music it-\nself and a bottom-level sequence that determines the actual\ntemporal ﬂuctuation of each performance. These two se-\nquences are fused into a hierarchical Bayesian HMM and\ncan be learned at the same time from the given perfor-\nmances. Since the top-level sequence assigns the same\nstate for note combinations that repeatedly appear within\na piece of music, we can unveil the latent structure of the\npiece. Moreover, we can easily compare different perfor-\nmances of the same piece by analyzing the bottom-level se-\nquences. Experimental evaluation showed that our method\noutperformed the conventional methods.\n1. INTRODUCTION\nMultiple audio alignment is one of the most important tasks\nin the ﬁeld of music information retrieval (MIR). A piece\nof music played by different people produces different ex-\npressive performances, each embedding the unique inter-\npretation of the player. To help a listener better understand\nthe variety of interpretation or discover a performance that\nmatches his/her taste, it is effective to clarify how multiple\nperformances differ by using visualization or playback in-\nterfaces [1–3]. Given multiple musical audio signals that\nplay a same piece of music from the beginning to the end,\nour goal is to ﬁnd a temporal mapping among different sig-\nnals while considering the underlying music score.\nThis paper presents a statistical method of ofﬂine mul-\ntiple audio alignment based on a probabilistic generative\nc⃝Akira Maeza\nwa, Katsutoshi Itoyama, Kazuyoshi Yoshii,\nHiroshi G. Okuno.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Akira Maezawa, Katsutoshi Itoyama,\nKazuyoshi Yoshii, Hiroshi G. Okuno.“Bayesian Audio Alignment Based\non a Uniﬁed Generative Model of Music Composition and Performance”,\n15th International Society for Music Information Retrieval Conference,\n2014.\nGenerative Model of \nMusic CompositionGenerative Model of \nMusic Composition Performance\nPerformance 1\nPerformance 2\nPerformance 3\nTimeFigure\n1. An overview of generative audio alignment.\nmodel that can integrate various sources of uncertainties in\nmusic, such as spectral shapes, temporal ﬂuctuations and\nstructural deviations. Our model expresses how a musical\ncomposition gets performed , so it must model how they\nare generated.1Such a requirement leads to a conceptual\nmodel illustrated in Figure 1, described using a combina-\ntion of two complementary models.\nTo represent the generative process of a musical com-\nposition, we focus on the general fact that small fragments\nconsisting of multiple musical notes form the basic build-\ning blocks of music and are organized into a larger work.\nFor example, the sonata form is based on developing two\ncontrasting fragments known as the “subject groups,” and a\nsong form essentially repeats the same melody. Our model\nis suitable for modeling the observation that basic melodic\npatterns are reused to form the sonata or the song.\nTo represent the generative process of each performance,\nwe focus on temporal ﬂuctuations from a common music\ncomposition. Since each performance plays the same mu-\nsical composition, the small fragments should appear in\nthe same order. On the other hand, each performance can\nbe played by a different set of musical instruments with a\nunique tempo trajectory.\nSince both generative processes are mutually dependent,\nwe integrate a generative model of music composition with\nthat of performance in a hierarchical Bayesian manner. In\nother words, we separate the characteristics of a given mu-\nsic audio signal into those originating from the underly-\ning music score and those from the unique performance.\nInspired by a typical preprocessing step in music struc-\nture segmentation [6,7], we represent a music composition\nas a sequence generated from a compact, ergodic Markov\nmodel (“latent composition”). Each music performance is\nrepresented as a left-to-right Markov chain that traverses\nthe latent composition with the state durations unique to\neach performance.2\n1A generati\nve audio alignment model depends heavily on the model of\nboth how the music is composed and how the composition is performed .\nThis is unlike generative audio-to-score alignment [4, 5], which does not\nneed a music composition model because a music score is already given.\n2Audio samples are available on the website of the ﬁrst author.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n233Underlying\nMusical \nComposition\nPerformance 1\nTime\nLatent \nComposition1\nAϕ11Performance\nSequence\nGenerative\nModel\nObservation2\nA3\nB4\nB5\nB6\nC7\nBPerformance 2\nTime\nLatent \nComposition1\nAPerformance\nSequence\nGenerative\nModel\nObservation2\nB3\nB4\nB5\nC6\nC7\nBPerformance D\nTime\nLatent \nComposition1\nAPerformance\nSequence\nGenerative\nModel\nObservation2\nA3\nA4\nB5\nB6\nC7\nC8\nB9\nBz1\nAz2\nBz3\nBz4\nCBz5Latent\nCompositionLatent\nCommon\nStructureA\nBCGenerative\nModel\nA\nB\nC\nϕ12ϕ13ϕ14ϕ15ϕ16ϕ17 ϕ21ϕ22ϕ23ϕ24ϕ25ϕ26ϕ27 ϕD1ϕD2ϕD3ϕD4ϕD5ϕD6ϕD7ϕD8ϕD9\nz1z2z3z4z5 z1z2 z1z2z3z4z5 z2z4 z1 z2z3z4z5 z4 z5 z1z1\nFigure\n2. The concept of our method. Music composition is modeled as a sequence (composition sequence) from an\nergodic Markov model, and each performance plays the composition sequence, traversing the composition sequence in the\norder it appears, but staying in each state with different duration.\n2. RELATED WORK\nAudio alignment is typically formulated as a problem of\nmaximizing the similarity or minimizing the cost between\na performance and another performance whose time-axis\nhas been “stretched” by a time-dependent factor, using dy-\nnamic time warping (DTW) and its variants [8, 9] or other\nmodel of temporal dynamics [10]. To permit the use of a\nsimple similarity measure, it is important to design robust\nacoustic features [11, 12].\nAlternatively, tackling alignment by a probabilistic gen-\nerative model has gathered attention, especially in the con-\ntext of audio-to-music score alignment [4, 5]. In general,\na probabilistic model is formulated to describe how each\nnote in a music score translates to an audio signal. It is\nuseful when one wishes to incorporate, in a uniﬁed frame-\nwork, various sources of uncertainties present in music,\nsuch as inclusion of parts [13], mistakes [14], or timbral\nvariations [15–17].\nPrevious studies in generative audio alignment [13, 18]\nignores the organization present in musical composition,\nby assuming that a piece of music is generated from a left-\nto-right Markov chain, i.e., a Markov chain whose state\nappears in the same order for all performances.\n3. FORMULATION\nWe formulate a generative model of alignment that aligns\nDperformances. We provide a conceptual overview, and\nthen mathematically formalize the concept.\n3.1 Conceptual Overview\nWe ﬁrst extract short-time audio features from each of D\nperformances. Let us denote the feature sequence for the\ndth performance at frame t2[1; T d]asxd;t, where Tdis\nthe total number of frames for the dth audio signal. Here,\nthe kind of feature is arbitrary, and depends on the gener-\native model of the short-time audio. Then, we model xd;t\nas a set of Dstate sequences. Each state is associated witha unique generative process of short-time audio feature. In\nother words, each state represents a distinct audio feature,\ne.g., distinct chord, f0and so on, depending on how the\ngenerative model of the feature is designed.\nFor audio alignment, the state sequence must abide by\ntwo rules. First, the order in which each state appears is\nthe same for all Dfeature sequences. In other words, every\nperformance is described by one sequence of distinct audio\nfeatures, i.e., the musical piece that the performances play\nin common. We call such a sequence the latent composi-\ntion. Second, the duration that each performance resides in\na given state in the latent composition can be unique to the\nperformance. In other words, each performance traverses\nthe latent composition with a unique “tempo curve.” We\ncall the sequence that each performance traverses over the\nlatent composition sequence as the performance sequence.\nThe latent composition is a sequence of length Ndrawn\nfrom an ergodic Markov model, which we call the latent\ncommon structure. We describe the latent composition as\nzn, a sequence of length NandSstates, where each state\ndescribes a distinct audio feature. In other words, we as-\nsume that the musical piece is described by at most Ndis-\ntinct audio events, using at most Sdistinct sounds. The\nlatent common structure encodes the structure inherent to\nthe music. The transition probabilities of each state sheds\nlight on a “typical” performance, e.g., melody line or har-\nmonic progression. Therefore, the latent common structure\nprovides a generative model of music composition.\nThe performance sequence provides a generative model\nof performance. Each audio signal is modeled as an emis-\nsion from a N-state left-to-right Markov model, where the\nnth state refers to the generative model associated with the\nnth position in the latent composition. Speciﬁcally, let\nus denote the performance sequence for audio dasϕd;t,\nwhich is a state sequence of length TdandNstates, such\nthat state nrefers to the nth element of the latent composi-\ntion. Each performance sequence is constrained such that\n(1) it begins in state 1and ends at state N, and (2) state n\nmay traverse only to itself or state n+1. In other words, we\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n234ϕd,1θs\nxd,1xd,t-1xd,txd,Tdϕd,t-1ϕd,tϕd,Tdzn-1z1znzN... ...\n... ...S\nD......τπ\nηS\nNDFigure\n3. Graphical model of our method. Dotted box\nindicates that the arrow depends on all variables inside the\ndotted box. Hyperparameters are omitted.\nconstrain each performance sequence to traverse the latent\ncomposition in the same order but with a unique duration.\nSuch a model conveys the idea that each performance can\nindependently play a piece in any tempo trajectory.\n3.1.1 An Example\nLet us illustrate our method in Figure 2. In the example,\nS= 3andN= 5, where state “A” corresponds to a combi-\nnation of notes G, C and F, “B” corresponds to the note C,\nand so on; moreover, znencodes the state sequence “AB-\nBCB,” as to reﬂect the underlying common music compo-\nsition that the performances play. Note that a single note\nmay be expressed using more than one state in the latent\ncomposition, e.g., both z2andz3describe the note “C.”\nNext, each performance aligns to the latent composition,\nthrough the performance sequence. Each state of the per-\nformance sequence is associated to a position in the latent\ncomposition. For example, ϕ1;3is associated to position\n2ofz,z2. Then, at each time, the observation is gener-\nated by emitting from the state in latent common structure\nreferred by the current frame of the current audio. This is\ndetermined hierarchically by looking up the state nof the\nperformance sequence of audio dat time t, and referring to\nthe state sof the nth element of the latent composition. In\nthe example, ϕ1;3refers to state n= 2, so the generative\nmodel corresponding to zn=2, or “B,” is referred.\n3.2 Formulation of the Generative Model\nLet us mathematically formalize the above concept using a\nprobabilistic generative model, summarized as a graphical\nmodel shown in Fig. 3.\n3.2.1 Latent Composition and Common Structure\nThe latent composition is described as zn=f1\u0001\u0001\u0001N g, aS-\nstate state sequence of length N, generated from the latent\ncommon structure. We shall express the latent composition\nznusing one-of-S representation; znis aS-dimensional\nbinary variable where, when the state of zniss,zn;s=\n1and all other elements are 0. Then, we model zas a\nsequence from the latent common structure, an ergodic\nMarkov chain with initial state probability \u0019and transition\nprobability \u001c:\np(zj\u0019;\u001c) =S∏\ns=1\u0019z1;s\nsN;S;S∏\nn=2;s′=1;s=1\u001czn\u00001;s′zn;s\ns;s′ (1)\nRepeating Section\nLatent Composition IndexLatent Composition Index\n0 100 200 300 400 5000100200300400500Section A\nSection B\nSection ACodaFigure\n4. Structural annotation on Chopin Op. 41-2 and\nthe similarity matrix computed from its latent composition.\nEach state sis associated with an arbitrary set of param-\neters\u0012sthat describes the generative process of the audio\nfeature. We assume that \u001csis generated from a conjugate\nDirichlet distribution, i.e.,\u001cs\u0018Dir(\u001c0;s). The same goes\nfor the initial state probability \u0019,i.e.,\u0019\u0018Dir(\u0019 0). The\nhyperparameters \u001c0;sand\u00190are set to a positive value less\nthan 1, which induces sparsity of \u001cand\u0019, and hence leads\nto a compact latent common structure.\nThe latent composition and structure implicitly convey\nthe information about how the music is structured and what\nits building blocks are. Figure 4 shows a similarity matrix\nderived from the estimated latent composition of Op. 41-2\nby F. Chopin3having the ternary form (a.k.a. ABA form).\nThe ﬁrst “A” section repeats a theme of form “DEDF” re-\npeated twice. The second section is in a modulated key.\nFinally, the last section repeats the ﬁrst theme, and ends\nwith a short coda, borrowing from “F” motive from the ﬁrst\ntheme. Noting that the diagonal lines of a similarity ma-\ntrix represent strong similarity, we may unveil such a trend\nby analyzing the matrix. The bottom-left diagonal lines in\nthe ﬁrst section, for example, shows that a theme repeats,\nand the top-left diagonal suggests that the ﬁrst theme is re-\npeated at the end. This suggests that the latent composition\nreﬂects the organization of music.\nNotice that this kind of structure arises because we ex-\nplicitly model the organization of music, conveyed through\nan ergodic Markov model; simply aligning multiple per-\nformances to a single left-to-right HMM [13, 18] is insuf-\nﬁcient because it cannot revisit a previously visited state.\n3.2.2 Performance Sequence\nRecall that we require the performance sequence such that\n(1) it traverses in the order of latent composition, and (2)\nthe duration that each performance stays in a particular\nstate in the latent composition is conditionally indepen-\ndent given the latent composition. To satisfy these require-\nments, we model the performance sequence as a N-state\nleft-to-right Markov chain of length Td,ϕd;t, where the\nﬁrst state of the chain is ﬁxed to the beginning of the latent\n3The similarity\nmatrix Ri;jwas determined by removing self-\ntransitions from znand assigning it to z′, and setting Ri;j= 1 if\nz′\ni=z′\nj, and 0 otherwise. Next, we convolved Rby a two-dimensional\nﬁlter that emphasizes diagonal lines.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n235Time [frame]Feature Dim.\n0 100 200 300 400\nPerformance 2\nFeature Dim.Performance 1Figure\n5. Feature sequences (chroma vector) of two per-\nformances, overlayed by points where the state of the latent\ncomposition changes.\ncomposition and the last state to be the end. This assumes\nthat there are no cuts or repeats unique to a performance.\nLet us deﬁne \u0011d;nto be the probability for performance dto\ntraverse from position nof the latent composition to n+ 1.\nThen, we model the performance sequence as follows:\np(ϕd;t=f1\u0001\u0001\u0001T dg) =\u000e(n;1)ϕd;1;n\u000e(n; S)ϕd;Td;n\n\u0002∏Td;N\nt=1;n =1[\n\u0011ϕd;t\u00001;nϕd;t;n +1\nd;n\n\u0002(1\u0000\u0011d;n)ϕd;t\u00001;nϕd;t;n]\n(2)\nwhere \u000e(x; y)indicates the Kronecker Delta, i.e., its value\nis 1 when x=yand 0 otherwise. We assume \u0011d;nis drawn\nfrom a conjugate Beta distribution, i.e.,\u0011d;n\u0018Beta( a0; b0).\nThe ratio a0=b0controls the likelihood of traversing to next\nstates, and their magnitudes control the inﬂuence of the ob-\nservation on the posterior distribution.\nFigure 5 shows excerpts of the feature sequences ob-\ntained from two performances, and blue lines indicating\nthe change of the state of the latent composition has changed.\nThe ﬁgure suggests that the state changes with a notable\nchange in the feature, such as when new notes are played.\nSince, by the deﬁnition of a left-to-right Markov model, the\nnumber of vertical lines is identical for all performances,\nwe can align audio signals by mapping the occurrences of\ntheith vertical line for all performances, for each i.\n3.2.3 Generating Audio Features\nBased on the previous expositions, we can see that at time t\nof performance d, the audio feature is generated by choos-\ning the state in the latent common structure that is referred\nat time tfor performance d. This state is extracted by re-\nferring to the performance sequence to recover the position\nof the latent composition. Therefore, the observation like-\nlihood is given as follows:\np(xd;tjz;ϕ;\u0012) =∏\ns;np(xd;tj\u0012s)zn;sϕd;t;n(3)\nHere, p(xj\u0012s)is the likelihood of observation feature xat\nstate sof the latent common structure, and its parameter\n\u0012sis generated from a prior distribution p(\u0012sj\u00120).\nFor the sake of simplicity, we let p(xd;tj\u0012s)be adim(x) -\ndimensional Gaussian distribution with its parameters \u0012s\ngenerated from its conjugate distribution, the Gaussian-\nGamma distribution. Speciﬁcally we let \u0012s=f\u0016s;\u0015sg,\n\u00120=fm0; \u00170; u0;k0g, and let xd;tj\u0016s;\u0015s\u0018N(\u0016s;\u0015\u00001\ns),with p(\u0016s;i;\u0015s;i)/\u0015u0\u00001\n2se\u0000(\u0016s;i\u0000m0;i)2\u0015s;i\u00170\u0000k0;i\u0015s;i. One\nmay incorporate a more elaborate model that better ex-\npresses the observation.\n3.3 Inferring the Posterior Distribution\nWe derive the posterior distribution to the model described\nabove. Since direct application of Bayes’ rule to arrive at\nthe posterior is difﬁcult, we employ the variational Bayes\nmethod [19] and ﬁnd an approximate posterior of form\nq(ϕ;z;\u0012;\u0011;\u0019;\u001c) =∏\ndq(ϕd;\u0001)q(z)q(\u0019)∏\nd;nq(\u0011d;n)∏\nsq(\u0012s)q(\u001cs)that minimizes the Kullback-Leibler (KL)\ndivergence to the true posterior distribution.\nq(ϕ)andq(z)can be updated in a manner analogous to\na HMM. For q(z), we perform the forward-backward al-\ngorithm, with the state emission probability gnat position\nnof the latent composition and the transition probability\nvsfrom state sgiven as follows:\nloggn;s=∑\nd;t⟨ϕd;t;n⟩⟨log p(xd;tj\u0012s)⟩ (4)\nlogvs;s′=⟨log\u001cs;s′⟩ (5)\nHere, ⟨f(x)⟩ denotes the expectation of f(x)w.r.t. q. Like-\nwise, for q(ϕd;t), we perform the forward-backward algo-\nrithm, with the state emission probability hd;nand transi-\ntion probability wd;sgiven as follows:\nloghd;t;n =∑\ns⟨zn;s⟩⟨log p(xd;tj\u0012s)⟩ (6)\nlogwd;n;n′={\n⟨log\u0011d;n⟩ n=n′\n⟨log(1 \u0000\u0011d;n)⟩n+ 1 = n′(7)\nWe can update \u0019asq(\u0019) = Dir(\u0019 0+⟨z1⟩),\u0011asq(\u0011d;n) =\nBeta(a 0+∑\nt⟨ϕd;t\u00001;nϕd;t;n⟩; b0+∑\nt⟨ϕd;t\u00001;n\u00001 ϕd;t;n⟩),\nand\u001casq(\u001cs) = Dir(\u001c 0;s+∑N\nn>1⟨zn\u00001;szn⟩).\nBased on these parameters, the generative model of au-\ndio features can be updated. Some commonly-used statis-\ntics for state sinclude the count \u0016Ns, the mean \u0016\u0016sand the\nvariance \u0016\u0006s, which are given as follows:\n\u0016Ns=∑\nd;n;t⟨zn;s⟩⟨ϕd;t;n⟩ (8)\n\u0016\u0016s=1\n\u0016Ns∑\nd;n;t⟨zn;s⟩⟨ϕd;t;n⟩xd;t (9)\n\u0016\u0006s=1\n\u0016Ns∑\nd;n;t⟨zn;s⟩⟨ϕd;t;n⟩(xd;t\u0000\u0016\u0016s)2(10)\nFor\nexample, the Gaussian/Gaussian-Gamma model de-\nscribed earlier can be updated as follows:\nq(\u0016s;\u0015s) =NG(\n\u00170+\u0016Ns;\u00170m0+\u0016Ns\u0016\u0016s\n\u00170+\u0016Ns;\nu0+\u0016Ns\n2;k0+1\n2(\n\u0016Ns\u0016\u0006s+\u00170\u0016Ns\n\u00170+\u0016Ns(\u0016\u0016s\u0000m0)2))\n(11)\nHyperparameters may\nbe set manually, or optimized by\nminimizing the KL divergence from qto the posterior.\n3.4 Semi-Markov Performance Sequence\nThe model presented previously implicitly assumes that\nthe state duration of the performance sequence follows the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n236geometric distrib\nution. In such a model, it is noted, espe-\ncially in the context of audio-to-score alignment [4], that\nfurther improvement is possible by incorporating a more\nexplicit duration probability using an extension of the HMM\nknown as the hidden semi-Markov models [5, 20].\nIn this paper, we assume that every performance plays a\nparticular position in the music composition with more-or-\nless the same tempo. Hence, we incorporate an explicit du-\nration probability to the performance sequence, such that\nthe duration of each state is concentrated about some av-\nerage state duration common to each performance. To this\nend, we assume that for each state nof the performance se-\nquence, the state duration lfollows a Gaussian distribution\nconcentrated about a common mean:\np(lj\rn; c) = N(\rn; c\r2\nn) (12)\nWe chose the Gaussian distribution due to convenience of\ninference. By setting cappropriately, we can provide a\ntrade-off between the tendency for every piece to play in a\nsame tempo sequence, and variation of tempo among dif-\nferent performances.\nTo incorporate such a duration probability in the perfor-\nmance sequence model, we augment the state space of the\nleft-to-right Markov model of the performance sequence\nby a “count-down” variable lthat indicates the number of\nframes remaining in the current state. Then, we assume\nthat the maximum duration of each state is L, and repre-\nsent each state of the performance ϕd;tas a tuple (n; l)2\n[1\u0001 \u0001 \u0001N]\u0002[1\u0001 \u0001 \u0001L],i.e.,ϕd;t;n;l . In this model, state (n;1)\ntransitions to (n+ 1; l)with probability p(lj\u0016n+1; c), and\nstate (n; l)forl > 1transitions to (n; l\u00001)with prob-\nability one. Finally, we constrain the terminal state to be\n(N;1). Note that \u0011is no longer used because state duration\nis now described explicitly. The parameter \rncan be op-\ntimized by maximum likelihood estimation of the second\nkind, to yield the following:\n\rn=∑\nd;t;ll⟨ϕd;t\u00001;n\u00001; 1ϕd;t;n;l⟩∑\nd;t;l⟨ϕd;t\u00001;n\u00001;1ϕd;t;n;l⟩(13)\ncmay be\noptimized in a similar manner, but we found that\nthe method performs better when cis ﬁxed to a constant.\n4. EVALUATION\nWe conducted two experiments to assess our method. First,\nwe tested the effectiveness of our method against exist-\ning methods that ignore the organization of music [13,18].\nSecond, we tested the robustness of our method to the length\nof the latent composition, which we need to ﬁx in advance.\n4.1 Experimental Conditions\nWe prepared two to ﬁve recordings to nine pieces of Chopin’s\nMazurka (Op. 6-4, 17-4, 24-2, 30-2, 33-2, 41-2, 63-3, 67-\n1, 68-3), totaling in 38 audio recordings. For each of the\nnine pieces, we evaluated the alignment using (1) DTW us-\ning path constraints in [21] that minimizes the net squared\ndistance (denoted “DTW”), (2) left-to-right HMM to model\nmusical audio as done in existing methods [13, 18] (de-\nnoted “LRHMM”), (3) proposed method (denoted “Pro-\n10% 30% 50% 70% 90%10ms100ms1s10s\nDTW\nLRHMMProposed\nProposed (HSMM)\nPercentileErrorFigure\n6. Percentile of absolute alignment error. Aster-\nisks indicate statistically signiﬁcant difference over DTW\n(p=0.05) and circles indicate statistically signiﬁcant differ-\nence over LRHMM (p=0.05), using Kruskal-Wallis H-test.\nposed”), and (4) proposed method with semi-Markov per-\nformance sequence (denoted “Proposed (HSMM)”). For\nthe feature sequence xd;t, we employed the chroma vector\n[11] and half-wave rectiﬁed difference of the chroma ( ∆\nchroma), evaluated using a frame length of 8192 samples\nand a 20% overlap with a sampling frequency of 44.1kHz.\nFor the proposed method, the hyperparameters related\nto the latent common structure were set to \u00190= 0: 1and\n\u001c0;s;s′= 0:9 + 10\u000e (s; s′); these parameters encourages\nsparsity of the initial state probability and the state tran-\nsitions, while encouraging self-transitions. The parame-\nters related to the observation were set to u0=k0= 1,\n\u00170= 0:1 andm0= 0; such a set of parameters en-\ncourages a sparse variance, and assumes that the mean\nis highly dispersed. Moreover, we used S= 100 and\nN= 0:3 min dTd. For the semi-Markov performance se-\nquence model, we set c= 0:1. This corresponds to having\na standard deviation of \rnp\n0:1, or\nallowing the notes to\ndeviate by a standard deviation of about 30%.\n4.2 Experimental Results\nWe present below the evaluation of the alignment accu-\nracy and the robustness to the length of the latent compo-\nsition. On a workstation with Intel Xeon CPU (3.2GHz),\nour method takes about 3 minutes to process a minute of\nsingle musical audio.\n4.2.1 Alignment Accuracy\nWe compared the aligned data to that given by reverse con-\nducting data of the Mazurka Project [1]. Figure 6 shows\nthe absolute error percentile. The ﬁgure shows that our\nmethod (“Proposed”) performs signiﬁcantly better than the\nexisting method based on a LRHMM. This suggests that,\nfor a generative model approach to alignment, not only is\nmodel of performance difference critical but also that of the\ncommon music that the performances play. We also note an\nimproved performance of the semi-Markov model perfor-\nmance sequence (“Proposed (HSMM)”) over the Marko-\nvian model (“Proposed”).\nNote that when using the same features and squared-\nerror model, the semi-Markovian model performs better\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n23710% 20% 30% 40% 50% 60% 70% 80% 90%α10ms100ms1s10sMedian Alignment ErrorFigure\n7. Median alignment error against \u000b.\nthan DTW. This result suggests that with appropriate struc-\ntural and temporal models, a generative model approach is\na viable alternative to audio alignment. The performance\ngain from Markov to semi-Markov model illuminates the\nforte of the generative model approach: temporal, spectral\nand structural constraints are mixed seamlessly to attain a\ntrade-off among the trichotomy.\nWe note that our model is weak to compositional devi-\nations, such as added ornaments and repeats because we\nassume every performance plays an identical composition.\nWe observed that our method deals with an added note as\na noise or a note that gets played very shortly by most of\nthe audio signals, but neither captures the nature of added\nnotes as structural deviations. Moreover, our method some-\ntimes gets “trapped” in local optima, most likely due to the\nstrong mutual dependency between the latent variables.\n4.2.2 Robustness to the Length of the Latent Composition\nSince our method requires the user to set the length of la-\ntent composition N, we evaluated the quality of alignment\nasNis varied. To evaluate the performance of our method\nwith different values of N, we evaluated the alignment of\nthe proposed method when Nis set to N=\u000bjTd=1j, with\n\u000branging from \u000b= 0:1to\u000b= 0:9 with an increment\nof0:1. Figure 7 shows the median alignment error. We\nﬁnd that when \u000bis too small, when there is an insufﬁ-\ncient number of states to describe a composition, the error\nincreases. The error also increases when \u000bis too large,\nsince the maximum total allowed deviation decreases (i.e. ,\nto about (1\u0000\u000b)Td=1). However, outside such extremities,\nthe performance is relatively stable for moderate values of\n\u000baround 0.5. This suggests that our method is relatively\ninsensitive to a reasonable choice of N.\n5. CONCLUSION\nThis paper presented an audio alignment method based on\na probabilistic generative model. Based on the insight that\na generative model of musical audio alignment should rep-\nresent both the underlying musical composition and how\nit is performed by each audio signal, we formulated a uni-\nﬁed generative model of musical composition and perfor-\nmance. The proposed generative model contributed to a\nsigniﬁcantly better alignment performance than existing\nmethods. We believe that our contribution brings genera-\ntive alignment on par with DTW-based alignment, opening\ndoor to alignment problem settings that require integration\nof various sources of uncertainties.Future study includes incorporating better models of\ncomposition, performance and observation in our uniﬁed\nframework. In addition, inference over highly coupled hi-\nerarchical discrete state models is another future work.\nAcknowledgment: This study was supported in part by JSPS\nKAKENHI 24220006 and 26700020.\n6. REFERENCES\n[1] C. S. Sapp. Comparative analysis of multiple musical perfor-\nmances. In ISMIR, pages 2–5, 2007.\n[2] S. Miki, T. Baba, and H. Katayose. PEVI: Interface for re-\ntrieving and analyzing expressive musical performances with\nscape plots. In SMC , pages 748–753, 2013.\n[3] C. Fremerey, F. Kurth, M. M ¨uller, and M. Clausen. A demon-\nstration of the SyncPlayer system. In ISMIR, pages 131–132,\n2007.\n[4] C. Raphael. A hybrid graphical model for aligning poly-\nphonic audio with musical scores. In ISMIR , pages 387–394,\n2004.\n[5] A. Cont. A coupled duration-focused architecture for real-\ntime music-to-score alignment. IEEE PAMI, 32(6):974–987,\n2010.\n[6] J. Paulus, M. Muller, and A. Klapuri. State of the art report:\nAudio-based music structure analysis. In ISMIR, pages 625–\n636, Aug. 2010.\n[7] S. A. Abdallah et al. Theory and evaluation of a Bayesian\nmusic structure extractor. In ISMIR, pages 420–425, 2005.\n[8] R. B. Dannenberg and N. Hu. Polyphonic audio matching\nfor score following and intelligent audio editors. In ICMC ,\nSeptember 2003.\n[9] M. Grachten et al. Automatic alignment of music perfor-\nmances with structural differences. In ISMIR, pages 607–\n612, 2013.\n[10] N. Montecchio and A. Cont. A uniﬁed approach to real time\naudio-to-score and audio-to-audio alignment using sequen-\ntial Monte-Carlo inference techniques. In ICASSP, pages\n193–196, 2011.\n[11] T. Fujishima. Realtime chord recognition of musical sound:\nA system using Common Lisp Music. In ICMC , pages 464–\n467, 1999.\n[12] S. Ewert, M. M ¨uller, and P. Grosche. High resolution au-\ndio synchronization using chroma onset features. In ICASSP,\npages 1869–1872, 2009.\n[13] A. Maezawa and H. G. Okuno. Audio part mixture align-\nment based on hierarchical nonparametric Bayesian model of\nmusical audio sequence collection. In ICASSP, pages 5232–\n5236, 2014.\n[14] T. Nakamura, E. Nakamura, and S. Sagayama. Acoustic\nscore following to musical performance with errors and ar-\nbitrary repeats and skips for automatic accompaniment. In\nSMC , pages 200–304, 2013.\n[15] A. Maezawa et al. Polyphonic audio-to-score alignment\nbased on Bayesian latent harmonic allocation hidden Markov\nmodel. In ICASSP, pages 185–188, 2011.\n[16] T. Otsuka et al. Incremental Bayesian audio-to-score align-\nment with ﬂexible harmonic structure models. In ISMIR,\npages 525–530, 2011.\n[17] C. Joder, S. Essid, and G. Richard. Learning optimal fea-\ntures for polyphonic audio-to-score alignment. IEEE TASLP ,\n21(10):2118–2128, 2013.\n[18] R. Miotto, N. Montecchio, and N. Orio. Statistical music\nmodeling aimed at identiﬁcation and alignment. In AdMiRE,\npages 187–212, 2010.\n[19] M. J. Beal. Variational Algorithms for Approximate Bayesian\nInference. PhD thesis, University College London, 2003.\n[20] S. Yu and H. Kobayashi. An efﬁcient forward-backward al-\ngorithm for an explicit-duration hidden Markov model. IEEE\nSPL, 10(1):11–14, Jan 2003.\n[21] N. Hu, R. B. Dannenberg, and G. Tzanetakis. Polyphonic au-\ndio matching and alignment for music retrieval. In WASPAA,\npages 185–188, 2003.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n238"
    },
    {
        "title": "An Analysis and Evaluation of Audio Features for Multitrack Music Mixtures.",
        "author": [
            "Brecht De Man",
            "Brett Leonard",
            "Richard L. King",
            "Joshua D. Reiss"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416832",
        "url": "https://doi.org/10.5281/zenodo.1416832",
        "ee": "https://zenodo.org/records/1416832/files/ManLKR14.pdf",
        "abstract": "Mixing multitrack music is an expert task where charac- teristics of the individual elements and their sum are ma- nipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight dif- ferent engineers. We consider a range of features describ- ing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automa- tion of various mixing processes are explored. Part of the data used in this work is published in a new online mul- titrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared.",
        "zenodo_id": 1416832,
        "dblp_key": "conf/ismir/ManLKR14",
        "keywords": [
            "mixing multitrack music",
            "expert task",
            "characteristics manipulation",
            "technical issues resolution",
            "creative vision",
            "engineers contribution",
            "dynamic characteristics",
            "spatial characteristics",
            "spectral characteristics",
            "multidimensional analysis"
        ],
        "content": "AN ANALYSIS AND EVALUATION OF AUDIO FEATURES FOR\nMULTITRACK MUSIC MIXTURES\nBrecht De Man1, Brett Leonard2, 3, Richard King2, 3, Joshua D. Reiss1\n1Centre for Digital Music, Queen Mary University of London\n2The Graduate Program in Sound Recording, Schulich School of Music, McGill University\n3Centre for Interdisciplinary Research in Music Media and Technology\nb.deman@qmul.ac.uk, brett.leonard@mail.mcgill.ca,\nrichard.king@mcgill.ca, joshua.reiss@qmul.ac.uk\nABSTRACT\nMixing multitrack music is an expert task where charac-\nteristics of the individual elements and their sum are ma-\nnipulated in terms of balance, timbre and positioning, to\nresolve technical issues and to meet the creative vision of\nthe artist or engineer. In this paper we conduct a mixing\nexperiment where eight songs are each mixed by eight dif-\nferent engineers. We consider a range of features describ-\ning the dynamic, spatial and spectral characteristics of each\ntrack, and perform a multidimensional analysis of variance\nto assess whether the instrument, song and/or engineer is\nthe determining factor that explains the resulting variance,\ntrend, or consistency in mixing methodology. A number of\nassumed mixing rules from literature are discussed in the\nlight of this data, and implications regarding the automa-\ntion of various mixing processes are explored. Part of the\ndata used in this work is published in a new online mul-\ntitrack dataset through which public domain recordings,\nmixes, and mix settings (DAW projects) can be shared.\n1. INTRODUCTION\nThe production of recorded music involves a range of ex-\npert signal processing techniques applied to recorded mu-\nsical material. Each instrument or element thereof exists\non a separate audio ‘track’, and this process of manipulat-\ning and combining these tracks is normally referred to as\nmixing. Strictly creative processes aside, each process can\ngenerally be classiﬁed as manipulating the dynamic (bal-\nance and dynamic range compression), spatial (stereo or\nsurround panning and reverberation), and spectral (equal-\nisation) features of the source material, or a combination\nthereof [1, 4, 8, 15].\nRecent years have seen a steep increase in research on\nautomatic mixing, where some of the tedious, routine tasks\nin audio production are automated to the beneﬁt of the in-\nexperienced amateur or the time constrained professional.\nc\rBrecht De Man, Brett Leonard, Richard King and Joshua\nD. Reiss.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Brecht De Man, Brett Leonard,\nRichard King and Joshua D. Reiss. “An Analysis and Evaluation of Au-\ndio Features for Multitrack Music Mixtures”, 15th International Society\nfor Music Information Retrieval Conference, 2014.Most research is concerned with the validation of a mixing\nrule based on knowledge derived from practical literature\nor expert interviews [2, 6, 7, 9], usually through an exper-\niment where a method based on this assumption is com-\npared to a set of alternative methods. Furthermore, some\nresearch has been done on machine learning systems for\nbalancing and panning of tracks [13]. In spite of these ef-\nforts, the relation between the characteristics of the source\nmaterial and the chosen processing parameters, as well as\nthe importance of subjective input of the individual ver-\nsus objective or generally accepted target features, is still\npoorly understood. Recurring challenges in this ﬁeld in-\nclude a lack of research data, such as high-quality mixes in\na realistic but sufﬁciently controlled setting, and tackling\nthe inherently high cross-adaptivity of the mixing problem,\nas the value of each processing parameter for any given\ntrack is usually dependent on features and chosen process-\ning parameters associated with other tracks as well.\nIn this work, we conduct an experiment where a group\nof mixing engineers mix the same material in a realistic\nsetting, with relatively few constraints, and analyse the ma-\nnipulation of the signals and their features. We test the\nvalidity of the signal-dependent, instrument-independent\nmodel that is often used in automatic mixing research [6,\n7], and try to identify which types of processing are largely\ndependent on instrument type, the song (or source mate-\nrial), or the individual mixing engineer. Consequently, we\nalso identify which types of processing are not clearly de-\nﬁned as a function of these parameters, and thus warrant\nfurther research to understand their relation to low-level\n(readily extracted) features or high-level properties (instru-\nment, genre, desired effect) of the source audio. We dis-\ncuss the relevance of a number of audio features for the\nassessment of music production and the underlying pro-\ncesses as described above. This experiment also provides\nan opportunity to validate some of the most common as-\nsumptions in autonomous mixing research.\n2. EXPERIMENT\nThe mixing engineers in this experiment were students of\nthe MMus in Sound Recording at the Schulich School of\nMusic at McGill University. They were divided in two\ngroups of eight, where each group corresponds with a class\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n137from a different year in the two-year programme, and each\ngroup was assigned a different set of four songs to mix.\nEach mixing engineer allocated up to 6 hours to each of\ntheir four mix assignments, and was allowed to use Avid’s\nPro Tools including built-in effects (with automation) and\nthe Lexicon PCM Native Reverb Plug-In Bundle, a set of\ntools they were familiar with.\nFour out of eight songs are available on a new multi-\ntrack testbed including raw tracks, the rendered mixes and\nthe complete Pro Tools project ﬁles, allowing others to re-\nproduce or extend the research. The testbed can be found\nonc4dm.eecs.qmul.ac.uk/multitrack. The au-\nthors welcome all appropriately licensed contributions con-\nsisting of shareable raw, multitrack audio, DAW project\nﬁles, rendered mixes, or a subset thereof. Due to copyright\nrestrictions, the other songs could not be shared.\nWe consider three types of instruments - drums, bass,\nand lead vocal - as they are featured in all test songs in this\nresearch, and as they are common elements in contempo-\nrary music in general. Furthermore, we split up the drums\nin the elements kick drum, snare drum, and ‘rest’. Three\nout of eight songs had a male lead vocalist, and half of\nthe songs featured a double bass (in one case part bowed)\nwhile the other half had a bass guitar for the bass part.\nFor the purpose of this investigation, we consider a frag-\nment of the song only, consisting of the second verse and\nchorus, as all considered sources (drums, bass and lead vo-\ncal) are active here.\nWhereas the audio was recorded and mixed at a sam-\npling ratio of 96 kHz, we converted all audio to 44.1 kHz\nto reduce computational cost and to calculate spectral fea-\ntures based on the mostly audible region. The processed\ntracks are rendered from the digital audio workstation with\nall other tracks inactive, but with an unchanged signal path\nincluding send effects and bus processing1.\n3. FEATURES\nThe set of features we consider (Table 1) has been tailored\nto reﬂect properties relevant to the production of music in\nthe dynamic, spatial and spectral domain. We consider the\nmean of the feature over all frames of a track fragment.\nWe use the perceptually informed measure of loudness\nrelative to the loudness of the mix, as a simple RMS level\ncan be strongly inﬂuenced by high energy at frequencies\nthe human ear is not very sensitive to. To accurately mea-\nsure loudness in the context of multitrack content, we use\nthe highest performing modiﬁcation in [12] (i.e. using a\ntime constant of 280ms and a pre ﬁlter gain of +10 dB)\non the most recent ITU standard on measuring audio pro-\ngramme loudness [3].\n1When disabling the other tracks, non-linear processes on groups of\ntracks (such as bus dynamic range compression) will result in a different\neffective effect on the rendered track since the processor may be trig-\ngered differently (such as a reduced trigger level). While for the purpose\nof this experiment, the difference in triggering of bus compression does\nnot affect the considered features signiﬁcantly, it should be noted that\nfor rigorous extraction of processed tracks, in such a manner that when\nsummed together they result in the ﬁnal mix, the true, time-varying bus\ncompression gain should be measured and applied on the single tracks.Category Feature Reference\nDynamic Loudness [3, 12]\nCrest factor (100 ms and 1 s) [17]\nActivity [7]\nSpatial SPS [16]\nP[band] [16]\nSide/mid ratio\nLeft/right imbalance\nSpectral Centroid [5]\nBrightness\nSpread\nSkewness\nKurtosis \u0001\nRolloff (.95 and .85) \u0001\nEntropy\nFlatness\nRoughness\nIrregularity\nZero-crossing rate\nLow energy [5]\nOctave band energies\nTable 1: List of extracted features\nTo reﬂect the properties of the signal related to dynamic\nrange on the short term, we calculate the crest factor over\na window of 100 ms and over a window of 1 s [17].\nTo quantify gating, muting, and other effects that make\nthe track (in)audible during processing, we measure the\npercentage of time the track is active, with the activity state\nindicated by a Schmitt trigger with thresholds at \u000025 and\n\u000030 dB LUFS [7].\nTo analyse the spatial processing, we use the Stereo\nPanning Spectrum (SPS), which shows the spatial position\nof a certain frequency bin in function of time, and the Pan-\nning Root Mean Square (P [band] ), the RMS of the SPS over\na number of frequency bins [16]. In this work, we use the\nabsolute value of SPS, averaged over time, and the stan-\ndardPtotal (all bins), Plow(0-250 Hz), Pmid(250-2500\nHz) and Phigh (2500-22050 Hz), also averaged over time.\nFurthermore, we propose a simple stereo width measure,\nthe side/mid ratio, calculated as the power of side chan-\nnel (sum of left and right channel) over the power of the\nmid channel (average of left channel and polarity-reversed\nright channel). We also deﬁne the left/right imbalance, as\n(R\u0000L)=(R +L)where Lis the total/average power of\nthe left channel, and Ris the total/average power of the\nright channel. A centred track has low imbalance and low\nside/mid ratio, while a hard panned track has high imbal-\nance and high side/mid ratio. Note that while these features\nare related, they do not mean the same thing. A source\ncould have uncorrelated signals with the exact same energy\nin the left and right channel respectively, which would lead\nto a low left/right imbalance and a high side/mid ratio.\nFinally, we use features included in the MIR Toolbox\n[5] (with the default 50 ms window length) as well as oc-\ntave band energies to describe the spectral characteristics\nof the audio.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1384. ANALYSIS AND DISCUSSION\n4.1 Analysis of variance\nTable 2 shows the mean values of the features, as well as\nthe standard deviation between different mixing engineers\nand the standard deviation between different songs. Most\nconsidered features show greater variance for the same en-\ngineer across different songs, than for the same song over\ndifferent engineers. Exceptions to this are the left/right im-\nbalance and spectral roughness, which on average appear\nto be more dependent on the engineer than on the source\ncontent. The change of features (difference before and af-\nter processing, where applicable) varies more for differ-\nent mixing engineers than for different songs, too, for all\nfeatures. However, when considering the features instru-\nment by instrument, the source material only rarely causes\nthe means of the feature to differ signiﬁcantly (the means\nare only signiﬁcantly different through the effect of source\nmaterial for the zero-crossing rate of the snare drum track,\nand for the spectral entropy of the vocal track). This sug-\ngests that engineers would disagree on processing values,\nwhereas the source material has less effect.\nFor each feature, we perform an analysis of variance\nto investigate for which feature we can reject the hypothe-\nsis that the different ‘treatments’ (different source material,\nmixing engineer or instrument) result in the same feature\nvalue. For those features for which there is a signiﬁcant ef-\nfect (p < 0:05), we perform a multiple comparison of pop-\nulation means using the Bonferroni correction to establish\nwhat the mean values of the feature are as a function of the\ndetermining factor, and which instruments or songs have a\nsigniﬁcantly lower or higher mean than others. We discuss\nthe outcome of these tests in the following paragraphs.\nAs some elements were not used by the mixing engi-\nneer, some missing values are dropped when calculating\nthe statistics in the following sections.\n4.2 Balance and dynamics processing\nIn general, the relative loudness of tracks, averaged over\nall instruments, is dependent on the song (p < 5\u000110\u000011).\nHowever, when looking at each instrument individually,\nthe relative loudness of the bass guitar (p < 0:01), snare\ndrum (p < 0:05) and other drum instruments (‘rest’, i.e.\nnot snare or kick drum, p < 5\u000110\u00004) is dependent on\nmixing engineer.\nIn automatic mixing research, a popular assumption is\nthat the loudness of the different tracks or sources should\nbe equal [7]. A possible exception to this is the main ele-\nment, usually the vocal, which can be set at a higher loud-\nness [1]. From Figure 1, it is apparent that the vocal is sig-\nniﬁcantly louder than the other elements considered here,\nwhereas no signiﬁcant difference of the mean relative loud-\nness of the other elements can be shown. Furthermore, the\nrelative loudness of the vocal shows a relative narrow range\nof values (\u00002:7 \u00061:6LU), suggesting an agreement on a\n‘target loudness’ of about \u00003LU relative to the overall mix\nloudness.\nIt should be noted that due to crosstalk between the\ndrum microphones, the effective loudness of the snare drum\nkick snare rest vocal bass−25−20−15−10−50Relative Loudness [LU]\nInstrumentFigure 1: Average and standard deviation of loudness of\nsources relative to the total loudness of the mix, across\nsongs and mixing engineers.\nand kick drum will differ from the loudness measured from\nthe snare drum and kick drum tracks. As a result, dis-\nagreement of the relative loudnesses of snare drum and\nother drum elements such as overhead and room micro-\nphones does not necessarily suggest a signiﬁcantly differ-\nent desired loudness of the snare drum, as the snare drum is\npresent in both of these tracks. In this work, however, we\nare interested in the manipulations of the different tracks\nas they are available to the engineer.\nThe crest factor is affected by both the instrument (p <\n5\u000110\u00003) and song (p < 10\u000020), and every instrument in-\ndividually shows signiﬁcantly different crest factor values\nfor different engineers (p < 5\u000110\u00003). One exception to\nthe latter is the kick drum for a crest factor window size of\n1 s, where the hypothesis was not disproved for one group\nof engineers.\nAll instruments show an increase in crest factor com-\npared to the raw values (ratio signiﬁcantly greater than\none). This means that the short-term dynamic range is\neffectively expanded, which can be an effect of dynamic\nrange compression as transients are left unattenuated due\nto the response time of the compressor, while the rest of\nthe signal is reduced in level.\nThe percentage of the time the track was active did not\nmeaningfully change under the inﬂuence of different source\nmaterial, individual mixing engineers or instruments. A\ndrop in activity in some instances is due to gating of kick\ndrum, but this is the decision of certain mixing engineers\nfor certain songs, and no consistent trend.\n4.3 Stereo panning\nBoth the average left/right imbalance and average side/mid\nratio were signiﬁcantly higher for the non-pop/rock songs\n(p < 10\u00006).\nThe Panning Root Mean Square values P[band] all show\na larger value for the total mix and for the ‘rest’ group. The\ndifference is signiﬁcant except for the lowest band, where\nonly the bass is signiﬁcantly more central than the total\nmix. This can be explained by noting that most of the low\nfrequency sources are panned centre (see further).\nIn literature on automatic mixing and mixing engineer-\ning textbooks, it is stated that low-frequency sources as\nwell as lead vocals and snare drums should be panned cen-\ntral [1, 2, 4, 6, 8–10, 14]. To quantify the spatialisation for\ndifferent frequencies, we display the panning as a function\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n139Feature Kick drum Snare drum Rest drums Bass Lead vocal Average Mix\nLoudness [LU] -13.15±4.05\n3.89-16.78 ±6.17\n4.57-12.68 ±5.46\n2.80-2.65 ±1.52\n1.31-9.50 ±3.51\n2.86-10.95 ±4.14\n3.09N/A\nCrest (100 ms) 3.599±0.603\n0.3304.968 ±0.998\n0.4694.510 ±1.065\n0.3542.565 ±0.443\n0.1663.315 ±0.403\n0.2083.791 ±0.634\n0.2743.332 ±0.294\n0.116\nCrest (1 s) 9.824±3.074\n1.91116.724 ±6.458\n3.13512.472 ±4.710\n1.8234.339 ±1.098\n0.4495.283 ±1.102\n0.5149.728 ±2.907\n1.3985.315 ±0.997\n0.554\nActivity 0.676±0.250\n0.1220.861 ±0.161\n0.0780.909 ±0.115\n0.0290.958 ±0.076\n0.0090.844 ±0.089\n0.0440.850 ±0.117\n0.0480.995 ±0.009\n0.004\nL/R imbalance 0.075±0.094\n0.1370.144 ±0.153\n0.2270.361 ±0.303\n0.2130.107 ±0.135\n0.1760.045 ±0.072\n0.0850.146 ±0.139\n0.1520.088 ±0.075\n0.074\nSide/mid ratio 0.036±0.055\n0.0760.036 ±0.040\n0.0430.242 ±0.183\n0.1540.009 ±0.013\n0.0150.022 ±0.018\n0.0220.069 ±0.060\n0.0590.101 ±0.049\n0.046\nPtotal 0.104±0.102\n0.0900.108 ±0.082\n0.0590.307 ±0.028\n0.0270.075 ±0.093\n0.0830.134 ±0.022\n0.0270.145 ±0.060\n0.0520.234 ±0.030\n0.027\nPlow 0.066±0.078\n0.0870.122 ±0.102\n0.0730.243 ±0.045\n0.0410.040 ±0.063\n0.0590.147 ±0.034\n0.0420.123 ±0.061\n0.0560.188 ±0.042\n0.034\nPmid 0.066±0.074\n0.0760.114 ±0.090\n0.0640.290 ±0.023\n0.0230.052 ±0.082\n0.0670.177 ±0.027\n0.0350.140 ±0.054\n0.0480.248 ±0.027\n0.023\nPhigh 0.106±0.104\n0.0910.105 ±0.081\n0.0580.309 ±0.029\n0.0280.076 ±0.094\n0.0850.124 ±0.022\n0.0280.144 ±0.061\n0.0530.231 ±0.033\n0.029\nCentroid [Hz] 2253.8±1065.6\n729.84395.3 ±1448.6\n554.24130.8 ±1228.1\n483.21046.5 ±520.1\n232.42920.2 ±452.1\n264.72949.3 ±872.1\n418.62478.8 ±517.9\n247.1\nBrightness 0.306±0.105\n0.1030.598 ±0.156\n0.0690.557 ±0.115\n0.0580.135 ±0.082\n0.0310.455 ±0.071\n0.0400.410 ±0.100\n0.0560.362 ±0.070\n0.034\nSpread 3250.1±783.2\n447.54363.6 ±701.9\n335.94422.1 ±734.6\n292.32426.6 ±559.2\n320.43369.9 ±324.6\n191.33566.5 ±587.5\n298.03453.2 ±421.7\n200.6\nSkewness 3.649±1.068\n0.8861.492 ±0.663\n0.3011.665 ±0.682\n0.2466.234 ±1.885\n0.6302.470 ±0.573\n0.2433.102 ±0.912\n0.4272.779 ±0.600\n0.257\nKurtosis 23.847±11.997\n9.1645.965 ±2.905\n1.4747.053 ±3.449\n1.26358.870 ±31.874\n11.10711.579 ±4.267\n1.78421.463 ±9.834\n4.47713.646 ±4.511\n2.073\nRolloﬀ .95 [Hz] 8880.1±3679.2\n2151.213450.9 ±3100.6\n1582.213373.4 ±2594.1\n1007.44389.4 ±2714.7\n1244.59879.0 ±1335.7\n725.39994.5 ±2498.0\n1240.89679.0 ±1563.8\n734.3\nRolloﬀ .85 [Hz] 4513.7±2736.6\n1788.88984.3 ±3139.7\n1348.58755.3 ±2742.5\n975.61625.5 ±1205.0\n594.35595.8 ±1121.4\n609.75894.9 ±2047.2\n986.15026.2 ±1337.8\n599.8\nEntropy 0.655±0.104\n0.0900.840 ±0.084\n0.0570.832 ±0.051\n0.0250.552 ±0.073\n0.0260.735 ±0.043\n0.0160.723 ±0.066\n0.0380.744 ±0.043\n0.015\nFlatness 0.148±0.072\n0.0510.350 ±0.142\n0.0560.337 ±0.118\n0.0450.073 ±0.035\n0.0200.167 ±0.030\n0.0180.215 ±0.074\n0.0350.174 ±0.046\n0.020\nRoughness 84.72±84.85\n98.3236.30 ±41.16\n43.3267.57 ±71.76\n46.28236.04 ±160.38\n176.05247.00 ±216.15\n247.36134.33 ±319.30\n338.441843.31 ±1341.50\n1419.35\nIrregularity 0.158±0.098\n0.0630.235 ±0.151\n0.0790.297 ±0.135\n0.0690.502 ±0.176\n0.0650.540 ±0.165\n0.0940.346 ±0.136\n0.0750.705 ±0.090\n0.078\nZero-crossing 584.7±509.5\n409.42222.0 ±1183.3\n604.71988.9 ±944.1\n466.1246.6 ±217.8\n89.61177.5 ±233.7\n143.61243.9 ±554.3\n305.4905.2 ±237.4\n118.8\nLow energy 0.752±0.113\n0.0810.723 ±0.084\n0.0550.682 ±0.047\n0.0340.507 ±0.096\n0.0330.544 ±0.065\n0.0480.641 ±0.073\n0.0480.541 ±0.035\n0.038\nFigure 1: default\n1Table 2: Average values of features per instrument, including average over instrument and value of total mix, with standard\ndeviation between different songs by the same mixing engineer (top), and between different mixes of the same song (bot-\ntom). Values for which the variation across different mixes for the same song is greater than the variation across different\nsongs for the same engineer are displayed in bold.\n10210310400.050.10.150.2\nFrequency [Hz]Average SPS\nFigure 2: Mean Stereo Panning Spectrum (with standard\ndeviation) over all mixes and songs\nof frequency in Figure 2, using the average Stereo Panning\nSpectrum over all mixes and songs. From this ﬁgure a clear\nincrease in SPS with increasing frequency is apparent be-\ntween 50 Hz and 400 Hz. However, this trend does not\nextend to the very low frequencies (20-50 Hz) or higher\nfrequencies (>400 Hz).\n4.4 Equalisation\nTo assess the spectral processing of sources, mostly equali-\nsation in this context, we consider both the absolute values\nof the spectral features (showing the desired features of the\nprocessed audio) as well as the change in features (show-\ning common manipulations of the tracks). When only tak-ing the manipulations into account, and not the features\nof the source audio, similar to blindly applying a software\nequaliser’s presets, the results would be less translatable\nto situations where the source material’s spectral charac-\nteristics differs from that featured in this work [2]. How-\never, considering the change in features could reveal com-\nmon practices that are less dependent on the features of the\nsource material. Therefore, we investigate both.\nThe spectral centroid of the whole mix varies strongly\ndepending on the mixing engineer (p < 5\u000110\u00006). The\ncentroid of the snare drum track is consistently increased\nthrough processing, due to a reduction of the low energy\ncontent as well as spill of instruments like kick drum (see\nfurther regarding the reduction of low energy) and/or the\nemphasis of a frequency range above the original centroid.\nThe brightness of each track except bass guitar and kick\ndrum (the sources with the highest amount of low energy)\nis increased.\nFor a large set of spectral features (spectral centroid,\nbrightness, skewness, roll-off, ﬂatness, zero-crossing, and\nroughness), the engineers disagree on the preferred value\nfor all instruments except kick drum. In other words, the\nvalues describing the spectrum of a kick drum across engi-\nneers are overlapping, implying a consistent spectral target\n(a certain range of appropriate values). For other features\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n140(spread, kurtosis and irregularity) the value correspond-\ning with the kick drum track is also signiﬁcantly different\nacross engineers. The roughness shows no signiﬁcantly\ndifferent means for any instrument except the ‘rest’ bus.\nThe low energy of each track is reduced for each in-\nstrument, with signiﬁcantly more reduction for snare drum\nthan for kick drum and bass guitar. Its absolute value for\nbass and vocal is signiﬁcantly different across engineers,\nwhereas there is a general overlap for all other instruments\nincluding the mix. As the variation in the resulting value of\nlow energy is higher than the variation for the unprocessed\nversions, no target value is apparent for any instrument, nor\nfor the total mix.\nAnalysis of the octave band energies reveals deﬁnite\ntrends across songs and mixing engineers, for a certain in-\nstrument as well as the mix. The standard deviation does\nnot consistently decrease or increase over the octave bands\nfor any instrument when compared to the raw audio. The\nsuggested ‘mix target spectrum’ is in agreement with [11],\nwhich derived a ‘target spectrum’ based on average spec-\ntra of number one hits from various genres and over sev-\neral decades. Figure 4 shows the measured average mix\nspectrum against the octave band values of the average\nspectrum of a number one hit after 2000 from that work,\nwhich lies within a standard deviation from our result with\nthe exception of the highest band. The average relative\nchange in energies is not signiﬁcantly different from zero\n(no bands are consistently boosted or cut for certain instru-\nments), but taking each song individually in consideration,\na strong agreement of reasonably drastic boosts or cuts is\nshown for some songs. This conﬁrms that the equalisation\nis highly dependent on the source material, and engineers\nlargely agree on the necessary treatment for source tracks\nshowing spectral anomalies.\n5. CONCLUSION\nWe conducted a controlled experiment where eight mul-\ntitrack recordings mixed by eight mixing engineers were\nanalysed in terms of dynamic, spatial and spectral process-\ning of common key elements.\nWe measured a greater variance of features across songs\nthan across engineers, for each considered instrument and\nfor the total mix, whereas the mean values corresponding\nto the different engineers were more often statistically dif-\nferent from each other.\nThe relative loudness of the lead vocal track was found\nto be signiﬁcantly louder than all other tracks, with an av-\nerage value of \u00003LU relative to the total mix loudness.\nThe amount of panning as a function of frequency was\ninvestigated, and found to be increasing with frequency up\nto about 400 Hz, above which it stays more or less con-\nstant.\nWe measured a consistent decrease of low frequency\nenergy and an increase of crest factor for all instruments,\nand an increase of the spectral centroid of the snare drum\ntrack. Spectral analysis has shown a deﬁnite target spec-\ntrum that agrees with the average spectrum of recent com-\nmercial recordings.(a) Kick drum\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]\n(b) Snare drum\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]\n(c) Drums rest\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]\n(d) Bass\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]\n(e) V ocal\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]\nFigure 3: Average octave band energies (blue) with stan-\ndard deviation (red) for different instruments after process-\ning, compared to the raw signal (black).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n14131.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200kickEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200snareEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200restEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200vocalEnergy [dB]\nBand [Hz]\n31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bassEnergy [dB]\nBand [Hz]31.5 63 125 250 500 1k 2k 4k 8k 16k−60−40−200bounceEnergy [dB]\nBand [Hz]Figure 4: Average octave band energies for total mix,\ncompared to ‘After 2000’ curve from [11] (green dashed\nline)\n6. FUTURE WORK\nFuture work will be concerned with perceptual evaluation\nof mixes and its relation to features, using both qualita-\ntive (‘which sonic descriptors correspond with which fea-\ntures?’) and quantitative analysis (‘which manipulation of\naudio is preferred?’).\nFurther research is needed to establish the desired loud-\nness of sources, as opposed to loudness of tracks, and its\nvariance throughout songs, genres, and mixing engineers.\nAn extrapolation of the analysis described in this paper\nto other instruments is needed to validate the generality of\nthe conclusions regarding the processing of drums, bass\nand lead vocal at the mixing stage, and to further explore\nlaws underpinning the processing of different instruments.\nBased on the ﬁndings of this work, which showed trends\nand variances of different relevant features, we can inform\nknowledge engineered or machine learning based systems\nthat automate certain mixing tasks (balancing, panning,\nequalising and compression).\nThis work was based on a still relatively limited set of\nmixes, for which the engineers came from the same insti-\ntution. Through initiatives such as the public multitrack\ntestbed presented in this paper, it will be possible to anal-\nyse larger corpora of mixes, where more parameters can be\ninvestigated with more signiﬁcance.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank George Fazekas for assis-\ntance with launching the multitrack testbed.\nThis study was funded in part by the Engineering and\nPhysical Sciences Research Council (EPSRC) Semantic\nMedia grant (EP/J010375/1).\n8. REFERENCES\n[1] Alex Case. Mix Smart: Professional Techniques for the\nHome Studio. Focal Press. Taylor & Francis, 2011.\n[2] Brecht De Man and Joshua D. Reiss. A knowledge-\nengineered autonomous mixing system. In 135th Con-\nvention of the Audio Engineering Society, 2013.\n[3] ITU. Recommendation ITU-R BS.1770-3 Algorithms\nto measure audio programme loudness and true-peak\naudio level. Technical report, RadiocommunicationSector of the International Telecommunication Union,\n2012.\n[4] Roey Izhaki. Mixing audio: concepts, practices and\ntools. Focal Press, 2008.\n[5] Olivier Lartillot and Petri Toiviainen. MIR in Matlab\n(II): A toolbox for musical feature extraction from au-\ndio. In Proceedings of the 8th International Society for\nMusic Information Retrieval Conference, 2007.\n[6] Stuart Mansbridge, Saoirse Finn, and Joshua D. Reiss.\nAn autonomous system for multi-track stereo pan posi-\ntioning. In 133rd Convention of the Audio Engineering\nSociety, 2012.\n[7] Stuart Mansbridge, Saoirse Finn, and Joshua D. Reiss.\nImplementation and evaluation of autonomous multi-\ntrack fader control. In 132nd Convention of the Audio\nEngineering Society, 2012.\n[8] Bobby Owsinski. The Mixing Engineer’s Handbook .\nCourse Technology, 2nd edition, 2006.\n[9] Enrique Perez-Gonzalez and Joshua D. Reiss. Auto-\nmatic mixing: Live downmixing stereo panner. In\n10th International Conference on Digital Audio Effects\n(DAFx-10), 2007.\n[10] Pedro Pestana. Automatic mixing systems using adap-\ntive digital audio effects. PhD thesis, Catholic Univer-\nsity of Portugal, 2013.\n[11] Pedro Duarte Pestana, Zheng Ma, Joshua D. Reiss, Al-\nvaro Barbosa, and Dawn A. A. Black. Spectral charac-\nteristics of popular commercial recordings 1950-2010.\nIn135th Convention of the Audio Engineering Society,\n2013.\n[12] Pedro Duarte Pestana, Joshua D. Reiss, and Alvaro\nBarbosa. Loudness measurement of multitrack audio\ncontent using modiﬁcations of ITU-R BS.1770. In Au-\ndio Engineering Society Convention 134, 2013.\n[13] Jeff Scott and Youngmoo E. Kim. Analysis of acoustic\nfeatures for automated multi-track mixing. In Proceed-\nings of the 12th International Society for Music Infor-\nmation Retrieval Conference, 2011.\n[14] Jeff Scott and Youngmoo E. Kim. Instrument identiﬁ-\ncation informed multi-track mixing. In Proceedings of\nthe 14th International Society for Music Information\nRetrieval Conference, 2013.\n[15] M. Senior. Mixing Secrets. Taylor & Francis, 2012.\n[16] George Tzanetakis, Randy Jones, and Kirk McNally.\nStereo panning features for classifying recording pro-\nduction style. In Proceedings of the 8th International\nSociety for Music Information Retrieval Conference,\n2007.\n[17] Earl Vickers. The loudness war: Background, specu-\nlation, and recommendations. 129th Convention of the\nAudio Engineering Society, 2010.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n142"
    },
    {
        "title": "Systematic Multi-scale Set-class Analysis.",
        "author": [
            "Agustín Martorell",
            "Emilia Gómez"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417595",
        "url": "https://doi.org/10.5281/zenodo.1417595",
        "ee": "https://zenodo.org/records/1417595/files/MartorellG14.pdf",
        "abstract": "This work reviews and elaborates a methodology for hi- erarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and rep- resentation of Sapp’s ‘keyscapes’ to the description stage, by introducing a set-class level of description. This pro- vides a systematic, mid-level, and standard analytical lex- icon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time se- ries encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informative- ness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal de- scriptors, and music information retrieval applications.",
        "zenodo_id": 1417595,
        "dblp_key": "conf/ismir/MartorellG14",
        "keywords": [
            "hierarchical",
            "multi-scale",
            "set-class",
            "analysis",
            "music",
            "pieces",
            "keyscapes",
            "representation",
            "description",
            "standard"
        ],
        "content": "SYSTEMATIC MULTI-SCALE SET-CLASS ANALYSIS\nAgust ´ın Martorell\nUniversitat Pompeu Fabra\nagustin.martorell@upf.eduEmilia G ´omez\nUniversitat Pompeu Fabra\nemilia.gomez@upf.edu\nABSTRACT\nThis work reviews and elaborates a methodology for hi-\nerarchical multi-scale set-class analysis of music pieces.\nThe method extends the systematic segmentation and rep-\nresentation of Sapp’s ‘keyscapes’ to the description stage,\nby introducing a set-class level of description. This pro-\nvides a systematic, mid-level, and standard analytical lex-\nicon, which allows the description of any notated music\nbased on ﬁxed temperaments. The method beneﬁts from\nthe representation completeness, the compromise between\ngeneralisation and discrimination of the set-class spaces,\nand the access to hierarchical inclusion relations over time.\nThe proposed class-matrices are multidimensional time se-\nries encoding the pitch content of every possible music\nsegment over time, regardless the involved time-scales, in\nterms of a given set-class space. They provide the simplest\ninformation mining methods with the ability of capturing\nsophisticated tonal relations. The proposed class-vectors,\nquantifying the presence of every possible set-class in a\npiece, are discussed for advanced explorations of corpora.\nThe compromise between dimensionality and informative-\nness provided by the class-matrices and class-vectors, is\ndiscussed in relation with standard content-based tonal de-\nscriptors, and music information retrieval applications.\n1. INTRODUCTION\nPitch-class set theory has been used in music analysis prac-\ntice since decades. However, its general applicability to\npost-tonal music has contributed, and still contributes, to\nbe perceived as for specialists only. This apparent difﬁ-\nculty is far from real, and just a matter of the application\ncontext. The systematic and objective nature of the the-\nory, together with the compactness of the basic representa-\ntions, constitutes a powerful and ﬂexible descriptive frame-\nwork suited for any kind of pitch-based music.1This de-\nscription level is purposeful for several music information\n1In which the concepts of octave equivalence and ﬁxed temperaments\nare applicable. Although the pitch relations of interest may be quite dif-\nferent, depending on the temperament and the applied context, any dis-\ncrete pitch organization of the octave can be handled by the general math-\nematical framework. In this work, we bound to the twelve-tone equal\ntemperament.\nc\rAgust ´ın Martorell, Emilia G ´omez.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Agust ´ın Martorell, Emilia G ´omez.\n“Systematic multi-scale set-class analysis”, 15th International Society for\nMusic Information Retrieval Conference, 2014.retrieval (MIR) applications, such as structural analysis,\nsimilarity, pattern ﬁnding, classiﬁcation, and generation of\ncontent-based metadata. More interestingly, it provides a\nmeans for approaching complex topics, such as similarity,\nin alternative and insightful musically-grounded scenarios.\nIn addition, the basic descriptors are trivial to compute, and\nthey can be readily exploited by standard information min-\ning techniques.\nThe remaining of this work is organised as follows. Sec-\ntion 2 introduces the basic set-theoretical concepts, and\ncontextualise them in terms of our systematic analysis en-\ndeavour. Section 3 describes the computational approach.\nSections 4 and 5 discuss the method in several application\ncontexts. Section 6 summarises the proposed method, and\npoints to future extensions.\n2. BACKGROUND\n2.1 Set-class description\nPitch class [1] is deﬁned, in the twelve-tone equal tem-\npered system (TET), as an integer representing the residue\nclass modulo 12 of a pitch, that is, any pitch is mapped to\na pitch class by removing its octave information. A pitch-\nclass set (henceforth pc-set ) is a set of pitch classes with-\nout repetitions in which the order of succession of the el-\nements in the set is not of interest. In the TET system,\nthere exist 212= 4096 distinct pc-sets, so a vocabulary of\n4096 symbols is required for describing any possible seg-\nment of music. Any pc-set can also be represented by its\nintervallic content [5]. Intervals considered regardless of\ntheir direction are referred to as interval classes. The to-\ntal count of interval classes in a pc-set can be arranged as a\nsix-dimensional data structure called an interval vector [4].\nRelevant relational concepts for analysis are the set-\nclass equivalences, whereby two pc-sets are considered\nequivalent if and only if they belong to the same class. As\npointed out by Straus, equivalence is not the same thing\nas identity, rather it is a link between musical entities that\nhave something in common. This commonality underlying\nthe surface may eventually lend unity and/or coherence to\nmusical works [12]. In this respect, the class equivalences\ncan be conceived as all or nothing similarity measures be-\ntween two pc-sets. In the context of pc-sets, the number\nof pitch classes in a set is referred to as its cardinality.\nThis is perhaps the coarsest measure of similarity. Despite\nits theoretical relevance, cardinality is too general a no-\ntion of similarity to be of use in many analytical situations.\nAmong the many equivalence systems in the set-theoretical\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n219literature, three of them are particularly useful:\n1.Interval vector equivalence (iv-equivalence), which\ngroups all the pc-sets sharing the same interval vec-\ntor. There exist 197 different iv-types.\n2.Transpositional equivalence (Tn-equivalence), which\ngroups all the pc-sets related to each other by trans-\nposition. There exist 348 distinct Tn-types.\n3.Inversional and transpositional equivalence (TnI-\nequivalence), which groups all the pc-sets related by\ntransposition and/or inversion. There exist 220 dif-\nferent TnI-types (also referred to as Tn=TnI-types).\nAside the comprehensive coverage of every possible pc-\nset, the compromise between discrimination and generali-\nsation of these class-equivalence systems ﬁts a wide range\nof descriptive needs, thus their extensive usage in general-\npurpose music analysis. From them, iv-equivalence is the\nmost general (197 classes). It shares most of its classes\nwithTnI-equivalence (220 classes), with some exceptions,\nnamed Z-relations [4], for which the same interval vector\ngroups pc-sets which are not TnI-equivalent [7]. The most\nspeciﬁc from the three systems is Tn-equivalence.\n2.2 Systematic approaches to set-class analysis\nTo date, one of the most systematic approaches to set-class\nsurface analysis is proposed in [6], under the concept of\n‘tail-segment array’, whereby every note in a composition\nis associated with all the possible segments of a given car-\ndinality that contains it. This segmentation is combined\nwith certain set-class-based ‘detector functions’, in order\nto obtain summarized information from music pieces and\ncollections. The usefulness of the method is comprehen-\nsively discussed in the context of style characterization.\nSome limitations of this technique are addressed in [8],\nby ﬁrst identifying the segmentation, description and rep-\nresentation stages of the method, and extending system-\natization to all of them simultaneously. This is done by\ncombining the exhaustive segmentation and representation\nof Sapp’s ‘keyscapes’ [11], with a systematic description\nof the segments in terms of set-classes. The multidimen-\nsional, massive and overlapping information resulting from\nthis method, is managed by summarising features and in-\nterfacing design, targeting speciﬁc analytical tasks.\n3. MULTI-SCALE SET-CLASS ANALYSIS\nThis work elaborates directly upon [8], in which detailed\nand extended discussions can be consulted. A description\nof our general method follows.\n3.1 Segmentation\nThe input to the system is a sequence of MIDI events,\nwhich can be of any rhythmic or polyphonic complexity.\nThis signal is processed by the segmentation stage, for\nwhich two different algorithms are used: a) an approximate\ntechnique, non comprehensive but practical for interactingwith the data; b) a fully systematic method, which exhausts\nall the segmentation possibilities.\nThe approximate method applies many overlapping slid-\ning windows, each of them scanning the music at a differ-\nent time-scale. The minimum window size and the number\nof time-scales are user parameters, and can be ﬁne tuned as\na trade-off between resolution and computational cost. The\nsame hop size is applied for all the time-scales, in order to\nprovide a regular grid for visualisation and interfacing pur-\nposes. Each segment is thus indexed by its centre location\n(time) and its duration (time-scale).\nThe fully systematic method is required for the quanti-\ntative descriptors in which completeness of representation\nis necessary. It is computed by ﬁnding every change in the\npc-set content, whether the product of onsets or offsets,\nand segmenting the piece by considering all the pairwise\ncombinations among these boundaries.\n3.2 Description\nDenoting pitch-classes by the ordinal convention (C=0, : : :,\nB=11), each segment is analysed as follows. Let bi=\n1if the pitch-class iis contained (totally or partially) in\nthe segment, or 0 otherwise. The pc-set in the segment\nis encoded as an integer p=11P\ni=0bi\u0001211\u0000i2[0,4095].\nThis integer serves as an index for a precomputed table of\nset classes,2including the iv-, TnI- and Tn-equivalences\n(discussed in Section 2.1). For systematisation complete-\nness, the three class spaces are extended to include the so-\ncalled trivial forms.3With this, the total number of inter-\nval vectors rises to 200, while the TnI- andTn-equivalence\nclasses sum to 223 and 351 categories respectively. In this\nwork, we use Forte’s cardinality-ordinal convention [4] to\nname the classes, as well as the usual A/B sufﬁx for refer-\nring to the prime/inverted forms under Tn-equivalence. We\nalso follow the conventional notation to name the Z-related\nclasses, by inserting a ‘Z’ between the hyphen and the or-\ndinal. As an example, a segment containing the pitches\nfG5,C3,E4,C4g is mapped to the pc-set f0,4,7g and coded\nasp= 2192 (100010010000 in binary). The precom-\nputed table is indexed by p, resulting in the interval vector\nh001110i (iv-equivalence, grouping all the sets containing\nexactly 1 minor third, 1 major third, and 1 fourth), the class\n3-11 (T nI-equivalence, grouping all the major and minor\ntrichords), and the class 3-11B (T n-equivalence, grouping\nall the major trichords). The discrimination between major\nand minor trichords is thus possible under Tn-equivalence\n(3-11A for minor, 3-11B for major), but not under iv- or\nTnI-equivalences.\n3.3 Representation\nThe main data structure, named class-scape, is the set-\nclass equivalent of Sapp’s ‘keyscapes’ [11]. It represents\nthe class content of every possible segment, indexed by\n2As formalised in [4]. See Supplemental material (Section 7).\n3The null set and single pitch classes (cardinalities 0 and 1, containing\nno intervals), the undecachords (cardinality 11) and the universal pc-set\n(cardinality 12, also referred to as the aggregate).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n220635\nClassTimeTimescale(a)\nTimeClass635\n(b)\n63520406080100\nClass% of duration\n(c)\nFigure 1: Debussy’s Voiles. a) class-scape; b) class-matrix; c)\nclass-vector.\ntheir time position and duration. The dimensionality of the\nclass-scapes (time, time-scale and class) is then reduced\nto more manageable, yet informative, data structures. The\nﬁrst reduction consists on projecting the class-scape to the\ntime-class plane, which results in the concept of class-\nmatrix. This is done by realising in time each point in the\nclass-scape, thus retaining a substantial information from\nthe lost dimension (time-scale). A further reduction sum-\nmarizes the class-matrix in a single vector, named class-\nvector, by quantifying the presence of every possible class\nin the piece as a percentage of the piece’s duration. The\nclass-scape, class-matrix and class-vector, computed from\nDebussy’s Voiles are depicted in Figure 1, with the promi-\nnent whole-tone scale (class 6-35) labelled as a reference.\n4. MINING CLASS-MATRICES\nIn this section, we will review and elaborate upon the infor-\nmation conveyed by the class-matrices. Even with the loss\nof information, the reduction process from the class-scape\nto the class-matrix guarantees that every instantiation ofevery class is represented in the class-matrix, regardless\nthe involved time-scales. The class-matrix represents the\ntemporal activation of every possible class over time. A\ntime point activated for a given class in the matrix means\nthat it exist at least one segment containing this time point\nwhich belongs to such class. As the representation guar-\nantees a strict class-wise separation, the class matrix con-\nstitutes a time-series of a special kind. It does not only\ncapture evidence from every class instantiation over time,\nbut it also informs about their set-class inclusion relations .\nThe class-matrix, thus, embeds a considerable hierarchical\ninformation, allowing the analysis of the speciﬁc construc-\ntions of the class instantiations.\n4.1 Case study: subclass analysis\nAn example of this analytical potential is depicted in Fig-\nure 2. It shows the comparison between the pure diatoni-\ncisms in Victoria’s parody masses in Ionian mode4and\nBach’s preludes and fugues in major mode from the Well\nTempered Clavier. This is done by ﬁrst isolating the dia-\ntonic segments (activation of 7-35 in the class-matrix) of\neach movement, and constructing a subclass-matrix with\nthe subset content of these segments. The differences can\nbe quantiﬁed by computing the corresponding subclass-\nvectors out of the subclass-matrices, and averaging them\nacross pieces in the corpora. This tells about what the par-\nticular diatonicisms (and only the diatonicisms) are made\nof. Some relevant differences stand out from the compar-\nison. Victoria’s larger usage of major and minor triads\n(3-11) and cadential chord sequences (5-27) stands out.\nOn the other hand, Bach makes more prominent usage of\nthe scalar formation 6-Z25: aside its instantiations as per-\nfect cadences, it is recurrent in many motivic progressions,\nwhich are not idiomatic in Victoria’s contrapuntal writing.\n311 527 6 Z25020406080\nVictoria (Ionian)\nBach (major)\nClass% of duration\nFigure 2: Diatonicism in Victoria and Bach. Mean subclass-\nvectors under 7-35.\n4.2 Case study: structural analysis\nSelf-similarity matrices (SSM) are a simple standard tool\nused for structural analysis [3]. Classical inputs to the SSM\nare spectral or chroma feature time series. Some of the\nSSM-based methods can handle different time-scales, and\nsome of the chroma methods allows transpositional invari-\nance [9]. These functionalities are usually implemented\nat the SSM computation stage, or as a post processing.\nIn the class-matrices, both the equivalence mappings (in-\ncluding their inherent hierarchies) and the multi-scale na-\n4Including Alma Redemptoris Mater, Ave Regina Caelorum, Laetatus\nSum, Pro Victoria, Quam Pulchri Sunt, and Trahe Me Post Te. See (Rive,\n1969) for a modal classiﬁcation.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n221ture of the information are already embedded in the fea-\nture time-series, so a plain SSM can be used for ﬁnding\nsophisticated recurrences. For instance, a passage com-\nprised of a chord sequence can be recognized as similar\nthan a restated passage with different arpeggiations and/or\ninversions of the chord intervals (e.g. from major to mi-\nnor triads). A vertical chord and its arpeggiated version\nmay not be recognized as very similar at the lowest cardi-\nnalities, but their common TnI-sonority will certainly do\nat their corresponding time-scales. Moreover, any sonor-\nity containing the chords (supersets) will also be captured\nat their proper time-scales, climbing up the hierarchy un-\ntil reaching the whole-piece segment, everything indexed\nby a common temporal axis. A quantiﬁcation of similar-\nity between variations may thus be possible at the level of\nembedded sonorities.\nThis is discussed next for large-scale recurrence ﬁnd-\ning in Webern’s Variations for piano, op.27/I. This serial\npiece presents an A-B-A’ structure, built upon several in-\nstantiations of the main twelve-tone row, at different trans-\npositional and/or inversional levels. Figure 3 (top) depicts\nthe class-scape of the piece, ﬁltered by the prominent hexa-\nchordal iv-sonority h332232i, and Figure 3 (bottom) shows\nthe well-known (extensively analysed in literature) struc-\nture of the row instantiations, annotated according to [2].\nFigure 4 depicts the output of a plain SSM, computed from\nthree different inputs: a) the pc-set time series;5b) the\nclass-matrix under Tn; c) the class-matrix under TnI. The\npc-equivalence does not capture any large-scale recurrence.\nThe restatement of the ﬁrst two phrases in Ais captured\nby the Tn-equivalence, as these phrases are mainly related\nby transposition in A’. Finally, the TnI-equivalence re-\nveals the complete recapitulation, including the last two\nphrases of A, which are restated in A’ in both transposed\nand inverted transformations. It is worth noting that the\nmethod does not limit to compare the general sonority, the\nubiquitoush332232i, but its speciﬁc construction down the\nsubclass hierarchy. This allows the discrimination of the B\nsection, built upon the same kind of row instantiations than\nAandA’, but presented in distinct harmonisations.\ntime-scale\ntime\nFigure 3: Webern’s op.27/I. Top: class-scape ﬁltered by\nh332232i; Bottom: structure.\nA relevant advantage of the pc-set-based spaces, with\nrespect to continuous ones,6is that music can be analysed\nin terms of different class systems at no extra computa-\ntional cost. Being ﬁnite and discrete spaces (4096 classes\nat most for the TET system), the whole equivalence sys-\ntems, including their inner metrics, can be precomputed.\n5In some respect, the discrete equivalent of the chroma features.\n6Such as chroma features, a ﬁnite, but continuous space.\n(a)\n (b)\n (c)\nFigure 4: SSM from Webern’s op.27/I. a) pc-equivalence; b) Tn-\nequivalence); c) TnI-equivalence).\nThe mapping from pc-sets to set-classes, as well as the dis-\ntances between any pair of music segments, can thus be im-\nplemented by table indexing. Once the pc-set of each pos-\nsible segment has been computed (which constitutes the\nactual bottleneck of the method), the rest of the process is\ninexpensive, and multiple set-class lenses can be changed\nin real time, allowing fast interactive explorations of the\nmassive data. This feature, alongside with a variety of ﬁl-\ntering options for visual exploration, can be tested with our\nproof-of-concept set-class analysis tool.7\n5. MINING CLASS-VECTORS\nIn this section, we will review and elaborate upon the in-\nformation conveyed by the class-vectors. For each class,\nthe corresponding value in the vector accounts for the rela-\ntive duration of the piece which is interpretable in terms of\nthe speciﬁc class, that means, the proportion of time points\nwhich are contained in some (at least one) instance of the\nclass. A dataset of class-vectors, thus, can be exploited\nin a variety of ways. Finding speciﬁc sonorities in large\ndatasets can be combined with the extraction of the actual\nsegments from the MIDI ﬁles. This can be exploited in\nvaried applications, ranging from corpora analysis to mu-\nsic education.\nA dataset of class-vectors was computed from 13480\nMIDI tracks, including works by Alb ´eniz, Albinoni, Alkan,\nBach, Beethoven, Brahms, Bruckner, Busoni, Buxtehude,\nByrd, Chopin, Clementi, Corelli, Couperin, Debussy, Dow-\nland, Frescobaldi, Gesualdo, Guerrero, Haydn, Josquin,\nLasso, Liszt, Lully, Mahler, Morales, Mozart, Pachelbel,\nPalestrina, Satie, Scarlatti, Shostakovich, Schumann, Scri-\nabin, Soler, Stravinsky, Tchaikovsky, Telemann, Victoria\nand Vivaldi. It also includes anonymous medieval pieces,\nchurch hymns, and the Essen folksong collection.\n5.1 Case study: query by set-class\nA simple but useful application is querying the dataset for\na given set-class sonority. It can be used, for instance, to\nﬁnd pieces with a relevant presence of exotic scales. Ta-\nble 1 shows 10 retrieved pieces with a notable presence\n(relative duration) of the sonority 7-22, usually referred to\nas the Hungarian minor scale.8Both monophonic and\npolyphonic pieces are retrieved, ranging different styles\n7SeeSupplemental material (Section 7).\n8Sometimes also called Persian, major gypsy, or double harmonic\nscale, among other denominations.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n222and historic periods, as the unique requisite for capturing a\ngiven sonority it its existence as a temporal segment.\nretrieved piece 7-22 (%)\nScriabin - Prelude op.33 n.3 68.61\nBusoni - 6 etudes op.16 n.4 63.22\nEssen - 6478 62.50\nLiszt - Nuages gris 42.41\nEssen - 531 36.67\nScriabin - Prelude op.51 n.2 31.74\nLully - Persee act-iv-scene-iv-28 29.73\nAlkan - Esquisses op.63 n.19 28.87\nSatie - Gnossienne n.1 28.15\nScriabin - Mazurka op.3 n.9 24.61\nTable 1: Retrieved pieces: 7-22\n5.1.1 Query by combined set-classes\nThe strict separation of classes in the class-vectors, allows\nthe exploration of any class combination, whether common\nor unusual. For instance, the ﬁrst movement of Stravin-\nsky’s Symphony of psalms is retrieved by querying for mu-\nsic containing substantial diatonic (7-35) and octatonic (8-\n28) material, certainly an uncommon musical combina-\ntion. The class-vector also reveals the balance between\nboth sonorities, as 30.18 % and 29.25 % of the piece du-\nration, respectively. As discussed in Section 4, the class-\nmatrices allow the hierarchical analysis of speciﬁc sonori-\nties. The class-vectors, on the other hand, summarise the\ninformation in a way in which it is not possible, in general,\nto elucidate the subclass content under a given class. How-\never, if the queried sonorities have a substantial presence\n(or absence) in the piece, the class-vectors alone can of-\nten account for some hierarchical evidence. Table 2 shows\n10 retrieved pieces, characterised by a notable presence\nof the so-called suspended trichord (3-9),9constrained to\ncases of mostly diatonic contexts (7-35). This situation,\nas reﬂected in the results, is likely to be found in me-\ndieval melodies, early counterpoint, or works composed\nas reminiscent of them. It is worth noting that the 3-9 in-\nstantiations appear in quite different settings, whether in\nmonophonic voices, as a combination of melody and tonic-\ndominant drones, and as actual suspended (voiced) chords.\nretrieved piece 3-9 (%) 7-35 (%)\nAnonym - Angelus ad virginem 1 56.79 100\nAnonym - Instrumental dances 7 50.41 100\nLully - Persee prologue-3c 47.11 100\nLully - Phaeton acte-i-scene-v 45.95 90.81\nLully - Persee prologue-3b 44.36 100\nAnonym - Ductia 43.82 100\nAnonym - Danse royale 35.58 100\nAnonym - Cantigas de Santa Maria 2 32.06 100\nAnonym - Instrumental dances 9 27.43 100\nFrescobaldi - Canzoni da sonare-11 26.69 81.34\nTable 2: Retrieved pieces: mostly 7-35 with 3-9.\nAs non-existing sonorities may also reveal important\ncharacteristics of music, the dataset can be queried for com-\nbinations of present and absent classes. For instance, the\n9A major trichord with the third degree substituted by the fourth.sonority of fully diatonic (7-35) pieces depends on whether\nthey contain major or minor trichords (3-11) or not. Re-\ntrieved pieces in the latter case (diatonic, not triadic) are\nmostly medieval melodies or early polyphonic pieces, prior\nto the establishment of the triad as a common sonority.\nThese results point to interesting applications related\nwith music similarity, such as music recommendation and\nmusic education. We ﬁnd of particular interest the poten-\ntial of retrieving pieces sharing relevant tonal-related prop-\nerties, but pertaining to different styles, composers, or his-\ntorical periods. Music similarity is, to a great extent, a\nhuman construct, as it depends on cultural factors and mu-\nsical background. It would thus be possible to learn to\nappreciate non familiar similarity criteria, which could be\nsuggested by music discovery or recommendation systems.\n5.2 On dimensionality and informativeness\nIn feature design, the ratio between the size of the feature\nspace and the informativeness of description is a relevant\nfactor. The class content of a piece, as described by its\nclass-vector, have 200, 223 or 351 dimensions, depend-\ning on the chosen equivalence (iv ,TnIorTn). Com-\npared with other tonal feature spaces, these dimensions\nmay seem quite large. However, the beneﬁts of class vec-\ntors are the systematicity, speciﬁcity and precision of the\ndescription. Several relevant differences with respect to\nother tonal-related features are to be noticed. A single\nclass-vector, computed after a fully systematic segmenta-\ntion, accounts for:\n1. Every different segment in the piece, regardless of\ntheir time position or duration. No sampling arte-\nfacts of any kind are introduced.\n2. Every possible sonority among the set-class space,\nwhich is complete. Every instantiation of every class\nis captured and represented.\n3. An objective and precise description of the set-class\nsonority. No probabilities or estimations are involved.\n4. A description in (high level) music theoretical terms,\nreadable and interpretable by humans. Set-classes\nconstitute a standard lexicon in music analysis.\n5. An objective quantiﬁcation of every possible sonor-\nity in terms of relative duration in the piece. No\nprobabilities or estimations are involved.\n6. A content-based, model-free, description of the piece.\nNeither statistics nor properties learned from datasets\nare involved.\n7. In cases of large presence or notable absences of\nsome sonorities, an approximation to the hierarchi-\ncal inclusion relations (fully available through the\nclass-matrices only).\nIn contrast, the most common tonal piecewise and la-\nbelwise feature (global key estimation) conveys:\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2231. A single label for the whole piece, often misleading\nfor music which modulates.\n2. 24 different labels, but actually two different sonori-\nties (major and minor), non representative of a vast\namount of music.\n3. An estimation of the key: not only because of the\ninherent ambiguity of tonality, but also because the\n(most often) limited tonal knowledge of the algo-\nrithms.\n4. A description in (high level) music theoretical terms,\nbut conveying very little musical information (e.g. at\ncompositional level).\n5. No quantiﬁcation, just a global label. At most, in-\ncluding an indicator of conﬁdence (in the descriptor\nterms), usually the key strength.\n6. A description based on speciﬁc models (e.g. proﬁl-\ning methods or rule-based), which do not generalize.\nSome models are trained from speciﬁc datasets, bi-\nasing the actual meaning of the descriptor.\n7. No access to the (very rich) hierarchical relations of\nthe piece’s tonality.\nWith this in mind, it seems to us that a piecewise de-\nscription in 200 dimensions is a reasonable trade-off be-\ntween size and informativeness. Considering the some-\nwhat sophisticated tonal information conveyed by the class-\nvectors, they may constitute a useful complementary fea-\nture for existing content-based metadata.\n6. CONCLUSIONS\nThe proposed systematic methodology for multi-scale set-\nclass analysis is purposeful for common music information\nretrieval applications. An appropriate mining of the class-\nmatrices can bring insights about the hierarchical relations\namong the sets, informing about the speciﬁc construction\nof the class sonorities. In combination with simple re-\ncurrence ﬁnding methods, the class-matrices can be used\nfor music structure analysis of complex music, beyond the\nscope of mainstream tonal features. The proposed class-\nvectors, as piecewise tonal summaries, convey a rich in-\nformation in terms of every possible class sonority. They\ncan be mined for querying tasks of some sophistication.\nTheir compromise between dimensionality and informa-\ntiveness, point to potential advances in music similarity\nand recommendation applications. The examples in this\nwork show that set-classes can inform about very differ-\nent music compositions, ranging simple folk tunes, early\npolyphony, common-practice period, exotic or uncommon\nscales, and atonal music. Besides our ongoing musico-\nlogical analyses, and current research with chroma-based\ntranscriptions from audio, future work may explore the po-\ntential of these methods in actual classiﬁcation and recom-\nmendation systems.7. SUPPLEMENTAL MATERIAL\nThe interactive potential of the methods discussed in this\nwork can be tested by our multi-scale set-class analysis\nprototype for Matlab, freely available from http://agustin-\nmartorell.weebly.com/set-class-analysis.html. A compre-\nhensive table of set-classes, and a growing dataset of class-\nvectors, are also available at this site.\n8. ACKNOWLEDGEMENTS\nThis work was supported by the EU 7th Framework Pro-\ngramme FP7/2007-2013 through PHENICX project [grant\nno. 601166].\n9. REFERENCES\n[1] M. Babbit: “Some Aspects of Twelve-Tone Composi-\ntion,” The Score and I.M.A. Magazine, V ol. 12, pp. 53–\n61, 1955.\n[2] N. Cook: A Guide to Music Analysis, J. M. Dent and\nSons, London, 1987.\n[3] J. Foote: “Visualizing Music and Audio Using Self-\nSimilarity,” Proceedings of the ACM Multimedia,\npp. 77–80, 1999.\n[4] A. Forte: “A Theory of Set-Complexes for Music,”\nJournal of Music Theory , V ol. 8, No. 2 pp. 136–183,\n1964.\n[5] H. Hanson: The Harmonic Materials of Modern Mu-\nsic: Resources of the Tempered Scale, Appleton-\nCentury-Crofts, New York, 1960.\n[6] E. Huovinen and A. Tenkanen: “Bird’s-Eye Views of\nthe Musical Surface: Methods for Systematic Pitch-\nClass Set Analysis,” Music Analysis, V ol. 26, No. 1–2\npp. 159–214, 2007.\n[7] D. Lewin: “Re: Intervallic Relations between Two\nCollections of Notes,” Journal of Music Theory, V ol. 3,\nNo. 2 pp. 298–301, 1959.\n[8] A. Martorell and E. G ´omez: “Hierarchical Multi-Scale\nSet-Class Analysis,” Journal of Mathematics and Mu-\nsic, Online pp. 1-14, 2014.\n[9] M. M ¨uller: Information Retrieval for Music and Mo-\ntion, Springer, Berlin, 2007.\n[10] T. N. Rive: “An Examination of Victoria’s Technique\nof Adaptation and Reworking in his Parody Masses -\nwith Particular Attention to Harmonic and Cadential\nProcedure,” Anuario Musical, V ol. 24, pp. 133–152,\n1969.\n[11] C. S. Sapp: “Visual Hierarchical Key Analysis,” Com-\nputers in Entertainment, V ol. 4, No. 4 pp. 1–19, 2005.\n[12] J. N. Straus: Introduction to Post-Tonal Theory,\nPrentice-Hall, Upper Saddle River, NJ, 1990.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n224"
    },
    {
        "title": "Spotting a Query Phrase from Polyphonic Music Audio Signals Based on Semi-supervised Nonnegative Matrix Factorization.",
        "author": [
            "Taro Masuda",
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Shigeo Morishima"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415780",
        "url": "https://doi.org/10.5281/zenodo.1415780",
        "ee": "https://zenodo.org/records/1415780/files/MasudaYGM14.pdf",
        "abstract": "This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The “phrase” in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals con- sisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the dis- tance between a query and partial components of a musi- cal piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The ex- perimental results showed that our method outperformed conventional matching methods.",
        "zenodo_id": 1415780,
        "dblp_key": "conf/ismir/MasudaYGM14",
        "keywords": [
            "query-by-audio",
            "detect temporal locations",
            "musical phrase",
            "query",
            "mixture signals",
            "gamma process nonnegative matrix factorization",
            "spectrogram",
            "basis spectra",
            "activation patterns",
            "distance calculation"
        ],
        "content": "SPOTTING A QUERYPHRASE FROMPOLYPHONICMUSIC AUDIO\nSIGNALSBASEDON SEMI-SUPERVISEDNONNEGATIVEMATRIX\nFACTORIZATION\nTaroMasuda1KazuyoshiYoshii2Masataka Goto3ShigeoMorishima1\n1WasedaUniversity2KyotoUniversity\n3NationalInstitute of AdvancedIndustrial Science and Technology(AIST)\nmasutaro@suou.waseda.jp yoshii@i.kyoto-u.ac.jp m.goto@aist.go.jp shigeo@waseda.jp\nABSTRACT\nThis paper proposes a query-by-audio system that aims to\ndetect temporal locations where a musical phrase given as\na query is played in musical pieces. The “phrase” in this\npaper means a short audio excerpt that is not limited to\na main melody (singing part) and is usually played by a\nsingle musical instrument. A main problem of this task\nis that the query is often buried in mixture signals con-\nsisting of various instruments. To solve this problem, we\npropose a method that can appropriately calculate the dis-\ntance between a query and partial components of a musi-\ncal piece. More speciﬁcally, gamma process nonnegative\nmatrix factorization (GaP-NMF) is used for decomposing\nthespectrogramofthequeryintoanappropriatenumberof\nbasisspectraandtheiractivationpatterns. Semi-supervised\nGaP-NMFisthenusedforestimatingactivationpatternsof\nthelearnedbasisspectrainthemusicalpiecebypresuming\nthepiece to partially consistof those spectra. Thisenables\ndistance calculation based on activation patterns. The ex-\nperimental results showed that our method outperformed\nconventionalmatching methods.\n1. INTRODUCTION\nOver a decade, a lot of effort has been devoted to devel-\noping music information retrieval (MIR) systems that aim\nto ﬁnd musical pieces of interest by using audio signals as\nthequery. Forexample,therearemanysimilarity-basedre-\ntrieval systems that can ﬁnd musical pieces having similar\nacousticfeaturestothoseofthequery[5,13,21,22]. Audio\nﬁngerprintingsystems,ontheotherhand,trytoﬁndamu-\nsicalpiecethatexactlymatchesthequerybyusingacoustic\nfeatures robust to audio-format conversion and noise con-\ntamination[6,12,27]. Query-by-humming(QBH)systems\ntry to ﬁnd a musical piece that includes the melody speci-\nﬁed by users’ singing or humming [19]. Note that in gen-\nc⃝Taro\nMasuda,KazuyoshiYoshii,MasatakaGoto,Shigeo\nMorishima.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Taro Masuda, Kazuyoshi Yoshii,\nMasatakaGoto,ShigeoMorishima. “SpottingaQueryPhrasefromPoly-\nphonicMusicAudioSignalsBasedonSemi-supervisedNonnegativeMa-\ntrix Factorization”, 15th International Society for Music Information Re-\ntrievalConference, 2014.\nTime\nSimilarityMusical piece\nQuery\nphrase\nLocation of the queryFigure\n1. An overviewof the proposed method.\neral information of musical scores [9,16,23,31,39] (such\nas MIDI ﬁles) or some speech corpus [36] should be pre-\npared for a music database in advance of QBH. To over-\ncome this limitation, some studies tried to automatically\nextract main melodies from music audio signals included\nin a database [25,34,35]. Other studies employ chroma\nvectorstocharacterizeaqueryandtargetedpieceswithout\nthe need of symbolic representation or transcription [2].\nWe propose a task that aims to detect temporal loca-\ntions at which phrases similar to the query phrase appear\nin different polyphonic musical pieces. The term “phrase”\nmeans a several-second musical performance (audio clip)\nusually played by a single musical instrument. Unlike\nQBH, our method needs no musical scores beforehand.\nA key feature of our method is that we aim to ﬁnd short\nsegments within musical pieces, not musical pieces them-\nselves. There are several possible application scenarios in\nwhich both non-experts and music professionals enjoy the\nbeneﬁts of our system. For example, ordinary users could\nintuitivelyﬁndamusicalpiecebyplayingjustacharacter-\nisticphraseusedinthepieceevenifthetitleofthepieceis\nunknown or forgotten. In addition, composers could learn\nwhat kinds of arrangements are used in existing musical\npieces that include a phrase speciﬁed as a query.\nThe major problem of our task lies in distance calcu-\nlation between a query and short segments of a musical\npiece. One approach would be to calculate the symbolic\ndistance between musical scores. However, this approach\nis impractical because even the state-of-the-art methods of\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n227automaticmusic\ntranscription[4,11,17,29,38]workpoorly\nforstandardpopularmusic. Conventionaldistancecalcula-\ntionbasedonacousticfeatures[5]isalsoinappropriatebe-\ncauseacousticfeaturesofaphrasearedrasticallydistorted\nif other sounds are superimposed in a musical piece. In\naddition, since it would be more useful to ﬁnd locations in\nwhich the same phrase is played by different instruments,\nwe cannot heavilyrely on acoustic features.\nIn this paper we propose a novel method that can per-\nformphrasespottingbycalculatingthedistancebetweena\nqueryand partialcomponentsofamusicalpiece. Ourcon-\njectureisthatwecouldjudgewhetheraphraseisincluded\nornotinamusicalpiecewithoutperfecttranscription,like\nthehumanearcan. Morespeciﬁcally,gammaprocessnon-\nnegative matrix factorization (GaP-NMF) [14] is used for\ndecomposing the spectrogram of a query into an appropri-\nate number of basis spectra and their activation patterns.\nSemi-supervisedGaP-NMFisthenusedforestimatingac-\ntivation patterns of the ﬁxed basis spectra in a target mu-\nsical piece by presuming the piece to partially consist of\nthosespectra. This enables appropriate matchingbased on\nactivationpatterns of the basis spectra forming the query.\n2. PHRASE SPOTTINGMETHOD\nThis section describes the proposed phrase-spotting\nmethod based on nonparametric Bayesian NMF.\n2.1 Overview\nOurgoalistodetectthestarttimesofaphraseinthepoly-\nphonicaudiosignalofamusicalpiece. Anoverviewofthe\nproposed method is shown in Figure 1. Let X2RM\u0002Nx\nandY2RM\u0002Nybe the nonnegative power spectrogram\nof a query and that of a target musical piece, respectively.\nOurmethodconsistsofthreesteps. First,weperformNMF\nfor decomposing the query Xinto a set of basis spectra\nW(x)and a set of their corresponding activations H(x).\nSecond, in order to obtain temporal activations of W(x)\nin the musical piece Y, we perform another NMF whose\nbasis spectra consist of a set of ﬁxed basis spectra W(x)\nand a set of unconstrained basis spectra W(f)that are re-\nquired for representing musical instrument sounds except\nforthephrase. Let H(y)andH(f)besetsofactivationsof\nYcorresponding to W(x)andW(f), respectively. Third,\nthe similarity between the activation patterns H(x)in the\nqueryandtheactivationpatterns H(y)inthemusicalpiece\niscalculated. Finally,wedetectlocationsofaphrasewhere\nthe similarity takeslargevalues.\nThere are two important reasons that “nonparametric”\n“Bayesian”NMFisneeded. 1)Itisbettertoautomatically\ndetermine the optimal number of basis spectra according\nto the complexity of the query Xand that of the musical\npieceY. 2) We need to put different prior distributions\nonH(y)andH(f)to put more emphasis on ﬁxed basis\nspectra W(x)than unconstrained basis spectra W(f). If\nno priors are placed, the musical piece Yis often repre-\nsented by using only unconstrained basis spectra W(f).\nA key feature of our method is that we presumethat thephraseisincludedinthemusicalpiecewhendecomposing\nY. Thismeansthatweneedtomakeuseof W(x)asmuch\nas possible for representing Y. The Bayesian framework\nis a natural choice for reﬂecting such a prior belief.\n2.2 NMF forDecomposing a Query\nWe use the gamma process NMF (GaP-NMF) [14] for\napproximating Xas the product of a nonnegative vector\n\u00122RKxand two nonnegative matrices W(x)2RM\u0002Kx\nandH(x)2RKx\u0002Nx. More speciﬁcally, the original ma-\ntrixXis factorizedas follows:\nXmn\u0019Kx∑\nk=1\u0012kW(x)\nmkH(x)\nkn; (1)\nwhere \u0012kis the overall gain of basis k,W(x)\nmkis the power\nof basis kat frequency m, and H(x)\nknis the activation of\nbasiskattime n. Eachcolumnof W(x)representsabasis\nspectrum and each row of H(x)represents an activation\npattern of the basis overtime.\n2.3 Semi-supervisedNMF forDecomposing a Musical\nPiece\nWe then perform semi-supervised NMF for decomposing\nthe spectrogram of the musical piece Yby ﬁxing a part\nof basis spectra with W(x). The idea of giving Was a\ndictionary during inference has been widely adopted [3,7,\n15,18,24,26,28,30,33,38].\nWeformulateBayesianNMFforrepresentingthespec-\ntrogram of the musical piece Yby extensively using the\nﬁxed bases W(x). To do this, we put different gamma pri-\norsonH(y)andH(f). Theshapeparameterofthegamma\nprior on H(y)is much larger than that of the gamma prior\nonH(f). Notethattheexpectationofthegammadistribu-\ntion is proportional to its shape parameter.\n2.4 CorrelationCalculation between Activation\nPatterns\nAfterthesemi-supervisedNMFisperformed,wecalculate\nthe similarity between the activation patterns H(x)in the\nqueryandtheactivationpatterns H(y)inamusicalpieceto\nﬁndlocationsofthephrase. Weexpectthatsimilarpatterns\nappearin H(y)whenalmostthesamephrasesareplayedin\nthemusicalpieceevenifthosephrasesareplayedbydiffer-\nentinstruments. Morespeciﬁcally,wecalculatethesumof\nthe correlation coefﬁcients rat time nbetween H(x)and\nH(y)as follows:\nr(n) =1\nKxNxKx∑\nk=1(\nh(x)\nk1\u0000h(x)\nk1)T(\nh(y)\nkn\u0000h(y)\nkn)\n\r\r\rh(x)\nk1\u0000h(x)\nk1\r\r\r\r\r\rh(y)\nkn\u0000h(y)\nkn\r\r\r;(2)\nwhere\nh(\n\u0001)\nki=[\nH(\u0001)\nki\u0001\u0001\u0001H(\u0001)\nk(i+Nx\u00001)]T\n; (3)\nh(\u0001)\nkn =1\nNxNx∑\nj=1H(\u0001)\nk(n+j\u00001)\u0002[1\u0001\u0001\n\u00011]T:(4)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n228Finally,\nwe detect a start frame nof the phrase by ﬁnding\npeaks of the correlation coefﬁcients over time. This peak\npicking is performed based on the following thresholding\nprocess:\nr(n)> \u0016 + 4\u001b; (5)\nwhere \u0016and\u001bdenote the overall mean and standard de-\nviation of r(n), respectively, which were derived from all\nthe musical pieces.\n2.5 VariationalInferenceof GaP-NMF\nThis section brieﬂy explains how to infer nonparametric\nBayesian NMF [14], given a spectrogram V2RM\u0002N.\nWe assume that \u00122RK,W2RM\u0002K, andH2RK\u0002N\nare stochastically sampled according to a generative pro-\ncess. We choose a gamma distribution as a prior distribu-\ntion on each parameter as follows:\np(Wmk) = Gamma(\na(W); b(W))\n;\np(Hkn) = Gamma(\na(H); b(H))\n;(6)\np(\u0012k) = Gamma(\u000b\nK; \u000b\nc)\n;\nwhere \u000bis a concentration parameter, Kis a sufﬁciently\nlarge integer (ideally an inﬁnite number) compared with\nthenumberofcomponentsinthemixedsound,and cisthe\ninverseof the mean valueof V:\nc=(\n1\nMN∑\nm∑\nnVmn)\u0000\n1\n: (7)\nWe then use the generalized inverse-Gaussian (GIG) dis-\ntributionas a posterior distributionas follows:\nq(Wmk) = GIG(\n\r(W)\nmk; \u001a(W)\nmk; \u001c(W)\nmk)\n;\nq(Hkn) = GIG(\n\r(H)\nkn; \u001a(H)\nkn; \u001c(H)\nkn)\n;(8)\nq(\u0012k) = GIG(\n\r(\u0012)\nk; \u001a(\u0012)\nk; \u001c(\u0012)\nk)\n:\nTo estimate the parameters of these distributions, we ﬁrst\nupdate other parameters, ϕkmn; !mn, using the following\nequations.\nϕkmn =Eq[1\n\u0012kWmkHkn]\u0000\n1\n; (9)\n!mn =∑\nkEq[\u0012kWmkHkn]:(10)After obtaining ϕkmnand!mn, we update the parameters\nof the GIG distributionsas follows:\n\r(W)\nmk=a(W); \u001a(W)\nmk=b(W)+Eq[\u0012k]∑\nnEq[Hkn]\n!mn;\n\u001c(W)\nmk=Eq[1\n\u0012k]∑\nnVmnϕ2\nkmnEq[1\nHkn]\n;(11)\n\r(\nH)\nkn=a(H); \u001a(H)\nkn=b(H)+Eq[\u0012k]∑\nmEq[Wmk]\n!mn;\n\u001c(H)\nkn=Eq[1\n\u0012k]∑\nmVmnϕ2\nkmnEq[1\nWmk]\n;(12)\n\r(\u0012)\nk=\u000b\nK; \u001a(\n\u0012)\nk=\u000bc+∑\nm∑\nnEq[WmkHkn]\n!mn;\n\u001c(\u0012)\nk=∑\nm∑\nnVmnϕ2\nkmnEq[1\nWmkHkn]\n:(13)\nThe\nexpectations of W,Hand\u0012are required in Eqs. (9)\nand (10). We randomly initialize the expectations of W,\nH, and\u0012and iteratively update each parameter by using\nthose formula. As the number of iterations increases, the\nvalue of Eq[\u0012k]over a certain level K+decreases. There-\nfore, if the value is 60 dB lower than∑\nkEq[\u0012k], we re-\nmove the related parameters from consideration, which\nmakes the calculation faster. Eventually, the number of\neffective bases, K+, gradually reduces during iterations,\nsuggestingthattheappropriatenumberisautomaticallyde-\ntermined.\n3. CONVENTIONALMATCHINGMETHODS\nWe describe three kinds of conventional matching meth-\nods used for evaluation. The ﬁrst and the second methods\ncalculate the Euclidean distance between acoustic features\n(Section 3.1) and that between chroma vectors (Section\n3.2),respectively. ThethirdmethodcalculatestheItakura-\nSaito (IS) divergencebetween spectrograms (Section 3.3).\n3.1 MFCC Matching Based on Euclidean Distance\nTemporal locations in which a phrase appears are detected\nby focusing on the acoustic distance between the query\nand a short segment extracted from a musical piece. In\nthis study we use Mel-frequency cepstrum coefﬁcients\n(MFCCs) as an acoustic feature, which have commonly\nbeen used in various research ﬁelds [1,5]. More specif-\nically, we calculate a 12-dimensional feature vector from\neach frame by using the Auditory Toolbox Version 2 [32].\nThe distance between two sequences of the feature vector\nextractedfromthequeryandtheshortsegmentisobtained\nby accumulating the frame-wise Euclidean distance over\nthe length of the query.\nThe above-mentioned distance is iteratively calculated\nbyshiftingthequeryframebyframe. Usingasimplepeak-\npickingmethod,wedetectlocationsofthephraseinwhich\nthe obtained distance is lower than m\u0000s, where mand\nsdenote the mean and standard deviation of the distance\noverall frames, respectively.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2293.2 Chr\nomagramMatching Based on Euclidean\nDistance\nIn this section, temporal locations in which a phrase ap-\npearsaredetectedinthesamemannerasexplainedinSec-\ntion3.1. Adifferenceisthatweextracteda12-dimentional\nchroma vector from each frame by using the MIRtool-\nbox[20]. Inaddition,weempiricallydeﬁnedthethreshold\nof the peak-picking method as m\u00003s.\n3.3 DP Matching Based on Itakura-Saito Divergence\nIn this section, temporal locations in which a phrase ap-\npears are detected by directly calculating the Itakura-Saito\n(IS)divergence[8,37]betweenthequery Xandthemusi-\ncal piece Y. The use of the IS divergence is theoretically\njustiﬁedbecausetheISdivergenceposesasmallerpenalty\nthanstandarddistancemeasuressuchastheEuclideandis-\ntance and the Kullback-Leibler (KL) divergence when the\npower spectrogram of the query is included in that of the\nmusical piece.\nTo efﬁciently ﬁnd phrase locations, we use a dynamic\nprogramming (DP) matching method based on the IS di-\nvergence. First, we make a distance matrix D2RNx\u0002Ny\ninwhicheachcell D(i; j)istheISdivergencebetweenthe\ni-th frame of Xandthe j-th frame of Y(1\u0014i\u0014Nxand\n1\u0014j\u0014Ny).D(i; j)is givenby\nD(i; j) =DIS(XijYj) =∑\nm(\n\u0000logXmi\nYmj+Xmi\nYmj\u00001)\n;\n(14)\nwhere mindicates a\nfrequency-bin index. We then let\nE2RNx\u0002Nybe a cumulative distance matrix. First, E\nisinitializedas E(1; j) = 0forany jandE(i;1) =1for\nanyi.E(i; j)can be sequentially calculated as follows:\nE(i; j) = min8\n<\n:1)E(i\u00001; j\u00002) + 2 D(i; j\u00001)\n2)E(i\u00001; j\u00001) +D(i; j)\n3)E(i\u00002; j\u00001) + 2 D(i\u00001; j)9\n=\n;\n+D(i; j):(15)\nFinally, we can obtain E(Nx; j)that represents the dis-\ntance between the query and a phrase ending at the j-th\nframe in the musical piece. We let C2RNx\u0002Nybe a cu-\nmulative cost matrix. According to the three cases 1), 2),\nand 3), C(i; j)is obtained as follows:\nC(i; j) =8\n<\n:1)C(i\u00001; j\u00002) + 3\n2)C(i\u00001; j\u00001) + 2\n3)C(i\u00002; j\u00001) + 3:(16)\nThis means that the length of a phrase is allowed to range\nfrom one half to twotimes of the query length.\nPhrase locations are determined by ﬁnding the local\nminimaoftheregularizeddistancegivenbyE(Nx;j)\nC(Nx;j). More\nspeciﬁcally\n, we detect locations in which values of the ob-\ntaineddistancearelowerthan M\u0000S=10,where MandS\ndenote the median and standard deviation of the distance\noverallframes,respectively. Areasonthatweusetheme-\ndian for thresholding is that the distance sometimes takesan extremely large value (outlier). The mean of the dis-\ntance tends to be excessively biased by such an outlier. In\naddition, we ignore values of the distance which are more\nthan 106when calculating Sfor practical reasons (almost\nallvaluesofE(Nx;j)\nC(Nx;j)rangefrom 103to104).\nOncetheend\npoint is detected, we can also obtain the start point of the\nphrase by simply tracing back along the path from the end\npoint.\n4. EXPERIMENTS\nThis section reports comparative experiments that were\nconductedforevaluatingthephrase-spottingperformances\nof the proposed method described in Section 2 and the\nthree conventionalmethods described in Section 3.\n4.1 Experimental Conditions\nThe proposed method and the three conventional methods\nweretestedunderthreedifferentconditions: 1)Exactlythe\nsamephrasespeciﬁedasaquerywasincludedinamusical\npiece (exact match). 2) A query was played by a different\nkind of musical instruments (timbre change). 3) A query\nwasplayed in a fastertempo (tempo change).\nWe chose four musical pieces (RWC-MDB-P-2001\nNo.1, 19, 42, and 77) from the RWC Music Database:\nPopular Music [10]. We then prepared 50 queries: 1) 10\nwere short segments excerpted from original multi-track\nrecordings of the four pieces. 2) 30 queries were played\nby three kinds of musical instruments (nylon guitar, clas-\nsicpiano,andstrings)thatweredifferentfromthoseorigi-\nnally used in the four pieces. 3) The remaining 10 queries\nwere played by the same instruments as original ones, but\ntheir tempi were 20% faster. Each query was a short per-\nformanceplayedbyasingleinstrumentandhadaduration\nranging from 4 s to 9 s. Note that those phrases were not\nnecessarily salient (not limited to main melodies) in musi-\ncal pieces. We dealt with monaural audio signals sampled\nat 16 kHz and applied the wavelet transform by shifting\nshort-time frames with an interval of 10 ms. The reason\nthat we did not use short-time Fourier transform (STFT)\nwas to attain a high resolution in a low frequency band.\nWe determined the standard deviation of a Gabor wavelet\nfunction to 3.75 ms (60 samples). The frequency interval\nwas 10 cents and the frequency ranged from 27.5 (A1) to\n8000 (much higher than C8) Hz.\nWhen a query was decomposed by NMF, the hyperpa-\nrameters were set as \u000b= 1, K= 100, a(W)=b(W)=\na(H)= 0:1, and b(H(x))=c. When a musical piece\nwas decomposed by semi-supervised NMF, the hyperpa-\nrameters were set as a(W)=b(W)= 0: 1,a(H(y))= 10,\na(H(f))= 0:01,and b(H)=c. Theinverse-scaleparameter\nb(H)wasadjustedtotheempiricalscaleofthespectrogram\nofatargetaudiosignal. Alsonotethatusingsmallervalues\nofa(\u0001)makesparameters sparser in an inﬁnite space.\nTo evaluate the performance of each method, we calcu-\nlated the average F-measure, which has widely been used\nintheﬁeldofinformationretrieval. Theprecisionratewas\ndeﬁned as a proportion of the number of correctly-found\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n230Precision(%) Recall(%) F-measure (%)\nMFCC 24.8 35.0 29.0\nChroma 33.4 61.0 43.1\nDP 1.9 55.0 3.6\nProposed 53.6 63.0 57.9\nTable\n1. Experimental results in a case that exactly the\nsamephrasespeciﬁedasaquerywasincludedinamusical\npiece.\nPrecision(%) Recall(%) F-measure (%)\nMFCC 0 0 0\nChroma 18.1 31.7 23.0\nDP 1.1 66.3 6.2\nProposed 26.9 56.7 36.5\nTable\n2. Experimental results in a case that a query was\nplayed by a differentkind of instruments.\nPrecision(%) Recall(%) F-measure (%)\nMFCC 0 0 0\nChroma 12.0 19.0 14.7\nDP 0.5 20.0 2.7\nProposed 15.8 45.0 23.4\nTable\n3. Experimental results in a case that the query\nphrases wasplayed in a fastertempo.\nphrases to that of all the retrieved phrases. The recall rate\nwas deﬁned as a proportion of the number of correctly-\nfoundphrasestothatofallphrasesincludedinthedatabase\n(each query phrase was included only in one piece of mu-\nsic). Subsequently, we calculated the F-measure Fby\nF=2PR\nP+R, where PandRdenote\nthe precision and re-\ncall rates, respectively. We regarded a detected point as a\ncorrect one when its error is within 50 frames (500 ms).\n4.2 Experimental Results\nTables1–3showtheaccuraciesobtainedbythefourmeth-\nods under each condition. We conﬁrmed that our method\nperformed much better than the conventional methods in\nterms of accuracy. Figure 2 shows the value of r(n)ob-\ntainedfromamusicalpieceinwhichaqueryphrase(orig-\ninallyplayedbythesaxophone)isincluded. Wefoundthat\nthe points at which the query phrase starts were correctly\nspotted by using our method. Although the MFCC-based\nmethod could retrieve some of the query phrases in the\nexact-match condition, it was not robust to timbre change\nandtempochange. TheDPmatchingmethod,ontheother\nhand,couldretrieveveryfewcorrectpointsbecausetheIS\ndivergence was more sensitive to volume change than the\nsimilarity based on spectrograms. Although local minima\nof the cost function often existed at correct points, those\nminima were not sufﬁciently clear because it was difﬁcult\ntodetecttheendpointofthequeryfromthespectrogramof\namixtureaudiosignal. Thechroma-basedmethodworked\nbetter than the other conventional methods. However, it\ndidnotoutperformtheproposedmethodsincethechroma-\n(a) \n(b) \n(c) \n(b) Figure\n2. Sum of the correlation coefﬁcients r(n). The\ntargetpiecewasRWC-MDB-P-2001No.42. (a)Thequery\nwas exactly the same as the target saxophone phrase. (b)\nThequerywasplayedbystrings. (c)Thequerywasplayed\n20% fasterthan the target.\nbased method often detected false locations including a\nsimilar chord progression.\nAlthoughourmethodworkedbestofthefour,theaccu-\nracy of the proposed method should be improved for prac-\ntical use. A major problem is that the precision rate was\nrelatively lower than the recall rate. Wrong locations were\ndetectedwhenquerieswereplayedin staccatomannerbe-\ncause many false peaks appeared at the onset of staccato\nnotes.\nAs for computational cost, it took 29746 seconds to\ncomplete the retrieval of a single query by using our\nmethod. This was implemented in C++ on a 2.93 GHz\nIntel Xeon Windows7 with 12 GB RAM.\n5. CONCLUSION AND FUTURE WORK\nThis paper presented a novel query-by-audio method that\ncan detect temporal locations where a phrase given as\na query appears in musical pieces. Instead of pursuing\nperfect transcription of music audio signals, our method\nused nonnegative matrix factorization (NMF) for calculat-\ningthedistancebetweenthequeryandpartialcomponents\nof each musical piece. The experimental results showed\nthatourmethodperformedbetterthanconventionalmatch-\ning methods. We found that our method has a potential to\nﬁnd correct locations in which a query phrase is played by\ndifferent instruments (timbre change) or in a faster tempo\n(tempo change).\nFuture work includes improvement of our method, es-\npecially under the timbre-change and tempo-change con-\nditions. One promising solution would be to classify basis\nspectra of a query into instrument-dependent bases ( e.g.,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n231noise from\nthe guitar) and common ones (e.g. , harmonic\nspectracorrespondingtomusicalnotes)ortocreateanuni-\nversal set of basis spectra. In addition, we plan to reduce\nthe computational cost of our method based on nonpara-\nmetric Bayesian NMF.\nAcknowledgment: This study was supported in part by the JST\nOngaCRESTproject.\n6. REFERENCES\n[1] J. J. Aucouturier and F. Pachet. Music Similarity Measures:\nWhat’sthe Use?, ISMIR,pp. 157–163, 2002.\n[2] C. de la Bandera, A. M. Barbancho, L. J. Tard ´on, S. Sam-\nmartino, and I. Barbancho. Humming Method for Content-\nBasedMusicInformationRetrieval, ISMIR,pp.49–54,2011.\n[3] L. Benaroya, F. Bimbot, and R. Gribonval. Audio Source\nSeparation with a Single Sensor, IEEE Trans. on ASLP ,\n14(1):191–199,2006.\n[4] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and A.\nKlapuri.AutomaticMusicTranscription: BreakingtheGlass\nCeiling,ISMIR,pp. 379–384, 2012.\n[5] A. Berenzweig, B. Logan, D. P. Ellis, and B. Whitman. A\nLarge-Scale Evaluation of Acoustic and Subjective Music-\nSimilarityMeasures, ComputerMusicJournal,28(2):63–76,\n2004.\n[6] P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A Review\nof Audio Fingerprinting, Journal of VLSI Signal Processing\nSystemsforSignal,ImageandVideoTechnology,41(3):271–\n284,2005.\n[7] Z. Duan, G. J. Mysore, and P. Smaragdis. Online PLCA for\nReal-Time Semi-supervised Source Separation, Latent Vari-\nableAnalysisandSignalSeparation,SpringerBerlinHeidel-\nberg,pp. 34–41, 2012.\n[8] A. El-Jaroudi and J. Makhoul. Discrete All-Pole Modeling,\nIEEETrans.on Signal Processing, 39(2):411–423, 1991.\n[9] A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith. Query\nby Humming: Musical Information Retrieval in an Audio\nDatabase, ACMMultimedia, pp. 231–236, 1995.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Popular, Classical, and Jazz Music\nDatabases, ISMIR, pp. 287–288, 2002.\n[11] G. Grindlay and D. P. W. Ellis. A Probabilistic Subspace\nModel for Multi-instrument Polyphonic Transcription, IS-\nMIR,pp. 21–26, 2010.\n[12] J. Haitsma and T. Kalker. A Highly Robust Audio Finger-\nprintingSystem, ISMIR, pp. 107–115, 2002.\n[13] M. Hel ´en and T. Virtanen. Audio Query by Example Using\nSimilarity Measures between Probability Density Functions\nof Features, EURASIP Journal on Audio, Speech, and Music\nProcessing,2010.\n[14] M. D. Hoffman, D. M. Blei, and P. R. Cook. Bayesian Non-\nparametric Matrix Factorization for Recorded Music, ICML,\npp.439–446, 2010.\n[15] X. Jaureguiberry, P. Leveau, S. Maller, and J. J. Burred.\nAdaptation of source-speciﬁc dictionaries in Non-Negative\nMatrixFactorizationforsourceseparation, ICASSP,pp.5–8,\n2011.\n[16] T. Kageyama,K. Mochizuki, and Y.Takashima. Melody Re-\ntrievalwith Humming, ICMC,pp.349–351, 1993.\n[17] H. Kameoka, K. Ochiai, M. Nakano, M. Tsuchiya, and S.\nSagayama.Context-free2DTreeStructureModelofMusical\nNotes for Bayesian Modeling of Polyphonic Spectrograms,\nISMIR,pp. 307–312, 2012.[18] H. Kirchhoff, S. Dixon, and A. Klapuri. Multi-Template\nShift-variant Non-negative Matrix Deconvolution for Semi-\nautomatic Music Transcription, ISMIR,pp. 415–420, 2012.\n[19] A. Kotsifakos, P. Papapetrou, J. Hollm ´en, D. Gunopulos,\nand V. Athitsos. A Survey of Query-By-Humming Similar-\nity Methods, InternationalConferenceon PETRA ,2012.\n[20] O.LartillotandP.Toiviainen.AMatlabToolboxforMusical\nFeature Extraction from Audio, DAFx,pp. 237–244, 2007.\n[21] T.LiandM.Ogihara.Content-basedMusicSimilaritySearch\nand Emotion Detection, ICASSP,Vol.5, pp. 705–708, 2004.\n[22] B. Logan and A. Salomon. A Music Similarity Function\nBasedonSignalAnalysis, InternationalConferenceonMul-\ntimedia and Expo (ICME) ,pp. 745–748, 2001.\n[23] R. J. McNab, L. A. Smith, I. H. Witten, C. L. Henderson,\nand S. J. Cunningham. Towards the Digital Music Library:\nTuneRetrievalfromAcousticInput, ACMinternationalcon-\nferenceon Digital libraries,pp. 11–18, 1996.\n[24] G.J.MysoreandP.Smaragdis.ANon-negativeApproachto\nSemi-supervised Separation of Speech from Noise with the\nUse of TemporalDynamics, ICASSP, pp. 17–20, 2011.\n[25] T.Nishimura,H.Hashiguchi,J.Takita,J.X.Zhang,M.Goto,\nand R. Oka. Music Signal Spotting Retrieval by a Humming\nQueryUsingStartFrameFeatureDependentContinuousDy-\nnamic Programming, ISMIR,pp. 211–218, 2001.\n[26] A. Ozerov, P. Philippe, R. Gribonval, and F. Bimbot. One\nMicrophoneSingingVoiceSeparationUsingSource-adapted\nModels,WASPAA,pp. 90–93, 2005.\n[27] M. Ramona and G. Peeters. AudioPrint: An efﬁcient audio\nﬁngerprintsystembasedonanovelcost-lesssynchronization\nscheme,ICASSP,pp. 818–822, 2013.\n[28] S. T.Roweis.OneMicrophone Source Separation, Advances\nin Neural Information Processing Systems , Vol. 13, MIT\nPress, pp. 793–799, 2001.\n[29] M. Ryyn ¨anen and A. Klapuri. Automatic Bass Line Tran-\nscription from Streaming Polyphonic Audio, ICASSP, pp.\nIV–1437–1440,2007.\n[30] M. N. Schmidt and R. K. Olsson. Single-Channel Speech\nSeparation using Sparse Non-Negative Matrix Factorization,\nInterspeech,pp. 1652–1655, 2006.\n[31] J. Shifrin, B. Pardo, C. Meek, and W. Birmingham. HMM-\nBased Musical Query Retrieval, ACM/IEEE-CS Joint Con-\nferenceon Digital Libraries,pp. 295–300, 2002.\n[32] M. Slaney. Auditory Toolbox Version 2, Technical Report\n#1998-010 ,IntervalResearch Corporation, 1998.\n[33] P. Smaragdis, B. Raj, and M. Shashanka. Supervised and\nSemi-supervised Separation of Sounds from Single-Channel\nMixtures, Independent Component Analysis and Signal Sep-\naration,Springer Berlin Heidelberg,pp. 414–421, 2007.\n[34] C. J. Song, H. Park, C. M. Yang, S. J. Jang, and S. P. Lee.\nImplementation of a Practical Query-by-Singing/Humming\n(QbSH) System and Its Commercial Applications, IEEE\nTrans.on Consumer Electronics,59(2):407–414, 2013.\n[35] J. Song, S. Y. Bae, and K. Yoon. Mid-Level Music Melody\nRepresentationofPolyphonicAudioforQuery-by-Humming\nSystem,ISMIR,pp. 133–139, 2002.\n[36] C.C.Wang,J.S.R.Jang,andW.Wang.AnImprovedQuery\nby Singing/Humming System Using Melody and Lyrics In-\nformation, ISMIR, pp. 45–50, 2010.\n[37] B. Wei and J. D. Gibson. Comparison of Distance Measures\nin Discrete Spectral Modeling, IEEEDSP Workshop,2000.\n[38] F. Weninger, C. Kirst, B. Schuller, and H. J. Bungartz. A\nDiscriminative Approach to Polyphonic Piano Note Tran-\nscription Using Supervised Non-negative Matrix Factoriza-\ntion,ICASSP,pp. 26–31, 2013.\n[39] Y. Zhu and D. Shasha. Query by Humming: a Time Series\nDatabase Approach, SIGMOD, 2003.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n232"
    },
    {
        "title": "Analyzing Song Structure with Spectral Clustering.",
        "author": [
            "Brian McFee",
            "Dan Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415778",
        "url": "https://doi.org/10.5281/zenodo.1415778",
        "ee": "https://zenodo.org/records/1415778/files/McFeeE14.pdf",
        "abstract": "Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self- similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encod- ing of repetition structure, and exposes the hierarchical re- lationships among structural components at differing lev- els of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation.",
        "zenodo_id": 1415778,
        "dblp_key": "conf/ismir/McFeeE14",
        "keywords": [
            "self-similarity matrix",
            "time-series features",
            "spectral graph theory",
            "repeated patterns",
            "building blocks",
            "large-scale structure",
            "low-dimensional encoding",
            "structural components",
            "hierarchical relationships",
            "music segmentation"
        ],
        "content": "ANALYZING SONG STRUCTURE WITH SPECTRAL CLUSTERING\nBrian McFee\nCenter for Jazz Studies\nColumbia University\nbrm2132@columbia.eduDaniel P.W. Ellis\nLabROSA\nColumbia University\ndpwe@ee.columbia.edu\nABSTRACT\nMany approaches to analyzing the structure of a musical\nrecording involve detecting sequential patterns within a self-\nsimilarity matrix derived from time-series features. Such\npatterns ideally capture repeated sequences, which then\nform the building blocks of large-scale structure.\nIn this work, techniques from spectral graph theory are\napplied to analyze repeated patterns in musical recordings.\nThe proposed method produces a low-dimensional encod-\ning of repetition structure, and exposes the hierarchical re-\nlationships among structural components at differing lev-\nels of granularity. Finally, we demonstrate how to apply\nthe proposed method to the task of music segmentation.\n1. INTRODUCTION\nDetecting repeated forms in audio is fundamental to the\nanalysis of structure in many forms of music. While small-\nscale repetitions — such as instances of an individual chord\n— are simple to detect, accurately combining multiple small-\nscale repetitions into larger structures is a challenging al-\ngorithmic task. Much of the current research on this topic\nbegins by calculating local, frame-wise similarities over\nacoustic features (usually harmonic), and then searching\nfor patterns in the all-pairs self-similarity matrix [3].\nIn the majority of existing work on structural segmenta-\ntion, the analysis is ﬂat, in the sense that the representation\ndoes not explicitly encode nesting or hierarchical structure\nin the repeated forms. Instead, novelty curves are com-\nmonly used to detect transitions between sections.\n1.1 Our contributions\nIn this paper, we formulate the structure analysis problem\nin the context of spectral graph theory. By combining local\nconsistency cues with long-term repetition encodings and\nanalyzing the eigenvectors of the resulting graph Lapla-\ncian, we produce a compact representation that effectively\nencodes repetition structure at multiple levels of granular-\nity. To effectively link repeating sequences, we formulate\nan optimally weighted combination of local timbre consis-\ntency and long-term repetition descriptors.\nc\rBrian McFee, Daniel P.W. Ellis.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Brian McFee, Daniel P.W. Ellis. “Ana-\nlyzing song structure with spectral clustering”, 15th International Society\nfor Music Information Retrieval Conference, 2014.To motivate the analysis technique, we demonstrate its\nuse for the standard task of ﬂat structural annotation. How-\never, we emphasize that the approach itself can be applied\nmore generally to analyze structure at multiple resolutions.\n1.2 Related work\nThe structural repetition features used in this work are in-\nspired by those of Serr `aet al. [11], wherein structure is de-\ntected by applying ﬁltering operators to a lag-skewed self-\nsimilarity matrix. The primary deviation in this work is\nthe graphical interpretation and subsequent analysis of the\nﬁltered self-similarity matrix.\nRecently, Kaiser et al. demonstrated a method to com-\nbine tonal and timbral features for structural boundary de-\ntection [6]. Whereas their method forms a novelty curve\nfrom the combination of multiple features, our feature com-\nbination differs by using local timbre consistency to build\ninternal connections among sequences of long-range tonal\nrepetitions.\nOur general approach is similar in spirit to that of Gro-\nhganz et al. [4], in which diagonal bands of a self-similarity\nmatrix are expanded into block structures by spectral anal-\nysis. Their method analyzed the spectral decomposition\nof the self-similarity matrix directly, whereas the method\nproposed here operates on the graph Laplacian. Similarly,\nKaiser and Sikora applied non-negative matrix factoriza-\ntion directly to a self-similarity matrix in order to detect\nblocks of repeating elements [7]. As we will demonstrate,\nthe Laplacian provides a more direct means to expose block\nstructure at multiple levels of detail.\n2. GRAPHICAL REPETITION ENCODING\nOur general structural analysis strategy is to construct and\npartition a graph over time points (samples) in the song.\nLetX= [x1;x2;:::;xn]2Rd\u0002ndenote ad-dimensional\ntime series feature matrix, e.g., a chromagram or sequence\nof Mel-frequency cepstral coefﬁcients. As a ﬁrst step to-\nward detecting and representing repetition structure, we\nform a binary recurrence matrix R2f0;1gn\u0002n, where\nRij(X)\u0001\u0001=(\n1xi;xjare mutualk-nearest neighbors\n0otherwise;\n(1)\nandk>0parameterizes the degree of connectivity.\nIdeally, repeated structures should appear as diagonal\nstripes inR. In practice, it is beneﬁcial to apply a smooth-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n405ing ﬁlter to suppress erroneous links and ﬁll in gaps. We\napply a windowed majority vote to each diagonal of R, re-\nsulting in the ﬁltered matrix R0:\nR0\nij\u0001\u0001= majfRi+t;j +tjt2\u0000w;\u0000w+ 1;:::;wg;(2)\nwherewis a discrete parameter that deﬁnes the minimum\nlength of a valid repetition sequence.\n2.1 Internal connectivity\nThe ﬁltered recurrence matrix R0can be interpreted as an\nunweighted, undirected graph, whose vertices correspond\nto samples (columns of X), and edges correspond to equiv-\nalent position within a repeated sequence. Note, however,\nthat successive positions (i;i+ 1) will not generally be\nconnected in R0, so the constituent samples of a particular\nsequence may not be connected.\nTo facilitate discovery of repeated sections, edges be-\ntween adjacent samples (i;i+ 1) and(i;i\u00001)are intro-\nduced, resulting in the sequence-augmented graph R+:\n\u0001ij\u0001\u0001=(\n1ji\u0000jj= 1\n0otherwise; (3)\nR+\nij\u0001\u0001= max(\u0001 ij;R0\nij): (4)\nWith appropriate normalization, R+characterizes a Markov\nprocess over samples, where at each step i, the process ei-\nther moves to an adjacent sample i\u00061, or a random repeti-\ntion ofi; a process exempliﬁed by the Inﬁnite Jukebox [8].\nEquation (4) combines local temporal connectivity with\nlong-term recurrence information. Ideally, edges would\nexist only between pairs fi;jgbelonging to the same struc-\ntural component, but of course, this information is hidden.\nThe added edges along the ﬁrst diagonals create a fully\nconnected graph, but due to recurrence links, repeated sec-\ntions will exhibit additional internal connectivity. Let iand\njdenote two repetitions of the same sample at different\ntimes; thenR+should contain sequential edges fi;i+ 1g,\nfj;j+ 1gand repetition edges fi;jg,fi+ 1;j+ 1g. On\nthe other hand, unrelated sections with no repetition edges\ncan only connect via sequence edges.\n2.2 Balancing local and global linkage\nThe construction of eq. (4) describes the intuition behind\ncombining local sequential connections with global repe-\ntition structure, but it does not balance the two competing\ngoals. Long tracks with many repetitions can produce re-\ncurrence links which vastly outnumber local connectivity\nconnections. In this regime, partitioning into contiguous\nsections becomes difﬁcult, and subsequent analysis of the\ngraph may fail to detect sequential structure.\nIf we allow (non-negative) weights on the edges, then\nthe combination can be parameterized by a weighting pa-\nrameter\u00162[0;1]:\nR\u0016\nij\u0001\u0001=\u0016R0\nij+ (1\u0000\u0016)\u0001ij: (5)\nThis raises the question: how should \u0016be set? Return-\ning to the motivating example of the random walk, we optfor a process that on average, tends to move either in se-\nquence or across (all) repetitions with equal probability. In\nterms of\u0016, this indicates that the combination should as-\nsign equal weight to the local and repetition edges. This\nsuggests a balancing objective for all frames i:\n\u0016X\njR0\nij\u0019(1\u0000\u0016)X\nj\u0001ij:\nMinimizing the average squared error between the two terms\nabove leads to the following quadratic optimization:\nmin\n\u00162[0;1]1\n2X\ni(\u0016di(R0)\u0000(1\u0000\u0016)di(\u0001))2; (6)\nwheredi(G)\u0001\u0001=P\njGijdenotes the degree (sum of inci-\ndent edge-weights) of iinG. Treating d(\u0001)\u0001\u0001= [di(\u0001)]n\ni=1\nas a vector in Rn\n+yields the optimal solution to eq. (6):\n\u0016\u0003=hd(\u0001);d (R0) +d(\u0001)i\nkd(R0) +d(\u0001)k2: (7)\nNote that because \u0001is non-empty (contains at least one\nedge), it follows that kd(\u0001)k2>0, which implies \u0016\u0003>0.\nSimilarly, if R0is non-empty, then \u0016\u0003<1, and the result-\ning combination retains the full connectivity structure of\nthe unweighted R+(eq. (4)).\n2.3 Edge weighting and feature fusion\nThe construction above relies upon a single feature rep-\nresentation to determine the self-similarity structure, and\nuses constant edge weights for the repetition and local edges.\nThis can be generalized to support feature-weighted edges\nby replacing R0with a masked similarity matrix:\nR0\nij7!R0\nijSij; (8)\nwhereSijdenotes a non-negative afﬁnity between frames\niandj,e.g., a Gaussian kernel over feature vectors xi;xj:\nSrep\nij\u0001\u0001= exp\u0012\n\u00001\n2\u001b2kxi\u0000xjk2\u0013\nSimilarly, \u0001can be replaced with a weighted sequence\ngraph. However, in doing so, care must be taken when se-\nlecting the afﬁnity function. The same features used to\ndetect repetition (typically harmonic in nature) may not\ncapture local consistency, since successive frames do not\ngenerally retain harmonic similarity.\nRecent work has demonstrated that local timbre differ-\nences can provide an effective cue for structural boundary\ndetection [6]. This motivates the use of two contrasting\nfeature descriptors: harmonic features for detecting long-\nrange repeating forms, and timbral features for detecting\nlocal consistency. We assume that these features are re-\nspectively supplied in the form of afﬁnity matrices Srepand\nSloc. Combining these afﬁnities with the detected repeti-\ntion structure and optimal weighting yields the sequence-\naugmented afﬁnity matrix A:\nAij\u0001\u0001=\u0016R0\nijSrep\nij+ (1\u0000\u0016)\u0001ijSloc\nij; (9)\nwhereR0is understood to be constructed solely from the\nrepetition afﬁnities Srep, and\u0016is optimized by solving (7)\nwith the weighted afﬁnity matrices.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4063. GRAPH PARTITIONING AND\nSTRUCTURAL ANALYSIS\nThe Laplacian is a fundamental tool in the ﬁeld of spec-\ntral graph theory, as it can be interpreted as a discrete ana-\nlog of a diffusion operator over the vertices of the graph,\nand its spectrum can be used to characterize vertex con-\nnectivity [2]. This section describes in detail how spectral\nclustering can be used to analyze and partition the repeti-\ntion graph constructed in the previous section, and reveal\nmusical structure.\n3.1 Background: spectral clustering\nLetDdenote the diagonal degree matrix ofA:\nD\u0001\u0001= diag(d(A)):\nThesymmetric normalized Laplacian Lis deﬁned as:\nL\u0001\u0001=I\u0000D\u00001=2AD\u00001=2: (10)\nThe Laplacian forms the basis of spectral clustering, in\nwhich vertices are represented in terms of the eigenvectors\nofL[15]. More speciﬁcally, to partition a graph into m\ncomponents, each point iis represented as the vector of\ntheith coordinates of the ﬁrst meigenvectors of L, corre-\nsponding to the msmallest eigenvalues.1The motivation\nfor this method stems from the observation that the multi-\nplicity of the bottom eigenvalue \u00150= 0corresponds to the\nnumber of connected components in a graph, and the cor-\nresponding eigenvectors encode component memberships\namongst vertices.\nIn the non-ideal case, the graph is fully connected, so\n\u00150has multiplicity 1, and the bottom eigenvector trivially\nencodes membership in the graph. However, in the case\nofA, we expect there to be many components with high\nintra-connectivity and relatively small inter-connectivity at\nthe transition points between sections. Spectral clustering\ncan be viewed as an approximation method for ﬁnding nor-\nmalized graph cuts [15], and it is well-suited to detecting\nand pruning these weak links.\nFigure 1 illustrates an example of the encoding pro-\nduced by spectral decomposition of L. Although the ﬁrst\neigenvector (column) is uninformative, the remaining bases\nclearly encode membership in the diagonal regions depicted\nin the afﬁnity matrix. The resulting pair-wise frame sim-\nilarities for this example are shown in Figure 2, which\nclearly demonstrates the ability of this representation to it-\neratively reveal nested repeating structure.\nTo apply spectral clustering, we will use k-means clus-\ntering with the (normalized) eigenvectors Y2Rn\u0002Mas\nfeatures, where M > 0is a speciﬁed maximum number\nof structural component types. Varying M— equivalently,\nthe dimension of the representation — directly controls the\ngranularity of the resulting segmentation.\n1An additional length-normalization is applied to each vector, to cor-\nrect for scaling introduced by the symmetric normalized Laplacian [15].Algorithm 1 Boundary detection\nInput: Laplacian eigenvectors Y2Rn\u0002m,\nOutput: Boundariesb, segment labels c2[m]n\n1:function BOUNDARY -DETECT (Y)\n2: ^yi Yi;\u0001=kYi;\u0001k \fNormalize each row Yi;\u0001\n3: Runk-means onf^yign\ni=1withk=m\n4: Letcidenote the cluster containing ^yi\n5:b fijci6=ci+1g\n6: return (b;c)\n3.2 Boundary detection\nFor a ﬁxed number of segment types m, segment bound-\naries can estimated by clustering the rows of Yafter trun-\ncating to the ﬁrst mdimensions. After clustering, segment\nboundaries are detected by searching for change-points in\nthe cluster assignments. This method is formalized in Al-\ngorithm 1. Note that the number of segment types is dis-\ntinct from the number of segments because a single type\n(e.g., verse) may repeat multiple times throughout the track.\n3.3 Laplacian structural decomposition\nTo decompose an input song into its structural components,\nwe propose a method, listed as Algorithm 2, to ﬁnd bound-\naries and structural annotations at multiple levels of struc-\ntural complexity. Algorithm 2 ﬁrst computes the Laplacian\nas described above, and then iteratively increases the set\nof eigenvectors for use in Algorithm 1. For m= 2, the\nﬁrst two eigenvectors — corresponding to the two smallest\neigenvalues of L— are taken. In general, for mtypes of\nrepeating component, the bottom meigenvectors are used\nto label frames and detect boundaries. The result is a se-\nquence of boundaries Bmand frame labels Cm, for values\nm22;3;:::;M .\nNote that unlike most structural analysis algorithms, Al-\ngorithm 2 does not produce a single decomposition of the\nsong, but rather a sequence of decompositions ordered by\nincreasing complexity. This property can be beneﬁcial in\nvisualization applications, where a user may be interested\nin the relationship between structural components at mul-\ntiple levels. Similarly, in interactive display applications, a\nuser may request more or less detailed analyses for a track.\nSince complexity is controlled by a single, discrete param-\neterM, this application is readily supported with a mini-\nmal set of interface controls (e.g., a slider).\nHowever, for standardized evaluation, the method must\nproduce a single, ﬂat segmentation. Adaptively estimating\nthe appropriate level of analysis in this context is somewhat\nill-posed, as different use-cases require differing levels of\ndetail. We apply a simple selection criterion based on the\nlevel of detail commonly observed in standard datasets [5,\n12]. First, the set of candidates is reduced to those in which\nthe mean segment duration is at least 10 seconds. Subject\nto this constraint, the segmentation level ~mis selected to\nmaximize frame-level annotation entropy. This strategy\ntends to produce solutions with approximately balanced\ndistributions over the set of segment types.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n407Time →Time →\nRecurrence matrix R\nTime →Time →\nAffinity matrix A\nY0Y1Y2Y3Y4Y5Y6Y7Y8Y9Time →Eigenvectors of L\n−1.0−0.8−0.6−0.4−0.20.00.20.40.60.81.0Figure 1. Left: the recurrence matrix RforThe Beatles — Come Together. Center: the sequence-augmented afﬁnity\nmatrixA; the enlarged region demonstrates the cumulative effects of recurrence ﬁltering, sequence-augmentation, and\nedge weighting. Right: the ﬁrst 10 basis features (columns), ordered left-to-right. The leading columns encode the primary\nstructural components, while subsequent components encode reﬁnements.\nm=1\n m=2\n m=3\n m=4\n m=5\nm=6\n m=7\n m=8\n m=9\n m=10\n−1.0−0.8−0.6−0.4−0.20.00.20.40.60.81.0\nFigure 2. Pair-wise frame similarities\u0000\nYYT\u0001\nusing the ﬁrst 10components for The Beatles — Come Together. The ﬁrst\n(trivial) component (m = 1) captures the entire song, and the second (m = 2) separates the outro (ﬁnal vamp) from the\nrest of the song. Subsequent reﬁnements separate the solo, refrain, verse, and outro, and then individual measures.\n4. EXPERIMENTS\nTo evaluate the proposed method quantitatively, we com-\npare boundary detection and structural annotation perfor-\nmance on two standard datasets. We evaluate the perfor-\nmance of the method using the automatic complexity esti-\nmation described above, as well as performance achieved\nfor each ﬁxed value of macross the dataset.\nFinally, to evaluate the impact of the complexity esti-\nmation method, we compare to an oracle model. For each\ntrack, a different m\u0003is selected to maximize the evalua-\ntion metric of interest. This can be viewed as a simula-\ntion of interactive visualization, in which the user has the\nfreedom to dynamically adapt the level of detail until she\nis satisﬁed. Results in this setting may be interpreted as\nmeasuring the best possible decomposition within the set\nproduced by Algorithm 2.4.1 Data and evaluation\nOur evaluation data is comprised of two sources:\nBeatles-TUT: 174 structurally annotated tracks from the\nBeatles corpus [10]. A single annotation is provided\nfor each track, and annotations generally correspond\nto functional components (e.g., verse, refrain, or solo).\nSALAMI: 735 tracks from the SALAMI corpus [12]. This\ncorpus spans a wide range of genres and instrumen-\ntation, and provides multiple annotation levels for\neach track. We report results on functional andsmall-\nscale annotations.\nIn each evaluation, we report the F-measure of bound-\nary detection at 0.5-second and 3-second windows. To\nevaluate structural annotation accuracy, we report pairwise\nframe classiﬁcation F-measure [9]. For comparison pur-\nposes, we report scores achieved by the method of Serr `aet\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n408Algorithm 2 Laplacian structural decomposition\nInput: Afﬁnities:Srep;Sloc2Rn\u0002n\n+, maximum number\nof segment types 0<M\u0014n\nOutput: Boundaries Bmand frame labels Cmfor\nm22:::M\n1:function LSD(Srep;Sloc;M)\n2:R eq. (1) onSrep\fRecurrence detection\n3:R0 eq. (2) onR \fRecurrence ﬁltering\n4:A eq. (9) \fSequence augmentation\n5:L I\u0000D\u00001=2AD\u00001=2\n6: form22;3;:::;M do\n7:Y bottommeigenvectors of L\n8: (Bm;Cm) BOUNDARY -DETECT (Y)\n9: returnf(Bm;Cm)gM\nm=2\nal., denoted here as SMGA [11].\n4.2 Implementation details\nAll input signals are sampled at 22050Hz (mono), and an-\nalyzed with a 2048-sample FFT window and 512-sample\nhop. Repetition similarity matrices Srepwere computed by\nﬁrst extracting log-power constant-Q spectrograms over 72\nbins, ranging from C2(32.7 Hz) to C8(2093.0 Hz).\nConstant-Q frames were mean-aggregated between de-\ntected beat events, and stacked using time-delay embed-\nding with one step of history as in [11]. Similarity matri-\nces were then computed by applying a Gaussian kernel to\neach pair of beat-synchronous frames iandj. The band-\nwidth parameter \u001b2was estimated by computing the aver-\nage squared distance between each xiand itskth nearest\nneighbor, with kset to 1 +d2 log2ne(wherendenotes the\nnumber of detected beats). The same kwas used to con-\nnect nearest neighbors when building the recurrence matrix\nR, with the additional constraint that frames cannot link\nto neighbors within 3 beats of each-other, which prevents\nself-similar connections within the same measure. The ma-\njority vote window was set to w= 17.\nLocal timbre similarity Slocwas computed by extracting\nthe ﬁrst 13 Mel frequency cepstral coefﬁcients (MFCC),\nmean-aggregating between detected beats, and then apply-\ning a Gaussian kernel as done for Srep.\nAll methods were implemented in Python with NumPy\nand librosa [1, 14].\n4.3 Results\nThe results of the evaluation are listed in Tables 1 to 3. For\neach ﬁxedm, the scores are indicated as Lm.Lindicates\nthe automatic maximum-entropy selector, and L\u0003indicates\nthe best possible mfor each metric independently.\nAs a common trend across all data sets, the automatic\nm-selector often achieves results comparable to the best\nﬁxedm. However, it is consistently outperformed by the\noracle model L\u0003, indicating that the output of Algorithm 2\noften contains accurate solutions, the automatic selector\ndoes not always choose them.Table 1. Beatles (TUT)\nMethod F0:5 F3 Fpair\nL2 0.307\u00060.14 0.429 \u00060.18 0.576 \u00060.14\nL3 0.303\u00060.15 0.544 \u00060.17 0.611 \u00060.13\nL4 0.307\u00060.15 0.568 \u00060.17 0.616 \u00060.13\nL5 0.276\u00060.14 0.553 \u00060.15 0.587 \u00060.12\nL6 0.259\u00060.14 0.530 \u00060.15 0.556 \u00060.12\nL7 0.246\u00060.13 0.507 \u00060.14 0.523 \u00060.12\nL8 0.229\u00060.13 0.477 \u00060.15 0.495 \u00060.12\nL9 0.222\u00060.12 0.446 \u00060.14 0.468 \u00060.12\nL10 0.214\u00060.11 0.425 \u00060.13 0.443 \u00060.12\nL 0.312\u00060.15 0.579 \u00060.16 0.628 \u00060.13\nL\u00030.414\u00060.14 0.684 \u00060.13 0.694 \u00060.12\nSMGA 0.293 \u00060.13 0.699 \u00060.16 0.715 \u00060.15\nTable 2. SALAMI (Functions)\nMethod F0:5 F3 Fpair\nL2 0.324\u00060.13 0.383 \u00060.15 0.539 \u00060.16\nL3 0.314\u00060.13 0.417 \u00060.16 0.549 \u00060.13\nL4 0.303\u00060.12 0.439 \u00060.16 0.547 \u00060.13\nL5 0.293\u00060.12 0.444 \u00060.16 0.535 \u00060.12\nL6 0.286\u00060.12 0.452 \u00060.16 0.521 \u00060.13\nL7 0.273\u00060.11 0.442 \u00060.16 0.502 \u00060.13\nL8 0.267\u00060.12 0.437 \u00060.16 0.483 \u00060.13\nL9 0.260\u00060.11 0.443 \u00060.16 0.464 \u00060.14\nL10 0.250\u00060.11 0.422 \u00060.16 0.445 \u00060.14\nL 0.304\u00060.13 0.455 \u00060.16 0.546 \u00060.14\nL\u00030.406\u00060.13 0.579 \u00060.15 0.652 \u00060.13\nSMGA 0.224 \u00060.11 0.550 \u00060.18 0.553 \u00060.15\nIn the case of SALAMI (small), the automatic selec-\ntor performs dramatically worse than many of the ﬁxed-m\nmethods, which may be explained by the relatively differ-\nent statistics of segment durations and numbers of unique\nsegment types in the small-scale annotations as compared\nto Beatles and SALAMI (functional).\nTo investigate whether a single mcould simultaneously\noptimize multiple evaluation metrics for a given track, we\nplot the confusion matrices for the oracle selections on\nSALAMI (functional) in Figure 3. We observe that the\nmwhich optimizes F3is frequently larger than those for\nF0:5— as indicated by the mass in the lower triangle of\nthe left plot — or Fpair— as indicated by the upper tri-\nangle of the right plot. Although this observation depends\nupon our particular boundary-detection strategy, it is cor-\nroborated by previous observations that the 0.5-second and\n3.0-second metrics evaluate qualitatively different objec-\ntives [13]. Consequently, it may be beneﬁcial in practice\nto provide segmentations at multiple resolutions when the\nspeciﬁc choice of evaluation criterion is unknown.\n5. CONCLUSIONS\nThe experimental results demonstrate that the proposed struc-\ntural decomposition technique often generates solutions which\nachieve high scores on segmentation evaluation metrics.\nHowever, automatically selecting a single “best” segmen-\ntation without a priori knowledge of the evaluation criteria\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4092345678910\nm (F3)2345678910m (F0.5)F0.5 vs F3\n2345678910\nm (F_pair)2345678910m (F0.5)F0.5 vs F_pair\n2345678910\nm (F_pair)2345678910m (F3)F3 vs F_pair\n0.000.020.040.060.080.100.120.14Figure 3. Confusion matrices illustrating the oracle selection of the number of segment types m2[2;10]for different pairs\nof metrics on SALAMI (functional). While m= 2is most frequently selected for all metrics, the large mass off-diagonal\nindicates that for a given track, a single ﬁxed mdoes not generally optimize all evaluation metrics.\nTable 3. SALAMI (Small)\nMethod F0:5 F3 Fpair\nL2 0.151\u00060.11 0.195 \u00060.13 0.451 \u00060.19\nL3 0.171\u00060.12 0.259 \u00060.16 0.459 \u00060.17\nL4 0.186\u00060.12 0.315 \u00060.17 0.461 \u00060.15\nL5 0.195\u00060.12 0.354 \u00060.17 0.455 \u00060.14\nL6 0.207\u00060.12 0.391 \u00060.18 0.452 \u00060.13\nL7 0.214\u00060.12 0.420 \u00060.18 0.445 \u00060.13\nL8 0.224\u00060.12 0.448 \u00060.18 0.435 \u00060.13\nL9 0.229\u00060.12 0.467 \u00060.18 0.425 \u00060.13\nL10 0.234\u00060.12 0.486 \u00060.18 0.414 \u00060.13\nL 0.192\u00060.11 0.344 \u00060.15 0.448 \u00060.16\nL\u00030.292\u00060.15 0.525 \u00060.19 0.561 \u00060.16\nSMGA 0.173 \u00060.08 0.518 \u00060.12 0.493 \u00060.16\nremains a challenging practical issue.\n6. ACKNOWLEDGMENTS\nThe authors acknowledge support from The Andrew W.\nMellon Foundation, and NSF grant IIS-1117015.\n7. REFERENCES\n[1] Librosa, 2014. https://github.com/bmcfee/librosa.\n[2] Fan RK Chung. Spectral graph theory, volume 92.\nAmerican Mathematical Soc., 1997.\n[3] Jonathan Foote. Automatic audio segmentation using\na measure of audio novelty. In Multimedia and Expo,\n2000. ICME 2000. 2000 IEEE International Confer-\nence on, volume 1, pages 452–455. IEEE, 2000.\n[4] Harald Grohganz, Michael Clausen, Nanzhu Jiang,\nand Meinard M ¨uller. Converting path structures into\nblock structures using eigenvalue decomposition of\nself-similarity matrices. In ISMIR, 2013.\n[5] Christopher Harte. Towards automatic extraction of\nharmony information from music signals. PhD thesis,\nUniversity of London, 2010.[6] Florian Kaiser and Geoffroy Peeters. A simple fusion\nmethod of state and sequence segmentation for music\nstructure discovery. In ISMIR, 2013.\n[7] Florian Kaiser and Thomas Sikora. Music structure\ndiscovery in popular music using non-negative matrix\nfactorization. In ISMIR, pages 429–434, 2010.\n[8] P. Lamere. The inﬁnite jukebox, November 2012.\nhttp://inﬁnitejuke.com/.\n[9] Mark Levy and Mark Sandler. Structural segmenta-\ntion of musical audio by constrained clustering. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 16(2):318–326, 2008.\n[10] Jouni Paulus and Anssi Klapuri. Music structure analy-\nsis by ﬁnding repeated parts. In Proceedings of the 1st\nACM workshop on Audio and music computing multi-\nmedia, pages 59–68. ACM, 2006.\n[11] J. Serr `a, M. M ¨uller, P. Grosche, and J. Arcos. Unsuper-\nvised music structure annotation by time series struc-\nture features and segment similarity. Multimedia, IEEE\nTransactions on, PP(99):1–1, 2014.\n[12] Jordan BL Smith, John Ashley Burgoyne, Ichiro Fuji-\nnaga, David De Roure, and J Stephen Downie. Design\nand creation of a large-scale database of structural an-\nnotations. In ISMIR, pages 555–560, 2011.\n[13] Jordan BL Smith and Elaine Chew. A meta-analysis of\nthe mirex structure segmentation task. In ISMIR, 2013.\n[14] Stefan Van Der Walt, S Chris Colbert, and Gael Varo-\nquaux. The numpy array: a structure for efﬁcient nu-\nmerical computation. Computing in Science & Engi-\nneering, 13(2):22–30, 2011.\n[15] Ulrike V on Luxburg. A tutorial on spectral clustering.\nStatistics and computing, 17(4):395–416, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n410"
    },
    {
        "title": "Audio-to-score Alignment at the Note Level for Orchestral Recordings.",
        "author": [
            "Marius Miron",
            "Julio José Carabias-Orti",
            "Jordi Janer"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416150",
        "url": "https://doi.org/10.5281/zenodo.1416150",
        "ee": "https://zenodo.org/records/1416150/files/MironCJ14.pdf",
        "abstract": "In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise align- ment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and off- sets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best com- bination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets.",
        "zenodo_id": 1416150,
        "dblp_key": "conf/ismir/MironCJ14",
        "keywords": [
            "offline",
            "audio-to-score alignment",
            "note level",
            "score alignment systems",
            "note onsets",
            "note offsets",
            "score-informed source separation",
            "complex orchestral mixtures",
            "audio and image processing techniques",
            "note-wise pitch salience function"
        ],
        "content": "AUDIO-TO-SCORE ALIGNMENT AT NOTE LEVEL FOR ORCHESTRAL\nRECORDINGS\nMarius Miron, Julio Jos ´e Carabias-Orti, Jordi Janer\nMusic Technology Group, Universitat Pompeu Fabra\nmarius.miron,julio.carabias,jordi.janer@upf.edu\nABSTRACT\nIn this paper we propose an ofﬂine method for reﬁning\naudio-to-score alignment at the note level in the context\nof orchestral recordings. State-of-the-art score alignment\nsystems estimate note onsets with a low time resolution,\nand without detecting note offsets. For applications such as\nscore-informed source separation we need a precise align-\nment at note level. Thus, we propose a novel method that\nreﬁnes alignment by determining the note onsets and off-\nsets in complex orchestral mixtures by combining audio\nand image processing techniques. First, we introduce a\nnote-wise pitch salience function that weighs the harmonic\ncontribution according to the notes present in the score.\nSecond, we perform image binarization and blob detection\nbased on connectivity rules. Then, we pick the best com-\nbination of blobs, using dynamic programming. We ﬁnally\nobtain onset and offset times from the boundaries of the\nmost salient blob. We evaluate our method on a dataset\nof Bach chorales, showing that the proposed approach can\naccurately estimate note onsets and offsets.\n1. INTRODUCTION\nAudio-to-score alignment concerns synchronizing the notes\nin a musical score with the corresponding audio rendition.\nAn additional step, alignment at the note level, aims at ad-\njusting the note onsets, in order to further minimize the\nerror between the score and audio. In the context of or-\nchestral music, this task is challenging; ﬁrst, because of\nthe complex polyphonies, and, second, because of the tim-\ning expressivity of classical music.\nAs possible applications of note alignment, deriving the\nexact locations of the note onsets and offsets could improve\ntasks as score-informed source separation [6], [2], [7].\nState-of-the-art score alignment methods use Non-\nnegative matrix factorization (NMF) [14], [11], template\nadaptation through expectation maximization [9], dynamic\ntime warping (DTW) [3], and Hidden Markov Models\n(HMM) [4, 6]. The method described in [11, p. 103] is\nthe only one addressing explicitly the topic of ﬁne note\nc\rMarius Miron, Julio Jos ´e Carabias-Orti, Jordi Janer.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Marius Miron, Julio Jos ´e Carabias-\nOrti, Jordi Janer. “Audio-to-score alignment at note level for orchestral\nrecordings”, 15th International Society for Music Information Retrieval\nConference, 2014.alignment as a post-processing step. A factorization is per-\nformed to obtain the onsets of the anchor notes. The basis\nvectors are trained with piano pitches models, and the on-\nsets are obtained from the activations matrix. Furthermore,\nan additional step is performed in order to look for onsets\nbetween anchors.\nHowever, the methods listed above have certain limita-\ntions. First, accurately detecting the offset of the note is a\nchallenging problem and none of these methods claim to\nsolve it. Second, the scope of the NMF-based systems is\nsolely piano recordings. Third, except [11], the algorithms\nconsider a large window to evaluate detected onsets. Note\nthat the MIREX Real-time Audio-to-Score Alignment task\nconsiders a 2000 ms window size.\nWith respect to image processing techniques deployed\nin music information research, a system to link audio and\nscores for makam music is presented in [13]. In this case,\nHough transform is used for picking the line correspond-\ning to the most likely path from a binarized distance ma-\ntrix. Additionally, the same transform is used in [1] to ﬁnd\nrepeating patterns for audio thumbnailing.\nIn this paper we propose a novel method for audio-to-\nscore alignment at the note level, which combines audio\nand image processing techniques. In comparison to classi-\ncal audio-to-score alignment methods, we aim to detect the\noffset of the note, along with its onset. Additionally, we do\nnot assume a constant delay between score and audio, thus\nwe do not use any information regarding the beats, tempo\nor note duration, in order to adjust the onsets. Therefore,\nour method can align notes when dealing with variable de-\nlays, as the ones resulting from automatic score alignment\nor the ones yielded by manually aligning the score at the\nbeat level.\nThe proposed method is based on two stages. First, the\naudio processing stage involves ﬁltering the spectral peaks\nin time and frequency for every note. Consequently, the\nﬁltering occurs in the time interval restricted for each note\nand in the frequency bands of the harmonic partials corre-\nsponding to its fundamental frequency. Furthermore, we\ndecrease the magnitudes of the peaks which are overlap-\nping in time and frequency with the peaks from other notes.\nUsing the ﬁltered spectral peaks, we compute the pitch\nsalience for each note using the harmonic summation algo-\nrithm described in [10]. Second, we detect the boundaries\nof the note using an image processing algorithm. The pitch\nsalience matrix associated to each note is binarized. Then,\nblobs, namely boundaries and shapes, are detected using\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n125the connectivity rules described in [12, p. 248]. From all\nthe blobs candidates associated to every note, we pick the\nbest combination of consecutive blobs using dynamic pro-\ngramming. The image processing part has the advantage\nthat the blob boundaries will deﬁne the note onsets along\nwith the corresponding offsets.\nThe remainder of this paper is structured as follows. In\nthe ﬁrst section we describe the note-wise pitch salience\ncomputation, followed by the blob selection using image\nprocessing methods. Then, we evaluate our algorithm on a\ndataset of Bach chorales [6] and we discuss the results.\n2. METHOD\nThe proposed method aims to detect the onsets and offsets\nof the notes from a monaural audio recording, where the\nscore is assumed to be automatically or manually aligned\na priori, assuming an error up to 200 ms.\nFigure 1. The two main sections of our method: audio and\nimage processing, and the corresponding steps.\nFigure 2 shows the block diagram of the proposed\nmethod. As can be seen, the method is subdivided in two\nstages. First, in the audio processing stage, a ﬁltered pitch\nsalience matrix is obtained for each of the notes in the\nscore, and for every instrument. Second, in the image pro-\ncessing stage, the pitch salience matrix is regarded as a\ngreyscale image, and blobs are detected in the binarized\nimage. Moreover, we construct a graph with all the blobs\nand we pick the best combination of blobs by using Dijk-\nstra’s algorithm to ﬁnd the best path in the graph. Finally,\nwe reﬁne the time boundaries for the blobs that overlap,\nusing an adaptive threshold binarization.\n2.1 Note-wise pitch salience computation\nFor each input signal, we ﬁrst compute the Short time\nFourier transform (STFT) and we extract the spectral\npeaks. Then, we analyze each single note in the score\nand we select only the spectral peaks in the frames around\nits approximate time location and the frequency bands as-\nsociated to its harmonic partials (i.e. multiples of the\nfundamental frequency). Finally, we compute the pitch\nsalience, using the harmonic summation algorithm de-\nscribed in [10].To select the time intervals at which we are going to\nlook for the note onsets and offsets, we analyze the pre-\naligned score that we want to reﬁne. We start from the\nassumption that the note onsets are played with an error\nlower than 200ms from the actual onset in the score. In\nother words, we set the search interval to \u0006200 ms from\nthe note onset at the score. Additionally, in the case of\nthe offset, we extend the possible duration of a note in the\nscore by 200ms or until another note in the score appears.\nIn the rest of the paper, this search interval will be referred\nto asTon(n)andToff(n).\nThen, we analyze the spectral peaks within the time in-\nterval deﬁned for each note, and we ﬁlter them according\nto the harmonic frequencies of the MIDI note ^Fn(i), where\n^Fn(0)is the fundamental frequency of note n. Namely, we\ntake the ﬁrst 16of the harmonic partials of this frequency,\n^Fn(i)withi2[0; :::; 15]. Taking into account vibratos,\nwe set a 1:4semitone interval around each of the harmonic\npartials. Consequently, we select a set of candidate peaks\nPn(k)and the associated amplitudes An(k)for note nat\nframe ksuch that Pn(k)2[^Fn(i)\u0000^Ln(i); :::; ^Fn(i) +\n^Ln(i)], where ^Ln(i)is a frequency band equivalent to 0:7\nof a semitone.\nAs a drawback, some of the selected peaks could over-\nlap in time and frequency. To overcome this problem, we\ndistribute the amplitude An(k)of the overlapped peaks\nPn(k)using a factor gi(Pn(k); Pm(k)), where nandm\nare the overlapped notes, giis a gaussian centered at the\ncorresponding frequency ^Fn(i)of the note nand the har-\nmonic partial i. The standard deviation equals to^Ln(i)\n2,\nthus:\ngi(x) = w\u00030:8i\u0003e\u0000(x\u0000^Fn(i))2\u001e\n^Ln(i)\n22\n(1)\nNote that the magnitude of the gaussian decreases with\nthe order of the harmonic, i, and is proportional to w, the\nweight of the rest of the instruments in current audio ﬁle, or\nthe coefﬁcient extracted from a pre-existing mixing matrix.\nFor example, if we align using solely a monaural signal in\nwhich all four instruments have the same weight, 0:25 for\nall four instruments, the coefﬁcient will be w= 0:75.\nThe factor gipenalizes frequencies which are in the al-\nlowed bands but are further away from the central frequen-\ncies. In this way, we eliminate transitions to other notes or\nenergy which can add up noise later on in the blob detec-\ntion stage.\nFinally, for each note nand its associated Pn(k)and\nAn(k)where k2[Ton(n); :::T off(n)], we use the pitch\nsalience function described in [10]. The algorithm calcu-\nlates a salience measure for each pitch candidate, starting\nat^Fn(0)\u0000^Ln(0), based on the presence of its harmonics\nand sub-harmonics partials, and the corresponding magni-\ntudes. Finally, the salience function for each time window\nis quantized into cent bins, thus the resulting matrix Sn\nhas the dimensions (Toff(n)\u0000Ton(n); Q ), where Qis\nthe number of frequency bins for the six octaves. In our\ncase, we experimentally choose Q= 600 bins.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1262.2 Blob selection using image processing\nThe goals of the image processing stage are to obtain the\nlocation of the note onset and offset by binarizing the note-\nwise pitch salience, and to detect shapes and contours in\nthe binarized image.\nAccounting that the image binarization is not a robust\nprocess [12], different results are expected as a function of\nthe amount of time overlap between notes, the salience of\nthe pitch and its fundamental frequency. Therefore, as the\nshape and contour detection heavily relies on this step, we\nneed a robust binarization, which would ﬁnally give us the\nbest information for detecting the boundaries of the note.\nPrevious approaches to improve binarization rely on\nbackground subtraction or local binarization [12]. There-\nfore, we propose a binarization method similar to the local\nbinarization, but adapted to our context: the pitch salience\nmatrix. On the assumption that the bins closer to the funda-\nmental frequency, ^Fn(0), are more salient than the ones at\nhigher frequencies, we split the binarization areas in sub-\nareas related to the harmonic partials ^Fn(i). Thus, the\nsalience matrix Snis binarized gradually and locally, ob-\ntaining a binary matrix Bn. Moreover, we consider las\nthe binarization step, moving gradually from 50to600in\nsteps of 50bins.\nFurthermore, we compute Bninlsteps, each time only\nfor the columns in the interval [l\u000050:::l].\nBn(i; j) =\u001a0;Sn(i; j)< mean(Sl\nn)\n1;Sn(i; j)\u0015mean(Sl\nn)(2)\nwhere i2[Ton(n); :::; T off(n)],j2[l\u000050:::l], andSl\nn\nis a submatrix of Sn, obtained by extracting the columns\nofSnin the interval [0..l].\nAs an example, a pitch salience matrix Snfor a bassoon\nnote is plotted in the Figure 2A. The green rectangles mark\nthe submatrices Sl\nnfor various values of l. The resulting\nbinarized image is depicted in Figure 2B.\nfrequency relative to note's f0 (cent bins)time (seconds)➩\n100200300400500\n0100200300400500\n0\n0.2 0.80.6 0.2 0.80.6A B\ntime (seconds)\nFigure 2 .Binarizing the spectral salience matrix (ﬁgure\nA) and detecting the blobs in the resulting image (ﬁgure\nB). Binarization is done gradually and locally, relative to\nthe green squares ´areas in ﬁgure A. The ground truth onset\nand offset of the note are marked by vertical red lines.\nThe next step is detecting boundaries and shapes on the\nbinarized image. We use the connectivity rules describedin [12, p. 248] in order to detect regions and the boundaries\nof these regions, namely the blobs. Thus, we want to label\neach pixel of the matrix Bnwith a number from 0tor,\nwhere ris the total number of detected blobs.\nHaving a pixel (i; j)withi2[Ton(n); :::; T off(n)]\nandj2[0; :::; Q], where Qis the number of frequency\nbins, we need to consider all the neighboring pixels and\nwe have to take into account their connectivity with the\ncurrent pixel. The 4-way connectivity rules account for the\nimmediate neighbors, as compared to 8-way connectivity\nwhich account for all the surrounding pixels. Because we\nare not interested in modeling transitions between notes,\nwe discard diagonal shapes by using the 4-way connec-\ntivity rules. Hence, the connectivity matrix, which deter-\nmines the neighborhood of the pixel (i; j), can be written\nas:\nM=2\n40 1 0\n1 1 1\n0 1 03\n5\nFor the matrix M, the central pixel with the coordinates\n(2,2) represents the origin pixel (i; j), and all the other non-\nzero pixels are the considered positions for the neighbors.\nThe algorithm, described in [12, p. 251], takes one pixel\nat a time and visits its non-zero neighbors. Then, we move\nsequentially from one pixel to its neighbors, setting bound-\naries for the pixels having neighbors equal to zero. Finally,\nthe shape is enclosed when the algorithm reaches the pixel\nof origin.\nFurthermore, once we have detected a set of blobs bn\nfor each note n, we need to compute the best combination\nof the blobs for all notes. Because search intervals for con-\nsecutive notes can overlap in time, choosing the best com-\nbination of blobs is not as trivial as picking the best blob\nin terms of area or salience, and the decisions that we take\nfor a current note, should take into account the decisions\nwe take for the previous and the next note. This kind of\nproblem, which chains up a set of decisions can be solved\nwith dynamic programming.\nConsequently, we consider the blobs to be the vertices\nof an oriented graph, in which the edges are assigned a cost\ndepending on the area of the two blobs and the overlapping\nbetween them, as seen in Figure 3. Basically, blobs with\nbigger area and little overlapping will have a lower cost,\nwhich makes them ideal candidates when we ﬁnd the best\npath in the graph. Additionally, we can have an edge only\nbetween blobs of consecutive notes, and we can remove\nthe edges between blobs which overlap more than 50% in\ntime.\nTherefore, we compute the area of each blob of the note\nnby summing up the values in the binarized matrix Bn,\nenclosed by the corresponding blob contours. Addition-\nally, we exclude the blobs which have the duration less than\n100ms, and the ones starting after the allowed interval for\nthe attack time.\nThe normalized area of blob ifor the note nisH(bi\nn)\nand is a value inversely proportional with the actual area,\nbecause we want the larger blobs to have a lower cost,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n127b1\nb2\nb3b1\nb2\nb3\nb4\nb5b1\nb2note n note n+1 note n-1Figure 3. A sample of the graph between three consecutive\nnotes. b[1::5] are the blobs detected for each note. Thicker\nlines represent lower costs. The red line represents the best\npath in the graph.\nwhen picking the best path. In the same manner, we must\nincrease the cost as the overlapping between the blobs\nincreases. Thus, for two adjacent notes nandn+ 1,\nO(bi\nn; bi\nn+1)has cost 1 if there is no overlapping, and an\nincreased value summing up the ratio of the the area of the\ntwo overlapping blobs. For instance, if 20% of the area of\nthe ﬁrst blobs overlaps with 70% of the area of the second\nblob, O= 1 + 0:2 + 0:7 = 1 :9.\nThus, the cost for the edges has the expression\ncost(bn\ni; bn\ni+1) =O(bi\nn; bi\nn+1)\u0003(H(bi\nn) +H(bi\nn+1))\nIn order to ﬁnd the shortest path between the vertices of\nthe ﬁrst note in the score and the last one, we use Dijkstra’s\nalgorithm described in [5]. The algorithm ﬁnds the shortest\npath for a graph with non-negative edges by assigning a\ntentative distance to each of the vertices and progressively\nadvancing by visiting the neighboring nodes.\nAdditionally, after the best path is computed, we can\nface the situation where two consecutive blobs overlap in\ntime due to the inaccuracy in binarization and the fact that\nthe minimum cost path does not guarantee no overlapping.\nBecause the melody for a particular instrument is consid-\nered to be monophonic, we do not allow overlapping be-\ntween two consecutive notes. Thus, we ought to ﬁnd a\nsplitting point between the starting point of the blob asso-\nciated with the next note and the ending point of the blob\nassociated with the current note.\nblob note before blob note after\nt=0.2\nt=1\nt=1.4\ntime(analysis windows)\ncent bins\nFigure 4. Blob reﬁnement using adaptive threshold bina-\nrization of two consecutive overlapping blobs in the best\npath. The minimum overlapping is achieved for threshold\nt= 1:4\nHaving two consecutive blobs from the best path, bn\nandbn+1, we take the image patches surrounding their\nboundaries and we adaptively increase the threshold of bi-narization until the minimum overlapping is achieved. Con-\nsequently, we consider the submatrices ^Snand^Sn+1of the\ncorresponding pitch salience matrices SnandSn+1, and\nfor a variable threshold t= [0:2::2], we compute the bi-\nnary matrices ^Bt\nnand^Bt\nn+1.\n^Bt\nn(i; j) =\u001a0;^Sn(i; j)< t\u0003mean(^Sn)\n1;^Sn(i; j)\u0015t\u0003mean(^Sn)(3)\nAs seen in Figure 4, the higher the threshold t, the less\npixels are be assigned to value 1in the binary matrices,\nthus we increase the threshold gradually until no overlap-\nping is achieved.\nFinally, the note onset and offset are extracted from the\nleftmost and the rightmost pixels of the reﬁned blobs in the\nbest path.\n3. EV ALUATION\n3.1 Experimental setup\nThe dataset used to evaluate our proposal consists of 10\nhuman played J.S. Bach four-part chorales, and is com-\nmonly known as Bach10 . The audio ﬁles are sampled\nfrom real music performances recorded at 44:1 kHz that\nare30seconds in length per ﬁle. Each piece is performed\nby a quartet of instruments: violin, clarinet, tenor saxo-\nphone and bassoon. Each musician’s part was recorded in\nisolation. Individual lines were then mixed to create 10\nperformances with four-part polyphony. More information\nabout this dataset can be found in [6].\nWe observe that the dataset has a few particularities.\nFirst, every recording presents fermatas, where the ﬁnal\nduration of the note is left at the discretion of the performer\nor the conductor, making it more difﬁcult to detect the on-\nset and offsets of the notes. Second, the chorales have a\npeculiar homophonic texture. Third, the annotated note\nonsets and offsets in the ground truth can have more or\nless notes than the actual score. We discovered that this\nmismatch comes from repeating notes, which in the origi-\nnal score are represented by a single larger note. This step\nalso makes the detection of the note offsets more difﬁcult.\nIn order to perform alignment at the note level, we gen-\nerate a misaligned score by introducing onset and offset\ntime deviations for all the notes and all the instruments in\nthe ground-truth score. The deviations are randomly and\nuniformly distributed in the intervals [\u0000200; :::;\u0000100] and\n[100; :::; 200] ms. Moreover, we aim at reﬁning the align-\nment of the algorithm proposed by [3]. Thus, we correct\nthe onset times and we detect the offsets around the begin-\nning of the next note. For both of these tasks we consider\nthe interval [\u0000200; :::; 200] ms.\nFurthermore, the STFT is computed using a Blackman-\nHarris 92dB window with a size of 128 ms and, a hop size\nof 6 ms. Additionally, we zero-pad the window by three\ntimes its length. Moreover, frequencies and magnitudes\nof the spectral peaks are extracted with the algorithm de-\nscribed in [8], which uses parabolic interpolation to accu-\nrately detect positive slopes in the spectrum computed at\nthe previous step.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1283.2 Results\nWe aim at correctly aligning the onsets and offsets of the\nmisaligned score described in Section 3.1 and we add up\n200 ms before and after the note boundaries in order to\nsearch for the exact starting and ending point of the note.\nThus, our algorithm can have up to 400ms in error for the\nonsets, and a larger error for the offset, because we are not\nconstraining the duration of the note to any interval.\nFor each piece, aligned rate (AR) or precision is deﬁned\nas the proportion of correctly aligned notes in the score and\nranges from 0to1. A note is said to be correctly aligned\nif its onset does not deviate more than a threshold from the\nreference alignment. To test the reliability of our method,\nwe tried different threshold values ranging form 15to140\nms. Other measures as the average offset (i.e. average\nabsolute-valued time offset between a reported note onset\nby the score follower and its real onset in the reference ﬁle)\nand the std offset (i.e. standard deviation of sign-valued\ntime offset) are also considered.\nAs illustrated in ﬁgure 5, the proposed system is able\nto accurately align more than the 30% of the onsets with a\ndetection threshold lower than 15ms. Furthermore, more\nthan80% of the onsets are accurately detected with a thresh-\nold of 60ms. Because the search time interval for the note\nallows for error larger than 200ms, the AR for the onset\ndoes not reach 100% int= 200ms, as less than 2%of the\nonsets have larger errors.\nFurthermore observe that we less accurate in detecting\nthe offsets, particularly when we do not know the approxi-\nmate note offset and we estimate it around the onset of the\nnext note, as when we take as input the alignment of the\nalgorithm proposed by [3]. The drop in performance of the\noffset detection can also be explained by the fact that the\nenergy of a note can decay below a threshold, thus exclud-\ning it when binarization is performed.\nFigure 6 shows boxplots of the average offset and the\nstd error for each instrument, and for the note onset and off-\nset, for the misaligned dataset. The lower and upper lines\nof each box show 25th and 75th percentiles of the sample.\nThe line in the middle of each box is the average offset.\nThe lines extending above and below each box show the\nextent of the rest of the samples, excluding outliers. Out-\nliers are deﬁned as points over 1.5 times the interquartile\nrange from the sample median and are shown as crosses.\nWe observe that performance is lower for violin com-\npared to the other instrument. This can be explained by\nthe fact that for this dataset the violin has noisier or soft\nattacks, which do not yield a high enough value in terms of\npitch salience, and is lost when binarizing the image.\nMoreover, the fact that we are able to detect most of\nthe onsets in the interval 0.06 seconds, which is an accept-\nable interval for the attack of the instruments aligned, point\nus on some limitation on using the pitch salience function,\nwhich is not able to be accurate enough with noisier at-\ntacks, as it happened for the violin.\nFurthermore, we want more insight on how the errors\nare distributed across the time range. Thus, we plot the 2-\nd histogram of the onset errors, as seen in Figure 7. We\nFigure 5. The proposed system improves the align rate of\n(A) the system proposed by [3] and of (B) the misaligned\ndataset, for onset errors, as well as offset errors\nobserve that even though the original dataset had large er-\nrors, our method was able to detect the note onsets within\na small time frame, as most of the errors are in the bin cen-\ntered at zero.\nMoreover, our method is better at ﬁxing the delays in\nthe note onsets. In comparison, we can commit more errors\nif the onset of the note is thought to be before the actual\nonset, because the window in which we have to look for it\noverlaps more with the previous note, hence we have more\ninterference.\nAdditionally, for every note and every instrument, we\ncompute the percentage of correctly detected frames with\nrespect to ground truth. Our algorithm is able to correctly\ndetect 89% of the frames of the ground truth notes. In com-\nparison, the notes in the misaligned dataset have a degree\nof 66% correctly detected frames.\nFinally, we compute the percentage of frames which are\nerroneously detected as part of the notes. We observe that\nsolely 0.07% of frames from the notes we reﬁne are out-\nside the boundaries of the ground truth notes, compared to\nthe misaligned dataset, for which 34% of the frames are\ndisplaced outside the time boundaries of the notes.\nTherefore, our algorithm is more likely to shorten the\nnotes, rather than making erroneous decisions regarding\ntheir time frame. This is due to the way we are picking\nthe best sequence of blobs, which penalizes the overlap-\nping, thus picking blobs which have a smaller area but less\noverlapping with the blobs from neighboring notes.\n4. CONCLUSIONS\nWe proposed a method to reﬁne the alignment of onsets\nand offsets in orchestral recordings, using audio and im-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n129Figure 6. The average offset and the std offset in terms of\n25th and 75th percentile of the proposed system for bas-\nsoon, clarinet saxophone, and violin, for note onsets, as\nwell as note offsets\nFigure 7. The histogram of error distribution in the onset\nalignment\nage processing techniques. We compute a note-wise pitch\nsalience function and we binarize it. Moreover, we detect\nblobs in the binarized image, and we pick the best blob\ncandidate for each note by ﬁnding the best path in the asso-\nciated graph. Furthermore, as offset detection is regarded\nas a more difﬁcult problem, the proposed method addresses\nthis issue by detecting image blobs to simultaneously label\nnote onsets and offsets.\nThe evaluation shows that our method is able to re-\nﬁne the alignment in a misaligned dataset, having detected\nmore than 80% of the onsets with an error of 60 ms. More-\nover, we analyzed the performance across all four instru-\nments, and we discovered that the accuracy drops for a vi-\nolin, as being higher for the other instruments. Thus, as a\nfuture step, we need to analyze what limitation has the al-\ngorithm regarding certain instrument classes. Additionally,\nthe proposed method should be tested with another dataset,\nwith more complex polyphonies and tempo variations.\nFurthermore, our method can be improved by using tim-\nbre models when ﬁltering the spectral peaks and decreas-\ning their magnitude. Additionally, choosing the best se-\nquence of blobs can be improved by using a better cost\nfunction for the Dijkstra’s algorithm. In addition, one could\nuse image processing with other data obtained by audio\nprocessing means, as the spectrogram or come with a more\nrobust approach than the pitch salience which does not cap-\nture noisy note attacks or noisy spectrum.Finally, the note reﬁnement can be used to improve the\nperformance of score informed source separation, in the\nsituation where the score is not well aligned with the audio.\n5. ACKNOWLEDGEMENTS\nThis work was supported by the European Commission, FP7 (Sev-\nenth Framework Programme), STREP project, ICT-2011.8.2 ICT\nfor access to cultural resources, grant agreement No 601166. Phenicx\nProject\n6. REFERENCES\n[1] J.-J. Aucouturier and M. Sandler. Finding repeating patterns\nin acoustic musical signals. VIRTUAL, SYNTHETIC, AND\nENTERTAINMENT AUDIO, pages 412–421, 2002.\n[2] J.J. Bosch, K. Kondo, R. Marxer, and J. Janer. Score-\ninformed and timbre independent lead instrument separa-\ntion in real-world scenarios. In Signal Processing Conference\n(EUSIPCO), 2012 Proceedings of the 20th European, pages\n2417–2421, Aug 2012.\n[3] J.J. Carabias-Orti, P. Vera-Candeas, F.J. Rodriguez-Serrano,\nand F.J. Canadas-Quesada. A RealTime Audio to Score\nAlignment System using Spectral Factorization and On-\nline Time Warping. IEEE Transactions on Multime-\ndia(submitted), 2014.\n[4] A. Cont. A coupled duration-focused architecture for real-\ntime music-to-score alignment. Pattern Anal. Mach. Intell.\nIEEE . . . , 32:974–987, 2010.\n[5] E. W. Dijkstra. A note on two problems in connexion\nwith graphs. NUMERISCHE MATHEMATIK, 1(1):269–271,\n1959.\n[6] Z. Duan and B. Pardo. Soundprism: An online system for\nscore-informed source separation of music audio. Selected\nTopics in Signal Processing, IEEE . . . , pages 1–12, 2011.\n[7] S. Ewert and M. Muller. Using score-informed constraints for\nNMF-based source separation. Acoustics, Speech and Signal\nProcessing ( . . . , 2012.\n[8] J. O. Smith Iii and X. Serra. Parshl: An analysis/synthesis\nprogram for non-harmonic sounds based on a sinusoidal rep-\nresentation . 1987.\n[9] C. Joder and B. Schuller. Off-line reﬁnement of audio-to-\nscore alignment by observation template adaptation. Acous-\ntics, Speech and Signal Processing (ICASSP), 2013 IEEE In-\nternational Conference on, pages 206–210, 2013.\n[10] A. Klapuri. Multiple fundamental frequency estimation by\nsumming harmonic amplitudes. In in ISMIR, pages 216–221,\n2006.\n[11] B. Niedermayer. Accurate Audio-to-Score Alignment Data\nAcquisition in the Context of Computational Musicology.\nPhD thesis, Johannes Kepler Universit ¨at, 2012.\n[12] M. Nixon. Feature Extraction and Image Processing. Elsevier\nScience, 2002.\n[13] S. Senturk, A. Holzapfel, and X. Serra. Linking Scores and\nAudio Recordings in Makam Music of Turkey. Journal of\nNew Music Research, pages 35–53, 2014.\n[14] T.M. Wang, P.Y . Tsai, and A.W.Y . Su. Score-informed\npitch-wise alignment using score-driven non-negative ma-\ntrix factorization. In Audio, Language and Image Processing\n(ICALIP), 2012 International Conference on, pages 206–211,\nJuly 2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n130"
    },
    {
        "title": "Evaluation Framework for Automatic Singing Transcription.",
        "author": [
            "Emilio Molina",
            "Ana M. Barbancho",
            "Lorenzo J. Tardón",
            "Isabel Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417729",
        "url": "https://doi.org/10.5281/zenodo.1417729",
        "ee": "https://zenodo.org/records/1417729/files/MolinaBTB14.pdf",
        "abstract": "In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors ma- de by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers.",
        "zenodo_id": 1417729,
        "dblp_key": "conf/ismir/MolinaBTB14",
        "keywords": [
            "evaluation strategies",
            "automatic singing transcription",
            "novel evaluation framework",
            "cross-annotated dataset",
            "extended evaluation measures",
            "Matlab toolbox",
            "standard MIREX note-tracking measures",
            "extra information about errors",
            "comparison of singing transcribers",
            "practical case of use"
        ],
        "content": "EV ALUATION FRAMEWORK FOR AUTOMATIC SINGING\nTRANSCRIPTION\nEmilio Molina, Ana M. Barbancho, Lorenzo J. Tard ´on, Isabel Barbancho\nUniversidad de M ´alaga, ATIC Research Group, Andaluc ´ıa Tech,\nETSI Telecomunicaci ´on, Campus de Teatinos s/n, 29071 M ´alaga, SPAIN\nemm@ic.uma.es, abp@ic.uma.es, lorenzo@ic.uma.es, ibp@ic.uma.es\nABSTRACT\nIn this paper, we analyse the evaluation strategies used\nin previous works on automatic singing transcription, and\nwe present a novel, comprehensive and freely available\nevaluation framework for automatic singing transcription.\nThis framework consists of a cross-annotated dataset and a\nset of extended evaluation measures, which are integrated\nin a Matlab toolbox. The presented evaluation measures\nare based on standard MIREX note-tracking measures, but\nthey provide extra information about the type of errors ma-\nde by the singing transcriber. Finally, a practical case of\nuse is presented, in which the evaluation framework has\nbeen used to perform a comparison in detail of several\nstate-of-the-art singing transcribers.\n1. INTRODUCTION\nSinging transcription refers to the automatic conversion of\na recorded singing signal into a symbolic representation\n(e.g. a MIDI ﬁle) by applying signal-processing meth-\nods [1]. One of its renowned applications is query-by-\nhumming [5], but other types of applications also are re-\nlated to this task, like singing tutors [2], computer games\n(e.g. Singstar1), etc. In general, singing transcription is\nconsidered a speciﬁc case of melody transcription (also\ncalled note tracking), which is more general problem. How-\never, singing transcription not only relates to melody tran-\nscription but also to speech recognition, and still nowadays\nit is a challenging problem even in the case of monophonic\nsignals without accompaniment [3].\nIn the literature, various approaches for singing tran-\nscription can be found. A simple but commonly referenced\napproach was proposed by McNab in 1996 [4], and it re-\nlied on several handcrafted pitch-based and energy-based\nsegmentation methods. Later, in 2001 Haus et al. used\na similar approach with some rules to deal with intona-\ntion issues [5], and in 2002, Clarisse et al. [6] contributed\nwith an auditory model, leading to later improved systems\n1http://www.singstar.com\nc\rEmilio Molina, Ana M. Barbancho, Lorenzo J. Tard ´on,\nIsabel Barbancho.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Emilio Molina, Ana M. Barbancho,\nLorenzo J. Tard ´on, Isabel Barbancho. “Evaluation framework for auto-\nmatic singing transcription”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.such as [7] (later included in MAMI project2and today in\nSampleSumo products3). Additionally, other more recent\napproaches use hidden Markov models (HMM) to detect\nnote-events in singing voice [8, 9, 11]. One of the most\nrepresentative HMM-based singing transcribers was pub-\nlished by Ryyn ¨anen in 2004 [9]. More recently, in 2013,\nanother probabilistic approach for singing transcription has\nbeen proposed in [3], also leading to relevant results. Re-\ngarding the evaluation methodologies used in these works\n(see Sections 2.1 and 3.1 for a review), there is not a stan-\ndard methodology.\nIn this paper, we present a comprehensive evaluation\nframework for singing transcription. This framework con-\nsists of a cross-annotated dataset (Section 2) and a novel,\ncompact set of evaluation measures (Section 3), which re-\nport information about the type of errors made by the sin-\nging transcriber. These measures have been integrated in\na freely available Matlab toolbox (see Section 3.3). Then,\nwe present a practical case in which the evaluation frame-\nwork has been used to perform a comparison in detail of\nseveral state-of-the-art singing transcribers (Section 4). Fi-\nnally, some relevant conclusions are presented in Section 5\n2. DATASETS\nIn this section, we review the evaluation datasets used in\nprior works on singing transcription , and we describe the\nproposed evaluation dataset and our strategy for ground-\ntruth annotation.\n2.1 Datasets used in prior works\nIn Table 1, we present the datasets used in some relevant\nworks on singing transcription. Note that none of the da-\ntasets fully represents the possible contexts in which sin-\nging transcription might be applied, since they are either\ntoo small (e.g. [5,6]), either very speciﬁc in style (e.g. [11]\nfor opera and [3] for ﬂamenco), or either they use an anno-\ntation strategy that may be subjective (e.g. [5, 6]), or only\nvalid for very good performances in rhythm and intonation\n(e.g. [8, 9]). In addition, only the ﬂamenco dataset used\nin [3] is freely available.\n2.2 Proposed dataset\nIn this section we describe the music collection, as well as\nthe annotation strategy used to build the ground-truth.\n2http://www.ipem.ugent.be/MAMI\n3http://www.samplesumo.com\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n567Author Year Dataset Audio Music Singing Ground-truth\n(GT) Tunning Freely\nsize quality style style annotation devs.\nanno- av\nai-\nstrategy tated in\nGT lable\nMcNab [4] 1996 NONE\nHaus & 2001 20 short Low\n& mode- Popular and Syllables: Annotated by No No\nPollastri [5] melodies rated noise scales ’na-na’... one musician\nClarisse 2002 22 short Low\n& mode- Popular Singing with\n& Annotation by No No\netal.\n[6] melodies rated noise without lyrics onemusician\nViitaniemi 2003 66 melodies High quality Folk\nsongs Singing, Original score\net al.\n[8] (120 (studio & scales humming used as No No\nRyyn ¨anen 2004 minutes) conditions) & whistling ground-truth\net al.\n[9]\nMulder 2004 52melo. Good &\nmode- Popular Syllables, Team No No\net al.\n[7] (1354 notes) rated noise songs singing & of\nwhistling musicologists\nKumar 2007 47 songs Good Indian Syllables: Manual annot.\nof No No\net al.\n[10] (2513 notes) music /la/ /da/\n/na/ vo\nwel onsets [REf]\nKrige 2008 13842 High quality Opera Time\nalign- No No\net al.\n[11] notes but\nstrong lessons Syllables ment using\nrev\nerberation & scales Viterbi\nG´omez & 2013 72 e\nxcerpts Good & Flamenco Lyrics\n& Musicians team Yes Yes\nBonada [3] (2803 notes) slightly noisy songs ornaments (cross-annotation)\nTable 1. Review of the evaluation datasets used in prior works on singing transcription. Some details about the dataset are\nnot provided in some cases, so certain ﬁelds can not be expressed in the same units (e.g. dataset size).\n2.2.1 Music collection\nThe proposed dataset consists of 38melodies sung by adult\nand child untrained singers, recorded in mono with a sam-\nple rate of 44100Hz and a resolution of 16bits. Generally,\nthe recordings are not clean and some background noise is\npresent. The duration of the excerpts ranges from 15to86\nseconds and the total duration of the whole dataset is 1154\nseconds. This music collection can be broken down into\nthree categories, according to the type of singer:\n\u000fChildren (our own recordings4):14melodies of tra-\nditional children songs (557 seconds) sung by 8dif-\nferent children (5-11 years old).\n\u000fAdult male: 13pop melodies (315 seconds) sung\nby8different adult male untrained singers. These\nrecordings were randomly chosen from the public\ndataset MTG-QBH5[12].\n\u000fAdult female: 11pop melodies (281 seconds) sung\nby5different adult female untrained singers, also\ntaken from MTG-QBH dataset.\nNote that in this collection the pitch and the loudness can\nbe unstable, and well performed vibratos are not frequent.\n2.2.2 Ground-truth: annotation strategy\nThe described music collection has been manually anno-\ntated to build the ground truth4. First, we have transcribed\nthe audio recordings with a baseline algorithm (Section\n4.2), and then all the transcription errors have been cor-\nrected by an expert musician with more than 10years of\nmusic training. Then, a second expert musician (with 7\nyears of music training) checked all the annotations until\nboth musicians agreed in their correctness. The transcrip-\ntion errors were corrected by listening, at the same time, to\nthe synthesized transcription and the original audio. The\n4Available at http://www.atic.uma.es/ismir2014singing\n5http://mtg.upf.edu/download/datasets/mtg-qbhmusicians were given a set of instructions about the spe-\nciﬁc criteria to annotate the singing melody:\n\u000fOrnaments such as pitch bending at the beginning\nof the notes or vibratos are not considered indepen-\ndent notes. This criterion is based on V ocaloid’s6\napproach, where ornaments are not modelled with\nextra notes.\n\u000fPortamento between two notes does not produce an\nextra third note (again, this is the criteria used in\nV ocaloid).\n\u000fThe onsets are placed at the beginning of voiced seg-\nments and in each clear change of pitch or phoneme.\nIn the case of ’l’, ’m’, ’n’ voiced consonants + vowel\n(e.g. ’la’), the onset is not placed at the beginning of\nthe consonant but at the beginning of the vowel.\n\u000fThe pitch of each note is annotated with cents reso-\nlution as perceived by the team of experts. Note that\nwe annotate the tuning deviation for each indepen-\ndent note.\n3. EV ALUATION MEASURES\nIn this section, we describe the evaluation measures used\nin prior works on automatic singing transcription, and we\npresent the proposed ones.\n3.1 Evaluation measures used in prior works\nIn Table 2, we review the evaluation measures used in some\nrelevant works on singing transcription. In some cases,\nonly the note and/or frame error is provided as a compact,\nrepresentative measure [5, 9], whereas other approaches\nprovide extra information about the type of errors made\nby the system using dynamic time warping (DTW) [6] or\nViterbi-based alignment [11]. In our case, we have taken\nthe most relevant aspects of these approaches and we added\nsome novel ideas in order to deﬁne a novel, compact and\ncomprehensive set of evaluations.\n6http://www.vocaloid.com\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n568Author Year Evaluation measures\nMcNab 1996 NONE\nHaus &\nPollastri [5]2001Rate of note pitch errors (segmen-\ntation errors are not considered)\nClarisse\net al. [6]2002DTW-based measurement of various\nnote errors, e.g. insertions deletions\nand substitutions.\nViitaniemi\net al. [8]2003Frame-based errors. Do not report\ninformation about type of errors\nmade.\nRyyn ¨anen\net al. [9]2004Note-based and frame-based errors.\nDo not report information about\ntype of errors made.\nMulder\net al. [7]2004DTW-based measurement of various\nnote errors, e.g. insertions deletions\nand substitutions.\nKumar\net al. [10]2007Onset detection errors (pitch and\ndurations are ignored).\nKrige\net al. [11]2008Viterbi-based measurement\nof deletions, insertions and\nsubstitutions (typical evaluation in\nspeech recognition).\nG´omez\n& Bonada [3]2013MIREX measures for audio\nmelody extraction\nand note-tracking. Do\nnot report information\nabout type of errors made.\nTable 2. Evaluation measures used in prior works on sin-\nging transcription.\n3.2 Proposed measures\nIn this section, we ﬁrstly present the notation and some\nneeded deﬁnitions that are used in the rest of sections, and\nthen we describe the evaluation measures used to quan-\ntify the proportion of correctly transcribed notes. Finally,\nwe present a set of novel evaluation measures that inde-\npendently report the importance of each type of error. In\nFigure 1 we show an example of the types of errors con-\nsidered.\nMIDI not e\n± 50ms± 20%duration±50 c ents\n606162\nCOnPOﬀ, COnP, COnCOnP, COnCOn59MIDI not e\n606162\nND\n59PUMMSMIDI not e\n606162\nOBO nOBP 59\nGROUND -TRUTH Transcription(a)\n(b)\n(c)\nFigure 1. Examples of the different proposed measures.\n3.2.1 Notation\nThei:th note of the ground-truth is noted as nGT\ni, and the\nj:th note of the transcription is noted as nTR\nj. The total\nnumber of notes in the ground-truth and the transcriptionareNGTandNTR, respectively. Regarding the expressions\nused in the for correct notes, we have used Precision, Re-\ncall and F-measure, which are deﬁned as follow:\nCX Precision =NGT\nCX\nNGT(1)\nCX Recall =NTR\nCX\nNTR(2)\nCX F-measure = 2\u0001CX Precision \u0001CX Recall\nCX Precision +CX Recall(3)\nwhere CXmakes reference to the speciﬁc category of cor-\nrect note: Correct Onset & Pitch & Offset (X = COnPOff),\nCorrect Onset & Pitch (X = COnP) or Correct Onset (X\n= COn). Finally, NGT\nCXandNTR\nCXare the total number of\nmatching CXconditions in the ground-truth and the tran-\nscription, respectively.\nRegarding the measures used for errors, we have com-\nputed the Error Rate with respect to NGT, or with respect\ntoNTR, as follow:\nXRateGT =NGT\nX\nNGT(4)\nXRateTR =NTR\nX\nNTR(5)\nFinally, in the case of segmentation errors (Section 3.2.5),\nwe also compute the mean number of notes tagged as Xin\nthe transcription for each note tagged as Xin the ground-\ntruth. This magnitude has been expressed as a ratio:\nXRatio =NTR\nX\nNGT\nX(6)\n3.2.2 Deﬁnition of correct onset/pitch/offset\nThe deﬁnitions of correctly transcribed notes (given in Sec-\ntion 3.2.3) consists of combinations of three independent\nconditions: correct onset, correct pitch and correct off-\nset. We have deﬁned these conditions according to MIREX\n(Multiple F0 estimation and tracking andAudio Onset De-\ntection tasks), and so they are deﬁned as follow:\n\u000fCorrect Onset: If the note’s onset of a transcribed note\nnTR\njis within a \u000650ms range of the onset of a ground-truth\nnotenGT\ni, i.e.:\nonset(nTR\nj)2[onset(nGT\ni)\u000050ms; onset(nGT\ni) + 50ms] (7)\nthen we consider that nGT\nihas a correct onset with respect\ntonTR\nj.\n\u000fCorrect Pitch: If the note’s pitch of a transcribed note\nnTR\njis within a \u00060:5 semitones range of the pitch of a\nground-truth note nGT\ni, i.e.:\npitch(nTR\nj)2[pitch(nGT\ni)\u00000:5st;pitch(nGT\ni) + 0:5 st](8)\nthen we consider that nGT\nihas a correct pitch with respect\ntonTR\nj.\n\u000fCorrect Offset: If the offsets of the ground-truth note\nnGT\niand the transcribed note nTR\njare within a range of\n\u000620% of the duration of nGT\nior\u000650 ms, whichever is\nlarger, i.e.:\noffset(nTR\nj)2[offset(nGT\ni)\u0000OffRan; offset(nGT\ni) +OffRan] (9)\nwhere OffRan =max(50ms; duration(nGT\ni)), then we con-\nsider that nGT\nihas a correct offset with respect to nTR\nj.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5693.2.3 Correctly transcribed notes\nThe deﬁnition of “correct note” should be useful to mea-\nsure the suitability of a given singing transcriber for a spe-\nciﬁc application. However, different applications may re-\nquire a different deﬁnition of correct note. Therefore, we\nhave chosen three different deﬁnitions of correct note as\ndeﬁned in MIREX:\n\u000fCorrect onset, pitch and offset (COnPOff): This is\na standard correctness criteria, since it is used in MIREX\n(Multiple F0 estimation and tracking task), and it is the\nmost restrictive one. The note nGT\niis assumed to be cor-\nrectly transcribed into the note nTR\njif it has correct on-\nset, correct pitch and correct offset (as deﬁned in Section\n3.2.2). In addition, one ground truth note nGT\nican only be\nassociated with one transcribed note nTR\nj. In our evalua-\ntion framework, we report Precision, Recall and F-measure\nas deﬁned in Section 3.2.1:\nCOnPOff Precision , COnPOff Recall and COnPOff F-measure .\n\u000fCorrect Onset, Pitch (COnP): This criteria is also used\nin MIREX, but it is less restrictive since it just considers\nonset and pitch, and ignores the offset value. Therefore,\nin COnP criteria, a note nGT\niis assumed to be correctly\ntranscribed into the note nTR\njif it has correct onset and\ncorrect pitch. In addition, one ground truth note nGT\nican\nonly be associated with one transcribed note nTR\nj. In our\nevaluation framework, we report Precision, Recall and F-\nmeasure:\nCOnP Precision , COnP Recall and COnP F-measure .\n\u000fCorrect Onset (COn): Additionally, we have included the\nevaluation criteria used in MIREX Audio Onset Detection\ntask. In this case, a note nGT\niis assumed to be correctly\ntranscribed into the note nTR\njif it has correct onset. In ad-\ndition, one ground truth note nGT\nican only be associated\nwith one transcribed note nTR\nj. In our evaluation frame-\nwork, we report Precision, Recall and F-measure:\nCOnPOff Precision , COnPOff Recall and COnPOff F-measure .\n3.2.4 Incorrect notes with one single error\nIn addition, we have included some novel evaluation mea-\nsures to identify the notes that are close to be correctly tran-\nscribed, but they fail in one single aspect. These measures\nare useful to identify speciﬁc weaknesses of a given sin-\nging transcriber. The proposed categories are:\n\u000fOnly-Bad-Onset (OBOn): A ground-truth note nGT\niis\nlabelled as OBOn if it has been transcribed into a note nTR\nj\nwith correct pitch and offset, but wrong onset. In order to\ndetect them, ﬁrstly we ﬁnd all ground-truth notes with cor-\nrect pitch and offset, taking into account that one ground-\ntruth note can only be associated with one transcribed note.\nThen, we remove all notes previously tagged as COnPOff\n(Section 3.2.3). The reported measure is the rate of OBOn\nnotes in the ground-truth:\nOBOn RateGT\n\u000fOnly-Bad-Pitch (OBP): A ground-truth note nGT\niis la-\nbelled as OBP if it has been transcribed into a note nTR\njwith correct onset and offset, but wrong pitch. In order to\ndetect them, ﬁrstly we ﬁnd all ground-truth notes with cor-\nrect onset and offset, taking into account that one ground-\ntruth note can only be associated with one transcribed note.\nThen, we remove all notes previously tagged as COnPOff\n(Section 3.2.3). The reported measure is the rate of OBP\nnotes in the ground-truth:\nOBP RateGT\n\u000fOnly-Bad-Offset (OBOff): A ground-truth note nGT\niis\nlabelled as OBOn if it has been transcribed into a note nTR\nj\nwith correct pitch and onset, but wrong offset. In order to\ndetect them, ﬁrstly we ﬁnd all ground-truth notes with cor-\nrect pitch and onset, taking into account that one ground-\ntruth note can only be associated with one transcribed note.\nThen, we remove all notes previously tagged as COnPOff\n(Section 3.2.3). The reported measure is the rate of OBOff\nnotes in the ground-truth:\nOBOff RateGT\n3.2.5 Incorrect notes with segmentation errors\nSegmentation errors refer to the case in which sung notes\nare incorrectly split or merged during the transcription. De-\npending on the ﬁnal application, certain types of segmenta-\ntion errors may not be important (e.g. frame-based systems\nfor query-by-humming are not affected by splits), but they\ncan lead to problems in many other situations. Therefore,\nwe have deﬁned two evaluation measures which are infor-\nmative about the segmentation errors made by the singing\ntranscriber.\n\u000fSplit (S): A split note is a ground truth note nGT\nithat\nis incorrectly segmented into different consecutive notes\nnTR\nj1,nTR\nj2\u0001\u0001\u0001nTR\njn. Two requirements are needed in a\nsplit: (1) the set of transcribed notes nTR\nj1; nTR\nj2; : : : nTR\njn\nmust overlap at least the 40% ofnGT\niin time (pitch is ig-\nnored), and (2) nGT\nimust overlap at least the 40% of every\nnotenTR\nj1; nTR\nj2; : : : nTR\njnin time (again, pitch is ignored).\nThese requirements are needed to ensure a consistent rela-\ntionship between ground truth and transcribed notes. The\nspeciﬁc reported measures are:\nSRateGT and S Ratio\nNote that in this case S Ratio>1.\n\u000fMerged (M): A set of consecutive ground-truth notes\nnGT\ni1,nGT\ni2,\u0001\u0001\u0001nGT\ninare considered to be merged if they\nall are transcribed into the same note nTR\nj. This is the com-\nplementary case of split. Again, two requirements must be\ntrue to consider a group of merged notes: (1) the set of\nground truth notes nGT\ni1,nGT\ni2,: : : nGT\ninmust overlap the\n40% ofnTR\njin time (pitch is ignored), and (2) nTR\njmust\noverlap the 40% of every note nGT\ni1,nGT\ni2,: : : nGT\ninin time\n(again, pitch is ignored). The speciﬁc reported measures\nare:\nMRateGT and M Ratio\nNote that in this case M Ratio<1.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5703.2.6 Incorrect notes with voicing errors\nV oicing errors happen when an unvoiced sound produces a\nfalse transcribed note (spurious note), or when a sung note\nis not transcribed at all (non-detected note). This situation\nis commonly associated to a bad performance of the voic-\ning stage within the singing transcriber. We have deﬁned\ntwo categories:\n\u000fSpurious notes (PU): A spurious note is a transcribed\nnotenTR\njthat does not overlap at all (neither in time nor in\npitch) any note in the ground truth. The associated reported\nmeasure is:\nPURateTR\n\u000fNon-detected notes (ND): A ground-truth note nGT\niis\nnon-detected if it does not overlap at all (neither in time\nnor in pitch) any transcribed note. The associated reported\nmeasure is:\nND RateGT\n3.3 Proposed Matlab toolbox\nThe presented evaluation measures have been implemented\nin a freely available Matlab toolbox4, which consists of a\nset of functions and structures, as well as a graphical user\ninterface to visually analyse the performance of the evalu-\nated singing transcriber.\nThe main function of our toolbox is evaluation.m,\nwhich receives the ground-truth and the transcription of an\naudio clip as inputs, and it outputs the results of all the\nevaluation measures. In addition, we have included a func-\ntion called listnotes.m, which receives as inputs the\nground-truth, the transcription and the category Xto be\nlisted, and it outputs a list (in a two-columns format: on-\nset time-offset time) of all the notes in the ground-truth\ntagged as Xcategory. This information is useful to isolate\nthe problematic audio excerpts for further analysis.\nFinally, we have implemented a graphical user inter-\nface, where the ground-truth and the transcription of a given\naudio clip can be compared using a piano-roll representa-\ntion. This interface also allows the user to highlight notes\ntagged as X(e.g. COnPOff, S, etc.).\n4. PRACTICAL USE OF THE PROPOSED\nTOOLBOX\nIn this section, we describe a practical case of use in which\nthe presented evaluation framework has been used to per-\nform an improved comparative study of several state-of-\nthe-art singing transcribers (presented in Section 4.1). In\naddition, a simple, easily reproducible baseline approach\nhas been included in this comparative study. Finally, we\nshow and discuss the obtained results.\n4.1 Compared algorithms\nWe have compared three state-of-the-art algorithms for sin-\nging transcription:\nMethod (a): G ´omez & Bonada (2013) [3]. It consists of\nthree main steps: tuning-frequency estimation, transcrip-\ntion into short notes, and an iterative process involving note\nconsolidation and reﬁnement of the tuning frequency. Forthe experiment, we have used a binary provided by the au-\nthors of the algorithm.\nMethod (b): Ryyn ¨anen (2008) [13]. We have used the\nmethod for automatic transcription of melody, bass line\nand chords in polyphonic music published by Ryyn ¨anen\nin 2008 [13], although we only focus on melody transcrip-\ntion. It is the last evolution of the original HMM-based\nmonophonic singing transcriber [9]. For the experiment,\nwe have used a binary provided by the authors of the algo-\nrithm.\nMethod (c): Melotranscript4(based on Mulder 2004\n[7]). It is the commercial version derived from the research\ncarried out by Mulder et al. [7]. It is based on an auditory\nmodel. For the experiment, we have used the demo version\navailable in SampleSumo website3.\n4.2 Baseline algorithm\nAccording to [8], the simplest possible segmentation con-\nsists of simply rounding a rough pitch estimate to the clos-\nest MIDI note niand taking all pitch changes as note bound-\naries. The proposed baseline method is based on such idea,\nand it uses Yin [14] to extract the F0 and aperiodicity at\nframe-level. A frame is classiﬁed as unvoiced if its ape-\nriodicity is under <0:4. Finally, all notes shorter than\n100ms are discarded.\n4.3 Results & discussion\nIn Figure 2 we show the results of our comparative analy-\nsis. Regarding the F-measure of correct notes (COnPOff,\nCOnP and COn), methods (a) and (c) attains similar values,\nwhereas method (b) performs slightly worse. In addition,\nit seems that method (a) is slightly superior to method (c)\nfor onset detection, but method (c) is superior when pitch\nand offset values must be also estimated. In all cases, the\nbaseline is clearly worse than the rest of methods.\nIn addition, we observed that the rate of notes with in-\ncorrect onset (OBOn) is equally high (20%) in all methods.\nAfter analysing the speciﬁc recordings, we concluded that\nonset detection within a range of \u000650ms is very restrictive\nin the case of singing voice with lyrics, since many onsets\nare not clear even for an expert musician (as proved during\nthe ground-truth building). Moreover, we also observed\nthat all methods, and especially method (a), have problems\nwith pitch bendings at the beginning of the notes, since\nthey tend to split them.\nRegarding the segmentation and voicing errors, we re-\nalised that method (a) tends to split notes, whereas method\n(b) tends to merge notes. This information, easily provided\nby our evaluation framework, may be useful to improve\nspeciﬁc weaknesses of the algorithms during the develop-\nment stage. Finally, we also realised that method (b) is\nworse than method (a) and (c) in terms of voicing.\nTo sum up, method (c) seems to be the best one in most\nmeasures, mainly due to a better performance in segmenta-\ntion and voicing. However, method (a) is very appropriate\nfor onset detection. Finally, although method (b) works\nclearly better than the baseline, has a poor performance\ndue to errors in segmentation (mainly merged notes) and\nvoicing (mainly spurious).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5710 0.2 0.4 0.6 0.8COnPOﬀ (Precision)\nCOnPOﬀ (Recall)\nCOnPOﬀ (F-measure)\nCOnP (Precision)\nCOnP (Recall)\nCOnP (F-measure)\nCOn (Precision)\nCOn (Recall)\nCOn (F-measure)\nOBOn (RateGT)\nOBP (RateGT)\nOBOﬀ (RateGT)\nSplit (RateGT)\nMerged (RateGT)\nSpurious (RateTR)\nNon-detected (RateTR)\nMeasure valueBaseline method\n(c) Melotranscript(b) Ryynänen(a) Gómez & BonadaFigure 2. Comparison in detail of several state-of-the-art\nsinging transcription systems using the presented evalua-\ntion framework.\n5. CONCLUSIONS\nIn this paper, we have presented an evaluation framework\nfor singing transcription. It consists of a cross-annotated\ndataset of 1154 seconds and a novel set of evaluation mea-\nsures, able to report the type of errors made by the sys-\ntem. Both the dataset, and a Matlab toolbox including the\npresented evaluation measures, are freely available4. In\norder to show the utility of the work presented in this pa-\nper, we have performed an detailed comparative study of\nthree state-of-the-art singing transcribers plus a baseline\nmethod, leading to relevant information about the perfor-\nmance of each method. In the future, we plan to expand our\nevaluation dataset in order to make it comparable to other\ndatasets7used in MIREX (e.g. MIR-1K or MIR-QBSH).\n6. ACKNOWLEDGEMENTS\nThis work has been funded by the Ministerio de Econom ´ıa\ny Competitividad of the Spanish Government under Project\nNo. TIN2013-47276-C6-2-R and by the Junta de Andaluc ´ıa\nunder Project No. P11-TIC-7154. The work has been done\nat Universidad de M ´alaga. Campus de Excelencia Interna-\ncional Andaluc ´ıa Tech.\n7. REFERENCES\n[1] M. Ryyn ¨anen, “Singing transcription,” in Signal Pro-\ncessing Methods for Music Transcription (A. Klapuri\nand M. Davy, eds.), pp. 361–390, Springer Science +\nBusiness Media LLC, 2006.\n[2] E. Molina, I. Barbancho, E. G ´omez, A. M. Barbancho,\nand L. J. Tard ´on, “Fundamental frequency alignment vs.\nnote-based melodic similarity for singing voice assess-\nment,” in Proceedings of the 2013 IEEE International\n7http://mirlab.org/dataSet/public/Conference on Acoustics, Speech and Signal Processing\nICASSP, pp. 744–748, 2013.\n[3] E. G ´omez and J. Bonada, “Towards computer-assisted\nﬂamenco transcription: An experimental comparison of\nautomatic transcription algorithms as applied to a cap-\npella singing,” Computer Music Journal, vol. 37, no. 2,\npp. 73–90, 2013.\n[4] R. J. McNab, L. A. Smith, and I. H. Witten, “Sig-\nnal Processing for Melody Transcription,” Proceedings\nof the 19th Australasian Computer Science Conference,\nvol. 18, no. 4, pp. 301–307, 1996.\n[5] G. Haus and E. Pollastri, “An audio front end for query-\nby-humming systems,” in Proceedings of the 2nd Inter-\nnational Symposium on Music Information Retrieval IS-\nMIR, pp. 65–72, sn, 2001.\n[6] L. P. Clarisse, J. P. Martens, M. Lesaffre, B. D. Baets,\nH. D. Meyer, and M. Leman, “An Auditory Model Based\nTranscriber of Singing Sequences,” in Proceedings of the\n3rd International Conference on Music Information Re-\ntrieval ISMIR, pp. 116–123, 2002.\n[7] T. De Mulder, J.P. Martens, M. Lesaffre, M. Leman, B.\nDe Baets, H. De Meyer, “Recent improvements of an\nauditory model based front-end for the transcription of\nvocal queries”, , Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Processing,\n(ICASSP 2004), Montreal, Quebec, Canada, May 17–21,\nV ol. IV , pp. 257–260, 2004.\n[8] T. Viitaniemi, A. Klapuri, and A. Eronen, “A probabilis-\ntic model for the transcription of single-voice melodies,”\ninProceedings of the 2003 Finnish Signal Processing\nSymposium FINSIG03, pp. 59–63, 2003.\n[9] M. Ryyn ¨anen and A. Klapuri, “Modelling of note events\nfor singing transcription,” in Proceedings of ISCA Tuto-\nrial and Research Workshop on Statistical and Percep-\ntual Audio Processing SAPA, (Jeju, Korea), Oct. 2004.\n[10] P. Kumar, M. Joshi, S. Hariharan, and P. Rao, “Sung\nNote Segmentation for a Query-by-Humming System”.\nInIntl Joint Conferences on Artiﬁcial Intelligence (IJ-\nCAI), 2007.\n[11] W. Krige, T. Herbst, and T. Niesler, “Explicit transition\nmodelling for automatic singing transcription,” Journal\nof New Music Research, vol. 37, no. 4, pp. 311–324,\n2008.\n[12] J. Salamon, J. Serr ´a and E. G ´omez, “Tonal Representa-\ntions for Music Retrieval: From Version Identiﬁcation to\nQuery-by-Humming”, International Journal of Multime-\ndia Information Retrieval, special issue on Hybrid Music\nInformation Retrieval, vol. 2, no. 1, pp. 45–58, 2013.\n[13] M. P. Ryyn ¨anen and A. P. Klapuri, “Automatic Tran-\nscription of Melody, Bass Line, and Chords in Poly-\nphonic Music,” in Computer Music Journal, vol.32, no.\n3, 2008.\n[14] A. De Cheveign ´e and H. Kawahara: “YIN, a fundamen-\ntal frequency estimator for speech and music,” Journal\nof the Acoustic Society of America , V ol. 111, No. 4, pp.\n1917-1930, 2002.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n572"
    },
    {
        "title": "The Importance of F0 Tracking in Query-by-singing-humming.",
        "author": [
            "Emilio Molina",
            "Lorenzo J. Tardón",
            "Isabel Barbancho",
            "Ana M. Barbancho"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416818",
        "url": "https://doi.org/10.5281/zenodo.1416818",
        "ee": "https://zenodo.org/records/1416818/files/MolinaTBB14.pdf",
        "abstract": "In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query- by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evalu- ation, we measured the QBSH performance for 189 differ- ent combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In ad- dition, we also found clear differences in robustness to F0 transcription errors between different matchers.",
        "zenodo_id": 1416818,
        "dblp_key": "conf/ismir/MolinaTBB14",
        "keywords": [
            "F0 trackers",
            "query-by-singing-humming",
            "MIR-QBSH dataset",
            "pub-style noise",
            "smartphone-style distortion",
            "audio-to-MIDI melodic matching",
            "state-of-the-art systems",
            "simple baseline method",
            "QBSH performance",
            "robustness to F0 transcription errors"
        ],
        "content": "THE IMPORTANCE OF F0 TRACKING IN\nQU\nERY-BY-SINGING-HUMMING\nEmilio Molina, Lorenzo J. Tard ´on, Isabel Barbancho, Ana M. Barbancho\nUniversidad de M´ alaga, ATIC Research Group, Andaluc´ ıa Tech,\nETSI Telecomunicaci´ on, Campus de Teatinos s/n, 29071 M´ alaga, SPAIN\nemm@ic.uma.es, lorenzo@ic.uma.es, ibp@ic.uma.es, abp@ic.uma.es\nABSTRACT\nIn this paper, we present a comparative study of several\nstate-of-the-art F0 trackers applied to the context of query-\nby-singing-humming (QBSH). This study has been carried\nout using the well known, freely available, MIR-QBSH\ndataset in different conditions of added pub-style noise and\nsmartphone-style distortion. For audio-to-MIDI melodic\nmatching, we have used two state-of-the-art systems and a\nsimple, easily reproducible baseline method. For the evalu-\nation, we measured the QBSH performance for 189 differ-\nent combinations of F0 tracker, noise/distortion conditions\nand matcher. Additionally, the overall accuracy of the F0\ntranscriptions (as deﬁned in MIREX) was also measured.\nIn the results, we found that F0 tracking overall accuracy\ncorrelates with QBSH performance, but it does not totally\nmeasure the suitability of a pitch vector for QBSH. In ad-\ndition, we also found clear differences in robustness to F0\ntranscription errors between different matchers.\n1. INTRODUCTION\nQuery-by-singing-humming (QBSH) is a music informa-\ntion retrieval task where short hummed or sung audio clips\nact as queries. Nowadays, several successful commercial\napplications for QBSH have been released, such as Musi-\ncRadar1or SoundHound2, and it is an active ﬁeld of re-\nsearch. Indeed, there is a task for QBSH in MIREX since\n2006, and every year novel and relevant approaches can be\nfound.\nTypically, QBSH approaches ﬁrstly extract the F0 con-\ntour and/or a note-level transcription for a given vocal query,\nand then a set of candidate melodies are retrieved from a\nlarge database using a melodic matcher module. In the lit-\nerature, many different approaches for matching in QBSH\ncan be found: statistical, note vs. note, frame vs. note,\nframe vs. frame. Generally, state-of-the-art systems for\nQBSH typically combines different approaches in order to\nachieve more reliable results [3, 12].\n1www.doreso.com\n2www.soundhound.com\nc/circlecopyrtEm ilio Molina, Lorenzo J. Tard´ on, Isabel Barbancho,\nAna M. Barbancho.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Emilio Molina, Lorenzo J. Tard´ on,\nIsabel Barbancho, Ana M. Barbancho. “The importance of F0 tracking in\nquery-by-singing-humming”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.However, even state-of-the-art systems for QBSH have\nnot a totally satisfactory performance in many real-world\ncases [1], so there is still room for improvement. Nowa-\ndays, some challenges related to QBSH are [2]: reliable\npitch tracking in noisy environments, automatic song data-\nbase preparation (predominant melody extraction and tran-\nscription), efﬁcient search in very large music collections,\ndealing with errors of intonation and rhythm in amateur\nsingers, etc.\nIn this paper, we analyse the performance of various\nstate-of-the-art F0 trackers for QBSH in different condi-\ntions of background noise and smartphone-style distortion.\nFor this study, we have considered three different melodic\nmatchers: two state-of-the-art systems (one of which ob-\ntained the best results in MIREX 2013), and a simple, eas-\nily reproducible baseline method based on frame-to-frame\nmatching using dynamic time warping (DTW). In Figure\n1, we show a scheme of our study.\n4431\n.wav\nqueriesControlled\naudio\ndegradationF0\ntrackingMelodic\nmatching2048\nMIDI\nsongsMatched\nsongs\n(Top-10)\nFigure 1 . Overall scheme of our study\nT\nhis paper is organized as follows: Section 2 and Sec-\ntion 3 present the studied algorithms for F0 tracking and\nmelodic matching, respectively. The evaluation strategy\nis presented in Section 4. Section 5 presents the obtained\nresults and Section 6 draws some conclusions about the\npresent study.\n2. F0 TRACKERS\nIn this section, we describe the F0 trackers considered in\nour study, together with their speciﬁc set of parameters.\nThe literature reports a wide set of algorithms oriented to\neither monophonic or polyphonic audio, so we have fo-\ncused on well-known, commonly used algorithms (e.g. Yin\n[4] or Praat-AC [8]), and some recently published algo-\nrithms for F0 estimation (e.g. pYin [6] or MELODIA [15]).\nMost of the algorithms analysed address F0 estimation in\nmonophonic audio, but we have also studied the perfor-\nmance of MELODIA, which is a method for predominant\nmelody extraction in polyphonic audio, using monophonic\naudio in noisy conditions. Regarding the used set of pa-\nrameters, when possible, they have been adjusted by trial\nand error using ten audio queries. The considered methods\nfor F0 tracking are the following ones:\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2772.1 YIN\nTh\ne Yin algorithm was developed by de Cheveign´ e and\nKawahara in 2002 [4]. It resembles the idea of the au-\ntocorrelation method [5] but it uses the cumulative mean\nnormalized difference function, which peaks at the local\nperiod with lower error rates than the traditional autocor-\nrelation function. In our study, we have used Matthias\nMauch’s V AMP plugin3in Sonic Annotator tool4.\nParameters used in YIN: step size = 80 samples (0.01\nseconds), Block size = 512 samples, Yin threshold = 0.15.\n2.2 pYIN\nThe pYin method has been published by Mauch in 2014\n[6], and it basically adds a HMM-based F0 tracking stage\nin order to ﬁnd a “smooth” path through the fundamen-\ntal frequency candidates obtained by Yin. Again, we have\nused the original Matthias Mauch’s V AMP plugin3in Sonic\nAnnotator tool4.\nParameters used in PYIN: step size = 80 samples (0.01\nseconds), Block size = 512 samples, Yin threshold distri-\nbution = Beta (mean 0.15).\n2.3 AC-DEFAULT and AC-ADJUSTED (Praat)\nPraat is a well-known tool for speech analysis [7], which\nincludes several methods for F0 estimation. In our case,\nwe have chosen the algorithm created by P. Boersma in\n1993 [8]. It is based on the autocorrelation method, but it\nimproves it by considering the effects of the window dur-\ning the analysis and by including a F0 tracking stage based\non dynamic programming. This method has 9 parameters\nthat can be adjusted to achieve a better performance for a\nspeciﬁc application. According to [9], this method signif-\nicantly improves its performance when its parameters are\nadapted to the input signal. Therefore, we have experi-\nmented not only with the default set of parameters (AC-\nDEFAULT), but also with an adjusted set of parameters in\norder to limit octave jumps and false positives during the\nvoicing process (AC-ADJUSTED). In our case, we have\nused the implementation included in the console Praat tool.\nParameters used in AC-DEFAULT: Time step = 0.01\nseconds, Pitch ﬂoor = 75Hz, Max. number of candidates =\n15, Very accurate = off, Silence threshold = 0.03, V oicing\nthreshold = 0.45, Octave cost = 0.01, Octave-jump cost =\n0.35, V oiced / unvoiced cost = 0.15, Pitch ceiling = 600\nHz.\nParameters used in AC-ADJUSTED: Time step = 0.01\nseconds, Pitch ﬂoor = 50Hz, Max. number of candidates =\n15, Very accurate = off, Silence threshold = 0.03, V oicing\nthreshold = 0.45, Octave cost = 0.1, Octave-jump cost =\n0.5, V oiced / unvoiced cost = 0.5, Pitch ceiling = 700 Hz.\n2.4 AC-LEIWANG\nIn our study we have also included the exact F0 tracker\nused in Lei Wang’s approach for QBSH [3], which ob-\ntained the best results for most of the datasets in MIREX\n2013. It is based on P. Boersma’s autocorrelation method\n3http://code.soundsoftware.ac.uk/projects/pyin\n4http://www.vamp-plugins.org/sonic-annotator/[8], but it uses a ﬁnely tuned set of parameters and a post-\nprocessing stage in order to mitigate spurious and octave\nerrors. This F0 tracker is used in the latest evolution of a\nset of older methods [11, 12] also developed by Lei Wang\n(an open source C++ implementation is available5).\n2.5 SWIPE’\nThe Swipe’ algorithm was published by A. Camacho in\n2007 [10]. This algorithm estimates the pitch as the funda-\nmental frequency of the sawtooth waveform whose spec-\ntrum best matches the spectrum of the input signal. The\nalgorithm proved to outperform other well-known F0 esti-\nmation algorithms, and it is used in the F0 estimation stage\nof some state-of-the-art query-by-humming systems [13].\nIn our study, we have used the original author’s Matlab\nimplementation6. The Matlab code does not provide a\nvoiced / unvoiced classiﬁcation of frames, but it outputs\na strength vector Swhich has been used for it. Speciﬁ-\ncally, a frame is considered voiced if its strength is above\na threshold Sth, otherwise they are considered unvoiced.\nParameters used in SWIPE’: DT (hop-size) = 0.01 sec-\nonds, pmin = 50 Hz, pmax = 700Hz, dlog2p = 1/48 (de-\nfault), dERBs = 0.1 (default), woverlap = 0.5 (default),\nvoicing threshold Sth= 0.3.\n2.6 MELODIA-MONO and MELODIA-POLY\nMELODIA is a system for automatic melody extraction in\npolyphonic music signals developed by Salamon in 2012\n[15]. This system is based on the creation and character-\nisation of pitch contours, which are time continuous se-\nquences of pitch candidates grouped using auditory stream-\ning cues. Melodic and non-melodic contours are distin-\nguished depending on the distributions of its characteris-\ntics. The used implementation is MELODIA V AMP plu-\ngin7in Sonic Annotator tool4. This plugin has two de-\nfault sets of parameters, adapted to deal with monophonic\nor polyphonic audio. We have experimented with both of\nthem, and therefore we have deﬁned two methods: MELO-\nDIA-MONO and MELODIA-POLY .\nParameters used in MELODIA-MONO: Program = Mono-\nphonic, Min Frequency = 55Hz, Max Frequency = 700Hz,\nV oicing Tolerance = 3,00, Monophonic Noise Filter = 0,00,\nAudio block size = 372 (not conﬁgurable), Window incre-\nment = 23 (not conﬁgurable).\nParameters used in MELODIA-POLY: Program = Poly-\nphonic, Min Frequency = 55Hz, Max Frequency = 700Hz,\nV oicing Tolerance = 0,20, Monophonic Noise Filter = 0,00,\nAudio block size = 372 (not conﬁgurable), Window incre-\nment = 23 (not conﬁgurable).\nNote that the time-step in this case can not be directly\nset to 0.01 seconds. Therefore, we have linearly interpo-\nlated the pitch vector in order to scale it to a time-step of\n0.01 seconds.\n5http://www.atic.uma.es/ismir2014qbsh/\n6http://www.cise.uﬂ.edu/ acamacho/publications/swipep.m\n7http://mtg.upf.edu/technologies/melodia\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2783. AUDIO-TO-MIDI MELODIC MATCHERS\nIn\nthis section, we describe the three considered methods\nfor audio-to-MIDI melodic matching: a simple baseline\n(Section 3.1) and two state-of-the-art matchers (Sections\n3.2 and 3.3).\n3.1 Baseline approach\nWe have implemented a simple, freely available5base-\nline approach based on dynamic time warping (DTW) for\nmelodic matching. Our method consists of four steps (a\nscheme is shown in Figure 2):\n(1) Model building: We extract one pitch vector Pk(in\nMIDI number) for every target MIDI song k∈1...N songs\nusing a hop-size of 0.01 seconds. Then we replace un-\nvoiced frames (rests) in Pkby the pitch value of the pre-\nvious note, except for the case of initial unvoiced frames,\nwhich are directly removed (these processed pitch vectors\nare labelled as P∗k). Then, each pitch vector P∗k∀k∈\n1...N songs is truncated to generate 7 pitch vectors with\nlengths [500, 600, 700, 800, 900, 1000, 1100] frames (cor-\nresponding to the ﬁrst 5, 6, 7, 8, 9, 10 and 11 seconds\nof the target MIDI song, which are reasonable durations\nfor an user query). We label these pitch vectors as P∗k\n5s,\nP∗k\n6s, . . .P∗k\n11s. Finally, all these pitch vectors are resam-\npled (through linear interpolation) to a length of 50points,\nand then zero-mean normalized (for a common key trans-\nposition), leading to P50∗k\nDuration∀Duration ∈5s...11sand\n∀k∈1...N songs. These vectors are then stored for later\nusage. Note that this process must be done only once.\n(2) Query pre-processing: The pitch vector PQof a\ngiven .wav query is loaded (note that all pitch vectors are\ncomputed with a hopsize equal to 0.01 seconds). Then, as\nin step (1), unvoiced frames are replaced by the pitch value\nof the previous note, except for the case of initial unvoiced\nframes, which are directly removed. This processed vector\nis then converted to MIDI numbers with 1 cent resolution,\nand labelled as P∗Q. Finally, P∗Qis resampled (using\nlinear interpolation) to a length L= 50 and zero-mean\nnormalized (for a common key transposition), leading to\nP50∗Q.\n(3) DTW-based alignment: Now we ﬁnd the optimal\nalignment between P50∗Qand all pitch vectors P50∗k\nDuration\n∀Duration ∈5s...11sand∀k∈1...N songs using dy-\nnamic time warping (DTW). In our case, each cost matrix\nCDuration ,kis built using the squared difference:\nCDuration,k(i,j) = (P50∗Q(i)−P50∗k\nDuration(j))2(1)\nWherekis the target song index, Duration represents the\ntruncation level (from 5s to 11s), and i,jare the time in-\ndices of the query pitch vector P50∗Qand the target pitch\nvectorP50∗k\nDuration , respectively. The optimal path is now\nfound using Dan Ellis’ Matlab implementation for DTW\n[16] (dpfast.m function), with the following allowed\nsteps and associated cost weights [∆i,∆j,W]:[1,1,1],\n[1,0,30],[0,1,30],[1,2,5],[2,1,5]. The allowed steps\nand weights have been selected in order to penalize 0or90\nangles in the optimal path (associated to unnatural align-\nments), and although they lead to acceptable results, they\nmay not be optimal.\n50\n51−20−1010\n10\n551010101515202020202020202525303030303535354040404545101010\n2020\n303030303030\n4040\n505050\n45101010\n510152025303540455010\n20\n30\n40\n50\n10\n10\n10\n0 5 10 15 20 25 30 35 40−20−100\nCandidate\n−20−10\n0 5 10 15 20 25 30 35 40 47−20−10010\n−20−10\n−20−10\n−20−10Candidate\nQueryCandidateCandidateCandidateCandidateCandidateCandidate\nCandidateQueryQueryCandidate\nQueryQueQueryQueryQueryQuery02004006008000100200300\nTime (frame)Frequency (Hz)\n01020304050−505\nTime (resampled)Semitone\n510152025303540455010\n20\n30\n40\n50\n0 5 10 15 20 25 30 35 40 45−10−50510\n  \nP\nPT\nop-10 song list -- DTW costQuer\ny pitch vector processing\n     D\nynamic Time Warping−10010\n−10010\n−10010\n−10010\n−10010\n−10010\n−10010\n−10010\n01020304050−100100 10 20 30 40 500 10 20 30 40 5000013.mid 00014.mid 00012.midINPUT\n: Query 00013.w av pitch vector S\nongs DB model\n00013.mid -- 10.07\n01662.mid -- 36.42\n00599.mid -- 46.26\n01235.mid -- 46.26\n01418.mid -- 47.51\n00320.mid -- 48.91\n00843.mid -- 49.26\n02004.mid -- 51.15\n01235.mid -- 52.75\n00014.mid -- 57.56P50\n*1211s\nP50*135s\nP50*136s\nP50*137s\nP50*138s\nP50*139s\nP50*1310s\nP50*1311s\nP50*145sP50*Q\n50*13\n50*Q8sC8s,13...\n...PQ\nFigure 2 . Scheme of the proposed baseline method for\na\nudio-to-MIDI melody matching.\n(4) Top-10 report: Once the P50∗Qhas been aligned\nwith all target pitch vectors (a total of 7×Nsongs vectors,\nsince we use 7 different durations), the matched pitch vec-\ntors are sorted according to their alignment total cost (this\nvalue consists of the matrix Dproduced by dpfast.m\nevaluated in the last position of the optimal path, Tcost=\nD(p(end),q(end)) ). Finally, the 10 songs with mini-\nmum cost are reported.\n3.2 Music Radar’s approach\nMusicRadar [3] is a state-of-the-art algorithm for melodic\nmatching, which participated in MIREX 2013 and obtained\nthe best accuracy in all datasets, except for the case of IOA-\nCAS8. It is the latest evolution of a set of systems devel-\noped by Lei Wang since 2007 [11, 12]. The system takes\nadvantage of several matching methods to improve its ac-\ncuracy. First, Earth Mover’s Distance (EMD), which is\nnote-based and fast, is adopted to eliminate most unlikely\ncandidates. Then, Dynamic Time Warping (DTW), which\nis frame-based and more accurate, is executed on these sur-\nviving candidates. Finally, a weighted voting fusion strat-\negy is employed to ﬁnd the optimal match. In our study,\nwe have used the exact melody matcher tested in MIREX\n2013, provided by its original author.\n3.3 NetEase’s approach\nNetEase’s approach [13] is a state-of-the-art algorithm for\nmelodic matching, which participated in MIREX 2013 and\n8http://www.music-ir.org/mirex/wiki/2013:Query by-\nSi\nnging/Humming\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n279obtained the ﬁrst position for IOACAS dataset8, a s well\nas relevant results in the rest of datasets. This algorithm\nadopts a two-stage cascaded solution based on Locality\nSensitive Hashing (LSH) and accurate matching of frame-\nlevel pitch sequence. Firstly, LSH is employed to quickly\nﬁlter out songs with low matching possibilities. In the sec-\nond stage, Dynamic Time Warping is applied to ﬁnd the\nN (set to 10) most matching songs from the candidate list.\nAgain, the original authors of NetEase’s approach (who\nalso authored some older works on query-by-humming [14])\ncollaborated in this study, so we have used the exact melody\nmatcher tested in MIREX 2013.\n4. EV ALUATION STRATEGY\nIn this section, we present the datasets used in our study\n(Section 4.1), the way in which we have combined F0 track-\ners and melody matchers (Section 4.2) and the chosen eval-\nuation measures (Section 4.3).\n4.1 Datasets\nWe have used the public corpus MIR-QBSH8(used in\nMIREX since 2005), which includes 4431 .wav queries\ncorresponding to 48 different MIDI songs. The audio que-\nries are 8 seconds length, and they are recorded in mono\n8 bits, with a sample rate of 8kHz. In general, the au-\ndio queries are monophonic with no background noise, al-\nthough some of them are slightly noisy and/or distorted.\nThis dataset also includes a manually corrected pitch vec-\ntor for each .wav query. Although these annotations are\nfairly reliable, they may not be totally correct, as stated in\nMIR-QBSH documentation.\nIn addition, we have used the Audio Degradation Tool-\nbox [17] in order to recreate common environments where\na QBSH system could work. Speciﬁcally, we have com-\nbined three levels of pub-style added background noise\n(PubEnvironment1 sound) and smartphone-style dis-\ntortion (smartPhoneRecording degradation), leading\nto a total of seven evaluation datasets: (1) Original MIR-\nQBSH corpus (2) 25 dB SNR (3) 25 dB SNR + smartphone\ndistortion (4) 15 dB SNR (5) 15 dB SNR + smartphone\ndistortion (6) 5 dB SNR (7) 5 dB SNR + smartphone dis-\ntortion. Note that all these degradations have been checked\nin order to ensure perceptually realistic environments.\nFinally, in order to replicate MIREX conditions, we have\nincluded 2000 extra MIDI songs (randomly taken from ES-\nSEN collection9) to the original collection of 48 MIDI\nsongs, leading to a songs collection of 2048 MIDI songs.\nNote that, although these 2000 extra songs ﬁt the style of\nthe original 48 songs, they do not correspond to any .wav\nquery of Jang’s dataset.\n4.2 Combinations of F0 trackers and melody\nmatchers\nFor each of the 7 datasets, the 4431 .wav queries have\nbeen transcribed using the 8 different F0 trackers men-\ntioned in Section 2. Additionally, each dataset also in-\ncludes the 4431 manually corrected pitch vectors of MIR-\nQBSH as a reference, leading to a total of 7 datasets ×(8\n9www.esac-data.org/F0 trackers + 1 manual annotation) ×4431 queries = 63 ×\n4431 queries = 279153 pitch vectors. Then, all these pitch\nvectors have been used as input to the 3 different melody\nmatchers mentioned in Section 3, leading to 930510 lists\nof top-10 matched songs. Finally, these results have been\nused to compute a set of meaningful evaluation measures.\n4.3 Evaluation measures\nIn this section, we present the evaluation measures used in\nthis study:\n(1) Mean overall accuracy of F0 tracking ( Acc ov) :\nFor each pitch vector we have computed an evaluation mea-\nsures deﬁned in MIREX Audio Melody Extraction task:\noverall accuracy (Acc ov) (a deﬁnition can be found in [15]).\nThe mean overall accuracy is then deﬁned as Acc ov=\n(\n1/N)/summationtextN\ni=1Acc ovi, whereNis the total number of que-\nries considered and Acc oviis the overall accuracy of the\npitch vector of the i:th query. We have selected this mea-\nsure because it considers both voicing and pitch, which are\nimportant aspects in QBSH. For this measure, our ground\ntruth consists of the manually corrected pitch vectors of\nthe .wav queries, which are included in the original MIR-\nQBSH corpus.\n(2) Mean Reciprocal Rank (MRR): This measure is\ncommonly used in MIREX Query By Singing Humming\ntask8, and it is deﬁned as: MRR = (1/N)/summationtextN\ni=1r−1\ni,\nwhereNis the total number of queries considered and ri\nis the rank of the correct answer in the retrieved melodies\nfori:th query.\n5. RESULTS & DISCUSSION\nIn this section, we present the obtained results and some\nrelevant considerations about them.\n5.1Acc ova nd MRR for each F0 tracker - Dataset -\nMatcher\nIn Table 1, we show the Acc ova nd the MRR obtained for\nthe whole dataset of 4431 .wav queries in each combina-\ntion of F0 tracker-dataset-matcher (189 combinations in\ntotal). Note that these results are directly comparable to\nMIREX Query by Singing/Humming task8(Jang Dataset).\nAs expected, the manually corrected pitch vectors produce\nthe best MRR in most cases (the overall accuracy is 100%\nbecause it has been taken as the ground truth for such mea-\nsure). Note that, despite manual annotations are the same\nin all datasets, NetEase and MusicRadar matchers do not\nproduce the exact same results in all cases. It is due to the\ngeneration of the indexing model (used to reduced the time\nsearch), which is not a totally deterministic process.\nRegarding the relationship between Acc ova nd MRR in\nthe rest of F0 trackers, we ﬁnd a somehow contradictory\nresult: the best Acc ovd oes not always correspond with\nthe best MRR. This fact may be due to two different rea-\nsons. On the one hand, the meaning of Acc ovm ay be dis-\ntorted due to annotation errors in the ground truth (as men-\ntioned in Section 4.1), or to eventual intonation errors in\nthe dataset. However, the manual annotations produce the\nbest MRR, what suggests that the amount of these types\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n280F0 Clean 25dB SNR 25 dB SNR 15dB SNR 15 dB SNR 5dB SNR 5 dB SNR\ntracker dataset + distortion + distortion + distortion\n(A) 100 / 0.82 / 0.89 / 0.96 100 / 0.82 / 0.89 / 0.96 100 / 0.82 / 0.89 / 0.95 100 / 0.82 / 0.89 / 0.96 100 / 0.82 / 0.89 / 0.96 100 / 0.82 / 0.89 / 0.96 100 / 0.82 / 0.88 / 0.95\n(B) 89 /0.80/0.89/0.96 89 /0.80/0.89/0.96 88/0.80/0.88/0.95 88 /0.79/0.88/0.94 84 / 0.71 / 0.86 / 0.94 78 / 0.50 / 0.73 / 0.85 67 / 0.33 / 0.57 / 0.73\n(C) 90/ 0.74 / 0.85 / 0.94 90 / 0.71 / 0.85 / 0.92 86 / 0.72 / 0.84 / 0.92 89 / 0.71 / 0.84 / 0.92 85 / 0.66 / 0.81 / 0.89 72 / 0.49 / 0.58 / 0.70 64 / 0.26 / 0.39 / 0.51\n(D) 90 / 0.71 / 0.83 / 0.92 90/ 0.74 / 0.85 / 0.93 85 / 0.74 / 0.85 / 0.94 90/ 0.78 / 0.87 / 0.94 85/0.77/0.87/0.94 79 /0.69/0.79/0.87 72 /0.58/0.69/0.81\n(E) 89 / 0.71 / 0.83 / 0.92 89 / 0.71 / 0.84 / 0.92 84 / 0.66 / 0.80 / 0.91 88 / 0.72 / 0.84 / 0.93 83 / 0.65 / 0.80 / 0.91 75 / 0.67 / 0.67 / 0.82 66 / 0.48 / 0.53 / 0.73\n(F) 86 / 0.62 / 0.81 / 0.89 86 / 0.70 / 0.83 / 0.92 81 / 0.64 / 0.78 / 0.89 82 / 0.60 / 0.77 / 0.88 75 / 0.50 / 0.67 / 0.82 48 / 0.03 / 0.08 / 0.04 44 / 0.04 / 0.04 / 0.03\n(G) 88 / 0.56 / 0.81 / 0.88 87 / 0.47 / 0.79 / 0.86 83 / 0.47 / 0.76 / 0.85 86 / 0.39 / 0.78 / 0.87 81 / 0.35 / 0.73 / 0.82 70 / 0.11 / 0.32 / 0.52 63 / 0.04 / 0.20 / 0.38\n(H) 87 / 0.66 / 0.83 / 0.87 87 / 0.67 / 0.82 / 0.87 83 / 0.64 / 0.78 / 0.84 86 / 0.66 / 0.81 / 0.84 82 / 0.58 / 0.74 / 0.80 83 / 0.51 / 0.73 / 0.75 73 / 0.32 / 0.55 / 0.62\n(I) 84 / 0.62 / 0.76 / 0.86 84 / 0.62 / 0.76 / 0.86 79 / 0.50 / 0.64 / 0.74 84 / 0.63 / 0.76 / 0.86 79 / 0.50 / 0.65 / 0.75 83/ 0.60 / 0.73 / 0.83 75/ 0.39 / 0.55 / 0.65\nTable 1 : F0 overall accuracy and MRR obtained for each case. F0 trackers: (A) M ANUALLY CORRECTED (B)AC-\nLEIWANG (C)AC-ADJUSTED (D)PYIN (E)SWIPE’ (F)YIN (G)AC-DEFAULT (H)MELODIA-MONO (I)MELODIA-\nPOLY . The format of each cell is: Acc ov( %) / MRR-baseline /MRR-NetEase /MRR-MusicRadar .\nof errors are low. On the other hand, the measure Acc ov\ni\ntself may not be totally representative of the suitability of\na pitch vector for QBSH. Indeed, after analysing speciﬁc\ncases, we observed that two pitch vectors with same F0\ntracking accuracy (according to MIREX measures) may\nnot be equally suitable for query-by-humming. For in-\nstance, we analysed the results produced by the baseline\nmatcher using two different pitch vectors (Figure 3) with\nexactly the same evaluation measures in MIREX Audio\nMelody Extraction task: vocing recall = 99.63% ,voic-\ning false-alarm = 48.40% ,raw pitch accuracy = 97.41% ,\nraw-chroma accuracy = 97.41% andoverall accuracy =\n82.91% . However, we found that pitch vector (a) matches\nthe right song with rank ri= 1 whereas pitch vector (b)\ndoes not matches the right song at all ( ri≥11). The rea-\nson is that MIREX evaluation measures do not take into\naccount the pitch values of false positives, but in fact they\nare important for QBSH. Therefore, we conclude that the\nhigh MRR achieved by some F0 trackers (AC-LEIW ANG\nwhen background noise is low, and PYIN for highly de-\ngraded signals), is not only due to the amount of errors\nmade by them, but also to the type of such errors.\nAdditionally, we observed that, in most cases, the que-\nries are matched either with rank ri= 1orri≥11(inter-\nmediate cases such as rank ri= 2 orri= 3 are less fre-\nquent). Therefore, the variance of ranks is generally high,\ntheir distribution is not Gaussian.\n5.2 MRR vs. Acc ovf or each matcher\nIn order to study the robustness of each melodic matcher to\nF0 tracking errors, we have represented the MRR obtained\nby each one for different ranges of Acc ov( Figure 4). For\nthis experiment, we have selected only the .wav queries\nwhich produce the right answer in ﬁrst rank for the three\nmatchers considered (baseline, Music Radar and NetEase)\nwhen manually corrected pitch vectors are used (around a\n70% of the dataset matches this condition). In this way, we\nensure that bad singing or a wrong manual annotation is\nnot affecting the variations of MRR in the plots. Note that,\nin this case, the results are not directly comparable to the\nones computed in MIREX (in contrast to the results shown\nin Section 5.1).150200250300\n  \n100 200 300 400 500 600 700 800100150200250300\n  Reference\nPitch vector (a)\nTime (frame = 0.01s)\nFrequency (Hz)Frequency (Hz)Reference\nPitch vector (b)\nFigure 3 . According to MIREX measures, these two pitch\nv\nectors (manually manipulated) are equally accurate; how-\never, they are not equally suitable for QBSH.\nRegarding the obtained results (shown in Figure 4), we\nobserve clear differences in the robustness to F0 estima-\ntion errors between matchers, which is coherent with the\nresults presented in Table 1. The main difference is found\nin the baseline matcher with respect to both NetEase and\nMusic Radar. Given that the baseline matcher only uses\nDTW, whereas the other two matchers use a combination\nof various searching methods (see Sections 3.2 and 3.3),\nwe hypothesise that such combination may improve their\nrobustness to F0 tracking errors. However, further research\nis needed to really test this hypothesis.\n6. CONCLUSIONS\nIn this paper, eight different state-of-the-art F0 trackers\nwere evaluated for the speciﬁc application of query-by-\nhumming-singing in different conditions of pub-style added\nnoise and smartphone-style distortion. This study was car-\nried out using three different matching methods: a simple,\nfreely available baseline (a detailed description has been\nprovided in Section 3.1) and two state-of-the-art match-\ners. In our results, we found that Boersma’s AC method\n[8], with an appropriate adjustment and a smoothing stage\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2810 16 32 45 55 65 73 81 88 94 10000.20.40.550.650.750.790.830.880.941\nF0 estimation Overall Accuracy (%) MRRM\nusic Radar matcher\nNe\ntEase matcher\nBaseline matcher\nFigure 4 . MRR obtained for each range of Overall Accu-\nr\nacy (each range is marked with coloured background rect-\nangles). We have considered only the .wav queries which,\nusing manually corrected F0 vectors, produce MRR = 1\nin all matchers.\nachieves the best results when the audio is not very de-\ngraded. In contrast, when the audio is highly degraded, the\nbest results are obtained with pYIN [6], even without fur-\nther smoothing. Considering that pYIN is a very recent,\nopen source approach, this result is promising in order to\nimprove the noise robustness of future QBSH systems. Ad-\nditionally, we found that F0 trackers perform differently on\nQBSH depending on the type of F0 tracking errors made.\nDue to this, MIREX measures do not fully represent the\nsuitability of a pitch vector for QBSH purposes, so the de-\nvelopment of novel evaluation measures in MIREX is en-\ncouraged to really measure the suitability of MIR systems\nfor speciﬁc applications. Finally, we observed clear differ-\nences between matchers regarding their robustness to F0\nestimation errors. However, further research is needed for\na deeper insight into these differences.\n7. ACKNOWLEDGEMENTS\nSpecial thanks to Doreso1team (especially to Lei Wang and\nYuhang Cao) and to Peng Li for their active collaboration in\nthis study. This work has been funded by the Ministerio de\nEconom´ ıa y Competitividad of the Spanish Government un-\nder Project No. TIN2013-47276-C6-2-R and by the Junta de\nAndaluc´ ıa under Project No. P11-TIC-7154. The work has\nbeen done at Universidad de M´ alaga. Campus de Excelencia\nInternacional Andaluc´ ıa Tech.\n8. REFERENCES\n[1] A. D. Brown and Brighthand staff: “SoundHound\nfor Android OS Review: ’Name That Tune,’ But At\nWhat Price?”, Brighhand Smartphone News & Re-\nview, 2012. Online: www.brighthand.com [Last Ac-\ncess: 28/04/2014]\n[2] J. -S. Roger Jang: “QBSH and AFP as Two Success-\nful Paradigms of Music Information Retrieval” Course\ninRuSSIR , 2013. Available at: http://mirlab.org/jang/\n[Last Access: 28/04/2014][3] Doreso Team (www.doreso.com): “MIREX 2013\nQBSH Task: Music Radar’s Solution” Extended ab-\nstract for MIREX , 2013.\n[4] A. De Cheveign´ e and H. Kawahara: “YIN, a fun-\ndamental frequency estimator for speech and music,”\nJournal of the Acoustic Society of America , V ol. 111,\nNo. 4, pp. 1917-1930, 2002.\n[5] L. Rabiner: “On the use of autocorrelation analysis for\npitch detection,” Acoustics, Speech and Signal Process-\ning, IEEE Transactions on, , V ol. 25, No.1, pp. 24-33.\n1977.\n[6] M. Mauch, and S. Dixon, “pYIN: A fundamental fre-\nquency estimator using probabilistic threshold distribu-\ntions,” Proceedings of ICASSP , 2014.\n[7] P. Boersma and D. Weenink: “Praat: a system for do-\ning phonetics by computer,” Glot international , V ol.\n5, No. 9/10, pp. 341-345, 2002. Software available at:\nwww.praat.org [Last access: 28/04/2014]\n[8] P. Boersma: “Accurate short-term analysis of the fun-\ndamental frequency and the harmonics-to-noise ratio of\na sampled sound,” Proceedings of the Institute of Pho-\nnetic Sciences , V ol. 17, No. 1193, pp 97-110, 1993.\n[9] E. Keelan, C. Lai, K. Zechner: “The importance of\noptimal parameter setting for pitch extraction,” Jour-\nnal of Acoustical Society of America , V ol. 128, No. 4,\npp. 2291–2291, 2010.\n[10] A. Camacho: “SWIPE: A sawtooth waveform inspired\npitch estimator for speech and music,” PhD disserta-\ntion, University of Florida, 2007.\n[11] L. Wang, S. Huang, S. Hu, J. Liang and B. Xu: “An\neffective and efﬁcient method for query-by-humming\nsystem based on multi-similarity measurement fusion,”\nProceedings of ICALIP , 2008.\n[12] L. Wang, S. Huang, S. Hu, J. Liang and B. Xu:\n“Improving searching speed and accuracy of query\nby humming system based on three methods: fea-\nture fusion, set reduction and multiple similarity mea-\nsurement rescoring,” Proceedings of INTERSPEECH ,\n2008.\n[13] P. Li, Y . Nie and X. Li: “MIREX 2013 QBSH Task:\nNetease’s Solution” Extended abstract for MIREX ,\n2013.\n[14] P. Li, M. Zhou, X. Wang and N. Li: “A novel MIR\nsystem based on improved melody contour deﬁnition,”\nProceedings of the International Conference on Multi-\nMedia and Information Technology (MMIT) , 2008.\n[15] J. Salamon and E. G´ omez: “Melody Extraction from\nPolyphonic Music Signals using Pitch Contour Char-\nacteristics,” IEEE Transactions on Audio, Speech and\nLanguage Processing , V ol. 20, No. 6, pp. 1759–1770,\n2012.\n[16] D. Ellis: “Dynamic Time Warp (DTW)\nin Matlab”, 2003. Web resource, available:\nwww.ee.columbia.edu/ dpwe/resources/matlab/dtw/\n[Last Access: 28/04/2014]\n[17] M. Mauch and S. Ewert: “The Audio Degradation\nToolbox and its Application to Robustness Evaluation,”\nProceedings of ISMIR , 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n282"
    },
    {
        "title": "Taste Space Versus the World: an Embedding Analysis of Listening Habits and Geography.",
        "author": [
            "Joshua L. Moore",
            "Thorsten Joachims",
            "Douglas R. Turnbull"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415178",
        "url": "https://doi.org/10.5281/zenodo.1415178",
        "ee": "https://zenodo.org/records/1415178/files/MooreJT14.pdf",
        "abstract": "Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the “taste space” in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In par- ticular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city- artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location.",
        "zenodo_id": 1415178,
        "dblp_key": "conf/ismir/MooreJT14",
        "keywords": [
            "Probabilistic embedding methods",
            "discrete objects",
            "human interaction data",
            "compact and accurate predictive model",
            "compact and flexible representation",
            "taste space",
            "Million Musical Tweets Dataset",
            "cultural and geographical differences",
            "listening patterns",
            "spatial representations"
        ],
        "content": "TASTE SPACE VERSUS THE WORLD: AN EMBEDDING ANALYSIS OF\nLISTENING HABITS AND GEOGRAPHY\nJoshua L. Moore, Thorsten Joachims\nCornell University, Dept. of Computer Science\nfjlmo|tjg@cs.cornell.eduDouglas Turnbull\nIthaca College, Dept. of Computer Science\ndturnbull@ithaca.edu\nABSTRACT\nProbabilistic embedding methods provide a principled way\nof deriving new spatial representations of discrete objects\nfrom human interaction data. The resulting assignment\nof objects to positions in a continuous, low-dimensional\nspace not only provides a compact and accurate predictive\nmodel, but also a compact and ﬂexible representation for\nunderstanding the data. In this paper, we demonstrate how\nprobabilistic embedding methods reveal the “taste space”\nin the recently released Million Musical Tweets Dataset\n(MMTD), and how it transcends geographic space. In par-\nticular, by embedding cities around the world along with\npreferred artists, we are able to distill information about\ncultural and geographical differences in listening patterns\ninto spatial representations. These representations yield a\nsimilarity metric among city pairs, artist pairs, and city-\nartist pairs, which can then be used to draw conclusions\nabout the similarities and contrasts between taste space and\ngeographic location.\n1. INTRODUCTION\nEmbedding methods are a type of machine learning algo-\nrithm for distilling large amounts of data about discrete ob-\njects into a continuous and semantically meaningful rep-\nresentation. These methods can be applied even when\nonly contextual information about the objects, such as co-\noccurrence statistics or usage data, is available. For this\nreason and due to the easy interpretability of the result-\ning models, embeddings have become popular for tasks\nin many ﬁelds, including natural language processing, in-\nformation retrieval, and music information retrieval. Re-\ncently, embeddings have been shown to be a useful tool for\nanalyzing trends in music listening histories [6].\nIn this paper, we learn embeddings that give insight into\nhow music preferences relate to geographic and cultural\nboundaries. Our input data is the Million Musical Tweets\nDataset (MMTD), which was recently collected and cu-\nrated by Hauger et al. [3]. This dataset consists of over a\nmillion tweets containing track plays and rich geograph-\nical information in the form of globe coordinates, which\nc\rJoshua L. Moore, Thorsten Joachims, Douglas Turnbull.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Joshua L. Moore, Thorsten Joachims,\nDouglas Turnbull. “Taste Space Versus the World: an Embedding Anal-\nysis of Listening Habits and Geography”, 15th International Society for\nMusic Information Retrieval Conference, 2014.Hauger et al. have matched to cities and other geographic\ndescriptors as well. Our goal in this work is to use em-\nbedding methods to enable a more thorough analysis of\ngeographic and cultural patterns in this data by embedding\ncities and the artists from track plays in those cities into\na joint space. The resulting taste space gives us a way to\ndirectly measure city/city, city/artist, and artist/artist afﬁni-\nties. After verifying the predictive ﬁdelity of the learned\ntaste space, we explore the surprisingly clear segmenta-\ntions in taste space across geographic, cultural, and lin-\nguistic borders. In particular, we ﬁnd that the taste space\nof cities gives us a remarkably clear image of some cultural\nand linguistic phenomena that transcend geography.\n2. RELATED WORK\nEmbeddings methods have been applied to many different\nmodeling and information retrieval tasks. In the ﬁeld of\nmusic IR, these models have been used for tag prediction\nand song similarity metrics, as in the work of Weston et\nal. [7]. However, instead of a prediction task such as this,\nwe intend to focus on data analysis tasks. Therefore, we\nrely on generative models like those proposed in our previ-\nous work [5, 6] and by Aizenberg et al [1]. Our prior work\nuses models which rely on sequences of songs augmented\nwith social tags [5] or per-user song sequences with tempo-\nral dynamics [6]. The aim of this work differs from that of\nour previous work in that we are interested in aggregate\nglobal patterns and not in any particular playlist-related\ntask, so we do not adopt the notion of song sequences. We\nalso are concerned with geographic differences in listen-\ning patterns, and so we ignore individual users in favor of\nembedding entire cities into the space.\nAizenberg et al. utilize generative models like those in\nour work for purposes of building a recommendation en-\ngine for music from Internet radio data on the web. How-\never, their work focuses on building a powerful recommen-\ndation system using freely available data, and does not fo-\ncus on the use of the resulting models for data analysis, nor\ndo they concern themselves with geographic data.\nThe data set which we will use throughout this work\nwas published by Hauger et al. [3]. The authors of this\nwork crawled Twitter for 17 months, looking for tweets\nwhich carried certain key words, phrases, or hashtags in\norder to ﬁnd posts which signal that a user is listening to a\ntrack and for which the text of the tweet could be matched\nto a particular artist and track. In addition, the data was se-\nlected for only tweets with geographical tags (in the form\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n439of GPS coordinates), and temporal data was retained. The\nﬁnal product is a large data set of geographically and tem-\nporally tagged music plays. In their work, the authors em-\nphasize the collection of this impressive data set and a thor-\nough description of the properties of the data set. The au-\nthors do add some analyses of the data, but the geographic\nanalysis is limited to only a few examples of coarse pat-\nterns found in the data. The primary contribution of our\nwork over the work presented in that paper is to greatly\nextend the scope of the geographic analysis, presenting a\nmuch clearer and more exhaustive view of the differences\nin musical taste across regions, countries, and languages.\nFinally, we describe how geographic information can\nbe useful for various music IR tasks. Knopke [4] also\ndiscusses how geospatial data can be exploited for music\nmarketing and musicological research. We use embedding\nas a tool to further explore these topics. Others, such as\nLamere’s Roadtrip Mixtape1app, have developed systems\nthat use a listeners location to generate a playlist of rele-\nvant music by local artists.\n3. PROBABILISTIC EMBEDDING MODEL\nThe embedding model used in this paper is similar to the\none used in our previous work [6]. However, the following\nanalysis focuses on geographical patterns instead of tem-\nporal dynamics and trends. In particular, we focus on the\nrelationships among cities and artists, and so we elect to\ncondense the geographical information in a tweet down to\nthe city from which it came. Similarly, we discard the track\nname from each tweet and use only the artist for the song.\nThis leads to a joint embedding of cities and artists.\nAt the core of the embedding model lies a probabilis-\ntic link function that connects the observed data to the\nunderlying semantic space. Intuitively, the link function\nwe use states that the probability Pr(ajc) of a given city\ncplaying a given artist ais proportional to the distance\njjX(c)\u0000Y(a)jj2\n2between that city and that artist in a Eu-\nclidean embedding space of a chosen dimension d.X(c)\nandY(a)are the embedding locations of city cand artist\narespectively. Similar to previous works, we also incor-\nporate a popularity bias term pafor each artist to model\nglobal popularity. More formally, the probability for a city\ncto play an artist ais:\nPr(ajc) =exp(\u0000jjX (c)\u0000Y(a)jj2\n2+pa)P\na02Aexp(\u0000jjX (c)\u0000Y(a0)jj2\n2+pa0):\nThe sum in the denominator is over the set Aof artists.\nThis sum is known as the partition function, denoted Z(\u0001),\nand serves to normalize the distribution over artists.\nDetermining the embedding locations X(c)andY(a)\nfor all cities and artists (and the popularity terms pa) is\nthe learning problem the embedding method must solve.\nTo ﬁt a model to the data, we maximize the log-likelihood\nformed by the sum of log-probabilities log(Pr(a ijci):\n1http://labs.echonest.com/CityServer/roadtrip.html(X;Y;p) = max\nX;Y;pX\n(ci;ai)2Dlog(Pr(a ijci))\n= max\nX;Y;pX\n(ci;ai)2D\u0000jjX (ci)\u0000Y(ai)jj2\n2+pai\u0000log(Z(ai)):\nWe solve this optimization problem using a Stochastic\nGradient Descent approach. First, each embedding vec-\ntorX(\u0001)andY(\u0001)is randomly initialized to a point in the\nunit ball in Rd(for the chosen dimension d). Then, the\nmodel parameters are updated in sequential stochastic gra-\ndient steps until convergence. The partition function Z(\u0001)\npresents an optimization challenge, in that a na ¨ıve opti-\nmization strategy requires O(jAj2)time for each pass over\nthe data. For this work, we used our C++ implementa-\ntion of the efﬁcient training method employed in [6], an\napproximate method that estimates the partition function\nfor efﬁcient training. This implementation is available by\nrequest, and will later be available on the project website,\nhttp://lme.joachims.org.\n3.1 Interpretation of Embedding Space\nAs deﬁned above, the model gives us a joint space in which\nboth cities and artists are represented through their respec-\ntive embedding vectors X(\u0001)andY(\u0001). Related works have\nfound such embedding spaces to be rich with semantic sig-\nniﬁcance, compactly condensing the patterns present in the\ntraining data. Distances in embedding space reveal rela-\ntionships between objects, and visual or spatial inspection\nof the resulting models quickly reveals a great deal of seg-\nmentation in the space. In particular, joint embeddings\nyield similarity metrics among the various types of em-\nbedded objects, even though individual dimensions in the\nembedding space have no explicit meaning (e.g. the em-\nbeddings are rotation invariant). In our case, this speciﬁ-\ncally entails the following three measures of similarity:\nCity to Artist: this is the only similarity metric explic-\nitly formulated in the model, and it reﬂects the distribution\nPr(ajc) that we directly observe data for. In particular, we\ndirectly optimize the positions of cities and artists so that\ncities have a high probability of listening to artists which\nthey were observed playing in the dataset. This requires\nplacing the city and artist nearby in the embedding space,\nso proximity in the embedding space can be interpreted as\nan afﬁnity between a city and an artist.\nArtist to Artist: due to the learned conditional proba-\nbility distributions’ being constrained by the metric space,\ntwo artists which are placed near each other in the space\nwill have a similar probability mass in each city’s distribu-\ntion. This implies a kind of exchangeability or similarity,\nsince any city which is likely to listen to one artist is likely\nto listen to the other in the model distribution.\nCity to City: ﬁnally, the form of similarity on which we\nwill most rely in this work is that among cities. Again due\nto the metric space, two nearby cities will assign similar\nmasses to each artist, and so will have very similar distri-\nbutions over artists in the model. This implies a similarity\nin musical taste or preferred artists between two cities.\nThe third type of similarity will form the basis for most\nof the analyses in this paper. In particular, we are interested\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n440Figure 1: Precision at kof our model, a cosine similar-\nity baseline, a tweet count ranking baseline, and a random\nbaseline on a city/artist tweet prediction task.\nin the connection between the metric space of cities in the\nembedding space and another metric space: the one formed\nby the geographic distribution of cities on the Earth’s sur-\nface. As we will see, these two spaces differ greatly, and\nthe taste space of cities gives us a clear image of some cul-\ntural and linguistic phenomena that transcend geography.\n4. EXPERIMENTS\nWe use the MMTD data set presented by Hauger et al. [3].\nThis data set contains nearly 1.1 million tweets with ge-\nographical data. We pre-process the data by condensing\neach tweet to a city/artist pair, which results in a city/artist\nafﬁnity matrix used to train the model. Next, we discard all\ncities and artists which have not appeared at least 100 times\nin the data, as well as all cities for which fewer than 30 dis-\ntinct users tweeted from that city. The post-processed data\ncontains 1,017 distinct cities and 1,499 distinct artists.\nFor choosing model parameters, we randomly selected\n80% of the tweets for the training set, and the remaining\n20% for the validation set. This resulted in a training set of\n390,077 tweets and a validation set of 97,592 tweets. We\nused the validation set both to determine stopping criteria\nfor the optimization as well as to choose the initial stochas-\ntic gradient step size \u00110from the setf0.25, 0.1, 0.05, 0.01g\nand to evaluate the quality of models of dimension f2, 50,\n100g. The optimal step size varied from model to model,\nbut the 100-dimensional model consistently out-performed\nthe others (although the difference between it and the 50-\ndimensional model was small).\nWe will analyze the data through the trained embedding\nmodels, both through spatial analyses (i.e. nearest neigh-\nbor queries and clusterings) and through visual inspection.\nIn general, the high-dimensional model better captures the\ndata, and so we will use it when direct visual inspection is\nnot required. But ﬁrst, we evaluate the quality of the model\nthrough quantitative means.4.1 Quantitative Evaluation of the Model\nBefore we inspect our model in order to make qualitative\nclaims about the patterns in the data, we ﬁrst wish to eval-\nuate it on a quantitative basis. This is essential in order\nto conﬁrm that the model accurately captures the relations\namong cities and artists, which will offer validation for the\nconclusions we draw later in the work.\n4.1.1 Evaluating Model Fidelity\nFirst, we considered the performance of the model in\nterms of perplexity, which is a reformulation of the log-\nlikelihood objective outside of a log scale. This is a com-\nmonly used measure of performance in other areas of re-\nsearch where models similar to ours are used, such as\nnatural language processing [2]. The perplexity pis re-\nlated to the average log-likelihood Lby the transformation\np= exp(\u0000L).\nOur baseline is the unigram distribution, which as-\nsumes that Pr(ajc) is directly proportional to the number\nof tweets artist areceived in the entire data set indepen-\ndent of the city. Estimating the unigram distribution from\nthe training set and using it to calculate the perplexity on\nthe validation set yielded a perplexity of 589 (very similar\nto the perplexity attained when estimating this distribution\nfrom the train set and calculating the perplexity on the train\nset itself). Our model offered a great improvement over\nthis – the 100-dimensional model yielded a perplexity on\nthe validation set of 290, while the 2-dimensional model\nreached a perplexity of 357. This improvement suggests\nthat our model has captured a signiﬁcant amount of useful\ninformation from the data.\n4.1.2 Evaluating Predictive Accuracy\nSecond, we created a task to evaluate the predictive power\nof our model. To this end, we split the data chronologically\ninto two halves, and further divided the ﬁrst half into a\ntraining set and a validation set. Using the ﬁrst half of the\ndata, we trained a 100-dimensional model. Our goal is to\nuse this model to predict which new artists various cities\nwill begin listening to in the second half of the data.\nWe accomplish this by considering, for each city, the set\nof artists which had no observed tweets in that city in the\nﬁrst half of the data. We then sorted these artists by their\nscore in the model – namely, for city cand artista, the\nfunction\u0000jjX (c)\u0000Y(a)jj2\n2+pa. Using this ordering as\na ranking function, we calculated the precision at kof our\nranking for various values of k, where an artist is consid-\nered to be relevant if that artist receives at least one tweet\nfrom that city in the second half of the data. We average\nthe results of each city’s ranking.\nWe compare the performance of our model on this task\nto three baselines. First, we consider a random ranking of\nall the artists which a city has not yet tweeted. Second,\nwe sort the yet untweeted artists by their raw global tweet\ncount in the ﬁrst half of the data – which we label the un-\nigram baseline. Third, we use the raw artist tweet counts\nfor a city’s nearest neighbor city in the ﬁrst half of data to\nrank untweeted artists for that city. In this case, the nearest\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n441neighbor is not determined using our embedding but rather\nbased on the maximum cosine similarity between the vec-\ntor of artist tweet counts for the city and the vectors of\ntweet count for all other cities.\nThe results can be seen in Figure 1. At k= 1, our model\ncorrectly guesses an artist that a city will later tweet with\n64% accuracy, compared to 46% for the cosine similar-\nity, 42% for unigram and around 5% for the random base-\nline. This advantage is consistent as kincreases, with our\nmethod attaining about 24% precision at 100, compared to\n18% for unigram and 14% for cosine similarity. We also\nshow the performance of the same model at this task when\npopularity terms are excluded from the scoring function at\nranking time. Interestingly, the performance in this case\nis still quite good. We see precision at 1 of about 51% in\nthis case, with the gap between this method and the method\nwith popularity terms growing smaller as kincreases. This\nsuggests that proximity in the space is very meaningful,\nwhich is an important validation of the analyses to follow.\nFinally, the good performance on this task invites an ap-\nplication of the space to making marketing predictions –\nwhich cities are prone to pick up on which artists in the\nnear future? – but we leave this for future work.\n4.2 Visual Inspection of the Embedding Space\nIn Figure 2 we present plots of the two-dimensional em-\nbedding space, with labels for some key cities (left) and\nartists (right). Note that the two plots are separated by city\nand artists only for readability, and that all points lie in\nthe same space. In this ﬁgure, we can already see a strik-\ning segmentation in city space, with extreme distinction\nbetween, e.g., Brazilian cities, Southeast Asian cities, and\nAmerican cities. We can also already see distinct regional\nand cultural groupings in some ways – the U.S. cities\nlargely form a gradient, with Chicago, Atlanta, Washing-\nton, D.C., and Philadelphia in the middle, Cleveland and\nDetroit on one edge of the cluster, and New York and Los\nAngeles on the opposite edge. Interestingly, Toronto is also\non the edge of the U.S. cluster, and on the same edge where\nNew York and Los Angeles – arguably the most “interna-\ntional” of the U.S. cities shown here – end up.\nIt is also interesting to note that the space has a very\nclear segmentation in terms of genre – just as clear as em-\nbeddings produced in previous work from songs alone [5]\nor songs and individual users [6]. Of course, this does not\ntranslate into an effective user model – surely there are\nmany users in Recife, Brazil that would quickly tire of a\nradio station inspired by Linkin Park – but we believe it is\nstill a meaningful phenomenon. Speciﬁcally, this suggests\nthat the taste of the average listener can vary dramatically\nfrom one city to the next, even within the same country.\nMore surprisingly, this variation in the average user is so\ndramatic that cities themselves can form nearly as coherent\na taste space as individual users, as the genre segmentation\nis barely any less clear than in other authors’ work with\nuser modeling.\n4.3 Higher-dimensional Models\nDirectly visualizing two-dimensional models can give us\nstriking images from which rough patterns can be easilygleaned. However, higher dimensional models are able to\nachieve perplexities on the validation set which far exceed\nthose of lower dimensional models. For example, as men-\ntioned before, our best performing 2-dimensional model\nattains a validation perplexity of 357, while our best per-\nforming 100-dimensional model attains a perplexity of 290\non the validation set. This suggests that higher dimensional\nmodels capture more of the nuanced patterns present in the\ndata. On the other hand, simple plotting is no longer suf-\nﬁcient to inspect high-dimensional data – we must resort\nto alternative methods, for example, clustering and nearest\nneighbor queries. First, in Figure 3, we present the re-\nsults of using k-means clustering in the city space of the\n100-dimensional model. The common algorithm for solv-\ning thek-means clustering problem is known to be prone\nto getting stuck in local optima, and in fact can be difﬁ-\ncult to validate properly. We attempted to overcome these\nproblems by using cross validation and repeated random\nrestarts. Speciﬁcally, we used 10-fold cross-validation on\nthe set of all cities in order to ﬁnd a validation objective for\neach candidate value of kfrom 2 to 20. Then, we selected\nthe parameter kby choosing the largest value for which no\nlarger value offers more than a 5% improvement over the\nimmediately previous value.\nOnce the value of kwas chosen, we tried to overcome\nthe problem of local optima by running the clustering al-\ngorithm 10 times on the entire set of cities with that value\nofkand different random initializations, ﬁnally choosing\nthe trial with the best objective value. This process resulted\nin optimalkvalues ranging from 6 to 13. Smaller values\nresulted in some clusterings with granularity too coarse to\nsee interesting patterns, while larger values were noisy and\nproduced unstable clusterings. Ultimately, we found that\nk= 9was a good trade-off.\nAdditionally, in Table 1, we obtain a complementary\nview of the 100-dimensional embedding by listing the re-\nsults of nearest-neighbor queries for some well-known,\nhand-selected cities. These queries give us an alternative\nperspective of the city space, pointing out similarities that\nmay not be apparent from the clustering alone. By com-\nbining these views, we can start to see many interesting\npatterns arise:\nThe French-speaking supercluster: French-speaking\ncities form an extremely tight cluster, as can also be seen\nin the 2-dimensional embedding in Figure 2. Virtually ev-\nery French city is part of this cluster, as well as French-\nspeaking cities in nearby European countries, such as\nBrussels and Geneva. Indeed even beyond the top 10 listed\nin Table 1, almost all of the top 100 nearest neighbors for\nParis are French-speaking. Language is almost certainly\nthe biggest factor in this effect, but if we consider the coun-\ntries near France, we see that despite linguistic divides, in\nthe clustering, many cities in the U.K. still group closely\nwith Dutch cities and even Spanish cities. Furthermore,\nthis grouping can be seen in every view of the data – in\nthe two-dimensional space, the clustering, and the nearest\nneighbor queries. It should be noted that in our own tri-\nals clustering the data, the French cluster is one of the ﬁrst\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n442Figure 2: The joint city/artist space with some key cities and artists labeled.\nFigure 3: A k-means clustering of cities around the world with k= 9.\nKuala Lumpur Paris Singapore Los Angeles, CA Chicago, IL S˜ao Paulo\nKulim Boulogne-Billancourt Hougang Grand Prairie, TX Buffalo, NY Osasco\nSungai Lembing Brussels Seng Kang Ontario, CA Clarksville, TN Jundia ´ı\nIpoh Rennes USJ9 Riverside, CA Cleveland, OH Carapicu ´ıba\nKuching Lille Subang Sacramento, CA Durham, NC Ribeir ˜ao Pires\nSunway City Aix-en-Provence Kota Bahru Salinas, CA Birmingham, AL Shinjuku\nSeremban Limoges Bangkok Paterson, NJ Flint, MI Vargem Grande Paulista\nSeri Kembangan Amiens Alam Damai San Bernardino, CA Montgomery, AL Santa Maria\nTaman Cheras Hartamas Marseille Kota Padawan Inglewood, CA Nashville, TN Itapevi\nKuantan Geneva Glenmarie Modesto, CA Jackson, MS Cascavel\nSelayang Grenoble Budapest Pomona, CA Paterson, NJ Embu das Artes\nBrooklyn, NY Atlanta, GA Madrid Amsterdam Sydney Montr ´eal\nMinneapolis, MN Savannah, GA Sevilla Eindhoven Toronto Montpellier\nWinston-Salem, NC Tallahassee, FL Granada Tilburg Denver, CO Geneva\nArlington, V A Cleveland, OH Barcelona Emmen Windhoek Raleigh, NC\nWaterbury, CT Washington, DC Murcia Nijmegen Angers Limoges\nWashington, DC Memphis, TN Sorocaba Enschede Rialto, CA Angers\nSyracuse, NY Flint, MI Ponta Grossa Zwolle Hamilton Ontario, CA\nJersey City, NJ Huntsville, AL Huntington Beach, CA Amersfoort Rotterdam Anchorage, AK\nLouisville, KY Montgomery, AL Istanbul Maastricht Ottawa Nice\nTallahassee, FL Jackson, MS Vigo Antwerp London - Tower Hamlets Lyon\nOntario, CA Lafayette, LA Oxford Coventry London - Southwark Rennes\nTable 1: Nearest neighbor query results in 100-dimensional city space. Brooklyn was chosen over New York, NY due to\nhaving more tweets in the data set. In addition, only result cities with population at least 100,000 are displayed.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n443Country Least typical Most typical\nBrazil Crici ´uma, Santa Catarina Itapevi, S ˜ao Paulo\nCanada Surrey, BC Toronto, ON\nNetherlands Leiden Emmen\nMexico Campeche, CM Cuauht ´emoc, DF\nIndonesia Panunggangan Barat RW 02\nFrance Bordeaux Mantes-la-Jolie, ˆIle-de-France\nUnited States Huntington Beach, CA Jackson, MS\nMalaysia Kota Damansara Kuala Lumpur\nUnited Kingdom Wolverhampton, England London Borough of Camden\nRussia Ufa Podgory\nSpain ´Alora, Andalusia Barcelona\nTable 2: Most and least typical cities in taste proﬁle for\nvarious countries.\nclusters to become apparent, as well as one of the most\nconsistent to appear. We can also see that the French clus-\nter is indeed a linguistic and cultural one which is not just\ndue to geographic proximity: although Montreal has sev-\neral nearest neighbors in North America, it is present in\nthe French group in the k-means clustering (as is Quebec\nCity) and is also very close to many French-speaking cities\nin Europe, such as Geneva and Lyon. We can also see that\nAbidjan, Ivory Coast joins the French k-means cluster, as\ndo Dakar in Senegal, Les Abymes in Guadeloupe and Le\nLamentin and Fort-de-France in Martinique – all cities in\ncountries which are members of the Francophonie.\nAustralia: Here again, despite the relatively tight geo-\ngraphical proximity of Australia and Southeast Asia, and\nthe geographic isolation of Australia from North America,\nAustralian cities tend to group closely with Canadian cities\nand some cities in the United Kingdom. One way of see-\ning this is the fact that Sydney’s nearest neighbors include\nToronto, Hamilton, Ontario, Ottawa, and two of London’s\nboroughs. In addition, other cities in Australia also be-\nlong to a cluster that mainly includes cities in the Com-\nmonwealth (e.g., U.K., Canada).\nCultural divides in the United States: the cities in the\nU.S. tend to form at least two distinct subgroups in terms\nof listening patterns. One group contains many cities in the\nSoutheast and Midwest, as well as a few cities on the south-\nern edge of what some might call the Northeast (Philadel-\nphia, for example). The other group consists primarily of\ncities in the Northeast, on the West Coast, and in the South-\nwest of the country, including most of the cities in Texas.\nIntuitively, there are two results that might be surprising\nto some here. The ﬁrst is that the listening patterns of\nChicago tend to cluster with listening patterns in the South\nand the rest of the Midwest, and not those of very large\ncities on the coasts (after all, Chicago is the third-largest\ncity in the country). The second is that Texas groups with\nthe West Coast and Northeast, and not with the Southeast,\nwhich would be considered by many to be more culturally\nsimilar in many ways.\n4.4 Most and least typical cities\nWe can also consider the relation of individual cities to\ntheir member countries. For this analysis, we considered\nall the countries which have at least 10 cities represented\nin the data. Then for each country we calculated the aver-\nage position in embedding space of cities in that country.\nWith this average city position, we can then measure the\ndistance of individual cities from the mean of cities in their\ncountry and ﬁnd the cities which have the most and leasttypical taste proﬁles for that country.\nThe results are shown in Table 2. We can see a few inter-\nesting patterns here. First, in Brazil, the most typical city is\nan outlying city near S ˜ao Paulo city, while the least typical\nis a city in Santa Catarina, the second southernmost state in\nBrazil, which is also less populous than the southernmost,\nRio Grande do Sul, which was also well-represented in the\ndata. In Canada, the least typical city is an edge city on\nVancouver’s east side, while the most typical is the largest\ncity, Toronto. In France, the most typical city is in ˆIle-de-\nFrance, not too far from Paris. We also see in England that\nthe least typical city is Wolverhampton, and edge city of\nBirmingham towards England’s industrial north, while the\nmost typical is a borough of London.\n5. CONCLUSIONS\nIn this work, we learned probabilistic embeddings of the\nMillion Musical Tweets Dataset, a large corpus of tweets\ncontaining track plays which has rich geographical infor-\nmation for each play. Through the use of embeddings, we\nwere able to easily process a large amount of data and sift\nthrough it visually and with spatial analysis in order to un-\ncover examples of how musical taste conforms to or tran-\nscends geography, language, and culture. Our ﬁndings re-\nﬂect that differences in culture and language, as well as his-\ntorical afﬁnities among countries otherwise separated by\nvast distances, can be seen very clearly in the differences in\ntaste among average listeners from one region to the next.\nMore generally, this paper shows how nuanced patterns in\nlarge collections of preference data can be condensed into\na taste space, which provides a powerful tool for discover-\ning complex relationships. Acknowledgments: This work\nwas supported by NSF grants IIS-1217485, IIS-1217686,\nIIS-1247696, and an NSF Graduate Research Fellowship.\n6. REFERENCES\n[1] N. Aizenberg, Y . Koren, and O. Somekh. Build your\nown music recommender by modeling internet radio\nstreams. In WWW, pages 1–10. ACM, 2012.\n[2] Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent,\nand Christian Janvin. A neural probabilistic language\nmodel. JMLR, 3:1137–1155, 2003.\n[3] D. Hauger, M. Schedl, A. Ko ˇsir, and M. Tkal ˇciˇc. The\nmillion musical tweets dataset - what we can learn from\nmicroblogs. In ISMIR, 2013.\n[4] I. Knopke. Geospatial location of music and sound ﬁles\nfor music information retrieval. ISMIR, 2005.\n[5] J. L. Moore, S. Chen, T. Joachims, and D. Turnbull.\nLearning to embed songs and tags for playlist predic-\ntion. In ISMIR, 2012.\n[6] J. L. Moore, Shuo Chen, T. Joachims, and D. Turnbull.\nTaste over time: the temporal dynamics of user prefer-\nences. In ISMIR, 2013.\n[7] J. Weston, S. Bengio, and P. Hamel. Multi-tasking with\njoint semantic spaces for large-scale music annotation\nand retrieval. JNMR, 40(4):337–348, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n444"
    },
    {
        "title": "A Combined Thematic and Acoustic Approach for a Music Recommendation Service in TV Commercials.",
        "author": [
            "Mohamed Morchid",
            "Richard Dufour",
            "Georges Linarès"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415098",
        "url": "https://doi.org/10.5281/zenodo.1415098",
        "ee": "https://zenodo.org/records/1415098/files/MorchidDL14.pdf",
        "abstract": "We hypothesize that different genres of writing use dif- ferent adjectives for the same concept. We test our hy- pothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four lead- ing newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popu- lar English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of ad- jectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these proba- bility distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more like- ly to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry.",
        "zenodo_id": 1415098,
        "dblp_key": "conf/ismir/MorchidDL14",
        "keywords": [
            "hypothesis",
            "genres",
            "different adjectives",
            "English Wikipedia",
            "news articles",
            "articles",
            "poetry",
            "probability distribution",
            "adjectives",
            "document classification"
        ],
        "content": "ARE POETRY AND LYRICS ALL THAT DIFFERENT? \n             Abhishek Si nghi  Daniel G. Brown  \n University of Waterloo  \nCheriton School of Computer Science  \n{asinghi,dan.brown}@uwaterloo.ca   \nABSTRACT \nWe hypothesize that different genres of writing use di f-\nferent adjectives for the same concept. We test our h y-\npothesis on lyrics, articles and poetry. We use the English \nWikipedia and over 13,000 news articles from four lea d-\ning newspapers for the article data set. Our lyrics data set \nconsists of lyrics of more than 10,000 songs by 56 pop u-\nlar English singers, and our poetry dataset is made up of \nmore than 20,000 poems from 60 famous poets. We find \nthe probability distribution of synonymous adjectives in \nall the three different categories and use it to predict if a \ndocument is an article, lyrics or poetry given its set of a d-\njectives. We achieve an accuracy level of 67% for lyrics, \n80% for articles and 57% for poetry. Using these prob a-\nbility distribution we show that adjectives more likely to \nbe used in lyrics are more rhymable than those more lik e-\nly to be used in poetry, but they do not differ significantly \nin their semantic orientations. Furthermore we show that \nour algorithm is successfully able to detect poetic lyricists \nlike Bob Dylan from non-poetic ones like Bryan Adams, \nas their lyrics are more often misclassified as poetry.  \n1. INTRODUCTION \nThe choice of a particular word, from a set of words that \ncan instead be used, depends on the context we use it in, \nand on the artistic decision of the authors. We believe that \nfor a given concept, the words that are more likely to be \nused in lyrics will be different from the ones which are \nmore likely to be used in articles or poems, because lyr i-\ncists have different objectives typically. We test our h y-\npothesis on adjective usage in these categories of doc u-\nments. We use adjectives, as a majority have synonyms \nthat can be used depending on context. To our surprise, \njust the adjective usage is sufficient to separate doc u-\nments quite effectively. \n     Finding the synonyms of a word is still an open pro b-\nlem. We used three different sources to obtain synonyms \nfor a word – the WordNet, Wikipedia and an online th e-\nsaurus. We prune synonyms, obtained from the three \nsources, which fall below an experimentally determined \nthreshold for the semantic distance between the synonyms and the word. The list of relevant synonyms obtained a f-\nter pruning was used to obtain the probability distribution \nover words. \n     A key requirement of our study is that there exists a \ndifference, albeit a hazy one, between poetry and lyrics. \nPoetry attracts a more educated and sensitive audience \nwhile lyrics are written for the masses. Poetry, unlike ly r-\nics, is often structurally more constrained, adhering to a \nparticular meter and style. Lyrics are often written kee p-\ning the music in mind while poetry is written against a s i-\nlent background. Lyrics, unlike poetry, often repeat lines \nand segments, causing us to believe that lyricists tend to \npick more rhymable adjectives; of course, some poetic \nforms also repeat lines, such as the villanelle. For twenty \ndifferent concepts we compare adjectives which are more \nlikely to be used in lyrics rather than poetry and vice ve r-\nsa. \n \nFigure 1 . The bold-faced words are the adjectives our \nalgorithm takes into account while classifying a doc u-\nment, which in this case in a snippet of lyrics by the \nBackstreet Boys. \n     We use a bag of words model for the adjectives, where \nwe do not care about their relative positions in the text, \nbut only their frequencies. Finding synonyms of a given \nword is a vital step in our approach and since it is still \nconsidered a difficult task improvement in synonyms \nfinding approaches will lead to an improvement in our \nclassification accuracy. Our algorithm has a linear run \ntime as it scans through the document once to come up \nwith the prediction, giving us an accuracy of 68% overall. \nLyricists with a relatively high percentage of lyrics mi s-\nclassified as poetry tend to be recognized for their poetic \nstyle, such as Bob Dylan and Annie Lennox. \n2. RELATED WORK \nWe do not know of any work on the classification of \ndocuments based on the adjective usage into lyrics, poe t-\nry or articles nor are we aware of any computational  \n © Abhishek Singhi , Daniel G.  Brown . \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Abhishek Singhi , Daniel G. \nBrown . “Are Poetry And Lyrics All That Different? ”, 15th \nInternational Society for Music Information Retrieval Conference, \n2014.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n471  \n \nwork which discerns poetic from non-poetic lyricists. \nPrevious works have used adjectives for various purposes \nlike sentiment analysis [1]. Furthermore in Music Info r-\nmation Retrieval, work on poetry has focused on poetry \ntranslator, automatic poetry generation. \n     Chesley et al. [1] classifies blog posts according to \nsentiment using verb classes and adjective polarity, \nachieving accuracy levels of 72.4% on objective posts, \n84.2% for positive posts, and 80.3% for negative posts. \nEntwisle et al. [2] analyzes the free verbal productions of \nninth-grade males and females and conclude that girls use \nmore adjectives than boys but fail to reveal differential \nuse of qualifiers by social class. \n     Smith et al. [13] use of tf-idf weighting to find typical \nphrases and rhyme pairs in song lyrics and conclude that \nthe typical number one hits, on average, are more cl i-\nchéd. Nichols et al. [14] studies the relationship between \nlyrics and melody on a large symbolic database of pop u-\nlar music and conclude that songwriters tend to align s a-\nlient notes with salient lyrics .  \n     There is some existing work on automatic generation \nof synonyms. Zhou et al. [3] extracts synonyms using \nthree sources - a monolingual dictionary, a bilingual co r-\npus and a monolingual corpus, and use a weighted e n-\nsemble to combine the synonyms produced from the \nthree sources. They get improved results when compared \nto the manually built thesauri, WordNet and Roget. \n      Christian et al. [4] describe an approach for using \nWikipedia to automatically build a dictionary of named \nentities and their synonyms. They were able to extract a \nlarge amount of entities with a high precision, and the \nsynonyms found were mostly relevant, but in some cases \nthe number of synonyms was very high. Niemi et al. [5] \nadd new synonyms to the existing synsets of the Finnish \nWordNet using Wikipedia’s links between the articles of \nthe same topic in Finnish and English. \n     As to computational poetry, Jiang et al. [6] use stati s-\ntical machine translation to generate Chinese couplets \nwhile Genzel et al. [7] use statistical machine translation \nto translate poetry keeping the rhyme and meter co n-\nstraints. \n3. DATA SET \nThe training set consists of articles, lyrics and poetry and \nis used to calculate the probability distribution of adje c-\ntives in the three different types of documents. We use \nthese probability distributions in our document classific a-\ntion algorithms, to identify poetic from non-poetic lyr i-\ncists and to determine adjectives more likely to be used in \nlyrics rather than poetry and vice versa. \n3.1 Articles \nWe take the English Wikipedia and over 13,000 news a r-\nticles from four major newspapers as our article data set. \nWikipedia, an enormous and freely available data set is edited by experts. Both of these are extremely rich \nsources of data on many topics. To remove the influence \nof the presence of articles about poems and lyrics in Wi k-\nipedia we set the pruning threshold frequency of adje c-\ntives to a high value, and we ensured that the articles were \nnot about poetry or music. \n3.2 Lyrics \nWe took more than 10,000 lyrics from 56 very popular \nEnglish singers. Both the authors listen to English music \nand hence it was easy to come up with a list which i n-\ncluded singers from many popular genres with diverse \nbackgrounds. We focus on English-language popular m u-\nsic in our study, because it is the closest to “universally” \npopular music, due to the strength of the music industry in \nEnglish-speaking countries. We do not know if our work \nwould generalize to non-English Language songs. Our \ndata set includes lyrics from the US, Canada, UK and Ir e-\nland.  \n3.3 Poetry \nWe took more than 20,000 poems from more than 60 f a-\nmous poets, like Robert Frost, William Blake and John \nKeats, over the last three hundred years. We selected the \ntop poets from Poem Hunter [19]. We selected a wide \ntime range for the poets, as many of the most famous \nEnglish poets are from that time period. None of the poe t-\nry selected were translations from another language. Most \nof the poets in our dataset are poets from North America \nand Europe. We believe that our training data, is repr e-\nsentative of the mean, as a majority of poetry and poetic \nstyle are inspired by the work of these few extremely f a-\nmous poets. \n3.4 Test Data \nFor the purpose of document classification we took 100 \nfrom each category, ensuring that they were not present in \nthe training set. While collecting the test data we ensured \nthe diversity, the lyrics and poets came from different \ngenres and artists and the articles covered different topics \nand were selected from different newspapers.  \n     To determine poetic lyricists from non-poetic ones we \ntook eight of each of the two types of lyricists, none of \nwhom were present in our lyrics data sets. We ensured \nthat the poetic lyricists we selected were indeed poetic by \nlooking up popular news articles or ensuring that they \nwere poet along with being lyricists. Our list for poetic \nlyricists included Bob Dylan and Annie Lennox etc. while \nthe non-poetic ones included Bryan Adams and Michael \nJackson. \n4. METHOD \nThese are the main steps in our method: \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n472  \n \n1) Finding the synonyms of all the words in the \ntraining data set. \n2) Finding the probability distribution of word for \nall the three types of documents. \n3) The document classification algorithm. \n4.1 Extracting Synonyms \nWe extract the synonyms for a term from three sources: \nWordNet, Wikipedia and an online thesaurus. \n     WordNet is a large lexical database of English where \nwords are grouped into sets of cognitive synonyms \n(synsets) together based on their meanings. WordNet i n-\nterlinks not just word forms but specific senses of words. \nAs a result, words that are found in close proximity to one \nanother in the network are semantically disambiguated. \nThe synonyms returned by WordNet need some pruning.  \n     We use Wikipedia  redirects to discover terms that are \nmostly synonymous. It returns a large number of words, \nwhich might not be synonyms, so we need to prune the \nresults. This method has been widely used for obtaining \nthe synonyms of named entities e.g. [4], but we get decent \nresults for adjectives too. \n     We also used an online Thesaurus  that lists words \ngrouped together according to similarity of meaning. \nThough it gives very accurate synonyms, pruning is ne c-\nessary to get better results. \n     We prune synonyms obtained from the three sources, \nwhich fall below an experimentally determined threshold \nfor the semantic distance between the synonyms and the \nword. To calculate the semantic similarity distance b e-\ntween words we use the method described by Pirro et al. \n[8]. Extracting synonyms for a given word is an open \nproblem and with improvement in this area our algorithm \nwill achieve better classification accuracy levels.  \n4.2 Probability Distribution \nWe believe that the choice of an adjective to express a \ngiven concept depends on the genre of writing: adjectives \nused in lyrics will be different from ones used in poems or \nin articles. We calculate the probability of a specific a d-\njective for each of the three document types.  \n     First, WordNet is used to identify the adjectives in our \ntraining sets. For each adjective we compute the freque n-\ncy of that were in the training set and the frequency of it \nand its synonyms; the ratio of these is the frequency wit h \nwhich that adjective represents its synonym group in that \nclass of writing. \n     We exclude adjectives that occur infrequently (fewer \nthan 5 times in our lyrics/poetry set or 50 in articles). The \nenormous size of the Wikipedia justifies the high thres h-\nold value. \n4.3 Document classification algorithm \nWe use a simple linear time algorithm which takes as i n-\nput the probability distributions for adjectives, calculated above, and the document(s) to be classified, calculates the \nscore of the document being an article, lyrics or poetry, \nand labels it with the class with the highest score. The a l-\ngorithm takes a single pass along the whole document and \nidentifies adjectives using WordNet. \n     For each word in the document we check its presence \nin our word list. If found, we add the probability to the \nscore, with a special penalty of -1 for adjectives never \nfound in the training set and a special bonus of +1 for \nwords with probability 1. The penalty and boosting values \nused in the algorithm were determined experimentally . \nSurprisingly, this simple approach gives us much better \naccuracy rates than Naïve Bayes, which we thought would \nbe a good option since it is widely used in classification \ntasks like spam filtering. We have decent accuracy rates \nwith this simple, naïve algorithm; one future task could be \nto come up with a better classifier. \n5. RESULTS \nFirst, we look at the classification accuracies between ly r-\nics, articles and poems obtained by our classifier. We \nshow that the adjectives used in lyrics are much more \nrhymable than the ones used in poems but they do not di f-\nfer significantly in their semantic orientations. Furthe r-\nmore, our algorithm is able to identify poetic lyricists \nfrom non-poetic ones using the word distributions, calc u-\nlated in earlier section. We also compare adjectives for a \ngiven concepts which are more likely to be used in lyrics \nrather than poetry and vice versa. \n5.1 Document Classification \nOur test set consists of the text of 100 each of our three \ncategories. Using our algorithm with the adjective distr i-\nbutions we get an accuracy of 67% for lyrics, 80% for a r-\nticles and 57% for poems.  \n    The confusion matrix, Table 1 we find the best accur a-\ncy for articles. This might be because of the enormous \nsize of the article training set which consisted of all En g-\nlish Wikipedia articles. A slightly more number of articles \nget misclassified as lyrics than poetry. \n     Surprisingly, a large number of misclassified poems \nget classified as articles rather than poetry, but most mi s-\nclassified lyrics get classified as poems.  \n5.2 Adjective Usage in Lyrics versus Poems \nPoetry is written against a silent background while lyrics \nare often written keeping the melody, rhythm, instrume n-\ntation, the quality of the singer’s voice and other qualities \nof the recording in mind. Furthermore, unlike most poe t-\nry, lyrics include repeated lines. This led us to believe the \nadjectives which were more likely to be used in lyrics r a-\nther than poetry would be more rhymable. \n     We counted the number of words an adjective in our \nlyrics and poetry list rhymes with from the website \nrhymezone.com. The values are tabulated in Table 2. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n473  \n \n     From the values in Table 2, we can clearly see that the \nadjectives which are more likely to be used in lyrics to be \nmuch more rhymable than the adjectives which are more \nlikely to be used in poetry. \n \n                                         Predicted  \nActual  Lyrics  Articles  Poems  \nLyrics  67 11 22 \nArticles  11 80 6 \nPoems  10 33 57 \nTable 1 . The confusion matrix for document classific a-\ntion. Many lyrics are categorized as poems, and many p o-\nems as articles. \n Lyrics  Poetry  \nMean  33.2 22.9 \nMedian  11 5 \n25th percentile  2 0 \n75th percentile  38 24 \nTable 2. Statistical values for the number of words an a d-\njective rhymes with. \n Lyrics  Poetry  \nMean  -.05 -.053 \nMedian 0.0 0.0 \n25th percentile  -0.27 -0.27 \n75th percentile  0.13 0.13 \nTable 3. Statistical values for the semantic orientation of \nadjectives used in lyrics and poetry. \n     We were also interested in finding if the adjectives \nused in lyrics and poetry differed significantly in their \nsemantic orientations. SentiWordNet assigns to each sy n-\nset of WordNet three sentiment scores: positivity, neg a-\ntivity, objectivity. We calculated the semantic orient a-\ntio\nns, which take a value between -1 and +1, using Se n-\ntiWordNet, of all the adjectives in the lyrics and poetry \nlist, the values are in Table 3. They show no difference \nbetween adjectives in poetry and those in lyrics. \n5.3 Poetic vs non-Poetic Lyricists \nThere are lyricists like Bob Dylan [15], Ani DiFranco \n[16], and Stephen Sondheim [ 17,18], whose lyrics are \nconsidered to be poetic, or indeed, who are published p o-\nets in some cases. The lyrics of such poetic lyricists po s-\nsibly could be structurally more constrained than a majo r-\nity of the lyrics or might adhere to a particular meter and \nstyle. While selecting the poetic lyricists we ensured that \npopular articles supported our claim or by going to their \nWikipedia page and ensuring that they were poets along \nwith being lyricists and hence the influence of their poetry \non lyrics.      Our algorithm consistently misclassifies a large fra c-\ntion of the lyrics of such poetic lyricists as poetry while \nthe percentage of misclassified lyrics as poetry for the \nnon-poetic lyricists is significantly much less. These va l-\nues for poetic and non-poetic lyricists are tabulated in t a-\nble 4 and table 5 respectively. \nPoetic Lyricists  % of lyrics misclass ified as \npoetry  \nBob Dylan  42% \nEd Sheeran  50% \nAni Di Franco  29% \nAnnie Lennox  32% \nBill Callahan  34% \nBruce Springsteen  29% \nStephen Sondheim  40% \nMorrissey  29% \nAverage misclassific ation \nrate 36%  \nTable 4. Percentage of misclassified lyrics as poetry for \npoetic lyricists. \nNon-Poetic Lyricists  % of lyrics misclassified as \npoetry  \nBryan Adams  14% \nMichael Jackson  22% \nDrake  7% \nBackstreet Boys  23% \nRadiohead  26% \nStevie Wonder  17% \nLed Zeppelin  8% \nKesha  18% \nAverage misclassification \nrate 17%  \nTable 5. Percentage of misclassified lyrics as poetry for \nnon-poetic lyricists. \n     From the values in table 4 and 5 we see that there is a \nclear separation between the misclassification rate b e-\ntween poetic and non-poetic lyricists. The maximum mi s-\nclassification rate for the non-poetic lyricists i.e. 26% is \nless than the minimum mis-classification rate for poetic \nlyricists i.e. 29%. Furthermore the difference in average \nmisclassification rate between the two groups of lyricists \nis 19%. Hence our simple algorithm can accurately ident i-\nfy poetic lyricists from non-poetic ones, based only on \nadjective usage. \n5.4 Concept representation in Lyrics vs Poetry \nWe compare adjective uses for common concepts. To \nrepresent physical beauty we are more likely to use words \nlike “sexy” and “hot” in lyrics but “gorgeous” and “han d-\nsome” in poetry. For 20 of these, results are tabulated in \nTable 6. The difference could possibly be because unlike \nlyrics, which are written for the masses, poetry is genera l-\nly written for people who are interested in literature. It \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n474  \n \nhas been shown that the typical number one hits, on ave r-\nage, are more clichéd [13]. \n \nLyrics  Poetry  \nproud, arrogant, cocky  haughty,  imperious  \nsexy, hot, beautiful, cute  gorgeous, handsome  \nmerry, ecstatic, elated  happy, blissful, joyous  \nheartbroken, brokenhear ted sad, sorrowful, dismal  \nreal genuine  \nsmart  wise, intelligent  \nbad, shady  lousy, immoral, dishonest  \nmad, outrageous  wrathful, furious  \nroyal  noble, aristocratic, regal  \npissed  angry, bitter  \ngreedy  selfish  \ncheesy  poor, worthless  \nlethal, dangerous, fatal  mortal, harmful, destructive  \nafraid, nervous  frightened, cowardly, timid  \njealous  envious, covetous  \nlax, sloppy  lenien t, indifferent  \nweak, fragile  feeble, powerless  \nblack  ebon  \nnaïve, ignorant  innocent, guileless, callow  \ncorny  dull, stale  \nTable 6. For twenty different concepts, we compare a d-\njectives which are more likely to be used in lyrics rather \nthan poetry and vice versa. \n6. APPLICATIONS \nThe algorithm developed has many practical applications \nin Music Information Retrieval (MIR). They could be \nused for automatic poetry/lyrics generation to identify a d-\njectives more likely to be used in a particular type of do c-\nument. As we have shown we can analyze documents, a n-\nalyze how lyrical, poetic or article-like a document is.  For \nlyricists or poets we can come up with alternate better a d-\njectives to make a document fit its genre better.  Using the \nword distributions we can come up with a better measure \nof distance between documents where the weights are a s-\nsigned to a word depending on its probability of usage in \na particular type of document. And, of course, our work \nhere can be extended to different genres of writings like \nprose or fiction.  7. CONCLUSION \nOur key finding is that the choice of synonym for even a \nsmall number of adjectives are sufficient to reliably ide n-\ntify genre of documents. In accordance with our hypoth e-\nsis, we show that there exist differences in the kind of a d-\njectives used in different genres of writing. We calculate \nthe probability distribution of adjectives over the three \nkinds of documents and using this distribution and a si m-\nple algorithm we are able to distinguish among lyrics, p o-\netry and article with an accuracy of 67%, 57% and 80% \nrespectively. \n     Adjectives likely to be used in lyrics are more \nrhymable than the ones used in poetry. This might be b e-\ncause lyrics are written keeping in mind the melody, \nrhythm, instrumenta tion, quality of the singer’s voice an d \nother qualities of the recording while poetry is without \nsuch concerns. There is no significant difference in the \nsemantic orientation of adjectives which are more likely \nto be used in lyrics and those which are more likely to be \nused in poetry. Using the probability distributions, o b-\ntained from training data, we present adjectives more lik e-\nly to be used in lyrics rather than poetry and vice versa for \ntwenty common concepts.  \n     Using the probability distributions and our algorithm \nwe show that we can discern poetic lyricists from non-\npoetic ones. Our algorithm consistently misclassifies a \nmajority of the lyrics of such poetic lyricists as poetry \nwhile the percentage of misclassified lyrics as poetry for \nthe non-poetic lyricists is significantly much les s. \n     Calculating the probability distribution of adjectives \nover the various document types is a vital step in our \nmethod which in turn depends on the synonyms extracted \nfor an adjective. Synonym extraction is still an open pro b-\nlem and with improvements in it our algorithm will give \nbetter accuracy levels. We extract synonyms from three \ndifferent sources – Wikipeia, WordNet and an online \nThesaurus, and prune the results based on the semantic \nsimilarity between the adjectives and the obtained syn o-\nnyms. \n     We use a simple naïve algorithm, which gives us better \nr\nesult than Naïve Bayes. An extension to the work can be \ncoming up with an improved version of the algorithm \nwith better accuracy levels. Future works can use a larger \ndataset for lyrics and poetry (we have an enormous d a-\ntaset for articles) to come up with better probability di s-\ntribution for the two document types or to identify parts \nof speech that effectively separates genres of writing. Our \nwork here can be extended to different genres of writings \nlike prose, fiction etc. to analyze the adjective usage in \nthose writings. It would be interesting to do similar work \nfor verbs and discern if different words, representing the \nsame action, are used in different genres of writings. \n8. ACKNOWLEDGEMENTS \nOur research is supported by a grant from the Natural \nSciences and Engineering Research Council of Canada to \nDGB . \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n475  \n \n9. REFERENCES \n[1] P. Chesley, B. Vincent., L. Xu, and R. Srihari, “U sing \nVerbs and Adjectives to Automatically Classify Blog \nSentiment”,  Training , volu me 580, number 263, pages \n233, 2006. \n[2] D.R. Entwisle and C. Garvey, “Verbal productivity \nand adjective usage”, Language and Speech , volume 15, \nnumber 3,  pages 288-298, 1972. \n[3] H. Wu and M. Zhou, “Optimizing Synonym Extra c-\ntion Using Monolingual and Bili ngual Resources”, in \nProceedings of the 2nd International Workshop on Par a-\nphrasing , volume 16, pages 72-79, 2003. \n[4] C. Bohn and K. Norvag , “Extracting Named E ntities \nand Synonyms from Wikipedia”, in Proceedings of 24th \nIEEE International Conference on Advanced Information \nNetworking and Applications, (AINA  ‘10), pages 1300 –\n1307.  \n[5] J. Niemi , K. Linden and M. Hyvarinen , “Using a b i-\nlingual resource to add synonyms to a wor d-\nnet:FinnWordNet and Wikipedia as an example”, in Pro-\nceedings of the  Global WordNet Association  , pages 227 –\n231, 2012. \n[6] L. Jiang and M. Zhou , “Generating Chinese couplets \nusing a statistical MT approach”, in Proceedings of the \n22nd International Conference on Computational Li n-\nguistics , pages 377 –384, 2008 . \n[7] D. Genzel, J. Uszkoreit and F. Och, “Poetic st atistical \nmachine translation: rhyme and meter”, in Proceedings of \nthe 2010 Conference on Empirical Methods in Natural \nLanguage Processing , pages 158–166, 2010 .  \n[8] G. Pirro and J. Euzenat, “A Feature and Information \nTheoretic Framework for Semantic Similarity and Rela t-\nedness”, in Proceedings of the 9th International Semantic \nWeb Conference (ISWC ‘10), pages 615-630, 2010. \n[9] G. Miller, “WordNet: A Lexical Database for En g-\nlish”, Communications of the ACM,  volume 38, number \n11, pages 39-41, 1995. \n[10] G. Miller and F. Christiane , WordNet: An Electronic \nLexical Database , 1998 . \n[11] S. Baccianella, A. Esuli,  and F. Sebastiani, “Sent i-\nWordNet 3.0: An Enhanced Lexical Resource for Sent i-\nment Analysis and Opinion Mining”, in Proceedings  of \nthe 7th Conference on International Language Resources \nand Evalua tion (LREC ‘ 10) , pages 2200 –2204 , 2010. \n[12] A. Esuli and F. Sebastiani, “SentiWordNet: A Pu b-\nlicly Available Lexical Resource for Opinion Mining”, in \nProceedings of the  5th Conference on Language R e-sources and Evalua tion (LREC ‘ 06), pages 417 –422, \n2006 .  \n[13] A.G. Smith , C. X. S. Z ee and A . L. Uitdenbogerd , \n“In your eyes: Identifying cliché in song lyrics”, in Pro-\nceedings of the Australasian Language Technology Ass o-\nciation Workshop , pages 88–96, 2012 . \n[14] E. Nichols, D. Morris, S. Basu, S. Christopher, “R e-\nlationships between lyrics and melody in popular mu sic”, \nin Proceedings of the 10th International Conference on \nMusic Information Retrieval (ISMIR ’09), 2009 . \n[15] K. Negus, Bob Dylan , Equinox London, 2008.  \n[16] A. DiFranco, Verses, Seven Stories, 2007.  \n[17] S. Sondheim, Look, I Made a Hat! New York: \nKnopf, 2011. \n[18] S. Sondheim, Finishing the H at, New York: Knopf , \n2010.  \n[19] http://www.poemhunter.com. \n \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n476"
    },
    {
        "title": "Merged-Output HMM for Piano Fingering of Both Hands.",
        "author": [
            "Eita Nakamura",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415152",
        "url": "https://doi.org/10.5281/zenodo.1415152",
        "ee": "https://zenodo.org/records/1415152/files/NakamuraOS14.pdf",
        "abstract": "This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of finger- ing for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the finger- ing model is also proposed and yields reasonable results.",
        "zenodo_id": 1415152,
        "dblp_key": "conf/ismir/NakamuraOS14",
        "keywords": [
            "piano fingering model",
            "ensemble scores",
            "quantifying difficulty",
            "automating piano reduction",
            "voice part separation",
            "merged-output hidden Markov model",
            "difficulty of performance",
            "hand application",
            "automation of decision",
            "previous difficulty"
        ],
        "content": "MERGED-OUTPUT HMM FOR PIANO FINGERING OF BOTH HANDS\nEita Nakamura\nNational Institute of Informatics\nTokyo 101-8430, Japan\neita.nakamura@gmail.comNobutaka Ono\nNational Institute of Informatics\nTokyo 101-8430, Japan\nonono@nii.ac.jpShigeki Sagayama\nMeiji University\nTokyo 164-8525, Japan\nsagayama@meiji.ac.jp\nABSTRACT\nThis paper discusses a piano ﬁngering model for both hands\nand its applications. One of our motivations behind the\nstudy is automating piano reduction from ensemble scores.\nFor this, quantifying the difﬁculty of piano performance is\nimportant where a ﬁngering model of both hands should\nbe relevant. Such a ﬁngering model is proposed that is\nbased on merged-output hidden Markov model and can be\napplied to scores in which the voice part for each hand is\nnot indicated. The model is applied for decision of ﬁnger-\ning for both hands and voice-part separation, automation of\nwhich is itself of great use and were previously difﬁcult. A\nmeasure of difﬁculty of performance based on the ﬁnger-\ning model is also proposed and yields reasonable results.\n1. INTRODUCTION\nMusic arrangement is one of the most important musical\nactivities, and its automation certainly has attractive appli-\ncations. One common form is piano arrangement of en-\nsemble scores, whose purposes are, among others, to en-\nable pianists to enjoy a wider variety of pieces and to ac-\ncompany other instruments by substituting the role of or-\nchestra. While certain piano reductions have high techni-\ncality and musicality as in the examples by Liszt [8], those\nfor vocal scores of operas and reduction scores of orchestra\naccompaniments are often faithful to the original scores in\nmost parts. The most faithful reduction score is obtained\nby gathering every note in the original score, but the result\ncan be too difﬁcult to perform, and arrangement such as\ndeleting notes is often in order.\nIn general, the difﬁculty of a reduction score can be re-\nduced by arrangement, but then the ﬁdelity also decreases.\nIf one can quantify the performance difﬁculty and the ﬁ-\ndelity to the original score, the problem of “minimal” pi-\nano reduction can be considered as an optimization prob-\nlem of the ﬁdelity given constraints on the performance\ndifﬁculty. A method for guitar arrangement based on prob-\nabilistic model with a similar formalization is proposed in\nRef. [5]. This paper is a step toward a realization of piano\nreduction algorithm based on the formalization.\nc\rEita Nakamura, Nobutaka Ono, Shigeki Sagayama.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Eita Nakamura, Nobutaka Ono,\nShigeki Sagayama. “ Merged-Output HMM for Piano Fingering of Both\nHands ”, 15th International Society for Music Information Retrieval Con-\nference, 2014.The playability of piano passages is discussed in Refs. [3,\n2] in connection with automatic piano arrangement. There,\nconstraints such as the maximal number of notes in each\nhand, the maximal interval being played, say, 10th, and\nthe minimal time interval of a repeated note are consid-\nered. Although these constraints are simple and effective\nto some extent, the actual situation is more complicated as\nmanifested in the fact that, for example, the playability can\nchange with tempos and players can arpeggiate chords that\ncannot be played simultaneously. In addition, the playabil-\nity can depend on the technical level of players [3]. Given\nthese problems, it seems appropriate to consider perfor-\nmance difﬁculty that takes values in a range.\nThere are various measures and causes of performance\ndifﬁculty including player’s movements and notational com-\nplexity of the score [12, 1, 15]. Here we focus on the difﬁ-\nculty of player’s movements, particularly piano ﬁngering,\nwhich is presumably one of the most important factors.\nThe difﬁculty of ﬁngering is closely related to the decision\nof ﬁngering [4, 7, 13, 16]. Given the current situation that a\nmethod of determining the ﬁngering costs from ﬁrst princi-\nples is not established, however, it is also effective to take a\nstatistical approach, and consider the naturalness of ﬁnger-\ning in terms of probability obtained from actual ﬁngering\ndata. With a statistical model of ﬁngering, the most natural\nﬁngering can be determined, and one can quantify the dif-\nﬁculty of ﬁngering in terms of naturalness. This will be ex-\nplained in Secs. 2 and 3. The practical importance of piano\nﬁngering and its applications are discussed in Ref. [17].\nSince voice parts played by both hands are not a priori\nseparated or indicated in the original ensemble score, a ﬁn-\ngering model must be applicable in such a situation. Thus,\na ﬁngering model for both hands and an algorithm to sep-\narate voice parts are necessary. We propose such a model\nand an algorithm based on merged-output hidden Markov\nmodel (HMM), which is suited for modeling multi-voice-\npart structured phenomena [10, 11]. Since multi-voice-part\nstructure of music is common and voice-part separation\ncan be applied for a wide range of information processing,\nthe results are itself of great importance.\n2. MODEL FOR PIANO FINGERING FOR BOTH\nHANDS\n2.1 Model for one hand\nBefore discussing the piano ﬁngering model for both hands,\nlet us discuss the ﬁngering model for one hand. Piano\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n531ﬁngering models and algorithms for decision of ﬁngering\nhave been studied in Refs. [13, 16, 4, 18, 19, 20, 7]. Here\nwe extend the model in Ref. [19] to including chords.\nPiano ﬁngering for one hand, say, the right hand, is indi-\ncated by associating a ﬁnger number fn= 1;\u0001\u0001\u0001;5(1 =\nthumb, 2 = the index ﬁnger;\u0001\u0001\u0001;5 = the little ﬁnger)\nto each note pnin a score1, wheren= 1;\u0001\u0001\u0001;Nin-\ndexes notes in the score and Nis the number of notes. We\nconsider the probability of a ﬁngering sequence f1:N=\n(fn)N\nn=1given a score, or a pitch sequence, p1:N= (pn)N\nn=1,\nwhich is written as P(f1:Njp1:N). As explained in detail\nin Sec. 3.1, an algorithm for ﬁngering decision can be ob-\ntained by estimating the most probable candidate ^f1:N=\nargmax\nf1:NP(f1:Njp1:N). The ﬁngering of a particular note\nis more inﬂuenced by neighboring notes than notes that are\nfar away in score position. Dependence on neighboring\nnotes is most simply described by that on adjacent notes,\nand it can be incorporated with a Markov model. It also\nhas advantages in efﬁciency in maximizing probability and\nsetting model parameters. Although the probability of ﬁn-\ngering may depend on inter-onset intervals between notes,\nthe dependence is not considered here for simplicity.\nAs proposed in Ref. [18, 19], the ﬁngering model can be\nconstructed with an HMM. Supposing that notes in score\nare generated by ﬁnger movements and the resulting per-\nformed pitches, their probability is represented with the\nprobability that a ﬁnger would be used after another ﬁnger\nP(fnjfn\u00001), and the probability that a pitch would result\nfrom succeeding two used ﬁngers. The former is called the\ntransition probability, and the latter output probability. The\noutput probability of pitch depends on the previous pitch\nin addition to the corresponding used ﬁngers, and it is de-\nscribed with a conditional probability P(pnjpn\u00001;fn\u00001;fn).\nIn terms of these probabilities, the probability of notes and\nﬁngerings is given as\nP(p1:N;f1:N) =NY\nn=1P(pnjpn\u00001;fn\u00001;fn)P(fnjfn\u00001);\n(1)\nwhere the initial probabilities are written as P(f1jf0)\u0011\nP(f1)andP(p1jp0;f0;f1)\u0011P(p1jf1). The probability\nP(f1:Njp1:N)can also be given accordingly.\nTo train the model efﬁciently, we assume some reason-\nable constraints on the parameters. First we assume that\nthe probability depends on pitches only through their ge-\nometrical positions on the keyboard which is represented\nas a two-dimensional lattice (Fig. 1). We also assume the\ntranslational symmetry in the x-direction and the time in-\nversion symmetry for the output probability. If the coordi-\nnate on the keyboard is written as `(p) = (` x(p);`y(p)),\nthe assumptions mean that the output probability has a form\nP(p0jp;f;f0) =F(`x(p0)\u0000`x(p);`y(p0)\u0000`y(p);f;f0),\nand it satisﬁes F(`x(p0)\u0000`x(p);`y(p0)\u0000`y(p);f;f0) =\nF(`x(p)\u0000`x(p0);`y(p)\u0000`y(p0);f0;f). A model for each\nhand can be obtained in this way, and it is written as\nF\u0011(`x(p0)\u0000`x(p);`y(p0)\u0000`y(p);f;f0)with\u0011=L;R .\n1We do not consider the so-called ﬁnger substitution in this paper.\nFigure 1. Keyboard lattice. Each key on a keyboard is\nrepresented by a point of a two-dimensional lattice.\nIt is further assumed that these probabilities are related\nby reﬂection in the x-direction, which yields FL(`x(p0)\u0000\n`x(p);`y(p0)\u0000`y(p);f;f0) =FR(`x(p0)\u0000`x(p);`y(p0)\u0000\n`y(p);f;f0).\nThe above model can be extended to be applied for pas-\nsages with chords, by converting a polyphonic passage to\na monophonic passage by virtually arpeggiating the chords\n[7]. Here, notes in a chord are ordered from low pitch to\nhigh pitch. The parameter values can be obtained from ﬁn-\ngering data.\n2.2 Model for both hands\nNow let us consider the ﬁngering of both hands in the sit-\nuation that it is unknown a priori which of the notes are\nto be played by the left or right hand. The problem can be\nstated as associating the ﬁngering information (\u0011n;fn)N\nn=1\nfor the pitch sequence p1:N, where\u0011n=L;R indicates the\nhand with which the n-th note is played.\nOne might think to build a model of both hands by sim-\nply extending the one-hand model and using (\u0011n;fn)as\na latent variable. However, this is not an effective model\nas far as it is a ﬁrst-order Markov model since, for exam-\nple, probabilistic constraints between two successive notes\nby the right hand cannot be directly incorporated when\nthey are interrupted by other notes of the left hand. Us-\ning higher-order Markov models leads to the problem of\nincreasing number of parameters that is hard to train as\nwell as the increasing computational cost. The underly-\ning problem is that the model cannot capture the structure\nof dependencies that is stronger among notes in each hand\nthan those across hands.\nRecently an HMM, called merged-output HMM, is pro-\nposed that is suited for describing such voice-part-structured\nphenomena [10, 11]. The basic idea is to construct a model\nfor both hands by starting with two parallel HMMs, called\npart HMMs, each of which corresponds to the HMM for\nﬁngering of each hand, and then merging the outputs of\nthe part HMMs. Assuming that only one of the part HMMs\ntransits and outputs an observed symbol at each time, the\nstate space of the merged-output HMM is given as a triplet\nk= (\u0011;fL;fR)of the hand information \u0011=L;R and\nﬁngerings of both hands: \u0011indicate which of the HMMs\ntransits, and fLandfRindicate the current states of the\npart HMMs. Let the transition and output probabilities\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n532of the part HMMs be a\u0011\nff0=P\u0011(f0jf)andb\u0011\nff0(`) =\nF\u0011(`;f;f0) (\u0011 =L;R). Then the transition and output\nprobabilities of the merged-output HMM are given as\nakk0=(\n\u000bLaL\nfLf0\nL\u000efRf0\nR; \u00110=L;\n\u000bRaR\nfRf0\nR\u000efLf0\nL; \u00110=R;(2)\nbkk0(`) =(\nbL\nfLf0\nL(`); \u00110=L;\nbR\nfRf0\nR(`); \u00110=R;(3)\nwhere\u000edenotes Kronecker’s delta. Here, \u000bL;Rrepresent\nthe probability of choosing which of the hands to play the\nnote, and practically, they satisfy \u000bL\u0018\u000bR\u00181=2. As\nshown in Ref. [11], certain interaction factors can be intro-\nduced to Eqs. (2) and (3). Although such interactions may\nbe important in the future [14], we conﬁne ourselves to the\ncase of no interactions in this paper for simplicity.\nBy estimating the most probable sequence ^k1:N, both\nthe optimal conﬁguration of hands ^\u00111:N, which yields a\nvoice-part separation, and that of ﬁngers (^fL;^fR)1:Nare\nobtained. For details of inference algorithms and other as-\npects of merged-output HMM, see Ref. [11].\n2.3 Model for voice-part separation\nThe model explained in the previous section involves both\nhands and the used hand and ﬁngers are modeled simulta-\nneously. We can alternatively consider the problem of as-\nsociating ﬁngerings of both hands as ﬁrst separating voice\nparts for both hands, and then associating ﬁngerings for\nnotes in each voice part. In this subsection, a simple model\nthat can be used for voice-part separation is given. The\nmodel is also based on a simpler merged-output HMM, and\nit yields more efﬁcient algorithm for voice-part separation.\nWe consider a merged-output HMM with a hidden state\nx= (\u0011;pL;pR), where\u0011=L;R indicates the voice part,\nandpL;Rdescribes the pitch played in each voice part. If\nthe pitch sequence in the score is denoted by (yn)n, the\ntransition and output probabilities are written as\naxx0=(\n\u000bLaL\npLp0\nL\u000epRp0\nR; \u00110=L;\n\u000bRaR\npRp0\nR\u000epLp0\nL; \u00110=R;(4)\nbx(y) =\u000ey;p\u0011: (5)\nHere the transition probability aL;R\npp0describes the pitch se-\nquence in each voice part directly, without any information\non ﬁngerings. The corresponding distributions can be ob-\ntained from actual data of piano pieces, as shown in Fig. 2.\nSo far we have considered a model of pitches and hor-\nizontal intervals for voice-part separation. The voice-part-\nseparation algorithm can be derived by applying the Viterbi\nalgorithm to the above model. In fact, a voice part in the\nscore played by one hand is also constrained by vertical\nintervals since it is physically difﬁcult to play a chord con-\ntaining an interval far wider than a octave by one hand. The\nconstraint on the vertical intervals can also be introduced\nin terms of probability.\n 00.020.040.060.08 0.10.12\n-40 -20  0  20  40Interval [semitone]Figure 2. Histograms of pitch transitions in piano scores\nfor each hand.\n3. APPLICATIONS OF THE FINGERING MODEL\n3.1 Algorithm for decision of ﬁngering\nA direct application of the model explained in Secs. 2.1\nand 2.2 is the decision of ﬁngering. The algorithm can be\nderived by applying the Viterbi algorithm. For one hand,\nthe derived algorithm is similar as the one in Ref. [19], but\nwe reevaluated the accuracy since the present model can\nbe applied for polyphonic passages and the details of the\nmodels are different.\nFor evaluation, we prepared manually labeled ﬁnger-\nings of classical piano pieces and compared them to the\none estimated with the algorithm. The test pieces were\nNos. 1, 2, 3, and 8 of Bach’s two-voice inventions, and\nthe introduction and exposition parts from Beethoven’s 8th\npiano sonata in C minor. The training and test of the al-\ngorithm was done with the leave-one-out cross validation\nmethod for each piece. To avoid zero frequencies in the\ntraining, we added a uniform count of 0.1 for every bin.\nThe averaged accuracy was 56.0% (resp. 55.4%) for the\nright (resp. left) hand where the number of notes was 5202\n(resp. 5539). Since the training data was not big, and we\nhad much higher rate of more than 70% for closed test,\nthe accuracy may improve if a larger set of training data is\ngiven. The results were better than the reported values in\nRef. [19]. The reason would be that the constraints of the\nmodel in the reference was too strong, which is relaxed in\nthe present model. For detailed analysis of the estimation\nerrors, see Ref. [19].\n3.2 Voice-part separation\nV oice-part separation between two hands can be done with\nthe model described in Sec. 2.3, and the algorithm can be\nobtained by the Viterbi algorithm. In fact, we can derive\na more efﬁcient estimation algorithm which is effectively\nequivalent since the model has noiseless observations as in\nEq. (5).\nIt is obtained by minimizing the following potential with\nrespect to the variables f(\u0011n;hn)g;hn= 0;1;\u0001\u0001\u0001;Nhfor\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n533Table 1. Error rates of the voice-part-separation algorithms.The 0-HMM (resp. 1-HMM, 2-HMM) indicates the algorithm\nwith the zeroth-order (resp. ﬁrst-order, second-order) HMM.\nPieces # Notes 0-HMM [%] 1-HMM [%] 2-HMM [%] Merged-output HMM [%]\nBach (15 pcs) 9638 5.1 5.3 6.1 1.9\nBeethoven (2 pcs) 18144 13.0 11.1 11.5 9.28\nChopin (5 pcs) 8508 5.7 4.0 4.29 3.8\nDebussy (3 pcs) 3360 17.8 14.8 14.8 18.7\nTotal 39650 9.9 8.5 8.9 7.1\neach note:\nV(\u0011;h) =\u0000X\nnlnQ(\u0011n\u00001;hn\u00001;\u0011n;hn); (6)\nQ(\u0011n\u00001;hn\u00001;\u0011n;hn)\n=(\n\u000b\u0011na(\u0011n)\nyn\u00001;yn\u000ehn;hn\u00001 +1; \u0011n=\u0011n\u00001;\n\u000b\u0011na(\u0011n)\nyn\u00002\u0000hn\u00001;yn\u000ehn;0; \u0011n6=\u0011n\u00001:(7)\nHerehnis necessary to memorize the current state of the\nvoice part opposite of \u0011n. The minimization of the poten-\ntial can be done with dynamic programming incrementally\nfor eachn. The estimation result is the same as the one\nwith the Viterbi algorithm applied to the model when Nh\nis sufﬁciently large, and we conﬁrmed that Nh= 50 is\nsufﬁcient to provide a good approximation.\nThe algorithm was evaluated by applying it to several\nclassical piano pieces. The used pieces were all pieces of\nBach’s two-voice inventions, the ﬁrst two piano sonatas by\nBeethoven, Chopin’s Etude Op. 10 Nos. 1–5, and the ﬁrst\nthree pieces in the ﬁrst book of Debussy’s Pr ´eludes. For\ncomparison, we also evaluated algorithms based on lower-\norder HMMs. The zeroth-order model with transition and\noutput probabilities P(\u0011)andP(pj\u0011)is almost equivalent\nto the keyboard splitting method, the ﬁrst-order model with\nP(\u00110j\u0011)andP(\u000epj\u0011;\u00110)and the second-order model are\nsimple applications of HMMs whose latent variables are\nhand informations \u0011=L;R .\nThe results are shown in Table 1. In total, the merged-\noutput HMM yielded the lowest error rate, with which rel-\natively accurate voice part separation can be done. On\nthe other hand, there were less changes in results for the\nlower-order HMMs, showing that the effectiveness of the\nmerged-output HMM. In Debussy’s pieces, the error rates\nwere relatively high since the pieces necessitate complex\nﬁngerings with wide movements of the hands. An exam-\nple of the voice-part separation result is shown in Fig. 3.\n3.3 Quantitative measure of difﬁculty of performance\nA measure of performance difﬁculty based on the natural-\nness of the ﬁngerings can be obtained by the probabilistic\nﬁngering model. Although global structures in scores may\ninﬂuence the difﬁculty, we concentrate on the effect of lo-\ncal structures. It is supposed that the difﬁculty is additive\nwith regard to performed notes and an increasing function\nof tempo. A quantity satisfying these conditions is the time\nrate of probabilistic cost. Let p(t) denote the sequence of\n&\n&44\n44œ œ# œœœœœn œ\nœœbœœœœœœœœœœœœœœœœ# œ# œ œ œ œ\nœœœœœœœœœœœœœ#œœœ(a) Passage in Bach’s two-voice invention No. 1.\nC4C5\n(b) Piano role representation of the voice-part separation result. Two voice\nparts are colored red and blue.\nFigure 3. Example of a voice-part separation result.\nnotes in the time range of [t\u0000\u0001t=2;t + \u0001t=2], and f(t)\nbe the corresponding ﬁngerings, where \u0001tis a width of the\ntime range to deﬁne the time rate. Then it is given as\nD(t) =\u0000lnP(p(t);f(t))=\u0001t: (8)\nSince the minimal time interval of successive notes are\nabout a few 10 milli seconds and it is hard to imagine that\ndifﬁculty is strongly inﬂuenced by notes that are separated\nmore than 10 seconds, it is natural to set \u0001twithin these\nextremes. The right-hand side is given by Eq. (1). It is pos-\nsible to calculate D(t)for a score without indicated ﬁnger-\nings by replacing f(t)with the estimated ﬁngerings ^f(t)\nwith the model in Sec. 2. In addition to the difﬁculty for\nboth hands, that for each hand DL;R(t)can also be deﬁned\nsimilarly.\nFig. 4 shows some examples of DL;R(t)calculated for\nseveral piano pieces. Here \u0001twas set to 1 sec. Although\nit is not easy to evaluate the quantity in a strict way, the\nresults seems reasonable and reﬂects generic intuition of\ndifﬁculty. The invention by Bach that can be played by\nbeginners yields DL;Rthat are less than about 10, the ex-\nample of Beethoven’s sonata which requires middle-level\ntechnicality has DL;Raround 20 to 30, and Chopin’s Fan-\ntasie Impromptu which involves fast passages and difﬁcult\nﬁngerings has DL;Rup to about 40. It is also worthy of not-\ning that relatively difﬁcult passages such as the fast chro-\nmatique passage of the right hand in the introduction of\nBeethoven’s sonata and ornaments in the right hand of the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n534(a) Difﬁculty for right hand DR\n (b) Difﬁculty for left hand DL\nFigure 4. Examples of DRandDL. The red (resp. green, blue) line is for Bach’s two-voice invention No.=1, (resp. Intro-\nduction and exposition parts of the ﬁrst movement of Beethoven’s eighth piano sonata, Chopin’s Fantasie Impromptu).\nslow part of the Fantasie Impromptu are also captured in\nterms of DR.\n4. CONCLUSIONS\nIn this paper, we considered a piano ﬁngering model of\nboth hands and its applications especially toward a piano\nreduction algorithm. First we reviewed a piano ﬁngering\nmodel for one hand based on HMM, and then constructed\na model for both hands based on merged-output HMM.\nNext we applied the model for constructing an algorithm\nfor ﬁngering decision and voice-part-separation algorithm\nand obtained a measure of performance difﬁculty. The al-\ngorithm for ﬁngering decision yielded better results than\nthe previously proposed one by a modiﬁcation in details\nof the model. The results of voice-part separation is quite\ngood and encouraging. The proposed measure of perfor-\nmance difﬁculty successfully captures the dependence on\ntempos and complexity of pitches and ﬁnger movements.\nThe next step to construct a piano reduction algorithm\naccording to the formalization mentioned in the Introduc-\ntion is to quantify the ﬁdelity of the arranged score to the\noriginal score and to integrate it with the constraints of\nperformance difﬁculty. The ﬁdelity can be described with\nedit probability, similarly as in Ref. [5], and an arrange-\nment model can be obtained by integrating the ﬁngering\nmodel with the edit probability. We are currently working\non these issues and the results will be reported elsewhere.\n5. ACKNOWLEDGMENTS\nThis work is supported in part by Grant-in-Aid for Sci-\nentiﬁc Research from Japan Society for the Promotion of\nScience, No. 23240021, No. 26240025 (S.S. and N.O.),\nand No. 25880029 (E.N.).\n6. REFERENCES\n[1] S.-C. Chiu and M.-S. Chen, “A study on difﬁculty\nlevel recognition of piano sheet music,” Proc. AdMIRe,pp. 10–12, 2012.\n[2] S.-C. Chiu et al., “Automatic system for the arrange-\nment of piano reductions,” Proc. AdMIRe, 2009.\n[3] K. Fujita et al., “A proposal for piano score genera-\ntion that considers proﬁciency from multiple part (in\nJapanese),” Tech. Rep. SIGMUS, MUS-77, pp. 47–52,\n2008.\n[4] M. Hart and E. Tsai, “Finding optimal piano ﬁnger-\nings,” The UMAP Journal, 21(1), pp. 167–177, 2000.\n[5] G. Hori et al., “Input-output HMM applied to auto-\nmatic arrangement for guitars,” J. Information Process-\ning,21(2), pp. 264–271, 2013.\n[6] Z. Ghahramani and M. Jordan, “Factorial Hidden\nMarkov Models,” Machine Learning, 29, pp. 245–273,\n1997.\n[7] A. Al Kasimi et al., “A simple algorithm for auto-\nmatic generation of polyphonic piano ﬁngerings,” IS-\nMIR, pp. 355–356, 2007.\n[8] F. Liszt, Musikalische Werke, Serie IV , Breitkopf &\nH¨artel, 1922.\n[9] J. Musaﬁa, The Art of Fingering in Piano Playing,\nMCA Music, 1971.\n[10] E. Nakamura et al., “Merged-output hidden Markov\nmodel and its applications to score following and\nhand separation of polyphonic keyboard music (in\nJapanese),” Tech. Rep. SIGMUS, 2013-EC-27, 15,\n2013.\n[11] E. Nakamura et al., “Merged-output hidden Markov\nmodel for score following of MIDI performance with\nornaments, desynchronized voices, repeats and skips,”\nto appear in Proc. ICMC, 2014.\n[12] C. Palmer, “Music performance,” Ann. Rev. Psychol.,\n48, pp. 115–138, 1997.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n535[13] R. Parncutt et al., “An ergonomic model of keyboard\nﬁngering for melodic fragments,” Music Perception,\n14(4), pp. 341–382, 1997.\n[14] R. Parncutt et al., “Interdependence of right and left\nhands in sight-read, written, and rehearsed ﬁngerings\nof parallel melodic piano music,” Australian J. of Psy-\nchology, 51(3), pp. 204–210, 1999.\n[15] V . S ´ebastien et al., “Score analyzer: Automatically\ndetermining scores difﬁculty level for instrumental e-\nlearning,” Proc. ISMIR, 2012.\n[16] H. Sekiguchi and S. Eiho, “Generating and displaying\nthe human piano performance,” 40(6), pp. 167–177,\n1999.\n[17] Y . Takegawa et al., “Design and implementation of a\nreal-time ﬁngering detection system for piano perfor-\nmance,” Proc. ICMC, pp. 67–74, 2006.\n[18] Y . Yonebayashi et al., “Automatic determination of\npiano ﬁngering based on hidden Markov model (in\nJapanese),” Tech. Rep. SIGMUS, 2006-05-13, pp. 7–\n12, 2006.\n[19] Y . Yonebayashi et al., “Automatic decision of piano\nﬁngering based on hidden Markov models,” IJCAI,\npp. 2915–2921, 2007.\n[20] Y . Yonebayashi et al. , “Automatic piano ﬁngering de-\ncision based on hidden Markov models with latent\nvariables in consideration of natural hand motions (in\nJapanese),” Tech. Rep. SIGMUS, MUS-71-29, pp. 179–\n184, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n536"
    },
    {
        "title": "Harmonic-Temporal Factor Decomposition Incorporating Music Prior Information for Informed Monaural Source Separation.",
        "author": [
            "Tomohiko Nakamura",
            "Kotaro Shikata",
            "Norihiro Takamune",
            "Hirokazu Kameoka"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417463",
        "url": "https://doi.org/10.5281/zenodo.1417463",
        "ee": "https://zenodo.org/records/1417463/files/NakamuraSTK14.pdf",
        "abstract": "For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative ma- trix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA- based approach called the “harmonic-temporal clustering (HTC)” aims to cluster the time-frequency components of an observed signal based on a constraint designed ac- cording to the local time-frequency structure common in many sound sources (such as harmonicity and the conti- nuity of frequency and amplitude modulations). This pa- per proposes a new approach for monaural source sepa- ration called the “Harmonic-Temporal Factor Decompo- sition (HTFD)” by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separa- tion scheme.",
        "zenodo_id": 1417463,
        "dblp_key": "conf/ismir/NakamuraSTK14",
        "keywords": [
            "non-negative matrix factorization (NMF)",
            "computational auditory scene analysis (CASA)",
            "harmonic-temporal clustering (HTC)",
            "harmonic-temporal factor decomposition (HTFD)",
            "spectrogram model",
            "musically relevant information",
            "separation scheme",
            "local time-frequency structure",
            "modulations",
            "source separation"
        ],
        "content": "HARMONIC-TEMPORAL F ACTOR DECOMPOSITION\nINCORPORATING MUSIC PRIOR INFORMATION FOR\nINFORMED MONAURAL SOURCE SEPARATION\nTomohiko Nakamura†, Kotaro Shikata†, Norihiro Takamune†, Hirokazu Kameoka†‡\n†Graduate School of Information Science and Technology, The University of Tokyo.\n‡NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation.\n{nakamura,k-shikata,takamune,kameoka}@hil.t.u-tokyo.ac.jp\nABSTRACT\nFor monaural source separation two main approaches have\nthus far been adopted. One approach involves applying\nnon-negative matrix factorization (NMF) to an observed\nmagnitude spectrogram, interpreted as a non-negative ma-\ntrix. The other approach is based on the concept of\ncomputational auditory scene analysis (CASA). A CASA-\nbased approach called the “harmonic-temporal clustering\n(HTC)” aims to cluster the time-frequency components\nof an observed signal based on a constraint designed ac-\ncording to the local time-frequency structure common in\nmany sound sources (such as harmonicity and the conti-\nnuity of frequency and amplitude modulations). This pa-\nper proposes a new approach for monaural source sepa-\nration called the “Harmonic-Temporal Factor Decompo-\nsition (HTFD)” by introducing a spectrogram model that\ncombines the features of the models employed in the NMF\nand HTC approaches. We further describe some ideas how\nto design the prior distributions for the present model to\nincorporate musically relevant information into the separa-\ntion scheme.\n1. INTRODUCTION\nMonaural source separation is a process in which the sig-\nnals of concurrent sources are estimated from a monaural\npolyphonic signal and is one of fundamental objectives of-\nfering a wide range of applications such as music informa-\ntion retrieval, music transcription and audio editing.\nWhile we can use spatial cues for blind source sepa-\nration with multichannel inputs, for monaural source sep-\naration we need other cues instead of the spatial cues.\nFor monaural source separation two main approaches have\nthus far been adopted. One approach is based on the con-\ncept of computational auditory scene analysis (e.g., [7]).\nThe auditory scene analysis process described by Breg-\nman [1] involves grouping elements that are likely to have\noriginated from the same source into a perceptual struc-\nture called an auditory stream. In [8, 10], an attempt\nhas been made to imitate this process by clustering time-\nfrequency components based on a constraint designed ac-\ncording to the auditory grouping cues (such as the har-\nc⃝Tomohik\no Nakamura†, Kotaro Shikata†, Norihiro\nTakamune†, Hirokazu Kameoka†‡.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Tomohiko Nakamura†, Kotaro\nShikata†, Norihiro Takamune†, Hirokazu Kameoka†‡. “Harmonic-\ntemporal factor decomposition incorporating music prior information for\ninformed monaural source separation”, 15th International Society for\nMusic Information Retrieval Conference, 2014.monicity and the coherences and continuities of ampli-\ntude and frequency modulations). This method is called\n“harmonic-temporal clustering (HTC).”\nThe other approach involves applying non-negative ma-\ntrix factorization (NMF) to an observed magnitude spec-\ntrogram (time-frequency representation) interpreted as a\nnon-negative matrix [19]. The idea behind this approach\nis that the spectrum at each frame is assumed to be repre-\nsented as a weighted sum of a limited number of common\nspectral templates. Since the spectral templates and the\nmixing weights should both be non-negative, this implies\nthat an observed spectrogram is modeled as the product of\ntwo non-negative matrices. Thus, factorizing an observed\nspectrogram into the product of two non-negative matri-\nces allows us to estimate the unknown spectral templates\nconstituting the observed spectra and decompose the ob-\nserved spectra into components associated with the esti-\nmated spectral templates.\nThe two approaches described above rely on diﬀerent\nclues for making separation possible. Roughly speaking,\nthe former approach focuses on the local time-frequency\nstructure of each source, while the latter approach fo-\ncuses on a relatively global structure of music spectro-\ngrams (such a property that a music signal typically con-\nsists of a limited number of recurring note events). Rather\nthan discussing which clues are more useful, we believe\nthat both of these clues can be useful for achieving a reli-\nable monaural source separation algorithm. This belief has\nled us to develop a new model and method for monaural\nsource separation that combine the features of both HTC\nand NMF. We call the present method “harmonic-temporal\nfactor decomposition (HTFD).”\nThe present model is formulated as a probabilistic gen-\nerative model in such a way that musically relevant infor-\nmation can be ﬂexibly incorporated into the prior distribu-\ntions of the model parameters. Given the recent progress\nof state-of-the-art methods for a variety of music informa-\ntion retrieval (MIR)-related tasks such as audio key detec-\ntion, audio chord detection, and audio beat tracking, in-\nformation such as key, chord and beat extracted from the\ngiven signal can potentially be utilized as reliable and use-\nful prior information for source separation. The inclusion\nof auxiliary information in the separation scheme is re-\nferred to as informed source separation and is gaining in-\ncreasing momentum in recent years (see e.g., among oth-\ners, [5,15,18,20]). This paper further describes some ideas\nhow to design the prior distributions for the present model\nto incorporate musically relevant information.\nWe henceforth denote the normal, Dirichlet and Poisson\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n623distributions\nbyN, Dir and Pois, respectively.\n2. SPECTROGRAM MODEL OF MUSIC SIGNAL\n2.1 Wavelet transform of source signal model\nAs in [8], this section derives the continuous wavelet trans-\nform of a source signal. Let us ﬁrst consider as a signal\nmodel for the sound of the kth pitch the analytic signal\nrepresentation of a pseudo-periodic signal given by\nfk(u)=N∑\nn=1ak,n(u)ej(nθ k(u)+φ k,n), (1)\nwhere udenotes the time, nθk(u)+φk,nthe instantaneous\nphase of the n-th harmonic and ak,n(u) the instantaneous\namplitude. This signal model implicitly ensures not to vi-\nolate the ‘harmonicity’ and ‘coherent frequency modula-\ntion’ constraints of the auditory grouping cues. Now, let\nthe wavelet basis function be deﬁned by\nψα,t(u)=1√\n2παψ(u−t\nα)\n, (2)\nwhereαis the\nscale parameter such that α > 0,tthe shift\nparameter and ψ(u) the mother wavelet with the center fre-\nquency of 1 satisfying the admissibility condition. ψα,t(u)\ncan thus be used to measure the component of period αat\ntime t. The continuous wavelet transform of fk(u) is then\ndeﬁned by\nWk(log1\nα,t)=∫∞\n−∞N∑\nn=1ak,n(u)ej(nθ k(u)+φk,n)ψ∗\nα,t(u)du. (3)\nSince the\ndominant part of ψ∗\nα,t(u) is typically localized\naround time t, the result of the integral in Eq. (3) shall\ndepend only on the values of θk(u) and ak,n(u) near t. By\ntaking this into account, we replace θk(t) and ak,n(t) with\nzero- and ﬁrst-order approximations around time t:\nak,n(u)≃ak,n(t), θ k(u)≃θk(t)+˙θk(t)(u−t). (4)\nNote that the variable ˙θk(u) corresponds to the instanta-\nneous fundamental frequency ( F0). By undertaking the\nabove approximations, applying the Parseval’s theorem,\nand putting x=log(1/α) and Ωk(t)=log˙θk(t), we can\nfurther write Eq. (3) as\nWk(x,t)=N∑\nn=1ak,n(t)Ψ∗(ne−x+Ω k(t))ej(nθk(t)+φk,n),(5)\nwhere xdenotes log-frequency and Ψthe Fourier transform\nofψ. Since the function Ψcan be chosen arbitrarily, as\nwith [8], we employ the following unimodal real function\nwhose maximum is taken at ω=1:\nΨ(ω)=e−(logω)2\n4σ2 (ω> 0)\n0\n(ω≤0). (6)\nEq. (5) can then be written as\nWk(x,t)=N∑\nn=1ak,n(t)e−(x−Ωk(t)−log n)2\n4σ2ej(nθ k(t)+φ k,n). (7)\nIf we\nnow assume that the time-frequency components are\nsparsely distributed so that the partials rarely overlap each\nother,|Wk(x,t)|2is given approximately as\n|Wk(x,t)|2≃N∑\nn=1|ak,n(t)|2e−(x−Ωk(t)−log n)2\n2σ2. (8)\nThis assumption\nmeans that the power spectra of the par-\ntials can approximately be considered additive. Note that\na cutting plane of the spectrogram model given by Eq. (8)at time tis expressed as a harmonically-spaced Gaussian\nmixture function. If we assume the additivity of power\nspectra, the power spectrogram of a superposition of K\npitched sounds is given by the sum of Eq. (8) over k. It\nshould be noted that this model is identical to the one em-\nployed in the HTC approach [8].\nAlthough we have deﬁned the spectrogram model above\nin continuous time and continuous log-frequency, we ac-\ntually obtain observed spectrograms as a discrete time-\nfrequency representation through computer implementa-\ntions. Thus, we henceforth use Yl,m:=Y(xl,tm) to de-\nnote an observed spectrogram where xl(l=1,..., L) and\ntm(m=1,..., M) stand for the uniformly-quantized log-\nfrequency points and time points, respectively. We will\nalso use the notation Ωk,mandak,n,mto indicate Ωk(tm) and\nak,n(tm).\n2.2 Incorporating source-ﬁlter model\nThe generating processes of many sound sources in real\nworld can be explained fairly well by the source-ﬁlter the-\nory. In this section, we follow the idea described in [12] to\nincorporate the source-ﬁlter model into the above model.\nLet us assume that each signal fk(u) within a short-time\nsegment is an output of an all-pole system. That is, if\nwe use fk,m[i] to denote the discrete-time representation\noffk(u) within a short-time segment centered at time tm,\nfk,m[i] can be described as\nβk,m[0]fk,m[i]=P∑\np=1βk,m[p]fk,m[i−p]+ϵk,m[i],(9)\nwhere i,ϵk,m[i], andβk,m[p] (p=0,..., P) denote the\ndiscrete-time index, an excitation signal, and the autore-\ngressive (AR) coe ﬃcients, respectively. As we have al-\nready assumed in 2.1 that the F0offk,m[i] is eΩk,m, to make\nthe assumption consistent, the F0of the excitation signal\nϵk,m[i] must also be eΩk,m. We thus deﬁne ϵk,m[i] as\nϵk,m[i]=N∑\nn=1vk,n,mejneΩk,miu0, (10)\nwhere u0denotes the sampling period of the discrete-time\nrepresentation and vk,n,mdenotes the complex amplitude of\nthenth partial. By applying the discrete-time Fourier trans-\nform (DTFT) to Eq. (9) and putting Bk,m(z) :=βk,m[0]−\nβk,m[1]z−1···−βk,m[P]z−P, we obtain\nFk,m(ω)=√\n2π\nBk,m(ejω)N∑\nn=1vk,n,mδ(ω−neΩk,mu0), (11)\nwhere Fk,mdenotes the\nDTFT of fk,m,ωthe normalized an-\ngular frequency, and δthe Dirac delta function. The inverse\nDTFT of Eq. (11) gives us another expression of fk,m[i]:\nfk,m[i]=N∑\nn=1vk,n,m\nBk,m(ejneΩk,mu0)ejneΩk,miu0. (12)\nBy comparing\nEq. (12) and the discrete-time representa-\ntion of Eq. (1), we can associate the parameters of the\nsource ﬁlter model deﬁned above with the parameters in-\ntroduced in 2.1 through the explicit relationship:\n|ak,n,m|=\f\f\f\f\f\fvk,n,m\nBk,m(ejneΩk,mu0)\f\f\f\f\f\f. (13)\n2.3 Constraining\nmodel parameters\nThe key assumption behind the NMF model is that the\nspectra of the sound of a particular pitch is expressed as\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n624a multiplication\nof time-independent and time-dependent\nfactors. In order to extend the NMF model to a more rea-\nsonable one, we consider it important to clarify which fac-\ntors involved in the spectra should be assumed to be time-\ndependent and which factors should not. For example, the\nF0must be assumed to vary in time during vibrato or porta-\nmento. Of course, the scale of the spectrum should also be\nassumed to be time-varying (as with the NMF model). On\nthe other hand, the timbre of an instrument can be consid-\nered relatively static throughout an entire piece of music.\nWe can reﬂect these assumptions in the present model in\nthe following way. For convenience of the following anal-\nysis, we factorize |ak,n,m|into the product of two variables,\nwk,n,mandUk,m\n|ak,n,m|=wk,n,m√\nUk,m. (14)\nwk,n,mcan be\ninterpreted as the relative power of the nth\nharmonic and Uk,mas the time-varying normalized ampli-\ntude of the sound of the kth pitch such that∑\nk,mUk,m=1.\nIn the same way, let us put vk,n,mas\nvk,n,m=˜wk,n,m√\nUk,m. (15)\nSince the\nall-pole spectrum 1/ |Bk,m(ejω)|2is related to the\ntimbre of the sound of the kth pitch, we want to constrain\nit to be time-invariant. This can be done simply by elimi-\nnating the subscript m. Eq. (13) can thus be rewritten as\nwk,n,m=\f\f\f\f\f\f˜wk,n,m\nBk(ejneΩk,mu0)\f\f\f\f\f\f. (16)\nWe\ncan use Ωk,mas is, since it is already dependent on m.\nTo sum up, we obtain a spectrogram model Xl,mas\nXl,m=K∑\nk=1Ck,l,m,Ck,l,m=N∑\nn=1w2\nk,n,me−(xl−Ωk,m−log n)2\n2σ2\n|  \n                       {z                         }\nHk,l,mUk,m,\n(17)\nwhere Ck,l,mstands for the spectrogram of the kth pitch. If\nwe denote the term inside the parenthesis by Hk,l,m,Xl,m\ncan be rewritten as Xl,m=∑\nkHk,l,mUk,mand so the relation\nto the NMF model may become much clearer.\n2.4 Formulating probabilistic model\nSince the assumptions and approximations we made so far\ndo not always hold exactly in reality, an observed spectro-\ngram Yl,mmay diverge from Xl,meven though the param-\neters are optimally determined. One way to simplify the\nprocess by which this kind of deviation occurs would be to\nassume a probability distribution of Yl,mwith the expected\nvalue of Xl,m. Here, we assume that Yl,mfollows a Poisson\ndistribution with mean Xl,m\nYl,m∼Pois(Y l,m;Xl,m), (18)\nwhere Pois(z; ξ)=ξze−ξ/Γ(z). This deﬁnes our likelihood\nfunction\np(Y|\u0012)=∏\nl,mPois( Yl,m;Xl,m), (19)\nwhere Ydenotes the set consisting of Yl,mandΘthe entire\nset consisting of the unknown model parameters. It should\nbe noted that the maximization of the Poisson likelihood\nwith respect to Xl,mamounts to optimally ﬁtting Xl,mtoYl,m\nby using the I-divergence as the ﬁtting criterion.\nEq. (16) implicitly deﬁnes the conditional distribution\nTime [s]Frequency [Hz]\n00.370.731.11.461.833906951238220539297000Figure\n1. Power spectrogram of a violin vibrato sound.\np(w|˜w,\f,Ω) expressed by the Dirac delta function\np(w|˜w,\f,Ω)=∏\nk,n,mδ(\nwk,n,m−\f\f\f\f\f\f˜wk,n,m\nBk(ejneΩk,mu0)\f\f\f\f\f\f)\n.(20)\nThe conditional\ndistribution p(w|\f,Ω) can thus be obtained\nby deﬁning the distribution p(˜w) and marginalizing over\n˜w. If we now assume that the complex amplitude ˜ wk,n,m\nfollows a circular complex normal distribution\n˜wk,n,m∼NC( ˜wk,n,m; 0,ν2),n=1,..., N, (21)\nwhereNC(z; 0,ξ2)=e−|z|2/ξ2/(πξ2), we can show, as in [12],\nthatwk,n,mfollows a Rayleigh distribution:\nwk,n,m∼Rayleigh(w k,n,m;ν/|Bk(ejneΩk,mu0)|), (22)\nwhere Rayleigh(z; ξ)=(z/ξ2)e−z2/(2ξ2). This deﬁnes the\nconditional distribution p(w|\f,Ω).\nThe F0of stringed and wind instruments often varies\ncontinuously over time with musical expressions such as\nvibrato. For example, the F0of a violin sound varies pe-\nriodically around the note frequency during vibrato, as de-\npicted in Fig. 1. Let us denote the standard log- F0cor-\nresponding to the kth note by µk. To appropriately de-\nscribe the variability of an F0contour in both the global\nand local time scales, we design a prior distribution for\nΩk:=(Ωk,1,Ωk,2,...,Ωk,M)Tby employing the product-\nof-experts (PoE) [6] concept using two probability distri-\nbutions. First, we design a distribution qg(Ωk) describ-\ning how likely Ωk,1,...,Ωk,Lstay nearµk. Second, we\ndesign another distribution ql(Ωk) describing how likely\nΩk,1,...,Ωk,Lare locally continuous along time. Here we\ndeﬁne qg(Ωk) and ql(Ωk) as\nqg(Ωk)=N(Ωk;µk1M,υ2\nkIM), (23)\nql(Ωk)=N(Ωk;0M,τ2\nkD−1), (24)\nD=1−1 0 0···0\n−1 2−1 0···0\n0−1 2−1···0\n...............\n0···0−1 2−1\n0···0 0−1 1, (25)\nwhere IMdenotes an M×Midentity matrix, DanM×M\nband matrix, 1ManM-dimensional all-one vector, and 0M\nanM-dimensional all-zero vector, respectively. υkdenotes\nthe standard deviation from mean µk, andτkthe standard\ndeviation of the F0jumps between adjacent frames. The\nprior distribution of Ωkis then derived as\np(Ωk)∝qg(Ωk)αgql(Ωk)αl(26)\nwhereαgandαlare the hyperparameters that weigh the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n625contributions\nofqg(Ωk) and ql(Ωk) to the prior distribution.\n2.5 Relation to other models\nIt should be noted that the present model is related to other\nmodels proposed previously.\nIf we do not assume a parametric model for Hk,l,mand\ntreat each Hk,l,mitself as the parameter, the spectrogram\nmodel Xl,mcan be seen as an NMF model with time-\nvarying basis spectra, as in [14]. In addition to this as-\nsumption, if we assume that Hk,l,mis time-invariant (i.e.,\nHk,l,m=Hk,l),Xl,mreduces to the regular NMF model [19].\nFurthermore, if we assume each basis spectrum to have\na harmonic structure, Xl,mbecomes equivalent to the har-\nmonic NMF model [16, 21].\nIf we assume that Ωk,mis equal over time m,Xl,mreduces\nto a model similar to the ones described in [17, 22]. Fur-\nthermore, if we describe Uk,musing a parametric function\nofm,Xl,mbecomes equivalent to the HTC model [8, 10].\nWith a similar motivation, Hennequin et al. developed\nan extension to the NMF model deﬁned in the short-time\nFourier transform domain to allow the F0of each basis\nspectrum to be time-varying [4].\n3. INCORPORATION OF AUXILIARY\nINFORMATION\n3.1 Use of musically relevant information\nWe consider using side-information obtained with the\nstate-of-the-art methods for MIR-related tasks including\nkey detection, chord detection and beat tracking to assist\nsource separation.\nWhen multiple types of side-information are obtained\nfor a speciﬁc parameter, we can combine the use of the\nmixture-of-experts and PoE [6] concepts according to the\n“AND” and “OR” conditions we design. For example,\npitch occurrences typically depend on both the chord and\nkey of a piece of music. Thus, when the chord and key in-\nformation are obtained, we may use the product-of-experts\nconcept to deﬁne a prior distribution for the parameters\ngoverning the likeliness of the occurrences of the pitches.\nIn the next subsection, we describe speciﬁcally how to de-\nsign the prior distributions.\n3.2 Designing prior distributions\nThe likeliness of the pitch occurrences in popular and clas-\nsical western music usually depend on the key or the chord\nused in that piece. The likeliness of the pitch occurrences\ncan be described as a probability distribution over the rel-\native energies of the sounds of the individual pitches.\nSince the number of times each note is activated is usu-\nally limited, inducing sparsity to the temporal activation of\neach note event would facilitate the source separation. The\nlikeliness of the number of times each note is activated can\nbe described as well as a probability distribution over the\ntemporal activations of the sound of each pitch.\nTo allow for designing such prior distributions, we de-\ncompose Uk,mas the product of two variables: the pitch-\nwise relative energy Rk=∑\nmUk,m(i.e.∑\nkRk=1), and\nthe pitch-wise normalized amplitude Ak,m=Uk,m/Rk(i.e.∑\nmAk,m=1). Hence, we can write\nUk,m=RkAk,m. (27)\nThis decomposition allows us to incorporate diﬀerent\nkinds of prior information into our model by separately\ndeﬁning prior distributions over R=(R1,..., RK)TandAk=(Ak,1,..., Ak,M)T. Here we introduce Dirichlet dis-\ntributions:\nAk∼Dir(Ak;\r(A)\nk),R∼Dir(R;\r(R)), (28)\nwhere Dir( z;\u0018)∝∏\niziξi,\r(A)\nk:=(γ(A)\nk,1,...,γ(A)\nk,M)T, and\n\r(R):=(γ(R)\n1,...,γ(R)\nK)T. For p(R), we setγ(R)\nkat a reason-\nably high value if the kth pitch is contained in the scale and\nvice versa. For p(Ak), we setγ(A)\nk,m<1 so that the Dirichlet\ndistribution becomes a sparsity inducing distribution.\n4. PARAMETER ESTIMATION ALGORITHM\nGiven an observed power spectrogram Y:={Yl,m}l,m,\nwe would like to ﬁnd the estimates of Θ :=\n{Ω,w,\f,V,R,A}that maximizes the posterior density\np(ΘjY )∝p(Y|Θ)p(Θ). We therefore consider the prob-\nlem of maximizing\nL(Θ) := lnp(Y|Θ)+lnp(Θ), (29)\nwith respect to Θwhere\nlnp(Y|Θ)=\nc∑\nl,m(Yl,mlnXl,m−Xl,m)(30)\nlnp(Θ)=lnp(w|\f,Ω)+∑\nklnp(Ωk)\n+lnp(R)+∑\nklnp(Ak). (31)\n=cdenotes equality up to constant terms. Since the ﬁrst\nterm of Eq. (30) involves summation over kandn, an-\nalytically solving the current maximization problem is in-\ntractable. However, we can develop a computationally e ﬃ-\ncient algorithm for ﬁnding a locally optimal solution based\non the auxiliary function concept, by using a similar idea\ndescribed in [8, 12].\nWhen applying an auxiliary function approach to a cer-\ntain maximization problem, the ﬁrst step is to deﬁne a\nlower bound function for the objective function. As men-\ntioned earlier, the di ﬃculty with the current maximization\nproblem lies in the ﬁrst term in Eq. (30) . By using the fact\nthat the logarithm function is a concave function, we can\ninvoke the Jensen’s inequality\nYl,mlnXl,m≥Yl,m∑\nk,nλk,n,l,mlnw2\nk,n,me−(xl−Ωk,m−log n)2\n2σ2 Uk,m\nλk,n,l,m,\n(32)\nto obtain\na lower bound function, where λk,n,l,mis a positive\nvariable that sums to unity:∑\nk,nλk,n,l,m=1. Equality of\n(32) holds if and only if\nλk,n,l,m=w2\nk,n,me−(xl−Ωk,m−log n)2\n2σ2 Uk,m\nXl,m. (33)\nAlthough one\nmay notice that the second term in\nEq. (30) is nonlinear in Ωk,m, the summation of Xl,m\nover lcan be approximated fairly well using the integral∫∞\n−∞X(x,tm)dx, since∑\nlXl,mis the sum of the values at the\nsampled points X(x1,tm),..., X(xL,tm) with an equal inter-\nval, say ∆x. Hence,\n∑\nlXl,m≃1\n∆x∫∞\n−∞X(x,tm)dx\n=1\n∆x∑\nk,nw2\nk,n,mUk,m∫∞\n−∞e−(x−Ωk,m−log n)2\n2σ2dx\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n626=√\n2πσ\n∆x∑\nkUk,m∑\nnw2\nk,n,m. (34)\nThis approximation\nimplies that the second term in Eq.\n(30) depends little on Ωk,m.\nAn auxiliary function can thus be written as\nL+(Θ,\u0015)=\nc∑\nl,mYl,m∑\nk,nλk,n,l,mlnw2\nk,n,me−(xl−Ωk,m−lnn)2\n2σ2Uk,m\nλk,n,l,m\n−√\n2πσ\n∆x∑\nm∑\nkUk,m∑\nnw2\nk,n,m+lnp(Θ). (35)\nWe\ncan derive update equations for the model parameters,\nusing the above auxiliary function. By setting at zero the\npartial derivative of L+(Θ,\u0015) with respect to each of the\nmodel parameters, we obtain\nw2\nk,n,m←∑\nlYl,mλk,n,l,m+1/2√\n2πR kAk,mσ/∆ x+ν2/(2|Bk(ejneΩk,mu0)|2),(36)\nΩk←αl\nτ2D+αg\nυ2\nkIM+∑\nn,ldiag( pk,n,l)−1\n×µkαg\nυ2\nk1M+∑\nn,l(xl−lnn)pk,n,l, (37)\nRk∝∑\nl,mYl,m∑\nnλk,n,l,m+γ(R)\nk−1\n∑\nm,nAk,mw2\nk,m,n, (38)\nAk,m∝∑\nlYl,m∑\nnλk,n,l,m+γ(A)\nk,m−1\nRk∑\nnw2\nk,m,n, (39)\npk,n,l:=1\nσ2[\nYl,1λk,n,l,1,Yl,2λk,n,l,2,··\n·,Yl,Mλk,n,l,M]⊤,(40)\nwhere diag( p) converts a vector pinto a diagonal matrix\nwith the elements of pon the main diagonal.\nAs for the update equations for the AR coeﬃcients \f,\nwe can invoke the method described in [23] with a slight\nmodiﬁcation, since the terms in the auxiliary function that\ndepend on \fhas the similar form as the objective function\ndeﬁned in [23]. It can be shown that L+can be increased\nby the following updates (the details are omitted owing to\nspace limitations):\nhk←ˆCk(\fk)\fk,\fk←C−1\nkhk, (41)\nwhere Ckand ˆCk(\fk) are ( P+1)×(P+1) Toeplitz matrices,\nwhose ( p,q)-th elements are\nCk,p,q=1\nMN∑\nm,nw2\nk,m,n\n2νcos[( p−q)neΩk,mu0],\nˆCk,p,q(\fk)=1\nMN∑\nm,n1\n|Bk(ejneΩk,mu0)|2cos[( p−q)neΩk,mu0].\n(42)\n5. EXPERIMENTS\nIn\nthe following preliminary experiments, we simpliﬁed\nHTFD by omitting the source ﬁlter model and assuming\nthe time-invariance of wk,m,n.\n5.1F0tracking of violin sound\nTo conﬁrm whether HTFD can track the F0contour of\na sound, we compared HTFD with NMF with the I-\ndivergence, by using a 16 kHz-sampled audio signal which\nTime [s]Frequency [Hz]\n2468174 551 17505555Figure\n2. Power spectrogram of a mixed audio signal of\nthree violin vibrato sounds (D ♭4, F4 and A♭4).\nwere artiﬁcially made by mixing D♭4, F4 and A♭ 4 violin\nvibrato sounds from the RWC instrument database [3]. In\nthis paper, the F0of the pitch name A4 was set at 440\nHz. The power spectrogram of the mixed signal is shown\nin Fig. 2. To convert the signal into a spectrogram, we\nemployed the fast approximate continuous wavelet trans-\nform [9] with a 16 ms time-shift interval. {xl}lranged 55\nto 7040 Hz per 10 cent. The parameters of HTFD were\nset at \r(A)\nk=(1−3.96×10−6)1I, (τk,vk)=(0.83,1.25)\nfor all k, (N,K,σ,α g,αs)=(8,73,0.02, 1,1),and\r(R)=\n(1−2.4×10−3)1K.{µk}kranged A1 to A♯7 with a chromatic\ninterval, i.e. µk=ln(55) +ln(2)×(k−1)/12. The number\nof NMF bases were set at three. The parameter updates of\nboth HTFD and NMF were stopped at 100 iterations.\nWhile the estimates of spectrograms obtained with\nNMF were ﬂat and the vibrato spectra seemed to be aver-\naged (Fig. 3 (a)), those obtained with HTFD tracked the F0\ncontours of the vibrato sounds appropriately (Fig. 3 (b)),\nand clear vibrato sounds were contained in the separated\naudio signals by HTFD.\n5.2 Separation using key information\nWe next examined whether the prior information of a\nsound improve source separation accuracy. The key of the\nsound used in 5.1, was assumed as D ♭major. The key in-\nformation was incorporated in the estimation scheme by\nsettingγ(R)\nk=1−2.4×10−3for the pitch indices that are\nnot contained in the D♭ major scale and γ(R)\nk=1−3.0×10−3\nfor the pitch indices contained in that scale. The other con-\nditions were the same as 5.1.\nWith HTFD without using the key information, the es-\ntimated activations of the pitch indices that were not con-\ntained in the scale, in particular D4, were high as illus-\ntrated in Fig. 4 (a). In contrast, those estimated activations\nwith HTFD using the key information were suppressed as\nshown in Fig. 4 (b). These results thus support strongly that\nincorporating prior information improve the source separa-\ntion accuracy.\n5.3 Transposing from one key to another\nHere we show some results of an experiment on automatic\nkey transposition [11] using HTFD. The aim of key trans-\nposition is to change the key of a musical piece to another\nkey. We separated the spectrogram of a polyphonic sound\ninto spectrograms of individual pitches using HFTD, trans-\nposed the pitches of the subset of the separated compo-\nnents, added all the spectrograms together to construct a\npitch-modiﬁed polyphonic spectrogram, and constructed a\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n627Time [s] Frequency [Hz] \n0 2 4 6311554988\nTime [s] Frequency [Hz] \n0 2 4 6311554988\nTime [s] Frequency [Hz] \n0 2 4 6311554988(a) Estimates\nof spectrograms and F0contours (orange lines) obtained with HTFD\nTime [s] Frequency [Hz] \n0 2 4 6311554988\nTime [s] Frequency [Hz] \n0 2 4 6311554988\nTime [s] Frequency [Hz] \n0 2 4 6311554988\n(b) Estimates\nof spectrograms obtained with NMF\nFigure 3 . Estimated spectrogram models by harmonic-temporal factor decomposi-\ntion (HTFD) and non-negative matrix factorization (NMF). In left-to-right fashion,\nthe spectrogram models are for D♭4, F4 and A♭ 4.\nPitch Ă\u0015 \n'\u0015 \nD̆\u0015 %\u0015 \n5JNF (a) W\nithout key information\nPitch Ă\u0015 \n'\u0015 \nD̆\u0015 %\u0015 \n5JNF \n(b) W\nith key information\nFigure 4. Temporal activations of\nA3–A♭4 estimated with HTFD using\nand without using prior information\nof the key. The red curves represent\nthe temporal activations of D4.\ntime-domain signal from the modiﬁed spectrogram using\nthe method described in [13]. For the key transposition,\nwe adopted a simple way: To transpose, for example, from\nAmajor scale to A natural minor scale, we changed the\npitches of the separated spectrograms corresponding to C♯ ,\nF♯and G♯to C, F and G, respectively.\nSome results are demonstrated in http://hil.t.\nu-tokyo.ac.jp/ ~nakamura/demo/HTFD.html .\n6. CONCLUSION\nThis paper proposed a new approach for monaural source\nseparation called the “Harmonic-Temporal Factor Decom-\nposition (HTFD)” by introducing a spectrogram model that\ncombines the features of the models employed in the NMF\nand HTC approaches. We further described some ideas\nhow to design the prior distributions for the present model\nto incorporate musically relevant information into the sep-\naration scheme.\n7. ACKNOWLEDGEMENTS\nThis work was supported by JSPS Grant-in-Aid for Young\nScientists B Grant Number 26730100.\n8. REFERENCES\n[1] A. S. Bregman: Auditory Scene Analysis , MIT Press, Cam-\nbridge, 1990.\n[2] J. S. Downie, D. Byrd, and T. Crawford: “Ten years of IS-\nMIR: Reﬂections on challenges and opportunities,” Proc. IS-\nMIR, pp. 13–18, 2009.\n[3] M. Goto: “Development of the RWC Music Database,” Proc.\nICA, pp. l–553–556, 2004.\n[4] R. Hennequin, R. Badeau, and B. David: “Time-dependent\nparametric and harmonic templates in non-negative matrix\nfactorization,” Proc. DAFx , pp. 246–253, 2010.\n[5] R. Hennequin, B. David, and R. Badeau: “Score informed\naudio source separation using a parametric model of non-\nnegative spectrogram,” Proc. ICASSP , pp. 45–48, 2011.\n[6] G. E. Hinton: “Training products of experts by minimiz-\ning contrastive divergence,” Neural Comput. , vol. 14, no. 8,\npp. 1771–1800, 2002.\n[7] G. Hu, and D. L. Wang: “An auditory scene analysis approach\nto monaural speech segregation,” Topics in Acoust. Echo and\nNoise Contr., pp. 485–515, 2006.\n[8] H. Kameoka: Statistical Approach to Multipitch Analysis ,\nPhD thesis, The University of Tokyo, Mar. 2007.[9] H. Kameoka, T. Tabaru, T. Nishimoto, and S. Sagayama:\n(Patent) Signal processing method and unit , in Japanese, Nov.\n2008.\n[10] H. Kameoka, T. Nishimoto, and S. Sagayama: “A multipitch\nanalyzer based on harmonic temporal structured clustering,”\nIEEE Trans. ASLP , vol. 15, no. 3, pp. 982–994, 2007.\n[11] H. Kameoka, J. Le Roux, Y . Ohishi, and K. Kashino: “Music\nFactorizer: A note-by-note editing interface for music wave-\nforms,” IPSJ SIG Tech. Rep., 2009-MUS-81-9, in Japanese,\nJul. 2009.\n[12] H. Kameoka: “Statistical speech spectrum model incorporat-\ning all-pole vocal tract model and F0contour generating pro-\ncess model,” IEICE Tech. Rep. , vol. 110, no. 297, SP2010-74,\npp. 29–34, in Japanese, Nov. 2010.\n[13] T. Nakamura and H. Kameoka: “Fast signal reconstruction\nfrom magnitude spectrogram of continuous wavelet trans-\nform based on spectrogram consistency,” Proc. DAFx, 40, to\nappear, 2014.\n[14] M. Nakano, J. Le Roux, H. Kameoka, Y . Kitano, N. Ono,\nand S. Sagayama: “Nonnegative matrix factorization with\nMarkov-chained bases for modeling time-varying patterns in\nmusic spectrograms,” Proc. LVA/ICA, pp. 149–156, 2010.\n[15] A. Ozerov, C. F ´evotte, R. Blouet, and J. L. Durrieu: “Mul-\ntichannel nonnegative tensor factorization with structured\nconstraints for user-guided audio source separation,” Proc.\nICASSP ., pp. 257–260, 2011.\n[16] S. A. Raczy ´nski, N. Ono, and S. Sagayama: “Multipitch anal-\nysis with harmonic nonnegative matrix approximation,” Proc.\nISMIR, pp. 381–386, 2007.\n[17] D. Sakaue, T. Otsuka, K. Itoyama, and H. G. Okuno:\n“Bayesian nonnegative harmonic-temporal factorization and\nits application to multipitch analysis,” Proc. ISMIR , pp. 91–\n96, 2012.\n[18] U. Simsekli and A. T. Cemgil: “Score guided musical source\nseparation using generalized coupled tensor factorization,”\nProc. EUSIPCO , pp. 2639–2643, 2012.\n[19] P. Smaragdis and J. C. Brown: “Non-negative matrix factor-\nization for polyphonic music transcription,” Proc. WASPAA ,\npp. 177–180, 2003.\n[20] P. Smaragdis and G. J. Mysore: “Separation by ”humming”:\nUser-guided sound extraction from monophonic mixtures,”\nProc. WASPAA, pp. 69–72, 2009.\n[21] E. Vincent, N. Bertin, and R. Badeau: “Harmonic and inhar-\nmonic nonnegative matrix factorization for polyphonic pitch\ntranscription,” Proc. ICASSP , pp. 109–112, 2008.\n[22] K. Yoshii and M. Goto: “Inﬁnite latent harmonic allocation:\nA nonparametric Bayesian approach to multipitch analysis,”\nProc. ISMIR , pp. 309–314, 2010.\n[23] A. El-Jaroudi, J. Makhoul: “Discrete all-pole modeling,”\nIEEE Trans. SP , vol. 39, no. 2, pp. 411–423, 1991.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n628"
    },
    {
        "title": "Identifying Polyphonic Musical Patterns From Audio Recordings Using Music Segmentation Techniques.",
        "author": [
            "Oriol Nieto",
            "Morwaread Mary Farbood"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417259",
        "url": "https://doi.org/10.5281/zenodo.1417259",
        "ee": "https://zenodo.org/records/1417259/files/NietoF14.pdf",
        "abstract": "This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may con- tain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmenta- tion, which exploits repetitive features in order to auto- matically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the re- cently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics.",
        "zenodo_id": 1417259,
        "dblp_key": "conf/ismir/NietoF14",
        "keywords": [
            "patterns",
            "note collections",
            "repeatedly occur",
            "musical work",
            "harmony",
            "timbre",
            "rhythm",
            "algorithm",
            "music segmentation",
            "audio recordings"
        ],
        "content": "IDENTIFYING POLYPHONIC PATTERNS FROM AUDIO RECORDINGS\nUSING MUSIC SEGMENTATION TECHNIQUES\nOriol Nieto and Morwaread M. Farbood\nMusic and Audio Research Lab\nNew York University\nforiol, mfarboodg@nyu.edu\nABSTRACT\nThis paper presents a method for discovering patterns of\nnote collections that repeatedly occur in a piece of music.\nWe assume occurrences of these patterns must appear at\nleast twice across a musical work and that they may con-\ntain slight differences in harmony, timbre, or rhythm. We\ndescribe an algorithm that makes use of techniques from\nthe music information retrieval task of music segmenta-\ntion, which exploits repetitive features in order to auto-\nmatically identify polyphonic musical patterns from audio\nrecordings. The novel algorithm is assessed using the re-\ncently published JKU Patterns Development Dataset, and\nwe show how it obtains state-of-the-art results employing\nthe standard evaluation metrics.\n1. INTRODUCTION\nThe task of discovering repetitive musical patterns (of which\nmotives, themes, and repeated sections are all examples)\nconsists of retrieving the most relevant musical ideas that\nrepeat at least once within a speciﬁc piece [1, 8]. Besides\nthe relevant role this task plays in musicological studies,\nespecially with regard to intra-opus analysis, it can also\nyield a better understanding of how composers write and\nhow listeners interpret the underlying structure of music.\nComputational approaches to this task can dramatically sim-\nplify not only the analysis of a speciﬁc piece, but of an\nentire corpus, potentially offering interesting explorations\nand relations of patterns across works. Other potential\napplications include the improved navigation across both\nlarge music collections and stand-alone pieces, or the de-\nvelopment of computer-aided composition tools.\nTypically the task of automatically discovering musical\npatterns uses symbolic representations of music [3]. Meth-\nods that assume a monophonic representation have been\nproposed, and operate on various musical dimensions such\nas chromatic/diatonic pitch, rhythm, or contour [4, 9, 10].\nOther methods focusing on polyphonic music as input have\nc\rOriol Nieto, Morwaread M. Farbood.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Oriol Nieto, Morwaread M. Farbood.\n“Identifying Polyphonic Patterns From Audio Recordings Using Music\nSegmentation Techniques”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.also been presented, mostly using geometric representa-\ntions in Euclidean space, with a different axis assigned to\neach musical dimension [6, 11]. Similar techniques have\nalso been explored [7, 11, 12] that attempt to arrive at a\ncompressed representation of an input, multidimensional\npoint set. Other methods using cognitively inspired rules\nwith symbolic representations of music have also been pro-\nposed [6, 16]. Working with the score of a musical piece\ninstead of its audio representation can indeed reduce the\ncomplexity of the problem, however this also signiﬁcantly\nnarrows the applicability of the algorithm, since it is not\nnecessarily common to have access to symbolic represen-\ntations of music, particularly when working with genres\nsuch as jazz, rock, or Western popular music.\nMethods using audio recordings as input have also been\nexplored. A good recent example is [3], where the authors\nﬁrst estimate the fundamental frequency (F0) from the au-\ndio in order to obtain the patterns using a symbolic-based\napproach. Another one uses a probabilistic approach to\nmatrix factorization in order to learn the different parts of\na western popular track in an unsupervised manner [20].\nYet another method uses a compression criterion where the\nmost informative (i.e., repeated) parts of a piece are iden-\ntiﬁed in order to automatically produce a musical “sum-\nmary” [17].\nIn this paper, we propose a method using audio record-\nings as input in an attempt to broaden the applicability of\npattern discovery algorithms. We make use of tools that\nare commonly employed in the music information retrieval\ntask of music segmentation combined with a novel score-\nbased greedy algorithm in order to identify the most re-\npeated parts of a given audio signal. Finally, we evaluate\nthe results using the JKU Patterns Development Dataset\nand the metrics proposed in the Music Information Re-\ntrieval Evaluation eXchange (MIREX) [1].\nThe outline of this paper is as follows: In Section 2 we\nreview a set of music segmentation techniques that will be\nused in our algorithm; in Section 3 we detail our method to\nextract musical patterns, including the score-based greedy\nalgorithm; in Section 4 we present the evaluation and the\nresults; and in Section 5 we draw various conclusions and\nidentify areas for future work.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4112. MUSIC SEGMENTATION TECHNIQUES\nThe task of music segmentation is well-established in the\nmusic informatics literature (see [18] for a review). Its goal\nis to automatically identify all the non-overlapping musi-\ncal segments (or sections) of a given track, such that the\nconcatenation of all of them reconstructs the entire piece.\nOnce these segments are identiﬁed, they are labeled based\non their similarity (e.g., verse, chorus, coda). Therefore,\nthis task can be divided into two different subproblems: the\ndiscovery of the boundaries that deﬁne all the segments,\nand the grouping of the segments into different labels. In\nthis work we will use tools that focus mainly on the former\nsubproblem.\nThere is general agreement among researchers that any\ngiven boundary is deﬁned by at least one of these three\ncharacteristics: repetition, homogeneity, and/or novelty\n[18]. In our case, we center the discussion on the repetition\nboundaries, since, as we will see in Section 3, repetition is\nthe deﬁning feature of the musical patterns.\n2.1 Extracting Repetitive Boundaries\nIn this subsection we review a standard technique to ex-\ntract boundaries characterized by repetition (also known as\na sequence approach), from an input audio signal x. For a\nmore detailed explanation, we refer the reader to [13]. The\nprocess can be summarized in three different steps:\ni The signal xis transformed into a series of feature vec-\ntorsC= (c 1;:::;cN)that dividexintoNframes and\ncapture speciﬁc frame-level characteristics of the given\nsignal. In our case, we will only focus on harmonic\nfeatures, more speciﬁcally on chromagrams (or pitch\nclass proﬁles).\niiCis used in order to obtain a self-similarity matrix\n(SSM)S, a symmetric matrix such that S(n;m) =\nd(cn;cm);8n;m2[1 :N], wheredis a distance func-\ntion (e.g. Euclidean, cosine, Manhattan).\niii The resulting matrix Swill contain diagonal paths (or\nsemi-diagonal in case of slight tempo variations) or\nstripes that will indicate the repetition of a speciﬁc part\nof the audio signal x. These paths can be extracted us-\ning greedy algorithms (e.g., as described in [13]). The\nﬁnal boundaries are given by the endpoints of these\npaths.\nAn example of an SSM using the Euclidean distance\nwith the identiﬁed boundaries is shown in Figure 1. As\ncan be seen, the annotated boundaries are visually asso-\nciated with the paths of the matrix. The identiﬁcation of\npatterns, as opposed to the task of segmentation, allows\noverlapping patterns and occurrences, so we base our al-\ngorithm on greedy methods to extract paths from an SSM.\n2.2 Transposition-Invariant Self-Similarity Matrix\nIt is common to analyze pieces that contain key-transposed\nrepetitions. It is therefore important for an algorithm to\nbe invariant to these these transpositions when identifying\n0 500 1000 1500\nTime frames0\n500\n1000\n1500Time framesFigure 1. Self-similarity matrix for Chopin’s Op. 24 No. 4,\nwith annotated boundaries as vertical and horizontal lines.\nrepetitions. One effective method for solving this prob-\nlem [14] involves a technique that can be described in two\nsteps: (1) compute 12 different SSMs from harmonic rep-\nresentations (e.g. chromagrams), each corresponding to a\ntransposition of the 12 pitches of the Western chromatic\nscale, and 2) obtain the transposition-invariant SSM by\nkeeping the minimum distance across the 12 matrices for\nall theN\u0002Ndistances in the output matrix. Formally:\nS(n;m) = mink2[0:11]fSk(n;m)g;8n;m2[1 :N](1)\nwhereSis the transposition-invariant SSM, and Skis the\nk-th transposition of the matrix S.\n3. IDENTIFYING MUSICAL PATTERNS\nThe discovery of patterns and their various occurrences in-\nvolves retrieving actual note collections (which may nest\nand/or overlap), and so this task can be seen as more com-\nplex than structural segmentation, which involves labeling\na single, temporal partition of an audio signal. We deﬁne\na repeating musical pattern to be a short idea that is re-\npeated at least once across the entire piece, even though\nthis repetition may be transposed or contain various time\nshifts. Therefore, each pattern is associated with a set of\noccurrences that will not necessarily be exact. The pat-\nterns and their occurrences may overlap with each other,\nand this is perfectly acceptable in the context of pattern\ndiscovery. An optimal algorithm for this task would (1)\nﬁnd all the patterns contained in a piece and (2) identify all\nthe occurrences across the piece for each pattern found. In\nthis section we describe our algorithm, which uses audio\nrecordings as input and ﬁnds polyphonic patterns as well\nas a list of all the discovered occurrences for each of the\npatterns. A block-diagram of the entire process is depicted\nin Figure 2.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n412Chromagram\nSelf Similarity \nMatrix\nKey Invariant \nSSM\nFind Repeated \nSegmentsν, θ, ρ\nCluster Segments \ninto PatternsAudio\nPatternsΘFigure 2. Block diagram of the proposed algorithm.\n3.1 Rhythmic-Synchronous Harmonic Feature\nExtraction\nGiven a one-channel audio signal xsampled at 11025 Hz\nrepresenting a piece of music, we compute the spectrogram\nusing a Blackman window of Nw= 290 milliseconds,\nwith a hop size of Nw=2. We then compute a constant-\nQ transform from the spectrogram starting at 55 Hz (cor-\nresponding to the note A1 in standard tuning) comprising\nfour octaves. Finally, we collapse each of the 12 pitches\nof the western scale into a single octave to obtain a chro-\nmagram, a matrix of 12\u0002N, which is commonly used to\nrepresent harmonic features [18]. We normalize the chro-\nmagram such that the maximum energy for a given time\nframe is 1. In this harmonic representation we can no\nlonger differentiate between octaves, but its compactness\nand the energy of each pitch class will become convenient\nwhen identifying harmonic repetitions within a piece.\nWe then use a beat tracker [5] in order to average the\ntime frames into rhythmic frames. Instead of using the\ntraditional beat-synchronous approach, which is typically\nemployed in a segmentation task, we divide each beat dura-\ntion by 4 and aggregate accordingly, thus having N= 4B\ntime frames, where Bis the number of beats detected in\nthe piece. The motivation behind this is that patterns may\nnot start at the beat level, as opposed to the case for long\nsections. Furthermore, adding a ﬁner level of granularity\n(i.e., analyzing the piece at a sixteenth-note level instead\nof every fourth note or at the beat level) should yield better\nresults.\n3.2 Finding Repeated Segments\nWe make use of the transposition-invariant SSM Sde-\nscribed in Section 2.2, computed from the chromagram of\na given audio signal using the Euclidean distance, in or-\nder to identify repeated segments. As opposed to the task\nof segmentation, the goal here is to ﬁnd allpossible re-\npeated segments in S, independent of how short they are\nor the amount of overlap present. The other major dif-\nference is that we do not aim to ﬁnd all of the segments\nof the piece, but rather identify all of the repeated ones.Repeated segments appear in Sas diagonal “stripes”, also\nknown as paths. If the beat-tracker results in no errors (or\nif the piece contains no tempo variations), these stripes will\nbe perfectly diagonal.\n3.2.1 Quantifying Paths with a Score\nWe propose a score-based greedy algorithm to efﬁciently\nidentify the most prominent paths in S. Starting from\nS2RN\u0002N, we set half of its diagonals to zero, including\nthe main one, due to its symmetrical properties, resulting\nin^S, s.t. ^S(n;m) = 0 ifn\u0014mand^S(n;m) =S(n;m)\nifn > m;8n;m2[1 :N]. We then compute a score\nfunction\u001bfor each possible path in all the non-zero di-\nagonals of ^S, resulting in a search space of N(N\u00001)=2\npossible positions in which paths can start.\nBefore introducing the score function \u001b, we deﬁne a\ntrace function given a square matrix X2RNx\u0002Nxwith an\noffset parameter !:\ntr(X;! ) =Nx\u0000!X\ni=1X(i;i+!);!2Z (2)\nAs can be seen from this equation, when != 0we have\nthe standard trace function deﬁnition.\nThe score function \u001buses various traces of the matrix\nthat comprises a possible path in order to quantify the de-\ngree of repetition of the path. If a possible path starts\nat indicesn;m and has a duration of Mtime frames,\nthen the matrix that the path deﬁnes is P2RM\u0002M, s.t.\nP(i;j) = ^S(n+i\u00001;m+j\u00001);8i;j2[1 :M]. We\nnow can deﬁne the score \u001bas the sum of the closest traces\nto the diagonal of P(i.e., those with a small !) and sub-\ntract the traces that are farther apart from the diagonal (i.e.,\nwhere!is greater). We then normalize in order to obtain\na score independent from the duration Mof the possible\npath:\n\u001b(\u001a) =\u0010P\u001a\u00001\n!=\u0000(\u001a\u00001)tr(P;! )\u0011\n\u0000tr(P;\u0006\u001a)\nM+P\u001a\u00001\ni=12(M\u0000i)(3)\nwhere\u001a2Nis the maximum offset to be taken into ac-\ncount when computing the traces of P. The greater the \u001a,\nthe greater the \u001bfor segments that contain substantial en-\nergy around their main diagonal (e.g., paths that contain\nsigniﬁcant rhythmic variations), even though the precision\ndecreases as \u001aincreases.\nExamples for various \u001b(\u001a)can be seen in Figure 3. For\na perfectly clean path (left), we see that \u001a= 1 gives the\nmaximum score of 1. However, the score decreases as \u001a\nincreases, since there is zero energy in the diagonals right\nnext to the main diagonal. On the other hand, for matrices\nextracted from audio signals (middle and right), we can see\nthat the scores \u001b(1)are low, indicating that the diagonals\nnext to the main diagonal contain amounts of energy simi-\nlar to the main diagonal. However, when \u001a>1, the score\nis substantially different from a matrix with a path (middle)\nand a matrix without one (right).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n413σ(1)=1\nσ(2)=0.36\nσ(3)=0.22\nσ(1)=-0.48\nσ(2)=0.44\nσ(3)=0.55\nσ(1)=-0.46\nσ(2)=0.21\nσ(3)=0.32Figure 3. Three examples showing the behavior of the path\nscore\u001b(\u001a). The one on the left shows a synthetic example of a\nperfect path. The one in the middle contains a real example of\na path in which there is some noise around the diagonal of the\nmatrix. In the example on the right, a matrix with no path is\nshown.\n3.2.2 Applying the Score\nFor allN(N\u00001)=2 positions in which paths can poten-\ntially start in ^S, we want to extract the most prominent ones\n(i.e., the ones that have a high \u001b). At the same time, we\nwant to extract the paths from beginning to end in the most\naccurate way possible. The algorithm that we propose as-\nsigns a certain \u001bto an initial possible path ^zof a minimum\nlength of\u0017time frames, which reduces the search space to\n(N\u0000\u0017+ 1)(N\u0000\u0017)=2. If the score \u001bis greater than a cer-\ntain threshold \u0012, we increase the possible path by one time\nframe, and recompute \u001buntil\u001b\u0014\u0012. By then, we can store\nthe path ^zas a segment in the set of segments Z. In order\nto avoid incorrectly identifying possible paths that are too\nclose to the found path, we zero out the found path from\n^S, including all the \u001aclosest diagonals, and keep looking,\nstarting from the end of the recently found path.\nThe pseudocode for this process can be seen in Algo-\nrithm 1, wherejxjreturns the length of the path x,fxg\nreturns the path in which all elements equal x, the func-\ntion ComputeScore computes the \u001b(\u001a)as described in Sec-\ntion 3.2.1, OutOfBounds(x;X ) checks whether the path x\nis out of bounds with respect to X, IncreasePath(x) in-\ncreases the path xby one (analogously as DecreasePath),\nand ZeroOutPath(X; x;\u001a)assigns zeros to the path xfound\ninX, including all the closest \u001adiagonals.\nAlgorithm 1 Find Repeated Segments\nRequire: ^S;\u001a;\u0012;\u0017\nEnsure:Z=fz1;:::;zkg\nfor^z2^S^j ^zj=\u0017^^z6=f0gdo\nb False\n\u001b ComputeScore(^ z;\u001a)\nwhile\u001b>\u0012^:OutOfBounds(^ z;^S)do\nb True\n^z IncreasePath(^ z)\n\u001b ComputeScore(^ z;\u001a)\nend while\nifbthen\nZ:add(DecreasePath(^ z))\nZeroOutPath( ^S;^z;\u001a)\nend if\nend for\nreturnZAn example of the paths found by the algorithm is\nshown in Figure 4. Parts of some segments are repeated\nas standalone segments (i.e., segments within segments),\ntherefore allowing overlap across patterns as expected in\nthis task. Observe how some of the segments repeat al-\nmost exactly across the piece—there is a set of patterns at\nthe top of the matrix that appears to repeat at least three\ntimes. The next step of the algorithm is to cluster these\nsegments together so that they represent a single pattern\nwith various occurrences.\n0 500 1000 1500\nTime Frames0\n500\n1000\n1500Time Frames\nFigure 4. Paths found (marked in white) using the proposed\nalgorithm for Chopin’s Op. 24 No. 4., with \u0012= 0:33;\u001a = 2.\n3.3 Clustering the Segments\nEach segment z2Z, deﬁned by the two indices in which\nit starts (s i;sj) and ends (e i;ej) in^S, contains two occur-\nrences of a pattern: the one that starts in siand ends in ei\nand the one that occurs between the time indices sjand\nej. In order to cluster the repeated occurrences of a single\npattern, we ﬁnd an occurrence for each segment z2Z if\none of the other segments in Zstarts and ends in similar\nlocations with respect to the second dimension of ^S. Note\nthat we set to zero the bottom left triangle of the matrix as\nexplained in Section 3.2.1, so we cannot use the ﬁrst di-\nmension to cluster the occurrences. Formally, a segment ^z\nis an occurrence of zif\n(sz\nj\u0000\u0002\u0014s^z\nj\u0014sz\nj+ \u0002)^(ez\nj\u0000\u0002\u0014e^z\nj\u0014ez\nj+ \u0002) (4)\nwheresz\njrepresents the starting point of the segment zin\nthe second dimension of ^Sand analogously ez\njis the end-\ning point, and \u0002is a tolerance parameter.\n3.4 Final Output\nAt this point, we have a set of patterns with their respective\noccurrences represented by their starting and ending time-\nframe indices. Even though the algorithm is not able to dis-\ntinguish the different musical lines within the patterns, we\ncan use the annotated score to output the exact notes that\noccur during the identiﬁed time indices, as suggested in\nthe MIREX task [1]. If no score is provided, only the time\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n414points will be presented. In order to overcome this limita-\ntion in future work, the audio should be source-separated to\nidentify the different lines and perform an F0 estimation to\ncorrectly identify the exact melody that deﬁnes the pattern\n(and not just the time points at which it occurs). Progress\ntoward this goal has been presented in [2].\n3.5 Time Complexity Analysis\nOnce the rhythm-synchronous chromagram is computed,\nthe process of calculating the transposition-invariant SSM\nisO(13N2) =O(N2), whereNis the number of time\nframes of the chromagram. The procedure to compute the\nscore given a path has a time complexity of O(2\u001aM ) =\nO(\u001aM), where\u001ais the required parameter for the compu-\ntation of the score, and Mis the length of the path from\nwhich to compute the score. The total process of iden-\ntifying segments is O\u0010\n(N\u0000\u0017+1)(N\u0000\u0017)\n2\u001aM\u0011\n=O((N\u0000\n\u0017)2\u001aM), where\u0017is the minimum number of time frames\nthat a pattern can have. Asymptotically, we can neglect the\nclustering of the segments, since the length of Zwill be\nmuch less than N. Therefore, the total time complexity of\nthe proposed algorithm is O(N2+ (N\u0000\u0017)2\u001aM).\n4. EVALUATION\nWe use the JKU Patterns Development Dataset1to evalu-\nate our algorithm. This dataset is comprised of ﬁve clas-\nsical pieces annotated by various musicologists and re-\nsearchers [1]. This dataset is the public subset of the\none employed to evaluate the Pattern Discovery task at\nMIREX, using the metrics described below.\n4.1 Metrics\nTwo main aspects of this task are evaluated: the patterns\ndiscovered and the occurrences of the identiﬁed patterns\nacross the piece. Collins and Meredith proposed metrics to\nquantify these two aspects, which are detailed in [1]; all of\nthese metrics use the standard F1accuracy score, deﬁned\nasF1= 2PR=(P +R), wherePis precision (such that\nP= 1if all the estimated elements are correct), and R= 1\nis recall (such that R= 1if all the annotated elements are\nestimated).\nEstablishment F 1Score (Fest): Determines how the\nannotated patterns are established by the estimated output.\nThis measure returns a score of 1 if at least one occurrence\nof each pattern is discovered by the algorithm to be evalu-\nated.\nOccurrence F 1Score (Fo): For all the patterns found,\nwe want to estimate the ability of the algorithm to capture\nall of the occurrences of these patterns within the piece in-\ndependently of how many different patterns the algorithm\nhas identiﬁed. Therefore, this score would be 1 if the al-\ngorithm has only found one pattern with all the correct oc-\ncurrences. A parameter ccontrols when a pattern is con-\nsidered to have been discovered, and therefore whether it\ncounts toward the occurrence scores. The higher the c, the\n1https://dl.dropbox.com/u/11997856/JKU/JKUPDD-Aug2013.zipsmaller the tolerance. In this evaluation, as in MIREX, we\nusec=:75andc=:5.\nThree-Layer F 1Score (F3): This measure combines\nboth the patterns established and the quality of their occur-\nrences into a single score. It is computed using a three-step\nprocess that yields a score of 1 if a correct pattern has been\nfound and all its occurrences have been correctly identi-\nﬁed.\n4.2 Results\nThe results of the proposed algorithm, computed using\nthe open source evaluation package mir_eval [19], are\nshown in Table 1, averaged for the entire JKU Dataset,\nalong with an earlier version of our algorithm submitted\nto MIREX [15], another recent algorithm called SIARCT-\nCFP [2] that is assessed using both audio and symbolic rep-\nresentations as input in [3], and “COSIATEC Segment”, a\nmethod that only uses symbolic inputs [12]. We use this\nlatter method for comparison because it is the only sym-\nbolic method in which we have access to all of the result-\ning metrics, and SIARCT-CFP since it is the most recent\nmethod that uses audio as input. The parameter values\nused to compute these results, \u0017= 8;\u0012 = 0:33;\u001a = 2,\nand\u0002 = 4, were found empirically. We can see how our\nalgorithm is better than [15] in all the metrics except run-\nning time; it also ﬁnds more correct patterns than [3] (the\ncurrent state-of-the-art when using audio as input).\nOur algorithm obtains state-of-the-art results when ex-\ntracting patterns from audio, obtaining an Festof 49.80%.\nThis is better than the symbolic version of [2] and almost\nas good as the algorithm described in [12]. The fact that\nour results are superior or comparable to the two other al-\ngorithms using symbolic representations indicates the po-\ntential of our method.\nWhen evaluating the occurrences of the patterns, we\nsee that our algorithm is still better than [15], but worse\nthan [2] (at least for c=:5, which is the only reported re-\nsult). Nevertheless, the numbers are much lower than [12].\nIn this case, working with symbolic representations (or es-\ntimating the F0 in order to apply a symbolic algorithm as\nin [2]) yields signiﬁcantly better results. It is interesting to\nnote that when the tolerance increases (i.e. c=:5), our\nresults improve as opposed to the other algorithms. This\nmight be due to the fact that some of the occurrences found\nin the SSM were actually very similar (therefore they were\nfound in the matrix) but were slightly different in the anno-\ntated dataset. A good example of this would be an occur-\nrence that contains only one melodic voice. Our algorithm\nonly ﬁnds points in time in which an occurrence might be\nincluded, it does not perform any type of source separation\nin order to identify the different voices. If the tolerance\ndecreases sufﬁciently, a polyphonic occurrence would be\naccepted as similar to a monophonic one corresponding to\nthe same points in time.\nOur three layer score (F 3) is the best result when using\naudio recordings, with an F-measure of 31.74% (unfortu-\nnately this metric was not reported in [2]). This metric\naims to evaluate the quality of the algorithm with a single\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n415Alg Pest Rest Fest Po(:75) Ro(:75) Fo(:75) P3 R3 F3Po(:5)Ro(:5) Fo(:5) Time (s)\nProposed 54.96 51.73 49.80 37.58 27.61 31.79 35.12 35.28 32.01 45.17 34.98 38.73 454\n[3] 14.9 60.9 23.94 – – – – – – 62.9 51.9 56.87 –\n[15] 40.83 46.43 41.43 32.08 21.24 24.87 30.43 31.92 28.23 26.60 20.94 23.18 196\n[3] 21.5 78.0 33.7 – – – – – – 78.3 74.7 76.5 –\n[12] 43.60 63.80 50.20 65.40 76.40 68.40 40.40 54.40 44.20 57.00 71.60 63.20 7297\nTable 1. Results of various algorithms using the JKU Patterns Development Dataset, averaged across pieces. The top rows of the table\ncontain algorithms that use deadpan audio as input. The bottom rows correspond to algorithms that use symbolic representations as input.\nscore, including both pattern establishment and occurrence\nretrieval. Our results are still far from perfect (32.01%), but\nwhen compared to an algorithm that uses symbolic repre-\nsentations [12] (44.21%), it appears our results are not far\nfrom the state-of-the-art for symbolic representations.\nFinally, our algorithm takes more than twice as long as\n[15]. However, our method is over 16 times faster than\n[12], indicating it is efﬁcient in terms of computation time.\nThis algorithm is implemented in Python and available for\npublic download.2\n5. CONCLUSIONS\nWe presented a method to discover repeating polyphonic\npatterns using audio recordings as input. The method\nmakes use of various standard techniques typically used for\nmusic segmentation. We evaluated our method using the\nJKU Pattern Development Dataset and showed how it ob-\ntains competent results when retrieving all the occurrences\nof the patterns and state-of-the-art results when ﬁnding pat-\nterns. When the algorithm is compared to others that use\nsymbolic representations, we see that it is comparable or\nsuperior in terms of the correct patterns found. In future\nwork, source separation might be needed to successfully\nidentify patterns that only comprise a subset of the differ-\nent musical lines.\n6. REFERENCES\n[1] T. Collins. Discovery of Repeated Themes & Sections, 2013.\n[2] T. Collins, A. Arzt, S. Flossmann, and G. Widmer. SIARCT-CFP:\nImproving Precision and the Discovery of Inexact Musical Patterns\nin Point-set Representations. In Proc. of the 14th International So-\nciety for Music Information Retrieval Conference, pages 549–554,\nCuritiba, Brazil, 2014.\n[3] T. Collins, B. Sebastian, F. Krebs, and G. Widmer. Bridging the\nAudio-Symbolic Gap: The Discovery of Repeated Note Content Di-\nrectly From Polyphonic Music Audio. In Audio Engineering Society\nConference: 53rd International Conference: Semantic Audio, pages\n1–12, London, UK, 2014.\n[4] D. Conklin and C. Anagnostopoulou. Representation and Discovery\nof Multiple Viewpoint Patterns . In Proc. of the International Com-\nputer Music Conference, pages 479–485, La Havana, Cuba, 2001.\n[5] D. P. W. Ellis and G. E. Poliner. Identifying ’Cover Songs’ with\nChroma Features and Dynamic Programming Beat Tracking. In Proc.\nof the 32nd IEEE International Conference on Acoustics Speech and\nSignal Processing, pages 1429–1432, Honolulu, HI, USA, 2007.\n[6] J. C. Forth. Cognitively-motivated Geometric Methods of Pattern Dis-\ncovery and Models of Similarity in Music. PhD thesis, Glodsmiths,\nUniversity of London, 2012.\n2https://github.com/urinieto/MotivesExtractor[7] J. C. Forth and G. A. Wiggins. An Approach for Identifying Salient\nRepetition in Multidimensional Representations of Polyphonic Mu-\nsic. In J. Chan, J. W. Daykin, and M. S. Rahman, editors, London\nAlgorithmics 2008: Theory and Practice, pages 44–58. UK: College\nPublications, 2009.\n[8] B. Janssen, W. B. D. Haas, A. V olk, and P. V . Kranenburg. Discov-\nering Repeated Patterns in Music: State of Knowledge, Challenges,\nPerspectives. In Proc. of the 10th International Symposium on Com-\nputer Music Multidisciplinary Research, Marseille, France, 2013.\n[9] O. Lartillot. Multi-Dimensional motivic pattern extraction founded\non adaptive redundancy ﬁltering. Journal of New Music Research,\n34(4):375–393, Dec. 2005.\n[10] K. Lemstr ¨om.String Matching Techniques for Music Retrieval. PhD\nthesis, University of Helsinki, Finland, 2000.\n[11] D. Meredith. Point-set Algorithms For Pattern Discovery And Pattern\nMatching In Music. In T. Crawford and R. C. Veltkamp, editors, Proc.\nof the Dagstuhl Seminar on Content-Based Retrieval., Dagstuhl, Ger-\nmany, 2006.\n[12] D. Meredith. COSIATEC and SIATECCompress: Pattern Discovery\nby Geometric Compression. In Music Information Retrieval Evalua-\ntion eXchange, Curitiba, Brazil, 2013.\n[13] M. M ¨uller. Information Retrieval for Music and Motion. Springer,\n2007.\n[14] M. M ¨uller and M. Clausen. Transposition-Invariant Self-Similarity\nMatrices. In Proc. of the 8th International Conference on Music In-\nformation Retrieval, pages 47–50, Vienna, Austria, 2007.\n[15] O. Nieto and M. Farbood. MIREX 2013: Discovering Musical Pat-\nterns Using Audio Structural Segmentation Techniques. In Music In-\nformation Retrieval Evaluation eXchange, Curitiba, Brazil, 2013.\n[16] O. Nieto and M. M. Farbood. Perceptual Evaluation of Automatically\nExtracted Musical Motives. In Proc. of the 12th International Con-\nference on Music Perception and Cognition, pages 723–727, Thessa-\nloniki, Greece, 2012.\n[17] O. Nieto, E. J. Humphrey, and J. P. Bello. Compressing Audio\nRecordings into Music Summaries. In Proc. of the 13th International\nSociety for Music Information Retrieval Conference, Porto, Portugal,\n2012.\n[18] J. Paulus, M. M ¨uller, and A. Klapuri. Audio-Based Music Structure\nAnalysis. In Proc of the 11th International Society of Music Informa-\ntion Retrieval, pages 625–636, Utrecht, Netherlands, 2010.\n[19] C. Raffel, B. Mcfee, E. J. Humphrey, J. Salamon, O. Nieto, D. Liang,\nand D. P. W. Ellis. mir eval: A Transparent Implementation of Com-\nmon MIR Metrics. In Proc. of the 15th International Society for Mu-\nsic Information Retrieval Conference, Taipei, Taiwan, 2014.\n[20] R. Weiss and J. P. Bello. Unsupervised Discovery of Temporal Struc-\nture in Music. IEEE Journal of Selected Topics in Signal Processing,\n5(6):1240–1251, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n416"
    },
    {
        "title": "Perceptual Analysis of the F-Measure to Evaluate Section Boundaries in Music.",
        "author": [
            "Oriol Nieto",
            "Morwaread Mary Farbood",
            "Tristan Jehan",
            "Juan Pablo Bello"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414958",
        "url": "https://doi.org/10.5281/zenodo.1414958",
        "ee": "https://zenodo.org/records/1414958/files/NietoFJB14.pdf",
        "abstract": "In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the bound- aries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F- measure rather than the recall component, therefore mak- ing the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experi- ments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over re- call, making the section boundary evaluation more expres- sive and reliable.",
        "zenodo_id": 1414958,
        "dblp_key": "conf/ismir/NietoFJB14",
        "keywords": [
            "limitations",
            "F-measure",
            "evaluation",
            "quality",
            "boundaries",
            "automatic segmentation",
            "music",
            "subjects",
            "rating",
            "perceptually informative"
        ],
        "content": "PERCEPTUAL ANALYSIS OF THE F-MEASURE FOR EVALUATING\nSECTION BOUNDARIES IN MUSIC\nOriol Nieto1,Morwaread M. Farbood1,Tristan Jehan2, and Juan Pablo Bello1\n1Music and Audio Research Lab, New York University, {oriol, mfarbood, jpbello}@nyu.edu\n2The Echo Nest, tristan@echonest.com\nABSTRACT\nIn this paper, we aim to raise awareness of the limitations\nof the F-measure when evaluating the quality of the bound-aries found in the automatic segmentation of music. Wepresent and discuss the results of various experiments wheresubjects listened to different musical excerpts containingboundary indications and had to rate the quality of theboundaries. These boundaries were carefully generatedfrom state-of-the-art segmentation algorithms as well ashuman-annotated data. The results show that humans tendto give more relevance to the precision component of the F-\nmeasure rather than the recall component, therefore mak-\ning the classical F-measure not as perceptually informativeas currently assumed. Based on the results of the experi-ments, we discuss the potential of an alternative evaluationbased on the F-measure that emphasizes precision over re-call, making the section boundary evaluation more expres-sive and reliable.\n1. INTRODUCTION\nOver the past decade, signiﬁcant effort has been made to-ward developing methods that automatically extract large-scale structures in music. In this paper, we use the termmusical structure analysis to refer to the task that identiﬁes\nthe different sections (or segments) of a piece. In West-ern popular music, these sections are commonly labeled asverse, chorus, bridge, etc. Given that we now have access\nto vast music collections, this type of automated analysiscan be highly beneﬁcial for organizing and exploring thesecollections.\nMusical structure analysis is usually divided into two\nsubtasks: the identiﬁcation of section boundaries and thelabeling of these sections based on their similarity. Here,we will only focus on the former. Section boundaries usu-ally occur when salient changes in various musical qual-ities (such as harmony, timbre, rhythm, or tempo) takeplace. See [9] for a review of some of the state of the art inmusical structure analysis.\nc\u0000Oriol Nieto, Morwaread M. Farbood, Tristan Jehan,\nJuan Pablo Bello. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Oriol Nieto, Mor-\nwaread M. Farbood, Tristan Jehan, Juan Pablo Bello. “Perceptual Analy-sis of the F-Measure for Evaluating Section Boundaries in Music”, 15thInternational Society for Music Information Retrieval Conference, 2014.Typically, researchers make use of various human-anno-\ntated datasets to measure the accuracy of their analysis al-\ngorithms. The standard methodology for evaluating theaccuracy of estimated section boundaries is to comparethose estimations with ground truth data by means of theF-measure (also referred to as the hit rate), which givesequal weight to the values of precision (proportion of theboundaries found that are correct) and recall (proportionof correct boundaries that are located). However, it is notentirely clear that humans perceive the type of errors thosetwo metrics favor or the penalties they impose as equallyimportant, calling into question the perceptual relevance ofthe F-measure for evaluating long-term segmentation. Tothe best of our knowledge, no empirical evidence or formalstudy exists that can address such a question in the contextof section boundary identiﬁcation. This work is an effortto redress that.\nOur work is motivated by a preliminary study we ran on\ntwo subjects showing a preference for high precision re-sults, thus making us reconsider the relevance of precisionand recall for the evaluation of section boundary estima-tions. As a result, in this paper we present two additionalexperiments aimed at validating and expanding those pre-liminary ﬁndings including a larger subject population andmore controlled conditions. In our experiments, we focuson the analysis of Western popular songs since this is thetype of data most segmentation algorithms in the MIR lit-erature operate on, and since previous studies have shownthat most listeners can conﬁdently identify structure in thistype of music [1].\nThe rest of this paper is organized as follows. We present\na review of the F-measure and a discussion of the prelimi-nary study in section 2. We describe the design of two ex-periments along with discussions of their results in sections3 and 4. We explore an alternative F-measure based on ourexperimental ﬁndings that could yield more expressive andperceptually relevant outcomes in section 5. Finally, wedraw conclusions and discuss future work in section 6.\n2. THE F-MEASURE FOR MUSIC BOUNDARIES\n2.1 Review of the F-measure\nIn order to evaluate automatically computed music bound-\naries, we have to deﬁne how we accept or reject an esti-mated boundary given a set of annotated ones (i.e., ﬁndthe intersection between these two sets). Traditionally, re-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n265searchers consider an estimated boundary correct as long\nas its maximum deviation to its closest annotated bound-\nary is ±3 seconds [8] (in MIReX,1inspired by [16], an\nevaluation that uses a shorter window of ±0.5 seconds is\nalso performed). Following this convention, we use a ±\n3-second window in our evaluation.\nLet us assume that we have a set of correctly estimated\nboundaries given the annotated ones (hits), a set of anno-tated boundaries that are not estimated (false negatives),and a set of estimated boundaries that are not in the an-notated dataset (false positives). Precision is the ratio be-tween hits and the total number of estimated elements (e.g.,we could have 100% precision with an algorithm that onlyreturns exactly one boundary and this boundary is correct).Recall is the ratio between hits and the total number of an-notated elements (e.g. we could have a 100% recall with analgorithm that returns one boundary every 3 seconds, sinceall the annotated boundaries will be sufﬁciently close to anestimated one). Precision and recall are deﬁned formallyas\nP=\n|hits |\n|bounds e|;R=|hits |\n|bounds a|(1)\nwhere |·|represents the cardinality of the set ·, bounds e\nis the set of estimated boundaries and bounds ais the set\nof annotated ones. Finally, the F-measure is the harmonicmean between PandR, which weights these two values\nequally, penalizes small outliers, and mitigates the impactof large ones:\nF=2P·R\nP+R(2)\nWhen listening to the output of music segmentation al-\ngorithms, it is immediately apparent that false negativesand false positives are perceptually very different (an initialdiscussion about assessing a synthetic precision of 100%\nwhen evaluating boundaries can be found in [14]). Thus,in the process of developing novel methods for structuresegmentation, we decided to informally assess the relativeeffect that different types of errors had on human evalua-tions of the accuracy of the algorithms’ outputs. The fol-lowing section describes the resulting preliminary study.\n2.2 Preliminary Study\nFor this study we compared three algorithms, which we\nwill term A,BandC.Ais an unpublished algorithm cur-\nrently in development that relies on homogeneous repeatedsection blocks; Bis an existing algorithm that uses nov-\nelty in audio features to identify boundaries; and Ccom-\nbines the previous two methods. All three methods wereoptimized to maximize their F-measure performance onthe structure-annotated Levy dataset [5]. Table 1 showseach method’s average F-measure, precision, and recallvalues across the entire set. Note how Cmaximizes the F-\nmeasure, mostly by increasing recall, while Ashows max-\nimum precision.\nWe asked two college music majors to rank the three\nalgorithms for every track in the Levy set. The goal was\n1http://www.music-ir.org/mirex/wiki/MIREX HOMEPreliminary Study\nAlgorithm F P R\nA 49% 57% 47%\nB 44% 46% 46%\nC 51% 47% 64%\nTable 1. Algorithms and their ratings used to generate the input\nfor the preliminary study. These ratings are averaged across the\n60 songs of the Levy dataset.\nnot to compare the results of the algorithms to the anno-\ntated ground truth, but to compare the algorithms with eachother and determine the best one from a perceptual pointof view. The participants were asked to listen to each ofthe algorithm outputs for all the songs and rank the algo-rithms by the quality of their estimated section boundaries;no particular constraints were given on what to look for.We used Sonic Visualiser [3] to display the waveform andthree section panels for each of the algorithms in parallel(see Figure 1). While playing the audio, listeners couldboth see the sections and hear the boundaries indicated bya distinctive percussive sound. The section panels wereorganized at random for each song so listeners could noteasily tell which algorithm they were choosing.\nFigure 1. Screenshot of Sonic Visualiser used in the preliminary\nexperiment. The song is “Smells Like Teen Spirit” by Nirvana.\nIn this case, algorithms are ordered as A,B, andCfrom top to\nbottom.\nAnalysis of the results showed that 68.3% of the time,\nthe two participants chose the same best algorithm. In\n23.3% of the cases, they disagreed on the best, and in just8.3% of the cases, they chose opposite rankings. Whenthey actually agreed on the best algorithm, they chose A\n58.5% of the time. Adid not have the highest F-measure\nbut the highest precision. Perhaps more surprising, theychose Conly 14.6% of the time even though that algorithm\nhad the highest F-measure.\nThese results raised the following questions: Is the F-\nmeasure informative enough to evaluate the accuracy ofautomatically estimated boundaries in a perceptually-mea-ningful way? Is precision more important than recall whenassessing music boundaries? Would the observed trendsremain when tested on a larger population of subjects? Canthese results inform more meaningful evaluation measures?We decided to address these questions by running two moreformal experiments in order to better understand this ap-parent problem and identify a feasible solution.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2663. EXPERIMENT 1: RATING BOUNDARIES\n3.1 Motivation\nThe results of the preliminary study suggested that preci-\nsion is more relevant than recall when perceiving bound-aries. However, to fully explore this hypothesis, these twovalues had to be carefully manipulated. For this experi-ment, a set of boundaries was synthesized by setting spe-ciﬁc values for precision and recall while maintaining anear-constant F-measure. Moreover, we wanted to ensurethat the ﬁndings were robust across a larger pool of sub-jects. With these considerations in mind, the experimentwas designed to be both shorter in time and available online.\n3.2 Methodology\nWe selected ﬁve track excerpts from the Levy catalog by\nﬁnding the one-minute segments containing the highest num-ber of boundaries across the 60 songs of the dataset. Byhaving short excerpts instead of full songs, we could re-duce the duration of the entire experiment with negligibleeffect on the results—past studies have shown that bound-aries are usually perceived locally instead of globally [15].We decided to use only ﬁve excerpts with the highest num-ber of boundaries in order to maintain participants’ atten-tion as much as possible. For each track excerpt, we syn-thesized three different segmentations: ground truth bound-aries (GT) with an F-measure of 100%; high precision (HP)boundaries with a precision of 100% and recall of around65%; and high recall (HR) boundaries with a recall of 100%and precision of around 65%. The extra boundaries forthe HR version were randomly distributed (using a normaldistribution) across a 3 sec window between the largest re-gions between boundaries. For the HP version, the bound-aries that were most closely spaced were removed. Table2 presents F-measure, precision, and recall values for theﬁve tracks along with the average values across excerpts.Note the closeness between F-measure values for HP andHR.\nExperiment 1 Excerpt List\nSong Name HP HR\n(Artist) F P R F P R\nBlack & White.809 1 .68 .794 .658 1(Michael Jackson)\nDrive.785 1 .647 .791 .654 1(R.E.M.)\nIntergalactic.764 1 .619 .792 .656 1(Beastie Boys)\nSuds And Soda.782 1 .653 .8 .666 1(Deus)\nTubthumping.744 1 .593 .794 .659 1(Chumbawamba)\nAverage .777 1 .636 .794 .659 1\nTable 2. Excerpt list with their evaluations for experiment1. The F-measure of GT is 100% (not shown in the table).\nSubjects had to rate the “quality” of the boundaries for\neach version of the ﬁve tracks by choosing a discrete valuebetween 1 and 5 (lowest and highest ratings respectively).Although this might arguably bias the subjects towards theexisting boundaries only (reducing the inﬂuence of the miss-ing ones), it is unclear how to design a similar experimentthat would avoid this. Excerpts were presented in randomorder. Participants were asked to listen to all of the ex-cerpts before submitting the results. As in the preliminaryexperiment, auditory cues for the section boundaries wereadded to the original audio signal in the form of a salientsharp sound. For this experiment, no visual feedback wasprovided because the excerpts were short enough for lis-teners to retain a general perception of the accuracy of theboundaries. The entire experiment lasted around 15 min-utes (5 excerpts ⇥3 versions ⇥one minute per excerpt)\nand was available on line\n2as a web survey in order to fa-\ncilitate participation.\nAn announcement to various specialized mailing lists\nwas sent in order to recruit participants. As such, most sub-jects had a professional interest in music, and some wereeven familiar with the topic of musical structure analysis.A total number of 48 participants took part in the experi-ment; subjects had an average of 3.1 ±1.6 years of musi-\ncal training and 3.7 ±3.3 years of experience playing an\ninstrument.\n3.3 Results and Discussion\nBox plots of accuracy ratings across versions can be seen\nin Figure 2. These experimental results show that higheraccuracy ratings were assigned to GT followed by HP, andthen HR.\nFigure 2. Average ratings across excerpts for Experiment 1; GT\n= ground truth; HP = high precition; HR = high recall.\nA two-way, repeated-measures ANOV A was performed\non the accuracy ratings with type (ground truth, high pre-\ncision, high recall) and excerpt (the ﬁve songs) as factors.There were 48 data points in each Type ⇥Excerpt cate-\ngory. The main effects of type, F(2, 94) = 90.74, MSE\n= 1.10, p<. 001, and excerpt, F(4, 188) = 59.84, MSE\n= 0.88, p<. 001, were signiﬁcant. There was also an\ninteraction effect, F(6.17, 290.01) = 9.42, MSE = 0.74,\np<. 001(Greenhouse-Geisser corrected), indicating that\nrating proﬁles differed based on excerpt. Mean ratings bytype and excerpt are shown in Figure 3.\nLooking at the data for each excerpt, there was a clear\npattern showing that subjects preferred segmentations with\n2http://urinieto.com/NYU/BoundaryExperiment/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n267high precision over high recall (Figure 3). Post-hoc multi-\nple comparisons indicated that differences between meansof all three types were signiﬁcant. The only excerpt whereprecision was not rated more highly than recall was in Ex-cerpt 5 (Tubthumping), a difference that contributed pri-marily to the interaction. In this case, the excerpt containsa distinctive chorus where the lyrics “I get knocked down”keep repeating. This feature is likely the reason some sub-jects were led to interpret every instance of this refrain asa possible section beginning even though the harmony un-derneath follows a longer sectional pattern that is anno-tated in the ground truth. On the other hand, Excerpt 3 (In-tergalactic) obtained similar ratings for ground truth andhigh precision, likely due to the high number of differentsections and silences it contains. This can become prob-lematic when extra boundaries are added (therefore obtain-ing poor ratings for the high-recall version). Nevertheless,given the subjectivity of this task [2] and the multi-layer or-ganization of boundaries [10], it is not surprising that thistype of variability appears in the results.\nFigure 3. Means for excerpt and version of the results of Exper-\niment 1.\nThe results of this experiment show that precision is\nmore perceptually relevant than recall for the evaluation\nof boundaries, validating the preliminary ﬁndings (Section2.2) in a controlled scenario and with a much larger popu-lation of subjects. Nevertheless, the number of tracks em-ployed in this experiment was limited. As a follow-up, weexplored these ﬁndings using a larger dataset in Experi-ment 2.\n4. EXPERIMENT 2: CHOOSING BOUNDARIES\n4.1 Motivation\nThe results of Experiment 1 show the relative importance\nof precision over recall for a reduced dataset of ﬁve tracks.However, it remains to be seen whether the F-measure, pre-cision, and recall can predict a listener’s preference whenfaced with a real-world evaluation scenario (i.e., bound-aries not synthesized but estimated from algorithms). Howthis information can be used to redesign the metric to bemore perceptually relevant is another question. In Experi-ment 2, we used excerpts sampled from a larger set of mu-sic, boundaries computed with state-of-the-art algorithms(thus recreating a real-world evaluation `a laMIReX), and\nlimited the evaluation to pairwise preferences.4.2 Methodology\nThe analysis methods used to compute the boundaries in-\ncluded structural features (SF, [12]), convex non-negativematrix factorization (C-NMF, [7]), and shift-invariant prob-abilistic latent component analysis (SI-PLCA, [17]). Thesethree algorithms yield ideal results for our experimentaldesign since SF provides one of the best results reportedso far on boundaries recognition (high precision and highrecall) footnoteRecently challenged by Ordinal Linear Dis-criminant Analysis [6]. C-NMF tends to over segment(higher recall than precision), and SI-PLCA, depending onparameter choices, tends to under segment (higher preci-sion than recall).\nWe ran these three algorithms on a database of 463 songs\ncomposed of the conjunction of the TUT Beatles dataset,\n3\nthe Levy catalogue [5], and the freely available songs ofthe SALAMI dataset [13]. Once computed, we ﬁltered theresults based on the following criteria for each song: (1)at least two algorithm outputs have a similar F-measure(within a 5% threshold); (2) the F-measure of both algo-rithms must be at least 45%; (3) at least a 10% differencebetween the precision and recall values of the two selectedalgorithm outputs exists.\nWe found 41 out of 463 tracks that met the above cri-\nteria. We made a qualitative selection of these ﬁlteredtracks (there are many free tracks in the SALAMI datasetthat are live recordings with poor audio quality or simplyspeech), resulting in a ﬁnal set of 20 songs. The number ofthese carefully selected tracks is relatively low, but we ex-cept it to be representative enough to address our researchquestions. Given the two algorithmic outputs maximizingthe difference between precision and recall, two differentlysegmented versions were created for each track: high pre-cision (HP) and high recall (HR). Moreover, similar to Ex-periment 1, only one minute of audio from each track wasutilized, starting 15 seconds into the song.\nTable 3 shows average metrics across the 20 selected\ntracks. The F-measures are the same, while precision andrecall vary.\nBoundaries Version F P R\nHP .65 .82 .56\nHR .65 .54 .83\nTable 3. Average F-measure, precision, and recall values for the\ntwo versions of excerpts used in Experiment 2.\nAs in Experiment 1, the interface for Experiment 2 was\non line4to facilitate participation. Each participant was\npresented with ﬁve random excerpts selected from the setof 20. Instead of assessing the accuracy on a scale, listen-ers had to choose the version they found more accurate. Inorder uniformly distribute excerpts across total trials, se-lection of excerpts was constrained by giving more prior-ity to those excerpts with fewer collected responses. Weobtained an average of 5.75 results per excerpt. The twoversions were presented in random order, and subjects had\n3http://www.cs.tut.ﬁ/sgn/arg/paulus/beatles sections TUT.zip\n4http://cognition.smusic.nyu.edu/boundaryExperiment2/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n268to listen to the audio at least once before submitting the re-\nsults. Boundaries were marked with a salient sound like inthe prior experiments.\nA total 23 subjects, recruited from professional mailing\nlists, participated in the experiment. Participants had anaverage of 2.8 ±1.4 years of musical training and 3.2 ±\n2.9 years of experience playing an instrument.\n4.3 Results and Discussion\nWe performed binary logistic regression analysis [11] on\nthe results with the goal of understanding what speciﬁcvalues of the F-measure were actually useful in predictingsubject preference (the binary values representing the ver-sions picked by the listeners). Logistic regression enablesus to compute the following probability:\nP(Y|X\n1,...,X n)=ek+\u00001X1+...+\u0000 nXn\n1+ek+\u00001X1+...+\u0000 nXn(3)\nwhere Yis the dependent, binary variable, Xiare the pre-\ndictors, \u0000iare the weights for these predictors, and kis a\nconstant value. Parameters \u0000iandkare learned through\nthe process of training the regressor. In our case, Ytells us\nwhether a certain excerpt was chosen or not according tothe following predictors: the F-measure (X\n1), the signed\ndifference between precision and recall (X 2), and the ab-\nsolute difference between precision and recall (X 3).\nSince 23 subjects took part in the experiment and there\nwere ﬁve different tracks with two versions per excerpt, wehad a total of 23⇥5⇥2 = 230 observations as input to\nthe regression with the parameters deﬁned above. We ranthe Hosmer & Lemeshow test [4] in order to understandthe predictive ability of our input data. If this test is not\nstatistically signiﬁcant (p> 0.05), we know that logistic\nregression can indeed help us predict Y. In our case, we\nobtain a value of p=.763(\u00002=4.946, with 8 degrees of\nfreedom) which tells us that the data for this type of analy-sis ﬁts well, and that the regressor has predictive power.\nThe analysis of the results of the learned model is shown\nin Table 4. As expected, the F-measure is not able to pre-dict the selected version (p =.992), providing clear evi-\ndence that the metric is inexpressive and perceptually irrel-evant for the evaluation of segmentation algorithms. Fur-thermore, we can see that P\u0000Rcan predict the results in a\nstatistically signiﬁcant manner (p =.000), while the abso-\nlute difference |P\u0000R|, though better than the F-measure,\nhas low predictive power (p =.482). This clearly illus-\ntrates the asymmetrical relationship between P and R: it isnot sufﬁcient that P and R are different, but the sign mat-ters: P has to be higher than R.\nBased on this experiment we can claim that, for these\nset of tracks, (1) the F-measure does not sufﬁciently char-acterize the perception of boundaries, (2) precision is clearlymore important than recall, and (3) there might be a bet-ter parameterization of the F-measure that encodes relativeimportance. We attempt to address this last point in thenext section.Logistic Regression Analysis of Experiment 2\nPredictor \u0000 S.E.\u0000 Wald’s \u00002df p e\u0000\nF-measure -.012 1.155 .000 1 .992 .988\nP\u0000R 2.268 .471 23.226 1 .000 1.023\n|P\u0000R| -.669 .951 .495 1 .482 .512\nk .190 .838 .051 1 .821 1.209\nTable 4. Analysis of Experiment 2 data using logistic re-gression. According to these results, P\u0000Rcan predict the\nversion of the excerpt that subjects will choose.\n5. ENHANCING THE F-MEASURE\nBased on our experiments, we have empirical evidence thathigh precision is perceptually more relevant than high re-call for the evaluation of segmentation algorithms. We canthen leverage these ﬁndings to obtain a more expressiveand perceptually informative version of the F-measure forbenchmarking estimated boundaries.\nThe F-measure is, in fact, a special case of the F\n↵-\nmeasure:\nF↵=( 1+ ↵2)P·R\n↵2P+R(4)\nwhere ↵=1, resulting in P and R having the same weight.\nHowever, it is clear from the equation that we should im-pose↵< 1in order to give more importance to Pand\nmake the F-measure more perceptually relevant. Note thatan algorithm that outputs fewer boundaries does not nec-essarily increase its F\n↵-measure, since the fewer predicted\nboundaries could still be incorrect. Regardless, the ques-tion remains: how is the value of ↵determined?\nA possible method to answer this question is to sweep\n↵from 0 to 1 using a step size of 0.05 and perform logistic\nregression analysis at each step using the F\n↵-measure as\nthe only predictor (X 1=F↵,n=1). The p-value of the\nF↵-measure predicting subject preference in Experiment 2\nacross all ↵is shown in Figure 4.\nFigure 4. Statistical signiﬁcance of the F ↵-measure predicting\nthe perceptual preference of a given evaluation for ↵2[0,1]\nIt is important to note that the data from Experiment 2\nis limited as it does not include information at the limitsof the difference between precision and recall. As a re-sult, our model predicts that decreases of ↵always lead\nto highest predictive power. Naturally, this is undesirablesince we will eventually remove all inﬂuence from recallin the measure and favor the trivial solutions discussed at\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n269the beginning of this paper. At some point, as P\u0000Rin-\ncreases, we expect subject preference to decrease, as pre-\nserving a minimum amount of recall becomes more impor-tant. Therefore, we could choose the ﬁrst value of ↵(0.58)\nfor which F\n↵-based predictions of subject preference be-\ncome accurate at the statistically signiﬁcant level of 0.01.\nWe can re-run the evaluation of Experiments 1 and 2\nusing the F 0.58-measure (i.e. ↵=0.58) to illustrate that it\nbehaves as expected. For Experiment 1, we obtain 83.3%for HP and 72.1% for HR (instead of 77.7% and 79.4% re-spectively). For Experiment 2, the values of HP and HRbecome 71.8% and 58.9% respectively, whereas they wereboth 65.0% originally. This shows how the new approxi-mated measure is well coordinated with the preferences ofthe subjects from Experiments 1 and 2, therefore makingthis evaluation of section boundaries more expressive andperceptually relevant.\nThis speciﬁc ↵value is highly dependent on the em-\npirical data, and we are aware of the limitations of usingreduced data sets as compared to the real world—in otherwords, we are likely overﬁtting to our data. Nonetheless,based on our ﬁndings, there must be a value of ↵< 1that\nbetter represents the relative importance of precision andrecall. Future work, utilizing larger datasets and a greaternumber of participants, should focus on understanding theupper limit of the difference between precision and recallin order to ﬁnd the speciﬁc inﬂection point at which higherprecision is not perceptually relevant anymore.\n6. CONCLUSIONS\nWe presented a series of experiments concluding that pre-cision is perceived as more relevant than recall when eval-uating boundaries in music. The results of the two mainexperiments discussed here are available on line.\n5More-\nover, we have noted the shortcomings of the current F-measure when evaluating results in a perceptually mean-ingful way. By using the general form of the F-measure,we can obtain more relevant results when precision is em-phasized over recall (↵< 1). Further steps should be\ntaken in order to determine a more speciﬁc and general-izable value of ↵.\n7. ACKNOWLEDGMENTS\nThis work was partially funded by Fundaci ´on Caja Madrid\nand by the National Science Foundation, under grant IIS-0844654.\n8. REFERENCES\n[1] G. Boutard, S. Goldszmidt, and G. Peeters. Browsing Inside\na Music Track, The Experimentation Case Study. In Proc.\nof the Workshop of Learning the Semantics of Audio Signals,\npages 87–94, 2006.\n[2] M. J. Bruderer, M. F. Mckinney, and A. Kohlrausch. The Per-\nception of Structural Boundaries in Melody Lines of WesternPopular Music. MusicæScientiæ, 13(2):273–313, 2009.\n5http://www.urinieto.com/NYU/ISMIR14-BoundariesExperiment.zip[3] C. Cannam, C. Landone, M. Sandler, and J. P. Bello. The\nSonic Visualiser: A Visualisation Platform for Semantic De-scriptors from Musical Signals. In Proc. of the 7th Inter-\nnational Conference on Music Information Retrieval, pages324–327, Victoria, BC, Canada, 2006.\n[4] D. W. Hosmer and S. Lemeshow. Applied Logistic Regres-\nsion. John Wiley & Sons, 2004.\n[5] M. Levy and M. Sandler. Structural Segmentation of Musical\nAudio by Constrained Clustering. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 16(2):318–326, Feb.2008.\n[6] B. McFee and D. P. W. Ellis. Learning to Segment Songs with\nOrdinal Linear Discriminant Analysis. In Proc. of the 39th\nIEEE International Conference on Acoustics Speech and Sig-nal Processing, Florence, Italy, 2014.\n[7] O. Nieto and T. Jehan. Convex Non-Negative Matrix Fac-\ntorization For Automatic Music Structure Identiﬁcation. InProc. of the 38th IEEE International Conference on Acous-tics Speech and Signal Processing, pages 236–240, Vancou-ver, Canada, 2013.\n[8] B. S. Ong and P. Herrera. Semantic Segmentation of Music\nAudio Contents. In Proc. 32nd of the International Computer\nMusic Conference, Barcelona, Spain, 2005.\n[9] J. Paulus, M. M ¨uller, and A. Klapuri. Audio-Based Music\nStructure Analysis. In Proc of the 11th International Soci-\nety of Music Information Retrieval, pages 625–636, Utrecht,Netherlands, 2010.\n[10] G. Peeters and E. Deruty. Is Music Structure Annotation\nMulti-Dimensional? A Proposal for Robust Local MusicAnnotation . In Proc. of the 3rd International Worskhop on\nLearning Semantics of Audio Signals, pages 75–90, Graz,Austria, 2009.\n[11] C.-Y. J. Peng, K. L. Lee, and G. M. Ingersoll. An Introduction\nto Logistic Regression Analysis and Reporting. The Journal\nof Educational Research, 96(1):3–14, Sept. 2002.\n[12] J. Serr `a, M. M ¨uller, P. Grosche, and J. L. Arcos. Unsuper-\nvised Music Structure Annotation by Time Series StructureFeatures and Segment Similarity. IEEE Transactions on Mul-\ntimedia, Special Issue on Music Data Mining, 2014.\n[13] J. B. Smith, J. A. Burgoyne, I. Fujinaga, D. De Roure, and\nJ. S. Downie. Design and Creation of a Large-Scale Databaseof Structural Annotations. In Proc. of the 12th International\nSociety of Music Information Retrieval, pages 555–560, Mi-ami, FL, USA, 2011.\n[14] J. B. L. Smith. A Comparison And Evaluation Of Approaches\nTo The Automatic Formal Analysis Of Musical Audio. Mas-ter’s thesis, McGill University, 2010.\n[15] B. Tillmann and E. Bigand. Global Context Effect in Nor-\nmal and Scrambled Musical Sequences. Journal of Exper-\nimental Psychology: Human Perception and Performance,27(5):1185–1196, 2001.\n[16] D. Turnbull, G. Lanckriet, E. Pampalk, and M. Goto. A Su-\npervised Approach for Detecting Boundaries in Music UsingDifference Features and Boosting. In Proc. of the 5th Interna-\ntional Society of Music Information Retrieval, pages 42–49,Vienna, Austria, 2007.\n[17] R. Weiss and J. P. Bello. Unsupervised Discovery of Tem-\nporal Structure in Music. IEEE Journal of Selected Topics in\nSignal Processing, 5(6):1240–1251, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n270"
    },
    {
        "title": "Transfer Learning by Supervised Pre-training for Audio-based Music Classification.",
        "author": [
            "Aäron van den Oord",
            "Sander Dieleman",
            "Benjamin Schrauwen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415890",
        "url": "https://doi.org/10.5281/zenodo.1415890",
        "ee": "https://zenodo.org/records/1415890/files/OordDS14.pdf",
        "abstract": "Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, be- cause the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classifica- tion tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning ap- proach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We eval- uate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets.",
        "zenodo_id": 1415890,
        "dblp_key": "conf/ismir/OordDS14",
        "keywords": [
            "publicly available",
            "increasing need",
            "digital distribution",
            "efficient cataloging",
            "easily browsable",
            "deep learning",
            "feature learning",
            "music information retrieval",
            "large amounts of training data",
            "supervised pre-training"
        ],
        "content": "TRANSFER LEARNING BY SUPERVISED PRE-TRAINING FOR\nAUDIO-BASED MUSIC CLASSIFICATION\nA¨aron van den Oord, Sander Dieleman, Benjamin Schrauwen\nElectronics and Information Systems department, Ghent University\nfaaron.vandenoord, sander.dieleman, benjamin.schrauweng@ugent.be\nABSTRACT\nVery few large-scale music research datasets are publicly\navailable. There is an increasing need for such datasets, be-\ncause the shift from physical to digital distribution in the\nmusic industry has given the listener access to a large body\nof music, which needs to be cataloged efﬁciently and be\neasily browsable. Additionally, deep learning and feature\nlearning techniques are becoming increasingly popular for\nmusic information retrieval applications, and they typically\nrequire large amounts of training data to work well. In this\npaper, we propose to exploit an available large-scale music\ndataset, the Million Song Dataset (MSD), for classiﬁca-\ntion tasks on other datasets, by reusing models trained on\nthe MSD for feature extraction. This transfer learning ap-\nproach, which we refer to as supervised pre-training, was\npreviously shown to be very effective for computer vision\nproblems. We show that features learned from MSD audio\nfragments in a supervised manner, using tag labels and user\nlistening data, consistently outperform features learned in\nan unsupervised manner in this setting, provided that the\nlearned feature extractor is of limited complexity. We eval-\nuate our approach on the GTZAN, 1517-Artists, Unique\nand Magnatagatune datasets.\n1. INTRODUCTION\nWith the exception of the Million Song Dataset (MSD) [3],\npublic large-scale music datasets that are suitable for re-\nsearch are hard to come by. Among other reasons, this\nis because unwieldy ﬁle sizes and copyright regulations\ncomplicate the distribution of large collections of music\ndata. This is unfortunate, because some recent develop-\nments have created an increased need for such datasets.\nOn the one hand, content-based music information re-\ntrieval (MIR) is ﬁnding more applications in the music in-\ndustry, in a large part due to the shift from physical to\ndigital distribution. Nowadays, online music stores and\nstreaming services make a large body of music readily\navailable to the listener, and content-based MIR can fa-\nc\rA¨aron van den Oord, Sander Dieleman, Benjamin\nSchrauwen.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: A¨aron van den Oord, Sander Diele-\nman, Benjamin Schrauwen. “Transfer learning by supervised pre-training\nfor audio-based music classiﬁcation”, 15th International Society for Mu-\nsic Information Retrieval Conference, 2014.cilitate cataloging and browsing these music collections,\nfor example by automatically tagging songs with relevant\nterms, or by creating personalized recommendations for\nthe user. To develop and evaluate such applications, large\nmusic datasets are needed.\nOn the other hand, the recent rise in popularity of\nfeature learning and deep learning techniques in the do-\nmains of computer vision, speech recognition and natu-\nral language processing has caught the attention of MIR\nresearchers, who have adopted them as well [13]. Large\namounts of training data are typically required for a fea-\nture learning approach to work well.\nAlthough the initial draw of deep learning was the abil-\nity to incorporate large amounts of unlabeled data into the\nmodels using an unsupervised learning stage called unsu-\npervised pre-training [1], modern industrial applications of\ndeep learning typically rely on purely supervised learning\ninstead. This means that large amounts of labeled data are\nrequired, and labels are usually quite costly to obtain.\nGiven the scarcity of large-scale music datasets, it\nmakes sense to try and leverage whatever data is available,\neven if it is not immediately usable for the task we are try-\ning to perform. We can use a transfer learning approach to\nachieve this: given a target task to be performed on a small\ndataset, we can train a model for a different, but related\ntask on another dataset, and then use the learned knowl-\nedge to obtain a better model for the target task.\nIn image classiﬁcation, impressive results have recently\nbeen attained on various datasets by reusing deep convo-\nlutional neural networks trained on a large-scale classiﬁ-\ncation problem: ImageNet classiﬁcation. The ImageNet\ndataset contains roughly 1.2 million images, divided into\n1,000 categories [5]. The trained network can be used to\nextract features from a new dataset, by computing the ac-\ntivations of the topmost hidden layer and using them as\nfeatures. Two recently released software packages, Over-\nFeat andDeCAF, provide the parameters of a number of\npre-trained networks, which can be used to extract the cor-\nresponding features [7,20]. This approach has been shown\nto be very competitive for various computer vision tasks,\nsometimes surpassing the state of the art [18, 26].\nInspired by this approach, we propose to train feature\nextractors on the MSD for two large-scale audio-based\nsong classiﬁcation tasks, and leverage them to perform\nother classiﬁcation tasks on different datasets. We show\nthat this approach to transfer learning, which we will refer\nto as supervised pre-training following Girshick et al. [9],\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n29consistently improves results on the tasks we evaluated.\nThe rest of this paper is structured as follows: in Section\n2, we give an overview of the datasets we used for training\nand evaluation. In Section 3 we describe our proposed ap-\nproach and brieﬂy discuss how it relates to transfer learn-\ning. Our experiments and results are described in Section\n4. Finally, we draw conclusions and point out some direc-\ntions for future work in Section 5.\n2. DATASETS\nThe Million Song Dataset [3] is a collection of meta-\ndata and audio features for one million songs. Although\nraw audio data is not provided, we were able to obtain\n30 second preview clips for almost all songs from 7digi-\ntal.com. A number of other datasets that are linked to the\nMSD are also available. These include the Taste Proﬁle\nSubset [15], which contains listening data from 1 million\nusers for a subset of about 380,000 songs in the form of\nplay counts, and the last.fm dataset, which provides tags\nfor about 500,000 songs. We will use the combination of\nthese three datasets to deﬁne two source tasks: user listen-\ning preference prediction and tag prediction from audio.\nWe will evaluate four target tasks on different datasets:\n\u000fgenre classiﬁcation on the GTZAN dataset [22], which\ncontains 1,000 audio clips, divided into 10 genres.\n\u000fgenre classiﬁcation on the Unique dataset [21], which\ncontains 3,115 audio clips, divided into 14 genres.\n\u000fgenre classiﬁcation on the 1517-artists dataset [21],\nwhich contains 3,180 full songs, divided into 19 genres.\n\u000ftag prediction on the Magnatagatune dataset [14],\nwhich contains 25,863 audio clips, annotated with 188\ntags.\n3. PROPOSED APPROACH\n3.1 Overview\nThere are many ways to transfer learned knowledge be-\ntween tasks. Pan and Yang [17] give a comprehensive\noverview of the transfer learning framework, and of the\nrelevant literature. In their taxonomy, our proposed super-\nvised pre-training approach is a form of inductive transfer\nlearning with feature representation transfer : target labels\nare available for both the source and target tasks, and the\nfeature representation learned on the source task is reused\nfor the target task.\nIn the context of MIR, transfer learning has been ex-\nplored by embedding audio features and labels from vari-\nous datasets into a shared latent space with linear transfor-\nmations [10]. The same shared embedding approach has\npreviously been applied to MIR tasks in a multi-task learn-\ning setting [24]. We refer to these papers for a discussion\nof some other work in this area of research.\nFor supervised pre-training, it is essential to have a\nsource task that requires a very rich feature representation,\nso as to ensure that the information content of this repre-\nsentation is likely to be useful for other tasks. For com-\nputer vision problems, ImageNet classiﬁcation is one suchtask, since it involves a wide range of categories. In this pa-\nper, we will evaluate two source tasks using the MSD: tag\nprediction and user listening preference prediction from\naudio. The goal of tag prediction is to automatically de-\ntermine which of a large set of tags are associated with a\ngiven song. User listening preference prediction involves\npredicting whether users have listened to a given song or\nnot.\nBoth tasks differ from typical classiﬁcation tasks in a\nnumber of ways:\n\u000fTag prediction is a multi-label classiﬁcation task: each\nsong can be associated with multiple tags, so the classes\nare not disjoint. The same goes for user listening pref-\nerence prediction, where we attempt to predict for each\nuser whether they have listened to a song. The listening\npreferences of different users are not disjoint either, and\none song is typically listened to by multiple users.\n\u000fThere are large numbers of tags and users; orders of\nmagnitude larger than the 1,000 categories of ImageNet.\n\u000fThe data is weakly labeled: if a song is not associated\nwith a particular tag, the tag may still be applicable to\nthe song. In the same way, if a user has not listened to\na song, they may still enjoy it (i.e. it would be a good\nrecommendation). In other words, some positive labels\nare missing.\n\u000fThe labels are redundant: a lot of tags are correlated, or\nhave the same meaning. For example, songs tagged with\ndisco are more likely to also be tagged with 80’s. The\nsame goes for users: many of them have similar listening\npreferences.\n\u000fThe labels are very sparse: most tags only apply to a\nsmall subset of songs, and most users have only listened\nto a small subset of songs.\nWe will tackle some of the problems created by these\ndifferences by ﬁrst performing dimensionality reduction in\nthe label space using weighted matrix factorization (WMF,\nsee Section 3.2), and then training models to predict the\nreduced label representations instead.\nWe will ﬁrst use the spherical K-means algorithm (see\nSection 3.3) to learn low-level features from audio spectro-\ngrams, and use them as input for the supervised models that\nwe will train to perform the source tasks. Feature learning\nusing K-means is very fast compared to other unsupervised\nfeature learning methods, and yields competitive results. It\nhas recently gained popularity for content-based MIR ap-\nplications [6, 19, 25].\nIn summary, our workﬂow will be as follows: we will\nﬁrst learn low-level features from audio spectrograms, and\napply dimensionality reduction to the target labels. We will\ntrain supervised models to predict the reduced label rep-\nresentations from the extracted low-level audio features.\nThese models can then be used to perform the source tasks.\nNext, we will use the trained models to extract higher-level\nfeatures from other datasets, and use those features to train\nshallow classiﬁers for different but related target tasks. We\nwill compare the higher-level features obtained from dif-\nferent model architectures and different source tasks by\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n30source task target task\naudio signals\nspectrogram extraction\nspectrograms\nK-means feature\nlearning + extraction\nlow-level features\ntrain linear regression or\nMLPlabels (tags or user\nlistening preferences)\ndimensionality reduction\nby WMF\nfactorsaudio signals\nspectrogram extraction\nspectrograms\nK-means feature\nextraction\nlow-level features\nfeature extraction using\nsource task modelhigh-level features\n(predicted factors)shallow classiﬁer\n(L2-SVM)labels\nFigure 1: Schematic overview of the workﬂow we will use for our supervised pre-training approach. Dashed arrows\nindicate transfer of the learned feature extractors from the source task to the target task.\nevaluating their performance on these target tasks. This\nworkﬂow is visualized in Figure 1. The key learning steps\nare detailed in the following subsections.\n3.2 Dimensionality reduction in the label space\nTo deal with large numbers of overlapping labels, we ﬁrst\nconsider the matrix of labels for all examples, and perform\nweighted matrix factorization (WMF) on it [12]. Given a\nbinarym\u0002n-matrixA(mexamples and nlabels), WMF\nwill ﬁnd anm\u0002f-matrixUand ann\u0002f-matrixV, so that\nA\u0019UVT. The hyperparameter fcontrols the rank of the\nresulting approximation. This approximation is found by\noptimizing the following weighted objective function:\nJ(U;V ) =C\u000e(A\u0000UVT)2+\u0015(jjUjj2\nF+jjVjj2\nF);\nwhereCis am\u0002nconﬁdence matrix, \u000erepresents el-\nementwise multiplication, the squaring is elementwise as\nwell, and\u0015is a regularization parameter. If the conﬁdence\nvalues inCare chosen to be 1for all zeroes in A, an efﬁ-\ncient alternating least squares (ALS) method exists to op-\ntimizeJ(U;V ), provided that Ais sparse. For details, we\nrefer to Hu et al. [12].\nAfter optimization, each row of Ucan be interpreted as\na reduced representation of the mlabels associated with\nthe corresponding example, which captures the latent fac-\ntors that affect its classiﬁcation. We can then train a model\nto predict these ffactors instead, which is much easier\nthan predicting mlabels directly (typically f\u001cm). We\nhave previously used a similar approach to do content-\nbased music recommendation with a convolutional neural\nnetwork [23]. In that paper, we showed that these factors\ncapture a lot of relevant information and can also be used\nfor tag prediction. We use the same settings and hyperpa-\nrameter values for the WMF algorithm in this work.\nOur choice for WMF over other dimensionality reduc-\ntion methods, such as PCA, is motivated by the particularstructure of the label space described earlier. WMF al-\nlows for the sparsity and redundancy of the labels to be\nexploited, and we can take into account that the data is\nweakly labeled by choosing Cso that positive signals are\nweighed more than negative signals.\nThe original label matrix for the tag prediction task\nhas 173,203 columns, since we included all tags from the\nlast.fm dataset that occur more than once. The matrix for\nthe user listening preference prediction task has 1,129,318\ncolumns, corresponding to all users in the Taste Proﬁle\nSubset. By applying WMF, we obtain reduced represen-\ntations with 400factors for both tasks. These factors will\nbe treated as ground truth target values in the supervised\nlearning phase.\n3.3 Unsupervised learning of low-level features\nWe learn a low-level feature representation from spectro-\ngrams in an unsupervised manner, to use as input for the\nsupervised pre-training stage. First, we extract log-scaled\nmel-spectrograms from single channel audio signals, with\na window size of 1024 samples and a hop size of 512. Con-\nversion to the mel scale reduces the number of frequency\ncomponents to 128. We then use the spherical K-means\nalgorithm (as suggested by Coates et al. [4]) to learn 2048\nbases from randomly sampled PCA-whitened windows of\n4 consecutive spectrogram frames. This is similar to the\nfeature learning approach proposed by Dieleman et al. [6].\nTo extract features, we divide the spectrograms into\noverlapping windows of 4 frames, and compute the dot\nproduct of each base with each PCA-whitened window.\nWe then aggregate the feature values across time by com-\nputing the maximal value for each base across groups of\nconsecutive windows corresponding to about 2 seconds of\naudio. Finally, we take the mean of these values across the\nentire audio clip to arrive at a 2048-dimensional feature\nrepresentation for each example. This two-stage temporal\npooling approach turns out to work well in practice.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n313.4 Supervised learning of high-level features\nFor both source tasks, we train three different model archi-\ntectures to predict the reduced label representations from\nthe low-level audio features: a linear regression model,\na multi-layer perceptron (MLP) with a hidden layer with\n1000 rectiﬁed linear units (ReLUs) [16], and an MLP\nwith two such hidden layers. The MLPs are trained us-\ning stochastic gradient descent (SGD) to mimize the mean\nsquared error (MSE) of the predictions, and dropout reg-\nularization [11]. The training procedure was implemented\nusing Theano [2].\nWe trained all these models on a subset of the MSD,\nconsisting of 373,855 tracks for which we were able to ob-\ntain audio samples, and for which listening data is avail-\nable in the Taste Proﬁle Subset. We used 308,443 tracks\nfor training, 18,684 for validation and 46,728 for testing.\nFor the tag prediction task, the set of tracks was further\nreduced to 253,588 tracks, including only those for which\ntag data is available in the last.fm dataset. For this task, we\nused 209,218 tracks for training, 12,763 for validation and\n31,607 for testing.\nThe trained models can be used to extract high-level\nfeatures simply by computing predictions for the reduced\nlabel representations and using those as features, yielding\nfeature vectors with 400values. For the MLPs, we can al-\nternatively compute the activations of the topmost hidden\nlayer, yielding feature vectors with 1000 values instead.\nThe latter approach is closer to the original interpretation\nof supervised pre-training as described in Section 1, but\nsince the trained models attempt to predict latent factor\nrepresentations, the former approach is viable as well. We\nwill compare both.\nTo evaluate the models on the source tasks, we compute\nthe predicted factors U0and obtain predictions for each\nclass by computing A0=U0VT. This matrix can then\nbe used to compute performance metrics.\n3.5 Evaluation of the features for target tasks\nTo evaluate the high-level features for the target tasks out-\nlined in Section 2, we train linear L2-norm support vector\nmachines (L2-SVMs) for all tasks with liblinear [8], us-\ning the features as input. Although using more powerful\nclassiﬁers could probably improve our results, the use of a\nshallow, linear classiﬁer helps to assess the quality of the\ninput features.\n4. EXPERIMENTS AND RESULTS\n4.1 Source tasks\nTo assess whether the models trained for the source tasks\nare able to make sensible predictions, we evaluate them by\ncomputing the normalized mean squared error (NMSE)1\nof the latent factor predictions, as well as the area under the\nROC curve (AUC) and the mean average precision (mAP)\n1The NMSE is the MSE divided by the variance of the target values\nacross the dataset.User listening preference prediction\nModel NMSE AUC mAP\nLinear regression 0.986 0.750 0.0076\nMLP (1 hidden layer) 0.971 0.760 0.0149\nMLP (2 hidden layers) 0.961 0.746 0.0186\nTag prediction\nModel NMSE AUC mAP\nLinear regression 0.965 0.823 0.0099\nMLP (1 hidden layer) 0.939 0.841 0.0179\nMLP (2 hidden layers) 0.924 0.837 0.0179\nTable 1: Results for the source tasks. For all three models,\nwe report the normalized mean squared error (NMSE) on\nthe validation set, and the area under the ROC curve (AUC)\nand the mean average precision (mAP) on a separate test\nset.\nof the class predictions2. They are reported in Table 1.\nNote that the latter two metrics are computed on a separate\ntest set, but the former is computed on the validation set\nthat we also used to optimize the hyperparameters for the\ndimensionality reduction of the labels. This is because the\nground truth latent factors, which are necessary to compute\nthe NMSE, are not available for the test set.\nIt is clear that using a more complex model (i.e. an\nMLP) results in better predictions of the latent factors in\nthe least-squares sense, as indicated by the lower NMSE\nvalues. However, when using the AUC metric, this does\nnot always seem to translate into better performance for the\ntask at hand: MLPs with only a single hidden layer perform\nbest for both tasks in this respect. The mAP metric seems\nto follow the NMSE on the validation set more closely.\nAlthough the NMSE values are relatively high, the class\nprediction metrics indicate that the predicted factors still\nyield acceptable results for the source tasks. In our prelimi-\nnary experiments we also observed that using fewer factors\ntends to result in lower NMSE values. In other words, as\nwe add more factors, they become less predictable. This\nimplies that the most important latent factors extracted\nfrom the labels are also the most predictable from audio.\n4.2 Target tasks\nWe report the L2-SVM classiﬁcation performance of the\ndifferent feature sets across all target tasks in Figure 2. For\nthe GTZAN, Unique and 1517-Artists datasets, we report\nthe average cross-validation classiﬁcation accuracy across\n10 folds. Error bars indicate the standard deviations across\nfolds. We optimize the SVM regularization parameter us-\ning nested cross-validation with 5 folds. Magnatagatune\ncomes divided into 16 parts; we use the ﬁrst 11 for training\nand the next 2 for validation. After hyperparameter opti-\nmization, we retrain the SVMs on the ﬁrst 13 parts, and\nthe last 3 are used for testing. We report the AUC aver-\n2The class predictions are obtained by multiplying the factor predic-\ntions with the matrix VT, as explained in the previous section.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n32listening data tag data0.550.600.650.700.750.800.850.900.95Accuracy(a) GTZAN\nlistening data tag data0.600.650.700.750.800.85Accuracylow-level features\nlinear regression\nMLP (1)\nMLP (2)\nMLP hiddens (1)\nMLP hiddens (2) (b) Unique\nlistening data tag data0.250.300.350.400.450.500.55Accuracy\n(c) 1517-Artists\nlistening data tag data0.760.780.800.820.840.860.880.90AUC (d) Magnatagatune (top 50 tags)\nlistening data tag data0.760.780.800.820.840.860.880.90AUC (e) Magnatagatune (all 188 tags)\nFigure 2: Target task performance of the different feature sets. The dashed line represents the performance of the low-level\nfeatures. From left to right, the ﬁve bars in the bar groups represent high-level features extracted with linear regression,\nan MLP with 1 hidden layer, an MLP with 2 hidden layers, the hidden layer of a 1-layer MLP, and the topmost hidden\nlayer of a 2-layer MLP respectively. Error bars for the ﬁrst three classiﬁcation tasks indicate the standard deviation across\ncross-validation folds. For Magnatagatune, no error bars are given because no cross-validation was performed.\naged across tags for the 50 most frequently occuring tags\n(Figure 2d), and for all 188 tags (Figure 2e).\nThe single bar on the left of each graph shows the per-\nformance achieved when training an L2-SVM directly on\nthe low-level features learned using spherical K-means.\nThe two groups of ﬁve bars show the performance of the\nhigh-level features trained in a supervised manner for the\nuser listening preference prediction task and the tag pre-\ndiction task respectively.\nAcross all tasks, using the high-level features results in\nimproved performance over the low-level features. This\neffect is especially pronounced for Magnatagatune, when\npredicting all 188 tags from the high-level features learned\non the tag prediction source task. This makes sense, as\nsome of the Magnatagatune tags are quite rare, and features\nlearned on this closely related source task must contain at\nleast some relevant information for these tags.\nComparing the performance of different source task\nmodels for user listening preference prediction, model\ncomplexity seems to play a big role. Across all datasets,\nfeatures learned with linear regression perform much better\nthan MLPs, despite the fact that the MLPs perform better\nfor the source task. Clearly the MLPs are able to achieve a\nbetter ﬁt for the source task, but in the context of transfer\nlearning, this is actually a form of overﬁtting, as the fea-\ntures generalize less well to the target tasks – they are too\nspecialized for the source task. This effect is not observed\nwhen the source task is tag prediction, because this task is\nmuch more closely related to the target tasks. As a result, a\nbetter ﬁt for the source task is more likely to result in better\ngeneralization across tasks.For MLPs, there is a limited difference in performance\nbetween using the predictions or the topmost hidden layer\nactivations as features. Sometimes the latter approach\nworks a bit better, presumably because the feature vectors\nare larger (1000 values instead of 400) and sparser.\nOn GTZAN, we are able to achieve a classiﬁcation ac-\ncuracy of 0:882\u00060:024 using the high-level features ob-\ntained from a linear regression model for the tag predic-\ntion task, which is competitive with the state of the art. If\nwe use the low-level features directly, we achieve an ac-\ncuracy of 0:851\u00060:034. This is particularly interesting\nbecause the L2-SVM classiﬁer is linear, and the features\nobtained from the linear regression model are essentially\nlinear combinations of the low-level features.\n5. CONCLUSION AND FUTURE WORK\nWe have proposed a method to perform supervised fea-\nture learning on the Million Song Dataset (MSD), by train-\ning models for large-scale tag prediction and user listening\npreference prediction. We have shown that features learned\nin this fashion work well for other audio classiﬁcation tasks\non different datasets, consistently outperforming a purely\nunsupervised feature learning approach.\nThis transfer learning approach works particularly well\nwhen the source task is tag prediction, i.e. when the source\ntask and the target task are closely related. Acceptable re-\nsults are also obtained when the source task is user listen-\ning preference prediction, although it is important to re-\nstrict the complexity of the model in this case. Otherwise,\nthe features become too specialized for the source task,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n33which hampers generalization to other tasks and datasets.\nIn future work, we would like to investigate whether we\ncan achieve transfer from more complex models trained\non the user listening preference prediction task, and other\ntasks that are less closely related to the target tasks. Since\na lot of training data is available for this task, using more\npowerful models than linear regression to learn features is\ndesirable, especially considering the complexity of mod-\nels used for supervised pre-training in the computer vision\ndomain. This will require a different regularization strat-\negy that takes into account generalization to other tasks\nand datasets, and not just to new examples within the same\ntask, as it seems that these two do not always correlate. We\nwill also look into whether using different dimensionality\nreduction techniques instead of WMF can lead to represen-\ntations that enable better transfer to new tasks.\n6. REFERENCES\n[1] Yoshua Bengio. Learning deep architectures for AI. Technical\nreport, Dept. IRO, Universit ´e de Montreal, 2007.\n[2] James Bergstra, Olivier Breuleux, Fr ´ed´eric Bastien, Pascal\nLamblin, Razvan Pascanu, Guillaume Desjardins, Joseph\nTurian, David Warde-Farley, and Yoshua Bengio. Theano: a\nCPU and GPU math expression compiler. In Proceedings of\nthe Python for Scientiﬁc Computing Conference (SciPy), June\n2010.\n[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,\nand Paul Lamere. The million song dataset. In Proceedings\nof the 11th International Conference on Music Information\nRetrieval (ISMIR), 2011.\n[4] Adam Coates and Andrew Y . Ng. Learning feature represen-\ntations with k-means. Neural Networks: Tricks of the Trade,\nReloaded, 2012.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern Recognition, 2009.\nCVPR 2009. IEEE Conference on, pages 248–255. IEEE,\n2009.\n[6] Sander Dieleman and Benjamin Schrauwen. Multiscale ap-\nproaches to music audio feature learning. In Proceedings of\nthe 14th International Conference on Music Information Re-\ntrieval (ISMIR), 2013.\n[7] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,\nNing Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep\nconvolutional activation feature for generic visual recogni-\ntion. CoRR, abs/1310.1531, 2013.\n[8] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui\nWang, and Chih-Jen Lin. LIBLINEAR: A library for large\nlinear classiﬁcation. Journal of Machine Learning Research,\n9:1871–1874, 2008.\n[9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. CoRR, abs/1311.2524, 2013.\n[10] Philippe Hamel, Matthew EP Davies, Kazuyoshi Yoshii, and\nMasataka Goto. Transfer learning in MIR: sharing learned la-\ntent representations for music audio classiﬁcation and simi-\nlarity. In ISMIR 2013, 2013.[11] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by prevent-\ning co-adaptation of feature detectors. Technical report, Uni-\nversity of Toronto, 2012.\n[12] Yifan Hu, Yehuda Koren, and Chris V olinsky. Collaborative\nﬁltering for implicit feedback datasets. In Proceedings of the\n2008 Eighth IEEE International Conference on Data Mining,\n2008.\n[13] Eric J. Humphrey, Juan P. Bello, and Yann LeCun. Moving\nbeyond feature design: Deep architectures and automatic fea-\nture learning in music informatics. In Proceedings of the 13th\nInternational Conference on Music Information Retrieval (IS-\nMIR), 2012.\n[14] Edith Law and Luis von Ahn. Input-agreement: a new mech-\nanism for collecting data using human computation games. In\nProceedings of the 27th international conference on Human\nfactors in computing systems, 2009.\n[15] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and\nGert R.G. Lanckriet. The million song dataset challenge. In\nProceedings of the 21st international conference companion\non World Wide Web, 2012.\n[16] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units im-\nprove restricted boltzmann machines. In Proceedings of the\n27th International Conference on Machine Learning (ICML-\n10), 2010.\n[17] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-\ning. Knowledge and Data Engineering, IEEE Transactions\non, 22(10):1345–1359, 2010.\n[18] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,\nand Stefan Carlsson. Cnn features off-the-shelf: an astound-\ning baseline for recognition. CoRR, abs/1403.6382, 2014.\n[19] Jan Schl ¨uter and Christian Osendorfer. Music Similarity Esti-\nmation with the Mean-Covariance Restricted Boltzmann Ma-\nchine. In Proceedings of the 10th International Conference\non Machine Learning and Applications (ICMLA), 2011.\n[20] Pierre Sermanet, David Eigen, Xiang Zhang, Micha ¨el Math-\nieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated\nrecognition, localization and detection using convolutional\nnetworks. CoRR, abs/1312.6229, 2013.\n[21] Klaus Seyerlehner, Gerhard Widmer, and Tim Pohle. Fusing\nblock-level features for music similarity estimation. In Proc.\nof the 13th Int. Conference on Digital Audio Effects (DAFx-\n10), pages 225–232, 2010.\n[22] George Tzanetakis and Perry Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Audio\nProcessing, 10:293–302, 2002.\n[23] A ¨aron van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommendation. In\nAdvances in Neural Information Processing Systems 26,\n2013.\n[24] Jason Weston, Samy Bengio, and Philippe Hamel. Large-\nscale music annotation and retrieval: Learning to rank in joint\nsemantic spaces. Journal of New Music Research, 2011.\n[25] J. W ¨ulﬁng and M. Riedmiller. Unsupervised learning of local\nfeatures for music classiﬁcation. In Proceedings of the 13th\nInternational Society for Music Information Retrieval Con-\nference (ISMIR), 2012.\n[26] Matthew D. Zeiler and Rob Fergus. Visualizing and un-\nderstanding convolutional networks. CoRR, abs/1311.2901,\n2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n34"
    },
    {
        "title": "Modeling Rhythm Similarity for Electronic Dance Music.",
        "author": [
            "Maria Panteli",
            "Niels Bogaards",
            "Aline K. Honingh"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416664",
        "url": "https://doi.org/10.5281/zenodo.1416664",
        "ee": "https://zenodo.org/records/1416664/files/PanteliBH14.pdf",
        "abstract": "A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop’, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between seg- ments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most no- tably: attack phase of onsets, periodicity of rhythmic el- ements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, af- ter which the similarity between segments can be calcu- lated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the over- all performance of the model with perceptual ratings of rhythm similarity.",
        "zenodo_id": 1416664,
        "dblp_key": "conf/ismir/PanteliBH14",
        "keywords": [
            "model",
            "rhythm similarity",
            "electronic dance music",
            "loop",
            "perceptual rhythmic streams",
            "onset detection",
            "downbeat detection",
            "feature vector",
            "listening experiment",
            "perceptual ratings"
        ],
        "content": "MODELING RHYTHM SIMILARITY FOR ELECTRONIC DANCE MUSIC\nMaria Panteli\nUniversity of Amsterdam,\nAmsterdam, Netherlands\nm.x.panteli@gmail.comNiels Bogaards\nElephantcandy,\nAmsterdam, Netherlands\nniels@elephantcandy.comAline Honingh\nUniversity of Amsterdam,\nAmsterdam, Netherlands\na.k.honingh@uva.nl\nABSTRACT\nA model for rhythm similarity in electronic dance music\n(EDM) is presented in this paper. Rhythm in EDM is builton the concept of a ‘loop’, a repeating sequence typicallyassociated with a four-measure percussive pattern. Thepresented model calculates rhythm similarity between seg-ments of EDM in the following steps. 1) Each segmentis split in different perceptual rhythmic streams. 2) Eachstream is characterized by a number of attributes, most no-tably: attack phase of onsets, periodicity of rhythmic el-ements, and metrical distribution. 3) These attributes arecombined into one feature vector for every segment, af-ter which the similarity between segments can be calcu-lated. The stages of stream splitting, onset detection anddownbeat detection have been evaluated individually, anda listening experiment was conducted to evaluate the over-all performance of the model with perceptual ratings ofrhythm similarity.\n1. INTRODUCTION\nMusic similarity has attracted research from multidisci-plinary domains including tasks of music information re-trieval and music perception and cognition. Especially forrhythm, studies exist on identifying and quantifying rhythmproperties [16, 18], as well as establishing rhythm similar-ity metrics [12]. In this paper, rhythm similarity is studiedwith a focus on Electronic Dance Music (EDM), a genrewith various and distinct rhythms [2].\nEDM is an umbrella term consisting of the ‘four on\nthe ﬂoor’ genres such as techno, house, trance, and the‘breakbeat-driven’ genres such as jungle, drum ‘n’ bass,breaks etc. In general, four on the ﬂoor genres are charac-terized by a four-beat steady bass-drum pattern whereasbreakbeat-driven exploit irregularity by emphasizing themetrically weak locations [2]. However, rhythm in EDMexhibits multiple types of subtle variations and embellish-ments. The goal of the present study is to develop a rhythmsimilarity model that captures these embellishments and al-lows for a ﬁne inter-song rhythm similarity.\nc\u0000Maria Panteli, Niels Bogaards, Aline Honingh.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Maria Panteli, Niels Bogaards, Aline\nHoningh. “Modeling rhythm similarity for electronic dance music”, 15thInternational Society for Music Information Retrieval Conference, 2014.Rhythm&in&Musical&&\nNotation&Attack&Positions&\nof&Rhythm&Most&Common&Instrumental&\nAssociations&\n&1/5/9/13 & Bass&drum &\n5/13& Snare&drum;&handclaps&\n3/7/11/15 &HiEhat&(open&or&closed);&also&\nsnare&drum&or&synth&“stabs”&\nAll& HiEhat&(closed) &\n&\nFigure 1: Example of a common (even) EDM rhythm [2].\nThe model focuses on content-based analysis of audio\nrecordings. A large and diverse literature deals with the\nchallenges of audio rhythm similarity. These include, a-mongst other, approaches to onset detection [1], tempo es-timation [9,25], rhythmic representations [15,24], and fea-ture extraction for automatic rhythmic pattern descriptionand genre classiﬁcation [5, 12, 20]. Speciﬁc to EDM, [4]study rhythmic and timbre features for automatic genreclassiﬁcation, and [6] investigate temporal and structuralfeatures for music generation.\nIn this paper, an algorithm for rhythm similarity based\non EDM characteristics and perceptual rhythm attributes ispresented. The methodology for extracting rhythmic ele-ments from an audio segment and a summary of the fea-tures extracted is provided. The steps of the algorithm areevaluated individually. Similarity predictions of the modelare compared to perceptual ratings and further considera-tions are discussed.\n2. METHODOLOGY\nStructural changes in an EDM track typically consist ofan evolution of timbre and rhythm as opposed to a verse-chorus division. Segmentation is ﬁrstly performed to splitthe signal into meaningful excerpts. The algorithm devel-oped in [21] is used, which segments the audio signal basedon timbre features (since timbre is important in EDM struc-ture [2]) and musical heuristics.\nEDM rhythm is expressed via the ‘loop’, a repeating\npattern associated with a particular (often percussive) in-strument or instruments [2]. Rhythm information can beextracted by evaluating characteristics of the loop: First,the rhythmic pattern is often presented as a combination ofinstrument sounds (eg. Figure 1), thus exhibiting a certain‘rhythm polyphony’ [3]. To analyze this, the signal is splitinto the so-called rhythmic streams. Then, to describe theunderlying rhythm, features are extracted for each streambased on three attributes: a) The attack phase of the on-sets is considered to describe if the pattern is performed on\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n537feature extraction\nstream # 2\nsegmentation\nfeature \nvector\nsimilarity\n metrical \ndistribution\nrhythmic \nstreams\ndetection\nperiodicity\nattack\ncharacterization\nonset \ndetectionfeature extraction\nmetrical \ndistribution\nstream # 1\nstream # 3\nFigure 2: Overview of methodology.\npercussive or non-percussive instruments. Although this\nis typically viewed as a timbre attribute, the percussive-ness of a sound is expected to inﬂuence the perceptionof rhythm [16]. b) The repetition of rhythmic sequencesof the pattern are described by evaluating characteristicsof different levels of onsets’ periodicity. c) The metricalstructure of the pattern is characterized via features ex-tracted from the metrical proﬁle [24] of onsets. Based onthe above, a feature vector is extracted for each segmentand is used to measure rhythm similarity. Inter-segmentsimilarity is evaluated with perceptual ratings collected viaa speciﬁcally designed experiment. An overview of themethodology is shown in Figure 2 and details for each stepare provided in the sections below. Part of the algorithm isimplemented using the MIRToolbox [17].\n2.1 Rhythmic Streams\nSeveral instruments contribute to the rhythmic pattern of\nan EDM track. Most typical examples include combina-tions of bass drum, snare and hi-hat (eg. Figure 1). Thisis mainly a functional rather than a strictly instrumental di-vision, and in EDM one ﬁnds various instrument soundsto take the role of bass, snare and hi-hat. In describingrhythm, it is essential to distinguish between these sourcessince each contributes differently to rhythm perception [11].\nFollowing this, [15, 24] describe rhythmic patterns of\nlatin dance music in two preﬁxed frequency bands (low andhigh frequencies), and [9] represents drum patterns as twocomponents, the bass and snare drum pattern, calculatedvia non-negative matrix factorization of the spectrogram.In [20], rhythmic events are split based on their perceivedloudness and brightness, where the latter is deﬁned as afunction of the spectral centroid.\nIn the current study, rhythmic streams are extracted with\nrespect to the frequency domain and loudness pattern. Inparticular, the Short Time Fourier Transform of the sig-nal is computed and logarithmic magnitude spectra are as-signed to bark bands, resulting into a total of 24 bands fora44.1kHz sampling rate. Synchronous masking is mod-\neled using the spreading function of [23], and temporalmasking is modeled with a smoothing window of 50ms.\nThis representation is hereafter referred to as loudness en-velope and denoted by L\nbfor bark bands b=1,..., 24.A\nself-similarity matrix is computed from this 24-band rep-resentation indicating the bands that exhibit similar loud-ness pattern. The novelty approach of [8] is applied tothe24⇥24similarity matrix to detect adjacent bands that\nshould be grouped to the same rhythmic stream. The peaklocations Pof the novelty curve deﬁne the number of the\nbark band that marks the beginning of a new stream, i.e., ifP={p\ni2{1,..., 24}|i=1,...,I }for total number of\npeaks I, then stream Siconsists of bark bands bgiven by,\nSi=⇢{b|b2[pi,pi+1\u00001]}fori=1,...,I \u00001\n{b|b2[pI,24]} fori=I.\n(1)\nAn upper limit of 6streams is considered based on the ap-\nproach of [22] that uses a total of 6bands for onset detec-\ntion and [14] that suggests a total of three or four bands formeter analysis.\nThe notion of rhythmic stream here is similar to the no-\ntion of ‘accent band’ in [14] with the difference that eachrhythmic stream is formed on a variable number of adja-cent bark bands. Detecting a rhythmic stream does notnecessarily imply separating the instruments, since if twoinstruments play the same rhythm they should be groupedto the same rhythmic stream. The proposed approach doesnot distinguish instruments that lie in the same bark band.The advantage is that the number of streams and the fre-quency range for each stream do not need to be predeter-mined but are rather estimated from the spectral represen-tation of each song. This beneﬁts the analysis of electronicdance music by not imposing any constraints on the possi-ble instrument sounds that contribute to the characteristicrhythmic pattern.\n2.1.1 Onset Detection\nTo extract onset candidates, the loudness envelope per bark\nband and its derivative are normalized and summed withmore weight on loudness than its derivative, i.e.,\nO\nb(n)=( 1 \u0000\u0000)Nb(n)+\u0000N0\nb(n) (2)\nwhere Nbis the normalized loudness envelope Lb,N0\nbthe\nnormalized derivative of Lb,n=1,...,N the frame num-\nber for a total of Nframes, and \u0000<0.5 the weighting fac-\ntor. This is similar to the approach described by Equation3in [14] with reduced \u0000, and is computed prior summation\nto the different streams as suggested in [14,22]. Onsets aredetected via peak extraction within each stream, where the(rhythmic) content of stream iis deﬁned as\nR\ni=⌃ b2SiOb (3)\nwith Sias in Equation 1 and Obas in Equation 2. This\nonset detection approach incorporates similar methodolog-ical concepts with the positively evaluated algorithms forthe task of audio onset detection [1] in MIREX 2012, andtempo estimation [14] in the review of [25].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n538!\n16 18 20 22 24 26 285101520Bark−Spectrum\ntime axis (in s.)Bark bands\nTime !\nBark!bands !\n(a) Bark-band spectrogram.!\n5 10 15 2020\n15\n10\n 5\nBark!bands !\nBark!bands !\n(b) Self-similarity matrix.!\n24681012141618202224\n10 15 20\nNovelty Value\nBark BandsNovelty Curve\nStream!1 !\nStream!3 !\nStream!2 !\nStream!4 !\nNovelty!value !\nBark!bands !\n(c) Novelty curve.\nFigure 3: Detection of rhyhmic streams using the novelty approach; ﬁrst a bark-band spectrogram is computed, then its\nself-similarity matrix, and then the novelty [7] is applied where the novelty peaks deﬁne the stream boundaries.\n2.2 Feature Extraction\nThe onsets in each stream represent the rhythmic elements\nof the signal. To model the underlying rhythm, featuresare extracted from each stream, based on three attributes,namely, characterization of attack, periodicity, and metri-cal distribution of onsets. These are combined to a featurevector that serves for measuring inter-segment similarity.The sections below describe the feature extraction processin detail.\n2.2.1 Attack Characterization\nTo distinguish between percussive and non-percussive pat-\nterns, features are extracted that characterize the attack pha-se of the onsets. In particular, the attack time and attackslope are considered, among other, essential in modelingthe perceived attack time [10]. The attack slope was alsoused in modeling pulse clarity [16]. In general, onsets frompercussive sounds have a short attack time and steep attackslope, whereas non-percussive sounds have longer attacktime and gradually increasing attack slope.\nFor all onsets in all streams, the attack time and at-\ntack slope is extracted and split in two clusters; the ‘slow’(non-percussive) and ‘fast’ (percussive) attack phase on-sets. Here, it is assumed that both percussive and non-percussive onsets can be present in a given segment, hencesplitting in two clusters is superior to, e.g., computing theaverage. The mean and standard deviation of the two clus-ters of the attack time and attack slope (a total of 8features)\nis output to the feature vector.\n2.2.2 Periodicity\nOne of the most characteristic style elements in the musical\nstructure of EDM is repetition; the loop, and consequentlythe rhythmic sequence(s), are repeating patterns. To ana-lyze this, the periodicity of the onset detection function perstream is computed via autocorrelation and summed acrossall streams. The maximum delay taken into account is pro-portional to the bar duration. This is calculated assuming asteady tempo and\n4\n4meter throughout the EDM track [2].\nThe tempo estimation algorithm of [21] is used.\nFrom the autocorrelation curve (cf. Figure 4), a total of\n5features are extracted:Lag duration of maximum autocorrelation: The lo-\ncation (in time) of the second highest peak (the ﬁrst beingat lag 0) of the autocorrelation curve normalized by the barduration. It measures whether the strongest periodicity oc-curs in every bar (i.e. feature value = 1), or every half bar(i.e. feature value = 0.5) etc.\nAmplitude of maximum autocorrelation: The am-\nplitude of the second highest peak of the autocorrelationcurve normalized by the amplitude of the peak at lag 0.It measures whether the pattern is repeated in exactly thesame way (i.e. feature value = 1) or somewhat in a similarway (i.e. feature value <1) etc.\nHarmonicity of peaks: This is the harmonicity as de-\nﬁned in [16] with adaptation to the reference lag l\n0cor-\nresponding to the beat duration and additional weightingof the harmonicity value by the total number of peaks ofthe autocorrelation curve. This feature measures whetherrhythmic periodicities occur in harmonic relation to thebeat (i.e. feature value = 1) or inharmonic (i.e. featurevalue = 0).\nFlatness: Measures whether the autocorrelation curve\nis smooth or spiky and is suitable for distinguishing be-tween periodic patterns (i.e. feature value = 0), and non-periodic (i.e. feature value = 1).\nEntropy: Another measure of the ‘peakiness’ of auto-\ncorrelation [16], suitable for distinguishing between ‘clear’repetitions (i.e. distribution with narrow peaks and hencefeature value close to 0) and unclear repetitions (i.e. widepeaks and hence feature value increased).\n2.2.3 Metrical Distribution\nTo model the metrical aspects of the rhythmic pattern, the\nmetrical proﬁle [24] is extracted. For this, the downbeatis detected as described in Section 2.2.4, onsets per streamare quantized assuming a\n4\n4meter and 16-th note resolu-\ntion [2], and the pattern is collapsed to a total of 4bars. The\nlatter is in agreement with the length of a musical phrasein EDM being usually in multiples of 4, i.e., 4-bar, 8-bar,\nor 16-bar phrase [2]. The metrical proﬁle of a given streamis thus presented as a vector of 64bins (4 bars⇥4beats\n⇥4sixteenth notes per beat) with real values ranging be-\ntween 0 (no onset) to 1 (maximum onset strength) as shownin Figure 5. For each rhythmic stream, a metrical pro-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5390 0.5 1 1.5 2 2.5−0.6−0.4−0.200.20.40.60.811.2\nLag (s)Normalized amplitude1 Beat 1 Bar\nFigure 4: Autocorrelation of onsets indicating high peri-\nodicities of 1 bar and 1 beat duration.\n!\n!\n!0102030\ntime (s)Stream 1051015\nStream 20102030Onset curve (Envelope) (half−wave rectified)\nStream 3\nBar 2 Bar 3 Bar 4 Bar 1\nDownbeat! Bar!1! ! Bar!2 !\n! \u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 ! \u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !\nStream!1! 1!0!0!0!1!0!0!0!1!0!0!0!1!0!0! 0!!1!0!0!0!1!0!0!0!1!0!0!0!1!0!0! 0!!\nStream! 2! 0!0!0!0!1!0!0!0!0!0!0!0!1!0!0! 0!!0!0!0!0!1!0!0!0!0!0!0!0!1!0!0! 0!!\nStream!3! 0!0!1!0!0!0!1!0!0!0!1!0!0! 0!1!0! !0!0!1!0!0!0!1!0!0!0!1!0!0! 0!1!0!\nStream!4! 1!1!1!1!1!1!1!1!1!1!1!1!1!1!1!1! 1!1!1!1!1!1!1!1!1!1!1!1!1!1!1!1!\n! Bar!1! ! Bar!2 ! Bar!3! ! Bar!4 !\n! \u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 ! \u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !\u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 ! \u0001!!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !!!!!!!!!!!!\u0001 !\nStream!3! 1!0!1!0!1!0!1!0!1!0!1!0!1!0!1!0! ! 1!0!1!0!1!0!1!0!1!0!1!0!1!0!1!0! !1!0!1!0!1!0!1!0!1!0!1!0!1!0!1!0! !1!0!1!0!1!0!1!0!1!0!1!0!1!0!1!0! !\nStream!2! 0!0!0!0! 1!0!0!0!0!0!0!0!1!0!0!0! ! 0!0!0!0!1!0!0!0!0!0!0!0!1!0!0!0! !0!0!0!0!1!0!0!0!0!0!0!0!1!0!0!0! !0!0!0!0!1!0!0!0!0!0!0!0!1!0!0!0! !\nStream!1! 1!0!0!0!0!0!0!0!1!0!0!0!0!0!0!0! ! 1!0!0!0!0!0!0!0!1!0!0!0!0!0!0!0! !1!0!0!0!0!0!0!0!1!0!0!0!0!0!0!0! !1!0!0!0!0!0!0!0!1!0!0!0!0!0!0!0! !Figure 5: Metrical proﬁle of the rhythm in Figure 1 assum-\ning for simplicity a 2-bar length and constant amplitude.\nﬁle is computed and the following features are extracted.Features are computed per stream and averaged across allstreams.\nSyncopation: Measures the strength of the events lying\non the weak locations of the meter. The syncopation modelof [18] is used with adaptation to account for the amplitude(onset strength) of the syncopated note. Three measures ofsyncopation are considered that apply hierarchical weightswith, respectively, sixteenth note, eighth note, and quarternote resolution.\nSymmetry: Denotes the ratio of the number of onsets\nin the second half of the pattern that appear in exactly thesame position in the ﬁrst half of the pattern [6].\nDensity: Is the ratio of the number of onsets over the\npossible total number of onsets of the pattern (in this case64).\nFullness: Measures the onsets’ strength of the pattern.\nIt describes the ratio of the sum of onsets’ strength over themaximum strength multiplied by the possible total numberof onsets (in this case 64).\nCentre of Gravity: Denotes the position in the pattern\nwhere the most and strongest onsets occur (i.e., indicateswhether most onsets appear at the beginning or at the endof the pattern etc.).\nAside from these features, the metrical proﬁle (cf. Fig-\nure 5) is also added to the ﬁnal feature vector. This wasfound to improve results in [24]. In the current approach,the metrical proﬁle is provided per stream, restricted to atotal of 4streams, and output in the ﬁnal feature vector in\norder of low to high frequency content streams.\n2.2.4 Downbeat Detection\nThe downbeat detection algorithm uses information from\nthe metrical structure and musical heuristics. Two assump-tions are made:\nAssumption 1: Strong beats of the meter are more likely\nto be emphasized across all rhythmic streams.\nAssumption 2: The downbeat is often introduced by\nan instrument in the low frequencies, i.e. a bass or a kickdrum [2, 13].\nConsidering the above, the onsets per stream are quan-\ntized assuming a\n4\n4meter, 16-th note resolution, and a set of\ndownbeat candidates (in this case the onsets that lie withinone bar length counting from the beginning of the seg-ment). For each downbeat candidate, hierarchical weights[18] that emphasize the strong beats of the meter as indi-cated by Assumption 1, are applied to the quantized pat-terns. Note, there is one pattern for each rhythmic stream.The patterns are then summed by applying more weight tothe pattern of the low-frequency stream as indicated by As-sumption 2. Finally, the candidate whose quantized patternwas weighted most, is chosen as the downbeat.\n3. EVALUATION\nOne of the greatest challenges of music similarity evalu-ation is the deﬁnition of a ground truth. In some cases,objective evaluation is possible, where a ground truth is de-ﬁned on a quantiﬁable criterion, i.e., rhythms from a partic-ular genre are similar [5]. In other cases, music similarityis considered to be inﬂuenced by the perception of the lis-tener and hence subjective evaluation is more suitable [19].Objective evaluation in the current study is not preferablesince different rhythms do not necessarily conform to dif-ferent genres or subgenres\n1. Therefore a subjective eval-\nuation is used where predictions of rhythm similarity arecompared to perceptual ratings collected via a listening ex-periment (cf. Section 3.4). Details of the evaluation ofrhythmic stream, onset, and downbeat detection are pro-vided in Sections 3.1 - 3.3. A subset of the annotationsused in the evaluation of the latter is available online\n2.\n3.1 Rhythmic Streams Evaluation\nThe number of streams is evaluated with perceptual anno-\ntations. For this, a subset of 120songs from a total of 60\nartists (2 songs per artist) from a variety of EDM genres\nand subgenres was selected. For each song, segmentationwas applied using the algorithm of [21] and a characteristicsegment was selected. Four subjects were asked to evalu-ate the number of rhythmic streams they perceive in eachsegment, choosing between 1to6, where rhythmic stream\nwas deﬁned as a stream of unique rhythm.\nFor106of the 120segments, the subjects’ responses’\nstandard deviation was signiﬁcantly small. The estimatednumber of rhythmic streams matched the mean of the sub-ject’s response distribution with an accuracy of 93%.\n1Although some rhythmic patterns are characteristic to an EDM genre\nor subgenre, it is not generally true that these are unique and invariant.\n2https://staff.fnwi.uva.nl/a.k.honingh/rhythm_\nsimilarity.html\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5403.2 Onset Detection Evaluation\nOnset detection is evaluated with a set of 25 MIDI and\ncorresponding audio excerpts, speciﬁcally created for thispurpose. In this approach, onsets are detected per stream,therefore onset annotations should also be provided perstream. For a number of different EDM rhythms, MIDIﬁles were created with the constraint that each MIDI in-strument performs a unique rhythmic pattern therefore rep-resents a unique stream, and were converted to audio.\nThe onsets estimated from the audio were compared to\nthe annotations of the MIDI ﬁle using the evaluation mea-sures of the MIREX Onset Detection task\n3. For this, no\nstream alignment is performed but rather onsets from allstreams are grouped to a single set. For 25excerpts, an\nF-measure of 85%, presicion of 85%, and recall of 86%\nare obtained with a tolerance window of 50ms. Inaccura-\ncies in onset detection are due (on average) to doubled thanmerged onsets, because usually more streams (and hencemore onsets) are detected.\n3.3 Downbeat Detection Evaluation\nTo evaluate the downbeat the subset of 120segments de-\nscribed in Section 3.1 was used. For each segment the\nannotated downbeat was compared to the estimated onewith a tolerance window of 50ms. An accuracy of 51%\nwas achieved. Downbeat detection was also evaluated atthe beat-level, i.e., estimating whether the downbeat cor-responds to one of the four beats of the meter (instead ofoff-beat positions). This gave an accuracy of 59%, mean-\ning that in the other cases the downbeat was detected on theoff-beat positions. For some EDM tracks it was observedthat high degree of periodicity compensates for a wronglyestimated downbeat. The overall results of the similaritypredictions of the model (Section 3.4) indicate only a mi-nor increase when the correct (annotated) downbeats aretaken into account. It is hence concluded that the down-beat detection algorithm does not have great inﬂuence onthe current results of the model.\n3.4 Mapping Model Predictions to Perceptual Ratings\nof Similarity\nThe model’s predictions were evaluated with perceptual\nratings of rhythm similarity collected via a listening ex-periment. Pairwise comparisons of a small set of segmentsrepresenting various rhythmic patterns of EDM were pre-sented. Subjects were asked to rate the perceived rhythmsimilarity, choosing from a four point scale, and report alsothe conﬁdence of their rating. From a preliminary collec-tion of experiment data, 28pairs (representing a total of 18\nunique music segments) were selected for further analysis.These were rated from a total of 28participants, with mean\nage27years old and standard deviation 7.3. The 50% of\nthe participants received formal musical training, 64% was\nfamiliar with EDM and 46% had experience as EDM mu-\nsician/producer. The selected pairs were rated between 3to\n5times, with all participants reporting conﬁdence in their\n3www.MIREX.orgr p features\n-0.17 0.22 attack characterization\n0.48 0.00 periodicity\n0.33 0.01 metrical distribution excl. metrical proﬁle0.69 0.00 metrical distribution incl. metrical proﬁle0.70 0.00 all\nTable 1: Pearson’s correlation randp-values between the\nmodel’s predictions and perceptual ratings of rhythm sim-ilarity for different sets of features.\nrating, and all ratings being consistent, i.e., rated similarity\nwas not deviating more than 1point scale. The mean of the\nratings was utilized as the ground truth rating per pair.\nFor each pair, similarity can be calculated via applying\na distance metric to the feature vectors of the underlyingsegments. In this preliminary analysis, the cosine distancewas considered. Pearson’s correlation was used to comparethe annotated and predicted ratings of similarity. This wasapplied for different sets of features as indicated in Table 1.\nA maximum correlation of 0.7was achieved when all\nfeatures were presented. The non-zero correlation hypoth-esis was not rejected (p> 0.05) for the attack character-\nization features indicating non-signiﬁcant correlation withthe (current set of) perceptual ratings. The periodicity fea-tures are correlated with r=0.48, showing a strong link\nwith perceptual rhythm similarity. The metrical distribu-tion features indicate a correlation increase of 0.36when\nthe metrical proﬁle is included in the feature vector. Thisis in agreement with the ﬁnding of [24].\nAs an alternative evaluation measure, the model’s pre-\ndictions and perceptual ratings were transformed to a bi-nary scale (i.e., 0being dissimilar and 1being similar)\nand their output was compared. The model’s predictionsmatched the perceptual ratings with an accuracy of 64%.\nHence the model matches the perceptual similarity ratingsat not only relative (i.e., Pearson’s correlation) but also ab-solute way, when a binary scale similarity is considered.\n4. DISCUSSION AND FUTURE WORK\nIn the evaluation of the model, the following considera-tions are made. High correlation of 0.69was achieved\nwhen the metrical proﬁle, output per stream, was added tothe feature vector. An alternative experiment tested the cor-relation when considering the metrical proﬁle as a whole,i.e., as a sum across all streams. This gave a correlation ofonly 0.59indicating the importance of stream separation\nand hence the advantage of the model to account for this.\nA maximum correlation of 0.7was reported, taking into\naccount the downbeat detection being 51% of the cases\ncorrect. Although regularity in EDM sometimes compen-sates for this, model’s predictions can be improved with amore robust downbeat detection.\nFeatures of periodicity (Section 2.2.2) and metrical dis-\ntribution (Section 2.2.3) were extracted assuming a\n4\n4me-\nter, and 16-th note resolution throughout the segment. This\nis generally true for EDM, but exceptions do exist [2]. The\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n541assumptions could be relaxed to analyze EDM with ternary\ndivisions or no4\n4meter, or expanded to other music styles\nwith similar structure.\nThe correlation reported in Section 3.4 is computed from\na preliminary set of experiment data. More ratings are cur-rently collected and a regression analysis and tuning of themodel is considered in future work.\n5. CONCLUSION\nA model of rhythm similarity for Electronic Dance Musichas been presented. The model extracts rhythmic featuresfrom audio segments and computes similarity by compar-ing their feature vectors. A method for rhythmic streamdetection is proposed that estimates the number and rangeof frequency bands from the spectral representation of eachsegment rather than a ﬁxed division. Features are extractedfrom each stream, an approach shown to beneﬁt the anal-ysis. Similarity predictions of the model match perceptualratings with a correlation of 0.7. Future work will ﬁne-tunepredictions based on a perceptual rhythm similarity model.\n6. REFERENCES\n[1] S. B ¨ock, A. Arzt, K. Florian, and S. Markus. On-\nline real-time onset detection with recurrent neural net-works. In International Conference on Digital Audio\nEffects, 2012.\n[2] M. J. Butler. Unlocking the Groove. Indiana University\nPress, Bloomington and Indianapolis, 2006.\n[3] E. Cambouropoulos. V oice and Stream: Perceptual and\nComputational Modeling of V oice Separation. Music\nPerception, 26(1):75–94, 2008.\n[4] D. Diakopoulos, O. Vallis, J. Hochenbaum, J. Murphy,\nand A. Kapur. 21st Century Electronica: MIR Tech-niques for Classiﬁcation and Performance. In ISMIR,\n2009.\n[5] S. Dixon, F. Gouyon, and G. Widmer. Towards Char-\nacterisation of Music via Rhythmic Patterns. In ISMIR,\n2004.\n[6] A. Eigenfeldt and P. Pasquier. Evolving Structures for\nElectronic Dance Music. In Genetic and Evolutionary\nComputation Conference, 2013.\n[7] J. Foote and S. Uchihashi. The beat spectrum: a new\napproach to rhythm analysis. In ICME, 2001.\n[8] J. T. Foote. Media segmentation using self-similarity\ndecomposition. In Electronic Imaging. International\nSociety for Optics and Photonics, 2003.\n[9] D. G ¨artner. Tempo estimation of urban music using\ntatum grid non-negative matrix factorization. In ISMIR,\n2013.\n[10] J. W. Gordon. The perceptual attack time of musical\ntones. The Journal of the Acoustical Society of Amer-\nica, 82(1):88–105, 1987.[11] T. D. Grifﬁths and J. D. Warren. What is an auditory\nobject? Nature Reviews Neuroscience, 5(11):887–892,\n2004.\n[12] C. Guastavino, F. G ´omez, G. Toussaint, F. Maran-\ndola, and E. G ´omez. Measuring Similarity between\nFlamenco Rhythmic Patterns. Journal of New Music\nResearch, 38(2):129–138, June 2009.\n[13] J. A. Hockman, M. E. P. Davies, and I. Fujinaga. One in\nthe Jungle: Downbeat Detection in Hardcore, Jungle,and Drum and Bass. In ISMIR, 2012.\n[14] A. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech and Language Processing,14(1):342–355, January 2006.\n[15] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern\nmodeling for beat and downbeat tracking in musicalaudio. In ISMIR, 2013.\n[16] O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari.\nMulti-feature Modeling of Pulse Clarity: Design, Vali-dation and Optimization. In ISMIR, 2008.\n[17] O. Lartillot and P. Toiviainen. A Matlab Toolbox for\nMusical Feature Extraction From Audio. In Interna-\ntional Conference on Digital Audio Effects, 2007.\n[18] H. C. Longuet-Higgins and C. S. Lee. The Rhyth-\nmic Interpretation of Monophonic Music. Music Per-\nception: An Interdisciplinary Journal, 1(4):424–441,1984.\n[19] A. Novello, M. M. F. McKinney, and A. Kohlrausch.\nPerceptual Evaluation of Inter-song Similarity in West-ern Popular Music. Journal of New Music Research,\n40(1):1–26, March 2011.\n[20] J. Paulus and A. Klapuri. Measuring the Similarity of\nRhythmic Patterns. In ISMIR, 2002.\n[21] B. Rocha, N. Bogaards, and A. Honingh. Segmentation\nand Timbre Similarity in Electronic Dance Music. InSound and Music Computing Conference, 2013.\n[22] E. D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. The Journal of the Acoustical Society\nof America, 103(1):588–601, January 1998.\n[23] M. R. Schroeder, B. S. Atal, and J. L. Hall. Optimizing\ndigital speech coders by exploiting masking propertiesof the human ear. The Journal of the Acoustical Society\nof America, pages 1647–1652, 1979.\n[24] L. M. Smith. Rhythmic similarity using metrical pro-\nﬁle matching. In International Computer Music Con-\nference, 2010.\n[25] J. R. Zapata and E. G ´omez. Comparative Evaluation\nand Combination of Audio Tempo Estimation Ap-proaches. In Audio Engineering Society Conference,\n2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n542"
    },
    {
        "title": "Improving Music Structure Segmentation using lag-priors.",
        "author": [
            "Geoffroy Peeters",
            "Victor Bisot"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414764",
        "url": "https://doi.org/10.5281/zenodo.1414764",
        "ee": "https://zenodo.org/records/1414764/files/PeetersB14.pdf",
        "abstract": "Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the sig- nal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. pro- posed in 2012 to cumulate these simultaneous values (us- ing a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test- sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large im- provement of the segmentation results.",
        "zenodo_id": 1414764,
        "dblp_key": "conf/ismir/PeetersB14",
        "keywords": [
            "repetitive content",
            "Self-Similarity-Matrix (SSM)",
            "sub-diagonals",
            "cumulate values",
            "constant-lag",
            "structure feature",
            "novelty curve",
            "MIREX segmentation evaluation measure",
            "large improvement",
            "combination approach"
        ],
        "content": "IMPROVING MUSIC STRUCTURE SEGMENTATION USING\nLAG-PRIORS\nGeoffroy Peeters\nSTMS IRCAM-CNRS-UPMC\ngeoffroy.peeters@ircam.frVictor Bisot\nSTMS IRCAM-CNRS-UPMC\nvictor.bisot@ircam.fr\nABSTRACT\nMethods for music structure discovery usually process a\nmusic track by ﬁrst detecting segments and then labeling\nthem. Depending on the assumptions made on the sig-\nnal content (repetition, homogeneity or novelty), different\nmethods are used for these two steps. In this paper, we deal\nwith the segmentation in the case of repetitive content. In\nthis ﬁeld, segments are usually identiﬁed by looking for\nsub-diagonals in a Self-Similarity-Matrix (SSM). In order\nto make this identiﬁcation more robust, Goto proposed in\n2003 to cumulate the values of the SSM over constant-lag\nand search only for segments in the SSM when this sum\nis large. Since the various repetitions of a segment start\nsimultaneously in a self-similarity-matrix, Serra et al. pro-\nposed in 2012 to cumulate these simultaneous values (us-\ning a so-called structure feature) to enhance the novelty of\nthe starting and ending time of a segment. In this work,\nwe propose to combine both approaches by using Goto\nmethod locally as a prior to the lag-dimensions of Serra\net al. structure features used to compute the novelty curve.\nThrough a large experiment on RWC and Isophonics test-\nsets and using MIREX segmentation evaluation measure,\nwe show that this simple combination allows a large im-\nprovement of the segmentation results.\n1. INTRODUCTION\nMusic structure segmentation aims at estimating the large-\nscale temporal entities that compose a music track (for ex-\nample the verse, chorus or bridge in popular music). This\nsegmentation has many applications such as browsing a\ntrack by parts, a ﬁrst step for music structure labeling or\naudio summary generation [15], music analysis, help for\nadvanced DJ-ing.\nThe method used to estimate the music structure seg-\nments (and/or labels) depends on the assumptions made\non the signal content. Two assumptions are commonly\nused [13] [14]. The ﬁrst assumption considers that the au-\ndio signal can be represented as a succession of segments\nwith homogeneous content inside each segment. This as-\nsumption is named “homogeneity assumption” and the es-\nc\rGeoffroy Peeters, Victor Bisot.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Geoffroy Peeters, Victor Bisot. “Im-\nproving Music Structure Segmentation using lag-priors”, 15th Interna-\ntional Society for Music Information Retrieval Conference, 2014.timation approach named “state approach”. It is closely re-\nlated to another assumption, named “novelty”, that consid-\ners that the transition between two distinct homogeneous\nsegments creates a large “novelty”. The second assump-\ntion considers that some segments in the audio signal are\nrepetitions of other ones. This assumption is named “rep-\netition assumption”. In this case the “repeated” segments\ncan be homogeneous or not. When they are not, the ap-\nproach is named “sequence approach”.\nIn this paper, we deal with the problem of estimat-\ning the segments (starting and ending times) in the case\nof repeated/ non-homogeneous segments (“sequence” ap-\nproach).\n1.1 Related works\nWorks related to music structure segmentation are numer-\nous. We refer the reader to [13] or [3] for a complete\noverview on the topic. We only review here the most im-\nportant works or the ones closely related our proposal.\nMethods relying on the homogeneity or novelty as-\nsumption. Because homogeneous segments form\n“blocks” in a time-time-Self-Similarity-Matrix (SSM) and\nbecause transitions from one homogeneous segment to the\nnext looks like a checkerboard kernel, Foote [5] proposes\nin 2000 to convolve the matrix with a 2D-checkerboard-\nkernel. The result of the convolution along the main di-\nagonal leads to large value at the transition times. Since,\nan assumption on the segment duration has to be made for\nthe kernel of Foote, Kaiser and Peeters [9] propose in 2013\nto use multiple-temporal-scale kernels. They also intro-\nduce two new kernels to represent transitions from homo-\ngeneous to non-homogeneous segments (and vice versa).\nOther approaches rely on information criteria (such as BIC,\nAkaike or GLR) applied to the sequence of audio fea-\ntures. Finally, labeling methods (such as k-means, hierar-\nchical agglomerative clustering of hidden-Markov-model)\nalso inherently allow performing time-segmentation.\nMethods relying on the repetition assumption. Be-\ncause repeated segments (when non-homogeneous) form\nsub-diagonals in a Self-Similarity Matrix (SSM), most\nmethods perform the segmentation by detecting these sub-\ndiagonals in the SSM.\nIf we denote by S(i;j) =S(ti;tj)i;j2[1;N ]the\ntime-time-SSM between the pairs of times tiandtj, the\ntime-lag-SSM [1] is deﬁned as L(i;l ) =L(ti;l=tj\u0000ti),\nsincetj\u0000ti\u00150the matrix is upper-diagonal. The lag-\nmatrix can be computed using L(i;l ) =S(i;j =i+l)\nwithi+l\u0014N.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n337In [6], Goto proposes to detect the repetitions in a time-\nlag-SSM using a two-step approach. He ﬁrst detects the\nvarious lags lkat which potential repetitions may occur.\nThis is done by observing that when a repetition (at the\nsame-speed) occurs, a vertical line (at constant lag) ex-\nists in the time-lag-SSM (see Figure 1). Therefore, the\nsum over the times of the time-lag-SSM for this speciﬁc\nlag will be large. He proposes to compute the function\nf(l) =P\nti2[0;N\u0000l]1\nN\u0000lL(ti;l). A peak in f(l)indicates\nthat repetitions exist at this speciﬁc lag. Then, for each de-\ntected peaks lk, the corresponding column of L(ti;lk)is\nanalyzed in order to ﬁnd the starting and ending times of\nthe segments.\nSerra et al. method [16] for music structure segmenta-\ntion also relies in the time-lag-SSM but works in the op-\nposite way. In order to compute the lower-diagonal part\nof the matrix (t j\u0000ti<0), They propose to apply cir-\ncular permutation. The resulting matrix is named circular-\ntime-lag-matrix (CTLM) and is computed using L\u0003(i;l) =\nS(i;k+ 1), fori;l2[1;N ]andk=i+l\u00002modulo N.\nThey then use the fact that the various repetitions of a same\nsegment start and end at the same times in the CTLM. They\ntherefore deﬁne a N-dimensional feature, named ”structure\nfeature”g(i), deﬁned as the row of the CTLM at ti. Start\nand end of the repetitions create large frame-to-frame vari-\nations of the structure feature. They therefore compute a\nnovelty curve deﬁned as the distance between successive\nstructure features g(i):c(i) =jjg(i+ 1)\u0000g(i)jj2(see\nFigure 1). Large values in this curve indicate starts or ends\ntimes of repetitions.\nTime t\nGoto: Serra:0\nNg(i)g(i+1)\ntilkLag lN0c(i)\nf(l)\nFigure 1. Illustration of Goto method [6] on a time-lag\nSelf-Similarity-Matrix (SSM) and Serra et al. method [16]\non a circular-time-lag-matrix (CTLM).\n1.2 Paper objective and organization\nIn this paper, we deal with the problem of estimat-\ning the segments (starting and ending times) in the case\nof repeated/ non-homogeneous segments (“sequence” ap-\nproach). We propose a simple, but very efﬁcient, method\nthat allows using Goto method as a prior lag-probabilityof segments in Serra et al. method. Indeed, Serra et al.\nmethod works efﬁciently when the “structure” feature g(i)\nis clean, i.e. contains large values when a segment crosses\ng(i)and is null otherwise. Since, this is rarely the case,\nwe propose to create a prior assumption f(l)on the di-\nmensions of g(i)that may contain segments. To create\nthis prior assumption, we use a modiﬁed version of Goto\nmethod applied locally in time to the CTLM (instead of to\nthe time-lag-SSM).\nOur proposed method for music structure segmentation\nis presented in part 2. We then evaluate it and compare its\nperformance to state-of-the-art algorithms in part 3 using\nthe RWC-Popular-Music and Isophonics/Beatles test-sets.\nDiscussions of the results and potential extensions are dis-\ncussed in part 4.\n2. PROPOSED METHOD\n2.1 Feature extraction\nIn order to represent the content of an audio signal, we\nuse the CENS (Chroma Energy distribution Normalized\nStatistics) features [12] extracted using the Chroma Tool-\nbox [11]. The CENS feature is a sort of quantized version\nof the chroma feature smoothed over time by convolution\nwith a long duration Hann window. The CENS features\nxa(ti)i2[1;N ]are 12-dimensional vector with a sam-\npling rate of 2 Hz. xa(ti)is in the range [0;1]. It should be\nnoted that these features are l2-normed1.\n2.2 Self-Similarity-Matrix\nFrom the sequence of CENS features we compute a time-\ntime Self-Similarity-Matrix (SSM) [4] S(i;j)using as\nsimilarity measure the scalar-product2between the feature\nvector at time tiandtj:S(i;j) =<xa(ti);xa(tj)>.\nIn order to highlight the diagonal-repetitions in the SSM\nwhile reducing the inﬂuence of noise values, we then apply\nthe following process.\n1. We apply a low-pass ﬁlter in the direction of the diag-\nonals and high-pass ﬁlter in the orthogonal direction. For\nthis, we use the kernel [\u00000:3; 1;\u00000:3] replicated 12 times\nto lead to a low-pass ﬁlter of duration 6 s.\n2. We apply a threshold \u001c2[0;1]to the resulting SSM.\n\u001cis chosen such as to keep only \f% of the values of the\nSSM. Values below \u001care set to a negative penalty-value \u000b.\nThe interval [\u001c;1]is then mapped to the interval [0;1].\n3. Finally, we apply a median ﬁlter over the diagonals of\nthe matrix. For each value S(i;j), we look in the backward\nand forward diagonals of \u000e-points duration each [(i\u0000\u000e;j\u0000\n\u000e):::(i;j):::(i+\u000e;j+\u000e)]. If more than 50% of these\npoints have a value of \u000b,S(i;j)is also set to \u000b.\nBy experiment, we found \f= 6% (percentage of values\nkept),\u000b=\u00002(lower values) and \u000e= 10 frames (interval\nduration3) to be good values.\n1P\na=[1;12]x2\na(ti) = 1\n2Since the vectors are l2-normed, this is equivalent to the use of a\ncosine-distance.\n3Since the sampling rate of xa(ti)is 2 Hz, this corresponds to a du-\nration of 5 s. The median ﬁlter is then applied on a window of 10 s total\nduration.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3382.3 Proposed method: introducing lag-prior\nAs mentioned before, Serra et al. method works efﬁciently\nwhen the “structure” feature g(i)is clean, i.e. contains\nlarge values when a segment crosses g(i)and is null other-\nwise. Unfortunately, this is rarely the case in practice.\nIf we model the structure feature g(i)as the true con-\ntribution of the segments ^g(i)and a background noise\n(modeled as a centered Gaussian noise) N\u0016=0;\u001b :g(i) =\n^g(i) +N\u0016=0;\u001b , one can easily shows that the expectation\nofc(i) =jjg(i+ 1)\u0000g(i)jj2is equal to\n\u000fK+ 2\u001b2for the starting/ending of K segments at ti\n\u000f2\u001b2otherwise.\nIf\u001b(the amount of background noise in the CTLM) is\nlarge, then it may be difﬁcult to discriminate between both\ncase for small K. In the opposite, the expectation of the\nvalues of Goto function f(l) =P\ntiL\u0003(ti;l)remains in-\ndependent of \u001bhence on the presence of background noise\n(in the case of a centered Gaussian noise).\nWe therefore propose to use f(l)as a prior on the lags,\ni.e. the dimensions of g(i). This will favor the discrimi-\nnation provided by c(i)(in Serra et al. approach, all the\nlags/dimensions of g(i)are considered equally).\nFor this, we consider, the circular time-lag (CMLT)\nL\u0003(t;l)as a joint probability distribution p(t;l).\nSerra et al. novelty curve c(i)can be expressed as\nc1(t) =Z\nl\f\f\f\f@\n@tp(t;l)\f\f\f\f2\ndl (1)\nIn our approach, we favor the lags at which segments\nare more likely. This is done using a prior p(l):\nc2(t) =Z\nlp(l)\f\f\f\f@\n@tp(t;l)\f\f\f\f2\ndl (2)\nIn order to compute the prior p(l)we compute f(l)\nas proposed by Goto but applied to the CMLT. In other\nwords, we compute, the marginal of p(t;l)overt:p(l) =Rt=N\nt=0p(t;l)dt.\nAs a variation of this method, we also propose to com-\npute the prior p(l)locally ont:pt(l) =R\u001c=t+\u0001\n\u001c=t\u0000\u0001p(\u001c;l )dt.\nThis leads to the novelty curve\nc3(t) =Z\nlpt(l)\f\f\f\f@\n@tp(t;l)\f\f\f\f2\ndl (3)\nBy experiment, we found \u0001 = 20 (corresponding to\n10 s), to be a good value.\n2.4 Illustrations\nIn Figure 2, we illustrate the computation of c1(t),c2(t)\nandc3(t)on a real signal (the track 19 from RWC Popular\nMusic).\nIn Figure 2 (A) we represent Serra et al. [16] method.\nOn the right of the time-lag-circular-matrix (CTLM), we\nrepresent the novelty curve c1(t)(red-curve) and super-\nimposed to it, the ground-truth segments (black dashed\nlines).(A) Computation of c1(t)\nLagTIme\n100 200 300 400 50050\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\n0 0.5 150\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\n(B) Computation of c2(t)\nTime\n100 200 300 400 50050\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\n100 200 300 400 50000.51Lag0 0.5 150\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\n(C) Computation of c3(t)\nTime\n100 200 300 400 50050\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\nLag100 200 300 400 500100\n200\n300\n400\n5000 0.5 150\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\nFigure 2. Illustration of the computation of c1(t),c2(t)\nandc3(t)on Track 19 from RWC Popular Music. See text\nof Section 2.4 for explanation.\nIn Figure 2 (B) we represent the computation of c2(t)\n(using a global lag-prior). Below the CTLM we repre-\nsent the global prior p(l)(blue curve) obtained using Goto\nmethod applied to the CMLT. On the right of the CTLM\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n339we represent c2(t)using this global lag-prior. Compared\nto the above c1(t), we see that c2(t)allows a larger dis-\ncrimination between times that correspond to ground-truth\nstarts and ends of segments and that do not.\nIn Figure 2 (C) we represent the computation of c3(t)\n(using a local lag-prior). Below the CTLM we represent\nthe local prior pt(l)in matrix form obtained using Goto\nmethod applied locally in time to the CMLT. On the right\nof the CTLM we represent c3(t)using this local lag-prior.\nCompared to the above c1(t)andc2(t), we see that c3(t)\nallows an even larger discrimination.\n2.5 Estimation of segments start and end times\nFinally, we estimate the starting and ending time of the\nrepetitions from the novelty curves c1(t),c2(t)orc3(t).\nThis is done using a peak picking process. c\u0003(t)is ﬁrst\nnormalized by min-max to the interval [0;1]. Only the val-\nues above 0.1 are considered. tiis considered as a peak if\ni= arg max jc\u0003(tj)withj2[i\u000010;i+ 10], i.e. if tiis\nthe maximum peak within a \u00065 s duration interval.\nThe ﬂowchart of our Music Structure Segmentation\nmethod is represented in the left part of Figure 3.\nSelf Similarity Matrix S(i,j) using cosine distanceAudioCENS feature xa(ti)Low-pass in t / High-pass in lCircular Time Lag Matrix p(t,l)Novelty c1,2,3(t)Peak-Picking of c1,2,3(t)Prior lag probability p(l) or pt(l)Segment start and end timeThresholdMedian FilterSerra et al. [17]Vector stackingSelf Similarity Matrix S(i,j)  using KNN rankingConvolution with bi-variate KernelCircular Time Lag Matrix p(t,l)Part 2.1\nPart 2.3Part 2.5Part 2.2Part 3.4\nFigure 3. Flowchart of the proposed Music Structure Seg-\nmentation method.\n3. EVALUATION\nIn this part, we evaluate the performances of our proposed\nmethod for estimating the start and end times of music\nstructure segments. We evaluate our algorithm using the\nthree methods described in part 2.3: – without lag-prior\nc1(t)(this is equivalent to the original Serra et al. algo-\nrithm although our features and the pre-processing of the\nCTLM differ from the ones of Serra et al.), – with global\nlag-priorc2(t), – with local lag-prior c3(t).\n3.1 Test-Sets\nIn order to allow comparison with previously published re-\nsults, we evaluate our algorithm on the following test-sets:RWC-Pop-A: is the RWC-Popular-Music test-set [8],\nwhich is a collection of 100 music tracks. The anno-\ntations into structures are provided by the AIST [7].\nRWC-Pop-B is the same test-set but with annotations pro-\nvided by IRISA [2]4.\nBeatles-B Is the Beatles test-set as part of the Isophonics\ntest-set, which is a collection of 180 music tracks\nfrom the Beatles. The annotations into structure are\nprovided by Isophonics [10].\n3.2 Evaluation measures\nTo evaluate the quality of our segmentation we use, as it is\nthe case in the MIREX (Music Information Retrieval Eval-\nuation eXchange) Structure Segmentation evaluation task,\nthe Recall (R), Precision (P) and F-Measure (F). We com-\npute those with a tolerance window of 3 and 0.5 s.\n3.3 Results obtained applying our lag-prior method to\nthe SSM as computed in part 2.2.\nIn Table 1 we indicate the results obtained for the various\nconﬁgurations and test-sets. We compare our results with\nthe ones published in Serra et al. [16] and to the best score\nobtained during the two last MIREX evaluation campaign:\nMIREX-2012 and MIREX-2013 on the same test-sets5 6.\nFor the three test-sets, and a 3 s tolerance window,\nthe use of our lag-prior allows a large increase of the F-\nmeasure:\nRWC-Pop-A: c1(t) : 66:0%,c2(t) : 72:9%,c3(t) : 76:9%.\nRWC-Pop-B: c1(t) : 67:3%, c2(t) : 72:6%,c3(t) : 78:2%.\nBeatles-B:c1(t) : 65:7%,c2(t) : 69:8%,c3(t) : 76:1%.\nFor the 0.5 s tolerance window, the F-measure also in-\ncrease but in smaller proportion.\nThe F-measure obtained by our algorithm is just be-\nlow the one of [16], but our features and pre-processing\nof the SSM much simpler. This means that applying our\nlag-priors to compute c2;3(t)on Serra et al. pre-processed\nmatrix could even lead to larger results. We discuss this in\nthe next part 3.4. We see that for the two RWC test-sets\nand a 3 s tolerance window, our algorithm achieves bet-\nter results than the best results obtained in MIREX (even\nthe ones obtained by Serra et al. – SMGA1). It should be\nnoted that the comparison for the Beatles-B test-set cannot\nbe made since MIREX use the whole Isophonics test-set\nand not only the Beatles sub-part.\nStatistical tests: For a @3s tolerance window, the dif-\nferences of results obtained with c3(t)andc2(t)are statis-\ntically signiﬁcant (at 5%) for all three test-sets. They are\nnot for a @0.5s tolerance window.\nDiscussion: For the RWC-Pop-B test-set, using c3(t)\ninstead ofc1(t)allows increasing the F@3s for 88/100\ntracks, for the Beatles-B for 144/180 tracks. In Figure 4,\n4These annotations are available at http://musicdata.\ngforge.inria.fr/structureAnnotation.html.\n5The MIREX test-set named ”M-2010 test-set Original” corresponds\nto RWC-Pop-A, ”M-2010 test-set Quaero” to RWC-Pop-B.\n6SMGA1 stands for [Joan Serra, Meinard Mueller, Peter Grosche,\nJosep Lluis Arcos]. FK2 stands for [Florian Kaiser and Geoffroy Peeters].\nRBH1 stands [Bruno Rocha, Niels Bogaards, Aline Honingh].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n340Table 1. Results of music structure segmentation using our lag-prior method applied to the SSM as computed in part 2.2.\nRWC-Pop-A\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nSerra et al. [16] 0.791 0.817 0.783\nMIREX-2012 (SMGA1 on M-2010 test-set Original) 0.7101 0.7411 0.7007 0.2359 0.2469 0.2319\nMIREX-2013 (FK2 on M-2010 test-set Original) 0.6574 0.8160 0.5599 0.3009 0.3745 0.2562\nc1(t)(without lag-prior) 0.660 0.700 0.648 0.315 0.338 0.308\nc2(t)(with global lag-prior) 0.729 0.739 0.737 0.349 0.354 0.353\nc3(t)(with local lag-prior) 0.769 0.770 0.78 0.386 0.392 0.390\nRWC-Pop-B\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nSerra et al. [16] 0.8 0.81 0.805\nMIREX-2012 (SMGA1 on M-2010 test-set Quaero) 0.7657 0.8158 0.7352 0.2678 0.2867 0.2558\nMIREX-2013 (RBH1 on M-2010 test-set Quaero) 0.6727 0.7003 0.6642 0.3749 0.3922 0.3682\nc1(t)(without lag-prior) 0.673 0.6745 0.689 0.238 0.223 0.263\nc2(t)(with global lag-prior) 0.726 0.704 0.766 0.250 0.231 0.281\nc3(t)(with local lag-prior) 0.782 0.782 0.816 0.281 0.264 0.31\nBeatles-B\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nSerra et al. [16] 0.774 0.76 0.807\nc1(t)(without lag-prior) 0.657 0.674 0.658 0.232 0.240 0.238\nc2(t)(with global lag-prior) 0.698 0.696 0.718 0.254 0.258 0.265\nc3(t)(with local lag-prior) 0.761 0.745 0.795 0.262 0.259 0.278\nwe illustrate one of the examples for which the use of c3(t)\ndecreases the results over c1(t). As before the discrimina-\ntion obtained using c3(t)(right sub-ﬁgure) is higher than\nthe ones obtained using c1(t)(left sub-ﬁgure). However,\nbecause of the use of the prior pt(l)which is computed on a\nlong duration window ([t \u0000\u0001;t+\u0001] represents 20 s), c3(t)\nfavors the detection of long-duration segments. In the ex-\nample of Figure 4, parts of the annotated segments (black\ndashed lines) are very short segments which therefore can-\nnot be detected with the chosen duration \u0001forpt(l).\n100 200 300 400 50050\n100\n150\n200\n250\n300\n350\n400\n450\n500\nLag100 200 300 400 500100\n200\n300\n400\n5000 0.5 150\n100\n150\n200\n250\n300\n350\n400\n450\n500\n0 0.5 150\n100\n150\n200\n250\n300\n350\n400\n450\n500TIme\nFigure 4. Illustration of a case for which c3(t)(right\nsub-ﬁgure) decrease the results over c1(t)(left sub-ﬁgure).\nF@3s(c 1(t)) = 0:93andF@3s(c 3(t)) = 0:67[Track 20\nform RWC-Pop-B].3.4 Results obtained applying our lag-prior method to\nthe SSM as computed by Serra et al. [16]\nIn order to assess the use of c2;3(t)as a generic process\nto improve the estimation of the segments on a SSM; we\nappliedc\u0003(t)to the SSM computed as proposed in [16] in-\nstead of the SSM proposed in part 2.2. The SSM will be\ncomputed using the CENS features instead of the HPCP\nused in [16]. For recall, in [16] the recent past of the fea-\ntures is taken into account by stacking the feature vectors\nof past frames (we used a value mcorresponding to 3 s).\nThe SSM is then computed using a K nearest neighbor al-\ngorithm (we used a value of \u0014= 0:04). Finally the SSM\nmatrix is convolved with a long bivariate rectangular Gaus-\nsian kernelG=gtgT\nl(we usedsl=0.5 sst=30 s and\n\u001b2= 0:16).c\u0003(t)is then computed from the resulting\nSSM. The ﬂowchart of this method is represented in the\nright part of Figure 3.\nResults are given in Table 2 for the various conﬁgura-\ntions and test-sets. c1(t)represents Serra et al. method\n[16]. As one can see, the use of a global prior (c 2(t)) al-\nlows to increase the results over c1(t)for the three test-sets\nand the two tolerance window (@3s and @0.5s). Surpris-\ningly, this time, results obtained with a local prior (c 3(t))\nare lower than the ones obtained with a global prior (c 2(t)).\nThis can be explained by the fact that Serra et al. method\napplies a long duration low-pass ﬁlter (s t=30s) to the\nSSM. It signiﬁcantly delays in time the maximum value\nof a segment in the SSM, hence delays pt(l), hence delays\nc3(t). In the opposite, because c2(t)is global, it is not\nsensitive to Serra et al. delay.\nStatistical tests: For a @3s tolerance window, the dif-\nference of results obtained with c2(t)(0.805) and c1(t)\n(0.772) is only statistically signiﬁcant (at 5%) for the\nBeatles-B test-set. For a @0.5s tolerance window, the dif-\nferences are statistically signiﬁcant (at 5%) for all three\ntest-sets.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n341Table 2. Results of music structure segmentation using our lag-prior method applied to the SSM as computed by [16] .\nRWC-Pop-A\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nc1(t)(without lag-prior) with Serra front-end 0.780 0.846 0.742 0.254 0.271 0.246\nc2(t)(with global lag-prior) with Serra front-end 0.784 0.843 0.750 0.289 0.316 0.275\nc3(t)(with local lag-prior) with Serra front-end 0.735 0.827 0.682 0.245 0.300 0.215\nRWC-Pop-B\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nc1(t)(without lag-prior) with Serra front-end 0.799 0.795 0.818 0.338 0.326 0.359\nc2(t)(with global lag-prior) with Serra front-end 0.823 0.846 0.820 0.389 0.408 0.381\nc3(t)(with local lag-prior) with Serra front-end 0.797 0.856 0.765 0.336 0.369 0.318\nBeatles-B\nMethod F @3s P @3s R @3s F @0.5s P @0.5s R @0.5s\nc1(t)(without lag-prior) with Serra front-end 0.772 0.792 0.773 0.371 0.365 0.394\nc2(t)(with global lag-prior) with Serra front-end 0.805 0.813 0.817 0.439 0.430 0.450\nc3(t)(with local lag-prior) with Serra front-end 0.799 0.790 0.827 0.422 0.416 0.442\n4. CONCLUSION AND FUTURE WORKS\nIn this paper, we have proposed a simple, but very efﬁcient,\nmethod that allows using Goto 2003 method as a prior lag-\nprobability on Serra et al. structure feature method. We\nprovided the rational for such a proposal, and proposed\ntwo versions of the method: one using a global lag prior,\none using a local lag prior. We performed a large-scale\nexperiment of our proposal in comparison to state-of-the-\nart algorithms using three test-sets: RWC-Popular-Music\nwith two sets of annotations and Isophonics/Beatles. We\nshowed that the introduction of the lag-prior allows a large\nimprovement of the F-Measure results (with a tolerance\nwindow of 3 s) over the three sets. Also, our method im-\nproves over the best results obtained by Serra et al. or dur-\ning MIREX-2012 and MIREX-2013.\nFuture works will concentrate on integrating this prior\nlag probability on an EM (Expectation-Maximization) al-\ngorithm to estimate the true p(t;l). Also, we would like to\nuse this segmentation as a ﬁrst step to a segment labeling\nalgorithm.\nAcknowledgements This work was partly founded\nby the French government Programme Investissements\nd’Avenir (PIA) through the Bee Music Project.\n5. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeﬁeld. To catch a chorus: Us-\ning chroma-based representations for audio thumbnailing. In Proc.\nof IEEE WASPAA (Workshop on Applications of Signal Processing\nto Audio and Acoustics), pages 15–18, New Paltz, NY , USA, 2001.\n[2] Fr ´ed´eric Bimbot, Emmanuel Deruty, Sargent Gabriel, and Em-\nmanuel Vincent. Methodology and conventions for the latent semi-\notic annotation of music structure. Technical report, IRISA, 2012.\n[3] Roger Dannenberg and Masataka Goto. Music structure analysis\nfrom acoustic signal. In Handbook of Signal Processing in Acous-\ntics Vol. 1, pages 305–331. Springer Verlag, 2009.\n[4] Jonathan Foote. Visualizing music and audio using self-similarity.\nInProc. of ACM Multimedia, pages 77–80, Orlando, Florida, USA,\n1999.\n[5] Jonathan Foote. Automatic audio segmentation using a measure of\naudio novelty. In Proc. of IEEE ICME (International Conference on\nMultimedia and Expo), pages 452–455, New York City, NY , USA,\n2000.[6] Masataka Goto. A chorus-section detecting method for musical au-\ndio signals. In Proc. of IEEE ICASSP (International Conference on\nAcoustics, Speech, and Signal Processing), pages 437–440, Hong\nKong, China, 2003.\n[7] Masataka Goto. Aist annotation for the rwc music database. In Proc.\nof ISMIR (International Society for Music Information Retrieval),\npages pp.359–360, Victoria, BC, Canada, 2006.\n[8] Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and\nRyuichi Oka. Rwc music database: Popular, classical, and jazz mu-\nsic databases. In Proc. of ISMIR (International Society for Music\nInformation Retrieval), pages pp. 287–288, Paris, France, 2002.\n[9] Florian Kaiser and Geoffroy Peeters. Multiple hypotheses at multiple\nscales for audio novelty computation within music. In Proc. of IEEE\nICASSP (International Conference on Acoustics, Speech, and Signal\nProcessing), Vancouver, British Columbia, Canada, May 2013.\n[10] Matthias Mauch, Chris Cannam, Matthew Davies, Simon Dixon,\nChristopher Harte, Sefki Klozali, Dan Tidhar, and Mark Sandler.\nOmras2 metadata project 2009. In Proc. of ISMIR (Late-Breaking\nNews), Kobe, Japan, 2009.\n[11] Meinard M ¨uller and Sebastian Ewert. Chroma toolbox: Matlab im-\nplementations for extracting variants of chroma-based audio fea-\ntures. In Proc. of ISMIR (International Society for Music Information\nRetrieval), Miami, Florida, USA, 2011.\n[12] Meinard M ¨uller, Franz Kurth, and Michael Clausen. Audio match-\ning via chroma-based statistical features. In Proc. of ISMIR (Interna-\ntional Society for Music Information Retrieval), London, UK, 2005.\n[13] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri. Audio-based mu-\nsic structure analysis. In Proc. of ISMIR (International Society for\nMusic Information Retrieval), Utrecht, The Netherlands, 2010.\n[14] Geoffroy Peeters. Deriving Musical Structures from Signal Anal-\nysis for Music Audio Summary Generation: Sequence and State\nApproach, pages 142–165. Lecture Notes in Computer Science.\nSpringer-Verlag Berlin Heidelberg 2004, 2004.\n[15] Geoffroy Peeters, Amaury Laburthe, and Xavier Rodet. Toward auto-\nmatic music audio summary generation from signal analysis. In Proc.\nof ISMIR (International Society for Music Information Retrieval),\npages 94–100, Paris, France, 2002.\n[16] Joan Serra, Meinard Muller, Peter Grosche, and Josep Ll. Arcos.\nUnsupervised detection of music boundaries by time series struc-\nture features. In Proc. of AAAI Conference on Artiﬁcial Intelligence,\n2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n342"
    },
    {
        "title": "Introducing a Dataset of Emotional and Color Responses to Music.",
        "author": [
            "Matevz Pesek",
            "Primoz Godec",
            "Mojca Poredos",
            "Gregor Strle",
            "Joze Guna",
            "Emilija Stojmenova",
            "Matevz Pogacnik",
            "Matija Marolt"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415926",
        "url": "https://doi.org/10.5281/zenodo.1415926",
        "ee": "https://zenodo.org/records/1415926/files/PesekGPSGSPM14.pdf",
        "abstract": "The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gath-ering user responses is described along with two new inter-faces for capturing emotional states: the MoodGraph and Mood- Stripe. An evaluation study showed both inter-faces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants’ emotional states and color associations, as well as relationships be- tween musically perceived and induced emotions. We be- lieve the size of the dataset, in-terfaces and multi-modal approach (connecting emo-tional, visual and auditory as- pects of human perception) give a valuable contribution to current research.",
        "zenodo_id": 1415926,
        "dblp_key": "conf/ismir/PesekGPSGSPM14",
        "keywords": [
            "new",
            "dataset",
            "mood-dependent",
            "color",
            "responses",
            "music",
            "methodology",
            "user",
            "responses",
            "interfaces"
        ],
        "content": "INTRODUCING A DATASET OF EMOTIONAL AND COLOR \nRESPONSES TO MUSIC \nMatevž Pesek1, Primož Godec1, Mojca Poredoš1, Gregor Strle2, Jože Guna3,  \nEmilija Stojmenova3, Matevž Pogačnik3, Matija Marolt1 \n1University of Ljubljana, Faculty of Computer and Information Science  \n[matevz.pesek,primoz.godec,mojca.poredos,matija.marolt] @fri.uni -lj.si \n2Scient ific Research Centre of the Slovenian Academy of Sciences and  Arts,  \nInstitute of Ethnomusicology  \ngregor.strle@zrc -sazu.si  \n3University of Ljubljana, Faculty of Electrotechnics  \n[joze.guna,emilija.stojmenova,matevz.pogacnik ]@fe.uni -lj.si  \nABSTRACT \nThe paper presents a new dataset of mood-dependent and \ncolor responses to music. The methodology of gath-ering \nuser responses is described along with two new inter-faces \nfor capturing emotional states: the MoodGraph and Mood-\nStripe. An evaluation study showed both inter-faces have \nsignificant advantage over more traditional methods in \nterms of intuitiveness, usability and time complexity. The \npreliminary analysis of current data (over 6.000 responses) \ngives an interesting insight into participants’ emotional  \nstates and color associations, as well as relationships be-\ntween musically perceived and induced emotions. We be-\nlieve the size of the dataset, in-terfaces and multi-modal \napproach (connecting emo-tional, visual and auditory as-\npects of human perception) give a valuable contribution to \ncurrent research.   \n1. INTRODUCTION \nThere is no denial that strong relationship exists between \nmusic and emotions . On one hand, music can express and \ninduce a variety of emotional responses in listeners and can \nchange our mood (e.g. make us happy – we consider mood \nto be a longer lasting state). On the other hand, our current \nmood strongly influences our choice of musi c - we listen \nto different music when we’re sad than when we’re happy.  \nIt is therefore not surprising that this relationship has \nbeen studied within a variety of fields, such as philosophy, \npsychology, musicology, anthropology or sociology [1] . \nWithin Music Information Retrieval, the focus has been on \nmood estimation from audio (a MIREX task since 2007) , \nlyrics or tags and its use for music recommendation and \nplaylist generation, e.g. [2-5].  \nTo estimate and analyze the relationship between mood \nand music, several datasets were made available in the past years. The soundtracks dataset for music and emotion con-\ntains single mean ratings of perceived emotions (labels and \nvalues in a three-dimensional model are given) for over \n400 film music excerpts [6]. The MoodSwings Turk Da-\ntaset contains on average 17 valence-arousal ratings for \n240 clips of popular music [7]. The Cal500 contains a set \nof mood labels for 500 popular songs [8], at around three \nannotations per song, and the MTV Music Data Set [9] a \nset of 5 bipolar valence-arousal ratings for 192 popular \nsongs. \nIn this paper, we introduce a new dataset that captures \nusers’ mood states, their perceived and induced emot ions \nto music and their association of colors with music. Our \ngoals when gathering the dataset were to capture data \nabout the user (emotional state, genre preferences, their \nperception of emotions) together with ratings of perceived \nand induced emotions on a set of unknown music excerpts \nrepresenting a variety of genres. We aimed for a large \nnumber of annotations per song, to capture the variability, \ninherent in user ratings.  \nIn addition, we wished to capture the relation between \ncolor and emotions, as well as color and music, as we be-\nlieve that color is an important factor in music visualiza-\ntions. A notable effort has been put into visualizing the mu-\nsic data on multiple levels: audio signal, symbolic repre-\nsentations and meta-data [10]. Color tone mappings can be \napplied onto the frequency, pitch or other spectral compo-\nnents [11], in order to describe the audio features of the \nmusic [12], or may represent music segments. The color \nset used for most visualizations is picked instinctively by \nthe creator. To be able to provide a more informed color \nset based on emotional qualities of music, our goal thus \nwas to find out whether certain uniformity exist in the per-\nception of relations between colors, emotions and music.  \nThe paper is structured as follows: section 2 describes \nthe survey and it design, section 3 provides preliminary \nanalyses of the gathered data and survey evaluation and \nsection 4 concludes the paper and describes our future \nwork.  © Matevž Pesek, Primož Godec, Mojca Poredoš, Gregor \nStrle, Jože Guna,  Emilija Stojmenova , Matevž Pogačnik , Matij a Marolt  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Matevž Pesek, Primož Godec, \nMojca Poredoš, Gregor Strle, Jože Guna,  Emilija Stojmenova , Matevž \nPogačnik , Matij a Marolt . “Introducing a dataset of emotional an d color \nresponses to music ”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3552. ONLINE SURV EY \nWe gathered the dataset with an online survey, with the \nintention to reach a wide audience and gather a large num-\nber of responses. We started our survey design with a pre-\nliminary questionnaire, which provided some basic guide-\nlines for the overall design . We formed several research \nquestions to drive the design and finally implemented the \nsurvey which captures the user’s current emotional state, \ntheir perception of colors and corresponding emotions, as \nwell as emotions perceived and induced from music, along \nwith the corresponding color. After the first round of re-\nsponse gathering was completed, we performed a new sur-\nvey designed to evaluate different aspects of user experi-\nence with our original survey. \n2.1 Preliminary study  \nAlthough there exists some consent that a common set of \nbasic emotions can be defined [13], in general there is no \nstandard set of emotion labels that would be used in music \nand mood researches. Some authors choose labeled sets in-\ntuitively, with no further explanation [14] .  In contrast, we \nperformed an initial study in order to establish the relevant \nset of labels. For the purpose of eliminating the cultural \nand lingual bias on the labelling, we performed our survey \nin Slovenian language for Slovene-speaking participants.  \nThe preliminary questionnaire asked the user to de-\nscribe their current emotional state through a set of 48 \nemotion labels selected from literature [15- 17] , each with \nan intensity-scale from 1 (inactive) to 7 (active). The ques-\ntionnaire was solved by 63 participants. Principal compo-\nnent analysis of the data revealed that first three compo-\nnents explain 64% of the variance in the dataset. These \nthree components strongly correlate to 17 emotion label s \nchosen as emotional descriptors for our survey. \nWe also evaluated the effectiveness of the continuous \ncolor wheel to capture relationships between colors and \nemotions. Responses indicated the continuous color scale \nto be too complex and misleading for some users. Thus, a \nmodified discrete-scale version with 49 colors displayed \non larger tiles was chosen for the survey instead. The 49 \ncolors have been chosen to provide a good balance be-\ntween the complexity of the full continuous color wheel \nand the limitations of choosing a smaller subset of colors.  \n2.2 The survey \nThe survey is structured into three parts, and contains \nquestions that were formulated according to our hypothe-\nses and research goals: \n• user’s mood impacts their emotional and color percep-\ntion of music; \n• relations between colors and emotions are uniform in \ngroups of users with similar mood and personal char-\nacteristics; • correlation between sets of perceived and induced \nemotions depends both on the personal musical prefer-\nences, as wel l as on the user’s current mood;  \n• identify a subset of emotionally ambiguous music ex-\ncerpts and study their characteristics; \n• mappings between colors and music depend on the mu-\nsic genre; \n• perceived emotions in a music excerpt are expected to \nbe similar across listeners, while induced emotions are \nexpected to be correlated across groups of songs and \nusers with similar characteristics. \n \nWe outline all parts of the survey in the following sub-\nsections, a more detailed overview can be found in [18]. \n2.2.1 Part one – personal characteristics \nThe first part of the survey contains nine questions that \ncapture personal characteristics of users. Basic de-\nmographics were captured: age, gender, area of living, na-\ntive language. We also included questions regarding their \nmusic education, music listening and genre preferences. \nWe decided not to introduce a larger set of personal ques-\ntions, as the focus of our research lies in investigating the \ninterplay of colors, music and emotions and we did not \nwant to irritate the users with a lengthy first part. Our goal \nwas to keep the amount of time spent for filling in the sur-\nvey to under 10 minutes.  \n2.2.2 Part two - mood, emotions and colors \nThe second part of our survey was designed to capture in-\nformation about the user’s current mood, their perception \nof relation between colors and emotions and their percep-\ntion of emotions in terms of pleasantness and activeness.  \nThe user’s emotional state was captured in several \nways. First, users had to place a point in the valence-\narousal space. This is a standard mood estimation ap-\nproach, also frequently used for estimation of perceived \nemotions in music. Users also indicated the preferred color \nof their current emotional state, as well as marked the pres-\nence of a set of emotion label s by using the MoodStripe  \ninterface (see Figure 1 ). \n \n \nFigure 1:  The MoodStripe  allows users to express their \nemotional state by dragging emotions onto a canvas, \nthereby denoting their activity  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n356To match colors with emotions, users had to pick a color \nin the color wheel that best matches a given emotion label \n(10 labels were presented to each user). Finally, users had \nto assess how they perceive the pleasantness and active-\nness of emotions by placing a set of emotion label s into the \nvalence-arousal space using the MoodGraph  (see Figure \n2). This enables us to evaluate the variability of placement \nof emotion labels in terms of their activeness and pleasant-\nness and compare it to data gathered in part three, where \nusers described musical excerpts in a similar manner.  \n2.2.3 Part three - music in relation to colors and emotions \nIn the third part of our survey users were asked to complete \ntwo tasks on a set of ten 15-second long music excerpts. \nThese were randomly selected from a database of 200 mu-\nsic excerpts. When compiling the database, we strived for \na diverse, yet unknown set of music pieces, to avoid judg-\nments based on familiarity with the content. The database \ncontains 80 songs from the royalty free online music ser-\nvice Jamendo , representing a diverse variety of “standard” \ngenres, with songs unknown to the wider audience. 80 \nsongs were included from a dataset of film music excerpts \n[6], 20 from a database of folk music and 20 from a con-\ntemporary electro-acoustic music collection.  \nAfter listening to an excerpt, users were first asked to \nchoose the color best representing the music from the color \nwheel. Next, users were asked to describe the music by \ndragging emotion labels onto the valence-arousal spa ce us-\ning the MoodGraph  interface ( Figure 2 ). Two different sets \nof labels were used for describing induced and perceived \nemotions, as different emotions correspond with respec-\ntive category[19], and at least one label from each category \nhad to be placed onto the space. shown and  \n \n  \nFigure 2 : The MoodGraph : users drag emotion labels onto \nthe valence-arousal space. Induced emotions are marked \nwith a person icon, perceived emotions with a note icon.  \n2.3 Evaluation survey \nAfter responses were gathered, we performed an additional \nevaluation survey, where we asked participants to evaluate the original survey. Although the survey was anonymous, \nusers had the opportunity to leave their email at the end, \nwhich we used to invite them to fill in the evaluation ques-\ntionnaire. Participants were presented a set of twelve ques-\ntions about different aspects of the survey: user experience, \ncomplexity of the questionnaire, and aspects of our new \nMoodGraph  and MoodStripe  interfaces. Some of the ques-\ntions were drawn from the existing evaluation standard \nNASA load task index [20], while others were intended to \nevaluate different aspects of our interfaces.  \n3. RESULTS \nThe survey was taken by 952 users, providing 6609 \nmood/color-perception responses for the 200 music ex-\ncerpts used.  We thus obtained a large number of responses \nper music excerpt (each has 33 responses on average), in-\ncluding sets of induced and perceived emotion labels, their \nplacement in the valence-arousal space, as well as the color \ndescribing the excerpt. To our knowledge, no currently \navailable mood-music dataset has such a high ratio of user \nannotations per music excerpt. The data, as well as music \nexcerpts will be made public as soon as the second round \nof response gathering, currently underway, will be fin-\nished. \nIn the following subsections, we provide some prelimi-\nnary analyses of our data. \n3.1 Demographic analysis \nThe basic demographic characteristics of the 952 partici-\npants are as follows. The average age of participants was \n26.5 years, the youngest had 15, the oldest 64 years. 65% \nof participants are women, 66% are from urban areas. 50% \nhave no music education, 47% do not play instruments or \nsing. The amount of music listening per day is evenly \nspread from less than 1 hour to over 4 hours. 3% claimed \nthey were under the influence of drugs when taking the \nsurvey. \n3.2 Colors and emotions \nIn the second part of the survey, participants indicated their \nemotional state within the valence-arousal space, as well \nas by choosing a color. Relations between the color hue \nand location in the valence-arousal space are not very con-\nsistent, but overall less active emotional states correspond \nmore with darker blue-violet hues, while the more active \nones to red-yellow-green hues. There is also a statistically \nsignificant positive correlation between color saturation \nand value (in a HSV color model) and activeness, as well \nas pleasantness of emotions: the more positive and active \nthe user’s emotional state is, the more vivid the colors are.  \nColors attributed to individual emotion labels, as well \nas the placement of labels in the valence-arousal space are \nvisible in Figure 3 . Associations between colors and emo-\ntions are quite consistent and in line with previous research \n[21-24]. Fear (A) and anger (F) are basic negative emo-\ntions and have dark blue/violet or black hues. Sadness (I) \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n357and relaxation (J), interestingly are also very similar, alt-\nhough different in valence. Energetic (C) as a very active \nmood is mostly red, joy (B) and liveliness (G) somewhat \nless (more yellowy, even green). Another interesting out-\ncome is that similar red-yellow-green hues are also preva-\nlent for disappointment (E) and discontent (H). Happiness \n(D) is very distinct, in pastels of green and blue (similar to \n[21-24]). As these hues are often related to inner balance \n(peace), their choice for happiness, by some definitions a \nstate where ones needs are satisfied, reflects the partici-\npants’ notion that happiness and inn er balance are re-\nlated[21, 24]. \n \n \nFigure 3:  position of emotions in the valence-arousal \nspace, and their colors . A: fear, B: joy, C: energy, D: hap-\npiness, E: disappointment, F: anger, G: liveliness, H: dis-\ncontent, I: relaxation, J: sadness  \n3.3 Relationships between induced and perceived emo-\ntions \nIn part three of the survey participants were asked to mark \ninduced and perceived emotions for individual music ex-\ncerpt by dragging emotion labels from the respective cate-\ngories onto the valence-arousal space (see Figure 2). Here, \nwe focus on the relationship between induced and per-\nceived emotions. \nFigure 4 shows the centroids (averages) for induced-per-\nceived emotion pairs of participants’ ratings for each mu-\nsic excerpt: 'anger', ' relaxed', ‘happi ness’, ‘joy’, 'sadness', \n'calmness', 'anticipation' and ‘fear’. Positions of induced -\nperceived emotion pairs (Figure 4) loosely correspond to \nthe positions of participant’s emotional states in the va-\nlence-arousal space from Figure 3, with some obvious dif-\nferences. For example (with respect to B, D and I on Figure \n3), positive induced-perceived emotion pairs, such as re-\nlaxed, happiness and joy (B, C and D in Figure 4) occupy \na more central space in the ‘pleasant/active’ quadrant of \nvalence-arousal space. Similarly, negative emotion pairs \n(A, E and H in Figure 4) are also more central on the ‘un-\npleasant’ quadrants than corresponding emotions on Fig-\nure 3, but have significantly larger variance and spread on \nvalence-arousal space compared to positive emotions \n(apart from relaxed (B)), especially along arousal dimen-\nsion. \nLet us compare the relationships in Figure 4. There is a \nnoticeable variance between induced and perceived emo-\ntions for negative emotions, such as fear (H), anger (A) and \nsadness (E), as they spread over both arousal and valence axes. The central position of sadness (E) along the arousal \ndimension is especially interesting, as it is typically asso-\nciated with low arousal (compare to J in Figure 3). Further-\nmore, all three negative emotions (A, E and H) are in cer-\ntain musical contexts experienced or perceived as pleasant. \nOn the other hand, positive induced-perceived emotion \npairs, such as joy (D) and happiness (C), tend to be more \nsimilar on both valence (positive) and arousal (relatively \nhigh) dimension and consequently have less variance. \nMore neutral emotions, such as calmness (F) and anticipa-\ntion (G), occupy the center, with relaxed (B) untypically \npotent on the arousal dimension. \n \n \nFigure 4 : Representation of relationships between in-\nduced-perceived emotion pairs of all music excerpts (in-\nduced centroid: green star, perceived centroid: red circle). \nA: anger, B. relaxation, C. happiness, D: joy, E: sadness, \nF: calmness, G: anticipation, H: fear  \nDiscriminating between induced and perceived emo-\ntions in music is a complex task and to date there is no \nuniversally agreed upon theory, or emotional model, that \nwould best capture emotional experiences of listeners (see \ne.g. [19, 25-29]). Many argue (e.g. [6, 19, 28, 30, 31]) that \nsimple valence-arousal dimensional model (one that \nMoodGraph  is based on) might be too reductionist, as it \nignores the variance of emotions and results in inherently \ndifferent emotions occupying similar regions of valence-\narousal space (e.g., compare regions of fear (H), anger (A) \nand sadness (E) in Figure 4). Our preliminary results nev-\nertheless show some interesting aspects of induction and \nperception of musical emotions. For example, the repre-\nsentations of relationships among and within induced-per-\nceived emotion pairs shown in Figure 4 support Gabriels-\nson’s theory of four basic types of relationship between in-\nduced and perceived emotions in relation to music: posi-\ntive/in agreement, negative/opposite, non-systematic/neu-\ntral and absent/no relationship [25]. Positive relationship \nis the most common (e.g., when music perceived to ex-\npress sad emotions also evokes such emotions in the lis-\ntener), resulting in the overlap (in some cases above 60%; \nsee e.g. [19, 26, 29]) of induced-perceived emotion pairs. \nIn one study [32], researchers found extremely strong pos-\nitive correlation for induced and perceived emotions on \nboth valence and arousal dimensions, and concluded that \nresults show “listeners will typically feel the emotions ex-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n358pressed by the song” [p. 93]. However, our preliminary re-\nsults do not support this claim. There is a significant vari-\nance among induced-perceived emotion pairs, particularly \namong negative emotions. Furthermore, while effects of \npositive correlation between induced and perceived emo-\ntions are evident (especially in positive emotions), other \ntypes of relationships are equally significant: from nega-\ntive/opposite, non-matching, to complex and neutral. The \npreliminary results clearly show differential variance \nacross induced and perceived emotions (in line with recent \nfindings [33] ). \nWhen analyzing the induced- perceived emotion pairs in \nMoodGraph , we’ve found that: a) they do not necessarily \npositively correlate, b) they occupy different regions and \nc) even when they fall into the same region of valence-\narousal space, both rotation and standard deviation within \neach induced-perceived emotion pair are significantly \nlarger than reported in some of the previous studies (e.g., \n[32]). This shows that participants understood both con-\ncepts (i.e. induced vs. perceived emotion) and were able to \ndifferentiate emotions from both categories on the va-\nlence-arousal space.  \nOne reason for large amount of variance in representa-\ntions of induced/perceived pairs is probably due to the \nmodel itself, as participants can rate both induced and per-\nceived emotions together and directly onto MoodGraph  af-\nter listening to the music excerpt. Another advantage, we \nargue, is the construction of the MoodGraph  itself. While \nbearing similarity with traditional approach to dimensional \nmodeling (a classic example being Russell’s circumplex \nmodel of affect [15]), the MoodGraph  has no pre-defined \nand categorically segmented/discrete regions of valence-\narousal space, hence avoiding initial bias, while still offer-\ning an intuitive interface  the participant is free to drag \nemotion labels onto MoodGraph according to her prefer-\nences and interpretation of the valence-arousal space. \n3.4 Evaluation survey \nThe online evaluation questionnaire was filled-in by 125 \nusers, who all took part in our survey. Results were posi-\ntive and indicate that the survey was properly balanced and \nthe new interfaces were appropriate. Detailed results can \nbe found in [34]. To summarize, responses show appropri-\nate mental difficulty of the questionnaire, while the physi-\ncal difficulty seems to be more uniformly distributed \nacross participants. Thus, it can be speculated that the lis-\ntening part of the questionnaire represents a physical chal-\nlenge to a significant number of participants. The pre-\nsented MoodGraph  interface was quite intuitive; however , \nit was also time demanding. Considering the task load of \nthe interface (combining three distinctive tasks), this was \nexpected. The number of emotions in MoodGraph  catego-\nries was slightly unbalanced and should be extended in our \nfuture work. The MoodStripe  interface represents a signif-\nicant improvement over a group of radio buttons, both in intuitiveness and time complexity. Participants also indi-\ncated that the set of 49 colors available for labeling emo-\ntions may not be large enough, so we will consider enlarg-\ning the set of color tones in our future work.  \n4. CONCLUSIONS \nWe intend to make the gathered dataset available to the \npublic, including the musical excerpts, data on users’ per-\nsonal characteristics and emotional state, their placement \nof emotions within the valence/arousal space, their per-\nceived and induced emotional responses to music and their \nperception of color in relation to emotions and music. This \nwill open new possibilities for evaluating and re-evaluat-\ning mood estimation and music recommendation ap-\nproaches on a well annotated dataset, where the ground \ntruth lies in the statistically significant amount of re-\nsponses per song, rather than relying on annotations of a \nsmall number of users.  \nShortly, we will start with the second round of response \ngathering with an English version of the survey. We also \nintend to enlarge the number of music excerpts in the mu-\nsic dataset and provide it to the users who have already \nparticipated in this study. Thus, we hope to further extend \nand diversify the dataset. \nPreliminary analyses already show new and interesting \ncontributions, and next to answering the questions already \nposed in section 2.2, the dataset will provide grounds for \nour future work (and work of others), including:  \n• previously introduced mood estimation algorithms will \nbe evaluated by weighting the correctness of their pre-\ndictions of perceived emotion responses for music ex-\ncerpts . New mood estimation algorithms will be devel-\noped, building upon the newly obtained data; \n• we will explore modelling of relations between music \nand colors chosen by users in the survey. Results may \nbe useful for music visualization, provided that corre-\nlations between audio and visual perception will be \nconsistent enough; \n• music recommendation interfaces will be explored , \npresenting recommendations in a visual manner with \nthe intent to raise user satisfaction by reducing the tex-\ntual burden placed on the user. The interface will in-\nclude personal characteristics and their variability in \nthe decision model; \n• the dataset can also be used in other domains, as r e-\nsponses that relate colors to emotions based on the \nuser’s emotional state can be used independently.  \n5. REFERENCES \n[1] P. Juslin and J. A. Sloboda, Music and Emotion: \nTheory and Research . USA: Oxford University Press, \n2001   \n[2] C. Laurier, O. Meyers, J. Serrà, M. Blech, P. Herrera, \nand X. Serra, \"Indexing music by mood: design and \nintegration of an automatic content-based annotator,\" \nMultimedia Tools Appl., vol. 48, pp. 161-184, 2010. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n359[3] M. Schedl, A. Flexer, and J. Urbano, \"The neglected \nuser in music information retrieval research,\" J. Intell. \nInf. Syst., vol. 41, pp. 523-539, 2013. \n[4] Y. Song, S. Dixon, and M. Pearce, \"A survey of music \nrecommendation systems and future perspectives,\" \npresented at the 9th Int. Symp. Computer Music \nModelling and Retrieval, London, UK, 2012. \n[5] Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, \nP. Richardson, J. J. Scott , et al. , \"State of the Art \nReport: Music Emotion Recognition: A State of the Art \nReview,\" presented at the ISMIR, 2010. \n[6] T. Eerola and J. K. Vuoskoski, \"A comparison of the \ndiscrete and dimensional models of emotion in music,\" \nPsychology of Music, vol. 39, pp. 18-49, 2011. \n[7] E. M. Schmidt and Y. E. Kim, \"Modeling musical \nemotion dynamics with conditional random fields,\" \npresented at the International Society for Music \nInformation Retrieval Conference, Miami, Florida, \n2011. \n[8] D. Turnbull, L. Barrington, D. Torres, and G. \nLanckriet, \"Semantic Annotation and Retrieval of \nMusic and Sound Effects,\" Audio, Speech, and \nLanguage Processing, IEEE Transactions on, vol. 16, \npp. 467-476, 2008. \n[9] B. Schuller, C. Hage, D. Schuller, and G. Rigoll, \n\"‘Mister D.J., Cheer Me Up!’: Musical and Textual \nFeatures for Automatic Mood Classification,\" Journa l \nof New Music Research, vol. 39, pp. 13-34, 2014/05/09 \n2010. \n[10] N. Orio, \"Music Retrieval: A Tutorial and Review,\" \nFoundations and Trends in Information Retrieval, vol. \n1, pp. 1-90, 2006. \n[11] S. Sagayama and K. Takahashi, \"Specmurt anasylis: A \npiano-ro ll-visualization of polyphonic music signal by \ndeconvolution of log-frequency spectrum,\" presented \nat the ISCA Tutorial and Research Workshop on \nStatistical and Perceptual Audio Processing, Jeju, \nKorea, 2004. \n[12] H.-H. Wu and J. P. Bello, \"Audio-based music \nv\nisualization for music structure analysis,\" presented at \nthe International Conference on Music Information \nRetrieval, Barcelona, Spain, 2010. \n[13] P. Ekman, \"Basic Emotions,\" in Handbook of \nCognition and Emotion , T. Dalgleish and M. Power, \nEds., ed Sussex, UK: John Wiley & Sons, 1999. \n[14] B. Wu, S. Wun, C. Lee, and A. Horner, \"Spectral \ncorrelates in emotion labeling of sustained musical \ninstrument tones \" presented at the International \nSociety for Music Information Retrieval Conference, \nCuritiba, Brasil, 2013. \n[15] J. Russell, \"A circumplex model of affect,\" Journal of \nPersonality and Social Psychology, vol. 39, pp. 1161-\n1178, 1980. \n[16] N. A. Remington, P. S. Visser, and L. R. Fabrigar, \n\"Reexamining the Circumplex Model of Affect,\" \nJurnal of Personality and Social Psychology, vol. 79, \npp. 286-300, 2000. \n[17] D. Watson, C. L. A., and A. Tellegen, \"Development \nand validation of brief measures of positive and \nnegative affect: The PANAS scales,\" Journal of \nPersonality and Social Psychology, vol. 54, pp. 1063-\n1070, 1988. \n[18] M. Pesek, P. Godec, M. Poredoš, G. Strle, J. Guna, E. \nStojmenova , et al. , \"Gathering a dataset of multi-modal \nmood-dependent perceptual responses to music,\" \npresented at the 2nd Workshop on \"Emotions and Personality in Personalized Services\", Aalborg, \nDenmark, 2014. \n[19] P. N. Juslin and P. Laukka, \"Expression, Perception, \nand Induction of Musical Emotions: A Review and a \nQuestionnaire Study of Everyday Listening,\" Journal \nof New Music Research, vol. 33, pp. 217-238, \n2014/05/09 2004. \n[20] S. G. Hart, \"Nasa-Task Load Index (NASA-TLX); 20 \nYears Later,\" Proceedings of the Human Factors and \nErgonomics Society Annual Meeting, vol. 50, pp. 904-\n-908, 2006. \n[21] P. Valdez and A. Mehrabian, \"Effects of Color on \nEmotions,\" Journal of Experimental Psychology: \nGeneral, vol. 123, pp. 394-409, 1994. \n[22] O. L. C., M. R. Luo, A. Woodcock, and A. Wright, \"A \nStudy of Colour Emotion and Color Preference. Part I: \nColour Emotions for Single Colours,\" Color research \nand application, vol. 29, pp. 232-240, 2004. \n[23] R. D. Norman and W. A. Scott, \"Color and affect: A \nreview and semantic evaluation,\" Journal of General \nPsychology, vol. 46, pp. 185-223, 1952. \n[24] L. B. Wexner \"The degree to which colors (hues) are \nassociated with mood-tones,\" Journal of Applied \nPsychology, vol. 38, pp. 432-435, 1954. \n[25] A. Gabrielsson, \"Emotion Perceived and Emotion Felt: \nSame or Different?,\" Musicae Scientiae, vol. 5, pp. \n123--147, 2002. \n[26] K. Kallinen and N. Ravaja, \"Emotion perceived and \nemotion felt: Same and different,\" Musicae Scientiae, \nvol. 10, pp. 191-213, 2006. \n[27] P. Evans and E. Schubert, \"Relationships between \nexpressed and felt emotions in music,\" Musicae \nScientiae, vol. 12, pp. 75-99, 2008. \n[28] E. Schubert, \"Measuring Emotion Continuously: \nValidity and Reliability of the Two-Dimensional \nEmotion-Space,\" Australian Journal of Psychology, \nvol. 51, pp. 154-165, 1999. \n[29] T. Eerola and J. K. Vuoskoski, \"A review of music and \nemotion studies: approaches, emotion models, and \nstimuli,\" vol. 30, pp. 307 –340, 2013. \n[30] T. a. V. J. K. Eerola, \"A comparison of the discrete and \ndimensional models of emotion in music,\" Psychology \nof Music, vol. 39, pp. 18 --49, 2010. \n[31] N. Haslam, \"The Discreteness of Emotion Concepts: \nCategorical Structure in the Affective Circumplex,\" \nPersonality and Social Psychology Bulletin, vol. 21, \npp. 1012-1019, 1995. \n[32] Y. Song, S. Dixon, M. Pearce, and A. R. Halpern, \"Do \nOnline Social Tags Predict Perceived or Induced \nEmotional Responses to Music?,\" presented at the \nInternational Society for Music Information Retrieval \nConference, Curitiba, Brasil, 2013. \n[33] E. Schubert, \"Emotion felt by listener and expressed by \nmusic: A literature review and theoretical \ninvestigation,\" Frontiers in Psychology, vol. 4, 2013. \n[34] M. Pesek, P. Godec, M. Poredoš, G. Strle, J. Guna, E. \nStojmenova , et al. , \"Capturing the Mood: Evaluation \nof the MoodStripe and MoodGraph Interfaces,\" in \nManagement Information Systems in Multimedia Art, \nEducation, Entertainment, and Culture (MIS-MEDIA), \nIEEE Internation Conference on Multimedia & Expo \n(ICME) , 2014, pp. 1-4.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n360"
    },
    {
        "title": "A Compositional Hierarchical Model for Music Information Retrieval.",
        "author": [
            "Matevz Pesek",
            "Ales Leonardis",
            "Matija Marolt"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416084",
        "url": "https://doi.org/10.5281/zenodo.1416084",
        "ee": "https://zenodo.org/records/1416084/files/PesekLM14.pdf",
        "abstract": "This paper presents a biologically-inspired composit- ional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear in- sight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower- level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by- one from music signals. Parts in each layer are compo- sitions of parts from previous layers based on statistical co-occurrences as the driving force of the learning pro- cess. We present the model’s structure and compare it to other deep architectures. A preliminary evaluation of the model’s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is pro- vided. Additionally, we show how the model can be ex- tended to event-based music processing, which is our final goal.",
        "zenodo_id": 1416084,
        "dblp_key": "conf/ismir/PesekLM14",
        "keywords": [
            "biologically-inspired",
            "compositional hierarchical",
            "MIR model",
            "deep learning",
            "alternative to neural networks",
            "generativeness",
            "transparency",
            "music signals",
            "multiple layers",
            "statistical co-occurrences"
        ],
        "content": "A COMPOSITIONAL HIERARCHICAL MODEL FOR MUSIC\nINFORMATION RETRIEV AL\nMatev ˇz Pesek\nUniversity of Ljubljana\nFaculty of computer\nand information science\nmatevz.pesek@fri.uni-lj.siAleˇs Leonardis\nCentre for Computational\nNeuroscience and Cognitive Robotics\nSchool of Computer Science\nUniversity of Birmingham\nales.leonardis@fri.uni-lj.siMatija Marolt\nUniversity of Ljubljana\nFaculty of computer\nand information science\nmatija.marolt@fri.uni-lj.si\nABSTRACT\nThis paper presents a biologically-inspired composit-\nional hierarchical model for MIR. The model can be treated\nas a deep learning model, and poses an alternative to deep\narchitectures based on neural networks. Its main features\nare generativeness and transparency that allow clear in-\nsight into concepts learned from the input music signals.\nThe model consists of multiple layers, each is composed\nof a number of parts. The hierarchical nature of the model\ncorresponds well with the hierarchical structures in music.\nParts in lower layers correspond to low-level concepts (e.g.\ntone partials), while parts in higher layers combine lower-\nlevel representations into more complex concepts (tones,\nchords). The layers are unsupervisedly learned one-by-\none from music signals. Parts in each layer are compo-\nsitions of parts from previous layers based on statistical\nco-occurrences as the driving force of the learning pro-\ncess. We present the model’s structure and compare it\nto other deep architectures. A preliminary evaluation of\nthe model’s usefulness for automated chord estimation and\nmultiple fundamental frequency estimation tasks is pro-\nvided. Additionally, we show how the model can be ex-\ntended to event-based music processing, which is our ﬁnal\ngoal.\n1. INTRODUCTION\nThe ﬁeld of music information retrieval (MIR) has reached\na signiﬁcant expansion in tasks and solutions in the short\ntimespan of its existence [3, 10]. The tasks include ex-\ntraction of high-level music descriptors from music, such\nas melody, chords and rhythm, as well as highly percep-\ntual tasks involving mood estimation, genre recognition\nand artist inﬂuence. Solutions have not come to a per-\nfect one for any of the described tasks yet; however, nu-\nmerous approaches proposed each year are improving the\nc\rMatev ˇz Pesek, Ale ˇs Leonardis, Matija Marolt.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Matev ˇz Pesek, Ale ˇs Leonardis, Matija\nMarolt. “A compositional hierarchical model for music information re-\ntrieval”, 15th International Society for Music Information Retrieval Con-\nference, 2014.state-of-the-art rapidly. Recently, deep belief networks as\nan alternative single model for a variety of tasks, have been\nsuccessfully introduced to the ﬁeld.\nThis paper presents a biologically-inspired composit-\nional hierarchical model for music information retrieval.\nThe proposed model poses an alternative to recent deep\nlearning architecture approaches [6,9]. Its main difference\nfrom the latter is in its transparent structure, thus allowing\nrepresentation and interpretation of the signal’s informa-\ntion extracted on different levels. We show the usefulness\nof our proposed approach in a preliminary evaluation of\nthe model for the tasks of automated chord estimation and\nmultiple fundamental frequency estimation. We also show\nhow the model can be extended to event-based music pro-\ncessing, and point out how the model’s transparency en-\nables other applications of the model, e.g. for music anal-\nysis, synthesis and visualization.\n2. DEEP ARCHITECTURES FOR MIR\nThe concept of deep learning has grown in popularity in\nthe ﬁelds of signal processing [15], audio processing [9]\nand MIR. Lee [7] presented one of the ﬁrst attempts of us-\ning deep belief networks (DBNs) on audio signals, where\nconvolutional DBNs were applied to the speaker identiﬁ-\ncation task. A DBN was used as a feature extractor, and a\nsupport vector machine for classiﬁcation.\nLater, Hamel and Eck [5], evaluated DBNs for genre\nrecognition using a ﬁve-layer DBN with three hidden lay-\ners for feature extraction. The support vector machine was\nused for classiﬁcation, where as raw spectral data was used\nas input to the DBN. DBNs show great potential for many\ntasks that involve high-level feature extraction, such as emo-\ntion recognition, since there is usually no trivial spectral or\ntemporal feature that could be used to model the high-level\nrepresentation in question. Schmidt and Kim [13] showed\npromising results by using a 5-layer DBN for extraction of\nemotion-based acoustic features. Other approaches mod-\neled temporal aspects of the audio signal. Conditional DBN-\ns were used by Battenberg and Wessel [1] for drum pat-\ntern analysis. Schmidt [12] took a step further and showed\nthat DBNs can be trained for discriminating rhythm and\nmelody.\nOverall, recent research has shown great interest and\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n131success in using features learned from music signals, in\ncontrast to previously used hand-crafted features. The re-\nsearch reviewed in this subsection took place only in the\nlast few years; thus, there is a vast expansion of deep learn-\ning in MIR to be expected, as anticipated by Humphrey [6].\n3. THE COMPOSITIONAL HIERARCHICAL\nMODEL\n3.1 Motivation and concept\nDBNs brought an improvement to many MIR tasks with\ntheir unsupervised learning of features and generative mod-\neling. However, they require a large set of hidden units\nper layer, and consequently, large training sets. Also, the\nhidden nature of units offers no clear explanation of the\nundergoing feature extraction process and the meaning of\nextracted features. It is our goal to overcome these lim-\nitations by developing a white-box compositional hierar-\nchical model with shareable parts, thus reducing the num-\nber of parts and learning data needed, as well as reaching\ntransparency in terms of interpretable internal structure of\nthe model.\nThe proposed model provides a hierarchical representa-\ntion of the audio signal, from the signal components on the\nlowest level, up to individual musical events on the highest\nlevels. It is built on the assumption that a complex signal\ncan be decomposed into a hierarchy of building blocks -\nparts. These parts exist at various levels of granularity and\nrepresent sets of entities describing the signal. According\nto their complexity, parts can be structured across several\nlayers from less to the more complex. Parts on higher lay-\ners are expressed as compositions of parts on lower layers\n(e.g.: a chord is composed of several pitches, each pitch of\nseveral harmonics etc.). A part can therefore describe indi-\nvidual frequencies in a signal, their combinations, as well\nas pitches, chords and temporal patterns, such as chord\nprogressions.\nThe structure of our model is inspired by work in com-\nputer vision, speciﬁcally the hierarchical compositional\nmodel presented by Leonardis and Fidler [8]. Their model\nrepresents objects in images in a hierarchical manner, struc-\ntured in layers from simple to complex image parts. The\nmodel is learned from the statistics of natural images and\ncan be employed as a robust statistical engine for object\ncategorization and other computer vision tasks. We believe\nthat such approach can also be used for music representa-\ntion and analysis, however the transformation of the model\nto a different domain is not trivial.\n3.2 Model structure\nThe compositional hierarchical model consists of several\nlayers. Each layer contains a set of parts. A part is a com-\nposition of two or more parts from a lower layer and may\nitself be part of any number of compositions on a higher\nlayer. Thus, the compositional model forms a hierarchy of\nparts, where each part represents a composition of lower-\nlayer parts, as seen in Figure 1. Connections in the ﬁgure\nrepresent compositions of parts.3.2.1 Input layer\nThe input layer of the model is derived from the time-\nfrequency representation of the music signal. We denote\nthis layer as layer L0. It contains a single atomic part,\nwhich is activated (produces output) at locations of all fre-\nquency components in the signal at a given time instance.\nAn example is given in Figure 1, although not all activa-\ntions are shown for clarity. More formally, a part’s activa-\ntion is deﬁned by two values: location LPthat corresponds\nto frequency, and magnitude AP, that corresponds to mag-\nnitude of the frequency component.\nFigure 1. Compositional hierarchical model. Parts on the\ninput layer correspond to signal components in the time-\nfrequency representation. Parts on higher layers are com-\npositions of lower-layer parts (denoted as links in the ﬁg-\nure). A part may be contained in several compositions, e.g.\nP11on the ﬁrst layer is part of compositions P21,P22and\nP2mon the second layer. Several depictions of the same\npart (e.g. part instances P11andP0\n11) denote several acti-\nvations of the part on different locations (all instances of a\npart on a layer are marked with the same outlined color).\nParts activated in t1are shown ﬁlled with color.\nAny time-frequency representation can be used for the\ninput layer, although logarithmic frequency spacing pro-\nduces more compact models due to the relative nature of\npart compositions on higher layers (as described further\non).\n3.2.2 Subsequent layers\nHigher layers of the model Lncontain sets of compositions\n- parts composed of parts from lower layers. Each compo-\nsition can contain any number of parts from the lower lay-\ners (for clarity we only use two-part compositions to ex-\nplain the model). A composition can be part of any num-\nber of compositions on higher layers. Compositions are\ndenoted as links between parts in Figure 1.\nComposition ion layerLncan be formally deﬁned as\na structure containing parts from a layer below: a central\npartC, and a secondary part S. We name the parts forming\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n132a composition subparts. A composition can be deﬁned as:\nPn;i=fCn\u00001;j;Sn\u00001;k;(\u0016n;i;\u001bn;i)g; (1)\nwhereCn\u00001;j andSn\u00001;k are the central and secondary\nsubparts from layer n\u00001, while\u0016n;iand\u001bn;ideﬁne a\nGaussian limiting the difference between locations of sub-\npart activations (see definition of activation below). For\nclarity, we shall omit subscripts in the following equations\nand useP,C,S,\u0016and\u001bto denote a part and its compo-\nnents.\nA composition is activated (propagates output to higher\nlayers) when all of its subparts are activated. This strict\ncondition can be softened with hallucination, as explained\nin section 3.3. Part activation is composed of two values:\nactivation location LP, which represents the location (fre-\nquency) at which the part is activated, and activation mag-\nnitudeAP, which represents the strength of activation. The\nlocation of part’s activation is deﬁned simply as the loca-\ntion of activation of its central subpart:\nLP=LC: (2)\nThus, central parts of compositions on different layers prop-\nagate their locations upwards through the hierarchy. The\nmagnitude of activation is deﬁned as:\nAP=tanh[G(L C\u0000LS;\u0016;\u001b)\u0001(AC+AS)]; (3)\nwhere tanh stands for the hyperbolic tangent function that\nlimits the magnitude to [0,1) and Grepresent the Gaussian\nthat limits the difference in locations of the central part and\nthe subpart according to \u0016and\u001b. As an example, P2;2in\nFigure 1 is deﬁned as\nP2;2=fP1;1;P1;3;(1200; 25)g; (4)\nwhere\u0016and\u001bare given in cents. Therefore, it will be acti-\nvated whenever P1;1andP1;3will be activated at locations\napproximately one octave (1200 cents) apart. Two such ac-\ntivations are shown in the ﬁgure, one at 294 Hz and one at\n440 Hz.\n3.3 Inference\nThe model can be used as a feature extractor over any de-\nsired dataset. An audio signal, transformed into a time-\nfrequency representation, serves as input for layer L0. Ac-\ntivations are then calculated layer-by-layer according to\nEquations 2 and 3. Additionally, two biologically-inspired\nmechanisms govern the inference process and increase ro-\nbustness of the model: hallucination andinhibition.\nBefore we deﬁne both mechanisms, we need to intro-\nduce the concept of coverage. Coverage c(P;L P)of part\nPactive at location LPrepresents all signal information\n(frequency components) covered by the part and its sub-\ntree of parts. It is calculated top-down from an active part\ntoL0as:\nc(P;L P) =[\nfc(C;L P);c(S;L P+\u0016)g: (5)\nFor theL0layer, coverage is deﬁned as the set of parts\nwith positive activations AP>0, thus representing theset of covered frequency components. An example from\nFigure 1: the coverage of P2;2active at 294 Hz is the set of\nfrequencies:f294Hz; 588Hz; 880Hzg.\n3.3.1 Hallucination\nHallucination deals with ﬁlling-in the missing or damaged\ninformation in the signal and is implemented by enabling\npart activation in presence of incomplete input. The miss-\ning information in the signal can be replaced with knowl-\nedge encoded in the model during learning by allowing ac-\ntivations of parts most ﬁttingly covering the information\npresent. This allows the model to produce hypotheses in\nsituations with no straight result. Hallucination also boosts\nalternative explanations of input data, thus increasing its\nexplanation power and robustness.\nHallucination is governed by parameter \u001c1which can\nbe deﬁned per layer and modiﬁed during the inference. It\nchanges the conditions under which a part may be acti-\nvated. The default condition, as explained in section 3.2, is\nthat activation of a part is possible when all of its subparts\nare active. With hallucination, a part Pmay be activated at\nlocationLP, when the number of frequency components it\ncoversjc(P;L P)j, divided by the maximal number of com-\nponents it may cover is larger than \u001c1. For example, a \u001c1\nof 0.75 means that3\n4of all possible frequency components\nmust be covered by the part for it to be activated. A \u001c1of\n1 represents the default behavior.\n3.3.2 Inhibition\nThe second biologically-inspired mechanism provides a bal-\nancing factor by reducing redundant activations, similar to\nlateral inhibition performed by the human auditory system.\nInhibition reﬁnes the set of parts that yield competing hy-\npotheses of the same fragments of information in the in-\nput signal. Parts with greater activation magnitudes are\nretained and weaker activations inhibited. Inhibition also\nreduces activations that result from noise in the signal.\nActivation of part PatLPis inhibited, when another\npartQwith activation LQon the same layer (or a set of\nparts) covers the same fragments of information in the in-\nput signal, but with higher activation. The condition can\nbe expressed as:\n9Q:jc(P;L P)nc(Q;L Q)j\njc(P;L P)j<\u001c2^AQ>AP; (6)\nwhere\u001c2deﬁnes the amount of inhibition. For example, a\nvalue of 0.5 means that activation of Pis inhibited if half\nof its coverage is already covered by another, stronger part.\nTo sum up: inference yields a set of activations on all\nmodel layers by calculating activations considering hallu-\ncination and inhibition over all layers in a bottom-up order\nand over all time-frames of the input signal. Resulting ac-\ntivations represent model features and can be directly in-\nterpreted or used as inputs for discriminative tasks.\n3.4 Learning\nThe model is learned in an unsupervised manner on a set\nof input signals. It is constructed layer-by-layer, similar\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n133to other deep architectures. The learning process relies on\nstatistics of part activations, thus signal regularities are the\ndriving force of the learning process.\nWhen building layer Ln, co-occurrences of activations\nof parts onLn\u00001are observed. Compositions are formed\nfrom parts that frequently activate together at similar dis-\ntances. All such parts are joined into compositions and\nadded to the set of candidate compositions P. When form-\ning a composition of two frequently co-occurring parts, the\npart at the lower location represents the central part of the\ncomposition, while parameters \u0016and\u001bare estimated from\nall co-occurring activations of the two parts.\nTo reduce the number of compositions on each layer and\nkeep only the most informative ones, the set of candidates\nPis reﬁned. The goal of reﬁnement is to reduce the num-\nber of compositions in the learned layer while maintaining\nsufﬁcient coverage of information in the learning set.\nReﬁnement is implemented with a greedy approach,\nwhere in each iteration, a part that contributes most to the\ncoverage of information in the learning set, is selected and\nadded to the layer. Reﬁnement is concluded when one of\nthe following two criteria are reached: a sufﬁcient percent-\nage of information in the learning set is covered (according\nto threshold \u001c3), or no part remaining in the candidate set\nadds to the cumulative coverage of information. Algorithm\n1 outlines the described approach.\nAlgorithm 1 Greedy approach for selection of composi-\ntions from the candidate set P. Parts that add most to\nthe coverage of information in the learning set are pre-\nferred. Function perc calculates the percentage of infor-\nmation covered in the learning set by the given set of parts.\n1:procedure REFINE (P)\n2:prevCov 0\n3:coverages ;\n4:Ln ;\n5: repeat\n6: forP2P do\n7:coverages [P] perc(L n[P)\n8:Chosen argmax\nP(coverages )\n9:Ln L n[Chosen\n10:P PnChosen\n11: ifcoverages [Chosen] = prevCov then\n12: break //No added coverage - ﬁnish\n13:prevCov coverages [Chosen]\n14: untilprevCov>\u001c 3_P=;\n3.5 Time\nThe model presented so far is time-independent. It oper-\nates on a frame-by-frame basis, where each time frame in\nthe time-frequency representation is treated independently\nfrom others. Music, however, evolves in time and models\nthat operate on such bases often fail to reﬂect the evolution\nof sound properly.\nThe proposed model can be naturally extended to in-\nclude the time dimension. Our ﬁrst step towards extending\nthe model for time-dependent processing was to implement\na short-time automatic gain control mechanism, similar to\nthe automatic gain control contrast mechanism in human\nand other animal perceptual systems. The mechanism inte-grates part activations at similar locations over time. When\na new part activation appears and persists, its value is ini-\ntially boosted to accentuate the onset and later suppressed\ntowards a stable value.\nThe mechanism operates on all layers, and has a short-\nterm effect on lower layers, and longer-term effect on higher\nlayers due to the upward propagation of activations. Its end\neffect is that it stabilizes activations, reduces noise, pro-\nduces smoother model output and boosts event onsets.\n3.6 Relation to Deep Architectures\nThe compositional hierarchical model shares a great deal\nof similarities with other deep learning architectures. The\nstructure of the model is similar in terms of learning a va-\nriety of signal abstractions on several layers of granularity.\nThe model is learned in an unsupervised generative man-\nner, thus, no annotated data is needed. The learning pro-\ncedure is similar: the structure is built layer-by-layer. The\nproposed model can also be used for discriminative tasks\nby observing activations of parts on multiple layers.\nWe see the biggest advantage of the proposed compo-\nsitional hierarchical model over other established deep ar-\nchitectures in its transparency. As parts are compositions\nof subparts, their activations are directly observable and\ninterpretable. This opens the model up for a variety of in-\nteresting usages, as it not only produces features that can\nbe used, but features that can be interpreted and explained.\nIn addition, the inhibition and hallucination mechanisms\nmake it possible to produce alternative explanations of the\ninput by suppressing the winning explanation and search\nfor alternatives. In comparison to DBNs, where the outputs\nof each layer can only be interpreted during the evaluation,\nthe proposed model offers a deeper analysis of results by\ntracing the higher layer activations over all layers and in-\nvestigating the impact of each subpart.\nAnother difference in comparison to DBNs is the share-\nability and relativeness of parts, which both lead to a small\nnumber of parts needed to represent complex signals. A\npart in the proposed model is deﬁned by the relative dis-\ntance between its subparts and can thus be activated on dif-\nferent locations along the frequency axis. Thus, the large\namount of layer units that DBNs need to cover the entire\nspectrum is not necessary and is replaced by reusing the\navailable parts. This relativeness is accompanied with the\nconcept of part shareability: parts on a layer may be shared\nby many compositions on higher layers. For example, a\nchord is composed of at least three pitches which may be\nidentical in their representation in our model.\nWe show the usefulness of the described model’s fea-\ntures in the evaluation section, where the model is used as\nboth feature extractor and a classiﬁer. Other possible appli-\ncations exploiting the the model’s structure are presented\nin section 5.\n4. EV ALUATION OF THE MODEL\nThe presented model is applicable to different MIR tasks.\nTo present the model’s usefulness, we built a three-layer\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n134model and evaluated it on two tasks: automated chord es-\ntimation and multiple fundamental frequency estimation.\nThe input layer was the same for both tasks. A constant-\nQ transform was used to transform music signals onto 345\nfrequency bins between 55 and 8000 Hz, with a step size\nof 50 ms and maximal window size of 100 ms. Two layers\nof compositionsL1andL2were learnt as described pre-\nviously. Due to the shareability of parts, the they contain\nonly 23 and 12 parts respectively. The small number of\nparts in the model should mean that the model could be\ntrained on a small learning set. We tested this hypothesis\nand trained the model on large and small datasets, and ob-\nserved few differences. We were therefore able to build the\nmodel by using only a small set of 88 piano key samples\nas our learning set. We used the L2layer for the task of\nmultiple fundamental frequency estimation. For the task\nof automated chord estimation, we provided an additional\nL3octave-invariant layer. The latter consists of 48 parts,\nwhereL3activations correspond to octave-invariant acti-\nvations of theL2.\n4.1 Automated Chord Estimation\nThe time-independent model was tested for the task of au-\ntomated chord estimation on the standard Beatles dataset,\nkindly provided by C. Harte. We used activations of the\noctave-invariantL3layer as features and made the classi-\nﬁcation by using a hidden Markov model (HMM) with 24\nstates, each representing a chord, as described by [2]. We\nused cross-validation for evaluation; one album was used\nfor HMM training and the rest of the dataset for estimation.\nOur per-frame classiﬁcation accuracy on the given data-\nset was 67.14 % with 0.1525 standard deviation. Com-\npared to other per-frame approaches, we ﬁnd our results\nslightly lower than for example [11], which also used per-\nframe technique for feature extraction. Nevertheless, we\nperformed the evaluation as a proof of concept with time-\nindependent feature extraction and no ﬁne-tuning of the\nmodel, its learning, nor tuning of HMM parameters. We\nanticipate signiﬁcant results increase by extending the mod-\nel to time-dependent evaluation, using the whole hierarchy\nfor classiﬁcation and parameter tuning.\n4.2 Multiple fundamental frequency estimation\nThe model was also tested for the task of multiple funda-\nmental frequency estimation (MFEE) on the two subsets\nof MAPS (MIDI Aligned Piano Sounds) dataset, provided\nby [4]. Activations of layer L2were directly used as funda-\nmental frequency estimations with no further processing.\nThe following metrics were used for evaluation: per-\nframe precision and recall, precision and recall without\npenalising for octave errors, and pitch-class precision and\nrecall. Results are shown in Table 1. Our results are sig-\nniﬁcantly lower when compared to recent approaches, e.g.\n[14] which reported 77.1% classiﬁcation accuracy on the\nsubsets. However, the mentioned approach differs signif-\nicantly from ours, as a severely larger dataset (approx. 4\ntimes larger than the test sets) was used for training the\nsupport vector machine (SVM) classiﬁer. In comparison,\nFigure 2. Hypotheses produced by our model for the task\nof multiple fundamental frequency estimation (A) and the\nground truth (B). X axis represent time (in frames), and y\nmidi pitches. Although the model produces many possi-\nble hypotheses per frame, only the ones with the highest\nmagnitudes are used for comparison. Colors represent the\nmagnitudes of activations in Fig. A or the MIDI velocity\nin Fig. B.\nour model was trained only on a small set of piano key\nsamples, so no parts of the MAPS dataset were used for\ntraining. It is also worth to mention that for this task, our\nmodel was used as a feature extractor and a classiﬁer at the\nsame time. We expect that accuracy would be improved if\na classiﬁer such as a SVM would be added on top of our\nmodel and would take features extracted on all layers for\ninputs. Our intention for this paper, however, is to present\nthe general applicability of the model for multiple tasks\nand to avoid ﬁne-tuning.\nTable 1. Classiﬁcation accuracy (CA) using all hypothe-\nses provided by the model, precision (Pr) and recall (Re)\nvalues over a part of the MAPS dataset. Results without\npenalising octave errors and considering only pitch classes\nare marked with OandPCsubscripts respectively.\nFolder name CA Pr Re\nAkPnBcht 56.53 % 19.40 % 55.69 %\nAkPnBsdf 66.17 % 22.05 % 61.27 %\nAkPnBcht O 67.08 % 35.37 % 64.55 %\nAkPnBsdf O 71.16 % 46.10 % 68.83 %\nAkPnBcht PC 86.20 % 51.83 % 86.59 %\nAkPnBsdf PC 88.23 % 58.68 % 70.99 %\n5. OTHER APPLICATIONS OF THE MODEL\nOur intention with developing the proposed model is to\nmake an interpretable model that overcomes some of the\nlimitations of DBNs and can be used for tackling various\nMIR tasks. Its transparency, however, also makes other\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n135uses of the model possible.\nThe hierarchical approach presented in this paper ﬁts\nwell with the hierarchical structure of music in frequency\nas well as in time domains. Each part of the model repre-\nsents an explainable entity (e.g. tone partial, pitch, chord).\nIn contrast to the DBNs, each part of the model can be visu-\nalized. Visualization not only exposes the layered structure\nof the model, but also discloses information processed by\nthe observed part and its inﬂuence on other parts and their\nactivations. This insight into the music signal can be used\nin several scenarios — music visualization, music analysis\nand music synthesis.\nWe have developed a real-time visualization of the model,\nenabling deeper understanding of the processed informa-\ntion. When observing an inferred audio signal, the output\nof all layers of the model is presented by visualizing acti-\nvations of parts. This insight enables detailed analysis of\neach event in the music signal and may bring additional\nevent details to light. For example, a chord inversion can\nbe observed by looking into the activated subtree of the\nchord from top layers to bottom-ones. Thus, visualization\nof our model offers an innovative user interface for music\nanalysis.\nThe transparency of the model can also be exploited\nfor music processing and synthesis. Parts across all lay-\ners form a variety of harmonic structures, and can be used\nfor signal manipulation and synthesis. By activating a set\nof parts at different locations, a new spectral representa-\ntion is produced. Although the interface may not provide\na sufﬁcient amount of features for a standalone music per-\nformance, it can be used as a sound generator in a com-\nbination with a music instrument, e.g. a MIDI keyboard.\nThe interface thus serves as an advanced tool for spectral\nmodiﬁcation, while the instrument provides the interface\nfor performance.\n6. CONCLUSION AND FUTURE WORK\nThis paper presents a compositional hierarchical model as\nan alternative to deep learning architectures based on neu-\nral networks. The model shares a great deal of similari-\nties with other deep architectures, including a multi-layer\nstructure, unsupervised generative learning and suitabil-\nity for discriminative tasks. Furthermore, the white-box\nstructure of the model offers new utilizations of the model.\nWe highlighted three possible applications: feature extrac-\ntion for MIR tasks, music visualization and music analy-\nsis/synthesis.\nThe model’s internals rely on ﬁndings in the ﬁelds of\nneurobiology and cognitive sciences. By implementing\nbiologically-inspired mechanisms into the model, we made\nan attempt to build a model which partially resembles a\nsubset of functions of the human auditory system. We in-\ntend to retain and further develop this aspect of the model\nwith an intention to bring the computational modeling\ncloser to human auditory perception.\nThe paper presents an initial development of our model.\nWe plan to further extend it with the focus on temporal\nmodeling. Parts can namely be extended into the time do-main, thus bringing their activations closer to event-based\nmodeling. We also plan to tackle temporal tasks, such as\nonset detection, as well as beat tracking and tempo estima-\ntion. The proposed model is also going to be evaluated for\npattern analysis of symbolic data, including discovery of\nrepeated themes, and symbolic melodic similarity.\n7. REFERENCES\n[1] Eric Battenberg and David Wessel. Analyzing Drum Patterns using\nConditional Deep Belief Networks. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (ISMIR), pages\n37–42, 2012.\n[2] Juan P. Bello and Jeremy Pickens. A robust mid-level representation\nfor harmonic content in music signals. In Proceedings of the Inter-\nnational Conference on Music Information Retrieval (ISMIR), pages\n304–311, London, 2005.\n[3] J. Stephen Downie, Andreas F. Ehmann, Mert Bay, and M. Cameron\nJones. The Music Information Retrieval Evaluation eXchange: Some\nObservations and Insights. In Wieczorkowska A.A. and Ras Z.W.,\neditors, Advances in Music Information Retrieval, pages 93–115.\nSpringer-Verlag, Berlin, 2010.\n[4] Valentin Emiya, Roland Badeau, and Bertrand David. Multipitch\nEstimation of Piano Sounds Using a New Probabilistic Spectral\nSmoothness Principle. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 18(6):1643–1654, August 2010.\n[5] Philippe Hamel and Douglas Eck. Learning Features from Music Au-\ndio with Deep Belief Networks. In Proceedings of the International\nConference on Music Information Retrieval (ISMIR), pages 339–344,\n2010.\n[6] Eric J. Humphrey, Juan P. Bello, and Yann LeCun. Moving beyond\nfeature design: deep architectures and automatic feature learning in\nmusic informatics. In Proceedings of the International Conference on\nMusic Information Retrieval (ISMIR), Porto, 2012.\n[7] Honglak Lee, Peter Pham, Yan Largman, and Andrew Y . Ng. Unsu-\npervised feature learning for audio classiﬁcation using convolutional\ndeep belief networks. In Advances in Neural Information Processing\nSystems, pages 1096–1104, 2009.\n[8] Ale ˇs Leonardis and Sanja Fidler. Towards scalable representations of\nobject categories: Learning a hierarchy of parts. Computer Vision and\nPattern Recognition, IEEE, pages 1–8, 2007.\n[9] Abdel-rahman Mohamed, George E. Dahl, and Geoffrey Hinton.\nAcoustic Modeling using Deep Belief Networks. IEEE Transactions\non Audio, Speech, and Language Processing, 20(1):14–22, 2010.\n[10] Nicola Orio. Music Retrieval: A Tutorial and Review. Foundations\nand Trends R\rin Information Retrieval, 1(1):1–90, 2006.\n[11] Helene Papadopoulos and Geoffroy Peeters. Large-case Study of\nChord Estimation Algorithms Based on Chroma Representation and\nHMM. Content-Based Multimedia Indexing, 53-60, 2007.\n[12] Eric M. Schmidt and Youngmoo E. Kim. Learning Rhythm and\nMelody Features with Deep Belief Networks. In Proceedings of the\nInternational Conference on Music Information Retrieval (ISMIR),\npages 21–26, 2013.\n[13] Erik M. Schmidt and Youngmoo E. Kim. Learning emotion-based\nacoustic features with deep belief networks. In 2011 IEEE Workshop\non Applications of Signal Processing to Audio and Acoustics (WAS-\nPAA), pages 65–68. IEEE, October 2011.\n[14] Felix Weninger, Christian Kirst, Bjorn Schuller, and Hans-Joachim\nBungartz. A discriminative approach to polyphonic piano note tran-\nscription using supervised non-negative matrix factorization. In Pro-\nceedings of International Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP), pages 6–10, Vancouver, 2013.\n[15] Dong Yu and Li Deng. Deep Learning and Its Applications to Signal\nand Information Processing [Exploratory DSP. IEEE Signal Process-\ning Magazine, 28(1):145–154, January 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n136"
    },
    {
        "title": "Frame-Level Audio Segmentation for Abridged Musical Works.",
        "author": [
            "Thomas Prätzlich",
            "Meinard Müller"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418039",
        "url": "https://doi.org/10.5281/zenodo.1418039",
        "ee": "https://zenodo.org/records/1418039/files/PratzlichM14.pdf",
        "abstract": "Large-scale musical works such as operas may last sev- eral hours and typically involve a huge number of mu- sicians. For such compositions, one often finds differ- ent arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller en- sembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automati- cally segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the un- known version. Considering the opera “Der Freisch¨utz” as an example scenario, we discuss how to balance out flex- ibility and robustness properties of our proposed frame- level segmentation procedure.",
        "zenodo_id": 1418039,
        "dblp_key": "conf/ismir/PratzlichM14",
        "keywords": [
            "audio recording",
            "semantically meaningful parts",
            "automatically segmenting",
            "reference segmentation",
            "abridged versions",
            "strongly abridged",
            "frame-level matching",
            "opera Der Freischütz",
            "flexibility",
            "robustness"
        ],
        "content": "FRAME-LEVEL AUDIO SEGMENTATION FOR ABRIDGED MUSICAL\nWORKS\nThomas Pr ¨atzlich, Meinard M ¨uller\nInternational Audio Laboratories Erlangen\n{thomas.praetzlich, meinard.mueller} @audiolabs-erlangen.de\nABSTRACT\nLarge-scale musical works such as operas may last sev-\neral hours and typically involve a huge number of mu-sicians. For such compositions, one often ﬁnds differ-ent arrangements and abridged versions (often lasting lessthan an hour), which can also be performed by smaller en-sembles. Abridged versions still convey the ﬂavor of themusical work containing the most important excerpts andmelodies. In this paper, we consider the task of automati-cally segmenting an audio recording of a given version intosemantically meaningful parts. Following previous work,the general strategy is to transfer a reference segmentationof the original complete work to the given version. Ourmain contribution is to show how this can be accomplishedwhen dealing with strongly abridged versions. To this end,opposed to previously suggested segment-level matchingprocedures, we adapt a frame-level matching approach fortransferring the reference segment information to the un-known version. Considering the opera “Der Freisch ¨utz” as\nan example scenario, we discuss how to balance out ﬂex-ibility and robustness properties of our proposed frame-level segmentation procedure.\n1. INTRODUCTION\nOver the years, many musical works have seen a greatnumber of reproductions, ranging from reprints of thesheet music to various audio recordings of performances.For many works this has led to a wealth of co-existing ver-sions including arrangements, adaptations, cover versions,and so on. Establishing semantic correspondences betweendifferent versions and representations is an important stepfor many applications in Music Information Retrieval. Forexample, when comparing a musical score with an audioversion, the goal is to compute an alignment between mea-sures or notes in the score and points in time in the au-dio version. This task is motivated by applications such asscore following [1], where the score can be used to navi-gate through a corresponding audio version and vice versa.The aligned score information can also be used to param-eterize an audio processing algorithm such as in score-\nc/circlecopyrtThomas Pr ¨atzlich, Meinard M ¨uller.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Thomas Pr ¨atzlich, Meinard M ¨uller.\n“Frame-Level Audio Segmentation for Abridged Musical Works”, 15thInternational Society for Music Information Retrieval Conference, 2014.Act I \n Overture Act II Act III \nAbridged version \nAnnotation of complete reference version \nTransfer of segmentation and label information \nFigure 1. Illustration of the proposed method. Given the\nannotated segments on a complete reference version of amusical work, the task is to transfer the segment informa-tion to an abridged version.\ninformed source separation [4, 12]. When working with\ntwo audio versions, alignments are useful for comparingdifferent performances of the same piece of music [2,3]. Incover song identiﬁcation, alignments can be used to com-pute the similarity between two recordings [11]. Align-ment techniques can also help to transfer meta data andsegmentation information between recordings. In [7], anunknown recording is queried against a database of musicrecordings to identify a corresponding version of the samemusical work. After a successful identiﬁcation, alignmenttechniques are used to transfer the segmentation given inthe database to the unknown recording.\nA similar problem was addressed in previous work,\nwhere the goal was to transfer a labeled segmentation ofa reference version onto an unknown version of the samemusical work [10]. The task was approached by a segment-level matching procedure, where one main assumption wasthat a given reference segment either appears more or lessin the same form in the unknown version or is omitted com-pletely.\nIn abridged versions of an opera, however, this assump-\ntion is often not valid. Such versions strongly deviate fromthe original by omitting a large portion of the musical ma-terial. For example, given a segment in a reference ver-sion, one may no longer ﬁnd the start or ending sections ofthis segment in an unknown version, but only an intermedi-ate section. Hence, alignment techniques that account forstructural differences are needed. In [5], a music synchro-nization procedure accounting for structural differences inrecordings of the same piece of music is realized with anadaption of the Needleman-Wunsch algorithm. The algo-rithm penalizes the skipping of frames in the alignmentby adding an additional cost value for each skipped frame.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n307Thus, the cost for skipping a sequence of frames is depen-\ndent on the length of the sequence. In abridged versions,however, omission may occur on an arbitrary scale, rang-ing from several musical measures up to entire scenes ofan opera. In such a scenario, a skipping of long sequencesshould not be more penalized as a skipping of short se-quences. In this work, we will therefore use a differentalignment strategy.\nIn this paper, we address the problem of transferring a\nlabeled reference segmentation onto an unknown versionin the case of abridged versions, see Figure 1. As our maincontribution, we show how to approach this task with aframe-level matching procedure, where correspondencesbetween frames of a reference version and frames of anunknown version are established. The labeled segment in-formation of the reference version is then transferred to theunknown version only for frames for which a correspon-dence has been established. Such a frame-level procedureis more ﬂexible than a segment-level procedure. However,on the downside, it is less robust. As a further contribution,we show how to stabilize the robustness of the frame-levelmatching approach while preserving most of its ﬂexibility.\nThe remainder of this paper is structured as follows:\nIn Section 2, we discuss the relevance of abridged mu-sic recordings and explain why they are problematic in astandard music alignment scenario. In Section 3, we re-view the segment-level matching approach from previouswork (Section 3.2), and then introduce the proposed frame-level segmentation pipeline (Section 3.3). Subsequently,we present some results of a qualitative (Section 4.2) and aquantitative (Section 4.3) evaluation and conclude the pa-per with a short summary (Section 5).\n2. MOTIV ATION\nFor many musical works, there exists a large number ofdifferent versions such as cover songs or different perfor-mances in classical music. These versions can vary greatlyin different aspects such as the instrumentation or the struc-ture. Large-scale musical works such as operas usuallyneed a huge number of musicians to be performed. Forthese works, one often ﬁnds arrangements for smaller en-sembles or piano reductions. Furthermore, performancesof these works are usually very long. Weber’s opera “DerFreisch ¨utz”, for example, has an average duration of about\ntwo hours. Taking it to an extreme, Wagner’s epos “DerRing der Nibelungen”, consists of four operas having anoverall duration of about 15 hours. For such large-scalemusical works, one often ﬁnds abridged versions. Theseversions usually present the most important material ofa musical work in a strongly shortened and structurallymodiﬁed form. Typically, these structural modiﬁcationsinclude omissions of repetitions and other “non-essential”musical passages. Abridged versions were very commonin the early recording days due to space constraints of thesound carriers. The opera “Der Freisch ¨utz” would have\nﬁlled 18 discs on a shellac record. More recently, abridgedversions or excerpts of a musical work can often be foundas bonus tracks on CD records. In a standard alignment \n \n \n Unknown \nRecording Reference \nAudio & Annotation \nAlignment Label Transfer Label Function \n \n  \n߮௘ \n Filter \n߮௘ \n ߮௥ \n \nFigure 2. Illustration of the proposed frame-level segmen-\ntation pipeline. A reference recording with a reference la-bel function ϕ\nris aligned with an unknown version. The\nalignment Lis used to transfer ϕrto the unknown version\nyieldingϕe.\nscenario, abridged versions are particularly problematic asthey omit material on different scales, ranging from theomission of several musical measures up to entire parts.\n3. METHODS\nIn this section, we show how one can accomplish the taskof transferring a given segmentation of a reference version,sayX, onto an unknown version, say Y. The general idea\nis to use alignment techniques to ﬁnd corresponding partsbetweenXandY, and then to transfer on those parts the\ngiven segmentation from XtoY.\nAfter introducing some basic notations on alignments\nand segmentations (Section 3.1), we review the segment-level matching approach from our previous work (Section3.2). Subsequently, we introduce our frame-level segmen-tation approach based on partial matching (Section 3.3).\n3.1 Basic Notations\n3.1.1 Alignments, Paths, and MatchesLet[1 :N]: ={1,2,...,N}be an index set represent-\ning the time line of a discrete signal or feature sequence\nX=(x\n1,x2,...,x N). Similarly, let [1 :M]be the time\nline of a second sequence Y=(y1,...,y M).A n align-\nment between two time lines [1 :N]and[1 :M]is mod-\neled as a set L=(p1,...,p L)⊆[1 :N]×[1 :M].\nAn element p/lscript=(n/lscript,m/lscript)∈L is called a cell and en-\ncodes a correspondence between index n/lscript∈[1 :N]of\nthe ﬁrst time line and index m/lscript∈[1 :M]of the second\none. In the following, we assume Lto be in lexicographic\norder.Lis called a match if(p/lscript+1−p/lscript)∈N×Nfor\n/lscript∈[1 :L−1]. Note that this condition implies strict\nmonotonicity and excludes the possibility to align an index\nof the ﬁrst time line with many indices of the other and viceversa. An alignment can also be constrained by requiring(p\n/lscript+1−p/lscript)∈Σfor a given set Σof admissible step sizes.\nA typical choice for this set is Σ={(1, 1),(1,0),(0,1)},\nwhich allows to align an index of one time line to manyindices of another, and vice versa. Sometimes other setssuch asΣ={(1, 1),(1,2),(2,1)}are used to align se-\nquences which are assumed to be structurally and tempo-rally mostly consistent. If Lfulﬁlls a given step size con-\ndition,P=Lis called a path. Note that alignments that\nfulﬁllΣ\n1andΣ2are both paths, but only an alignment ful-\nﬁllingΣ2is also a match.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3083.1.2 Segments and Segmentation\nWe formally deﬁne a segment to be a set α=[s:t]⊆\n[1 :N]speciﬁed by its start index sand its end index t. Let\n|α|:=t−s+1 denote the length of α. We deﬁne a (partial)\nsegmentation of sizeKto be a set A:={α1,α2,...,α K}\nof pairwise disjoint segments: αk∩αj=∅fork,j∈\n[1 :K],k/negationslash=j.\n3.1.3 LabelingLet[0 :K]be a set of labels. The label 0plays a spe-\ncial role and is used to label everything that has not been\nlabeled otherwise. A label function ϕmaps each index\nn∈[1 :N]to a label k∈[0 :K]:\nϕ:[ 1:N]→[0 :K].\nThe pair ([1 :N],ϕ)is called a labeled time line. Let\nn∈[1 :N]be an index, α=[s:t]be a segment, and\nk∈[0 :K]be a label. Then the pair (n,k)is called\nalabeled index and the pair (α,k)a labeled segment. A\nlabeled segment (α,k)induces a labeling of all indices\nn∈α. LetA:={α\n1,α2,...,α K}be a segmentation\nof[1 :N]and[0 :K]be the label set. Then the set\n{(αk,k)|k∈[1 :K]}is called a labeled segmentation\nof[1 :N]. From a labeled segmentation one obtains a la-\nbel function on [1 :N]by setting ϕ(n): =k forn∈αk\nandϕ(n): =0 forn∈[1 :N]\\/uniontext\nk∈[1:K ]αk. Vice versa,\ngiven a label function ϕ, one obtains a labeled segmenta-\ntion in the following way. We call consecutive indices withthe same label a run. A segmentation of [1 :N]is then de-\nrived by considering runs of maximal length. We call thissegmentation the segmentation induced by ϕ.\n3.2 Segment-Level Matching Approach\nThe general approach in [10] is to apply segment-level\nmatching techniques based on dynamic time warping(DTW) to transfer a labeled reference segmentation to anunknown version. Given a labeled segmentation Aof\nX, eachα\nk∈A is used as query to compute a ranked\nlist of matching candidates in Y. The matching candi-\ndates are derived by applying a subsequence variant ofthe DTW algorithm using the step size conditions Σ=\n{(1,1),(1,2),(2,1)}, see [8, Chapter 5]. The result of the\nsubsequence DTW procedure is a matching score and analignment path P=(p\n1,...,p L)withp/lscript=(n/lscript,m/lscript).P\nencodes an alignment of the segment αk:= [n 1:nL]⊆\n[1 :N]and the corresponding segment [m1:mL]⊆\n[1 :M]inY. To derive a ﬁnal segmentation, one seg-\nment from each matching candidate list is chosen such thatthe sum of the alignment scores of all chosen segments ismaximized by simultaneously fulﬁlling the following con-straints. First, the chosen segments have to respect the tem-poral order of the reference segmentation and second, nooverlapping segments are allowed in the ﬁnal segmenta-tion. Furthermore, the procedure is adapted to be robustto tuning differences of individual segments, see [10] forfurther details.\n  \n1700 1800 19005300540055005600\n00.51Time in seconds\nTime in seconds\n  \n1700 1800 19005300540055005600\n00.51Time in seconds\nTime in seconds\n  \n1700 1800 19005300540055005600\n00.51Time in seconds\nTime in seconds\n  \n1700 1800 19005300540055005600\n00.51Time in seconds\nTime in seconds\nFigure 3. Excerpt of similarity matrices of the referenceKle1973 andKna1939 before (top) and after enhance-\nment (bottom), shown without match (left) and with match(right).\n3.3 Frame-Level Segmentation Approach\nThe basic procedure of our proposed frame-level segmen-\ntation is sketched in Figure 2. First, we use a partial match-\ning algorithm (Section 3.3.1) to compute an alignment L.\nUsingLand the reference label function ϕ\nrobtained from\nthe reference annotation AofX,a n induced label function\nϕeto estimate the labels on Yis derived (Section 3.3.2).\nFinally, we apply a mode ﬁlter (Section 3.3.3) and a ﬁllingup strategy (Section 3.3.4) to derive the ﬁnal segmentationresult.\n3.3.1 Partial Matching\nNow we describe a procedure for computing a partial\nmatching between two sequences as introduced in [8]. Tocompare the two feature sequences XandY, we com-\npute a similarity matrix S(n,m): =s(x\nn,ym), wheres\nis a suitable similarity measure. The goal of the partialmatching procedure is to ﬁnd a score-maximizing matchthrough the matrix S. To this end, we deﬁne the accu-\nmulated score matrix DbyD(n,m): =m a x {D(n−\n1,m),D(n,m−1),D(n−1,m−1) +S(n,m)}with\nD(0,0) :=D(n,0) :=D(0,m): =0 for1≤n≤N\nand1≤m≤M. The score maximizing match can then\nbe derived by backtracking through D, see [8, Chapter 5].\nNote that only diagonal steps contribute to the accumulatedscore inD. The partial matching algorithm is more ﬂexi-\nble in aligning two sequences than the subsequence DTWapproach, as it allows for skipping frames at any point inthe alignment. However, this increased ﬂexibility comesat the cost of loosing robustness. To improve the robust-ness, we apply path-enhancement (smoothing) on S, and\nsuppress other noise-like structures by thresholding tech-niques [9, 11]. In this way, the algorithm is less likely toalign small scattered fragments. Figure 3 shows an excerptof a similarity matrix before and after path-enhancementtogether with the computed matches.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3093.3.2 Induced Label Function\nGiven a labeled time line ([1 :N],ϕr)and an alignment\nL, we derive a label function ϕeon[1 :M]by setting:\nϕe(m): =/braceleftBigg\nϕr(n) if(n,m)∈L\n0 else,\nform∈[1 :M]. See Figure 4 for an illustration.\n3.3.3 Local Mode FilteringThe framewise transfer of the labels may lead to very\nshort and scattered runs. Therefore, to obtain longerruns and a more homogeneous labeling, especially atsegment boundaries, we introduce a kind of smoothingstep by applying a mode ﬁlter. The mode of a se-\nquenceS=(s\n1,s2,...,s N)is the most frequently ap-\npearing value and is formally deﬁned by mode(S): =\nargmaxs∈S|{n∈[1 :N]:sn=s}|.Alocal mode ﬁlter\nof length L=2q+1 withq∈Nreplaces each element\nsn∈S ,n∈[1 :N], in a sequence by the mode of its\nneighborhood (sn−q,...,s n+q):\nmodeﬁlt q(S)(n): =m o d e ( sn−q,...,s n+q).\nNote that the mode may not be unique. In this case, we ap-ply the following strategy in the mode ﬁlter. If the elements\nnis one of the modes, snis left unmodiﬁed by the ﬁlter.\nOtherwise, one of the modes is chosen arbitrarily.\nIn our scenario, we apply the local mode ﬁlter on a la-\nbeled time line ([1 :N],ϕe)by inputting the sequence\nϕe([1 :N]) := (ϕ e(1),ϕe(2),...,ϕ e(N))) into the ﬁl-\nter, see Figure 4 for an illustration. The reason to use themode opposed to the median to ﬁlter segment labels, is thatlabels are nominal data and therefore have no ordering (in-teger labels were only chosen for the sake of simplicity).\n3.3.4 From Frames to Segments (Filling Up)\nIn the last step, we derive a segmentation from the label\nfunctionϕ\ne. As indicated in Section 3.1.3, we could sim-\nply detect maximal runs and consider them as segments.However, even after applying the mode ﬁlter, there maystill be runs sharing the same label that are interrupted bynon-labeled parts (labeled zero). In our scenario, we as-sume that all segments have a distinct label and occur inthe same succession as in the reference. Therefore, in thecase of a sequence of equally labeled runs that are inter-rupted by non-labeled parts, we can assume that the runsbelong to the same segment. Formally, we assign an in-dex in between two indices with the same label (excludingthe zero label) to belong to the same segment as these in-dices. To construct the ﬁnal segments, we iterate over eachk∈[1 :K]and construct the segments α\nk=[sk:ek],\nsuch that sk=m i n{m∈[1 :M]:ϕ(m)=k}, and\nek=m a x{m∈[1 :M]:ϕ(m)=k}, see Figure 4 for an\nexample.\n4. EV ALUATION\nIn this section, we compare the previous segment-levelmatching procedure with our novel frame-level segmenta-1 1 1 0 1 2 2 2 2 2 0 0 0 2 3 3 0 0 3 3 \n1 1 1 1 1 2 2 2 2 2 0 0 0 0 3 3 0 0 3 3 \n1 1 1 1 1 2 2 2 2 2 0 0 0 0 3 3 3 3 3 3 1 1 1 1 1 1 2 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 3 3 3 \n1 1 1 1 2 2 2 2 2 2 0 0 0 0 3 3 3 3 3 0 modefilt ଵ(߮௘)\nfilling up(a)\n(b)\n(c)\n(d)\n(e)߮௥ \n߮௘ \n߮௔ \nFigure 4. Example of frame-level segmentation. The ar-rows indicate the match between the reference version andthe unknown version. (a): Reference label function. (b):\nInduced label function. (c): Mode ﬁltered version of (b)\nwith length L=3 .(d): Filling up on (c). (e): Ground\ntruth label function.\ntion approach based on experiments using abridged ver-\nsions of the opera “Der Freisch ¨utz”. First we give an\noverview of our test set and the evaluation metric (Sec-tion 4.1). Subsequently, we discuss the results of thesegment-level approach and the frame-level procedure onthe abridged versions (Section 4.2). Finally, we presentan experiment where we systematically derive syntheticabridged versions from a complete version of the opera(Section 4.3).\n4.1 Tests Set and Evaluation Measure\nIn the following experiments, we use the recording of Car-\nlos Kleiber performed in 1973 with a duration of 7763\nseconds as reference version. The labeled reference seg-mentation consists of 38musical segments, see Figure 5.\nFurthermore, we consider ﬁve abridged versions that wererecorded between 1933 and 1994. The segments of theopera that are performed in these versions are indicated byFigure 5. Note that the gray parts in the ﬁgure correspondto dialogue sections in the opera. In the following exper-iments, the dialogue sections are considered in the sameway as non-labeled (non-musical) parts such as applause,noise or silence. In the partial matching algorithm, they areexcluded from the reference version (by setting the simi-larity score in these regions to minus inﬁnity), and in thesegment-level matching procedure, the dialogue parts arenot used as queries.\nThroughout all experiments, we use CENS features\nwhich are a variant of chroma features. They are com-puted with a feature rate of 1Hz (derived from 10 Hz pitch\nfeatures with a smoothing length of 41 frames and a down-sampling factor of 10), see [8]. Each feature vector coversroughly4.1seconds of the original audio.\nIn our subsequent experiments, the following segment-\nlevel matching (M4) and frame-level segmentation (F1–F4) approaches are evaluated:(M4) – Previously introduced segment-level matching, see\nSection 3.2 and [10] for details.(F1) – Frame-level segmentation using a similarity matrix\ncomputed with the cosine similarity sdeﬁned by s(x,y)=\n/angbracketleftx,y/angbracketrightfor features xandy, see Section 3.3.\n(F2) – Frame-level segmentation using a similarity matrix\nwith enhanced path structures using the SM Toolbox [9].For the computation of the similarity matrix, we used for-ward/backward smoothing with a smoothing length of 20\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3101M 4M1D 6D18M 27M 38MSch1994Ros1956Mor1939Kri1933Kna1939Kle1973\nFigure 5. Visualization of relative lengths of the abridged\nversions compared to the reference version Kle1973.\nThe gray segments indicate dialogues whereas the coloredsegments are musical parts.\nframes (corresponding to 20 seconds) with relative tempi\nbetween0.5−2, sampled in 15 steps. Afterwards, a thresh-\nolding technique that retained only 5% of the highest val-\nues in the similarity matrix and a scaling of the remainingvalues to [0,1]is applied. For details, we refer to [9] and\nSection 3.3.(F3) – The same as in F2 with a subsequent mode ﬁltering\nusing a ﬁlter length L=2 1 frames, see Section 3.3.3 for\ndetails.(F4) – The segmentation derived from F3 as described in\nSection 3.3.4.\n4.1.1 Frame Accuracy\nTo evaluate the performance of the different segmentation\napproaches, we calculate the frame accuracy, which is de-\nﬁned as the ratio of correctly labeled frames and the totalnumber of frames in a version. Given a ground truth labelfunctionϕ\naand an induced label function ϕe, the frame\naccuracy Afis computed as following:\nAf:=/summationtext\nk∈[0:K ]/vextendsingle/vextendsingleϕa−1(k)∩ϕe−1(k)/vextendsingle/vextendsingle\n/summationtext\nk∈[0:K ]|ϕa−1(k)|\nWe visualize the accuracy by means of an agreement se-\nquenceΔ(ϕa,ϕe)which we deﬁne as Δ(ϕa,ϕe)(m): =\n1(white) if ϕa(m)=ϕ e(m) andΔ(ϕa,ϕe)(m): =0\n(black) otherwise. The sequences Δ(ϕa,ϕe)visually cor-\nrelates well with the values of the frame accuracy Af, see\nTable 1 and the Figure 6. Note that in structural segmenta-\ntion tasks, it is common to use different metrics such as thepairwise precision, recall, and f-measure [6]. These met-rics disregard the absolute labeling of a frame sequence byrelating equally labeled pairs of frames in an estimate toequally labeled frames in a ground truth sequence. How-ever, in our scenario, we want to consider frames that aredifferently labeled in the ground truth and the induced la-bel function as wrong. As the pairwise f-measure showedthe same tendencies as the frame accuracy (which can beeasily visualized), we decided to only present the frameaccuracy values.\n4.2 Qualitative Evaluation\nIn this section, we qualitatively discuss the results of our\napproach in more detail by considering the evaluation ofthe version Kna1939. For each of the ﬁve approaches,\nthe results are visualized in a separate row of Figure 6,(M4)\n  \n0 200 400 600 800 1000 1200 1400 1600 1800ϕa\nϕe\nΔ\n(F1)\n  \n0 200 400 600 800 1000 1200 1400 1600 1800ϕa\nϕe\nΔ\n(F2)\n  \n0 200 400 600 800 1000 1200 1400 1600 1800ϕa\nϕe\nΔ\n(F3)\n  \n0 200 400 600 800 1000 1200 1400 1600 1800ϕa\nϕe\nΔ\n(F4)\n  \n0 200 400 600 800 1000 1200 1400 1600 1800ϕa\nϕe\nΔ\nTime in seconds\nFigure 6. Segmentation results on Kna1939 showing the\nground truth label function ϕa, the induced label function\nϕe, and the agreement sequence Δ: =Δ( ϕa,ϕe). White\nencodes an agreement and black a disagreement betweenϕ\naandϕe.(M4),(F1),(F2),(F3),(F4): See Section 4.1.\nshowing the ground truth ϕa, the induced label function\nϕeand the agreement sequence Δ(ϕa,ϕe).\nForKna1939, the segment-level matching approach\nM4 does not work well. Only 28% of the frames are la-\nbeled correctly. The red segment, for example, at around1500 seconds is not matched despite the fact that it has\nroughly the same overall duration as the correspondingsegment in the reference version, see Figure 5. Undercloser inspection, it becomes clear that it is performedslower than the corresponding segment in the referenceversion, and that some material was omitted at the start,in the middle and the end of the segment. The frame-levelmatching approach F1 leads to an improvement, having aframe accuracy of A\nf=0.520. However, there are still\nmany frames wrongly matched. For example, the overtureof the opera is missing in Kna1939, but frames from the\noverture (yellow) of the reference are matched into a seg-ment from the ﬁrst act (green), see Figure 6. Consideringthat the opera consists of many scenes with harmonicallyrelated material and that the partial matching allows forskipping frames at any point in the alignment, it sometimesoccurs that not the semantically corresponding frames arealigned, but harmonically similar ones. This problem isbetter addressed in approach F2, leading to an improvedframe accuracy of 0.788. The enhancement of path struc-\ntures in the similarity matrix in this approach leads to anincreased robustness of the partial matching. Now, all highsimilarity values are better concentrated in path structuresof the similarity matrix.\nAs a result, the algorithm is more likely to follow se-\nquences of harmonically similar frames, see also Figure 3.However, to follow paths that are not perfectly diagonal,the partial matching algorithm needs to skip frames in thealignment, which leads to a more scattered label function.This is approached by F3 which applies a mode ﬁlter onthe label function from F2, resulting in an improved frame\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n311dur. (s) M4 F1 F2 F3 F4\nKna1939 1965 0.283 0.520 0.788 0.927 0.934\nKri1933 1417 0.390 0.753 0.777 0.846 0.870\nMor1939 1991 0.512 0.521 0.748 0.841 0.919\nRos1956 2012 0.887 0.749 0.817 0.850 0.908\nSch1994 2789 0.742 0.895 0.936 0.986 0.989\nmean 2035 0.563 0.687 0.813 0.890 0.924\nTable 1. Frame accuracy values on abridged versions. M4:\nSegment-level matching, F1: Frame-level segmentation,F2: Frame-level segmentation with path-enhanced similar-ity matrix, F3: Mode ﬁltering with L=2 1 seconds on F2.\nF4: Derived Segmentation on F4.\naccuracy of 0.927. In F4, the remaining gaps in the label\nfunction of F3 are ﬁlled up, which leads to a frame accu-\nracy of0.934.\n4.3 Quantitative Evaluation\nIn this section, we discuss the results of Table 1. Note that\nall abridged versions have less than 50% of the duration\nof the reference version (7763 seconds). From the mean\nframe accuracy values for all approaches, we can concludethat the segment-level matching (0. 563) is not well suited\nfor dealing with abridged versions, whereas the differentstrategies in the frame-level approaches F1 (0. 687)–F 4\n(0.924) lead to a subsequent improvement of the frame ac-\ncuracy. Using the segment-level approach, the frame ac-curacies for the versions Ros1956 (0.887) and Sch1994\n(0.742) stand out compared to the other versions. The seg-\nments that are performed in these versions are not short-ened and therefore largely coincide with the segments ofthe reference version. This explains why the segment-levelmatching still performs reasonably well on these versions.\nIn Figure 7, we show the frame accuracy results for\nthe approaches M4 and F4 obtained from an experimenton a set of systematically constructed abridged versions.The frame accuracy values at 100% correspond to a subset\nof10segments (out of 38) that were taken from a com-\nplete recording of the opera “Der Freisch ¨utz” recorded by\nKeilberth in 1958. From this subset, we successively re-moved10% of the frames from each segment by remov-\ning5% of the frames at the start, and 5% of the frames\nat the end sections of the segments. In the last abridgedversion, only 10% of each segment remains. This exper-\niment further supports the conclusions that the segment-level approach is not appropriate for dealing with abridgedversions, whereas the frame-level segmentation approachstays robust and ﬂexible even in the case of strong abridg-ments.\n5. CONCLUSIONS\nIn this paper, we approached the problem of transferringthe segmentation of a complete reference recording onto anabridged version of the same musical work. We comparedthe proposed frame-level segmentation approach based onpartial matching with a segment-level matching strategy.In experiments with abridged recordings, we have shownthat our frame-level approach is robust and ﬂexible when100% 90% 80% 70% 60% 50% 40% 30% 20% 10%00.20.40.60.81\n  \n(M4) Segment-level\n(F4) Frame-levelAf\nPercentage of remaining material per segment\nFigure 7. Performance of segment-level approach (M4)versus frame-level approach (F4) on constructed abridgedversions. See Section 4.3\nenhancing the path structure of the used similarity matrix\nand applying a mode ﬁlter on the labeled frame sequencebefore deriving the ﬁnal segmentation.\nAcknowledgments: This work has been supported by\nthe BMBF project Freisch ¨utz Digital (Funding Code\n01UG1239A to C). The International Audio Laboratories\nErlangen are a joint institution of the Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg (FAU) and Fraunhofer IIS.\n6. REFERENCES\n[1] Roger B. Dannenberg and Ning Hu. Polyphonic audio matching for\nscore following and intelligent audio editors. In Proceedings of the In-\nternational Computer Music Conference (ICMC), pages 27–34, San\nFrancisco, USA, 2003.\n[2] Simon Dixon and Gerhard Widmer. MA TCH: A music alignment tool\nchest. In Proceedings of the International Conference on Music Infor-\nmation Retrieval (ISMIR), London, GB, 2005.\n[3] Sebastian Ewert, Meinard M ¨uller, V erena Konz, Daniel M ¨ullensiefen,\nand Geraint Wiggins. Towards cross-version harmonic analysis ofmusic. IEEE Transactions on Multimedia, 14(3):770–782, 2012.\n[4] Sebastian Ewert, Bryan Pardo, Meinard M ¨uller, and Mark D. Plumb-\nley. Score-informed source separation for musical audio recordings:An overview. IEEE Signal Processing Magazine, 31(3):116–124,\nMay 2014.\n[5] Maarten Grachten, Martin Gasser, Andreas Arzt, and Gerhard Wid-\nmer. Automatic alignment of music performances with structural dif-ferences. In ISMIR, pages 607–612, 2013.\n[6] Hanna Lukashevich. Towards quantitative measures of evaluating\nsong segmentation. In Proceedings of the International Conference\non Music Information Retrieval (ISMIR), pages 375–380, Philadel-phia, USA, 2008.\n[7] Nicola Montecchio, Emanuele Di Buccio, and Nicola Orio. An efﬁ-\ncient identiﬁcation methodology for improved access to music her-itage collections. Journal of Multimedia, 7(2):145–158, 2012.\n[8] Meinard M ¨uller. Information Retrieval for Music and Motion .\nSpringer V erlag, 2007.\n[9] Meinard M ¨uller, Nanzhu Jiang, and Harald Grohganz. SM Toolbox:\nMA TLAB implementations for computing and enhancing similiartymatrices. In Proceedings of the AES Conference on Semantic Audio,\nLondon, GB, 2014.\n[10] Thomas Pr ¨atzlich and Meinard M ¨uller. Freisch ¨utz digital: A case\nstudy for reference-based audio segmentation of operas, to appear. InProceedings of the International Conference on Music InformationRetrieval (ISMIR), pages 589–594, Curitiba, Brazil, 2013.\n[11] Joan Serr `a, Emilia G ´omez, Perfecto Herrera, and Xavier Serra.\nChroma binary similarity and local alignment applied to cover songidentiﬁcation. IEEE Transactions on Audio, Speech and Language\nProcessing, 16:1138–1151, 2008.\n[12] John Woodruff, Bryan Pardo, and Roger B. Dannenberg. Remixing\nstereo music with score-informed source separation. In Proceedings\nof the International Conference on Music Information Retrieval (IS-MIR), pages 314–319, Victoria, Canada, 2006.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n312"
    },
    {
        "title": "MuSe: A Music Recommendation Management System.",
        "author": [
            "Martin Przyjaciel-Zablocki",
            "Thomas Hornung 0001",
            "Alexander Schätzle",
            "Sven Gauß",
            "Io Taxidou",
            "Georg Lausen"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418195",
        "url": "https://doi.org/10.5281/zenodo.1418195",
        "ee": "https://zenodo.org/records/1418195/files/Przyjaciel-ZablockiHSGTL14.pdf",
        "abstract": "Evaluating music recommender systems is a highly repet- itive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE – a music recommen- dation management system – for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off- the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating on- line streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender al- gorithms can be plugged in comfortably and evaluations can be configured and managed online.",
        "zenodo_id": 1418195,
        "dblp_key": "conf/ismir/Przyjaciel-ZablockiHSGTL14",
        "keywords": [
            "Evaluating music recommender systems",
            "repetitive yet non-trivial task",
            "immediate evaluation by listening",
            "MUSE system",
            "off-the-shelf evaluation algorithms",
            "online evaluation system",
            "automatic reporting",
            "legal evaluation in real time",
            "built-in user management system",
            "privacy standards"
        ],
        "content": "MUSE: A MUSIC RECOMMENDATION MANAGEMENT SYSTEM\nMartin Przyjaciel-Zablocki, Thomas Hornung, Alexander Sch ¨atzle,\nSven Gauß, Io Taxidou, Georg Lausen\nDepartment of Computer Science, University of Freiburg\nzablocki,hornungt,schaetzle,gausss,taxidou,lausen@informatik.uni-freiburg.de\nABSTRACT\nEvaluating music recommender systems is a highly repet-\nitive, yet non-trivial, task. But it has the advantage over\nother domains that recommended songs can be evaluated\nimmediately by just listening to them.\nIn this paper, we present M USE– a music recommen-\ndation management system – for solving the typical tasks\nof an in vivo evaluation. M USEprovides the typical off-\nthe-shelf evaluation algorithms, offers an online evaluation\nsystem with automatic reporting, and by integrating on-\nline streaming services also a legal possibility to evaluate\nthe quality of recommended songs in real time. Finally, it\nhas a built-in user management system that conforms with\nstate-of-the-art privacy standards. New recommender al-\ngorithms can be plugged in comfortably and evaluations\ncan be conﬁgured and managed online.\n1. INTRODUCTION\nOne of the hallmarks of a good recommender system is a\nthorough and signiﬁcant evaluation of the proposed algo-\nrithm(s) [6]. One way to do this is to use an ofﬂine dataset\nlikeThe Million Song Dataset [1] and split some part of\nthe data set as training data and run the evaluation on top\nof the remainder of the data. This approach is meaning-\nful for features that are already available for the dataset,\nsuch as e.g. tag prediction for new songs. However, some\naspects of recommending songs are inherently subjective,\nsuch as serendipity [12], and thus the evaluation of such\nalgorithms can only be done in vivo, i.e. with real users\nnot in an artiﬁcial environment.\nWhen conducting an in vivo evaluation, there are some\ntypical issues that need to be considered:\nUser management. While registering for evaluations, users\nshould be able to provide some context information about\nthem to guide the assignment in groups for A/B testing.\nPrivacy & Security. User data is highly sensitive, and\nhigh standards have to be met wrt. who is allowed to access\nc\rMartin Przyjaciel-Zablocki, Thomas Hornung, Alexan-\nder Sch ¨atzle, Sven Gauß, Io Taxidou, Georg Lausen.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Martin Przyjaciel-Zablocki, Thomas\nHornung, Alexander Sch ¨atzle, Sven Gauß, Io Taxidou, Georg Lausen.\n“MuSe: A Music Recommendation Management System”, 15th Interna-\ntional Society for Music Information Retrieval Conference, 2014.the data. Also, an evaluation framework needs to ensure\nthat user data cannot be compromised.\nGroup selection. Users are divided into groups for A/B\ntesting, e.g. based on demographic criteria like age or gen-\nder. Then, recommendations for group A are provided by a\nbaseline algorithm, and for group B by the new algorithm.\nPlaying songs. Unlike other domains, e.g. books, users\ncan give informed decisions by just listening to a song.\nThus, to assess a recommended song, it should be possi-\nble to play the song directly during the evaluation.\nEvaluation monitoring. During an evaluation, it is impor-\ntant to have an overview of how each algorithm performs\nso far, and how many and how often users participate.\nEvaluation metrics. Evaluation results are put into graphs\nthat contain information about the participants and the per-\nformance of the evaluated new recommendation algorithm.\nBaseline algorithms. Results of an evaluation are often\njudged by improvements over a baseline algorithm, e.g. a\ncollaborative ﬁltering algorithm [10].\nIn this paper, we present M USE– a music recommen-\ndation management system – that takes care of all the reg-\nular tasks that are involved in conducting an in vivo eval-\nuation. Please note that M USEcan be used to perform in\nvivo evaluations of arbitrary music recommendation algo-\nrithms. An instance of M USEthat conforms with state-of-\nthe-art privacy standards is accessible by using the link be-\nlow, a documentation is available on the M USEwebsite2.\nmuse.informatik.uni-freiburg.de\nThe remainder of the paper is structured as follows: Af-\nter a discussion of related work in Section 2, we give an\noverview of our proposed music recommendation manage-\nment system in Section 3 with some insights in our evalu-\nation framework in Section 4. Included recommenders are\npresented in Section 5, and we conclude with an outlook\non future work in Section 6.\n2. RELATED WORK\nThe related work is divided in three parts: (1) music based\nframeworks for recommendations, (2) recommenders’ eval-\nuation, (3) libraries and platforms for developing and plu-\ngin recommenders.\nMusic recommendation has attracted a lot of interest\nfrom the scientiﬁc community since it has many real life\napplications and bears multiple challenges. An overview\n2MUSE- Mu sic Se nsing in a Social Context:\ndbis.informatik.uni-freiburg.de/MuSe\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n543of factors affecting music recommender systems and chal-\nlenges that emerge both for the users’ and the recommend-\ners side are highlighted in [17]. Improving music rec-\nommendations has attracted equal attention. In [7, 12],\nwe built and evaluated a weighted hybrid recommender\nprototype that incorporates different techniques for mu-\nsic recommendations. We used Youtube for playing songs\nbut due to a complex process of identifying and matching\nsongs, together with some legal issues, such an approach\nis no longer feasible. Music platforms are often combined\nwith social media where users can interact with objects\nmaintaining relationships. Authors in [2] leverage this rich\ninformation to improve music recommendations by view-\ning recommendations as a ranking problem.\nThe next class of related work concerns evaluation of\nrecommenders. An overview of existing systems and meth-\nods can be found in [16]. In this study, recommenders are\nevaluated based on a set of properties relevant for differ-\nent applications and evaluation metrics are introduced to\ncompare algorithms. Both ofﬂine and online evaluation\nwith real users are conducted, discussing how to draw valu-\nable conclusion. A second review on collaborative recom-\nmender systems speciﬁcally can be found in [10]. It con-\nsists the ﬁrst attempt to compare and evaluate user tasks,\ntypes of analysis, datasets, recommendation quality and\nattributes. Empirical studies along with classiﬁcation of\nexisting evaluation metrics and introduction of new ones\nprovide insights into the suitability and biases of such met-\nrics in different settings. In the same context, researchers\nvalue the importance of user experience in the evaluation\nof recommender systems. In [14] a model is developed\nfor assessing the perceived recommenders quality of users\nleading to more effective and satisfying systems. Similar\napproaches are followed in [3, 4] where authors highlight\nthe need for user-centric systems and high involvement of\nusers in the evaluation process. Relevant to our study is\nthe work in [9] which recognizes the importance for on-\nline user evaluation, while implementing such evaluations\nsimultaneously by the same user in different systems.\nThe last class of related work refers to platforms and\nlibraries for developing and selecting recommenders. The\nauthors of [6] proposed LensKit, an open-source library\nthat offers a set of baseline recommendation algorithms\nincluding an evaluation framework. MyMediaLite [8] is\na library that offers state of the art algorithms for collabo-\nrative ﬁltering in particular. The API offers the possibility\nfor new recommender algorithm’s development and meth-\nods for importing already trained models. Both provide a\ngood foundation for comparing different research results,\nbut without a focus on in vivo evaluations of music rec-\nommenders, thus they don’t offer e.g. capabilities to play\nand rate songs or manage users. A patent in [13] describes\na portal extension with recommendation engines via inter-\nfaces, where results are retrieved by a common recommen-\ndation manager. A more general purpose recommenders\nframework [5] which is close to our system, allows using\nand comparing different recommendation methods on pro-\nvided datasets. An API offers the possibility to develop andincorporate algorithms in the framework, integrate plugins,\nmake conﬁgurations and visualize the results. However,\nour system offers additionally real-time online evaluations\nof different recommenders, while incorporating end users\nin the evaluation process. A case study of using Apache\nMahout, a library for distributed recommenders based on\nMapReduce can be found in [15]. Their study provides in-\nsights into the development and evaluation of distributed\nalgorithms based on Mahout.\nTo the best of our knowledge, this is the ﬁrst system\nthat incorporates such a variety of characteristics and offers\na full solution for music recommenders development and\nevaluation, while highly involving the end users.\n3. MUSE OVERVIEW\nWe propose M USE: a web-based music recommendation\nmanagement system, built around the idea of recommend-\ners that can be plugged in. With this in mind, M USEis\nbased on three main system design pillars:\nExtensibility. The whole infrastructure is highly extensi-\nble, thus new recommendation techniques but also other\nfunctionalities can be added as modular components.\nReusability. Typical tasks required for evaluating music\nrecommendations (e.g. managing user accounts, playing\nand rating songs) are already provided by M USEin ac-\ncordance with current privacy standards.\nComparability. By offering one common evaluation frame-\nwork we aim to reduce side-effects of different systems\nthat might inﬂuence user ratings, improving both compa-\nrability and validity of in-vivo experiments.\nA schematic overview of the whole system is depicted\nin Fig. 1. The M USEServer is the core of our music rec-\nommendation management system enabling the communi-\ncation between all components. It coordinates the inter-\naction with pluggable recommenders, maintains the data\nin three different repositories and serves the requests from\nmultiple M USEclients. Next, we will give some insights\nin the architecture of M USEby explaining the most rele-\nvant components and their functionalities.\n3.1 Web-based User Interface\nUnlike traditional recommender domains like e-commerce,\nwhere the process of consuming and rating items takes up\nto several weeks, recommending music exhibits a highly\ndynamic nature raising new challenges and opportunities\nfor recommender systems. Ratings can be given on the ﬂy\nand incorporated immediately into the recommending pro-\ncess, just by listening to a song. However, this requires\na reliable and legal solution for playing a large variety of\nsongs. M USEbeneﬁts from a tight integration of Spotify3,\na music streaming provider that allows listening to millions\nof songs for free. Thus, recommended songs can be em-\nbedded directly into the user interface, allowing to listen\nand rate them in a user-friendly way as shown in Fig. 2.\n3A Spotify account is needed to play songs\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n544MuSe ClientMuSe Server\nMusic \nRepository\nUser\nProfile\nUser \nContextMusic Retrieval EngineTrack \nManagerCharts\nLast.fm APIWeb Services\nWrapper/  \nConnector  1\nUser Profile EngineUser \nManagerSocial \nConnector  1HTML\nXML\nLast.fm API\nothersSocial Networks\nXML\nHTMLRecommender  Manager\nCoordinator\nREST \nWebserviceEvaluation\nFrameworkRecommendation  \nList BuilderPluggable  Recommender  1\nRecommendation  \nModelRecommender\nRecommender  \nInterfaceRecommender  1\nWeb‐based User Interface\nRecommendation  \nList\nAdministrationSpotify \nConnectorAJAX\nSpotify API JSONData RepositoriesFigure 1. Muse – Music Recommendation Management System Overview\nFigure 2. Songs can be played & rated\nIn order to make sure that users can obtain recommen-\ndations without having to be long-time M USEusers, we\nask for some contextual information during the registra-\ntion process. Each user has to provide coarse-grained de-\nmographic and preference information, namely the user’s\nspoken languages, year of birth, and optionally a Last.fm\nuser name. In Section 5, we will present ﬁve different\napproaches that utilize those information to overcome the\ncold start problem. Beyond that, these information is also\nexploited for dividing users into groups for A/B testing.\nFig. 3 shows the settings pane of a user. Note, that this\nwindow is available only for those users, who are not par-\nticipating in an evaluation. It allows to browse all available\nrecommenders and compare them based on meta data pro-\nvided with each recommender. Moreover, it is also pos-\nsible to control how recommendations from different rec-\nommenders are amalgamated to one list. To this end, a\nsummary is shown that illustrates the interplay of novelty,\naccuracy, serendipity and diversity. Changes are applied\nand reﬂected in the list of recommendations directly.3.2 Data Repositories\nAlthough recommenders in M USEwork independently of\neach other and may even have their own recommendation\nmodel with additional data, all music recommenders have\naccess to three global data structures.\nThe ﬁrst one is the Music Repository that stores songs\nwith their meta data. Only songs in this database can be\nrecommended, played and rated. The Music Retrieval En-\ngine periodically collects new songs and meta data from\nWeb Services, e.g. chart lists or Last.fm. It can be easily\nextended by new sources of information like audio analy-\nsis features from the Million Song Dataset [1], that can be\nrequested periodically or dynamically. Each recommender\ncan access all data stored in the Music Repository.\nThe second repository stores the User Proﬁle, hence\nit also contains personal data. In order to comply with\nGerman data privacy requirements only restricted access is\ngranted for both, recommenders and evaluation analyses.\nThe last repository collects the User Context, e.g. which\nsongs a user has listened to with the corresponding rating\nfor the respective recommender.\nAccess with anonymized user IDs is granted for all rec-\nommenders and evaluation analyses. Finally, both user-\nrelated repositories can be enriched by the User Proﬁle\nEngine that fetches data from other sources like social net-\nworks. Currently, the retrieval of listening proﬁles of pub-\nlicly available data from Last.fm and Facebook is supported.\n3.3 Recommender Manager\nThe Recommender Manager has to coordinate the interac-\ntion of recommenders with users and the access to the data.\nThis process can be summarized as follows:\n\u000fIt coordinates access to the repositories, forwards\nuser request for new recommendations, and receives\ngenerated recommendations.\n\u000fIt composes a list of recommendations by amalga-\nmating recommendations from different recommend-\ners into one list based on individual user settings.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n545Figure 3. Users can choose from available recommenders\n\u000fA panel for administrative users allows enabling, dis-\nabling and adding of recommenders that implement\nthe interface described in Section 3.4. Moreover,\neven composing hybrid recommenders is supported.\n3.4 Pluggable Recommender\nA cornerstone of M USEis its support for plugging in rec-\nommenders easily. The goal was to design a rather simple\nand compact interface enabling other developers to imple-\nment new recommenders with enough ﬂexibility to incor-\nporate existing approaches as well. This is achieved by\na predeﬁned Java interface that has to be implemented for\nany new recommender. It deﬁnes the interplay between the\nMUSERecommender Manager and its pluggable recom-\nmenders by (1) providing methods to access all three data\nrepositories, (2) forwarding requests for recommendations\nand (3) receiving recommended items. Hence, new rec-\nommenders do not have to be implemented within M USE\nin order to be evaluated, it sufﬁces to use the interface to\nprovide a mapping of inputs and outputs4.\n4. EVALUATION FRAMEWORK\nThere are two types of experiments to measure the per-\nformance of recommenders: (1) ofﬂine evaluations based\non historical data and (2) in vivo evaluations where users\ncan evaluate recommendations online. Since music is of\nhighly subjective nature with many yet unknown correla-\ntions, we believe that in vivo evaluations have the advan-\ntage of also capturing subtle effects on the user during the\nevaluation. Since new songs can be rated within seconds\nby a user, such evaluations are a good ﬁt for the music do-\nmain. M USEaddresses the typical issues that are involved\nin conducting an in-vivo evaluation and thus allows re-\nsearches to focus on the actual recommendation algorithm.\nThis section gives a brief overview of how evaluations\nare created, monitored and analyzed.\n4More details can be found on our project website.4.1 Evaluation Setup\nThe conﬁguration of an evaluation consists of three steps\n(cf. Fig. 4): (1) A new evaluation has to be scheduled,\ni.e. a start and end date for the evaluation period has to\nbe speciﬁed. (2) The number and setup of groups for A/B\ntesting has to be deﬁned, where up to six different groups\nare supported. For each group an available recommender\ncan be associated with the possibility of hybrid combina-\ntions of recommenders if desired. (3) The group placement\nstrategy based on e.g. age, gender and spoken languages is\nrequired. As new participants might join the evaluation\nover time, an online algorithm maintains a uniform distri-\nbution with respect to the speciﬁed criteria. After the setup\nis completed, a preview illustrates how group distributions\nwould resemble based on a sample of registered users.\nFigure 4. Evaluation setup via Web interface\nWhile an evaluation is running, both registered users\nand new ones are asked to participate after they login to\nMUSE. If a user joins an evaluation, he will be assigned to\na group based on the placement strategy deﬁned during the\nsetup and all ratings are considered for the evaluation. So\nfar, the following types of ratings can be discerned:\nSong rating. The user can provide three ratings for the\nquality of the recommended song (“love”, “like”, and “dis-\nlike”). Each of these three rating options is mapped to a\nnumerical score internally, which is then used as basis for\nthe analysis of each recommender.\nList rating. The user can also provide ratings for the entire\nlist of recommendations that is shown to him on a ﬁve-\npoint Likert scale, visualized by stars.\nQuestion. To measure other important aspects of a rec-\nommendation like its novelty or serendipity, an additional\nﬁeld with a question can be conﬁgured that contains either\na yes/no button or a ﬁve-point Likert scale.\nThe user may also decide not to rate some of the rec-\nommendations. In order to reduce the number of non-rated\nrecommendations in evaluations, the rating results can only\nbe submitted when at least 50% of the recommendations\nare rated. Upon submitting the rating results, the user gets\na new list with recommended songs.\n4.2 Monitoring Evaluations\nRunning in vivo evaluations as a black box is undesirable,\nsince potential issues might be discovered only after the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n546evaluation is ﬁnished. Also, it is favorable to have an over-\nview of the current state, e.g. if there are enough partici-\npants, and how the recommenders perform so far. M USE\nprovides comprehensive insights via an administrative ac-\ncount into running evaluations as it offers an easy acces-\nsible visualization of the current state with plots. Thus,\nadjustments like adding a group or changing the runtime\nof the evaluation can be made while the evaluation is still\nrunning.\nFigure 5. Evaluation results are visualized dynamically\n4.3 Analyzing Evaluations\nFor all evaluations, including running and ﬁnished ones,\na result overview can be accessed that shows results in a\ngraphical way to make them easier and quicker to grasp\n(c.f. Fig. 5). The plots are implemented in a dynamic fash-\nion allowing to adjust, e.g., the zoom-level or the displayed\ninformation as desired. They include a wide range of met-\nrics like group distribution, number of participants over\ntime, averaged ratings, mean absolute error, accuracy per\nrecommender, etc. Additionally, the complete dataset or\nparticular plotting data can be downloaded in CSV format.\n5. RECOMMENDATION TECHNIQUES\nMUSEcomes with two types of recommenders out-of-the-\nbox. The ﬁrst type includes traditional algorithms, i.e. Con-\ntend Based andCollaborative Filtering [10] that can be\nused as baseline for comparison. The next type of recom-\nmenders is geared towards overcoming the cold start prob-\nlem by (a) exploiting information provided during regis-\ntration (Annual, Country, and City Charts recommender),\nor (b) leveraging knowledge from social networks (Social\nNeighborhood andSocial Tags recommender).\nAnnual Charts Recommender. Studies have shown, that\nthe apex of evolving music taste is reached between theage of 14and20[11]. The Annual Charts Recommender\nexploits this insight and recommends those songs, which\nwere popular during this time. This means, when a user\nindicates 1975 as his year of birth, he will be assigned to\nthe music context of years 1989 to1995, and obtain recom-\nmendations from that context. The recommendation rank-\ning is deﬁned by the charts position in the corresponding\nannual charts, where the following function is used to map\nthe charts position to a score, with csas the position of\nsongsin charts candnis the maximum rank of charts c:\nscore (s) =\u0000log (1\nncs) (1)\nCountry Charts Recommender. Although music taste\nis subject to diversiﬁcation across countries, songs that a\nuser has started to listen to and appreciate oftentimes have\npeaked in others countries months before. This latency as-\npect as well as an inter-country view on songs provide a\ngood foundation for serendipity and diversity. The source\nof information for this recommender is the spoken lan-\nguages, provided during registration, which are mapped to\na set of countries for which we collect the current charts.\nSuppose there is a user awith only one country Aassigned\nto his spoken languages, and CAthe set of charts songs for\nA. Then, the set CRof possible recommendations for ais\ndeﬁned as follows, where L is the set of all countries:\nCR= ([\nX2LCX)nCA\nThe score for a song s2CRis deﬁned by the average\ncharts position across all countries, where Function (1) is\nused for mapping the charts position into a score.\nCity Charts Recommender. While music tastes differ\nacross countries, they may likewise differ across cities in\nthe same country. We exploit this idea by the City Charts\nRecommender, hence it can be seen as a more granular\nvariant of the Country Charts Recommender. The set of\nrecommendations CRis now composed based on the city\ncharts from those countries a user was assigned to. Hereby,\nthe ranking of songs in that set is not only deﬁned by the\naverage charts position, but also by the number of cities\nwhere the song occurs in the charts: The fewer cities a\nsong appears in, the more “exceptional” and thus relevant\nit is.\nSocial Neighborhood Recommender. Social Networks\nare, due to their growing rates, an excellent source for con-\ntextual knowledge about users, which in turn can be uti-\nlized for better recommendations. In this approach, we use\nthe underlying social graph of Last.fm to generate recom-\nmendations based on user’s Last.fm neighborhood which\ncan be retrieved by our User Proﬁle Engine . To compute\nrecommendations for a user a, we select his ﬁve closest\nneighbors, an information that is estimated by Last.fm in-\nternally. Next, for each of them, we retrieve its recent top\n20 songs and thus get ﬁve sets of songs, namely N1:::N 5.\nSince that alone would provide already known songs in\ngeneral, we deﬁne the set NRof possible recommenda-\ntions as follows, where Nais the set of at most 25 songs a\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n547userarecently listened to and appreciated:\nNR= ([\n1\u0014i\u00145Ni)nNa\nSocial Tags Recommender. Social Networks collect an\nenormous variety of data describing not only users but also\nitems. One common way of characterising songs is based\non tags that are assigned to them in a collaborative man-\nner. Our Social Tag Recommender utilizes such tags to\ndiscover new genres which are related to songs a user liked\nin the past. At ﬁrst, we determine his recent top ten songs\nincluding their tags from Last.fm. We merge all those tags\nand ﬁlter out the most popular ones like “rock” or “pop” to\navoid getting only obvious recommendations. By count-\ning the frequency of the remaining tags, we determine the\nthree most common thus relevant ones. For the three se-\nlected tags, we use again Last.fm to retrieve songs where\nthe selected tags were assigned to most frequently.\nTo test our evaluation framework as well as to assess the\nperformance of our ﬁve recommenders we conducted an in\nvivo evaluation with M USE. As a result 48registered users\nrated a total of 1567 song recommendations conﬁrming the\napplicability of our system for in vivo evaluations. Due\nto space limitations, we decided to omit a more detailed\ndiscussion of the results.\n6. CONCLUSION\nMUSEputs the fun back in developing new algorithms for\nmusic recommendations by taking the burden from the re-\nsearcher to spent cumbersome time on programming yet\nanother evaluation tool. The module-based architecture of-\nfers the ﬂexibility to immediately test novel approaches,\nwhereas the web-based user-interface gives control and in-\nsight into running in vivo evaluations. We tested M USE\nwith a case study conﬁrming the applicability and stability\nof our proposed music recommendation management sys-\ntem. As future work, we envision to increase the ﬂexibility\nof setting up evaluations, add more metrics to the result\noverview, and to develop further connectors for social net-\nworks and other web services to enrich the user’s context\nwhile preserving data privacy.\n7. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInISMIR, 2011.\n[2] Jiajun Bu, Shulong Tan, Chun Chen, Can Wang, Hao\nWu, Lijun Zhang 0005, and Xiaofei He. Music rec-\nommendation by uniﬁed hypergraph: combining social\nmedia information and music content. In ACM Multi-\nmedia, pages 391–400, 2010.\n[3] Li Chen and Pearl Pu. User evaluation framework of\nrecommender systems. In Workshop on Social Recom-\nmender Systems (SRS’10) at IUI, volume 10, 2010.[4] Paolo Cremonesi, Franca Garzotto, Sara Negro,\nAlessandro Vittorio Papadopoulos, and Roberto Tur-\nrin. Looking for ”good” recommendations: A compar-\native evaluation of recommender systems. In INTER-\nACT (3), pages 152–168, 2011.\n[5] Aviram Dayan, Guy Katz, Naseem Biasdi, Lior\nRokach, Bracha Shapira, Aykan Aydin, Roland\nSchwaiger, and Radmila Fishel. Recommenders\nbenchmark framework. In RecSys, pages 353–354,\n2011.\n[6] Michael D. Ekstrand, Michael Ludwig, Joseph A. Kon-\nstan, and John Riedl. Rethinking the recommender\nresearch ecosystem: reproducibility, openness, and\nLensKit. In RecSys, pages 133–140, 2011.\n[7] Simon Franz, Thomas Hornung, Cai-Nicolas Ziegler,\nMartin Przyjaciel-Zablocki, Alexander Sch ¨atzle, and\nGeorg Lausen. On weighted hybrid track recommen-\ndations. In ICWE, pages 486–489, 2013.\n[8] Zeno Gantner, Steffen Rendle, Christoph Freuden-\nthaler, and Lars Schmidt-Thieme. Mymedialite: a free\nrecommender system library. In RecSys, pages 305–\n308, 2011.\n[9] Conor Hayes and P ´adraig Cunningham. An on-line\nevaluation framework for recommender systems. Trin-\nity College Dublin, Dep. of Computer Science, 2002.\n[10] Jonathan L. Herlocker, Joseph A. Konstan, Loren G.\nTerveen, and John Riedl. Evaluating collaborative ﬁl-\ntering recommender systems. In ACM Trans. Inf. Syst.,\npages 5–53, 2004.\n[11] Morris B Holbrook and Robert M Schindler. Some\nexploratory ﬁndings on the development of musical\ntastes. Journal of Consumer Research, pages 119–124,\n1989.\n[12] Thomas Hornung, Cai-Nicolas Ziegler, Simon Franz,\nMartin Przyjaciel-Zablocki, Alexander Sch ¨atzle, and\nGeorg Lausen. Evaluating Hybrid Music Recom-\nmender Systems. In WI, pages 57–64, 2013.\n[13] Stefan Liesche, Andreas Nauerz, and Martin Welsch.\nExtendable recommender framework for web-based\nsystems, 2008. US Patent App. 12/209,808.\n[14] Pearl Pu, Li Chen, and Rong Hu. A user-centric evalu-\nation framework for recommender systems. In RecSys\n’11, pages 157–164, New York, NY , USA, 2011. ACM.\n[15] Carlos E Seminario and David C Wilson. Case study\nevaluation of mahout as a recommender platform. In\nRecSys, 2012.\n[16] Guy Shani and Asela Gunawardana. Evaluating recom-\nmendation systems. In Recommender Systems Hand-\nbook, pages 257–297, 2011.\n[17] Alexandra L. Uitdenbogerd and Ron G. van Schyndel.\nA review of factors affecting music recommender suc-\ncess. In ISMIR, 2002.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n548"
    },
    {
        "title": "Verovio: A library for Engraving MEI Music Notation into SVG.",
        "author": [
            "Laurent Pugin",
            "Rodolfo Zitellini",
            "Perry Roland"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417589",
        "url": "https://doi.org/10.5281/zenodo.1417589",
        "ee": "https://zenodo.org/records/1417589/files/PuginZR14.pdf",
        "abstract": "Rendering symbolic music notation is a common compo- nent of many MIR applications, and many tools are avail- able for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a li- brary and toolkit for rendering MEI. A significant ad- vantage of Verovio is that it implements MEI’s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last op- tion is particularly interesting because it provides a com- plete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily address- able, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license.",
        "zenodo_id": 1417589,
        "dblp_key": "conf/ismir/PuginZR14",
        "keywords": [
            "MEI notation",
            "Music Encoding Initiative",
            "Verovio",
            "native rendering",
            "Music research projects",
            "internal structure",
            "command-line tool",
            "library",
            "lightweight tool",
            "Emscripten LLVM-to-JavaScript"
        ],
        "content": "VEROVIO: A LIBRARY FOR ENGRAVING  \nMEI MUSIC NOTATION INTO SVG  \nLaurent Pugin Rodolfo Zitellini Perry Roland \nSwiss RISM Office \nlaurent.pugin@rism -ch.org Swiss RISM Office \nrodolfo.zitellini@rism -ch.org University of Virginia \npdr4h@virginia.edu  \nABSTRACT  \nRendering symbolic music notation is a common comp o-\nnent of many MIR applications, and many tools are avai l-\nable for this task. There is, however, a need for a tool that \ncan natively render the Music E ncoding Initiative (MEI) \nnotation encodings that are increasingly used in music \nresearch projects. In this paper, we present Verovio, a l i-\nbrary and toolkit for rendering MEI. A significant ad-\nvantage of Ve rovio is that it implements MEI’ s structure \ninternally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a \nfast, portable, lightweight tool written in pure  standard \nC++ with no dependencies on third- party frameworks or \nlibraries. It can be used as a comm and-line rendering tool, \nas a library, or it can be compiled to JavaScript  using the \nEmscripten LLVM -to-JavaScript compiler. This last o p-\ntion is particularly interesting because it provides a co m-\nplete in- browser music MEI typesetter. The SVG output \nfrom Ve rovio is organized in such a way that the MEI \nstructure is preserved as much as possible. Since every graphic in SVG i s an XML element that is easily address-\nable, Verovio is particularly well -suited for interactive \napplications, especially in web browsers.  Verovio is \navailable under the GPL open -source license.  \n1. INTRODUCTION  \nA few decades ago, rendering music notation by compu t-\ner almost exclusively targeting  printed output, most often \nin Postscript of PDF formats. Today, partly in response to the development of MIR applications, rendering of music \nnotation can be necessary in a wide range of contexts, for \nexample within standalone desktop applications, in ser v-\ner-side web application scenarios, or directly in a web \nbrowser. For example, music notation might need to be \nrendered  for displaying  search results or for visualizing \nanalysis outputs . Another example is score -following a p-\nplications, where the passage cu rrently played needs to be \ndisplayed and possibly highlighted . Rendering music n o-\ntation by computer, however, is a complex task. Powerful \nmusic notation rendering engines exist in commercial and open- source notation editors , but these are usually not \nvery modular and cannot easily  be integrated within other \napplications.  Other rendering engines, such as LilyPond  \n[13] or Mup [1], can be used; however, they usually r e-\nquire the encoding to be converted to a particular typeset-\nting input  syntax. Their  architectu res and dependencies \nalso often limit the contexts in which their use is possible.   \nIn recent  years, the Music Encoding Initiative (MEI) \nhas been increasingly adopted for music research projects \n[6]. Its large scope (MEI can be used to encode a wide \nrange of music notations, from medieval neumes to \ncommon Western music notation), modularity, rich metadata header and numerous other  features, including \nalignment with audio files or performance annotations, \nmake it appropriate  for a wide range of MIR applicati ons. \nUnfortunately, m ost of the solutions currently available \nfor render ing MEI involve a conversion to another for-\nmat, either explicitly or internally in the software applic a-\ntion used for rendering.   \nIn this paper, we present the Verovio  project, a library \nand toolkit for rendering MEI natively  in SVG . Its pu r-\npose is to provide a self -contained typesetting engine that \nis capable of creating high -quality graphical output and \nthat can also be used in different application contexts . In \nthe f ollowing section, we describe previous work and e x-\nisting solution s for rendering MEI  and the use of SVG for \nmusic notation . We then in troduce Verovio , describe the \nMEI structure on which it is built, outline its progra m-\nming architecture , and highlight  features currently avail a-\nble. We then present possible uses and output examples \nand conclude the paper with the future work that is planned for Verovio.  \n2. PREVIOUS WORK  \nOne currently available option for rendering MEI is con-\nversion  to another format  in order to use existing tools \nthat do not support MEI . For software applications or  \nrendering  engines that support the import of the M u-\nsicXML interchange format, MEI can be conver ted with \nthe mei2musicxml  XSL  stylesheet  [9]. Another option is \nto convert MEI directly to a typesetting format, such as \nMup . Mup is a C rendering engine that was made open -\nsource in 2012. It uses its own typesetting syntax and \nproduces high quality Postscript output. The conversion \nof MEI to Mup can be achieved in one step using the \nmei2mup  XSL  stylesheet  [8]. A similar approach is pos-\n  \n © Laurent Pugin , Rodolfo Zitellini , Perry Roland . \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Laurent Pugin , Rodolfo Zitellini , \nPerry Roland . “Verovio: A Library for Engraving MEI Music Notation \ninto SVG”, 15th International So ciety for Music Information Retrieval \nConference, 2014.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n107  \n \nsible for rendering MEI in a web browser, using a con-\nversion to the ABC format. ABC is an encoding format \nprimarily targeting material with fairly limited notational \nfeatures, such as folk and traditional melod ies. It can be \nrendered in a web browser  with the abcjs  renderer  [15], \nand the conversion from MEI to  ABC c an be achieved \nwith the mei2abc  converter  [5]. There is also a new Ja-\nvaScript library, MEItoVexflow  [18], that makes it possi-\nble to render MEI direct ly in web browser s using the \nVexflow  API [12]. Another tool for rendition of MEI  \nonline is N eon.js  [3]. The tool not only render s, but also \nacts as a full  online editor for neumatic medieval not a-\ntion. \nSVG for music notation has been  used in several pro-\njects. On e early attempt was made in 2003 for  converting  \nMusicXML  to SVG using XSLT  [14]. A framework with \nan editor was also developed for outputting SVG from \nGuidoXML notation  as part of a dissertation thesis [2 ]. \nWith MEI, SVG rendering was used for the  first time in \nthe DiMusEd  project , a critical edition of songs of Hilde-\ngard von Bingen (1098- 1179)  [11]. In this web-based edi-\ntion of neumatic notation , MEI rendering is performed on \nthe server side with a custom  rendering  engine. There are \nalso attempts to use XSLT to  transform MEI to SVG  di-\nrectly in the browser . This approach is used  in mono:di, \nthe transcription software of the Corpus Monodicum edi-\ntorial project  sponsored by the Akademie der  Wisse n-\nschaften und der Literatur in  Mainz , also focu sed on m e-\ndieval notation [4 ]. Finally, SVG is a possible back- end \nfor the aforementioned Vexflow API  in conjunction  with \nthe Raphael JavaScript  library. \nThese solutions all have strengths and drawbacks in \nterms of compatibility, usability, speed, output quality , \nand music notation features available. Many of them, \nhowever, have limitations when the format to which MEI is converted for rendering does not support some features encoded in the MEI source or has a different structure, with the consequence that part of the  encoding will be \nlost in conversion, or not rendered appropriately.  \n3. VEROVIO  \n3.1 MEI structure  \nThe MEI schema  provides multiple options for structu r-\ning the musical content. The most widely -used option is \nthe score -based structure, where all the parts of a mus ical \nscore are encoded together in the same XML sub -tree. \nThe MEI schema  also includes a part- based option, where \neach part is stored in a separate XML sub -tree. The \nchoice between these options can depend not only on the \ntype of document being encoded but  also on the type of \napplication. The Verovio library was designed as a direct implementation of the MEI structure.  However, since it is \nrendering -focused, it is built around  another content o r-\nganization of MEI, a page -based customization more a p-\npropriate for graphical display . In a rendering task, the page (or more generically, the rendering surface) is a  re-\nquired high- level entity on which element s can be laid out \nby the rendering process. The page -based customization  \nis a more fitting  alternat ive data or ganization that pro-\nvides a page top -level entity. It prioritizes the hierarchy \nthat is treated as secondary when encoded with milestone \nelements \n<pb> in other MEI representation s. \nIn the page -based customization, t he content of the \nmusic is en coded in <page>  elements that are themselves \ncontai ned in a <pages> element within <mdiv>  as \nshown in Figure 1 . A <page>  element contain s <sys-\ntem>  elements. From then on, the encoding is identical to \nstandard MEI. That is, a <system>  element will contain \n<measure>  elemen ts or <staff>  elements that are both \nun-customized, depending on whether or not the music is \nmeasured or un -measured , respectively.  Since the modif i-\ncations introduced by the customization are very limited, the Verovio library can be used to render un -custo mized \nMEI files.  When loading un -customized MEI documents, \nsome MEI elements are loaded by Verovio and converted to a page -based representation. Typically,  \n<pb> mile-\nstone elements are converted to  <page>  container ele-\nments. Conversely , <section>  container elements are \nconverted  to <secb>  milestone elements.  \n \n \n \nFigure 1. The page -based MEI structure  used by  Vero-\nvio. The <mdiv>  element c ontains <pages> , <page>  \nand <system> elements.  \n \n3.1.1 Layout and positioning  \nIn addition to making rendering simpler  and faster , the \nidea of the  page -based customization is also to make it \npossible to encode the positioning of elemen ts directly in \nthe content tree without having to refer to the facsimile sub-tree. The lat ter traditional approach  remains available \nwith the page -based customization for more detailed and \nmore complex referencing to facsimile images. However, the page -based customization introduces a lightweight \npositioning and referencing  system  that can be useful \nwhen  the encoding represents a single source with one \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n108  \n \nimage per  page. This is typically the case with optical \nmusic recognition applications  for which the encoding of  \nthe position of each encoded element is necessary . An-\nother possible use is the  creation  of overlay images to be \ndisplayed on top of facsimile im ages where the position \nof each symbol also needs to be encoded.  Verovio su p-\nports both positioned elements and automatic layout. Au-\ntomatic layout  will be executed when un -customized MEI \nfiles are rendered . \n3.1.2 Additional supported formats  \nIn addi tion to MEI, V erovio can render Plain and Easy \n(PAE) code [7] and DARMS code [16]. PAE and \nDARMS  encodings are widely used for encoding incipits, \nincluding those for the Répertoire International des \nSources Musical es (RISM) project . In Verovio, the se \nformats are converted to MEI internally, which means \nthat the toolkit can also be used to convert them to MEI  \nfor purposes other than rendering . \n3.2 SVG output  \nOne significant  advantage of SVG rendering over other \nformats (e.g., Postscript or  PDF) is that it is rendered n a-\ntively in most modern web browser s with  no plug- in re-\nquired. Because SVG  is XML, it has an advantage  over \nraster image formats that every graphical element is a d-\ndressable, making it well -suited for interactive applic a-\ntions . In a web environme nt, this makes it easy  to hig h-\nlight notes or symbols, for example. In addition, s ince \nSVG is a vector format, the output can also be used for \nhigh- quality printing.  \n \n \nFigure 2. The output of Vervovio for two bars. The  \nbuilt-in layout engine of Verovio  avoids symbol colli-\nsions as much as poss ible. \n One interesting feature of  Verovio  is that the SVG is \norganized in such a way that the MEI structure is pr e-\nserved as much as possible. For ex ample, a \n<note>  ele-\nment with an @xml:id attribute in the MEI file will have \na corresponding <g> element in the SVG with an @class  \nattribute  equal to  \"note\"  and an @id attribute corre-\nsponding to the @xml:id of the MEI note. This makes \ninteraction with the SVG very easy. The hierarchy of the element s is als o preserved. For example, in MEI, a \n<beam>  can be the child element of a <tuplet> , but the \nopposite is also possible. The hierarchy is fully preserved in the SVG as shown in Figure 3 .  \nFigure 3. Comparison of MEI and SVG file structures.  \nThe hierarchy of the MEI is preserved in the SVG.  \n \n3.3 Programming a rchitecture  \nVerovio is designed as a fast, portable , lightweight tool \nusable as a one- step conversion program . It is written in \npure standard C++ with no dependencies on third -party \nframeworks and libraries. This ensures maximum port a-\nbility of  the codebase. Verovio implements its own re n-\ndering engine, which can produce  SVG with all the mus i-\ncal symbols embedded in it . The musical glyphs  are \nthemselves SVG graphics that are included in the Vero-\nvio output. This means that no external font needs to be \nincluded in the SVG  generated from  Verovio , limiting \ndependencie s and  reducing as far as possible any poten-\ntial compatibility issues between SVG rendering engines .  \nThe Verovio  rendering engine itself is defined as an \nabstract class , and the  SVG output is the default concrete \nclass. This makes it relatively easy to implement a re n-\ndering back- end different from SVG  (e.g., PDF, or \nHTML Canvas) , if necessary . \nThe Verovio  toolkit has several options for controlling \nthe output.  These include optio ns for defining the page \nsize (i.e., the surface, or \n<svg>  element size), for setting \nthe amount of zoom , and  for choosing whether layout i n-\nformation  contained  in the MEI file must  be taken into \naccount. When there is no layout information provided in \nthe MEI file  (no system or page breaks, for example) , or \nwhen the option for ignoring them is selected, Verovio \nwill extrapolate  the necessary layout information.  \n3.4 Features  \nVerovio currently support s the basic features of simple \ncommon Western music notation and mensural notation. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n109  \n \nTable 1  shows a list of music notation snippets rendered \nwith Verovio.  Figure 4  illustrates how the SVG output of \nVerovio can be used as facsimile overlay when the pos i-\ntioning feature  of the MEI page -based customization is \nused. The example also illustrate s mensural music not a-\ntion support.  \nBeams and tuplets  \n \nMeasure rests and key and time signature changes  \n \nClef changes  \n \nTrills and fermata \n \nTies \n \nGrace notes (accaciature)  \n \nGrace notes (appogiature)  \n \n \nTable 1. A list of music notation snippets rendered with \nthe Verovio toolkit.  The basic features of simple common \nWeste rn music notation are accounted for . \n \n \nFigure 4. An example of the output of Verovio placed \nback on top of a facsimile image and acting as transcrip-\ntion overlay.  In this case, positioning information was \navailable in the page -based MEI encoding.  4. USE OF VEROVIO  \n4.1 C++  tools and library \nSeveral  use cases can be imagined for the Verovio toolkit. \nFirst of all, it can be built and used as a standalone com-\nmand -line tool . This option is well -suited to scripting e n-\nvironments and applications. The command -line tool can \nbe used to render music notation files (in MEI, PAE or \nDARMS ) into SVG. It can also be used to  convert \nDARMS or PAE to MEI. Another option is to use Ver o-\nvio as a music notation  rendering library that can be stat i-\ncally or dynamically linked to full applications. In such \ncases, it is also relatively easy to implement another \ndrawing back -end for the corresponding C++ graphic e n-\nvironment  for the music to be rendered directly on  the \nscreen. This is the case with  the Aruspix optical music \nrecognition  software  application where Verovio provides \na screen rende ring u sing a wxWidgets back -end instead \nof the standard SVG  one. This approach is conceivable \nfor any C++ graphic al environment s, be they cross-\nplatform , like the Qt or JUCE  toolkits , or platform speci f-\nic. \n4.2 JavaScript  toolkit  \nThe Verovio toolkit can also be compiled to JavaScript  \nusing the Emscripten LLVM -to-JavaScri pt compiler  [19]. \nIn this case, it behaves similarly to the command -line tool \nbut in the web browser  context . This approach is partic u-\nlarly interesting because it provides a complete i n-\nbrowser music MEI typesetter that can be easily integra t-\ned into web-based applications.  \nEmscripten does not directly translate  C++ into JavaS-\ncript. Instead it takes the LLVM  (Low Level Virtual M a-\nchine) byte code generated by the Clang compiler from \nthe C++ code as a base for the conversion to JavaScript . \nThis has sever al advantages. Most importantly , the level \nof completeness in terms of C++ language feature support \nis extremely high since the  idiomatic features  of C++  did \nnot have  to be explicitly translated into JavaScript  in the \nEmscripten com piler (only the translat ion from LLVM \nwas necessary ). In fact, for the Verovi o toolkit, the E m-\nscripten compil er is applied on exactly the sam e codebase \nas the C++ compiler, and no change to the code had to be \ndone for this to work. Only the compilation makefile is \ndifferent.  \nAnot her advantage of this approach is that the JavaS-\ncript produced is very fast because it benefits from all the \ncode optimization performed by the Clang compiler when \ngenerating the LLVM byte code.  Furthermore, i n addition \nto standard JavaScript , Emscripten  can also generate \nasm.js code, a  subset of JavaScript  that has the advantage \nof being highly optimizable. On web browser s that su p-\nport asm.js (currently Firefox, Chrome and Opera), the \nexecu tion speed is only up to about 1.6 times  slower  than \nwith the native C++ executable. T able 2  shows the sys-\ntem time required  to load a n MEI file of 120 pages of \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n110  \n \nmusic (7  MB) and for displaying th e first page with the \nnative ex ecutable and with three  web browser s. The fi g-\nures are the median value of the operation repeated 100 \ntimes . \n \n Native  Firefox  Chrome  Safari  \nSystem  time  \nin sec.  0.657 1.054 1.364 1.811 \nComparison \nto native - 1.6 2.1 2.8 \n \nTable 2. The system time in seconds for loading an MEI \nfiles (120 pages, 7  MB) and for displaying the first page. \nThe second line gives the ratio with the native executable \ntime for the three web browser s used for comparison . \n \nThe JavaScript  version of the Verovio toolkit is easy to \nuse in web  environments . It is packed in one single file \nwhich  size is  only about  1.2 MB . It is available  as a Ja-\nvaScript  class, and a ll the options of the command -line \nversion are supported  in the toolkit . The o ptions  can be \npassed to the toolkit  in JSON  format , and the SVG output \ncan be directly fed to HTML ob jects for dis play.  The \nFigure 5 shows a  HTML and Javascript  code snippet for \nloading an MEI file using a jQuery HTTP GET request.  \n \n \nFigure 5. A JavaScript  example for loading an MEI file . \nThe toolkit parameters can be set using JSON.  \n \nThe layout of the MEI data is performed on loading. \nOnce the file is loaded into memory, it remains accessible \nin the toolkit instance . The class provides methods for \ngetting the number of pages or for navigating through \nthem, making it convenient  to integ rate the toolkit in a \nJavaScript application.  \nThe Figure 6  shows a screenshot of a web application \nwhere the toolkit was turned into  an online MEI file \nviewer.  The application works on desktop computers but \nalso on tablets and mobile devices. The Java Script tool kit \nhas been tested with recent versions of the most widely used web- browsers. Internet Explorer requires at least \nversion 10 . \n  \nFigure 6. An example of a web -based MEI viewer built \nwith the Verovio toolkit. Large MEI files can be loaded \nand displayed in the web browser  in a very convenient \nway.  \n \n5. CONCLUSI ON AND FUTURE WORK  \nVerovio is a toolkit  for rendering MEI in SVG that can be \nused in different application environments , including \nonline . It is de signed with MEI in mind, making it the \nright basis for implementing encoding features that are \nspecific to MEI. It will avoid problematic situation s that \noccur when using rendering engines based on other fo r-\nmats and that implement  a different data structure. Even \nif at this stage, the supported features can be in some ca s-\nes more limited than with other rendering options, Ver o-\nvio already implements many important features for ren-dering both common Western music notation and mensu-\nral notation.  \nCurrent work on  Verovio includes the adoption of the \nStandard Music Font Layout (SMuFL)  [17] for support-\ning other  fonts co nverted to SVG glyphs, the im prove-\nment of the SVG stru cture and ad ding support for add i-\ntional MEI e lements and at tributes. The priority is given \nto features specific to MEI. The f uture work will include \nthe deve lopment of a prot otype for making Vero vio a  \npossible b asis for an online MEI editor. It will also i n-\nclude the cre ation of an MEI application profile for Ver o-\nvio using the TEI One Doc ument Does -it-all (ODD) a p-\nproach. T he correspon ding X SL stylesheets for co nvert-\ning to it other MEI pr ofiles will also be pr ovided. Adding \nthe import of other enco ding formats is also e nvisaged in \nthe future.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n111  \n \n6. AVAILABILITY  \nVerovio can be downloaded f rom http:// www.verovio.org  \nand is availa ble un der the GPLv3 open- source license . \nThe website also includes documentation on currently \navailable features.  \n7. REFERENCES \n[1] Arkkra Enterprises, Mup.  <http://www.arkkra.com>  \n[2] G. A. Bays : ScoreSVG: A New Software Framework \nfor Capturing the Semantic Meaning and Graphical \nRepresentation of M usical Scores Using Java2D, \nXML,  and SVG . Diss. Georgia State Univ., 2005.  \n[3] G. Burlet, A. Porter, A.  Hankinson, and I.  Fujinaga : \n“Neon.js: Neume Editor Online,”  Proceedings of the  \n13th International Society  on Music Information \nRetrieval  Conference, pp. 121– 6, 2012.  \n[4] Corpus M onodicum,  mono:di . \n<http://mon odi.corpus- monodicum.de > \n[5] Edirom, mei2abc . <https://github.com/e dirom/  \nmei2abc>  \n[6] A. Hankinson , P. Roland, and I. Fujinaga:  “The \nMusic Encoding Initiative a s a document -encoding \nframework, ” Proceedings of the  12th International \nSociety  on Music Information Retrieval  Conference, \npp. 293– 8, 2011.  \n[7] J. Howard : “Plaine and Easie code : A code for \nmusic bibliography ,” in Selfridge -Field, E. (Ed.), \nBeyond MIDI: The Handbook of Musical Codes . \nThe MIT Press, Cambridge, pp. 362 –72, 1997.  \n[8] MEI, mei2mup. <http ://code.google.com/p/music -\nencoding/source/browse/trunk/tools/mei2mup>  \n[9] MEI, mei2musicxml . <https ://code.google.com/p/  \nmusic -encoding/source/browse/trunk/tools/ \nmei2musicxml>  \n[10] MEI-incubator, page -based customization , \n<https ://code.google.com/p/mei -incubator/source/  \nbrowse/page- based>  \n[11] S. Morent:  “Digitale Edition älterer Musik am \nBeispiel des Projekts TüBingen ,” in Digitale Edition \nzwischen Experim ent und Standardisierung. Musik – \nText – Codierung , pp. 89– 109, 2009. \n[12] M. Muthanna, VexFlow.  <https://github.com/0xfe/  \nvexflow > \n[13] H. W. Nienhuys  and J. Nieuwenhuizen:  “LilyPond, a \nsystem for automated music engraving,” \nProceedings of the XIV Colloquium on Musical \nInformatics (XIV CIM 2003) , pp. 167 –72, 2003.  [14] L. O’Shea: “Stirring XML: Visualizations in SVG: MusicML2SVG,” Proceedings of the SVGOpen2003 \nConference, pp. 2– 6, 2003.  \n[15] P. Rosen, abcjs . <http ://github.com/paulrosen/abcjs>  \n[16] E. Selfridge -Field:  “DARMS, its dialects, its uses ,” \nin Selfridge -Field, E. (Ed.), Beyond  MIDI: The \nHandbook of Musical Codes . The MIT Press, \nCambridge,  pp. 163– 74, 1997. \n[17] Steinberg, Standard Music Font Layout.  \n<http://www.smufl.org>  \n[18] TEI Music SIG, MEItoVexFlow . <http://github.com/ \ntei-music -sig/meitovexflow > \n[19] A. Zakai:  “Emscripten:  an LLVM -to-JavaScript \ncompiler,”  Companion to the 26th Annual ACM \nOOPSLA  Conference , pp. 301 –12, 2011.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n112"
    },
    {
        "title": "MIR_EVAL: A Transparent Implementation of Common MIR Metrics.",
        "author": [
            "Colin Raffel",
            "Brian McFee",
            "Eric J. Humphrey",
            "Justin Salamon",
            "Oriol Nieto",
            "Dawen Liang",
            "Daniel P. W. Ellis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416528",
        "url": "https://doi.org/10.5281/zenodo.1416528",
        "ee": "https://zenodo.org/records/1416528/files/RaffelMHSNLE14.pdf",
        "abstract": "Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval’s architecture, design, and intended use.",
        "zenodo_id": 1416528,
        "dblp_key": "conf/ismir/RaffelMHSNLE14",
        "keywords": [
            "evaluation",
            "algorithms",
            "music",
            "data",
            "MIR",
            "software",
            "library",
            "metrics",
            "performance",
            "MIR algorithms"
        ],
        "content": "mir_eval:\nA TRANSPARENT IMPLEMENTATION OF COMMON MIR METRICS\nColin Raffel1,*, Brian McFee1,2, Eric J. Humphrey3, Justin Salamon3,4, Oriol Nieto3,\nDawen Liang1, and Daniel P. W. Ellis1\n1LabROSA, Dept. of Electrical Engineering, Columbia University, New York\n2Center for Jazz Studies, Columbia University, New York\n3Music and Audio Research Lab, New York University, New York\n4Center for Urban Science and Progress, New York University, New York\nABSTRACT\nCentral to the ﬁeld of MIR research is the evaluation of\nalgorithms used to extract information from music data. We\npresent mir_eval , an open source software library which\nprovides a transparent and easy-to-use implementation of\nthe most common metrics used to measure the performance\nof MIR algorithms. In this paper, we enumerate the metrics\nimplemented by mir_eval and quantitatively compare\neach to existing implementations. When the scores reported\nbymir_eval differ substantially from the reference, we\ndetail the differences in implementation. We also provide\na brief overview of mir_eval ’s architecture, design, and\nintended use.\n1. EV ALUATING MIR ALGORITHMS\nMuch of the research in Music Information Retrieval (MIR)\ninvolves the development of systems that process raw music\ndata to produce semantic information. The goal of these\nsystems is frequently deﬁned as attempting to duplicate the\nperformance of a human listener given the same task [5].\nA natural way to determine a system’s effectiveness might\nbe for a human to study the output produced by the sys-\ntem and judge its correctness. However, this would yield\nonly subjective ratings, and would also be extremely time-\nconsuming when evaluating a system’s output over a large\ncorpus of music.\nInstead, objective metrics are developed to provide a\nwell-deﬁned way of computing a score which indicates\neach system’s output’s correctness. These metrics typically\ninvolve a heuristically-motivated comparison of the sys-\ntem’s output to a reference which is known to be correct.\nOver time, certain metrics have become standard for each\n\u0003Please direct correspondence to craffel@gmail.com\nc\rColin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel P. W. Ellis.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Colin Raffel, Brian McFee, Eric J.\nHumphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel P. W. Ellis.\n“mir_eval:\nA Transparent Implementation of Common MIR Metrics”, 15th Interna-\ntional Society for Music Information Retrieval Conference, 2014.task, so that the performance of systems created by different\nresearchers can be compared when they are evaluated over\nthe same dataset [5]. Unfortunately, this comparison can\nbe confounded by small details of the implementations or\nprocedures that can have disproportionate impacts on the\nresulting scores.\nFor the past 10 years, the yearly Music Information Re-\ntrieval Evaluation eXchange (MIREX) has been a forum\nfor comparing MIR algorithms over common datasets [6].\nBy providing a standardized shared-task setting, MIREX\nhas become critically useful for tracking progress in MIR\nresearch. MIREX is built upon the Networked Environment\nfor Music Analysis (NEMA) [22], a large-scale system\nwhich includes exhaustive functionality for evaluating, sum-\nmarizing, and displaying evaluation results. The NEMA\ncodebase includes multiple programming languages and\ndependencies (some of which, e.g.Matlab, are proprietary)\nso compiling and running it at individual sites is nontrivial.\nIn consequence, the NEMA system is rarely used for evalu-\nating MIR algorithms outside of the setting of MIREX [6].\nInstead, researchers often create their own implementations\nof common metrics for evaluating their algorithms. These\nimplementations are thus not standardized, and may contain\ndifferences in details, or even bugs, that confound compar-\nisons.\nThese factors motivate the development of a standard-\nized software package which implements the most common\nmetrics used to evaluate MIR systems. Such a package\nshould be straightforward to use and well-documented so\nthat it can be easily adopted by MIR researchers. In addi-\ntion, it should be community-developed and transparently\nimplemented so that all design decisions are easily under-\nstood and open to discussion and improvement.\nFollowing these criteria, we present mir_eval , a soft-\nware package which intends to provide an easy and stan-\ndardized way to evaluate MIR systems. This paper ﬁrst dis-\ncusses the architecture and design of mir_eval in Section\n2, then, in Section 3, describes all of the tasks covered by\nmir_eval and the metrics included. In order to validate\nour implementation decisions, we compare mir_eval to\nexisting software in Section 4. Finally, we discuss and\nsummarize our contributions in Section 5.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3672.mir_eval’S ARCHITECTURE\nmir_eval is a Python library which currently includes\nmetrics for the following tasks: Beat detection, chord esti-\nmation, pattern discovery, structural segmentation, melody\nextraction, and onset detection. Each task is given its own\nsubmodule, and each metric is deﬁned as a separate func-\ntion in each submodule. Each task submodule also includes\ncommon data pre-processing steps for the task. Every met-\nric function includes detailed documentation, example us-\nage, input validation, and references to the original pa-\nper which deﬁned the metric. mir_eval also includes\na submodule iowhich provides convenience functions\nfor loading in task-speciﬁc data from common ﬁle for-\nmats (e.g. comma/tab separated values, .lab ﬁles [7],\netc.). For readability, all code follows the PEP8 style\nguide [21]. mir_eval ’s only dependencies outside of\nthe Python standard library are the free and open-source\nSciPy/Numpy [9] and scikit-learn [15] libraries.\nIn order to simplify the usage of mir_eval , it is pack-\naged with a set of “evaluator” scripts, one for each task.\nThese scripts include all code necessary to load in data,\npre-process it, and compute all metrics for a given task.\nThe evaluators allow for mir_eval to be called directly\nfrom the command line so that no knowledge of Python\nis necessary. They are also distributed as executables for\nWindows and Mac OS X, so that mir_eval may be used\nwith no dependencies installed.\n3. TASKS INCLUDED IN mir_eval\nIn this section, we enumerate the tasks and metrics im-\nplemented in mir_eval . Due to space constraints, we\nonly give high-level descriptions for each metric; for exact\ndeﬁnitions see the references provided.\n3.1 Beat Detection\nThe aim of a beat detection algorithm is to report the times\nat which a typical human listener might tap their foot to a\npiece of music. As a result, most metrics for evaluating the\nperformance of beat tracking systems involve computing the\nerror between the estimated beat times and some reference\nlist of beat locations. Many metrics additionally compare\nthe beat sequences at different metric levels in order to deal\nwith the ambiguity of tempo [4].\nmir_eval includes the following metrics for beat track-\ning, which are deﬁned in detail in [4]: The F-measure of\nthe beat sequence, where an estimated beat is considered\ncorrect if it is sufﬁciently close to a reference beat; Cemgil’s\nscore , which computes the sum of Gaussian errors for each\nbeat; Goto’s score , a binary score which is 1 when at least\n25% of the estimated beat sequence closely matches the\nreference beat sequence; McKinney’s P-score , which com-\nputes the cross-correlation of the estimated and reference\nbeat sequences represented as impulse trains; continuity-\nbased scores which compute the proportion of the beat\nsequence which is continuously correct; and ﬁnally the In-\nformation Gain of a normalized beat error histogram over\na uniform distribution.3.2 Chord Estimation\nDespite being one of the oldest MIREX tasks, evaluation\nmethodology and metrics for automatic chord estimation is\nan ongoing topic of discussion, due to issues with vocab-\nularies, comparison semantics, and other lexicographical\nchallenges unique to the task [14]. One source of difﬁculty\nstems from an inherent subjectivity in “spelling” a chord\nname and the level of detail a human observer can provide\nin a reference annotation [12]. As a result, a consensus\nhas yet to be reached regarding the single best approach to\ncomparing two sequences of chord labels, and instead are\noften compared over a set of rules, i.e Root, Major-Minor,\nand Sevenths, with or without inversions.\nTo efﬁciently compare chords, we ﬁrst separate a given\nchord label into a its constituent parts, based on the syn-\ntax of [7]. For example, the chord label G:maj(6)/5 is\nmapped to three pieces of information: the root (“G”), the\nroot-invariant active semitones as determined by the quality\nshorthand (“maj”) and scale degrees (“6”), and the bass\ninterval (“5”).\nBased on this representation, we can compare an esti-\nmated chord label with a reference by the following rules as\nused in MIREX 2013 [2]: Root requires only that the roots\nare equivalent; Major-Minor includes Root, and further\nrequires that the active semitones are equivalent subject to\nthe reference chord quality being Maj or min; Sevenths\nfollows Major-minor, but is instead subject to the reference\nchord quality being one of Maj, min, Maj7, min7, 7, or\nminmaj7; and ﬁnally, Major-Minor-Inv andSevenths-Inv\ninclude Major-Minor and Sevenths respectively, but fur-\nther require that the bass intervals are equivalent subject to\nthe reference bass interval being an active semitone. The\n“subject to. . . ” conditions above indicate that a compari-\nson is ignored during evaluation if the given criteria is not\nsatisﬁed.\nTrack-wise scores are computed by weighting each com-\nparison by the duration of its interval, over all intervals in\nan audio ﬁle. This is achieved by forming the union of\nthe boundaries in each sequence, sampling the labels, and\nsumming the time intervals of the “correct” ranges. The cu-\nmulative score, referred to as weighted chord symbol recall,\nis tallied over a set audio ﬁles by discrete summation, where\nthe importance of each score is weighted by the duration of\neach annotation [2].\n3.3 Pattern Discovery\nPattern discovery involves the identiﬁcation of musical pat-\nterns (i.e. short fragments or melodic ideas that repeat at\nleast twice) both from audio and symbolic representations.\nThe metrics used to evaluation pattern discovery systems\nattempt to quantify the ability of the algorithm to not only\ndetermine the present patterns in a piece, but also to ﬁnd all\nof their occurrences.\nCollins compiled all previously existent metrics and\nproposed novel ones [3] which resulted in 19 different\nscores, each one implemented in mir_eval :Standard\nF-measure, Precision, and Recall , where an estimated\nprototype pattern is considered correct only if it matches\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n368(up to translation) a reference prototype pattern; Establish-\nment F-measure, Precision, and Recall , which compute\nthe number of reference patterns that were successfully\nfound, no matter how many occurrences were found; Oc-\ncurrence F-measure, Precision, and Recall , which mea-\nsure whether an algorithm is able to retrieve all occurrences\nof a pattern; Three-layer F-measure, Precision, and Re-\ncall, which capture both the establishment of the patterns\nand the occurrence retrieval in a single set of scores; and\ntheFirst Npatterns metrics , which compute the target\nproportion establishment recall and three-layer precision\nfor the ﬁrst Npatterns only in order to measure the ability\nof the algorithm to sort the identiﬁed patterns based on their\nrelevance.\n3.4 Structural Segmentation\nEvaluation criteria for structural segmentation fall into two\ncategories: boundary annotation and structural annotation.\nBoundary annotation is the task of predicting the times\nat which structural changes occur, such as when a verse\ntransitions to a refrain. Structural annotation is the task of\nassigning labels to detected segments. The estimated labels\nmay be arbitrary strings — such as A,B,C,etc.— and\nthey need not describe functional concepts. In both tasks,\nwe assume that annotations express a partitioning of the\ntrack into intervals.\nmir_eval implements the following boundary detec-\ntion metrics: Boundary Detection Precision, Recall, and\nF-measure Scores where an estimated boundary is con-\nsidered correct if it falls within a window around a ref-\nerence boundary [20]; and Boundary Deviation which\ncomputes median absolute time difference from a refer-\nence boundary to its nearest estimated boundary, and vice\nversa [20]. The following structure annotation metrics are\nalso included: Pairwise Classiﬁcation Precision, Recall,\nand F-measure Scores for classifying pairs of sampled\ntime instants as belonging to the same structural compo-\nnent [10]; Rand Index1which clusters reference and es-\ntimated annotations and compares them by the Rand In-\ndex [17]; and the Normalized Conditional Entropy where\nsampled reference and estimated labels are interpreted as\nsamples of random variables YR; YEfrom which the condi-\ntional entropy of YRgiven YE(Under-Segmentation ) and\nYEgiven YR(Over-Segmentation) are estimated [11].\n3.5 Melody Extraction\nMelody extraction algorithms aim to produce a sequence\nof frequency values corresponding to the pitch of the domi-\nnant melody from a musical recording [19]. An estimated\npitch series is evaluated against a reference by computing\nthe following ﬁve measures deﬁned in [19], ﬁrst used in\nMIREX 2005 [16]: Voicing Recall Rate which computes\nthe proportion of frames labeled as melody frames in the ref-\nerence that are estimated as melody frames by the algorithm;\nVoicing False Alarm Rate which computes the proportion\nof frames labeled as non-melody in the reference that are\n1The MIREX results page refers to Rand Index as “random clustering\nindex”.mistakenly estimated as melody frames by the algorithm;\nRaw Pitch Accuracy which computes the proportion of\nmelody frames in the reference for which the frequency\nis considered correct (i.e. within half a semitone of the\nreference frequency); Raw Chroma Accuracy where the\nestimated and reference f0sequences are mapped onto a\nsingle octave before computing the raw pitch accuracy; and\ntheOverall Accuracy , which computes the proportion of\nall frames correctly estimated by the algorithm, including\nwhether non-melody frames where labeled by the algorithm\nas non-melody. Prior to computing these metrics, both the\nestimate and reference sequences must be sampled onto the\nsame time base.\n3.6 Onset Detection\nThe goal of an onset detection algorithm is to automatically\ndetermine when notes are played in a piece of music. As is\nalso done in beat tracking and segment boundary detection,\nthe primary method used to evaluate onset detectors is to\nﬁrst determine which estimated onsets are “correct”, where\ncorrectness is deﬁned as being within a small window of\na reference onset [1]. From this, Precision ,Recall , and\nF-measure scores are computed.\n4. COMPARISON TO EXISTING\nIMPLEMENTATIONS\nIn order to validate the design choices made in mir_eval ,\nit is useful to compare the scores it reports to those reported\nby an existing evaluation system. Beyond pinpointing inten-\ntional differences in implementation, this process can also\nhelp ﬁnd and ﬁx bugs in either mir_eval or the system it\nis being compared to.\nFor each task covered by mir_eval , we obtained a\ncollection of reference and estimated annotations and com-\nputed or obtained a score for each metric using mir_eval\nand the evaluation system being compared to. In order to\nfacilitate comparison, we ensured that all parameters and\npre-processing used by mir_eval were equivalent to the\nreference system unless otherwise explicitly noted. Then,\nfor each reported score, we computed the relative change\nbetween the scores as their absolute difference divided by\ntheir mean, or\njsm\u0000scj\n(sm+sc)=2\nwhere smis the score reported by mir_eval andscis the\nscore being compared to. Finally, we computed the average\nrelative change across all examples in the obtained dataset\nfor each score.\nFor the beat detection, chord estimation, structural seg-\nmentation, and onset detection tasks, MIREX releases the\nthe output of submitted algorithms, the ground truth anno-\ntations, and the reported score for each example in each\ndata set. We therefore can directly compare mir_eval\nto MIREX for these tasks by collecting all reference and\nestimated annotations, computing each metric for each ex-\nample using identical pre-processing and parameters as ap-\npropriate, and comparing the result to the score reported by\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n369MIREX. We chose to compare against the results reported\nin MIREX 2013 for all tasks.\nIn contrast to the tasks listed above, MIREX does not\nrelease ground truth annotations or algorithm output for\nthe melody extraction and pattern discovery tasks. As a\nresult, we compared mir_eval ’s output on smaller de-\nvelopment datasets for these tasks. For melody extraction,\nthe ADC2004 dataset used by MIREX is publicly available.\nWe performed melody extraction using the “SG2” algo-\nrithm evaluated in 2011 [18] and compared mir_eval ’s\nreported scores to those of MIREX. For pattern discovery,\nwe used the development dataset released by Collins [3] and\nused the algorithms submitted by Nieto and Farbood [13]\nfor MIREX 2013 to produce estimated patterns. We eval-\nuated the estimated patterns using the MATLAB code re-\nleased by Collins [3]. The number of algorithms, examples,\nand total number of scores for all tasks are summarized in\nTable 1.\nTask Algorithms Examples Scores\nBeat Detection 20 679 13580\nSegmentation 8 1397 11176\nOnset Detection 11 85 935\nChord Estimation 12 217 2604\nMelody 1 20 20\nPattern Discovery 4 5 20\nTable 1 . Number of scores collected for each task for\ncomparison against mir_eval.\nThe resulting average relative change for each metric is\npresented in Table 2. The average relative change for all of\nthe pattern discovery metrics was 0, so they are not included\nin this table. For many of the other metrics, the average rel-\native change was less than a few tenths of a percent, indicat-\ning that mir_eval is equivalent up to rounding/precision\nerrors. In the following sections, we enumerate the known\nimplementation differences which account for the larger\naverage relative changes.\n4.1 Non-greedy matching of events\nIn the computation of the F-measure, Precision and Recall\nmetrics for the beat tracking, boundary detection, and onset\ndetection tasks, an estimated event is considered correct (a\n“hit”) if it falls within a small window of a reference event.\nNo estimated event is counted as a hit for more than one ref-\nerence event, and vice versa. In MIREX, this assignment is\ndone in a greedy fashion, however in mir_eval we use an\noptimal matching strategy. This is accomplished by comput-\ning a maximum bipartite matching between the estimated\nevents and the reference events (subject to the window\nconstraint) using the Hopcroft-Karp algorithm [8]. This ex-\nplains the observed discrepancy between mir_eval and\nMIREX for each of these metrics. In all cases where the\nmetric differs, mir_eval reports a higher score, indicat-\ning that the greedy matching strategy was suboptimal.4.2 McKinney’s P-score\nWhen computing McKinney’s P-score [4], the beat se-\nquences are ﬁrst converted to impulse trains sampled at\na 10 millisecond resolution. Because this sampling involves\nquantizing the beat times, shifting both beat sequences by\na constant offset can result in substantial changes in the\nP-score. As a result, in mir_eval , we normalize the beat\nsequences by subtracting from each reference and estimated\nbeat location the minimum beat location in either series. In\nthis way, the smallest beat in the estimated and reference\nbeat sequences is always 0and the metric remains the same\neven when both beat sequences have a constant offset ap-\nplied. This is not done in MIREX (which uses the Beat\nEvaluation Toolbox [4]), and as a result, we observe a con-\nsiderable average relative change for the P-score metric.\n4.3 Information Gain\nThe Information Gain metric [4] involves the computation\nof a histogram of the per-beat errors. The Beat Evaluation\nToolbox (and therefore MIREX) uses a non-uniform his-\ntogram binning where the ﬁrst, second and last bins are\nsmaller than the rest of the bins while mir_eval uses\na standard uniformly-binned histogram. As a result, the\nInformation Gain score reported by mir_eval differs sub-\nstantially from that reported by MIREX.\n4.4 Segment Boundary Deviation\nWhen computing the median of the absolute time differ-\nences for the boundary deviation metrics, there are often an\neven number of reference or estimated segment boundaries,\nresulting in an even number of differences to compute the\nmedian over. In this case, there is no “middle” element\nto choose as the median. mir_eval follows the typical\nconvention of computing the mean of the two middle ele-\nments in lieu of the median for even-length sequences, while\nMIREX chooses the larger of the two middle elements. This\naccounts for the discrepancy in the reference-to-estimated\nand estimated-to-reference boundary deviation metrics.\n4.5 Interval Sampling for Structure Metrics\nWhen computing the structure annotation metrics (Pairwise\nPrecision, Recall, and F-measure, Rand Index, and Normal-\nized Conditional Entropy Over- and Under-Segmentation\nScores), the reference and estimated labels must be sampled\nto a common time base. In MIREX, a ﬁxed sampling grid\nis used for the Rand Index and pairwise classiﬁcation met-\nrics, but a different sampling rate is used for each, while a\nﬁxed number of samples is used for the conditional entropy\nscores. In mir_eval , the same ﬁxed sampling rate of 100\nmilliseconds is used for all structure annotation metrics, as\nspeciﬁed in [23].\nFurthermore, in MIREX the start and end time over\nwhich the intervals are sampled depends on both the ref-\nerence and estimated intervals while mir_eval always\nsamples with respect to the reference to ensure fair compar-\nison across multiple estimates. This additionally requires\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n370Beat Detection\nF-measure Cemgil Goto P-score CMLc CMLt AMLc AMLt In. Gain\n0.703% 0.035% 0.054% 0.877% 0.161% 0.143% 0.137% 0.139% 9.174%\nStructural Segmentation\nNCE-Over NCE-under Pairwise F Pairwise P Pairwise R Rand F@.5 P@.5 R@.5\n3.182% 11.082% 0.937% 0.942% 0.785% 0.291% 0.429% 0.088% 1.021%\nStructural Segmentation (continued) Onset Detection\nF@3 P@3 R@3 Ref-est dev. Est-ref dev. F-measure Precision Recall\n0.393% 0.094% 0.954% 0.935% 0.000% 0.165% 0.165% 0.165%\nChord Estimation Melody Extraction\nRoot Maj/min Maj/min + Inv 7ths 7ths + Inv Overall Raw pitch Chroma V oicing R V oicing FA\n0.007% 0.163% 1.005% 0.483% 0.899% 0.070% 0.087% 0.114% 0.000% 10.095%\nTable 2 . Average relative change for every metric in mir_eval when compared to a pre-existing implementation. The\naverage relative change for all pattern discovery metrics was 0, so they are not shown here.\nthat estimated intervals are adjusted to span the exact du-\nration speciﬁed by the reference intervals. This is done by\nadding synthetic intervals when the estimated intervals do\nnot span the reference intervals or otherwise trimming esti-\nmated intervals. These differences account for the average\nrelative changes for the structure annotation metrics.\n4.6 Segment Normalized Conditional Entropy\nWhen adding intervals to the estimated annotation as de-\nscribed above, mir_eval ensures that the labels do not\nconﬂict with existing labels. This has the effect of changing\nthe normalization constant in the Normalized Conditional\nEntropy scores. Furthermore, when there’s only one label,\nthe Normalized Conditional Entropy scores are not well de-\nﬁned. MIREX assigns a score of 1 in this case; mir_eval\nassigns a score of 0. This results in a larger average change\nfor these two metrics.\n4.7 Melody Voicing False Alarm Rate\nWhen a reference melody annotation contains no unvoiced\nframes, the V oicing False Alarm Rate is not well deﬁned.\nMIREX assigns a score of 1 in this case, while mir_eval\nassigns a score of 0 because, intuitively, no reference un-\nvoiced frames could be estimated, so no false alarms should\nbe reported. In the data set over which the average relative\nchange for the melody metrics was computed, one reference\nannotation contained no unvoiced frames. This discrepancy\ncaused a large inﬂation of the average relative change re-\nported for the V oicing False Alarm Rate due to the small\nnumber of examples in our dataset.\n4.8 Weighted Chord Symbol Recall\nThe non-negligible average relative changes seen in the\nchord metrics are caused by two main sources of ambiguity.\nFirst, we ﬁnd some chord labels in the MIREX reference\nannotations lack well-deﬁned, i.e.singular, mappings into a\ncomparison space. One such example is D:maj( *1)/#1 .While the quality shorthand indicates “major”, the asterisk\nimplies the root is omitted and thus it is unclear whether\nD:maj( *1)/#1 is equivalent to D:maj1 . Second, and\nmore importantly, such chords are likely ignored during\nevaluation, and we are unable to replicate the exact exclu-\nsion logic used by MIREX. This has proven to be particu-\nlarly difﬁcult in the two inversion rules, and manifests in\nTable 2. For example, Bb:maj(9)/9 wasnotexcluded\nfrom the MIREX evaluation, contradicting the description\nprovided by the task speciﬁcation [2]. This chord alone\ncauses an observable difference between mir_eval and\nMIREX’s results.\n5. TOWARDS TRANSPARENCY AND\nCOMMUNITY INVOLVEMENT\nThe results in Section 4 clearly demonstrate that differences\nin implementation can lead to substantial differences in\nreported scores. This corroborates the need for transparency\nand community involvement in comparative evaluation. The\nprimary motivation behind developing mir_eval is to\nestablish an open-source, publicly reﬁned implementation\nof the most common MIR metrics. By encouraging MIR\nresearchers to use the same easily understandable evaluation\ncodebase, we can ensure that different systems are being\ncompared fairly.\nWhile we have given thorough consideration to the de-\nsign choices made in mir_eval , we recognize that stan-\ndards change over time, new metrics are proposed each\nyear, and that only a subset of MIR tasks are currently im-\nplemented in mir_eval . Towards this end, mir_eval\nis hosted on Github,2which provides a straightforward\nway of proposing changes and additions to the codebase\nusing the Pull Request feature. With active community\nparticipation, we believe that mir_eval can ensure that\nMIR research converges on a standard methodology for\nevaluation.\n2http://github.com/craffel/mir_eval\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3716. ACKNOWLEDGEMENTS\nThe authors would like to thank Matthew McVicar for help-\nful advice on comparing chord labels and Tom Collins for\nsharing his MATLAB implementation to evaluate musical\npatterns. Support provided in part by The Andrew W. Mel-\nlon Foundation and the National Science Foundation, under\ngrants IIS-0844654 and IIS-1117015.\n7. REFERENCES\n[1]S. B¨ock, F. Krebs, and M. Schedl. Evaluating the online\ncapabilities of onset detection methods. In Proceedings\nof the 13th International Society for Music Information\nRetrieval Conference (ISMIR), pages 49–54, 2012.\n[2]K. Choi and J. A. Burgoyne. MIREX task: Au-\ndio chord estimation. http://www.music-ir.\norg/mirex/wiki/2013:Audio_Chord_\nEstimation, 2013. Accessed: 2014-04-30.\n[3]T. Collins. MIREX task: Discovery of repeated\nthemes & sections. http://www.music-ir.\norg/mirex/wiki/2013:Discovery_of_\nRepeated_Themes_&_Sections , 2013. Ac-\ncessed: 2014-04-30.\n[4]M. E. P. Davies, N. Degara, and M. D. Plumbley. Evalua-\ntion methods for musical audio beat tracking algorithms.\nTechnical Report C4DM-TR-09-06, Centre for Digital\nMusic, Queen Mary University of London, London,\nEngland, October 2009.\n[5]J. S. Downie. Toward the scientiﬁc evaluation of music\ninformation retrieval systems. In Proceedings of the 4th\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 25–32, 2003.\n[6]J. S. Downie. The music information retrieval evalu-\nation exchange (2005-2007): A window into music\ninformation retrieval research. Acoustical Science and\nTechnology, 29(4):247–255, 2008.\n[7]C. Harte. Towards Automatic Extraction of Harmony\nInformation from Music Signals. PhD thesis, Queen\nMary University of London, August 2010.\n[8]J. E. Hopcroft and R. M. Karp. An nˆ5/2 algorithm for\nmaximum matchings in bipartite graphs. SIAM Journal\non computing, 2(4):225–231, 1973.\n[9]E. Jones, T. Oliphant, P. Peterson, et al. SciPy: Open\nsource scientiﬁc tools for Python, 2001–.\n[10] M. Levy and M. Sandler. Structural segmentation of\nmusical audio by constrained clustering. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n16(2):318–326, 2008.\n[11] H. M. Lukashevich. Towards quantitative measures of\nevaluating song segmentation. In Proceedings of the 9th\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 375–380, 2008.[12] Y . Ni, M. McVicar, R. Santos-Rodriguez, and T. De Bie.\nUnderstanding effects of subjectivity in measuring\nchord estimation accuracy. IEEE Transactions on Audio,\nSpeech, and Language Processing, 21(12):2607–2615,\n2013.\n[13] O. Nieto and M. Farbood. Discovering musical pat-\nterns using audio structural segmentation techniques.\n7th Music Information Retrieval Evaluation eXchange\n(MIREX), 2013.\n[14] J. Pauwels and G. Peeters. Evaluating automatically\nestimated chord sequences. In Acoustics, Speech and\nSignal Processing (ICASSP), 2013 IEEE International\nConference on, pages 749–753. IEEE, 2013.\n[15] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cour-\nnapeau, M. Brucher, M. Perrot, and E. Duchesnay.\nScikit-learn: Machine learning in Python. Journal of\nMachine Learning Research, 12:2825–2830, 2011.\n[16] G. E. Poliner, D. P. W. Ellis, A. F. Ehmann, E. G ´omez,\nS. Streich, and B. Ong. Melody transcription from mu-\nsic audio: Approaches and evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n15(4):1247–1256, 2007.\n[17] W. M. Rand. Objective criteria for the evaluation of\nclustering methods. Journal of the American Statistical\nassociation, 66(336):846–850, 1971.\n[18] J. Salamon and E. G ´omez. Melody extraction from poly-\nphonic music signals using pitch contour characteristics.\nIEEE Transactions on Audio, Speech, and Language\nProcessing, 20(6):1759–1770, Aug. 2012.\n[19] J. Salamon, E. G ´omez, D. P. W. Ellis, and G. Richard.\nMelody extraction from polyphonic music signals: Ap-\nproaches, applications and challenges. IEEE Signal Pro-\ncessing Magazine, 31(2):118–134, March 2014.\n[20] D. Turnbull, G. Lanckriet, E. Pampalk, and M. Goto. A\nsupervised approach for detecting boundaries in music\nusing difference features and boosting. In Proceedings\nof the 8th International Society for Music Information\nRetrieval Conference (ISMIR), pages 51–54, 2007.\n[21] G. van Rossum, B. Warsaw, and N. Coghlan.\nPEP 8–style guide for python code. http://www.\npython.org/dev/peps/pep-0008 , 2001. Ac-\ncessed: 2014-04-30.\n[22] K. West, A. Kumar, A. Shirk, G. Zhu, J. S. Downie,\nA. Ehmann, and M. Bay. The networked environment\nfor music analysis (nema). In IEEE 6th World Congress\non Services (SERVICES 2010), pages 314–317. IEEE,\n2010.\n[23] C. Willis. MIREX task: Structural segmentation.\nhttp://www.music-ir.org/mirex/wiki/\n2013:Structural_Segmentation , 2013.\nAccessed: 2014-04-30.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n372"
    },
    {
        "title": "Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis.",
        "author": [
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416030",
        "url": "https://doi.org/10.5281/zenodo.1416030",
        "ee": "https://zenodo.org/records/1416030/files/RepettoS14.pdf",
        "abstract": "Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are in- timately combined. As an oral tradition, its musical di- mension is the result of the application of a series of pre- defined conventions and it offers unique concepts for mu- sicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, edito- rial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the con- tent of the corpus, and evaluate its suitability for compu- tational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, espe- cially for melodic analysis, and suggest approaches for future work.",
        "zenodo_id": 1416030,
        "dblp_key": "conf/ismir/RepettoS14",
        "keywords": [
            "Jingju",
            "Chinese traditional performing art",
            "Oral tradition",
            "Musical dimension",
            "Pre-defined conventions",
            "Musicological research",
            "Computational analyses",
            "CompMusic project",
            "Corpus of jingju music",
            "Acquisition criteria"
        ],
        "content": "CREATING A CORPUS OF JINGJU (BEIJING OPERA) MUSIC AND \nPOSSIBILITIES FOR MELODIC ANALYSIS \nRafael C aro Repetto  Xavier Serra  \nMusic Technology Group,  \nUniversitat Pompeu Fabra, Barcelona  Music Technology Group,  \nUniversitat Pompeu Fabra, Barcelona  \nrafael.caro@upf.edu  xavier.sera @upf.edu  \nABSTRACT \nJingju (Beijing opera) is a Chinese traditional performing \nart form in which theatrical and musical elements are i n-\ntimately combined. As an oral tradition, its musical d i-\nmension is the result of the application of a series of pre-\ndefined conventions and it offers unique concepts for m u-\nsicological research. Computational analyses of jingju \nmusic are still scarce, and only a few studies have dealt \nwith it from an MIR perspective. In this paper we present \nthe creation of a corpus of jingju music in the framework \nof the CompMusic project that is formed by audio, edit o-\nrial metadata, lyrics and scores. We discuss the criteria \nfollowed for the acquisition of the data, describe the con-\ntent of the corpus, and evaluate its suitability for comp u-\ntational and musicological research. We also identify \nseveral research problems that can take advantage of this \ncorpus in the context of computational musicology, esp e-\ncially for melodic analysis, and suggest approaches for \nfuture work. \n1. INTRODUCTION \nJingju (also known as Peking or Beijing opera) is one of \nthe most representative genres of xiqu, the Chinese trad i-\ntional form of performing arts. Just as its name suggests, \nit consists of a theatrical performance, xi, in which the \nmain expressive element is the music, qu. Although it has \ncommonalities with theatre and opera, it cannot be fully \nclassified as any of those. In xiqu there are not equivalent \nfigures to that of the theatre director or opera composer; \ninstead, the actor is the main agent for creativity and pe r-\nformance. Each of the skills that the actor is expected to \nmaster, encompassing poetry, declamation, singing, \nmime, dance and martial arts, is learned and executed as \npre-defined, well established conventions. It is precisely \nthe centrality of the actor and the acting through conve n-\ntions what make xiqu unique. Its musical content is also \ncreated by specific sets of such conventions. \nXiqu  genres developed as adaptations of the general \ncommon principles of the art form to a specific region, \nespecially in terms of dialect and music. The adoption of \nlocal dialects was a basic requirement for the intelligibi l-ity of the performance by local audiences. The phonetic \nfeatures of these dialects, including intonation and esp e-\ncially linguistic tones, establish a melodic and rhythmic \nframework for the singing. The musical material itself \nderives from local tunes, which is precisely the literal \nmeaning of qu. This implies that music in xiqu is not an \noriginal creation by the actors, but an adaptation of pre-\nexisting material. Furthermore, each genre employs also \nthe most representative instruments of the region for a c-\ncompaniment, conveying the regional filiation also ti m-\nbrally. These local features are what define each xiqu \ngenre’s individuality. Jingju is then the regional genre of \nxiqu that formed in Beijing during the 19th Century, \nachieving one of the highest levels of refinement and \ncomplexity. \nDespite the uniqueness of this tradition, the interesting \naspects for musicological analysis it offers, and its inte r-\nnational recognition, jingju music has barely been a p-\nproached computationally. Most of the few studies of \njingju in MIR have focused on its acoustic and timbral \ncharacteristics. Zhang and Zhou have drawn on these fe a-\ntures for classification of jingju in comparison with other \nmusic traditions [18 , 19] and other xiqu genres [20]. \nSundberg et al. have analyzed acoustically the singing of \ntwo role- types [14], whilst Tian et al. have extracted ti m-\nbral features for onset detection of percussion instruments \n[15]. More recently, Zhang and Wang have integrated \ndomain knowledge for musically meaningful segment a-\ntion of jingju arias [21]. Related to melodic analysis, \nChen [3] has implemented a computational analysis of \njinju music for the characterization of pitch intonation. \nThe main concepts that define jingju music are \nshengqiang , banshi  and role-type. As stated previously, \nthe melodic material used in xiqu genres is not original, \nbut derived from local tunes. These tunes share common \nfeatures that allow them to be recognized as pertaining to \nthat specific region, such as usual scale, characteristic \ntimbre, melodic structure, pitch range and tessitura, o r-\nnamentation, etc. This set of features is known as \nshengqiang , usually translated into English as ‘mode’ or \n‘modal system’ [16]. Each xiqu genre can use one or \nmore shengqiang , and one single shengqiang  can be \nshared by different genres. There are two main \nshengqiang in jingju, namely xipi and erhuang (see Table \n2). Their centrality in the genre is such that jingju music \nas a whole has been also named by the combination of \nthese two terms, pihuang .  © Rafael Caro Repetto, Xavier Serra.  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Rafael Caro Repetto , Xavier Serra . \n“Creating a corpus of jingju (Beijing opera) music and possibilities for \nmelodic analysis”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n313  \n \nThe melodic features determined by the shengqiang  \nare rhythmically rendered through a series of metrical \npatterns called banshi . These banshi  are individually l a-\nbelled and defined by a unit of metre, a tempo value and a \ndegree of melodic density; they are associated as well to \nan expressive function. The system of banshi  is co n-\nceived as derived from an original one, called yuanban , \nso that the rest of them are expansions, reductions or free \nrealizations of the first one [8]. The banshi  system in \njingju consists of a core of eight patterns commonly used , \nplus some variants. \nEach of the characters of a play is assigned to one sp e-\ncific, pre-defined acting class, according to their gender, \nage, social status, psychological profile and emotional \nbehavior. These acting classes are known as hangdang  or \nrole-types, and each actor is specialized in the perfo r-\nmance of one of them. Each role-type determines the sp e-\ncific set of conventions that must be used for the creation \nof the character, including those attaining to music. Co n-\nsequently, shengqiang and banshi  will be expressed di f-\nferently by each role-type, so that these concepts cannot \nbe studied without referencing each other. In jingju there \nare four general categories of role-types, with further \nsubdivisions. We consider that the five main role-types \nregarding musical expression are sheng  (male characters), \ndan (female characters), jing (painted-face), xiaosheng  \n(young males), and laodan  (old females). They are usua l-\nly classified into two styles of singing, the male style , \ncharacterized for using chest voice, used by sheng , jing \nand laodan , and the female one, sung in falsetto and \nhigher register, used by dan and xiaosheng . \nThe fullest expression of such melodic concepts occurs \nin the singing sections called changduan , which can be \ncompared, but not identified, with the concept of aria in \nWestern opera (to ease readability, we will use the term \n‘aria’ throughout the paper). Consequently, we have d e-\ntermined the aria as our main research object, and it has \nbeen the main concern for the creation of our corpus and \nthe analyses suggested. \nIn this paper we present a corpus of jingju music that \nwe have gathered for its computational analysis. We e x-\nplain the criteria followed for the collection of its diffe r-\nent types of data, describe the main features of the corpus \nand discuss its suitability for research. Thereupon we e x-\nplore the possibilities that the corpus offers to comput a-\ntional musicology, focusing in melodic analysis, specif i-\ncally in the concepts of shengqiang and role-type. \n2. JINGJU MUSIC RESEARCH CORPUS \nIn order to undertake a computational analysis of jingju \nmusic, and to exploit the unique musical concepts of this \ntradition from an MIR perspective, we have gathered in \nthe CompMusic project a research corpus [12] that in-\ncludes audio, editorial metadata, lyrics and scores. We \nintroduce here the criteria for the selection of the data , \ndescribe its content and offer a general evaluation. 2.1 Criteria for data collection \nFor the collection of audio recordings, which is the core \nof the corpus, we have considered three main criteria: \nrepertoire to be covered , sound quality and recording \nunit. In order to take maximum advantage of the unique \nfeatures of jingju music, we have gathered recordings of \nmostly traditional repertoire, as well as some modern \ncompositions based on the traditional methods. The so-\ncalled contemporary plays, since they have been created \nintegrating compositional techniques from the Western \ntradition, have been disregarded for our corpus. Regar d-\ning the sound quality needed for a computational anal y-\nsis, and considering the material to which we have had \naccess, the recordings that best suited our requirements \nhave been commercial CDs released in the last three de c-\nades in China. Finally, since our main research object is \nthe aria, we have acquired CDs releases of single arias \nper track. This means that full play or full scene CDs, and \nvideo material in VCD and DVD have not been consi d-\nered. The se CDs have  been the source from which we \nhave extracted the editorial metadata contained in the \ncorpus, which have been stored in MusicBrainz.1 This \nplatform assigns one unique ID to each enti ty in the co r-\npus, so that they can be easily searchable and retrievable. \nOur aim has been to include for each audio recording , \nwhenever possible, its corresponding lyrics and music \nscore . All releases gathered included the lyrics in their \nleaflets for the arias recorded. However, since they are \nnot usable for computational purposes, we get them from \nspecialized free repositories in the web.2 As for the scores, \nan explanation of its function in the tradition is first nee d-\ned. Since jingju  music ha s been created traditionally by \nactors, no composers, drawing on pre -existing material, \nscores appeared only as an aide -mémoire and a tool fo r \npreserving the repertoire. Although barely used by pr o-\nfessional actors, scores have been widely spread among \namateur singers and aficionados, and are a basic resource \nfor musicological research. In fact, in the last decades \nthere has been a remarkable ef fort to publish thoroughly \nedited collections of scores . Although many scores are \navailable in the web, they have not been systematically \nand coherently stored, what makes them no t easily r e-\ntrievable. Furthermore, they consist of image or pdf files, \nnot usable computationally.  Consequently, we have a c-\nquired printed publications that meet the academic stan d-\nards of edition , but that will require to be converted  into a \nmachine readable format . \n2.2 Description of the corpus \nThe audio data collection of the corpus is formed by 78 \nreleases containing 113 CDs , consisting of collections of \nsingle arias per track. Besides these, due to the fact that \n                                                           \n1http://musicbrainz.org/collection/40d0978b-0796-4734-9fd4-2b3ebe0f664c   \n2Our main sources for lyrics are the websites http://www.jingju.com , \nhttp://www.jingju.net  and http://www.xikao.com . \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n314  \n \nmany of the consumers of these recordings are amateur \nsingers, many releases contain extra CDs with just the \ninstrumental accompaniment of the arias recorded in the \nmain ones. Consequently, the corpus also contains 19 \nCDs with just instrumental accompaniments.  \nAlthough we do not have complete figures yet, we \nhave computed statistics for different aspects of the co r-\npus. The releases contain recordings by 74 singers, b e-\nlonging to 7 different role-types, as indicated in Table 1. \nAs for the works, the corpus covers 653 arias from 215 \ndifferent plays. Table 2 shows the distribution of these \narias according to role-type and shengqiang . Since the \nnumber of banshi  is limited and all of them frequently \nused, an estimation of its appearance in the corpus is not \nmeaningful. As shown in Table 2, the corpus contains \nhighly representative samples for the research of the two \nmain shengqiang  and the five main role-types as d e-\nscribed in section 1. Table 3 displays more detailed nu m-\nbers concerning these specific entities. The editorial \nmetadat a stored in MusicBrainz include textual info r-\nmation as well as cover art. For the former the original \nlanguage has been maintained, that is, Chinese in simpl i-\nfied characters, with romanizations in the Pinyin system, \nstored either as pseudo-releases or aliases. \nRegarding the scores, the corpus contains two colle c-\ntions of full play scores [5, 22] and an anthology of s e-\nlected arias [6]. The two collections contain a total of 155 \nplays, 26 of which appear in both publications; the a n-\nthology contains 86 scores. This material offers scores for \n317 arias of the corpus, that is 48.5% of the total. \nApart from the research corpus, but related to it, spe-\ncific test corpora will be developed , consisting of colle c-\ntions of data used as ground truth for specific research \ntasks, as defined by Serra [12]. The test corpora created \nin the framework of the CompMusic project are access i-\nble from the website http://compmusic.upf.edu/datasets . \nTo date there are two test corpora related to the jingju \nmusic corpus, namely the Beijing opera percussion i n-strument dataset ,3 which contains 236 audio samples of \njingju percussion instruments, used by Tian et al. [15] for \nonset detection of these instruments, and the Beijing \nopera percussion pattern dataset , 4 formed by 133 audio \nsamples of five jingju percussion patterns, supported by \ntranscriptions both in staff and syllable notations. Srin i-\nvasamurthy et al. [ 13] have used this dataset for the a u-\ntomatic recognition of such patterns in jingju recordings. \n2.3 Evaluation of the corpus \nFor the evaluation of the corpus, we will draw on some of \nthe criteria defined by Serra [ 12] for the creation of cu l-\nture specific corpora, specifically coverage and co m-\npleteness, and discuss as well the usability of the data for \ncomputational analyses. \n2.3.1. Coverage \nAssessing the coverage of the jingu music corpus is not \nan easy task, since, to the best of our knowledge, there is \nno reference source that estimates the number of plays in \nthis tradition. However, compared with the number of \ntotal plays covered in our collections of full play scores, \nwhich are considered to be the most prominent public a-\ntions in this matter, the number of plays represented in \nour corpus is considerable higher. Besides, these releases \nhave been purchased in the specialized bookshop located \nin the National Academy of Chinese Theatre Arts, Be i-\njing, the only institution of higher education in China \ndedicated exclusively to the training of xiqu actors, and \none of the most acclaimed ones for jingju. Our corpus \ncontains all the releases available in this bookshop at the \ntime of writing this paper that met the criteria settled in \nsection 2.1. Regarding the musical concepts represented \nin the corpus, Table 2 shows that both systems of role-\ntype and shengqiang  are equally fully covered, with un e-\nqual proportion according to their relevance in the trad i-\ntion, as explained in the introduction. As for the banshi , \nas stated previously, they are fully covered due to their \nlimited number and varied use . Consequently, we argue \nthat the coverage of our corpus is highly satisfactory, in \nterms of variety of repertoire, availability in the market \nand representation of musical entities. \n2.3.2. Completeness \nConsidering the musicological information needed for \neach recording according to our purposes, the editorial \nmetadata contained in the releases are fully complete, \nwith the exception of the 5 arias mentioned in Table 2 \n(0.8% of the total) , which lack information about \nshengqiang  and banshi . One important concept for af i-\ncionados of this tradition is the one of liupai , or perfor m-\ning schools. However, this concept is far from being well \ndefined, and depends both on the play and on the pe r-\n                                                           \n3http://compmusic.upf.edu/bo-perc-dataset  \n4http://compmusic.upf.edu/bopp-dataset  Role -types   Shengqiang  \nLaosheng  224  Xipi 324 \nJing 55  Erhuang  200 \nLaodan  66  Fan’erhuang  31 \nDan 257  Nanbangzi  25 \nXiaosheng  43  Sipingdiao  23 \nWusheng  3  Others  45 \nChou  5  Unknown  5 \n \nTable 2. Distribution of the arias in the corpus according \nto role -type and shengqiang .  Laosheng  20 Xiaosheng  9 \n Jing 7 Wusheng  3 \n Laodan  8 Chou  3 \n Dan 24   \nTable 1. Number of singers per role -type in the corpus.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n315  \n \nformer, and usually is not specifically stated in the relea s-\nes. Finally, the information related to the publication of \nthe recordings is not consistent . Usually, the dates of r e-\ncording and releasing are not available from the CDs. \nHowever, the releasing period has been restricted to the \nlast three decades by our criteria, as stated in section 2.1, \nalthough in some rare cases some of these releases may \ncontain recordings from earlier periods. \n2.3.3. Usability \nThe data contained in the corpus are fully suitable for \nanalysis of jingju music according to the musical co n-\ncepts explained in section 1. However, not all the data are \nequally usable. The main difficulty is presented by the \nscores, to date available only in printed edition. Cons e-\nquently, for their computational exploitation they need to \nbe converted into a machine readable format. In the \nCompMusic project we intend to use MusicXML, mai n-\ntaining the so called jianpu  notation used in the originals . \nAs for the lyrics, although most of them are freely acce s-\nsible on the web, due to the fact that singers may make \nsome changes according to their needs, some problems \nfor the recognition of related lyrics for a specific aria \nmight rise. \nTo access the corpus for research purposes, we refer to \nthe website http://compmusic.upf.edu/corpora . The co r-\npus will eventually be also available through Dunya [9],5 \na web based browsing tool developed by the CompMusic \nproject, which also displays content-based analyses ca r-\nried out in its framework for each of the culturally speci f-\nic corpora that it has gathered. \n3. RESEARCH POSSIBILITIES FOR THE JINGJU \nMUSIC CORPUS \nIn this section we introduce research issues of relevance \nfor each data type in our corpus with a special focus on \nthe melodic analysis. We discuss the application of state \nof the art analytic approaches to our corpus, and propose \nspecific future work. \n3.1 Analyses of audio, lyrics and scores \nAccording to the research objectives in the CompMusic \nproject, in whose framework our corpus has been gat h-\n                                                           \n5http://dunya.compmusic.upf.edu  ered, audio data is the main research object, supported by \ninformation from metadata, lyrics and scores. For the \nanalysis of the musical elements described in the first se c-\ntion, the vocal line of the arias is the most relevant ele-\nment, since it renders the core melody of the piece. Co n-\nsequently, segmentation of the vocal part and extraction \nof its pitch are needed steps. However, the timbral and \ntextural characteristics of jingju music pose important \nchallenges for these tasks. The timbre of the main a c-\ncompanying instrument, the jinghu, a small, two-stringed \nspike fiddle, is very similar to that of the voice. Besides, \nthe typical heterophonic texture results in the simultan e-\nous realization of different versions of the same melody. \nThese features make the extraction of the vocal line from \nthe accompaniment difficult. Besides, octave errors are \nstill frequent to state of the art algorithms for predom i-\nnant melody extraction, especially for role-types of the \nmale style of singing. \nIf audio is the main research objet, the other types of \ndata in our corpus offer equally interesting and challen g-\ning tasks for computational analysis. The delivery of the \nlyrics is the main goal of singing in jingju; therefore their \nanalysis is essential for the understanding of the genre. Of \nspecial importance for its musical implications is the \nanalysis of the poetic structure of the lyrics, since it d e-\ntermines the musical one, as well as their meaning, what \nwould help to better define the expressive function of \nshengqiang  and banshi . Methods from natural language \nprocessing can be applied for the identification of poetic \nformulae, commonly used by actors for the creation of \nnew lyrics. As for the scores, their analysis will be ben e-\nficial for the computation of intervallic preferences, cre a-\ntion of cadential schemata and detection of stable pitches. \nHowever, as stated previously, the main use of lyrics \nand scores according to our research purposes will be as \nsupporting elements for audio analysis. To that aim, the \nmain computational task is the alignment of both data \ntypes to audio. This is a challenging task, since the actors, \nin a tradition without the authority of a composer or \nplaywright, have certain margins to modify lyrics and \nmelody according to their own interpretation, as far as the \nmain features of the aria, as sanctioned by the tradition, \nare maintained. In the case of lyrics, this task is even \nmore complex due to the fact that jinju uses an art la n-\nguage of its own, that combines linguistic features from \ntwo dialects, the Beijing dialect and the Huguang dialect Role -type Singers  Xipi Erhuang  Total  \nRecordings  Duration  Recordings  Duration  Recordings  Duration  \nLaosheng  18 179 12h 09m 47s  147 13h 30m 51s  326 25h 40m 38s  \nJing 6 41 3h 04m 30s  43 3h 21m 39s  84 6h 26m 09s  \nLaodan  8 30 2h 00m 54s  52 4h 37m 54s  82 6h 38m 48s  \nDan 24 224 17h 26m 00s  101 11h 13m 02s  325 28h 39m 02s  \nXiaosheng  9 40 3h 19m 00s  7 41m 14s  47 4h 00m 14s  \nTotal  65 514 38h 00m 11s  350 33h 24m 40s  864 71h 24m 51s  \nTable  3. Data in our corpus for the analysis of the two main shengqiang  and five main role -types . \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n316  \n \nfrom the South [8, 16]. This combination is far from b e-\ning systematic and consistent, what in many cases poses \ndifficulties to the prediction of the phonetic represent a-\ntion required for lyrics to audio alignment. The music \ntr\naditions research ed in the CompMusic project present \nsimilar problems for these tasks, and specific approaches \nhave been proposed by Şentürk et al. [11] and Dzhamb a-\nzov et al. [4]. We intend to benefit from these works for \nthe development of specific methods for jingju music. \nAlignment of lyrics and scores to audio will be an i m-\nportant step for several analytical tasks. The information \nfrom these data types combined with the audio will allow \na musically informed segmentation of the recording, e i-\nther in vocal or instrumental sections, or in different \nstructural units, from the couplet, a poetic structure which \nis the basic unit also for the musical one, to the syllable \nlevel. The absolute pitch value of the first degree can be \ncomputed by the combined information from the score \nand the pitch track. Finally, an especially interesting topic \nis the study of how the tonal information of the syllable is \nexpressed in the melody. Zhang et al. [17] have applied \ncomputational methods to this issue with in the context of \nthe CompMusic project. \n3.2. Characterization of shengqiang  and role-type \nAs stated in the introduction, the two more relevant co n-\ncepts for the melodic aspect of jingju are shengqiang  and \nrole-type. Chen [3] has attempted a characterization of \nthese entities by means of pitch histograms. For the cla s-\nsification of audio fragments as vocal and non-vocal \nChen drew on machine learning, and extracted the pitch \nof the vocal line with the algorithm proposed by Salamon \nand Gómez [10]. In order to overcome some limitations \nof the results in this work, we have carried out an initial \nexperiment in which we have extracted pitch tracks for a \nsubset of 30 arias from our corpus that have been manua l-\nly pre-processed. The sample contains three arias for each \nof the ten combinations of the two main shengqiang  and \nthe five main role-types. We use mp3 mono files with a \nsampling rate of 44,100 Hz, and have annotated them \nwith Praat [1] to the syllable level for segmentation. Pitch \ntracks have been obtained with the aforementioned alg o-\nrithm [10] implemented in Essentia [2], whose param e-\nters have been manually set for each aria.  \nThe obtained pitch tracks have been used for the com-\nputation of pitch histograms. Koduri et al. have succes s-\nfully characterized Carnatic ragas by histogram peak pa r-\nametrization [7]. Chen has applied this methodology for \nthe characterization of male and female styles of singing \nin jingju. We have expanded the same approach to our \nsubset of 30 arias, with the aim of characterizing the ten \ncombinations of shengqiang  and role-types. Our initial \nobservations give some evidence that pitch histograms \nwill help describe some aspects of shengqiang  and role-\ntypes as stated in the musicological literature, such us \nmodal center, register with respect to the first degree, range and hierarchy of scale degrees, so that differences \ncan be established between each category. Our results a l-\nso show that the approach is efficient for the characteriz a-\ntion of different role-types for the same shengqiang . \nHowever, differences are not meaningful when the two \nshengqiang  are compared for one single role-type. Figure \n1 shows how the modal center for both xipi and erhuag in \na dan role-type is located around the fifth and sixth d e-\ng\nrees, register with respect to the first degree and range \nare practically identical, and the differences in the hiera r-\nchy of scale degrees are not relevant enough. \nXipi \n Erhuang  \n \nFigure 1 . Pitch histograms for the dan role-type in the \ntwo shengqiang . \nIn our future work we propose to expand this approach \nby integrating the information obtained from the lyrics \nand the scores. In the specific case of shengqiang , a work \nof melodic similarity between arias of the shengqiang  ac-\ncording to their musical structure, specially determined \nby the banshi , will shed light on the melodic identity of \nthese entities. As for the role-type, we argue that an ana l-\nysis of timbre, dynamics and articulation for each categ o-\nry, especially at the syllable level, will offer characteri z-\ning features that complete the information obtained from \nthe pitch histograms. \n3.3. Other research tasks \nBeyond the tasks described previously, jingju music offer \na wide range of research possibilities. One important a s-\npect is the rhythmic component of the arias, mainly d e-\ntermined by the concept of banshi . An automatic identif i-\ncation of banshi  and segmentation of the aria in these sec-\ntions is a musically meaningful, but computational cha l-\nlenging task, due to the different renditions of the same \nbanshi  by different role-types and even different actors, \nas well as to the rhythmic flexibility that characterizes \njingju music. The instrumental sections of the jingju arias \nare also an interesting research topic, especially regarding \ntheir relationship with the melody of the vocal part and \nhow shengqiang  and banshi  define their features. To this \ntask, the CDs with only accompaniment tracks will be \nvaluable. In the framework of the CompMusic project, \nSrinivasamurthy et al. [13] have presented a comput a-\ntional model for the automatic recognition of percussion \npatterns in jingju. Finally, due to the importance of the \nacting component of this genre, and its intimate relatio n-\nship with the music, jingju is a perfect case for a co m-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n317  \n \nbined research of visual and musical features, integrating \ncomputational analysis of video and audio material. \nShould this task be undertaken, our corpus should be e x-\npanded to include also video material. \n4. SUMMARY \nIn this paper we have presented a corpus of jingju music, \ngathered with the purpose of researching its musical fe a-\ntures from an MIR methodology. After discussing the cr i-\nteria for its creation, describing its different data types \nand offering a general evaluation, we have suggested an a-\nlytical tasks for its computational exploitation, especially \nfocus ed on melodic analysis. Some state of the art a p-\nproaches have been applied to a small sample of the co r-\npus, in order to analyze their results and propose cons e-\nquently further work and future tasks. \n5. ACKNOWLEDGEMENTS \nThis research was funded by the European Research \nCouncil under the European Union’s Seventh Framework \nProgram, as part of the CompMusic project (ERC grant \nagreement 267583). We are thankful to G. K. Koduri for \nproviding and helping with his code. \n6. REFERENCES \n[1] P. Boersma and  D. Weenink: “Praat, a system for \ndoing phonetics by computer,” Glot International , \nVol. 5, No. 9/10, pp. 341 –345. \n[2] D. Bogdanov, N. Wack, E. Gómez, S. Gulati, P. \nHerrera, O. Mayor, G. Roma, J. Salamon , J. Zapata, \nand X. Serra: “ESSENTIA: an Audio Analysis \nLibrary for Music Information Retrieval,” ISMIR \n2013 , pp. 423 –498, 2013 . \n[3] K. Chen: Characterization of Pitch Intonation of \nBeijing Opera , Master thesis, Universitat Pompeu \nFabra, Barcelona, 2013. \n[4] G. Dzhambazov, S. Şentürk , and X. Serra : \n“Automatic Lyrics- to-Audio Alignment in Classical \nTurkish Music ,” The 4th International Workshop on \nFolk Music Analysis , pp. 61 –64 , 2014. \n[5] Jingju qupu jicheng  京剧曲谱集成  (Collection of \njingju scores), 10 vols., Shanghai wenyi chubanshe, \nShanghai, 199 8. \n[6] Jingju qupu jingxuan  京剧曲谱精选  (Selected \nscores of jingju), 2 vols., Shanghai yinyue \nchubanshe, Shanghai, 1998 –2005.  \n[7] G. K. Koduri, V. Ishwar, J. Serrà, X. Serra, and H. \nMurthy: “Intonation analysis of ragas in Carnatic \nmusic,” Journal of New Music Research , Vol. 43, \nNo. 1, pp. 72 –93. \n[8] J. Liu 刘吉典 : Jingju yinyue gailun  京剧音乐概论  \n(Introduction to jingju music), Renmin yinyue \nchubanshe, Beijing, 1993.  [9] A. Porter, M. Sordo, and X. Serra: “Dunya : A \nsystem for browsing audio music collections \nexploiting cultural context,” ISMIR 2013 , pp. 101 –\n106, 2013. \n[10] J. Salamon and E. Gómez: “Melody Extraction From \nPolyphonic Music Signals Using Pitch Contour \nCharacteristics,” IEEE Transactions on Audio, \nSpeech, and Language Processing , Vol. 20, No. 6, \npp. 1759– 1770, 2012. \n[11] S. Şentürk, A. Holzapfel, and X. Serra: “Linking \nScores and Audio Recordings in Makam Music of \nTurkey,” JNMR , Vol. 43, No. 1, pp. 34 –52, 2014. \n[12] X. Serra: “Creating Research Corpora for the \nComputational Study of Music: the case of the \nCompMusic Project,” Proceedings of the AES 53rd \nInternational Conference , pp. 1 –9, 2014. \n[13] A. Srinivasamurthy, R. Caro Repetto, S. \nHarshavardhan , and X. Serra: “Transcription and \nRecognition of Syllable based Percussion Patterns: \nThe Case of Beijing Opera,” ISMIR 2014 . \n[14] J. Sundberg, L. Gu, Q. Huang, and P. Huang: \n“Acoustical study of classical Peking Opera \nsinging,” Journal of Voice , Vol. 26, No. 2, pp. 137 –\n143, 2012. \n[15] M. Tian, A. Srinivasamurthy, M. Sandler, and X. \nSerra: “A study of instrument -wise onset detection \nin Beijing opera percussion ensembles,” ICASSP \n2014 , pp. 2174 –2178 , 2014. \n[16] E. Wichmann: Listening to theatre: the aural \ndimension of Beijing opera , University of Hawaii \nPress, Honolulu, 1991. \n[17] S. Zhang, R. Caro Repetto, and X. Serra: “Study of \nsimilarity between linguistic tones and melodic \ncontours in Beijing Opera,”  ISMIR 2014 . \n[18] Y. Zhang and J. Zhou: “A Study on Content -Based \nMusic Classif ication,” Proceedings of the Seventh \nInternational Symposium on Signal Processing and \nIts Applications , pp. 113 –162, 2003. \n[19] Y. Zhang and J. Zhou: “Audio Segmentation Based \non Multi- Scale Audio Classification,” ICASSP 2004 , \npp. 349 –352, 2004. \n[20] Y. Zhang, J. Zh ou, and X. Wang: “A Study on \nChinese Traditional Opera,” Proceedings of the \nSeventh International Conference on Machine \nLearning and Cybernetics , pp. 2476 –2480, 2008. \n[21] Z. Zhang and X. Wang: “Structure Analysis of \nChinese Peking Opera,” Seventh International \nConference on Natural Computation ,  pp. 237 –241, \n2011.  \n[22] Zhongguo jingju liupai jumu jicheng  中国京剧流派\n剧目集成 (Collection of plays of Chinese jingju \nschools), 21 vols., Xueyuan chubanshe, Beijing, \n2006 –2010.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n318"
    },
    {
        "title": "Multi-Strategy Segmentation of Melodies.",
        "author": [
            "Marcelo Enrique Rodríguez-López",
            "Anja Volk",
            "Dimitrios Bountouridis"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418013",
        "url": "https://doi.org/10.5281/zenodo.1418013",
        "ee": "https://zenodo.org/records/1418013/files/Rodriguez-LopezVB14.pdf",
        "abstract": "Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a ‘single strategy’ (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and eval- uate a ‘multi-strategy’ system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a gi- ven boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We com- pare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the in- strumental set.",
        "zenodo_id": 1418013,
        "dblp_key": "conf/ismir/Rodriguez-LopezVB14",
        "keywords": [
            "melodic segmentation",
            "automatic music processing",
            "single strategy",
            "multiple cues",
            "symbolically encoded melodies",
            "multi-strategy system",
            "perceptual relevance",
            "boundary detection models",
            "plausible segmentation",
            "corpus of instrumental and vocal folk melodies"
        ],
        "content": "MULTI-STRATEGY SEGMENTATION OF MELODIES\nMarcelo Rodr ´ıguez-L ´opez\nUtrecht University\nm.e.rodriguezlopez@uu.nlAnja Volk\nUtrecht University\na.volk@uu.nlDimitrios Bountouridis\nUtrecht University\nd.bountouridis@uu.nl\nABSTRACT\nMelodic segmentation is a fundamental yet unsolved\nproblem in automatic music processing. At present most\nmelody segmentation models rely on a ‘single strategy’\n(i.e. they model a single perceptual segmentation cue).\nHowever, cognitive studies suggest that multiple cues need\nto be considered. In this paper we thus propose and eval-\nuate a ‘multi-strategy’ system to automatically segment\nsymbolically encoded melodies. Our system combines the\ncontribution of different single strategy boundary detection\nmodels. First, it assesses the perceptual relevance of a gi-\nven boundary detection model for a given input melody;\nthen it uses the boundaries predicted by relevant detection\nmodels to search for the most plausible segmentation of\nthe melody. We use our system to automatically segment a\ncorpus of instrumental and vocal folk melodies. We com-\npare the predictions to human annotated segments, and to\nstate of the art segmentation methods. Our results show\nthat our system outperforms the state-of-the-art in the in-\nstrumental set.\n1. INTRODUCTION\nIn Music Information Retrieval (MIR), segmentation refers\nto the task of dividing a musical fragment or a complete\npiece into smaller cognitively-relevant units (such as notes,\nmotifs, phrases, or sections). Identifying musical segments\naids (and in some cases enables) many tasks in MIR, such\nas searching and browsing large music collections, or vi-\nsualising and summarising music. In MIR there are three\nmain tasks associated with music segmentation: (1)the\nsegmentation of musical audio recordings into notes, as\npart of transcription systems, (2)the segmentation of sym-\nbolic encodings of music into phrases, and (3)the segmen-\ntation of both musical audio recordings and symbolic en-\ncodings into sections. In this paper we focus on the second\ntask, i.e. identifying segments resembling the musicolog-\nical concept of phrase. Currently automatic segmentation\nof music into phrases deals mainly with monophony. Thus,\nthis area is commonly referred to as melody segmentation.\nWhen targeting melodies, segmentation is usually re-\nc\rMarcelo Rodr ´ıguez-L ´opez, Anja V olk, Dimitrios Boun-\ntouridis.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Marcelo Rodr ´ıguez-L ´opez, Anja\nV olk, Dimitrios Bountouridis. “MULTI-STRATEGY SEGMENTATION\nOF MELODIES”, 15th International Society for Music Information Re-\ntrieval Conference, 2014.duced to identifying segment boundaries , i.e. locate the\npoints in time where one segment transitions into an-\nother.1Computer models of melody segmentation often\nfocus on modelling boundary cues, i.e. the musical factors\nthat have been observed or hypothesised to trigger human\nperception of boundaries. Two common examples of boun-\ndary cues are: (a) the perception of ‘gaps’ in a melody (e.g.\nthe sensation of a ‘temporal gap’ due to long note durations\nor rests) and (b) the perception of repetitions (e.g. recog-\nnising a melodic ﬁgure as a modiﬁed instance of a previ-\nously heard ﬁgure). The ﬁrst cue mentioned is thought to\nsignal the endof phrases, and conversely the second one is\nthought to signal the start of phrases.\nFindings in melodic segment perception studies suggest\nthat, even in short melodic excerpts, listeners are able to\nidentify multiple cues, and what is more, that the role and\nrelative importance of these cues seems to be contextual\n[3,6]. Yet, most computer models of melody segmentation\nrely on a single strategy, meaning that they often focus on\nmodelling a single type of cue. For instance, [4] focuses on\nmodelling cues related only to melodic gaps, while [1, 5]\naim to modelling cues related only to melodic repetitions.\nIn this paper we propose and evaluate a multi-strategy\nsystem that combines single strategy models of melodic\nsegmentation. In brief, our system ﬁrst estimates the cues\n(and hence the single strategy models) that might be more\n‘relevant’ for the segmentation of a particular input me-\nlody, combines the boundaries predicted by the models es-\ntimated relevant, and then selects which boundaries result\nin the ‘most plausible’ segmentation of the input melody.\nContribution: ﬁrst, we bring together single strategy\nmodels that have not been previously tested in combina-\ntion; second, our evaluation results show that our system\noutperforms the state-of-the-art of melody segmentation in\ninstrumental folk songs.\nThe remainder of this paper is organised as follows:\nx2 reviews music segmentation related work using multi-\nstrategy approaches, x3 presents a theoretical overview of\nthe proposed system, x4 describes implementation details\nof the system,x5 describes and discusses our evaluation of\nthe system, and ﬁnally, x6 provides conclusions and out-\nlines possibilities of future work.\n1Other subtasks associated to segmentation such as boundary pairing,\nas well as labelling of segments, are not considered.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2072. RELATED WORK\nMelody segmentation models often focus on modelling a\nsingle cue (e.g. [1, 4, 5]), leaving only a handful of models\nthat have proposed ways to combine different cues. Per-\nhaps the best known multi-strategy model is Grouper [11],\nwhich relies on three cues: temporal gaps, metrical par-\nallelism, and segment length. Grouper employs temporal\ngap detection heuristics to infer a set of candidate bound-\naries, and uses dynamic programming to ﬁnd an ‘optimal’\nsegmentation given the candidate boundaries and two reg-\nularisation constraints (metrical parallelism and segment\nlength). Grouper constitutes the current state-of-the-art in\nmelodic segmentation. However, Grouper relies entirely\non temporal information, and as such might have difﬁcul-\nties segmenting melodies with low rhythmic contrast or no\ndiscernible metric.\nAnother multi-strategy model is ATTA [7], which\nmerges gap, metrical, and self-similarity related cues. In\nATTA the relative importance of each cue is assigned man-\nually, requiring the tuning of over 25 parameters. Param-\neter tuning in ATTA is time consuming (estimated to be\n\u001810 mins per melody in [7]). Moreover, the parameters\nare non-adaptive (set at initialization), and thus make the\nmodel potentially insensitive to changes in the relative im-\nportance of a given cue during the course of a melody.\nThe main differences between the research discussed\nand ours are: (a) our system integrates single strategy mod-\nels that have not been previously used (and systematically\ntested) in combination, and (b) our system provides ways\nto select which single strategy models to use for a partic-\nular melody. Inx5.3.2 we compare our system to the two\nmodels that have consistently performed best in compara-\ntive studies, namely Grouper [11] and LBDM [4].2\n3. THEORETICAL OVERVIEW OF OUR SYSTEM\nIn this section we describe our system, depicted in Fig-\nure 1. In module 1, our system takes a group of single\nstrategy segmentation models (henceforth ‘cue models’),\nselects which might be more relevant to segment the cur-\nrent input melody, and combines the estimated boundary\nlocations into a single list. In module 2, the system as-\nsesses the segmentation produced by combinations of the\nselected boundary candidates in respect to corpus-learnt\npriors on segment contour and segment length. Below we\ndescribe in more detail the input/output characteristics of\nour system, as well as each processing module.\n3.1 Input/Output\nThe input to our system consists of a melody and a set\nof boundaries predicted by cue models. The melody is\nencoded as a sequence of temporally ordered note events\ne=e1;:::e i;:::;e n. Ineeach note event is represented\nby its chromatic pitch and quantized duration (onset, off-\nset) values. The output of our system is a set of ‘opti-\nmum’ boundary locations boptof lengthm, constituting\n2The manual tuning feature of ATTA made it impossible to include it\nin our evaluation.\necandidate boundaries + melody set of segment start positionsmelodic corpusModule 1\nModule 2segment\npriorscandidate\nsegments\nevaluation\ngenerated segmentsofpredictioncue relevance\nvoting scheme&melodic\nfeature\nextractioncue models melody encodingsymbolic\n&list of cues\nboundariespredictedFigure 1 . General diagram of our system. Within the mod-\nules#= input elements, and \u0003= processing stages.\na set of segments Sopt=fsig1\u0014i<m , where each segment\nsi= [bi;bi+1).\n3.1.1 Cue Models Characteristics\nEach cue model transforms einto a set of sequences, each\nrepresenting a melodic attribute (e.g. pitch class, inter-\nonset-interval, etc.). The speciﬁc set of attribute sequen-\nces produced by each cue model used within our system is\ndiscussed inx4.2. Each cue model processes the attribute\nsequences linearly, moving in steps of one event, produc-\ning a boundary strength proﬁle. A boundary strength pro-\nﬁle is simply a normalized vector of length n, where each\nelement value encodes the strength with which a cue mo-\ndel ‘perceives’ a boundary at the temporal location of the\nelement. In these proﬁles segment boundaries correspond\nto local maxima, and thus candidate boundary locations\nare obtained via peak selection. The method used to select\npeaks is discussed in x4.2.\n3.2 Module 1: Multiple-Cue Boundary Detection\nModule 1 takes as input a set of features describing the\nmelody, and a set of boundary locations predicted by cue\nmodels. Module 1 is comprised of two processing stages,\nnamely ‘cue relevance prediction’ and ‘voting scheme’.\nThe ﬁrst uses the input melodic features to estimate the\n‘relevance’ of a given cue for the perception of boundaries\nin the input melody, and the second merges and ﬁlters the\npredicted boundary locations.\n3.2.1 Cue Relevance Prediction\nFor a given set of kcue models C=fcig1\u0014i\u0014k ,\nand a set of hfeatures describing the melodies F=\nffjg1\u0014j\u0014h, we need to estimate how well a given cue mo-\ndel might perform under a given performance measure M\nasP(MjCi;Fj). In this paper we use the common F1,\nprecision, and recall measures to evaluate performance\n(seex5.2). In module 1 we focus on predicting a cue mo-\ndel’sprecision (assuming high recall can be achieved by\nthe combined set of candidate boundaries).\n3.2.2 Voting Scheme\nOnce we have estimated the relevance value of each cue\nmodel for the input melody P(MjCi;Fj), we combine the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n208candidate boundaries by simply adding the relevance val-\nues of candidate boundaries in close proximity (i.e. \u00061\nnote event apart). We assume that if boundaries from dif-\nferent cues are located \u00061note event apart, one of them\nmight be identifying a beginning and the other an end of\nsegment, and thus for the processing in module 2 is bene-\nﬁcial to keep both.\nThe ﬁnal output of this module is a single list of boun-\ndary locations b, each boundary with its own relevance\nvalue.\n3.3 Module 2: Optimality-Based Segment Formation\nModule 2 takes as input b,e, and length/contour3priors\ncomputed from a melodic corpus. The task of this module\nis to ﬁnd the ‘most plausible’ set of segments Soptfrom\nthe space of all possible candidate segmentations. The\nidea is to evaluate segmentations according to two em-\npirical constraints: one, melodic segments tend to show\nsmall deviations from a ‘typical’ segment length, and two,\nmelodic segments tend to show a reduced set of prototypi-\ncal melodic contour shapes. We address the task of ﬁnding\nthe most plausible set of segments given these two con-\nstraints as an optimisation problem. Thus, for a given can-\ndidate segmentation Sc=fsig1\u0014i<t , derived from a sub-\nset oftcandidate boundaries c2b, wheresi= [ci;ci+1),\nour cost function is deﬁned as:\nC(Sc) =t\u00001X\ni=1T(si) (1)\nwith\nT(si) = \b(si) +\u000b(\u0007(s i) + \t(s i)) (2)\nWhere,\n\u000f\b(si)is the cost associated to each candidate boun-\ndary demarcating si(i.e. the inverse of the relevance\nvalue of each candidate boundary).\n\u000f\u0007(si)is a cost associated to the deviation of skfrom\nan expected phrase contour. The cost of \u0007(si)is\ncomputed as\u0000log (\u0001)of the probability of the con-\ntour of the candidate phrase segment si.\n\u000f\t(si)is a cost of the deviation from the length of\nsifrom an expected length. The cost of \t(si)is\ncomputed as\u0000log (\u0001)of the probability of the length\nof the candidate phrase segment si.\n\u000f\u000bis a user deﬁned parameter that balances the boun-\ndary related costs against the segment related costs.\nDetails for the computation of Soptand priors on segment\nlength/contour are given in x4.4.\n3Melodic contour can be seen as an overall temporal development of\npitch height4. SYSTEM IMPLEMENTATION\nIn this section we ﬁrst describe the selection and tuning of\nthe cue models used within our system, then provide some\ndetails on the implementation of modules 1 and 2.\n4.1 Cue Models: Selection\nWe selected and implemented four cue models based\non two conditions: (a) the models have shown relatively\nhigh performance in previous studies, (b) the cues mod-\nelled have been identiﬁed as being important for melody\nsegmentation within music cognition studies. All imple-\nmented models follow the same processing chain, de-\nscribed inx3.1, i.e. each model derives a set of melodic\nattribute sequences, processes each sequence linearly, and\noutputs a boundary strength proﬁle bsp. Below we list and\nbrieﬂy describe the cue models used within our system.\nCM1 - gap detection: Melodic gap cues are assumed to\ncorrespond to points of signiﬁcant local change, e.g. a pitch\nor duration interval that is perceived as ‘overly large’ in\nrespect to its immediate vicinity. We implemented a model\nof melodic gap detection based on [4]. The model uses a\ndistance metric to measure local change,4and generates\nabspwhere peaks correspond to large distances between\ncontiguous melodic events. Large local distances are taken\nas boundary candidates.\nCM2 - contrast detection: Melodic contrast cues are as-\nsumed to correspond to points of signiﬁcant change (which\nrequire a mid-to-large temporal scale to be perceptually\ndiscernible), e.g. a change in melodic pace, or a change of\nmode. We implemented a contrast detection model based\non [9]. The model employs a probabilistic representation\nof melodic attributes and uses an information-theoretic di-\nvergence measure to determine contrast. The model gener-\nates abspwhere peaks correspond to large divergences be-\ntween attribute distributions representing contiguous sec-\ntions of the melody. The model identiﬁes boundaries by\nrecursively locating points of maximal divergence.\nCM3 - repetition detection: Melodic repetition cues are\nassumed to correspond to salient (exact or approximate)\nrepetitions of melodic material. We implemented a model\nto locate salient repeated fragments of a melody based on\n[5]. The model uses an exact-match string pattern search\nalgorithm to extract repeated melodic fragments, and in-\ncludes a method to score the salience of repetitions based\non the length, frequency, and temporal overlap of the ex-\ntracted fragments. The model generates a bspwhere peaks\ncorrespond to the starting points of salient repetitions.\nCM4 - closure detection: Tonal closure cues are assumed\nto correspond to points where an ongoing cognitive process\nof melodic expectation is disrupted. One way in which\nexpectation of continuation might be disrupted is when a\nmelodic event following a given context is unexpected. We\nimplemented an unexpected-event detection model based\non [8].5The model employs unsupervised probabilistic\n4The model employs both pitch and temporal information, but in our\ntests only temporal information is used\n5Our implementation is however less sophisticated than that of [8], as\nit requires the user to provide an upper limit for context length (specifed\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n209learning and prediction to measure the degree of unexpect-\nedness of each note event in the input melody, given a ﬁ-\nnite preceding context. The model generates a bspwhere\npeaks correspond to signiﬁcant increases in (information-\ntheoretic) surprise. Candidate boundaries are placed before\nsurprising note events.\n4.2 Cue Models: Tuning\nWe tuned the cue models used within our system to achieve\nmaximal precision. This involved a selection of melody\nrepresentation (choice of melodic attribute sequences to be\nprocessed),6tuning of parameters exclusive to the cue mo-\ndel, and choice and tuning of a peak selection mechanism.\nThe choice of attribute sequence selection and parame-\nter tuning per cue model is listed in Table 1. The abbrevi-\nations of melodic attributes correspond to: cp: chromatic\npitch, ioi: inter onset interval, ooi: onset to offset in-\nterval, cpiv: chromatic pitch interval, pcls: pitch class.\nTo select peaks as boundary candidates, we experimented\nwith several peak selection algorithms, settling for the al-\ngorithm proposed in [8].7This peak selection algorithm\nhas only one parameter k. The optimal values of kfor\neach cue model are given in the rightmost column of Ta-\nble 1. We also provide details on the choice of parameters\nexclusive to each cue model, for an elaboration on their in-\nterpretation we refer the reader to the original publications.\nCue model\nattribute sequence set parameters\nCM1fcpiv, ioi, ooig k= 2\nCM2f\npcls, ioig -\nCM3fcp,ioig F= 3\nL= 3\nO= 1\nk= 3\nCM4fcp,pcls, cpivg PPM-C, with exclusion\nSTM: order 5\nLTM: order 2\nLTM: 400 EFSC melodies\nk= 2:5\nTable 1. Attributes and parameter settings of cue models.\n4.3 Module 1: Predictors and Feature Selection\nTo evaluate cue relevance prediction, we ﬁrst select a sub-\nset of 200 boundary annotated melodies from the melodic\ncorpora used in this paper (see x5.1), and then run the cue\nmodels to obtain precision performance values for each\nmelody. To allow an estimation of precision we partition\nits range into a discrete set of categories.8\nas the Markov order in Table 1).\n6While some cue models, e.g. [4, 11] have already a preferred choice\nof melodic attribute representation, the other cue models used within our\nsystem allow for many choices, and where thus selected through experi-\nmental exploration.\n7This algorithm proved to work better than the alternatives for all\nmodels but CM3, for which its own peak selection heuristic worked best.\n8In our experiments we used a set dividing a model’s precision\ninto two categories (1:poor, 2:good ). The exact mapping precision :\n[0;1]!f1; 2gwas selected manually for each cue model, to ensure a\nsufﬁcient number of melodies representing each performance category is\navailable for training.To determine cue relevance prediction, we experimented\nwith several off-the-shelf classiﬁers available as part of\nWeka9. We selected features using the common BestFirst\nwith a 10-fold cross validation. The selected features were\nthose used in all folds.\nThe melodic features used to predict precision by the\nclassiﬁers where taken from the Fantastic10and jSym-\nbolic11feature extractor libraries, which add up to over\n200. After selection, 17 features are kept: ‘melody length’,\n‘pitch standard deviation, skewness, kurtosis, and entropy’,\n‘pitch interval standard deviation, skewness, kurtosis, and\nentropy’, ‘duration standard deviation, skewness, kurtosis,\nand entropy’, ‘tonal clarity’, ‘m-type mean entropy’, ‘m-\ntype Simpson’s D’, ‘m-type productivity’ (please refer to\nthe Fantastic library documentation for deﬁnitions).\nThe classiﬁers we experimented with are Sequential Min-\nimal Optimization (SMO, with the radial basis function\nkernel), K-Nearest Neighbours (K*) and Bayesian Network\n(BNet ). To evaluate each classiﬁer we use 10-fold cross\nvalidation. The classiﬁer with the best performance-to-\nefﬁciency ratio is SMO for models CM2-CM4, with an av-\nerage accuracy of 72:21%, and the simple K*forCM1 with\nan average accuracy of 66:37%.\n4.4 Module 2: Computation of Priors and Choice of \u000b\nTo compute the optimal sequence of segments Soptwe\nminimise the cost function in Eq. 1 using a formulation of\nthe Viterbi algorithm based on [10]. The minimisation of\nEq. 1 is subject to constraints on segment contour and seg-\nment length, and to a choice for parameter \u000b. We tuned\u000b\nmanually (a value of 0.6 worked best in our experiments).\nTo model constraints in segment contour and segment len-\ngth we use probability priors. Below we provide details on\ntheir computation.\nA priorP(contour (sk))is computed employing a\nGaussian Mixture Model (GMM). Phrase contours are\ncomputed using the polynomial contour feature of the Fan-\ntastic library. A contour model with four nodes was se-\nlected. The GMM (one Gaussian per node) is ﬁtted to con-\ntour distributions obtained from a subset of 1000 phrases\nselected randomly from the boundary annotated corpora\nused in this paper (see x5.1).\nA prior of segment length P(lk)is computed employing\na Gaussian ﬁtted to a distribution of lengths obtained from\nthe same 1000 phrase subset used to derive contours.\n5. EV ALUATION\nIn this section we describe our test database and evaluation\nmetrics, and subsequently describe experiments and results\nobtained by our system. A prototype of our system was im-\nplemented using a combination of Matlab, R, and Python.\nSource ﬁles and test data are available upon request.\n9http://www.cs.waikato.ac.nz/ml/weka/\n10http://www.doc.gold.ac.uk/ \u0018mas03dm/\n11http://jmir.sourceforge.net/jSymbolic.html\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2105.1 Melodic Corpora\nTo evaluate our system we employed a set of 100 instru-\nmental folk songs randomly sampled from the Liederen-\nbank collection12(LC) and 100 vocal folk songs randomly\nsampled from the German subset of the Essen Folk Song\nCollection13(EFSC). We chose to use the EFSC due to its\nbenchmark status in the ﬁeld of melodic segmentation. Ad-\nditionally, we chose to use the LCto compare the perfor-\nmance of segmentation models in vocal and non-vocal me-\nlodies.14\nTheEFSC consists of \u00186000 songs, mostly of German\norigin. The EFSC data was compiled and encoded from\nnotated sources. The songs are available in EsAC and\n**kern formats. The origin of phrase boundary markings\nin the EFSC has not been explicitly documented (yet it is\ncommonly assumed markings coincide with breath marks\nor phrase boundaries in the lyrics of the songs).\nThe instrumental (mainly ﬁddle) subset of the LCcon-\nsists of \u00182500 songs. The songs were compiled and en-\ncoded from notated sources. The songs are available in\nMIDI and**kern formats. Segment boundary mark-\nings for this subset comprise two levels: ‘hard’ and ‘soft’.\nHard (section) boundary markings correspond with struc-\ntural marks found in the notated sources. Soft (phrase)\nboundary markings correspond to the musical intuition of\ntwo experts annotators.15\n5.2 Evaluation Measures\nTo evaluate segmentation results, we encode both predicted\nand human-annotated phrase boundary markings as binary\nvectors. Using these vectors we compute the number of\ntrue positives tp(hits), false positives fp(insertions), and\nfalse negatives fn(misses).16We then quantify the simi-\nlarity between predictions and human annotations using\nthe well known F1 =2\u0001p\u0001r\np+r, where precision p=tp\ntp+fp\nand recall r=tp\ntp+fn. While the F1has its downsides (it\nassumes independence between boundaries),17it has been\nused extensively in the ﬁeld and thus allows us to establish\na comparison to previous research.\n5.3 Experiments & Results\nIn our experiments we compare our system to the melody\nsegmentation models that have consistently scored best in\ncomparative studies: GROUPER [11] and LBDM [4]. The\nﬁrst is a multi-strategy model, and the second a single stra-\n12http://www.liederenbank.nl/\n13http://www.esac-data.org\n14V ocal music has dominated previous evaluations of melodic segmen-\ntation (especially large-scale evaluations), which might give an incom-\nplete picture of the overall performance and generalisation capacity of\nsegmentation models\n15Instructions to annotate boundaries were related to performance prac-\ntice (e.g. “where would you change movement of bow”).\n16The ﬁrst and last boundaries are treated as trivial cases which cor-\nrespond, respectively, to the beginning and ending notes of a melodic\nphrase. These trivial cases are excluded from the evaluation. Also, we\nallow a tolerance of \u00061note event for the computation of tp.\n17By assuming independence between boundaries aspects such as seg-\nment length and position are discarded from the evaluationtegy (gap detection) model.18We also compare our sys-\ntem to its performance when only one module is active.\nAdditionally we compare to two na ¨ıve baselines: always,\nwhich predicts a segment boundary at every melodic event\nposition, and never which does not make predictions.\nTable 2 shows the performance results of all models\nover the instrumental and vocal melodic sets. We refer to\nour model as COMPLETE , and to the conﬁgurations when\neither module 1 or 2 are active as MOD 1ONand MOD 2ON,\nrespectively.\nWe tested the statistical signiﬁcance of the paired F1\ndifferences between the three conﬁgurations of our sys-\ntem, the two state-of-the-art models, and the baselines. For\nthe statistical testing we used a non-parametric Friedman\ntest (\u000b = 0:05). Furthermore, to determine which pairs\nof measurements signiﬁcantly differ, we conducted a post-\nhoc Tukey HSD test. All pair-wise differences among con-\nﬁgurations were found to be statistically signiﬁcant, except\nthose between MOD 1ONand MOD 2ONin the vocal set and\nbetween LBDM and MOD 2ONin the instrumental set. In\nTable 2 the highest performances are highlighted in bold.\nDatabase Instrumental V\nocal\nModel RPF1RPF1\nCO\nMPLETE 0.56 0.62 0.54 0.49 0.67 0.56\nGR\nOUPER 0.81 0.31 0.44 0.60 0.62 0.61\nLB\nDM 0.57 0.49 0.45 0.56 0.55 0.52\nMO\nD2ON 0.51 0.49 0.44 0.48 0.45 0.47\nMO\nD1ON 0.52 0.47 0.42 0.63 0.42 0.46\nalways 0.06 1.00 0.09 0.08 1.00 0.12\nnever 0.00 0.00 0.00 0.00 0.00 0.00\nTable 2. Performance of models and baselines sorted in\norder of mean recall R, precisionP, andF1for instrumen-\ntal and vocal melodies. The results presented in this table\nwere obtained comparing predictions to the ‘soft’ boun-\ndary markings of the LC.\n5.3.1 Summary of Main Results\nIn general, F1 performances obtained by the segmentation\nmodels in the vocal set are consistently higher than in the\ninstrumental set. This might be simply an indication that\nin the instrumental set melodies constitute a more chal-\nlenging evaluation scenario. However, the F1 differences\nmight also be an indication that relevant perceptual boun-\ndary cues are not covered by the evaluated models.\nIn the instrumental set, COMPLETE outperforms both\nLBDM and GROUPER by a relatively large margin (\u0015 10%).\nIn the vocal set, on the other hand, GROUPER obtains the\nbest performance. Below we discuss the three conﬁgura-\ntions of our system ( COMPLETE ,MOD 1ON,MOD 2ON).\n5.3.2 Discussion\nIn both melodic sets MOD 1ONshows considerably higher\nrecall than precision. These recall/precision differences\nagree with intuition, since the output of MOD 1ONcon-\nsists of the combination of all boundaries predicted by\n18For our tests we ran GROUPER and LBDM with their default settings.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n211the cue models, and can hence be expected to contain a\nrelatively large number of false positives. On the other\nhand, MOD 2ONshows smaller differences between preci-\nsion and recall values, and shows higher F1 performances\nthan MOD 1ONin both melodic sets (although the differ-\nence between performances is signiﬁcant only for the in-\nstrumental set). This last result highlights the robustness\nof the optimisation procedure driving MOD 2ON.19\nThe large F1 differences between MOD 1ON and\nMOD 2ONin respect to COMPLETE suggest that segmen-\ntation at the phrase level is a perceptual process which, de-\nspite happening in ‘real time’ (i.e. as music unfolds itself,\nrepresented more closely by module 1), might still require\nrepeated exposure and retrospective listening (represented\nmore closely by module 2).\nManual examination COMPLETE reveals that, when seg-\nmenting the vocal melody set, the prediction stage of mo-\ndule 1 tends to overestimate the importance of cue models\n(i.e. it often misclassiﬁes models as relevant when they are\nnot). However, when altering the settings of COMPLETE so\nthat the prediction stage of model 1 is more conservative\n(i.e. so that it predicts fewer boundaries), there is no sig-\nniﬁcant improvement in performance. Closer analysis of\nthese results points to a trade-off in performance, i.e. while\na conservative setting increases precision (predictions have\nfewer ‘false positives’), it also decreases recall (predictions\nhave fewer ‘correct positives’). This suggests that the pre-\ndiction stage of module 1 might require estimation of cue\nrelevance at a local level, i.e. on subsections of the melody\nrather than on the whole melody.\n6. CONCLUSION\nIn this paper we introduce a multi-strategy system for the\nsegmentation of symbolically encoded melodies. Our sys-\ntem combines the contribution of single strategy models\nof melody segmentation. The system works in two stages.\nFirst, it estimates how relevant the boundaries computed by\neach selected single strategy model are to the melody being\nanalysed, and then combines boundary predictions using\nheuristics. Second, it assesses the segmentation produced\nby combinations of the selected boundary candidates in re-\nspect to corpus-learnt priors on segment contour and seg-\nment length.\nWe tested our system on 100 vocal and 100 instrumen-\ntal folk song melodies. The performance of our system\nshowed a considerable (10% F1) improvement upon the\nstate-of-the-art in melody segmentation for instrumental\nfolk music, and showed to perform second best in the case\nof vocal folk songs.\nIn future work we will test if the relevance of cue mod-\nels can be accurately estimated for sections of the melody\n(and not the whole melody as it is done in this paper). This\n19If we consider that (with MOD 1ONbypassed) the number of candi-\ndate boundaries taken as input to MOD 2ONoften exceeds ‘correct’ (hu-\nman annotated) boundaries by a factor 2 or 3, then the number of possible\nsegmentations of the melody shows an exponential increase, leading to lo-\ncal minima issues, and so it would be reasonable to expect a performance\nequal or worse than that of MOD 1ON.‘local’ account of relevance might play a major role in im-\nproving the system’s precision. Also, we will incorporate a\nmore advanced model of prior segment knowledge of seg-\nment structure in our system. We hypothesise that a model\nof the characteristics of [2] could constitute a good alterna-\ntive to model not only segment length and contour, but also\nto incorporate knowledge of ‘template’ phrase structure\nforms. Lastly, we will continue testing our model’s gen-\neralisation capacity by evaluating on larger sample sizes\nand genres other than folk (for the latter the authors are\ncurrently in the process of annotating a corpus of Jazz me-\nlodies).\nAcknowledgments: We thank Frans Wiering, Remco\nVeltkamp, and the anonymous reviewers for the useful\ncomments on earlier drafts of this document. Marcelo\nRodr ´ıguez-L ´opez and Anja V olk (NWO-VIDI grant\n276-35-001) and Dimitrios Bountouridis (NWO-CATCH\nproject 640.005.004) are supported by the Netherlands Or-\nganization for Scientiﬁc Research.\n7. REFERENCES\n[1] S. Ahlb ¨ack. Melodic similarity as a determinant of\nmelody structure. Musicae Scientiae, 11(1):235–280,\n2007.\n[2] R. Bod. Probabilistic grammars for music. In Belgian-\nDutch Conference on Artiﬁcial Intelligence (BNAIC),\n2001.\n[3] M. Bruderer, M. McKinney, and A. Kohlrausch. The\nperception of structural boundaries in melody lines of\nwestern popular music. Musicae Scientiae, 13(2):273–\n313, 2009.\n[4] E. Cambouropoulos. The local boundary detection mo-\ndel (LBDM) and its application in the study of expres-\nsive timing. In Proceedings of the International Com-\nputer Music Conference (ICMC01), pages 232–235,\n2001.\n[5] E. Cambouropoulos. Musical parallelism and melodic\nsegmentation. Music Perception, 23(3):249–268, 2006.\n[6] E. Clarke and C. Krumhansl. Perceiving musical time.\nMusic Perception, pages 213–251, 1990.\n[7] M. Hamanaka, K. Hirata, and S. Tojo. ATTA: Au-\ntomatic time-span tree analyzer based on extended\nGTTM. In ISMIR Proceedings, pages 358–365, 2005.\n[8] M. Pearce, D. M ¨ullensiefen, and G. Wiggins. The role\nof expectation and probabilistic learning in auditory\nboundary perception: A model comparison. Percep-\ntion, 39(10):1365, 2010.\n[9] M. Rodr ´ıguez-L ´opez and A. V olk. Melodic segmenta-\ntion using the jensen-shannon divergence. In Interna-\ntional Conference on Machine Learning and Applica-\ntions (ICMLA12), volume 2, pages 351–356, 2012.\n[10] G. Sargent, F. Bimbot, E. Vincent, et al. A regularity-\nconstrained Viterbi algorithm and its application to the\nstructural segmentation of songs. In ISMIR Proceed-\nings, 2011.\n[11] D. Temperley. The cognition of basic musical struc-\ntures. MIT Press, 2004.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n212"
    },
    {
        "title": "LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics.",
        "author": [
            "Shoto Sasaki",
            "Kazuyoshi Yoshii",
            "Tomoyasu Nakano",
            "Masataka Goto",
            "Shigeo Morishima"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418075",
        "url": "https://doi.org/10.5281/zenodo.1418075",
        "ee": "https://zenodo.org/records/1418075/files/SasakiYNGM14.pdf",
        "abstract": "This paper presents a lyrics retrieval system called Lyric- sRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user’s intention behind a query when a word given as a query can be used in different con- texts. For example, the word“tears”can appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we pro- pose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent top- ics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are ar- ranged close to each other. The subjective experiments us- ing 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests.",
        "zenodo_id": 1418075,
        "dblp_key": "conf/ismir/SasakiYNGM14",
        "keywords": [
            "lyrics retrieval system",
            "interactive browsing",
            "visualizing topics",
            "latent Dirichlet allocation",
            "topic radar chart",
            "topic visualization",
            "two-dimensional space",
            "song lyrics",
            "similar topics",
            "user intention"
        ],
        "content": "LYRICSRAD AR:A LYRICSRETRIEVALSYSTEM\nBASEDON LATENTTOPICSOF LYRICS\nShotoSasaki∗1KazuyoshiYoshii∗∗2TomoyasuNakano∗∗∗3MasatakaGoto∗∗∗4ShigeoMorishima∗5\n∗WasedaUniversity ɹ∗∗KyotoUniversity\n∗∗∗NationalInstitute of AdvancedIndustrial Science and Technology(AIST)\n1joudanjanai-ss[at]akane.waseda.jp2yoshii[at]kuis.kyoto-u.ac.jp\n3;4(t.nakano,m.goto)[at]aist.go.jp5shigeo[at]waseda.jp\nABSTRACT\nThis paper presents a lyrics retrieval system called Lyric-\nsRadarthat enables users to interactively browse song\nlyricsbyvisualizingtheirtopics. Sinceconventionallyrics\nretrieval systems are based on simple word search, those\nsystemsoftenfailtoreﬂectuser’sintentionbehindaquery\nwhenawordgivenasaquerycanbeusedindifferentcon-\ntexts. Forexample,theword ʠtearsʡcanappearnotonlyin\nsadsongs(e.g.,feelheartrending),butalsoinhappysongs\n(e.g., weep for joy). To overcome this limitation, we pro-\nposetoautomaticallyanalyzeandvisualizetopicsoflyrics\nby using a well-known text analysis method called latent\nDirichlet allocation (LDA). This enables LyricsRadar to\noffertwotypesoftopicvisualization. Oneisthetopicradar\nchart that visualizes the relative weights of ﬁve latent top-\nics of each song on a pentagon-shaped chart. The other is\nradar-like arrangement of all songs in a two-dimensional\nspace in which song lyrics having similar topics are ar-\nrangedclosetoeachother. Thesubjectiveexperimentsus-\ning 6,902 Japanese popular songs showed that our system\ncan appropriately navigateusers to lyrics of interests.\n1. INTRODUCTION\nSome listeners regard lyrics as essential when listening to\npopular music. It was, however, not easy for listeners to\nﬁnd songs with their favorite lyrics on existing music in-\nformation retrieval systems. They usually happen to ﬁnd\nsongs with their favorite lyrics while listening to music.\nThegoalofthisresearchistoassistlistenerswhothinkthe\nlyricsareimportanttoencountersongswithunfamiliarbut\ninteresting lyrics.\nAlthough there were previous lyrics-based approaches\nfor music information retrieval, they have not provided an\ninterface that enables users to interactively browse lyrics\nof many songs while seeing latent topics behind those\nlyrics. We call these latent topics lyrics topics. Several\nc⃝Shoto Sasaki,\nKazuyoshi Yoshii, Tomoyasu Nakano,\nMasataka Goto, Shigeo Morishima.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Shoto Sasaki, Kazuyoshi Yoshii,\nTomoyasu Nakano, Masataka Goto, Shigeo Morishima. LyricsRadar: A\nLyrics Retrieval System Based on Latent Topics of Lyrics, 15th Interna-\ntional Society for Music Information RetrievalConference, 2014.\nFigure\n1. Overviewof topic modeling of LyricsRadar .\napproaches analyzed the text of lyrics by using natural\nlanguage processing to classify lyrics according to emo-\ntions, moods, and genres [2,3,11,19]. Automatic topic\ndetection [6] and semantic analysis [1] of song lyrics have\nalso been proposed. Lyrics can be used to retrieve songs\n[5] [10], visualize music archives [15], recommend songs\n[14], and generate slideshows whose images are matched\nwith lyrics [16]. Some existing web services for lyrics re-\ntrievalarebasedonsocialtags,suchas“love”and“gradu-\nation”. Those services are useful, but it is laborious to put\nappropriate tags by hands and it is not easy to ﬁnd a song\nwhose tags are also put to many other songs. Macrae et\nal.showedthatonlinelyricsareinaccurateandproposeda\nranking method that considers their accuracy [13]. Lyrics\nare also helpful for music interfaces: LyricSynchronizer\n[8] and VocaReﬁner [18], for example, show the lyrics of\na song so that a user can click a word to change the cur-\nrent playback position and the position for recording, re-\nspectively. Latent topics behind lyrics, however, were not\nexploitedto ﬁnd favoritelyrics.\nWe therefore propose a lyrics retrieval system, Lyric-\nsRadar, that analyzes the lyrics topics by using a machine\nlearningtechniquecalledlatentDirichletallocation(LDA)\nand visualizes those topics to help users ﬁnd their favorite\nlyrics interactively (Fig.1). A single word could have dif-\nferent topics. For example, “diet” may at least have two\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n585Figure\n2. Example display of LyricsRadar .\nlyrics topics. When it is used with words related to meal,\nvegetables,andfat,itslyricstopic“foodandhealth”could\nbe estimated by the LDA. On the other hand, when it is\nusedwithwordslikegovernment,law,andelections,“pol-\nitics” could be estimated. Although the LDA can esti-\nmatevariouslyricstopics,ﬁvetypicaltopicscommontoall\nlyrics in a given database were chosen. The lyrics of each\nsongarerepresentedbytheuniqueratiosoftheseﬁvetop-\nics, which are displayed as pentagon-shaped chart called\nas atopic radar chart . This chart makes it easy to guess\nthe meaning of lyrics before listening to its song. Further-\nmore,userscandirectlychangetheshapeofthischartasa\nquery to retrievelyrics havinga similar shape.\nInLyricsRadar , all the lyrics are embedded in a two-\ndimensional space, mapped automatically based on the ra-\ntios of the ﬁve lyrics topics. The position of lyrics is such\nthat lyrics in close proximity have similar ratios. Users\ncannavigateinthisplanebymouseoperationanddiscover\nsome lyrics which are located very close to their favorite\nlyrics.\n2. FUNCTIONALITYOF LYRICSRADAR\nLyricsRadar enablestobringagraphicaluserinterfaceas-\nsisting users to navigate in a two dimensional space in-\ntuitively and interactively to come across the target song.\nThis space is generated automatically by analysis of the\ntopics which appear in common with the lyrics of many\nmusicalpiecesindatabaseusingLDA.Alsoalatentmean-\ning of lyrics is visualized by the topic radar chart based\non the combination of topics ratios. Lyrics that are similar\ntoauser’spreference(target )canbeintuitivelydiscovered\nby clicking of the topic radar chart or lyrics representingby dots. So this approach cannot be achieved at all by the\nconventionalmethodwhichdirectlysearchesforasongby\nthekeywordsorphrasesappearinginlyrics. Sincelinguis-\nticexpressionsofthetopicarenotnecessary,usercanﬁnd\na target song intuitively even when user does not have any\nknowledgeabout lyrics.\n2.1 Visualizationbased on the topic of lyrics\nLyricsRadar hasthefollowingtwovisualizationfunctions:\n(1) the topic radar chart; and (2) a mapping to the two-\ndimensional plane. Figure 2 shows an example display\nof our interface. The topic radar chart shown in upper-\nleft corner of Figure 2 is a pentagon-shape chart which\nexpresses the ratio of ﬁve topics of lyrics. Each colored\ndot displayed in two dimensional plane shown in Figure\n2 means the relative location of lyrics in a database. We\ncall these colored dot representations of lyrics lyrics dot.\nUser can see lyrics, its title and artist name, and the topic\nratiobyclickingthelyricsdotplacedonthe2Dspace,this\nsupports to discover lyrics interactively. While the lyrics\nmapping assists user to understand the lyrics topic by the\nrelative location in the map, the topic radar chart helps to\nget the lyrics image intuitively by the shape of chart. We\nexplaineach of these in the followingsubsections.\n2.1.1 Topicradarchart\nThe values of the lyrics topic are computed and visualized\nas the topic radar chart which is pentagon style. Each ver-\ntexofthepentagoncorrespondstoadistincttopic,andpre-\ndominant words of each topic (e.g., “heart”, “world”, and\n“life” for the topic 3) are also displayed at the ﬁve corner\nof pentagon shown in Figure 2. The predominant words\nhelp user to guess the meaning of each topic. The center\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n586Figure\n3. Anexampledisplayoflyricsbyaselectedartist.\nof the topic radar chart indicates 0 value of a ratio of the\nlyricstopicinthesamemannerasthecommonradarchart.\nSincethesumoftheﬁvecomponentsisaconstantvalue,if\ntheratioofatopicstandsout,itwillclearlybeseenbythe\nuser. Itis easy tograsp the topicof selected lyricsvisually\nand to makean intuitivecomparison between lyrics.\nFurthermore, the number of topics in this interface is\nset to ﬁve to strike a balance between the operability of\ninterfaceand the varietyof topics1.\n2.1.2 Plane-mapped lyrics\nThe lyrics of musical pieces are mapped onto a two-\ndimensional plane, in which musical pieces with almost\nthesametopicratiocangetclosertoeachother. Eachmu-\nsical piece is expressed by colored dot whose RGB com-\nponents are corresponding to 3D compressed axis for ﬁve\ntopics’ values. This space can be scalable so that the local\nor global structure of each musical piece can be observed.\nThe distribution of lyrics about a speciﬁc topic can be rec-\nognized by the color of the lyrics. The dimension com-\npression in mapping and coloring used t-SNE [9]. When\na user mouseovers a point in the space, it is colored pink\nand meta-information about the title, artist, the topic radar\nchart appears simultaneously.\nBy repeating mouseover, lyrics and names of its artist\nand songwriter are updated continuously. Using this ap-\nproach, other lyrics with the similar topics to the input\nlyrics can be discovered. The lyrics map can be moved\nandzoomedbydraggingthemouseorusingaspeciﬁckey-\nboardoperation. Furthermore,itispossibletovisualizethe\nlyrics map specialized to artist and songwriter, which are\n1If the\nnumber of topics was increased, a more subdivided and exact-\ningsemanticcontentcouldhavebeenrepresented;however,theoperation\nfor a user will be getting more complicated.\nFigure\n4. Mapping of 487 English artists.\nassociated with lyrics as metadata. When an artist name\nis chosen, as shown in the right side of Figure 3, the point\noftheartist’slyricswillbegettingyellow;similarly,when\na songwriter is chosen, the point of the songwriter’s lyrics\nwill be changed to orange. While this is somewhat equiv-\nalent to lyrics retrieval using the artist or songwriter as a\nquery,itisourinnovativepointinthesensethatausercan\nintuitivelygrasphowartistsandsongwritersaredistributed\nbased on the ratio of the given topic. Although music re-\ntrieval by artist is very popular in a conventional system, a\nretrieval by songwriter is not focused well yet. However,\nin the meaning of lyrics retrieval, it is easier for search by\nsongwriter to discover songs with one’s favorite lyrics be-\ncause a songwriter has his ownlyrics vocabulary.\nMoreover, we can make a topic analysis depending on\naspeciﬁcartistinoursystem. Intuitivelysimilarartistsare\nalso located and colored closer in a topic chart depending\non topic ratios. The artist is colored based on a topic ratio\nin the same way as that of the lyrics. In Figure 4, the size\nof a circle is proportional to the number of musical pieces\neach artist has. In this way, other artists similar to one’s\nfavoriteartist can be easily discovered.\n2.2 Lyricsretrievalusing topic of lyrics\nInLyricsRadar ,inadditiontotheabilitytotraverseandex-\nplore a map to ﬁnd lyrics, we also propose a system to di-\nrectly enter a topic ratio as an intuitiveexpressionof one’s\nlatent feeling. More speciﬁcally, we consider the topic\nradar chart as an input interface and provide a means by\nwhichausercangivetopicratiosforﬁveelementsdirectly\nto search for lyrics very close to one’s latent image. This\ninterfacecansatisfythesearchqueryinwhichauserwould\nlike to search for lyrics that contain more of the same top-\nics using the representative words of each topic. Figure\n5 shows an example in which one of the ﬁve topics is in-\ncreasedbymousedrag,thenthebalanceofﬁvetopicsratio\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n587Figure\n5. Anexampleofthedirectmanipulationofthetopicratioonthetopicradarchart. Eachtopicratiocanbeincreased\nby dragging the mouse.\nhas changed because the sum of ﬁve components is equal\nto1.0. Ausercanrepeattheseprocessesbyupdatingtopics\nratios or navigating the point in a space interactively until\nﬁnding interesting lyrics. As with the above subsections,\nwe have substantiated our claims for a more intuitive and\nexploratorylyrics retrievalsystem.\n3. IMPLEMENTATIONOF LYRICSRADAR\nLyricsRadar used LDA [4] for the topic analysis of lyrics.\nLDAisatypicaltopicmodelingmethodbymachinelearn-\ning. SinceLDAassignseachwordwhichconstituteslyrics\nto a different topic independently, the lyrics include a va-\nriety of topics according to the variation of words in the\nlyrics. In our system, Ktypical topics which constitute\nmany lyrics in database are estimated and a ratio to each\ntopic is calculated for lyrics with unsupervised learning.\nAs a result, appearance probability of each word in every\ntopiccanbecalculated. Thetypicalrepresentativewordto\neach topic can be decided at the same time.\n3.1 LDAforlyrics\nThe observed data that we consider for LDA are Dinde-\npendent lyrics X=fX1, ...,XDg. The lyrics Xdcon-\nsist of Ndword series Xd=fxd;1, ...,xd;Ndg. The size\nof all vocabulary that appear in the lyrics is V,xd;nis\naV-dimensional “1-of-K “ vector (a vector with one el-\nement containing 1 and all other elements containing 0).\nThe latent variable (i.e., the topics series) of the observed\nlyrics XdisZd=fzd;1, ...,zd;Ndg. The number of top-\nics is K, sozd;nindicates a K-dimensional 1-of- Kvec-\ntor. Hereafter, all latent variables of lyrics Dare indicated\nZ=fZ1, ...,ZDg. Figure 6 shows a graphical represen-\ntation of the LDA model used in this paper. The full joint\ndistributionis givenby\np(X,Z,π,ϕ) = p(XjZ, ϕ)p(Zjπ)p(π )p(ϕ)(1)\nwhere πindicates the mixing weights of the multiple top-\nics of lyrics ( Dof the K-dimensional vector) and ϕin-\ndicates the unigram probability of each topic (K of the\nV-dimensional vector). The ﬁrst two terms are likelihood\nFigure\n6. Graphical representation of the latent Dirichlet\nallocation (LDA).\nfunctions, whereas the other two terms are prior distribu-\ntions. The likelihoodfunctions themselvesare deﬁned as\np(XjZ, ϕ) =D∏\nd=1Nd∏\nn=1V∏\nv=1(K∏\nk=1ϕzd;n;k\nk;v)xd;n;v\n(2)\np(Zjπ) =D∏\nd=1Nd∏\nn=1K∏\nk=1πzd;n;k\nd;k(3)\nWethen introduce conjugatepriors as\np(π) =D∏\nd=1Dir(πdjα(0)) =D∏\nd=1C(α(0))K∏\nk=1π\u000b(0)\u00001\nd;k\n(4)\np(ϕ) =K∏\nk=1Dir(ϕ kjβ(0)) =K∏\nk=1C(β(0))V∏\nv=1ϕ\f(0)\nv\u00001\nk;v\n(5)\nwhere p(π)andp(ϕ)are products of Dirichlet distribu-\ntions,α(0)andβ(0)arehyperparameters,and C(α(0))and\nC(β(0))are normalization factorscalculated as follows:\nC(x) =\u0000(^x)\n\u0000(x 1)\u0001 \u0001\n\u0001\u0000(xI),^x=I∑\ni=1xi(6)\nAlsonotethat πisthetopicmixtureratiooflyricsused\nas the topic radar chart by normalization. The appearance\nprobability ϕof the vocabulary in each topic was used to\nevaluate the high-representative word that is strongly cor-\nrelated with each topic of the topic radar chart.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5883.2 T\nrainingof LDA\nThe lyrics database contains 6902 Japanese popular songs\n(J-POP) and 5351 English popular songs. Each of these\nsongs includes more than 100 words. J-POP songs are se-\nlected from our own database and English songs are from\nMusicLyricsDatabase v.1.2.72. J-POPdatabase has 1847\nartists and 2285 songwriters and English database has 398\nartists. For the topic analysis per artist, 2484 J-POP artists\nand 487 English artists whose all songs include at least\n100 words are selected. 26229 words in J-POP and 35634\nwords in English which appear more than ten times in all\nlyrics is used for the value Vwhich is the size of vocabu-\nlary in lyrics. In J-POP lyrics, MeCab [17] was used for\nthe morphological analysis of J-POP lyrics. The noun,\nverb, and adjective components were extracted and then\nthe original and the inﬂected form were counted as one\nword. In English lyrics, we use stopwords using Full-Text\nStopwords in MySQL3to remove commonly-used words.\nHowever,wordswhichappearedofteninmanylyricswere\ninconvenient to analyze topics. To lower the importance\nofsuchwordsinthetopicanalysis,theywereweightedby\ninversedocument frequency(idf).\nInthetrainingtheLDA,thenumberoftopics (K)isset\nto5. All initial values of hyperparameters α(0)andβ(0)\nwere set to 1.\n4. EVALUATIONEXPERIMENTS\nTo verify the validity of the topic analysis results (as re-\nlated to the topic radar chart and mapping of lyrics) in\nLyricsRadar ,weconductedasubjectiveevaluationexperi-\nment. There were 17 subjects (all Japanese speakers) with\nages from 21 to 32. We used the results of LDA for the\nlyrics of the 6902 J-POP songs described in Section 3.2.\n4.1 Evaluationof topic analysis\nOur evaluation here attempted to verify that the topic ra-\ntio determined by the topic analysis of LDA could appro-\npriately represent latent meaning of lyrics. Furthermore,\nwhen the lyrics of a song are selected, relative location to\nother lyrics of the same artist or songwriter in the space is\ninvestigated.\n4.1.1 Experimental method\nIn our experiment, the lyrics of a song are selected at ran-\ndom in the space as basislyrics and also targetlyrics of\nfour songs are selected to be compared according to the\nfollowingconditions.\n(1)Thelyrics closest to the basislyricson lyrics map\n(2)The lyrics closest to the basislyrics with same song-\nwriter\n(3)Thelyrics closest to the basislyricswith same artist\n2“Music L\nyrics Database v.1.2.7,” http://www.odditysoftware.\ncom/page-datasales1.htm .\n3“Full-Text Stopwords in MySQL,” http://dev.mysql.com/doc/\nrefman/5.5/en/fulltext-stopwords.html.\nFigure\n7. Results of our evaluation experiment to evalu-\nate topic analysis; the score of (1) was the closest to 1.0,\nshowingour approach to be effective.\n(4)Thelyrics selected at random\nEach subject evaluated the similarity of the impression\nreceivedfromthetwolyricsusingaﬁve-stepscale(1: clos-\nest, 2: somehow close, 3: neutral, 4: somehow far, and 5:\nmost far), comparing the basislyrics and one of the target\nlyrics after seeing the basislyrics. Presentation order to\nsubjectswasrandom. Furthermore,eachsubjectdescribed\nthe reason of evaluationscore.\n4.1.2 Experimental results\nTheaveragescoreoftheﬁve-stepevaluationresultsforthe\nfourtargetlyrics by all subjects is shown in the Figure 7.\nAs expected, lyrics closest to the basislyrics on the lyrics\nmap were evaluated as the closest in terms of the impres-\nsionofthe basislyrics,becausethescoreof(1)wasclosest\nto1.0. Resultsof targetlyrics(2)and(3)werebothcloseto\n3.0. Thelyricsclosesttothe basislyricsofthesamesong-\nwriterorartistastheselectedlyricsweremostlyjudgedas\n“3: neutral.”Finally,thelyricsselectedatrandom(4)were\nappropriately judged to be far.\nAs the subjects’ comments about the reason of de-\ncision, we obtained such responses as a sense of the\nseason, positive-negative, love, relationship, color, light-\ndark,subjective-objective,andtension. Responsesdiffered\ngreatly from one subject to the next. For example, some\nfelt the impression only by the similarity of a sense of the\nseasonoflyrics. TrialusageofLyricsRadarhasshownthat\nit is a useful tool for users.\n4.2 Evaluationof the number of topics\nThe perplexity used for the quality assessment of a lan-\nguagemodelwascomputedforeachnumberoftopics. The\nmore the model is complicated, the higher the perplexity\nbecomes. Therefore,wecanestimatethattheperformance\nof language model is good when the value of perplexity is\nlow. Wecalculated perplexityas\nperplexity (X) = exp(\n\u0000∑D\nd=1logp(X d)\n∑D\nd=1Nd)\n(7)\nIn case\nthe number of topics (K)is ﬁve, the perplexity is\n1150 which is evenhigh.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n589Figure\n8. Perplexityfor the number of topics.\nOntheotherhand,becauseMillershowedthatthenum-\nber of objects human can hold in his working memory is\n7\u00062[7],thenumberoftopicsshouldbe1to5inorderto\nobtain information naturally. So we decided to show ﬁve\ntopics in the topic radar chart.\nFigure8showscalculationresultsofperplexityforeach\ntopic number. Blue points represent perplexity for LDA\napplied to lyrics and red points represent perplexity for\nLDAappliedtoeachartist. Orangebarindicatestherange\nofhuman capacity for processing information. Since there\nexists a tradeoff between the number of topics and oper-\nability,we found that ﬁveis appropriate number of topics.\n5. CONCLUSIONS\nIn this paper, we propose LyricsRadar , an interface to as-\nsistausertocomeacrossfavoritelyricsinteractively. Con-\nventionally lyrics were retrieved by titles, artist names, or\nkeywords. Our main contribution is to visualize lyrics in\nthe latent meaning level based on a topic model by LDA.\nByseeingthepentagon-styleshapeofTopicRadarChart,a\nuser can intuitively recognize the meaning of given lyrics.\nTheusercanalsodirectlymanipulatethisshapetodiscover\ntargetlyrics even when the user does not know any key-\nword or any query. Also the topic ratio of focused lyrics\ncan be mapped to a point in the two dimensional space\nwhich visualizes the relative location to all the lyrics in\nourlyricsdatabaseandenablestheusertonavigatesimilar\nlyrics by controlling the point directly.\nFor future work, user adaptation is inevitable task be-\ncause every user has an individual preference, as well as\nimprovementsto topic analysis by using hierarchical topic\nanalysis[12]. Furthermore,torealizetheretrievalinterface\ncorrespondingtoaminortopicoflyrics,afuturechallenge\nistoconsiderthevisualizationmethodthatcanreﬂectmore\nnumbers of topics by keepingan easy-to-use interactivity.\nAcknowledgment: Thisresearchwassupportedinpart\nbyOngaCREST,CREST,JST.\n6. REFERENCES\n[1] B. Logan et al.: “Semantic Analysis of Song Lyrics,” Pro-\nceedingsof IEEE ICME 2004 Vol.2,pp. 827–830, 2004.[2] C. Laurier et al.: “Multimodal Music Mood Classiﬁcation\nUsing Audio and Lyrics,” Proceedings of ICMLA 2008, pp.\n688–693,2008.\n[3] C. McKay et al.: “Evaluating the genre classiﬁcation perfor-\nmance of lyrical features relative to audio, symbolic and cul-\ntural features,” Proceedings of ISMIR 2008 , pp. 213–218,\n2008.\n[4] D. M. Blei et al.: “Latent Dirichlet Allocation,” Journal of\nMachineLearning Research Vol.3,pp. 993–1022, 2003.\n[5] E.BrochuandN.deFreitas: ““NameThatSong!”: AProba-\nbilistic Approach to Querying on Music and Text,” Proceed-\ningsof NIPS 2003 ,pp. 1505–1512, 2003.\n[6] F.Kleedorfer etal.: “OhOhOhWhoah! TowardsAutomatic\nTopicDetectionInSongLyrics,” ProceedingsofISMIR2008 ,\npp. 287–292, 2008.\n[7] G. A. Miller: “The magical number seven, plus or minus\ntwo: Some limits on our capacity for processing informa-\ntion,”JournalofthePsychologicalReview Vol.63(2),pp. 81–\n97,1956.\n[8] H. Fujihara et al.: “LyricSynchronizer: Automatic Synchro-\nnizationSystembetweenMusicalAudioSignalsandLyrics,”\nJournal of IEEE Selected Topics in Signal Processing, Vol.5,\nNo.6,pp. 1252–1261, 2011.\n[9] L.MaatenandG.E.Hinton: “VisualizingHigh-Dimensional\nData Using t-SNE,” Journal of Machine Learning Research ,\nVol.9,pp. 2579–2605, 2008.\n[10] M. M ¨ulleret al.: “Lyrics-based Audio Retrieval and Mul-\ntimodal Navigation in Music Collections,” Proceedings of\nECDL2007 , pp. 112–123, 2007.\n[11] M. V. Zaanen and P. Kanters: “Automatic Mood Classiﬁca-\ntion Using TF*IDF Based on Lyrics,” Proceedings of ISMIR\n2010, pp. 75–80, 2010.\n[12] R. Adams et al.: “Tree-Structured Stick Breaking Processes\nfor Hierarchical Data,” Proceedings of NIPS 2010 , pp. 19–\n27,2010.\n[13] R.MacraeandS.Dixon: “RankingLyricsforOnlineSearch,”\nProceedingsof ISMIR 2012 , pp. 361–366, 2012.\n[14] R. Takahashi et al.: “Building and combining document and\nmusicspacesformusicquery-by-webpagesystem,” Proceed-\ningsof Interspeech2008 , pp. 2020–2023, 2008.\n[15] R. Neumayer and A. Rauber: “Multi-modal Music Informa-\ntionRetrieval: VisualisationandEvaluationofClusteringsby\nBoth Audio and Lyrics,” Proceedings of RAO 2007 , pp. 70–\n89,2007.\n[16] S.Funasawa etal.: “AutomatedMusicSlideshowGeneration\nUsing Web Images Based on Lyrics,” Proceedings of ISMIR\n2010, pp. 63–68, 2010.\n[17] T.Kudo: “MeCab: YetAnotherPart-of-SpeechandMorpho-\nlogical Analyzer,” http://mecab.googlecode.com/\nsvn/trunk/mecab/doc/index.html.\n[18] T. Nakano and M. Goto: “VocaReﬁner: An Interactive\nSinging Recording System with Integration of Multiple\nSinging Recordings,” Proceedings of SMC 2013 , pp. 115–\n122,2013.\n[19] Y. Hu et al.: “Lyric-based Song Emotion Detection with Af-\nfective Lexicon and Fuzzy Clustering Method,” Proceedings\nofISMIR 2009 , pp. 122–128, 2009.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n590"
    },
    {
        "title": "On The Changing Regulations of Privacy and Personal Information in MIR.",
        "author": [
            "Pierre Saurel",
            "Francis Rousseaux",
            "Marc Danger"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416638",
        "url": "https://doi.org/10.5281/zenodo.1416638",
        "ee": "https://zenodo.org/records/1416638/files/SaurelRD14.pdf",
        "abstract": "In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regula- tions to take effect in the coming years regarding how such information can be collected, stored and manipulat- ed, with equally strict penalties for being found in viola- tion of the law. Here, we provide a summary of these changes, consid- er how they relate to our data sources and research prac- tices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such chang- es might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our per- sonal legal interpretations are strictly mentioned for illus- tration purpose, and reader should seek proper legal counsel.",
        "zenodo_id": 1416638,
        "dblp_key": "conf/ismir/SaurelRD14",
        "keywords": [
            "user feedback",
            "human subjects data",
            "personal information",
            "European Union regulations",
            "new strict regulations",
            "data collection",
            "data storage",
            "data manipulation",
            "legal penalties",
            "subject-friendly research"
        ],
        "content": "ON THE CHANGING REGULATIONS OF PRIVACY \nAND PERSONAL INFORMATION IN MIR  \nPierre Saurel  Francis Rousseaux  Marc Danger  \nUniversité Paris -Sorbonne  \npierre.saurel \n@paris-sorbonne .fr IRCAM  \nfrancis.rousseaux  \n@ircam.fr ADAMI  \nmdanger \n@adami.fr \nABSTRACT \nIn recent years, MIR research has continued to focus \nmore and more on user feedback, human subjects data, \nand other forms of personal information. Concurrently, \nthe European Union has adopted new, stringent regula-\ntions to take effect in the coming years regarding how \nsuch information can be collected, stored and manipulat-\ned, with equally strict penalties for being found in viola-\ntion of the law. \nHere, we provide a summary of these changes, consid-\ner how they relate to our data sources and research prac-\ntices, and identify promising methodologies that may \nserve researchers well, both in order to be in compliance \nwith the law and conduct more subject-friendly research. \nWe additionally provide a case study of how such chang-\nes might affect a recent human subjects project on the \ntopic of style, and conclude with a few recommendations \nfor the near future. \nThis paper is not intended to be legal advice: our per-\nsonal legal interpretations are strictly mentioned for illus-\ntration purpose, and reader should seek proper legal \ncounsel. \n1. INTRODUCTION \nThe International Society for Music Information \nRetrieval address es a wide range of scienti fic, technical \nand social challenges, dealing with processing, searching, \norganizing and accessing music -related data and digital \nsounds through many aspects, considering real scale use -\ncases and designing innovative applications, exceeding its \nacademic -only initiatory aims.  \nSome recent Music Information Retrieval tools and \nalgorithms aim to attribute authorship and to characteriz e \nthe structure of style, to reproduce the user’s style and to \nmanipulate one’s style as a content  [8], [1]. They deal for \ninstance with active listening, authoring or personalised \nreflexive feedback. These tools will allow identification \nof users in the big data: authors, listeners, performers.  \nAs the emerging MIR scientific community leads to \nindustrial applications of interest to the international \nbusiness (start-up, Majors, content providers, platforms) \nand to experimentations involving many users in living labs (for MIR teaching, for multicultural emotion com-\nparisons, or for MIR user requirement purposes) the iden-\ntification of legal issues becomes essential or strategic. \nLegal issues related to copyright and Intellectual Prop-\nerty have already been identified and expressed into Digi-\ntal Rights Management by the MIR community [2], [7 ], \nwhen those related to security, business models and right \nto access have been expressed by Information Access [4 ], \n[11]. Privacy is another important legal issue. To address \nit properly one needs first to classify the personal data \nand processes. A naive classification appears when you \nquickly look at the kind of personal data MIR deals with:  \n User’s comments, evaluation, annotation and music \nrecommendations are obvious personal data as long as \nthey are published under their name or pseudo;  \n Addresses allowing identification of a device or an in-\nstrument and Media Access Control addresses are \nlinked to personal data; \n Any information allowing identification of a natural \nperson, as some MIR processes do, shall be qualified \nas personal data and processing of personal data. \nBut the legal professionals do not unanimously ap-\nprove this classification. For instance the Court of Appeal \nin Paris judged in two decisions (2007/04/27 and \n2007/05/15) that the Internet Protocol address is not a \npersonal data. \n2. WHAT ARE PROCESSES OF PERSONAL \nDATA AND HOW THEY ARE REGULATED \nA careful consideration of the applicable law of personal \ndata is necessary to elaborate a proper classification of \nMIR personal data processes taking the different interna-\ntional regulations into account. \n2.1 Europe vs. United States: two legal approaches \nEurope regulates data protection through one of the high-\nest State Regulations in the world [3], [9] when the Unit-\ned States lets contractors organize data protection through \nagreements supported by consideration and entered into \nvoluntarily by the parties. These two approaches are \ndeeply divergent. United States lets companies specify \ntheir own rules with their consumers while Europe en-\nforces a unique regulated framework on all companies \nproviding services to European citizens. For instance any \ncompany in the United States can define how long they \nkeep the personal data, when the regulations in Europe \nwould specify a maximum length of time the personal \n © Pierre Saurel, Francis Rousseaux, Marc Danger.  \nLicensed under a Creative Commons  Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Pierre Saurel, Francis Rousseaux, \nMarc Danger . “On the Changing Regulations of Privacy and Personal \nInformation in MIR”, 15th International Society for Music Information \nRetrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n597data is to be stored. And this applies to any company of-\nfering the same service. \nA prohibition is at the heart of the European Commis-\nsion’s Directive on Data Protection (95/46/CE –  The Di-\nrective) [3]. The transfer of personal data to non-\nEuropean Union countries that do not meet the European \nUnion adequacy standard for privacy protection is strictly \nforbidden [3, article 25]1. The divergent legal approaches \nand this prohibition alone would outlaw the proposal by \nAmerican companies of many of their IT services to Eu-\nropean citizens. In response the U.S. Department of \nCommerce and the European Commission developed the \nSafe Harbor Framework (SHF) [6], [14 ]. Any non-\nEuropean organization is free to self-certify with the SHF \nand join. \nA new Proposal for a Regulation on the protection of \nindividuals with regard to the processing of personal data \nwas adopted the 12 March 2014 by the European Parlia-\nment [9]. The Directive allows adjustments from one Eu-\nropean country to another and therefore diversity of im-\nplementation in Europe when the regulation is direct ly \nenforceable and should therefore be implemented directly \nand in the same way in all countries of the European Un-\nion. This regulation should apply in 201 6. This regulation \nenhances data protection and sanctions to anyone who \ndoes not comply with the obligations laid down in the \nRegulation. For instance [9, article 79] the supervisory \nauthority will impose, as a possible sanction, a fine of up \nto one hundred million Euros or up to 5% of the annual \nworldwide turnover in case of an enterprise. \n2.2 Data protection applies to any information con-\ncerning an identifiable natural person \nUntil French law applied the 95/46/CE European Di-\nrective, personal data was only defined considering sets \nof data containing the name of a natural person. This def-\ninition has been extended; the 95/46/CE European Di-\nrective (ED) defines ‘personal data’ [ 3, article 2] as: “any \ninformation relating to an identified or identifiable natu-\nral pe rson (‘data subject’); an identifiable person is one \nwho can be identified, directly or indirectly, in particular \nby reference to an identification number or to one or \nmore factors specific to his physical, physiological, men-\ntal, economic, cultural or social identity ”. \nFor instance the identification of an author through the \nstructure of his style as depending on his mental, cultural \nor social identity is a process that must comply with the \nEuropean data privacy principles. \n2.3 Safe Harbor is the Framework ISMIR affiliates \nneed not to pay a fine up to hundreds million Euros \n1 Argentina, Australia, Canada, State of Israel, New Zealand, United \nStates – Transfer of Air Passenger Name Record (PNR) Data, United \nStates – Safe Harbor, Eastern Republic of Uruguay are, to date, the only \nnon-European third countries ensuring an adequate level of protection: \nhttp://ec.europa.eu/justice/data-protection/document/international-\ntransfers/adequacy/index_en.htm   Complying with Safe Harbor is the easiest way for an or-\nganization using MIR processing to fulfill the high level \nEuropean standard about personal data, to operate \nworldwide and to avoid prosecution regarding personal \ndata. As explained below any non-European organization \nmay enter the US – EU SHF’s requirement and publicly \ndeclare that they do so. In that case the organization must \ndevelop a data privacy policy that conforms to the seven \nSafe Harbor Principles (SHP) [14 ]. \nFirst of all organizations must identify personal data \nand personal data processes. Then they apply the SHP to \nthese data and processes. By joining the SHF, organiza-\ntions must implement procedures and modify their own \ninformation system whether paper or electronic. \nOrganizations must notify (P1) individuals about the \npurposes for which they collect and use information \nabout them,  to whom the information can be disclosed \nand the choices and means offered for limiting its disclo-\nsure. Organizations must explain how they can be con-\ntacted with any complaints. Individuals should have the \nchoice (P2) (opt out) whether their personal information \nis disclosed or not to a third party. In case of sensitive in-\nformation explicit choice (opt in) must be given. A trans-\nfer to a third party (P3) is only possible if the individual \nmade a choice and if the third party subscribed to the \nSHP or was subject to any adequacy finding regarding to \nthe ED. Individuals must have access (P4) to personal \ninformation about them and be able to correct, amend or \ndelete this information. Organizations must take reasona-\nble precautions (P5) to prevent loss, misuse, disclosure, \nalteration or destruction of the personal information. Per-\nsonal information collected must  be relevant (P6: data \nintegrity) for the purpose for which it is to be used. Sanc-\ntions (P7 enforcement) ensure compliance by the organi-\nzation. There must be a procedure for verifying the im-\nplementation of the SHP and the obligation to remedy \nproblems aris ing out of a failure to comply with the SHP.  \n3. CLASSIFICATION FOR M IR \nPERSONAL DATA PROCESSING \nConsidering the legal definition of personal data we can \nnow propose a less naive classification of MIR processes \nand data into three sets: (i) nominative data, (ii) data lead-\ning to an easy identification of a natural person and (iii) \ndata leading indirectly to the identification of a natural \nperson through a complex process. \n3.1 Nominative data and data leading easily to the \nidentification of a natural person \nThe first set of processes deals with all the situations giv-\ning the name of a natural person directly. The second set \ndeals with the cases of a direct or an indirect identifica-\ntion easily done for instance through devices . \nIn these two sets we find that t he most obvious  set of \ndata concer ns the “Personal Music L ibraries” and “rec-\nommendation s”. Looking at the topics that characterize \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n598ISMIR papers from year 2000 to 2013, we find more than \n30 papers and posters dealing with those topics as their \nmain topic . Can one recommend music to a user or ana-\nlyze their personal library without tackling privacy? \n3.2 Data leading to the identification of a natural per-\nson through a complex process \nThe third set of personal data deals with cases when a \nnatural person is indirectly identifiable using a complex \nprocess, like some of the MIR processes. \nCan one work on “Classification” or “Learning” , pro-\nducing 130 publications (accepted contributions at ISMIR \nfrom year 2000 to year 2013) without considering users  \nthroughout their taste s or style?  The processes used under \nthese headings belong for the most part to this third set. \nLooking directly at the data without any sophisticated \ntool does not allow any identification of the natural per-\nson. On the contrary, using some MIR algorithms or ma-\nchine learning can lead to indirect identifications  [12]. \nMost of the time these non-linear methods use inputs \nto build new data which are outputs or data stored inside \nthe algorithm, like weights for instance in a neural net. \n3.3 The legal criteria of the costs and the amount of \ntime required for identification \nThis third set of personal data is not as homogeneous as it \nseems to be at first glance. Can we compare sets of data \nthat lead to an identification of a natural person through a \ncomplex process? \nThe European Proposal for a Regulation specifies the \nconcept of “identifiability ”. It tries to define legal criteria \nto decide if an identifiable set of data is or is not personal \ndata. It considers the identification process [9, recital 23] \nas a relative one depending on the means used for that \nidentification: “ To determine whether a person is identi-\nfiable, account should be taken of all the means reasona-\nbly likely to be used either by the controller or by any \nother person to identify or single out the individual di-\nrectly or indirectly . To ascertain whether means are rea-\nsonably likely to be used to identify the individual, ac-\ncount should be taken of all objective factors, such as the \ncosts of and the amount of time required for identifica-\ntion, taking into consideration both available technology \nat the time of the processing and technological develop-\nment. ” \nBut under what criteria should we, as MIR practition-\ners, specify when a set of data allows an easy identifica-\ntion and belongs to the second set or, on the contrary, is \ntoo complex or reaches a too uncertain identification so \nthat we would not legally say that these are personal da-\nta? To answer these questions, we must be able to com-\npare MIR processes with new criteria.  \n4. MANAGING THE TWO FIRST SETS \nOn an example chosen to be problematic (but increasing-\nly common in the industry) , we show how to manage per-sonal data in case of a simple direct or indirect identifica-\ntion process.  \n4.1 Trends in terms of use and innovative technology \nDatabases of personal data are no more clearly identified. \nWe can view the situation as combining five aspects, \nwhich lead to new scientific problems concerning MIR \npersonal data processing. \nData Sources Explosion.  The number of databases for \nretrieving information is growing dramatically. Applica-\ntions are also data sources. Spotify for instance provides a \nlive flow of music consumption information from mil-\nlions of users. Data from billions of sensors will soon be \nadded. This profusion of data does not mean quality. Ac-\ncessible does not mean legal or acceptable for a user. \nThose considerations are essential to build reliable and \nsustainable systems. \nCrossing & Reconciling Data.  Data sources are no \nlonger isolated islands. Once the user can be identified \n(cookie, email, customer id), it is possible to match, ag-\ngregate and remix data that was previously isolated. \nTime Dimension.  The web has a good memory that \nhumans are generally not familiar with. Data can be pub-\nlic one day and be considered as very private 3 years lat-\ner. Many users forget they posted a picture after a student \nparty. And the picture has the misfortune to crop up again \nwhen you apply for a job. And it is not only a question of \nhuman memory: Minute traces collected one day can be \nexploited later and provide real information. \nPermanent Changes. The general instability of the \ndata sources, technical formats and flows, applications \nand use is another strong characteristic of the situation. \nThe impact on personal data is very likely. If the architec-\nture of the systems changes a lot and frequently, the so-\ncial norms also change. Users today publicly share infor-\nmation that they would have considered totally private a \nfew years earlier. And the opposite could be the case. \nUser Understandability and Control.  Because of the \ncomplexity of changing systems and complex interactions \nusers will less and less control over their information. \nThis lack of control is caused by the characteristics of the \nsystems and by the mistakes and the misunderstandings \nof human users. The affair of the private Facebook mes-\nsages appearing suddenly on timeline (Sept. 2012) is sig-\nnificant. Facebook indicates that there was no bug. Those \nmessages were old wall posts that are now more visible \nwith the new interface. This is a combination of bad user \nunderstanding and fast moving systems. \n4.2 The case of an Apache Hadoop File System \n(AHFS) on which some machine learning is applied \nEveryone produces data and personal data without being \nalways aware that they provide data revealing their iden-\ntification. When a user tags / rates musical items [13], he \ngives personal information. If a music recommender ex-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n599ploits this user data without integrating privacy concepts, \nhe faces legal issues and strong discontent from the users. \nThe data volume has increased faster than “Moore’s \nlaw”: This is what is meant by “Big Data”. New data is \ngenerally unstructured and traditional database systems \nsuch as Relational Database Management Systems cannot \nhandle the volume of data produced by users & machines \n& sensors. This challenge was the main drive for Google \nto define a new technology: the Apache Hadoop File Sys-\ntem (AHFS). Within this framework, data and computa-\ntional activities are distributed on a very large number of \nservers. Data is not loaded for computation, nor the re-\nsults stored. Here, the algorithm is close to the data. This \nsituation leads to the epistemological problem of separa-\nbility into the field of MIR personal data processing: are \nall MIR algorithms (and for instance the authorship at-\ntribution algorithms) separable into data and processes? \nAn answer to this question is required for any algorithm \nto be able to identify the set of personal data it deals with. \nNow, let us consider a machine learning classifi-\ner/recommender trained on user data. In this sense, the \nalgorithm is inseparable from the data it uses to function. \nAnd, if the machine is internalizing identifiable infor-\nmation from a set of users in a certain state (let say EU) , \nit is then in violation to share the resulting function in a \nnon-adequate country (let say Brazil) the EU if it was \ntrained in, say, the US. \n4.3 Analyzing the multinational AHFS case \nRegarding to the European regulation rules [3, art. 25], \nyou may not transfer personal data collected in Europe to \na non-adequate State ( see list of adequate countries \nabove). If you build a multinational AHFS system, you \nmay collect data in Europe and in US depending on the \nway you localized the AHFS servers. The European data \nmay not be transferred to Brazil. Even the classifier \nwould not legally be used in Brazil as long as it internal-\nizes some identifiable European personal information. \nIn practice one should then localize the AHFS files \nand machine-learning processes to make sure no identifi-\nable data will be transferred from one country with a spe-\ncific regulation to another with another regulation about \npersonal data. We call these systems “heterarchical” due \nto the blended situation of a hierarchical system (the \nglobal AHFS management) and the need of a heterogene-\nous local regulation. \nTo manage properly the global AHFS system we need \na first analysis  of the system dispatching the different \nfiles on the right legal places. Privacy by Design (PbD) is \na useful methodology to do so.  \n4.4 Foundations Principals of Privacy by Design \nPbD was first developed by Ontario’s Information and \nPrivacy Commissioner, Dr. Ann Cavoukian, in the 1990s, \nat the very birth of the future big data phenomenon. This solution has gained widespread international recognition, \nand was recently recognized as a  global privacy standard . \nAccording to its Canadian inventor1, is PbD based on \nseven Foundation Principles (FP): PbD “is an approach \nto protect privacy by embedding it into the design specifi-\ncations of technologies, business practices, and physical \ninfrastructures. That means building in privacy up front – \nright into the design specifications and architecture of \nnew systems and processes. PbD is predicated on the \nidea that, at the outset, technology is inherently neutral. \nAs much as it can be used to chip away at privacy, it can \nalso be enlisted to protect privacy. The same is true of \nprocesses and physical infrastructure”:  \n Proactive not Reactive (FP1): the PbD approach is \nbased on proactive measures anticipating and \npreventing privacy invasive events before they occur;  \n Privacy as the Default Setting (FP2): the default rules \nseek to deliver the maximum degree of privacy; \n Privacy embedded into Design (FP3): Privacy is \nembedded into the architecture of IT systems and \nbusiness practices; \n Full Functionality – Positive Sum, not Zero-Sum \n(FP4): PbD seeks to accommodate all legitimate \ninterests and objectives (security, etc.) in a “ win-win” \nmanner; \n End- to-End Security – Full Lifecycle Protection (FP5): \nsecurity measures are essential to privacy, from start to \nfinish; \n Visibility and Transparency — Keep it Open (FP6): \nPbD is subject to independent verification. Its \ncomponent parts and operations remain visible and \ntransparent, to users and providers alike; \n Respect for User Privacy — Keep it User-Centric \n(FP7): PbD requires architects and operators to keep \nthe interests of the individual uppermost. \nAt the time of digital data exchange through networks, \nPbD is a key-concept in legacy [10]. In Europe, where \nthis domain has been directly inspired by the Canadian \nexperience, the EU2 affirm s: “PbD means that privacy \nand data protection are embedded throughout the entire \nlife cycle of technologies, from the early design stage to \ntheir deployment, use and ultimate disposal ”. \n4.5 Prospects for a MIR Privacy by Design \nPbD is a reference for designing systems and processing \ninvolving personal data , enforced by the new European \nproposal for a Regulation [9, art. 23]. It becomes a meth-\nod for these designs whereby it includes signal analysis \nmethods and may interest MIR developers. \nThis proposal leads to new questions, such as the fol-\nlowing: Is PbD a universal methodological solution about \npersonal data for all MIR projects? Most of ISMIR con-\ntributions are still research oriented which doesn’t mean \n1 http://www.ipc.on.ca/images/Resources/7foundationalprinciples.pdf  \n2 “Safeguarding Privacy in a Connected World –  A European Data Pro-\ntection Framework for the 21st Century” COM  (2012) 9 final.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n600that they fulfill the two specific exceptions [9, art. 83]1. \nTo say more about that intersection, we need to survey \nthe ISMIR scientific production, throughout the main \nFPs. FP6 (transparency) and FP7 (user-centric) are usual-\nly respected among the MIR community as source code \nand processing are often (i) delivered under GNU like \nlicensing allowing audit and traceability (ii) user-friendly. \nHowever, as long as PbD is not embedded, FP3 cannot be \nfulfilled and accordingly FP2 (default setting), FP5 (end-\nto-end), FP4 (full functionality) and FP1 (proactive) can-\nnot be fulfilled even. Without any PbD embedded into \nDesign, there are no default settings ( FP2), you cannot \nfollow an end- to-end approach ( FP5), you cannot define \nfull functionality regarding to personal data ( FP4) nor be \nproactive. Principle of pro-activity (FP1) is the key. Ful-\nfilling FP1 you define the default settings (FP2), be fully \nfunctional (FP4) and define an end- to-end process (FP5). \nIn brief is PbD useful to MIR developers even if it is \nnot the definitive martingale! \n5. EXPLORING THE THIRD SET \n“Identifiability ” is the  potentiality of a set of data to lead \nto the identification of its source. A set of data should be \nqualified as being personal data if the cost and the \namount of time required for identification are reasonable. \nThese new criteria are a step forward since the qualifica-\ntion is not an absolute one anymore and depends specifi-\ncally on the state of the art. \n5.1 Available technology and technological develop-\nment to take into account at this present moment \nChanges in Information Technology lead to a shift in the \napproach of data management: from computational to da-\nta exploration. The main question is “What to look for?” \nMany companies build new tools to “make the data \nspeak”. This is the case considering the trend of personal-\nized marketing. Engineers using big data build systems \nthat produce new personal dataflow. \nIs it possible to stabilize these changes through stand-\nardization of metadata? Is it possible to develop a stand-\nardization of metadata which could ease the classification \nof MIR processing of personal data into identifying and \nnon-identifying processes. \nMany of the MIR methods are stochastic, probabilistic \nor designed to cost and more generally non-deterministic. \nOn the contrary the European legal criteria [9, recital 23] \n(see above § 3.3) to decide whether a data is personal or \nnot (the third set) seem to be much to deterministic to fit \nthe effective new practices about machine learning on \npersonal data. \n1 (i) these processing cannot be fulfilled otherwise and (ii) data permit-\nting the identification are kept separately from the other information, or \nwhen the bodies conducting these data respect three conditions: (i) con-\nsent of the data subject, (ii) publication of personal data is necessary and \n(iii) data are made public This situation leads to a new scientific problem: Is \nthere an absolute criterion about the identifiability of per-\nsonal data extracted from a set of data with a MIR pro-\ncess? What characterizes a maximal subset from the big \ndata that could not ever be computed by any Turing ma-\nchine to identify a natural person with any algorithm? \n5.2 What about the foundational separation in com-\nputer science between data and process? \nComputer science is based on a strict separation between \ndata and process (dual as these two categories are inter-\nchangeable at any time; data can be activated as a process \nand a process can be treated as a data ). \nWe may wonder about the possibility of maintaining \nthe data/process separation paradigm if i) the data stick to \nthe process and ii) the legal regulation leads to a location \nof the data in the legal system in which those data were \nproduced. \n6. CONCLUSION \n6.1 When some process lead to direct or indirect per-\nsonal data identification \nMethodological Recommendations.  MIR researchers \ncould first audit their algorithm and data, and check if \nthey are able to identify a natural person (two first sets of \nour classification). If so they could use the SHF which \ncould already be an industrial challenge for instance re-\ngarding Cyber Security (P5). Using the PbD methodology \ncertainly leads to operational solutions in these situations. \n6.2 When some process may lead to indirect personal \ndata identification through some complex process \nIn many circumstances, the MIR community develops \nnew personal data on the fly, using the whole available \nrange of data analysis and data building algorithm. Then \nresearchers could apply the PbD methodology, to insure \nthat no personal data is lost during the system design. \nHere PbD is not a universal solution because the time \nwhen data (on the one hand) and processing (on the other \nhand) were functionally independent, formally and se-\nmantically separated, has ended. Nowadays, MIR re-\nsearchers currently use algorithms that support effective \ndecision, supervis ed or not, without introducing ‘pure’ \ndata or ‘pure’ processing, but building up acceptable so-\nlutions together with machine learning [5] or heuristic \nknowledge that cannot be reduced to data or processing: \nThe third set of personal data may appear, and raise theo-\nretical scientific problems. \nPolitical Opportunities. The MIR community has a \npolitical role to play in the data privacy domain, by ex-\nplaining to lawyers —joining expert groups in the US, \nUE or elsewhere — what we are doing and how we over-\nlap with the tradition in style description, turning it into a \ncomputed style genetic, which radically questions the \nanalysis of data privacy traditions, cultures and tools. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n601Future Scientific Works.  In addition to methodologi-\ncal and political ones, we face purely scientific challeng-\nes, which constitute our research program for future \nworks. Under  what criteria should we, as MIR practition-\ners, specify when a set of data allows an easy identifica-\ntion and belongs to the second set or on the contrary is \ntoo complex or all ows a too uncertain identification so \nthat we would say that these are not personal data?  What \ncharacterizes a maximal subset from the big data that \ncould not ever be computed by any Turing machine to \nidentify a natural person with any algorithm?  \n7. REFERENCES \n[1] S. Argamon, K. Burns, S. Dubnov (Eds): The \nStructure of Style, Springer-Verlag, 2010. \n[2] C. Barlas: “Beating Babel - Identification, Metadata \nand Rights”, Invited Talk, Proceedings of the \nInternational Symposium on Music Information \nRetrieval , 2002.  \n[3] Directive (95/46/EC) of 24 October 1995 Official \nJournal L 281, 23/11/1995 P. 0031 - 0050 : http://eur-\nlex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31995L00\n46:en:HTML  \n[4] J.S. Downie, J. Futrelle, D. Tcheng: “The \nInternational Music Information Retrieval Systems \nEvaluation Laboratory: Governance, Access and \nSecurity”, Proceedings of the International \nSymposium on Music Information Retrieval , 2004. \n[5] A. Gkoulalas-Divanis, Y. Saygin, Vassilios S. \nVerykios: “Special Issue on Privacy and Security \nIssues in Data Mining and Machi ne Learning” , \nTransactions on Data Privacy,  Vol. 4, Issue 3, \npp. 127-187, December 2011. \n[6] D. Greer: “Safe Harbor - A Fra mework that Works” , \nInternational Data Privacy Law,  Vol.1, Issue 3, \npp. 143-148, 2011. \n[7] M. Levering: “Intellectual Property Rights in \nMusical Works: Overview, Digital Library Issues \nand Related Initiatives”, Invited Talk, Proceedings \nof the International Symposium on Music \nInformation Retrieval , 2000. \n[8] F. Pachet, P. Roy: “Hit Song Science is Not Yet a \nScience”, Proceedings of the International \nSymposium on Music Information Retrieval , 2008. \n[9] Proposal for a Regulation on the protection of \nindividuals with regard to the processing of personal \ndata was adopted the 12 March 2014 by the \nEuropean Parliament : \nhttp://www.europarl.europa.eu/sides/getDoc.do?type\n=TA&reference=P7- TA-2014 -0212& language=EN  [10] V. Reding: “The European Data Protection \nFramework for the Twenty- first century” , \nInternational Data Privacy Law,  volume 2, issue 3, \npp.119-129, 2012. \n[11] A. Seeger: “I Found It, How Can I Use It? - Dealing \nWith the Ethical and Legal Constraints of \nInformation Access”, Proceedings of the \nInternational Symposium on Music Information \nRetrieval , 2003. \n[12] A.B. Slavkovic , A. Smith: “ Special Issue on \nStatistical and Learning-Theoretic Challenges in \nData Privacy” , Journal of Privacy and \nConfidentiality, Vol. 4, Issue 1, pp. 1-243, 2012. \n[13] P. Symeonidis, M. Ruxanda, A. Nanopoulos, Y. \nManolopoulos: “Ternary Semantic Analysis of \nSocial Tags for Personalized Music \nRecommendation”,  Proceedings of the International \nSymposium on Music Information Retrieval,  2008. \n[14] U.S. – EU Safe Harbor: \nhttp://www.export.gov/safeharbor/eu/eg_main_018365.asp   \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n602"
    },
    {
        "title": "Music Analysis as a Smallest Grammar Problem.",
        "author": [
            "Kirill A. Sidorov",
            "Andrew Jones",
            "A. David Marshall"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417653",
        "url": "https://doi.org/10.5281/zenodo.1417653",
        "ee": "https://zenodo.org/records/1417653/files/SidorovJM14.pdf",
        "abstract": "In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explain- ing a musical work’s structure. The proposed method is predicated on the hypothesis that the shortest possible gram- mar provides a model of the musical structure which is a good representation of the composer’s intent. The effec- tiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our ap- proach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection.",
        "zenodo_id": 1417653,
        "dblp_key": "conf/ismir/SidorovJM14",
        "keywords": [
            "automatically generated grammar",
            "explaining musical works structure",
            "composers intent",
            "effectiveness comparison",
            "error detection",
            "intuitive editing",
            "creative transformation",
            "automatic summarization",
            "musical complexity estimation",
            "plagiarism detection"
        ],
        "content": "MUSIC ANALYSIS AS A SMALLEST GRAMMAR PROBLEM\nKirill Sidorov Andrew Jones David Marshall\nCardiff University, UK\nfK.Sidorov, Andrew.C.Jones, Dave.Marshallg@cs.cardiff.ac.uk\nABSTRACT\nIn this paper we present a novel approach to music analysis,\nin which a grammar is automatically generated explain-\ning a musical work’s structure. The proposed method is\npredicated on the hypothesis that the shortest possible gram-\nmar provides a model of the musical structure which is a\ngood representation of the composer’s intent. The effec-\ntiveness of our approach is demonstrated by comparison of\nthe results with previously-published expert analysis; our\nautomated approach produces results comparable to human\nannotation. We also illustrate the power of our approach\nby showing that it is able to locate errors in scores, such as\nintroduced by OMR or human transcription. Further, our ap-\nproach provides a novel mechanism for intuitive high-level\nediting and creative transformation of music. A wide range\nof other possible applications exists, including automatic\nsummarization and simpliﬁcation; estimation of musical\ncomplexity and similarity, and plagiarism detection.\n1. INTRODUCTION\nIn his Norton Lectures [1], Bernstein argues that music can\nbe analysed in linguistic terms, and even that there might be\n“a worldwide, inborn musical grammar”. Less speciﬁcally,\nthe prevalence of musical form analyses, both large-scale\n(e.g. sonata form) and at the level of individual phrases,\ndemonstrates that patterns, motifs, etc., are an important\nfacet of a musical composition, and a grammar is certainly\none way of capturing these artefacts.\nIn this paper we present a method for automatically\nderiving a compact grammar from a musical work and\ndemonstrate its effectiveness as a tool for analysing musical\nstructure. A key novelty of this method is that it operates au-\ntomatically, yet generates insightful results. We concentrate\nin this paper on substantiating our claim that generating\nparsimonious grammars is a useful analysis tool, but also\nsuggest a wide range of scenarios to which this approach\ncould be applied.\nPrevious research into grammar-based approaches to\nmodelling music has led to promising results. Treating har-\nmonic phenomena as being induced by a generative gram-\nmar has been proposed in [9, 24, 27], and the explanatory\n© Kirill Sidorov, Andrew Jones, David Marshall.\nLicensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Kirill Sidorov, Andrew Jones, David Marshall.\n“Music Analysis as a Smallest Grammar Problem”, 15th International\nSociety for Music Information Retrieval Conference, 2014.power of such grammars has been demonstrated. The use of\nmusical grammar based of the Generative Theory of Tonal\nMusic [19] has been proposed in [11 –13], for the analysis\nof music. Similarly, there is a number of grammar-based ap-\nproaches to automatic composition, including some which\nautomatically learn stochastic grammars or derive gram-\nmars in an evolutionary manner, although some researchers\ncontinue to craft grammars for this purpose by hand [7].\nHowever, in these works the derivation of grammar rules\nthemselves is performed manually [27] or semi-automati-\ncally [11 –13] from heuristic musicological considerations.\nIn some cases generative grammars (including stochastic\nones) are derived or learned automatically, but they describe\ngeneral patterns in a corpus of music, e.g.for synthesis [16,\n22], rather than being precise analyses of individual works.\nIn a paper describing research carried out with a different,\nmore precise aim of visualising semantic structure of an\nindividual work, the authors remark that they resorted to\nmanual retrieval of musical structure data from descriptive\nessays “since presently there is no existing algorithm to\nparse the high-level structural information automatically\nfrom MIDI ﬁles or raw sound data” [5].\nIn this paper we present a method which addresses the\nabove concern expressed by Chan et al. in [5], but which\nat the same time takes a principled, information-theoretical\napproach. We argue that the best model explaining a given\npiece of music is the most compact one. This is known as\nMinimum Description Length principle [23] which, in turn,\nis a formal manifestation of the Occam’s razor principle:\nthe best explanation for data is the most compressive one.\nHence, given a piece of music, we seek to ﬁnd the short-\nest possible context free grammar that generates this piece\n(and only this piece). The validity of our compressive mod-\nelling approach in this particular domain is corroborated by\nevidence from earlier research in predictive modelling of\nmusic [6] and from perception psychology [14,25]: humans\nappear to ﬁnd strongly compressible music (which therefore\nhas a compact grammar) appealing.\n2. COMPUTING THE SMALLEST GRAMMAR\nGiven a piece of music, we treat it as a sequence(s) of\nsymbols (see Section 2.1) and we seek to ﬁnd the shortest\npossible context-free grammar that generates this (and only\nthis) piece. Following [21], we deﬁne the size of a gram-\nmarGto be the total length of the right hand sides of all\nthe production rules Riplus one for each rule (length of a\nseparator or cost of introducing a new rule):\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n301jGj=X\ni(jRij+1): (1)\nSearching for such a grammar is known as the smallest\ngrammar problem. It has recently received much attention\ndue its importance in compression and analysis of DNA\nsequences, see e.g.[4]. For an overview of the smallest\ngrammar problem the reader is referred to [3].\nComputing the smallest grammar is provably NP-hard\n(see [18] Theorem 3.1), therefore in practice we are seeking\nan approximation to the smallest grammar.\nVarious heuristics have been proposed in order to tackle\nthe smallest grammar problem in tractable time by greedy\nalgorithms [4, 21]. A fast on-line (linear time) algorithm\ncalled S EQUITUR has been proposed in [20]. While the fo-\ncus of [20] was fast grammar inference for large sequences,\nrather than strong compression, [20] contains an early men-\ntion that such techniques may be applied to parsing of mu-\nsic. (In Section 3 we compare grammars produced by S E-\nQUITUR with our approach.)\nIn [4, 21], a class of algorithms involving iterative re-\nplacement of a repeated substring is considered (termed\nthere iterative repeat replacement (IRR)). We employ a\nsimilar procedure here, summarised in Alg. 1. First, the\ngrammar Gis initialised with top level rule(s), whose right-\nhand sides initially are simply the input string(s). Then, in\nthe original IRR scheme, a candidate substring cis selected\naccording to some scoring function F. All non-overlapping\noccurrences of this substring in the grammar are replaced\nwith a new symbol Rn+1, and a new rule is added to the gram-\nmar: Rn+1!c. Replacement of a substring of length min a\nstring of length ncan be done using the Knuth-Morris-Pratt\nalgorithm [17] in O(m+n)time. The replacement procedure\nrepeats until no further improvement is possible.\nIn [4, 21] the various heuristics according to which such\ncandidate substitutions can be selected are examined; the\nconclusion is that the “locally most compressive” heuristic\nresults in the shortest ﬁnal grammars. Suppose a substring\nof length Loccurring Ntimes is considered for replacement\nwith a new rule. The resulting saving is, therefore [21]:\nF= \u0001jGj=(LN)\u0000(L+1+N): (2)\nHence, we use Eq. (2)(the locally most compressive heuris-\ntic) as our scoring function when selecting candidate sub-\nstrings (in line 2 of Alg. 1). We found that the greedy\niterative replacement scheme of [21] does not always pro-\nduce optimal grammars. We note that a small decrease\nin grammar size may amount to a substantial change in\nthe grammar’s structure, therefore we seek to improve the\ncompression performance.\nTo do so, instead of greedily making a choice at each\niteration, we recursively evaluate (line 9) multiple ( w) candi-\ndate substitutions (line 2) with backtracking, up to a certain\ndepth dmax(lines 9–15). Once the budgeted search depth\nhas been exhausted, the remaining substitutions are done\ngreedily (lines 4–7) as in [21]. This allows us to control\nthe greediness of the algorithm from completely greedy\n(dmax=0) to exhaustive search ( dmax=1). We observed\nthat using more than 2–3levels of backtracking usually does\nnot yield any further reduction in the size of the grammar.Algorithm 1 CompGram (Compress grammar)\nRequire: Grammar G; search depth d.\n(Tuning constants: max depth dmax; width w)\n1:loop\n2: Find wbest candidate substitutions C=fciginG.\n3: ifC=?then return G; end if\n4: ifrecursion depth d>dmaxthen\n5: Greedily choose best cbest\n6: G0:=replace( G,cbest, new symbol)\n7: return CompGram( G0,d+1)\n8: else\n9: Evaluate candidates:\n10: forci2Cdo\n11: G0:=replace( G,ci, new symbol)\n12: G00\ni:=CompGram( G0,d+1)\n13: end for\n14: b:=arg minijG00\nij\n15: return G00\nb\n16: end if\n17:end loop\nSelecting a candidate according to Eq. (2)in line 2 in-\nvolves maximising the number of non-overlapping occur-\nrences of a substring, which is known as the string statistics\nproblem, the solutions to which are not cheap [2]. There-\nfore, as in [4] we approximate the maximal number of\nnon-overlapping occurrences with the number of maximal\nrepeats [10]. All zmaximal repeats in a string (or a set of\nstrings) of total length ncan be found very fast (in O(n+z)\ntime) using sufﬁx arrays [10]. In principle, it is possible\nto construct an example in which this number will be dras-\ntically different from the true number of non-overlapping\noccurrences (e.g. a long string consisting of a repeated\nsymbol). However, this approximation was shown to work\nwell in [4] and we have conﬁrmed this in our experiments.\nFurther, this concern is alleviated by the backtracking pro-\ncedure we employ.\n2.1 Representation of Music\nIn this paper, we focus on music that can be represented\nas several monophonic voices (such as voices in a fugue,\nor orchestral parts), that is, on the horizontal aspects of the\nmusic. We treat each voice, or orchestral part, as a string.\nWe use (diatonic) intervals between adjacent notes, ignoring\nrests, as symbols in our strings. For ease of explanation of\nour algorithm we concentrate on the melodic information\nonly, ignoring rhythm (note durations). Rhythmic invari-\nance may be advantageous when melodic analysis is the\nprime concern. However, it is trivial to include note dura-\ntions, and potentially even chord symbols and other musical\nelements, as symbols in additional (top level) strings.\nNote that even though we take no special measures to\nmodel the relationship between the individual voices, this\nis happening automatically: indeed, all voices are encom-\npassed in the same grammar and are considered for the\niterative replacement procedure on equal rights as the gram-\nmar is updated.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n302Rule R9R8R5R18R10R11R4R19R7R20\nFreq. 2 2 4 2 5 5 2 2 3 3\nLen. 16 12 6 7 5 5 4 6 5 4\nE. len. 99 94 24 47 11 11 37 34 17 16\nComp. 96 91 67 44 38 38 34 31 30 28\nTable 1 . Grammar statistics for Fugue 10. The ten most\ncompressing rules are shown. For each rule Ri:Freq. is\nthe number of times a rule occurs in the grammar, Len. is\nits right hand side length, E. len. is the length of the rule’s\nexpansion, and Comp. is the total saving due to this rule.\n3. RESULTS AND APPLICATIONS\n3.1 Automatic Structural Analysis\nWe have applied our method to automatically detect the\nstructure of a selection of Bach’s fugues. (Eventually we\nintend to analyse all of them in this way.) Figure 1 shows\none example of such analysis. We show the voices of the\nfugue in piano roll representation, with the hierarchy of\nthe grammar on top: rules are represented by brackets la-\nbelled by rule number. For completeness, we give the entire\ngrammar (of sizejGj=217) obtained for Fugue 10 later,\nin Fig. 7. Figure 2 zooms in onto a fragment of the score\nwith the rules overlaid. For comparison, we show manual\nanalysis by a musicologist [26] in Fig. 3. Observe that all\nthe main structural elements of the fugue have been cor-\nrectly identiﬁed by our method (e.g. exp. and 1st dev. in\nruleR8, re-exp. and 4th dev. in rule R9, variant of re-exp.\nand 2nd dev in R18andR19) and our automatic analysis is\ncomparable to that by a human expert.\nIt is possible to use structures other than individual notes\nor intervals as symbols when constructing grammars. Fig-\nure 4 shows the simpliﬁed grammar for Fugue 10 gen-\nerated using entire bars as symbols. In this experiment we\nﬁrst measured pairwise similarity between all bars (using\nLevenshtein distance [8]) and denoted each bar by a sym-\nbol, with identical or almost identical bars being denoted\nby the same symbol. The resulting grammar (Fig. 4) can be\nviewed as a coarse-grained analysis. Observe again that it\nclosely matches human annotation (Fig. 3).\nOur approach can also be used to detect prominent high-\nlevel features in music. We can compute the usage fre-\nquency for each rule and the corresponding savings in\ngrammar size (as shown in Table 1 for Fugue 10). Most\ncompressing rules, we argue, correspond to structurally im-\nportant melodic elements. The present example illustrates\nour claim: rule R8corresponds to the fugue’s exposition,\nR9to re-exposition, and R5to the characteristic chromatic\nﬁgure in the opening (cf. Figs. 1 to 3 and the score).\nIn addition to high-level analysis, our approach can be\nused to detect the smallest constituent building blocks of a\npiece. For example, Fig. 5 shows the lowest level rules (that\nuse only terminals) produced in analysis of Fugue 10,\nand the frequency of each rule. These are the elementary\n“bricks” from which Bach has constructed this fugue.\nIn [20], S EQUITUR is applied to two Bach chorales.\nIn Fig. 6 we replicate the experiment from [20] and com-\n0123∆|G| (%)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42Voice 1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\nBar numberVoice 21\n8\n4\n5\n22\n1413\n23 2317\n211510\n3\n15 153\n15 15 1216\n14 14153\n15 153\n15 1516\n14 14153\n15 153\n15 156\n21 216\n21 2118\n3\n15 15 154\n5\n22\n1413\n23 2317\n211510\n3\n15 153\n15 15 1219\n11\n3\n15 153\n15 157\n22\n1411\n3\n15 153\n15 1522\n1411\n3\n15 153\n15 1522\n1410\n3\n15 153\n15 15 245\n22\n1413\n23 2317\n211520\n10\n3\n15 153\n15 15 12 2410\n3\n15 153\n15 15 12 246\n21 219\n3\n15 15 15 12 2410\n3\n15 153\n15 15 215\n22\n1413\n23 2317\n21157\n22\n1411\n3\n15 153\n15 157\n22\n1411\n3\n15 153\n15 1511\n3\n15 153\n15 1522\n1413\n23 23 1515 12 12 2317\n21\n2\n5\n22\n1413\n23 2317\n211520\n10\n3\n15 153\n15 15 1220\n10\n3\n15 153\n15 15 126\n2121 129\n3\n15 15 15 12 2410\n3\n15 153\n15 15 215\n22\n1413\n23 2317\n21157\n22\n1411\n3\n15 153\n15 157\n22\n1411\n3\n15 153\n15 1511\n3\n15 153\n15 158\n4\n5\n22\n1413\n23 2317\n211510\n3\n15 153\n15 15 1216\n14 14153\n15 153\n15 1516\n14 14153\n15 153\n15 156\n21 216\n21 2118\n3\n15 15 154\n5\n22\n1413\n23 2317\n211510\n3\n15 153\n15 1519\n11\n3\n15 153\n15 157\n22\n1411\n3\n15 153\n15 1522\n1411\n3\n15 153\n15 15 15 12 1513\n23 23 23Figure 1 . Automatic analysis of Bach’s Fugue 10 from\nWTK book I. On top: sensitivity to point errors as measured\nby the increase in grammar size \u0001jGj.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n303Figure 2 . Close-up view of the ﬁrst four bars: rules R4,R5,\nR10, and R12overlaid with the score (lower level rules are\nnot shown).\nFigure 3 . Manual analysis of Fugue 10 by a musicologist\n(from [26] with permission).\nVoice 11\n4\n6 6 35\n6 3Voice 22\n5\n6 34\n6 6 3\nBar number1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41\nFigure 4 . Simpliﬁed automatic analysis of Fugue 10\nusing whole bars as symbols.\npare the grammars of these two chorales generated by S E-\nQUITUR and by our approach. The chorales are very similar\nexcept for a few subtle differences. We note that our method\nwas able to produce a shorter grammar ( jGourj=50vs.\njGSequiturj=59) and hence revealed more of the relevant\nstructure, while the grammar of the more greedy (and hence\nless compressing) S EQUITUR was compromised by the\nsmall differences between the chorales.\n3.2 Error Detection and Spell-checking\nWe investigated the sensitivity of the grammars generated\nby our method to alterations in the original music. In one\nexperiment, we systematically altered each note in turn\n(introducing a point error) in the 1st voice of Fugue 10\nand constructed a grammar for each altered score. The\nchange in grammar size relative to that of the unaltered\nscore is plotted in Fig. 1 (top) as a function of the alteration’s\nposition. Observe that the grammar is more sensitive to\nalterations in structurally dense regions and less sensitive\nelsewhere, e.g. in between episodes. Remarkably, with\nR15(10) R12(9) R21(4) R23(4) R24(4) R14(3)\nFigure 5 . Atomic (lowest level) rules with their frequencies\nin brackets.\n2 3 4 5 6 7 8 9 10 11 12 13Chorale 1\n2 3 4 5 6 7 8 9 10 11 12 13\nSequiturChorale 21310813\n5 5 14 59\n117\n14 12413\n5 5 14 6310813\n5 5 14 59\n117\n14 12 5 67\n14\n2\n8\n13\n5 5 149\n117\n14 124\n13\n5 5 14 610\n8\n13\n5 5 14 5 11 1213\n5 5 14 14\n2 3 4 5 6 7 8 9 10 11 12 13Chorale 1\n2 3 4 5 6 7 8 9 10 11 12 13\nOur methodChorale 21\n6\n38\n3 5 34\n57\n38\n3 56\n38\n3 5 3 57\n38\n3 5\n2\n6\n38\n3 54\n57\n38\n3 56\n38\n3 5 37\n38\n3 5 5\nFigure 6 . The grammars for the Bach chorales (from [20])\nproduced by S EQUITUR (above),jGSequiturj=59, and by\nthe proposed approach (below), jGourj=50.\nvery few exceptions (e.g. bars 10,26) altering the piece\nconsistently results in the grammar size increasing. We\nobserved a similar effect in other Bach fugues and even in\n19th century works (see below). We propose, only partially\nin jest, that this indicates that Bach’s fugues are close to\nstructural perfection which is ruined by even the smallest\nalteration.\nHaving observed the sensitivity of the grammar size\nto point errors (at least in highly structured music), we\npropose that grammar-based modelling can be used for\nmusical “spell-checking” to correct errors in typesetting\n(much like a word processor does for text), or in optical\nmusic recognition (OMR). This is analogous to compressive\nsensing which is often used in signal and image processing\n(seee.g.[15]) for denoising: noise compresses poorly. We\ncan regard errors in music as noise and use the grammar-\nbased model for locating such errors. We investigated this\npossibility with the following experiment.\nAs above, we introduce a point error (replacing one note)\nat a random location in the score to simulate a “typo” or\nan OMR error. We then systematically alter every note\nin the score and measure the resulting grammar size in\neach case. When the error is thus undone by one of the\nmodiﬁcations, the corresponding grammar size should be\nnoticeably smaller, and hence the location of the error may\nthus be revealed. We rank the candidate error positions\nby grammar size and consider suspected error locations\nwith grammar size less than or equal to that of the ground\ntruth error, i.e.the number of locations that would need\nto be manually examined to pin-point the error, as false\npositives. We then report the number of false positives as\na fraction of the total number of notes in the piece. We\nrepeat the experiment for multiple randomly chosen error\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n304S1!R82\"R18R121\"R195#7\"R111\"R223#R102#R244#R5R20\nR240R10R12R246#R62#1\"1#3\"R9R22R13R153\"R15R121#\nR12R23R171#4\"\nS2!R5R209#R209#5\"R6R121#2\"R9R83\"R182#2#2\"3\"R19\n7#9\"R111\"3#2#2#11\"1\"R151#R12R157\"R13R237#4#\nR3!R15R15 R4!R55\"4\"R10\nR5!R22R133\"1#R17R15 R6!R21R211\"\nR7!2#R225#5\"R11\nR8!R4R121\"4#R166#R162#4#R6R61#\nR9!R3R15R12R244\"R101#R214#R5R71#R73\"R111\"\nR10!1\"R32\"R31\"\nR11!R31#5\"R31# R12!1\"2#1\"\nR13!R23R232#2\"2#2\"3# R14!2\"2\"\nR15!1#1#\nR16!R142\"4#R142\"R152\"R3R35\"\nR17!1\"4#2\"3#1#R212#\nR18!R3R155\"6#5\"6#R4\nR19!R111#R71#2#R22 R20!5\"7\"R10R12\nR21!1\"1\" R22!R143\"\nR23!1#1\" R24!2#5\"\nFigure 7 . Automatically generated shortest grammar for\nFugue10. Here, Riare production rules ( Siare the top\nlevel rules corresponding to entire voices), numbers with\narrows are terminal symbols (diatonic intervals with the\narrows indicating the direction).\nPiece F/P Piece F/P\nFugue116:07% Fugue 2 2:28%\nFugue10 1:55% Fugue 9 9:07%\nBvn. 5th str. 2:28% Elgar Qrt. 14:22%\nBlz. SF. str. 16:71% Mndsn. Heb. 16:28%\n1Fugues are from WTC book I.\nTable 2 . Spell-checking performance: fraction of false\npositives.\njGj=217 jGj=208\nFigure 8 . Selecting between two editions of Fugue 10\nusing grammar size.\nlocations and report median performance over 100exper-\niments in Table 2. We have performed this experiment\non Bach fugues, romantic symphonic works (Beethoven’s\n5th symphony 1st mvt., Berlioz’s “Symphonie Fantastique”\n1st mvt., Mendelssohn’s “Hebrides”) and Elgar’s Quartet\n3rd mvt. We observed impressive performance (Table 2)\non Bach’s fugues (error location narrowed down to just a\nfew percent of the score’s size), and even in supposedly\nless-structured symphonic works the algorithm was able to\nsubstantially narrow down the location of potential errors.\nThis suggests that our approach can be used to effectively\nlocate errors in music: for example a notation editor using\nour method may highlight potential error locations, thus\nwarning the user, much like word processors do for text.\nA variant of the above experiment is presented in Fig. 8.\nWe want to select between two editions of Fugue 10\nin which bar 33differs. We measured the total grammar\nsize for the two editions and concluded that the variant in\n12 13 14 15 16Voice 1\n12 13 14 15 16Voice 2\n12 13 14 15 16Voice 3\n12 13 14 15 16\nBar numberVoice 496\n9 95\n10 9 9 10\n6\n9 95\n10 9 9 107\n105\n10 97\n10 10 9 9\n8\n5\n10 98\n5\n10 97\n105\n10 9\n6\n9 95\n10 98\n5\n10 9Original:\nEdited with our method:\nFigure 9 . High-level editing. Above: automatic analysis of\nFugue16 (fragment); middle: original; below: rules R6\nandR8were edited with our method to obtain a new fugue.\nEdition B is more logical as it results in smaller grammar\nsizejGj=208(vs.jGj=217for Edition A).\n3.3 High-level Editing\nA grammar automatically constructed for a piece of music\ncan be used as a means for high-level editing. For exam-\nple, one may edit the right-hand sides of individual rules\nto produce a new similarly-structured piece, or, by oper-\nating on the grammar tree, alter the structure of a whole\npiece. We illustrate such editing in Fig. 9. We have auto-\nmatically analysed Fugue 16 with our method and then\nedited two of the detected rules ( R6andR8) to obtain a new\nfugue (expanding the grammar back). This new fugue is\npartially based on new material, yet maintains the structural\nperfection of the original fugue. We believe this may be a\nuseful and intuitive next-generation method for creatively\ntransforming scores.\n3.4 Further Applications\nWe speculate that in addition to the applications discussed\nabove, the power of our model may be used in other ways:\nfor estimation of complexity and information content in\na musical piece; as means for automatic summarisation\nby analysing the most compressive rules; for improved de-\ntection of similarity and plagiarism (including structural\nsimilarity); for automatic simpliﬁcation of music (by trans-\nforming a piece so as to decrease its grammar size); and for\nclassiﬁcation of music according to its structural properties.\nHaving observed that size of grammar is a good measure\nof the “amount of structure” in a piece, we suggest that our\nmodel can even be used to tell good music from bad music.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n305Hypothetically, a poor composition would remain poor even\nwhen (random) alterations are made to it and hence its\ngrammar size would be insensitive to such alterations, while\nwell-constructed works (like those of Bach in our examples)\nwould suffer, in terms of grammar size, from perturbation.\n4. CONCLUSIONS AND FUTURE WORK\nWe have posed the analysis of music as a smallest grammar\nproblem and have demonstrated that building parsimonious\ncontext-free grammars is an appealing tool for analysis of\nmusic, as grammars give insights into the underlying struc-\nture of a piece. We have discussed how such grammars may\nbe efﬁciently constructed and have illustrated the power of\nour model with a number of applications: automatic struc-\ntural analysis, error detection and spell-checking (without\nprior models), high-level editing.\nFuture work would include augmenting the presented\nautomatic grammatical analysis to allow inexact repetitions\n(variations or transformations of material) to be recognised\nin the grammar, and, in general, increasing the modelling\npower by recognising more disguised similarities in music.\n5. REFERENCES\n[1]L. Bernstein. Norton lectures. http://www.\nleonardbernstein.com/norton.htm.\n[2]G. Brodal, R. B. Lyngsø, Anna ¨Ostlin, and Christian\nN. S. Pedersen. Solving the string statistics problem in\ntime O(nlogn). In Proc. ICALP ’02, pages 728–739,\n2002.\n[3]R. Carrascosa, F. Coste, M. Gall ´e, and G. G. I. L ´opez.\nThe smallest grammar problem as constituents choice\nand minimal grammar parsing. Algorithms, 4(4):262–\n284, 2011.\n[4]R. Carrascosa, F. Coste, M. Gall ´e, and G. G. I. L ´opez.\nSearching for smallest grammars on large sequences\nand application to DNA. J. Disc. Alg., 11:62–72, 2012.\n[5]W.-Y . Chan, H. Qu, and W.-H. Mak. Visualizing the\nsemantic structure in classical music works. IEEE Trans.\nVis. and Comp. Graph., 16(1):161–173, 2010.\n[6]D. Conklin and I. H. Witten. Multiple viewpoint systems\nfor music prediction. Journal of New Music Research,\n24:51–73, 1995.\n[7]J. D. Fernandez and F. J. Vico. AI methods in algo-\nrithmic composition: A comprehensive survey. J. Artif.\nIntell. Res. (JAIR), 48:513–582, 2013.\n[8]M. Grachten, J. L. Arcos, and R. L. de M ´antaras.\nMelodic similarity: Looking for a good abstraction level.\nInProc. ISMIR, 2004.\n[9]M. Granroth-Wilding and M. Steedman. Statistical pars-\ning for harmonic analysis of jazz chord sequences. In\nProc. ICMC, pages 478–485, 2012.[10] D. Gusﬁeld. Algorithms on Strings, Trees, and Se-\nquences: Computer Science and Computational Biol-\nogy. Cambridge University Press, 1997.\n[11] M. Hamanaka, K. Hirata, and S. Tojo. Implementing a\ngenerative theory of tonal music. Journal of New Music\nResearch, 35(4):249–277, 2006.\n[12] M. Hamanaka, K. Hirata, and S. Tojo. FATTA: Full au-\ntomatic time-span tree analyzer. In Proc. ICMC, pages\n153–156, 2007.\n[13] M. Hamanaka, K. Hirata, and S. Tojo. Grouping struc-\nture generator based on music theory GTTM. J. of Inf.\nProc. Soc. of Japan, 48(1):284–299, 2007.\n[14] N. Hudson. Musical beauty and information compres-\nsion: Complex to the ear but simple to the mind? BMC\nResearch Notes, 4(1):9+, 2011.\n[15] J. Jin, B. Yang, K. Liang, and X. Wang. General image\ndenoising framework based on compressive sensing\ntheory. Computers & Graphics, 38(0):382–391, 2014.\n[16] R. M. Keller and D. R. Morrison. A grammatical ap-\nproach to automatic improvisation. In Proc. SMC ’07,\npages 330–337, 2007.\n[17] D. E. Knuth, J. H. Morris, and V . R. Pratt. Fast pat-\ntern matching in strings. SIAM Journal of Computing,\n6(2):323–350, 1977.\n[18] E. Lehman and A. Shelat. Approximation algorithms\nfor grammar-based compression. In Proc. ACM-SIAM\nSODA ’02, pages 205–212, 2002.\n[19] F. Lerdahl and R. Jackendoff. A generative theory of\ntonal music. The MIT Press, 1983.\n[20] C. G. Nevill-Manning and I. H. Witten. Identifying hier-\narchical structure in sequences: A linear-time algorithm.\nJ. Artif. Intell. Res. (JAIR), 7:64–82, 1997.\n[21] C.G. Nevill-Manning and I.H. Witten. Online and of-\nﬂine heuristics for inferring hierarchies of repetitions in\nsequences. In Proc. IEEE, volume 88, pages 1745–1755,\n2000.\n[22] D. Quick and P. Hudak. Grammar-based automated\nmusic composition in Haskell. In Proc. ACM SIGPLAN\nFARM’13, pages 59–70, 2013.\n[23] J. Rissanen. Modeling by shortest data description. Au-\ntomatica, 14(5):465–471, 1978.\n[24] M. Rohrmeier. Towards a generative syntax of tonal\nharmony. J. of Math. and Music, 5(1):35–53, 2011.\n[25] J. Schmidhuber. Low-complexity art. Leonardo,\n30(2):97–103, 1997.\n[26] Tim Smith. Fugues of the Well-Tempered Clavier.\nhttp://bach.nau.edu/clavier/nature/\nfugues/Fugue10.html, 2013.\n[27] M. J. Steedman. A generative grammar for jazz chord\nsequences. Music Perception: An Interdisciplinary Jour-\nnal, 2(1):52–77, 1984.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n306"
    },
    {
        "title": "An RNN-based Music Language Model for Improving Automatic Music Transcription.",
        "author": [
            "Siddharth Sigtia",
            "Emmanouil Benetos",
            "Srikanth Cherla",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez",
            "Simon Dixon"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416792",
        "url": "https://doi.org/10.5281/zenodo.1416792",
        "ee": "https://zenodo.org/records/1416792/files/SigtiaBCWGD14.pdf",
        "abstract": "In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcrip- tion performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal struc- ture present in symbolic music data. Similar to the func- tion of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the oc- currence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior infor- mation from the MLM is incorporated into the transcrip- tion framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic mu- sic and report a significant 3% improvement in terms of F- measure, when compared to using an acoustic-only model.",
        "zenodo_id": 1416792,
        "dblp_key": "conf/ismir/SigtiaBCWGD14",
        "keywords": [
            "Music Language Models",
            "Automatic Music Transcription",
            "Nottingham dataset",
            "Recurrent Neural Network",
            "probabilistic latent component analysis",
            "Dirichlet priors",
            "F-measure",
            "symbolic polyphonic music",
            "acoustic-only model",
            "3% improvement"
        ],
        "content": "AN RNN-BASED MUSIC LANGUAGE MODEL FOR IMPROVING\nAUT\nOMATIC MUSIC TRANSCRIPTION\nSiddharthSigtia†, EmmanouilBenetos‡, Srikanth Cherla‡,\nTillman Weyde‡, Artur S. d’Avila Garcez‡, and Simon Dixon†\n†Centre for Digital Music, Queen Mary University of London\n‡Department of Computer Science,City University London\n† {s.s.sigtia, s.e.dixon} @qmul.ac.uk\n‡ {emmanouil.benetos.1,srikanth.cherla.1,t.e.weyde,a.garcez} @city.ac.uk\nABSTRACT\nIn this paper, we investigate the use of Music Language\nModels(MLMs)forimprovingAutomaticMusicTranscrip-\ntion performance. The MLMs are trained on sequences of\nsymbolic polyphonic music from the Nottingham dataset.\nWe train Recurrent Neural Network (RNN)-based models,\nas they are capable of capturing complex temporal struc-\nture present in symbolic music data. Similar to the func-\ntion of language models in automatic speech recognition,\nweusetheMLMstogenerateapriorprobabilityfortheoc-\ncurrenceofasequence. TheacousticAMTmodelisbased\nonprobabilisticlatentcomponentanalysis,andpriorinfor-\nmation from the MLM is incorporated into the transcrip-\ntion framework using Dirichlet priors. We test our hybrid\nmodelsonadatasetofmultiple-instrumentpolyphonicmu-\nsicandreportasigniﬁcant3%improvementintermsofF-\nmeasure,whencomparedtousinganacoustic-onlymodel.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) involves automat-\nically generating a symbolic representation of an acoustic\nmusicalsignal[4]. AMTisconsideredtobeafundamental\ntopic in the ﬁeld of music information retrieval (MIR) and\nhas numerous applications in related ﬁelds in music tech-\nnology, such as interactive music applications and compu-\ntational musicology. The majority of recent transcription\npapers utilise and expand spectrogram factorisation tech-\nniques, such as non-negative matrix factorisation (NMF)\n[18] and its probabilistic counterpart, probabilistic latent\ncomponent analysis (PLCA) [25]. Spectrogram factori-\nsation techniques decompose an input spectrogram of the\naudio signal into a product of spectral templates (that typ-\nically correspond to musical notes) and component activa-\ntions (that indicate whether each note is active at a given\nc/circlecop†rtS.Sigtia,E.Benetos,S.Cherla,T.Weyde,A.S.d’Avila\nGarcez, and S. Dixon.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: S. Sigtia, E. Benetos, S. Cherla, T.\nWeyde, A. S. d’Avila Garcez, and S. Dixon. “An RNN-based Music\nLanguage Model for Improving Automatic Music Transcription”, 15th\nInternational Society for Music Information RetrievalConference, 2014.time frame). Spectrogram factorisation-based AMT sys-\ntems include the work by Bertin et al. [7], who proposed a\nBayesian framework for NMF, which considers each pitch\nasamodelofGaussiancomponentsinharmonicpositions.\nBenetosandDixon[3]proposedaconvolutivemodelbased\non PLCA, which supports the transcription of multiple-\ninstrumentmusicandsupportstuningchangesandfrequency\nmodulations (modelled as shiftsacross log-frequency).\nIn terms of connectionist approaches for AMT, Nam et\nal.[20]proposedamethodwherefeaturessuitablefortran-\nscribingmusicarelearnedusingadeepbeliefnetworkcon-\nsistingofstackedrestrictedBoltzmannmachines(RBMs).\nThe model performed classiﬁcation using support vector\nmachinesandwasappliedtopianomusic. B ¨ockandSchedl\nused recurrent neural networks (RNNs) with Long Short-\nTermMemoryunitsforperformingpolyphonicpianotran-\nscription [8], with the system being particularly good at\nrecognising note onsets.\nThere is no doubt that a reliable acoustic model is im-\nportant for generating accurate symbolic transcriptions of\na given music signal. However, since music exhibits a fair\namount of structural regularity much like language, it is\nnaturalforonetothinkofthepossibilityofimprovingtran-\nscription accuracy using a music language model (MLM)\ninamannerakintotheuseofalanguagemodeltoimprove\nthe performance of a speech recognizer [21]. In [9], the\npredictions of a polyphonic MLM were used to this end,\nwhichwasfurtherdevelopedin[10],whereaninput/output\nextension of the RNN-RBM was proposed that learned to\nmap input sequences to output sequences in the context of\nAMT.Bothin[9]and[10],evaluationswereperformedus-\ningsynthesizedMIDIdata. In[22],Raczy ´nskietal. utilise\nchord and key information for improving an NMF-based\nAMTsysteminapost-processingstep. Amajoradvantage\nofusingahybridacoustic+languagemodelsystemisthat\nthe two models can be trained independently using data\nfromdifferentsources. Thisisparticularlyusefulsincean-\nnotated audio data is scarce while it is relatively easy to\nﬁnd MIDI data for training robust language models.\nInthepresentwork,weintegrateaMLMwithanAMT\nsystem,inordertoimprovetranscriptionperformance. Specif-\nically, we make use of the predictions made by a Recur-\nrent Neural Network (RNN) and a RNN-Neural Autore-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n53gressiveDistributionEstimator(RNN-NADE)basedpoly-\nphonic\nMLM proposed in [9] to reﬁne the transcriptions\nof a PLCA-based AMT system [2,3]. Information from\nthe MLM is incorporated into the PLCA-based acoustic\nmodel as a prior for pitch activations during parameter es-\ntimation. It is observed that combining the two models\nin this way boosts transcription accuracy by +3% on the\nBach10 dataset of multiple-instrument polyphonic music\n[13], compared to using the acoustic AMT system only.\nTheoutlineofthispaperisasfollows. ThePLCA-based\ntranscription system is presented in Section 2. The RNN-\nbasedpolyphonicmusicpredictionsystemthatisusedasa\nmusiclanguagemodelisdescribedinSection3. Thecom-\nbinationofthetwoaforementionedsystemsispresentedin\nSection 4. The employed dataset, evaluation metrics, and\nexperimental results are shown in Section 5; ﬁnally, con-\nclusions are drawn in Section 6.\n2. AUTOMATIC MUSIC TRANSCRIPTION\nSYSTEM\nForcombiningacousticandmusiclanguageinformationin\nan AMT context, we employ the model of [3], which sup-\nports the transcription of multiple-instrument polyphonic\nmusic and also supports pitch deviations and frequency\nmodulations. Themodelof[3]isbasedonPLCA,whichis\na latent variable analysis method which has been used for\ndecomposing spectrograms. For computational efﬁciency\npurposes, we employ the fast implementation from [2],\nwhich utilized pre-extracted note templates that are also\npre-shifted across log-frequency, in order to account for\nfrequency modulations or tuning changes. In addition, as\nwas shown in [24], PLCA-based models can utilise priors\nfor estimating unknown model parameters, which will be\nusefulinthispaperforinformingtheacoustictranscription\nsystem with symbolic information.\nThetranscriptionmodeltakesasinputanormalisedlog-\nfrequency spectrogram Vω,t(ωis the log-frequency index\nandtis the time index) and approximates it as a bivariate\nprobability distribution P(ω,t).P(ω,t)is decomposed\ninto a series of log-frequency spectral templates per pitch,\ninstrument,andlog-frequencyshifting(whichindicatesde-\nviation with respect to the ideal tuning), as well as proba-\nbility distributions for pitch, instrument, and tuning.\nThe model is formulated as:\nP(ω,t) =P(t)/summationdisplay\np,f,sP(ω|s,p,f)Pt(f|p)Pt(s|p)Pt(p)\n(1)\nwherepdenotes pitch, sdenotes the musical instrument\nsource, and fdenotes log-frequency shifting. P(t)is the\nenergy of the log-spectrogram, which is a known quantity.\nP(ω|s,p,f)denotes pre-extracted log-spectral templates\nper pitch pand instrument s, which are also pre-shifted\nacross log-frequency. The pre-shifting operation is made\ninordertoaccountforpitchdeviations,withoutneedingto\nformulateaconvolutivemodelacrosslog-frequency. Pt(f|p)\nis the time-varying log-frequency shifting distribution per\npitch,Pt(s|p)is the time-varying source contribution perpitch, and ﬁnally, Pt(p)is the pitch activation, which es-\nsentially is the resulting music transcription. As a time-\nfrequency representation in the log-frequency domain we\nuse the constant-Q transform (CQT) with a log-spectral\nresolution of 60 bins/octave [23].\nThe unknown model parameters (P t(f|p),Pt(s|p), and\nPt(p)) can be iteratively estimated using the expectation-\nmaximisation (EM) algorithm [12]. For the Expectation\nstep, the following posterior is computed:\nPt(p,f,s|ω) =P(ω|s,p,f)Pt(f|p)Pt(s|p)Pt(p)/summationtext\np,f\n,sP(ω|s,p,f)Pt(f|p)Pt(s|p)Pt(p)\n(2)\nFor theMaximization step (without using any priors)\nunknownmodelparametersareupdatedusingtheposterior\ncomputed from the Expectation step:\nPt(f|p)∝/summationdisplay\nω,sPt(p,f,s|ω)Vω,t (3)\nPt(s|p)∝/summationdisplay\nω,fPt(p,f,s|ω)Vω,t (4)\nPt(p)∝/summationdisplay\nω,f,sPt(p,f,s|ω)Vω,t (5)\nWeconsiderthesoundstatetemplatestobeﬁxed,sono\nupdate rule for P(ω|s,p,f)is applied. Using ﬁxed tem-\nplates, 20-30 iterations using the update rules presented in\nthepresentsectionaresufﬁcientforconvergence. Theout-\nput of the system is a pitch activation which is scaled by\nthe energy of the log-spectrogram:\nP(p,t) =P(t)Pt(p) (6)\nAfterperforming5-samplemedianﬁlteringfornotesmooth-\ning,thresholdingisperformedon P(p,t)followedbymin-\nimumnotedurationpruningsetto40msinordertoconvert\nP(p,t)intoabinarypiano-rollrepresentation,whichisthe\noutputofthetranscriptionsystem,andisalsousedforeval-\nuation purposes.\n3. POLYPHONIC MUSIC PREDICTION SYSTEM\nTaking inspiration from speech recognition, it has been\nshown that a good statistical model of symbolic music can\nhelp the transcription process [11]. However there are 2\nmainreasonsfortheuseofMLMsinAMTnotbeingmore\ncommon.\n1. Training models that capture the temporal structure\nandcomplexityofsymbolicpolyphonicmusicisnot\nan easy task. In speech recognition, often simple\nlanguage models like n-grams work extremely well.\nHowever, music has a more complex structure and\nsimple statistical models like n-grams and HMMs\nfailtomodelthesecharacteristicsaccuratelyevenfor\nmusic with simple structure [9].\n2. There is no consensus on how to incorporate this\npriorinformationwithinthetranscriptionsystem. How-\never, recently there have been some successful at-\ntemptsatusingthispriorinformationtoimprovethe\naccuracy on AMT tasks [9,10].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n54In this section we discuss the details of the music pre-\ndiction\nsystem and the models used. In the next section\nwe discuss how we incorporate the predictions from these\nmodels in a PLCA-based music transcription system.\n3.1 Recurrent Neural Networks\nArecurrentneuralnetwork(RNN)isapowerfulmodelfor\ntime-seriesdatawhichcanaccountforlong-termtemporal\ndependencies, over multiple time-scales when trained ef-\nfectively. Given a sequence of inputs v1,v2,...,v Teach\ninRn, the network computes a sequence of hidden states\nˆh1,ˆh2,...,ˆhTeach inRm, and a sequence of predictions\nˆy1,ˆy2,...,ˆyTeach inRkby iterating the equations\nˆht=e(Wˆhxvt+Wˆhˆhˆht−1+bˆh)(7)\nˆyt=g(Wyˆh) (8)\nwhereWyˆh,Wˆhx,Wˆhˆhare the weight matrices, bˆhis the\nbias andeandgare activation functions which are typi-\ncally non-linear and applied element-wise.\nAn RNN can be trained using the gradient-based Back-\nPropagation Through Time algorithm [27] using the ex-\nactlycomputableerrorgradientsinthenetwork. However,\n1stordergradientmethodsfailtocorrectlytrainRNNsfor\nmanyreal-worldproblems. Thisdifﬁcultyhasbeenassoci-\natedwithwhatisknownasthe vanishing/explodinggradi-\nentsphenomenon [6],wheretheerrorsexhibitexponential\ndecay/growth as they are back-propagated through time.\nyears [15,16,19].\nHowever, recent work in the ﬁeld of neural networks\nand deep learning has led to several improvements in gra-\ndient based optimization methods that make training of\nRNNs possible. Most notably, the Hessian Free (HF) opti-\nmization algorithm has been used to train RNNs success-\nfully on several real world datasets, including symbolic\npolyphonicmusicdata[19]. Apartfromsecondordermeth-\nods like HF, several modiﬁcations to ﬁrst-order gradient\nbased methods exist that currently form the state of the art\nin training RNNs [5].\n3.2 Recurrent Neural Network-based models\nOneofthedrawbacksofusingRNNstopredictpolyphonic\nsymbolicmusicisthatanyoutputofthenetwork, ˆyiattime\nstept, is conditionally independent of ˆyj,∀j∝negationslash=igiven\nthe sequence of input vectors v1,v2,...,v T. This is a se-\nvere constraint when used for modelling polyphonic mu-\nsic, where notes often appear in very correlated patterns\nwithin a frame. In order to overcome this limitation, mod-\nelsderivedfromRNNshavebeenproposedwhicharebet-\nter at modelling high-dimensional sequences [9,26].\nThe ﬁrst RNN-based model that tried to model high-\ndimensionalsequencesistheRecurrentTemporalRestricted\nBoltzmann Machine (RTRBM) [26]. This model was ex-\ntended to the more general RNN-RBM model, where the\nhidden states for the RBM and RNN were not constrained\nto be the same. For our prediction system, we make use of\na variant of the RNN-RBM, called the RNN-NADE. The\nonly difference is that the conditional distributions at eachstep are modelled by a Neural Autoregressive Distribution\nEstimator (NADE) [17] as opposed to an RBM. As dis-\ncussedinthenextsection,tocombinethepredictionswith\nthe transcription system, we need individual pitch activa-\ntionprobabilitiesateachtime-step. Obtainingtheseproba-\nbilities from an RBM is intractable as it requires summing\nover all possible hidden states. However the NADE is a\ntractable distribution estimator and we can easily obtain\ntheseprobabilitiesfromtheNADE.TheNADEmodelsthe\nprobability of occurrence of a vector pas:\nP(p) =D/productdisplay\ni=1P(pi|p<i) (9)\nwherep∈RD,piis the pitch activation and p<iis the\nvector containing all the pitch activations pjsuch that j <\ni.\nIn our system we utilise each of the conditional proba-\nbilitiesp(pi|p<i)as probabilities of the pitch activations.\nAlthough the pitch activation probabilities are only con-\nditioned on p<i, we hypothesize that this will be a better\nmodel than the RNN, where the pitch activation probabil-\nities are completely independent. Another motivation for\nusing the NADE is that the gradients can be computed ex-\nactly, and therefore we can employ HF optimization for\ntraining the RNN-NADE.\n4. COMBINING TRANSCRIPTION AND\nPREDICTION\nIn this section, we describe the process for combining the\nacousticmodelwiththemusiclanguagemodelforderiving\nanimprovedtranscription. Firstly,theinputmusicsignalis\ntranscribed using the process described in Section 2. The\nresulting piano-roll representation of the transcription sys-\ntem is considered to be a sequence p1,p2,...,p Tthat is\nplaced as input to the MLM presented in Section 3. For\nthe RNN-NADE, we compute the probability P(pi|p<i)\nfor all time frames, and use that as prior information for\nthe combined model, with the prior information denoted\nasPMLM(p,t), where PMLM(p=i,t) =P(pi|p<i).\nFor the RNN, the prediction output is directly denoted as\nPMLM(p,t), since pitch probabilities are independent.\nAsshownin[24],PLCA-basedmodelsusemultinomial\ndistributions; since the Dirichlet distribution is conjugate\nto the multinomial, a Dirichlet prior can be used to en-\nforce structure on the pitch activation distribution Pt(p).\nFollowing the procedure of [24], we deﬁne the Dirichlet\nhyperparameter for the pitch activation as:\nαt(p)∝Pt(p)PMLM(p,t) (10)\nwhereαt(p)essentially is a pitch activation probability\nwhich is ﬁltered through a pitch indicator function com-\nputed from the symbolic prediction step (the denominator\nis simply for normalisation purposes).\nTherecordingisthenre-transcribed,usingasadditional\ninformationthepriorcomputedfromthetranscriptionstep.\nThemodiﬁedupdateforthepitchactivationwhichreplaces\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n55(5) is given by:\nPt(p)∝/summationdisplay\nω,f\n,sPt(p,f,s|ω)Vω,t+καt(p)(11)\nwhereκis a weight parameter expressing how much the\nprior should be imposed; as in [24], the weight decreases\nfrom 1 to 0 throughout the iterations. To summarize, the\ntranscription creates a symbolic prediction, which in turn\nimproves the subsequent re-transcription of the music sig-\nnal. An overview of the complete transcription-prediction\nsystem architecture can be seen in Fig. 1.\n5. EVALUATION\n5.1 Dataset\nFortestingthetranscriptionsystem,weemploytheBach10\ndataset [13], which is a freely available multi-track collec-\ntion of multiple-instrument polyphonic music. It consists\nof ten recordings of J.S. Bach chorales, performed by vi-\nolin, clarinet, saxophone, and bassoon. Pitch ground truth\nfor each instrument is also provided. Due to the tonal and\nhomogeneouscontentofthedataset(singlecomposer,sin-\ngle music language), it is suitable for testing the incorpo-\nration of music language models in a multiple-instrument\ntranscription system. For training the transcription sys-\ntem, pre-extracted and pre-shifted spectral templates are\nextracted for the instruments present in the dataset, using\nisolated note samples from the RWC database [14].\nFortrainingtheMLMsweusetheNottinghamdataset1,\nacollectionof1200musicpiecesinsymbolicABCformat,\nwhich contain simple chord combinations and tunes. We\ntrained the RNN and the RNN-NADE models using both\nStochastic Gradient Descent (SGD) and HF to compare\nperformance. The inputs to both the models are sequences\noflength200whereeachframeofthesequenceisabinary\nvectoroflength88whichcoversthefullpianonoterange.\nWetrainboththeRNNandtheRNN-NADEtopredictthe\nnext vector given a sequence of input vectors. We train\nthe models by minimizing the negative log-likelihood of\nthe sequences using the cross-entropy/summationtext\nitilogpi+(1−\nti)log(1−pi)whereisumsoverallthedimensionsofthe\nbinary vector and tiis the pitch target.\n5.2 Metrics\nForevaluatingtheperformanceoftheproposedsystemfor\nmulti-pitch detection, we employ the precision (Pre ), re-\ncall (Rec ), and F-measure (F ) metrics, which are com-\nmonlyusedintranscriptionevaluations[1]. Asinthepub-\nlicevaluationsonmulti-pitchdetectioncarriedoutthrough\nthe MIREX framework [1], a detected note is considered\ncorrectifitspitchisthesameasthegroundtruthpitchand\nits onset is within a 50ms tolerance interval of the ground-\ntruth onset.\n1ifdo.ca/∼seymour/nottingham/nottingham.htmlModel Pre\nRNN (SGD) 67.89%\nRNN (HF) 69.61%\nRNN-NADE (SGD) 68.89%\nRNN-NADE (HF) 70.61%\nTable 1. Validation results for MLMs\n5.3\nResults\nTovalidatetheperformanceoftheMLMs,wecalculatethe\nprediction precision on unseen sequences of music from\nthe Nottingham dataset of folk melodies. We utilise 694\ntracks for training, 173 tracks for validation and 170 for\ntesting2. For both the RNN and RNN-NADE models we\nsample10vectorsfromtheconditionaldistributionateach\ntime-step and calculate the expected precision against the\nground truth. The reported precision is found by ﬁnd-\ning the mean over the predictions of every frame. Table\n1 shows the results of the validation experiments. These\nresults are of the same order as the prediction accuracies\nreportedin[9]. Wefoundthatforboththemodels,HFop-\ntimization gave better precision than SGD. Training with\nHF was also easier as there were less hyper parameters\nto be tuned when compared to SGD, where learning rate\nneeds tobeupdated tomake suretraining iseffective. The\nRNN models had a hidden layer of size 150, while the\nRNN-NADE models had a hidden layer of size 100 and\nthe NADE consisted of a hidden layer of size 150.\nMulti-pitch detection experiments are performed using\nthe proposed system, with various conﬁgurations. A ﬁrst\nconﬁgurationonlyconsidersthetranscriptionsystemfrom\nSection 2. A second conﬁguration takes the output of the\ntranscription system and gives it as input to the prediction\nsystem of Section 3, where the ﬁnal piano-roll is the out-\nputofthepredictionstep. Athirdconﬁguration(presented\nin Section 4), re-transcribes the recording, having the pre-\ndiction as a prior information for estimating the pitch acti-\nvations. For the prediction system, experiments were per-\nformed using both the RNN-NADE and the RNN.\nResultsusingthevarioussystemconﬁgurationsaredis-\nplayed in Table 2. It can be seen that the best perfor-\nmance is achieved by the 3rd conﬁguration when using\nthe NADE-HF model for prediction, which surpasses the\nacoustic-only transcription system by more than 3%. In\ngeneral, it can be seen that using the prediction system as\na post-processing step (2nd conﬁguration) always leads to\nan improvement over the acoustic-only model (1st conﬁg-\nuration). A similar trend can be observed when integrat-\ning the prediction information as a prior in the transcrip-\ntion system (conﬁguration 3) compared to just using the\nprediction system as post-processing (conﬁguration 2); an\nimprovement is always reported. Another observation can\nbe made when comparing the RNN-NADE with the RNN,\nwith the former providing a clear improvement. For com-\nparative purposes, we also trained MLMs using 500 MIDI\nﬁles of J.S. Bach chorales3and tested the models on the\n2http://www-etud.iro.umontreal.ca/∼boulanni/icml2012\n3http://www.jsbchorales.net/sets.shtml\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n56AUDIOTIME-FREQUENCY\nREPRESENTATIONTRANSCRIPTION PREDICTIONPIANO-ROLL\nDICTIONARY\nFigure 1. Proposed system diagram.\nConﬁguration F Pre Rec\nConﬁguration 1 62.02% 58.51% 66.12%\nConﬁguration 2 - NADE 62.62% 59.70% 65.92%\nConﬁguration 3 - NADE 64.08% 61.96% 66.44%\nConﬁguration 2 - RNN 62.29% 59.08% 65.98%\nConﬁguration 3 - RNN 63.85% 61.14% 66.90%\nConﬁguration 2 - NADE-HF 62.20% 59.14% 65.68%\nConﬁguration 3 - NADE-HF 65.16% 62.80% 67.78%\nConﬁguration 2 - RNN-HF 62.44% 59.28% 66.07%\nConﬁguration 3 - RNN-HF 62.87% 60.03% 66.11%\nTable 2. Transcription results using various system con-\nﬁgurations.\nBach10\nrecordings. Using the Bach MLMs, the system\nreachedF= 63.58%, which is an improvement over the\nacoustic-only system, but is outperformed by the Notting-\nham language model.\nQualitatively, the MLMs are able to improve transcrip-\ntion performance by providing a rough estimate of which\npitches areexpected toappear intherecording (and which\npitches are not expected to appear). The language mod-\nels were trained using simple chord sequences (from the\nNottinghamdataset)thatarerepresentativeofsimpletonal\nmusic and are applicable as language models to the more\ncomplex Bach chorales. We believe that the reason for the\nJ.S. Bach MLMs not performing as well as the Notting-\nham MLMs is due to the fact that predicting Bach’s music\nisacomplextask(manyexceptions,keychanges,modula-\ntions), whereas a simple tonal model like the Nottingham\ndataset can work as a general-purpose language model in\nmany types of music (thisis also veriﬁed in [9]).\nBycomparingwiththemethodof[13](wheretheBach10\ndataset was ﬁrst introduced), the proposed method using\ntheframe-basedaccuracymetricreaches74.3%fortheNADE-\nHFusingthe3rdconﬁguration,whereasthemethodof[13]\nreaches 69.7% (with unknown polyphony). As an exam-\npleoftheproposedsystem’sperformance,thespectrogram\nandrawoutputofthesystemusingthe3rdconﬁgurationis\ndisplayedforaBach10recordingFig. 2,whereasthepost-\nprocessed transcription output along with the ground truth\nfor the same recording is shown in Fig. 3.\n6. CONCLUSIONS\nIn this paper, we proposed a system for automatic music\ntranscription which incorporated prior information from\na polyphonic music prediction model based on recurrent\np\nt(sec)(b)ω(a)\n2 4 6 8 10 12 14 16 18 20 222 4 6 8 10 12 14 16 18 20 22\n10203040506070100200300400500\nFigure 2. (a) The spectrogram Vω ,tfor recording “Ach\nLieben Christen” from the Bach10 dataset. (b) The pitch\nactivation P(p,t)usingthetranscription-predictionsystem\nusing the 3rd conﬁguration, with the NADE-HF.\nneural networks. The acoustic transcription model was\nbased on probabilistic latent component analysis, and in-\nformationfromthepredictionsystemwasincorporatedus-\ningDirichletpriors. Experimentalresultsusingthemultiple-\ninstrumentBach10datasetshowedthatthereisaclearand\nsigniﬁcant improvement (3% in terms of F-measure) by\ncombiningamusiclanguagemodelwithanacousticmodel\nfor improving the performance of the latter. These results\nalsodemonstratethattheMLMcanbetrainedonsymbolic\nmusicdatafromadifferentsourceastheacousticdata,thus\neliminatingtheneedtoacquirecollectionsofsymbolicand\ncorresponding acoustic data (which are scarce).\nIn the current system, the language models are trained\non only one dataset. In the future, we would like to eval-\nuate the proposed system using language models trained\nfromdifferentsourcestoseeifthishelpstheMLMsgener-\nalize better. We will also investigate different system con-\nﬁgurations, to test whether iterating the transcription and\nprediction steps leads to improved performance. We will\nalso investigate the effect of using different RNN archi-\ntectures like Long Short Term Memory (LSTM) and bi-\ndirectional RNNs and LSTMs. Finally, we would like to\nextend the current models for high-dimensional sequences\ntobetterﬁttherequirementsformusiclanguagemodelling.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n57t (sec)p(b)p(a)\n2 4 6 8 10 12 14 16 18 20 222 4 6 8 10 12 14 16 18 20 22\n1020304050607010203040506070\nFigure 3. T ranscription example for recording “Ach\nLieben Christen” from the Bach10 dataset. (a) The post-\nprocessed output of the transcription-predicton system us-\ning the 3rd conﬁguration, with the NADE-HF. (b) The\npitch ground truthof the recording.\n7. ACKNOWLEDGEMENT\nSSissupportedbyaCityUniversityLondonPump-Priming\nGrantandtheQueenMaryUniversityofLondonPostgrad-\nuate Research Fund. EB is supported by a City University\nLondon Research Fellowship. SC is supported by a City\nUniversity London Research Studentship.\n8. REFERENCES\n[1] MusicInformationRetrievalEvaluationeXchange(MIREX).\nhttp://music-ir.org/mirexwiki/.\n[2] E. Benetos, S. Cherla, and T. Weyde. An effcient shiftinvari-\nantmodelforpolyphonicmusictranscription.In 6thInterna-\ntional Workshop on Machine Learning and Music, 2013.\n[3] E. Benetos and S. Dixon. A shift-invariant latent variable\nmodel for automatic music transcription. Computer Music\nJournal, 36(4):81–94, 2012.\n[4] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: challenges and\nfuture directions. Journal of Intelligent Information Systems ,\n41(3):407–434, December 2013.\n[5] Y.Bengio,N.Boulanger-Lewandowski,andR.Pascanu. Ad-\nvances in optimizing recurrent networks. In ICASSP, pages\n8624–8628, May 2013.\n[6] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term\ndependencies with gradient descent is difﬁcult. IEEE Trans.\nNeural Networks ,5(2):157–166, 1994.\n[7] N.Bertin,R.Badeau,andE.Vincent. Enforcingharmonicity\nand smoothness in Bayesian non-negative matrix factoriza-\ntion applied to polyphonic music transcription. IEEE Trans.\nAudio, Speech, and Language Processing, 18(3):538–549,\nMarch 2010.\n[8] S. B¨ock and M. Schedl. Polyphonic piano note transcription\nwith recurrent neural networks. In ICASSP, pages 121–124,\nMarch 2012.[9] N.Boulanger-Lewandowski,Y.Bengio,andP.Vincent.Mod-\neling temporal dependencies in high-dimensional sequences:\nApplication to polyphonic music generation and transcrip-\ntion. In29th Int. Conf. Machine Learning, 2012.\n[10] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent.\nHigh-dimensional sequence transduction. In ICASSP, pages\n3178–3182, May 2013.\n[11] A. T. Cemgil. Bayesian Music Transcription. PhD thesis,\nRadboud University of Nijmegen, 2004.\n[12] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum\nlikelihood from incomplete data via the EM algorithm. Jour-\nnal of the Royal Statistical Society, 39(1):1–38, 1977.\n[13] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamental fre-\nquency estimation by modeling spectral peaks and non-peak\nregions.IEEE Trans. Audio, Speech, and Language Process-\ning, 18(8):2121–2133, November 2010.\n[14] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWC\nmusic database: music genre database and musical instru-\nment sound database. In ISMIR, Baltimore, USA, October\n2003.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\n[16] H.Jaeger.Adaptivenonlinearsystemidentiﬁcationwithecho\nstatenetworks.In Advancesinneuralinformationprocessing\nsystems, pages 593–600, 2002.\n[17] H. Larochelle and I. Murray. The neural autoregressive dis-\ntribution estimator. Journal of Machine Learning Research,\n15:29–37, 2011.\n[18] D. D. Li and H. S. Seung. Learning the parts of objects by\nnon-negative matrix factorization. Nature, 401:788–791, Oc-\ntober 1999.\n[19] J. Martens and I. Sutskever. Learning recurrent neural net-\nworkswithHessian-freeoptimization. In 28thInt.Conf.Ma-\nchine Learning, pages 1033–1040, 2011.\n[20] J. Nam, J. Ngiam, H. Lee, and M. Slaney. A classiﬁcation-\nbased polyphonic piano transcription approach using learned\nfeature representations. In ISMIR, pages 175–180, October\n2011.\n[21] L. Rabiner and B.-H. Juang. Fundamentals of speech recog-\nnition. 1993.\n[22] S.A. Raczynski, E. Vincent, and S. Sagayama. Dynamic\nBayesian networks for symbolic polyphonic pitch modeling.\nIEEETransactionsonAudio,Speech,andLanguageProcess-\ning, 21(9):1830–1840, 2013.\n[23] C. Sch ¨orkhuber and A. Klapuri. Constant-Q transform tool-\nboxformusicprocessing.In 7thSoundandMusicComputing\nConf., Barcelona, Spain, July 2010.\n[24] P. Smaragdis and G. Mysore. Separation by “humming”:\nuser-guided sound extraction from monophonic mixtures. In\nIEEEWASPAA, pages 69–72, October 2009.\n[25] P. Smaragdis, B. Raj, and M. Shashanka. A probabilistic la-\ntent variable model for acoustic modeling. In Neural Infor-\nmationProcessingSystemsWorkshop,Whistler,Canada,De-\ncember 2006.\n[26] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent\ntemporalrestrictedBoltzmannmachine.In AdvancesinNeu-\nralInformationProcessingSystems ,pages1601–1608,2008.\n[27] P.J.Werbos.Backpropagationthroughtime: whatitdoesand\nhow to do it. Proceedings of the IEEE, 78(10):1550–1560,\n1990.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n58"
    },
    {
        "title": "Music Classification by Transductive Learning Using Bipartite Heterogeneous Networks.",
        "author": [
            "Diego Furtado Silva",
            "Rafael Geraldeli Rossi",
            "Solange Oliveira Rezende",
            "Gustavo Enrique De Almeida Prado Alves Batista"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1418265",
        "url": "https://doi.org/10.5281/zenodo.1418265",
        "ee": "https://zenodo.org/records/1418265/files/SilvaRRB14.pdf",
        "abstract": "The popularization of music distribution in electronic for- mat has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled in- stances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous net- works have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they oc- cur. Thus, there is no parameter or additional costs to gen- erate such networks. In this paper, we propose the use of the bipartite network representation to perform trans- ductive classification of music, using a bag-of-frames ap- proach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available.",
        "zenodo_id": 1418265,
        "dblp_key": "conf/ismir/SilvaRRB14",
        "keywords": [
            "music distribution",
            "incomplete metadata",
            "music recommendation",
            "transductive classification",
            "label propagation",
            "networks",
            "data representation",
            "parameters",
            "classification accuracy",
            "bipartite networks"
        ],
        "content": "MUSIC CLASSIFICATION BY TRANSDUCTIVE LEARNING USING\nBIPARTITE HETEROGENEOUS NETWORKS\nDiego F. Silva, Rafael G. Rossi, Solange O. Rezende, Gustavo E. A. P. A. Batista\nInstituto de Ci ˆencias Matem ´aticas e de Computac ¸ ˜ao, Universidade de S ˜ao Paulo\nfdiegofsilva,ragero,solange,gbatistag@icmc.usp.br\nABSTRACT\nThe popularization of music distribution in electronic for-\nmat has increased the amount of music with incomplete\nmetadata. The incompleteness of data can hamper some\nimportant tasks, such as music and artist recommendation.\nIn this scenario, transductive classiﬁcation can be used to\nclassify the whole dataset considering just few labeled in-\nstances. Usually transductive classiﬁcation is performed\nthrough label propagation, in which data are represented as\nnetworks and the examples propagate their labels through\ntheir connections. Similarity-based networks are usually\napplied to model data as network. However, this kind of\nrepresentation requires the deﬁnition of parameters, which\nsigniﬁcantly affect the classiﬁcation accuracy, and presents\na high cost due to the computation of similarities among all\ndataset instances. In contrast, bipartite heterogeneous net-\nworks have appeared as an alternative to similarity-based\nnetworks in text mining applications. In these networks,\nthe words are connected to the documents which they oc-\ncur. Thus, there is no parameter or additional costs to gen-\nerate such networks. In this paper, we propose the use\nof the bipartite network representation to perform trans-\nductive classiﬁcation of music, using a bag-of-frames ap-\nproach to describe music signals. We demonstrate that the\nproposed approach outperforms other music classiﬁcation\napproaches when few labeled instances are available.\n1. INTRODUCTION\nThe popularity of online music services has dramatically\nincreased in the last decade. The revenue of online ser-\nvices, such as music streaming, has more than triplicate in\nthe last three years. Online services already account for a\nsigniﬁcant 40% of the overall industry trade revenues [1].\nHowever, the popularization of music and video clips dis-\ntribution in electronic format has increased the amount of\nmusic with incomplete metadata. The incompleteness of\nc\rDiego F. Silva, Rafael G. Rossi, Solange O. Rezende,\nGustavo E. A. P. A. Batista.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Diego F. Silva, Rafael G. Rossi,\nSolange O. Rezende, Gustavo E. A. P. A. Batista. “Music Classiﬁca-\ntion by Transductive Learning Using Bipartite Heterogeneous Networks”,\n15th International Society for Music Information Retrieval Conference,\n2014.the data can hamper some important tasks such as index-\ning, retrieval and recommendation.\nFor instance, users of music services commonly deﬁne\ntheir preferences based on genre information. A recom-\nmendation system can make use of such information to\nsuggest other music conditional to the expressed prefer-\nences. The lack of genre information on music imposes\nlimits to the capability of the recommendation systems to\ncorrectly identify consume patterns as well as to recom-\nmend a diverse set of music. Similar statements can be\nmade for music indexing and retrieval.\nDue to the academic and commercial importance of dig-\nital music, we have witnessed in the last decade a tremen-\ndous increase of interest for Music Information Retrieval\n(MIR) tasks. Most of the proposed MIR methods are based\non supervised learning techniques. Supervised learning\nusually requires a substantial amount of correctly labeled\ndata in order to induce accurate classiﬁers. Although la-\nbeled data can be obtained with human supervision, such\nprocess is usually expensive and time consuming. A more\npractical approach is to employ methods that can avail\nof both a small number of labeled instances and a large\namount of unlabeled data.\nTransductive learning directly estimates the labels of\nunlabeled instances without creating a classiﬁcation model.\nA common approach to perform transductive classiﬁcation\nis label propagation, in which the dataset is represented as\na network and the labels of labeled instances are propa-\ngated to the unlabeled instances through the network con-\nnections. Similarity-based networks are usually applied\nto represent data as networks for label propagation [19].\nHowever, they present a high cost due to the computation\nof the similarities among all dataset instances, and require\nthe deﬁnition of several graph construction parameters that\ncan signiﬁcantly affect the classiﬁcation accuracy [11].\nBipartite heterogeneous networks have appeared as an\nalternative to similarity-based networks in sparse domains,\nsuch as text mining [9, 10]. In these networks, words are\nconnected to documents in which they occur. Thus, there\nare no parameters or additional costs to generate such net-\nworks. In a similar way, we can represent music collec-\ntions as a bipartite network though the use of a bag-of-\nframes (BoF) representation. The BoF is a variation of\nbag-of-words (BoW) representation used in text analysis\nand has been applied in studies of genre recognition, mu-\nsic similarity, and others [12].\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n113In this paper, we propose the use of the bipartite net-\nwork representation to perform transductive classiﬁcation\nof music, using a BoF approach to describe music signals.\nWe demonstrate that the proposed approach outperforms\nother music classiﬁcation approaches when few labeled in-\nstances are available.\n2. BACKGROUND & RELATED WORK\nTransductive classiﬁcation is a useful way to classify all\ndataset instances when just few labeled instances are avail-\nable [19]. Perhaps the most common and intuitive way to\nperform transductive classiﬁcation is through label propa-\ngation, which is commonly made by using similarity-based\nnetworks to represent the data. Usual ways to generate\nsimilarity-based networks are [17–19]: (i) fully connected-\nnetwork, in which every pair of instances are connected;\n(ii)knearest neighbor, in which an instance is connected\nwith itskmost similar instances; and (iii) \u000fnetwork, in\nwhich two instances are connected if their distance is above\na threshold.\nBipartite networks have appeared as an alternative to\nsimilarity-based networks in sparse domains such as texts\n[9, 10]. The use of bipartite networks to represent text\ncollections and the use of algorithms which perform label\npropagation in bipartite networks obtained results as good\nas the obtained by similarity-based networks [10]. How-\never, the computation cost to generate similarity-based net-\nworks isO(jIj2\u0002jAj), in whichjIjis the number of in-\ntances andjAjis the number of attributes of a dataset,\nwhile the computational cost to generate bipartite networks\nisO(jIj\u0002jAj). Morevorer, the gerenation of bipartite net-\nworks is parameter-free.\nWe can represent music collections as a bipartite net-\nwork though the generation of a bag-of-frames (BoF).\nMethods using BoF has became common in different MIR\ntasks, including similarity, genre, emotion and cover song\nrecognition [12]. Such strategies basically consist of three\nmain steps: feature extraction, codebook generation and\nlearning/classiﬁcation.\nProbably, the most simple and commonly used strat-\negy for the codebook generation is the Vector Quantization\n(VQ). Basically, the VQ uses clustering algorithms on the\nframe-level features and consider the center of clusters as\nthe words of a dictionary. The simple k-means is, prob-\nably, the most used algorithm in this step and showed to\nachieve similar results to other methods [8].\nRecently, new tools have emerged for creating code-\nbooks. Speciﬁcally, strategies based on Sparse Cod-\ning [5, 15] and Deep Belief Networks [3, 7] have been\nwidely used. However, even though these strategies often\nimprove the results in different domains, they can present\nsimilar performance to simple strategies such as VQ in cer-\ntain tasks [7].\n3. PROPOSED FRAMEWORK: MC-LPBN\nIn this paper we propose a framework called MC-LPBN\n(Music Classiﬁcation through Label Propagation in Bi-\nMFCC  \nWord  \nCandidate  \nMFCC  ... Music Signal  Figure 1. Word candidates generation process\nCodebook  Clustering  \nAlgorithm  \n(k-means)  \nWord  \nCandidates  \nWords  \nFigure 2. The word candidates are clustered and each cen-\ntroid is elected as a codeword. The word frequency is di-\nrectly related to the candidates count in each cluster\npartite Networks) to perform transductive classiﬁcation of\nmusics. The proposed framework has three main steps: (i)\ncodebook generation, (ii) network generation for transduc-\ntive classiﬁcation, and (iii) transductive classiﬁcation using\nbipartite heterogeneous networks. In the next subsections\nwe present the details of each step.\n3.1 Codebook Generation and Bag-of-Frames\nIn order to represent a music collection as a BoF it is nec-\nessary to extract a set of words. Such procedure starts with\nthe extraction of word candidates. A word candidate is a\nset of features extracted from a single window. As a sliding\nwindow swipes across the entire music signal, each music\ngives origin to a set of word candidates. In this work we\nuse MFCC as feature extraction procedure. Figure 1 illus-\ntrates the word candidates generation process.\nThe next step is the creation of a codebook. A codebook\nis a set of codewords used to associate the word candidates\nto a ﬁnite set of words. The idea is to select the most rep-\nresentative codeword for each word candidate. To do this,\nwe use a clustering algorithm with the word candidates and\nconsider the center of each cluster as a codeword. So, each\ncandidate is associated to the codeword that represents the\ncluster it belongs. In this step, we used the ubiquitous k-\nmeans algorithm, due to its simplicity and efﬁciency. Fig-\nure 2 illustrates this procedure.\nFinally, there is a step to the generation of a BoF ma-\ntrix. In such a matrix, each line corresponds to a music\nrecording, each column corresponds to a word and the cells\ncorrespond to the frequency of occurrence of the word in\nthe music. The BoF allows the generation of bipartite net-\nworks for transductive classiﬁcation, as we discuss in the\nnext subsection.\n3.2 Network Generation for Transductive\nClassiﬁcation\nFormally, a network is deﬁned by N=hO;E;Wi, in\nwhichOrepresents the set of objects (entities) of a prob-\nlem,Erepresents the set of connections among the objects\nandWrepresents the weights of the connections. When\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n114Ois composed by a single type of object, the network is\ncalled homogeneous network. When Ois composed by h\ndifferent types of objects, i.e., O=O1[\u0001\u0001\u0001[O h, the\nnetworks is called heterogeneous network [6].\nThe music collection can be represented by a bipar-\ntite heterogeneous network with O=M[T , in which\nM=fm1;m2;:::;mngrepresents the set of music and\nT=ft1;t2;:::;tlgrepresents the set of words. Mis\ncomposed by labeled (ML) and unlabeled (MU) music,\ni.e.,M=ML[MU. A musicmi2M and a word\ntj2T are connected if tjoccurs inmi. The weight of the\nrelation between miandtj(wmi;tj) is the frequency of tj\ninmi. Thus, only the words and their frequencies in the\nmusic are needed to generate the bipartite network.\nFor transductive classiﬁcation based on networks, let\nC=fc1;c2;:::;clgbe the set of possible labels, and let\nfoi=ff1;f2;:::;f jCjgTbe the weight vector of an ob-\njectoi, which determines its weight or relevance for each\nclass. Hence, it is also referred as class information vector.\nThe class information of an object mi2M or an object\ntj2 T is denoted respectively by fmiandftj. All the\nclass information of objects in MorTis denoted by the\nmatrices F(M) =ffm1;fm2;:::;fmjMjgTandF(T) =\nfft1;ft2;:::;ftjT jgT. The class information of a labeled\nmusicmiis stored in a vector ymi=fy1;y2;:::;y jCjgT,\nwhich has the value 1 to the corresponding class position\nand 0 to the others. The weights of connections among ob-\njects are stored in a matrix W. A diagonal matrix Dis used\nto store the degree of the objects, i.e., the sum of the con-\nnection weights of the objects. Thus the degree of a music\nmiin a bipartite network is (d mi=di;i=P\ntj2Twmi;tj).\n3.3 Transductive Classiﬁcation Using Bipartite\nHeterogeneous Networks\nThe main algorithms for transductive classiﬁcation in data\nrepresented as networks are based on regularization [19],\nwhich have to satisfy two assumptions: (i) the class infor-\nmation of neighbors must be close; and (ii) the class infor-\nmation assigned during the classiﬁcation process must be\nclose to the real class information. In this paper we used\nthree regularization-based algorithms: (i) Tag-based clas-\nsiﬁcation Model (TM) [16], (ii) Label Propagation based\non Bipartite Heterogeneous Networks (LPBHN) [10], and\n(iii) GNetMine (GM) [6].\nTM algorithm minimizes the differences among (i) the\nreal class information and the class information assigned to\nmusic (M), (ii) the real class information and the class in-\nformation assigned to and objects from other domains that\naid the classiﬁcation (A ), and (iii) the class information\namong words (T ) and objects in (M) or (A ), as presented\nin Equation 1.\nQ(F) =\u000bX\nai2Ajjfai\u0000yaijj2+\fX\nmi2MLjjfmi\u0000ymijj2(1)\n+\rX\nmi2MUjjfmi\u0000ymijj2+X\noi2M[AX\ntj2Twoi;tjjjfoi\u0000ftjjj2The parameters \u000b,\fand\rcontrol the importance given\nfor each assumption of TM. Objects are classiﬁed using\nclass mass normalization [18].\nLPBHN is a parameter-free algorithm based on the\nGaussian Fields and Harmonic Functions (GFHF) algo-\nrithm [18], which performs label propagation in homoge-\nneous networks. The difference is that LPBHN considers\nthe relations among different types of objects. The func-\ntion to be minimized by LPBHN is:\nQ(F) =1\n2X\nmi2MX\ntj2Twmi;tjjjfmi\u0000ftjjj2(2)\n+ lim\n\u0016!1\u0016X\nmi2MLjjfmi\u0000ymijj2;\nin which\u0016tending to inﬁnity means that fmi\u0011ymi, i.e.,\nthe information class of labeled musics do not change.\nThe GM framework is based on the Learning with\nLocal and Global Consistency (LLGC) algorithm [17],\nwhich performs label propagation in homogeneous net-\nworks. The difference between the algorithms is that GM\nconsiders the different types of relations among the differ-\nent types of objects. For the problem of music classiﬁ-\ncation using bipartite networks, GM minimizes the differ-\nences of (i) the class information among music and words\nand (ii) the class information assigned to labeled music\nduring the classiﬁcation and their real class information.\nThe function to be minimized by GM is:\nQ(F) =X\nmi2MX\ntj2Twmi;tj\f\f\f\f\f\f\f\f\f\ffmip\ndmi\u0000ftjpdtj\f\f\f\f\f\f\f\f\f\f2\n(3)\n+X\nmi2M\u0016jjfmi\u0000ymijj2;\nin which 0<\u0016< 1.\nWe highlight that all the algorithms presented above\nhave iterative solutions to minimize the respective equa-\ntions. This allows to obtain similar results to the closed\nsolutions with a lower computational time.\n4. EXPERIMENTAL EV ALUATION\nTo illustrate the generality of our approach, we evaluate\nour framework in two different scenarios. In this sec-\ntion, we describe the tasks we considered, the experimen-\ntal setup used in our experiments, as well as the results\nobtained and a short discussion about them.\n4.1 Tasks Description\nWe evaluate our framework in genre recognition and cover\nsong recognition scenarios. The remaining of this section\ncontains a brief description of each task and the datasets\nused to each end.\n4.1.1 Genre Recognition\nGenre recognition is an important task in several applica-\ntions. Genre is a quality created by the human beings to\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n115intuitively characterize music [14]. For humans, the clas-\nsiﬁcation of music by genre is relatively simple task, and\ncan be done by listening to a short excerpt of a music.\nTherefore, most of the existing data for this task con-\nsiders a short duration excerpt for each recording. In this\nwork, we use the GTZAN1and Homburg2datasets. The\nﬁrst has 1;000snippets of 30seconds of ten different gen-\nres. The number of instances of each class is equally dis-\ntributed. The Homburg dataset, in turn, has ten seconds\nsections of 1;886recordings, belonging to nine genres. In\nthis case, the genre with fewer instances has only 47exam-\nples, while the largest has 504.\n4.1.2 Cover Song Recognition\nCover song may be deﬁned as distinct performances of\nthe same music with differences in tempo, instrumentation,\nstyle or other characteristics [4]. Finding reinterpreted mu-\nsic is an important task mainly to commercial ends. For ex-\nample, it can be used to ensure copyright in websites which\nallow users to create content.\nIn this paper, we evaluate our framework in a task sim-\nilar to the cover song recognition. But, instead ﬁnd the\noriginal recording of a query music, we consider all differ-\nent interpretations of the same music as the same class.\nTo evaluate our proposal we used the Mazurkas Project\ndata3, in which each music has several versions. This\ndataset contains 2914 recordings of 49Chopin’s mazurkas\nfor piano (from 43to97versions per class).\n4.2 Experimental Setup\nWe evaluated our framework considering different conﬁg-\nurations for the 1stand3rdsteps. For the 1ststep, we\nconsider variations of parameters of the feature extraction\nand codebook generation phases. In this work, we use 20\nMFCC as frame-level features. This number is a com-\nmon choice in MIR papers [2]. We use 5different win-\ndow sizes, with an overlap of 50% between them: 0:0625,\n0:125, 0:25, 0:5and0:75 seconds. Finally, we applied the\nk-means using k2f100;200;400;800;1600; 3200g.\nFor the 3rd, we consider the algorithms presented in\nSection 3.3: Label Propagation using Bipartite Heteroge-\nneous Networks (LPBHN), Tag-based Model (TM), and\nGNetMine (GM). For GM we use the parameter \u000b2\nf0:1; 0:3;0:5;0:7;0:9g. For TM we use \u000b= 0, since\nthere are no objects from different domains, \f2f0:1;1:0;\n10;100;1000g, and \r2 f0:1; 1:0;10:0; 100:0; 1000:0g.\nThe iterative solutions proposed by the respective authors\nwere used for all the algorithms. The maximum number of\niterations is set to 1000 since this is a common limit value\nfor iterative solutions.\nWe also carried out experiments using two algorithms\nfor label propagation in similarity-based networks, Learn-\ning with Local and Global Consistency (LLGC) [17] and\nGaussian Fields and Harmonic Functions (GFHF) [18],\n1http://marsyas.info/download/data sets/\n2http://www-ai.cs.uni-dortmund.de/audio.html\n3http://www.mazurka.org.uk/and two classical supervised learning algorithms for com-\nparison with the proposed approach, kNearest Neighbors\n(kNN) and Support Vector Machines (SVM) [13].\nWe build similarity-based networks using the fully con-\nnected approach with \u001b= 0:5 [19] and we set \u000b=\nf0:1; 0:3;0:5;0:7;0:9g for LLGC algorithm. For kNN we\nsetk= 7and weighted vote, and for SVM we used linear\nkernel andC= 1[9].\nThe metric used for comparison was the classiﬁcation\naccuracy, i.e., the percentage of correctly classiﬁed mu-\nsic recordings. The accuracies are obtained considering\nthe average accuracies of 10 runs. In each run we ran-\ndomize the dataset and select xexamples as labeled ex-\namples. The remaining jMj\u0000xexamples are used for\nmeasuring the accuracy. We carried out experiments us-\ningx=f1;10;20;30;40;50gto analyze the trade-off be-\ntween the number of labeled documents and classiﬁcation\naccuracy. The best accuracies obtained by some set of pa-\nrameters of the algorithms are used for comparison.\n4.3 Results and Discussion\nGiven the large amount of results obtained in this work,\ntheir complete presentation becomes impossible due to\nspace constraints. Thus, we developed a website for this\nwork, where detailed results can be found4. In this sec-\ntion, we summarize the results from different points of\nview.\n4.3.1 Inﬂuence of Parameters Variation\nOur ﬁrst analysis consists in evaluating the inﬂuence of\nthe variation of the codebook generation step parameters.\nFigure 3 presents the variation of accuracy for both genre\nrecognition dataset and each window size according to a\ndifferent number of words in the dictionary. To do this,\nwe ﬁxed the number of labeled examples in 10. This num-\nber represents a good trade-off between the classiﬁcation\naccuracy of the algorithms and the human effort to label\nmusic. But, we note that the behaviors are similar to other\nnumbers of properly labeled examples.\nThe results show that the transductive learning methods\ncan achieve similar or even superior results than the ob-\ntained by using inductive models. In the case of GTZAN\ndata, there is a clear increasing pattern when the number of\nwords varies. Using higher values to it, both strategies per-\nform well, but the transductive learning obtained the higher\naccuracies. The results obtained by similarity-based net-\nworks were slightly better in most of conﬁgurations. But,\nas mentioned before, similarity-based networks has a high\ncost to calculate the similarities between all the examples\nand require the setting of several parameters to construct\nthe network. In the Homburg dataset, transductive learn-\ning is better independently of the parameter conﬁguration.\nIn this case, there are no signiﬁcant differences between bi-\npartite network approaches, but they performed better than\nthe similarity-based networks in most of cases.\nIn order to evaluate our framework in the cover song\nrecognition, we used the LPBHN algorithm, that achieved\n4http://sites.labic.icmc.usp.br/dfs/ismir2014\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n116100 200 400 800 1600 3200\n# Words3540455055Accuracy (%)(a) GTZAN - 0.0625\n100 200 400 800 1600 3200\n# Words3540455055Accuracy (%) (b) GTZAN - 0.125\n100 200 400 800 1600 3200\n# Words3540455055Accuracy (%) (c) GTZAN - 0.25\n100 200 400 800 1600 3200\n# Words3540455055Accuracy (%) (d) GTZAN - 0.5\n100 200 400 800 1600 3200\n# Words3540455055Accuracy (%) (e) GTZAN - 0.75\n100 200 400 800 1600 3200\n# Words152025303540Accuracy (%)\n(f) Homburg - 0.0625\n100 200 400 800 1600 3200\n# Words152025303540Accuracy (%) (g) Homburg - 0.125\n100 200 400 800 1600 3200\n# Words152025303540Accuracy (%) (h) Homburg - 0.25\n100 200 400 800 1600 3200\n# Words152025303540Accuracy (%) (i) Homburg - 0.5\n100 200 400 800 1600 3200\n# Words152025303540Accuracy (%) (j) Homburg - 0.75\nFigure 3. Accuracy for genre recognition by varying the number of words in the codebook. The number of labeled\nexamples per class is ﬁxed in 10.\nsimilar result to other transductive methods and has the ad-\nvantage of being parameter-free, and both, SVM and kNN,\ninductive approaches. The results show a high increasing\npattern when transductive learning is used, and a more sta-\nble pattern to supervised methods. Figure 4 shows the ac-\ncuracy achieved by ﬁxing the number of labeled examples\nin10. We ﬁxed the window size to 0:75 seconds, since\nMazurkas is the larger dataset used in this work and this is\nthe fastest conﬁguration to the feature extraction phase. We\nbelieve that the performance of transductive learning can\novercome the SVM if we increase the number of words.\n100 200 400 800 1600 3200\n# Words60657075808590Accuracy (%)KNN\nSVM\nLPBHN\nFigure 4. Accuracy for Mazurkas by varying the number\nof words in the codebook. The number of labeled examples\nis ﬁxed in 10and the window length in 0:75 seconds.\n4.3.2 Number of Labeled Examples Variation\nThe evaluation of the performance variation according to\nthe number of labeled examples is an important analysis in\nthe context of transductive learning. Figure 5 shows the be-\nhavior of accuracy in genre recognition task when there is\na variation in the number of labeled examples that belong\nto each class. The results were obtained by ﬁxing the num-\nber of words in 3200, in which good results were achieved\nin several conﬁgurations, and a window size of the middle\nvalue of 0:25 seconds, since the results were similar than\nthe obtained with other values to this parameter.\nTo analyze these graphs, it is interesting to know the\nproportion that the number of labeled examples represents\nin each dataset. For example, in the case of GTZAN set,50examples correspond to exactly 50% of the examples in\neach classes. In the case of Homburg, it represents 100%\nof the minority class, but less than 10% of the majority.\nIn both cases, the behavior of the accuracies was simi-\nlar. As the number of labeled samples increases, the per-\nformance becomes better. The transductive learning meth-\nods performed better across the curve. As the proportion\nof the number of labeled instances increases, the tendency\nis that the performance of inductive algorithms approaches\nthe performance of transductive algorithms.\nFor sake of space limitations, we omitted the results for\nthe cover song recognition task. However, we point out\nthat the behavior of accuracy rates were very similar to ob-\ntained in the other task.\n5. CONCLUSION\nIn this paper, we presented a framework for transductive\nclassiﬁcation of music using bipartite heteregeneous net-\nworks. We show that we can have a better performance by\nusing this approach instead the traditional inductive learn-\ning. Our results were close or superior to the obtained by\nsimilarity-based networks. This kind of network, however,\nrequires several parameters and has a high cost due to the\ncalculation of the similarity between the instances.\nWe should note that the accuracy rates achieved in this\npaper are worse than some results presented in related\nworks. For example, there are some papers that achieved\naccuracy higher than 80% to the GTZAN dataset using\nBoF approaches. However, these results were probably\nobtained due to the choice of speciﬁc features and param-\neters. Moreover, these papers used inductive learning ap-\nproaches, with labels for the entire dataset. Nevertheless,\nwe demonstrated that, for the same parameter set, the use\nof bipartite heterogeneous network achieved the best re-\nsults.\nAs future work we will investigate a better feature con-\nﬁguration and different codebook generation strategies.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1171 10 2 0 30 4 0 50\n# labeled examples for each class2030405060Accuracy (%)(a) GTZAN\n1 10 2 0 30 4 0 50\n# labeled examples for each class0102030405060Accuracy (%) (b) Homburg\nFigure 5. Accuracy of genre recognition by varying the numbers of labeled examples for each class. The number of words\nand the windows length are ﬁxed in 3200 and0:25 seconds, respectively.\nACKNOWLEDGEMENTS: We would like to thank the\nﬁnancial supports by the grants 2011/12823-6, 2012/50714-\n7, 2013/26151-5, and 2014/08996-0, Sao Paulo Research\nFoundation (FAPESP).\n6. REFERENCES\n[1] Recording industry in numbers. Technical report, In-\nternational Federation of the Phonographic Industry,\n2014.\n[2] M. A. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges. Proceedings of the IEEE, 96(4):668–696, 2008.\n[3] S. Dieleman, P. Brakel, and B. Schrauwen. Audio-\nbased music classiﬁcation with a pretrained convolu-\ntional network. In ISMIR, pages 669–674, 2011.\n[4] D. P. W. Ellis and T. Bertin-Mahieux. Large-scale cover\nsong recognition using the 2d fourier transform magni-\ntude. In ISMIR, pages 241–246, 2012.\n[5] M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y . LeCun.\nUnsupervised learning of sparse features for scalable\naudio classiﬁcation. In ISMIR, pages 681–686, 2011.\n[6] M. Ji, Y . Sun, M. Danilevsky, J. Han, and J. Gao.\nGraph regularized transductive classiﬁcation on het-\nerogeneous information networks. In Proc. Eur. Conf.\non Machine Learning and Knowledge Discovery in\nDatabases, pages 570–586. Springer-Verlag, 2010.\n[7] J. Nam, J. Herrera, M. Slaney, and J. O. Smith. Learn-\ning sparse feature representations for music annotation\nand retrieval. In ISMIR, pages 565–570, 2012.\n[8] M. Riley, E. Heinen, and J. Ghosh. A text retrieval\napproach to content-based audio retrieval. In ISMIR,\npages 295–300, 2008.\n[9] R. G. Rossi, de T. P. Faleiros, de A. A. Lopes, and\nS. O. Rezende. Inductive model generation for text cat-\negorization using a bipartite heterogeneous network. In\nProc. Intl. Conf. on Data Mining, pages 1086–1091.\nIEEE, 2012.[10] R. G. Rossi, A. A. Lopes, and S. O. Rezende. A\nparameter-free label propagation algorithm using bi-\npartite heterogeneous networks for text classiﬁcation.\nInProc. Symp. on Applied Computing, pages 79–84.\nACM, 2014.\n[11] C. A. R. Sousa, S. O. Rezende, and G. E. A. P. A.\nBatista. Inﬂuence of graph construction on semi-\nsupervised learning. In Proc. Eur. Conf. Machine\nLearning and Knowledge Discovery in Databases,\npages 160–175. Springer-Verlag, 2013.\n[12] L. Su, C.-C.M. Yeh, J.-Y . Liu, J.-C. Wang, and Y .-\nH. Yang. A systematic evaluation of the bag-of-frames\nrepresentation for music information retrieval. IEEE\nTransactions on Multimedia, 16(5):1188–1200, Aug\n2014.\n[13] P.-N. Tan, M. Steinbach, and V . Kumar. Introduction to\nData Mining. Addison-Wesley, 2005.\n[14] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE transactions on Speech and\nAudio Processing, 10(5):293–302, 2002.\n[15] C. C. M. Yeh, L. Su, and Y . H. Yang. Dual-layer\nbag-of-frames model for music genre classiﬁcation. In\nIntl. Conf. on Acoustics, Speech and Signal Processing,\n2013.\n[16] Z. Yin, R. Li, Q. Mei, and J. Han. Exploring social\ntagging graph for web object classiﬁcation. In Proc.\nIntl. Conf. on Knowledge Discovery and Data Mining,\npages 957–966, 2009.\n[17] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and\nB. Sch ¨olkopf. Learning with local and global consis-\ntency. In Advances in Neural Information Processing\nSystems, volume 16, pages 321–328, 2004.\n[18] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-\nsupervised learning using gaussian ﬁelds and harmonic\nfunctions. In Proc. Intl. Conf. on Machine Learning,\npages 912–919. AAAI Press, 2003.\n[19] X. Zhu, A. B. Goldberg, R. Brachman, and T. Diet-\nterich. Introduction to Semi-Supervised Learning. Mor-\ngan and Claypool Publishers, 2009.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n118"
    },
    {
        "title": "On Cultural, Textual and Experiential Aspects of Music Mood.",
        "author": [
            "Abhishek Singhi",
            "Daniel G. Brown 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417391",
        "url": "https://doi.org/10.5281/zenodo.1417391",
        "ee": "https://zenodo.org/records/1417391/files/SinghiB14.pdf",
        "abstract": "We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years.  While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hear- ing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music.  We conclude that people as- sign different moods to the same song in these three sce- narios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyr- ics does not help reduce the difference in music mood perception between Canadian and Chinese listeners sig- nificantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context.",
        "zenodo_id": 1417391,
        "dblp_key": "conf/ismir/SinghiB14",
        "keywords": [
            "music mood perception",
            "Canadian listeners",
            "Chinese-Canadians",
            "Chinese people",
            "user study",
            "lyrics presence",
            "mood assignments",
            "stable mood assignments",
            "recent popular English songs",
            "access to lyrics"
        ],
        "content": "ON CULTURAL, TEXTUAL AND EXPERIENTIAL ASPECTS \nOF MUSIC MOOD \n             Abhishek Si nghi  Daniel G. Brown  \n University of Waterloo  \nCheriton School of Computer Science  \n{asinghi,dan.brown}@uwaterloo.ca   \nABSTRACT \nWe study the impact of the presence of lyrics on music \nmood perception for both Canadian and Chinese listeners \nby conducting a user study of Canadians not of Chinese \norigin, Chinese-Canadians, and Chinese people who have \nlived in Canada for fewer than three years.  While our \noriginal hypotheses were largely connected to cultural \ncomponents of mood perception, we also analyzed how \nstable mood assignments were when listeners could read \nthe lyrics of recent popular English songs they were hea r-\ning versus when they only heard the songs . We also \nshowed the lyrics of some songs to participants without \nplaying the recorded music.  We conclude that people a s-\nsign different moods to the same song in these three sc e-\nnarios. People tend to assign a song to the mood cluster \nthat includes “melancholy” more often when  they read the \nlyrics without listening to it, and having access to the ly r-\nics does not help reduce the difference in music mood \nperception between Canadian and Chinese listeners si g-\nnificantly. Our results cause us to question the idea that \nsongs have “i nherent mood ”. Rather, we suggest that the \nmood depends on both cultural and experiential context. \n1. INTRODUCTION \nMusic mood detection has been identified as an important \nMusic Information Retrieval (MIR) task. For example, \nthere is a MIREX audio mood classification task [12] . \nThough most automatic mood classification systems are \nsolely based on the audio content of the song, some sy s-\ntems have used lyrics or have combined audio and lyrics \nfeatures ( e.g., [3-5] and [6-7]) Previous studies have \nshown that combing these features improves classification \naccuracy ( e.g., [6-7] and [9]) but as mentioned by Downie \net al. in [3], there is no consensus on whether audio or lyr-\nical features are more useful.  \n     Implicit in “mood identification ” is the belief that \nsongs ha ve “inherent mood,” but in practice this assig n-\nment is unstable.  Recent work has focused on associating \nsongs with more than one mood label, where similar mood tags are generally grouped together into the same \nlabel ( e.g., [2]), but this still tends to be in a stable liste n-\ning environment.   \n     Our focus is instead on the cultural and experiential \ncontext in which people interact with a work of music.  \nPeople's cultural origin may affect their response to a \nwork of art, as may their previous exposure to a song, \ntheir perception of its genre, or the role that a song or \nsimilar songs has had in their life experiences .  \n     We focus on people's cultural origin, and on how they \ninteract with songs (for example, seeing the lyrics sheet or \nnot). Listening to songs while reading lyrics is a common \nactivity: for example, there are “ lyrics videos”  (which o n-\nly show lyrics text) on YouTube with hundreds of mi l-\nlions of views ( e.g. “Boulevard of Broken Dreams”) , and \nCD liner notes often include the text of lyrics.  Our core \nhypothesis is that there is enough plasticity in assigning \nmoods to songs, based on context, to argue that many \nsongs have no inher ent “mood”.  \n     Past studies have shown that there exist differences in \nmusic mood perception among Chinese and American lis-\nteners ( e.g., [8]). We surmised that some of this diffe r-\nence in mood perception is due to weak English language \nskills of Chinese listeners: perhaps such listeners are una-\nble to grasp the wording in the audio. We expected that \nthey might more consistently match the assignments of \nnative English-speaking Canadians when shown the lyrics \nto songs they are hearing than in their absence. We a d-\ndressed the cultural hypothesis by exploring Canadians of \nChinese origin, most of whom speak English natively bu t \nhave been raised in households that are at least somewhat \nculturally Chinese. If such Chinese-Canadians match C a-\nnadians not of Chinese origin in their assignments of \nmoods to songs, this might at least somewhat argue \nagainst the supposition that being Chinese in culture had \nan effect on mood assignment, and would support our b e-\nlief that linguistic skills account for at least some of the \ndifferences. Our campus has many Chinese and Chinese-\nCanadians, which also facilitated our decision to focus on \nthese communities. \n     In this study we use the same five mood clusters as are \nused in the MIREX audio mood classification task and \nask the survey participants to assign a song to only one \nmood cluster. A multimodal mood classification could be \na possible extension to our work here. Earlier works in \nMIR [11] had used Russell’s valence -arousal model \nwhere the mood is determined by the valence and arousal \nscores of the song ; we stick to the simpler classification \nhere.  \n © Abhishek Singhi , Daniel G. Brown . \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Abhishek Singhi, Daniel G. \nBrown . “On Cultural , Textual A nd Experie ntial Aspects  Of Music \nMood ”, 15th International Society for Music Information Retrieval \nConf erence, 2014.  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n3  \n \n     In practice, our hypotheses about language expertise \nwere not upheld by our experimental data.  Rather, our \ndata support the claim that both cultural background and \nexperiential context have significant impact on the mood \nassigned by listeners to songs, and this effect makes us \nquestion the meaningfulness of “mood” as a category in \nMIR. \n2. RELATED WORK \nMood classification is a classic task in MIR, and is one \nof the MIREX challenges. Several projects have used ly r-\nics as part of the mood prediction task. Lu et al. [1] and \nTrohidis et al. [2] come up with an automatic mood cla s-\nsification system solely based on audio. Several projects \nlike Downie et al. [3], Xiong et al. [4] and Chen et al. [5], \nhave used lyrics as part of the mood prediction task. \nDownie et al. [3] show that features derived from lyrics \noutperform audio features in 7 out of the 8 categories. \nDownie et al. [6], Laurier et al. [7] and Yang et al. [9] \nshow that systems which combine audio and lyrics fe a-\ntures outperform systems using only audio or only lyrics \nfeatures. Downie et al. [6] show that using a combination \nof lyrics and audio features reduces the need of training \ndata required to achieve the same or better accuracy le v-\nels than only-audio or only-lyrics systems. \n     Lee et al. [8] study the difference in music mood pe r-\nception between Chinese and American listeners on a set \nof 30 songs and conclude that mood judgment differs b e-\ntween Chinese and American participants and that people \nbelonging to the same culture tend to agree more on m u-\nsic mood judgment. That study primarily used the co m-\nmon Beatles data set, which may have been unfamiliar to \nall audiences, given its age. Their study collected mood \njudgments solely based on the audio; we also ask partic i-\npants to assign mood to a song based on its lyrics or by \npresenting both audio and lyrics together. To our \nknowledge no work has been done on the mood of a song \nwhen both audio and lyrics of the song is made available \nto the participants, which as we have noted is a common \nexperience . Kosta et al. [11] study if Greeks and non-\nGreeks agree on arousal and valence rating for Greek \nmusic. They conclude that there is a greater degree of \nagreement among Greeks compared to non-Greeks po s-\nsibly because of acculturation to the songs.   \n     Downie et al. [3] , Laurier et al. [7] and Lee et al. [8] \nuse 18 mood tags derived from social tags and use mu l-\ntimodal mood classification system. Trohidis et al. [2] \nuse multi modal mood classification into six mood clu s-\nters. Kosta et al. [11] use Russell’s valence -arousal mo d-\nel which has 28 emotion denoting adjectives in a two d i-\nmensional space. Downie et al. [10] use the All Music \nGuide datasets to come up with 29 mood tags and cluster \nit into five groups. These five mood clusters are used in \nthe MIREX audio music mood classification task. We use these clusters where each song is assigned a single \nmood cluster. \nMood Clu sters Mood Tags  \nCluster 1  passionate, rousing, confident, boiste r-\nous, rowdy  \nCluster 2  rollicking, cheerful, fun, sweet, ami a-\nble/good natured  \nCluster 3  literate, poignant, wistful, bittersweet, \nautumnal, brooding  \nCluster 4  humorous, silly, campy, quirky, whi m-\nsical, witty, wry  \nCluster 5  aggressive, fiery, tense/anxious, i n-\ntense, volatile, visceral  \nTable 1.  The mood clusters used in the study. \n3. METHOD \n3.1 Data S et \nWe selected fifty very popular English-language songs of \nthe 2000’s, with songs from all popular genres, and with \nan equal number of male and female singers. We verified \nthat the selected songs were international hits by going to \nthe songs' Wikipedia pages and analyzing the peak pos i-\ntion reached in various geographies.       \n     We focus on English-language popular music in our \nstudy, because it is the closest to “un iversally” popular \nmusic currently extant, due to the strength of the music \nindustry in English-speaking countries. Our data set i n-\ncludes music from the US, Canada and the UK and Ir e-\nland. \n3.2 Participants \nThe presence of a large Chinese and Canadian population \nat our university, along with obvious cultural differences \nbetween the two communities, convinced us to use them \nfor the study. We also include Canadians of Chinese \norigin; we are unaware of any previous MIR work that \nhas considered such a group.  We note that the Chinese-\nCanadian group is diverse: while some speak Chinese \nlanguages, others have comparatively little exposure to \nC\nhinese language or culture. \n     We recruited 100 participants, mostly university st u-\ndents, from three groups: \n 33 Chinese, living in Canada for less than 3 \nyears. \n 33 Canadians, not of Chinese origin, born and \nbrought up in Canada, with English as their \nmother tongue. \n 34 Canadians, of Chinese origin, born and \nbrought up in Canada. \n3.3 Survey \nEach participant was asked to assign a mood cluster to \neach song in a set of 10 songs. For the first three songs \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4  \n \nthey saw only the lyrics; for the next three songs they only \nheard the first 90 seconds of the audio; and for the last \nfour songs they had access to both the lyrics and the first \n90 seconds of the audio simultaneously. They assigned \neach song to one of the five mood clusters shown in Table \n1. We collected 1000 music mood responses for 50 songs, \n300 each based solely either on audio or lyrics and 400 \nbased on both audio and lyrics together. We note that due \nto their high popularity, some songs shown only via lyrics \nmay have been known to some participants. We did not \nask participants if this was the case. \n4. RESULTS \nWe hypothesized that the difference in music mood pe r-\nception between American and Chinese listeners demo n-\nstrated by Hu and Lee [8] is because of the weak spoken \nEnglish language skills of Chinese students, and that this \nmight give them some difficulty in understand the wor d-\ning of songs; this is why we allowed our participants to \nsee the lyrics for seven out of ten songs. \n     We had the following set of hypotheses before our \nstudy: \n People often assign different mood to the same \nsong depending on whether they read the lyrics, \nor listen the audio or both simultaneously. \n Chinese-born Chinese listeners will have less \nconsistency in the assignment of moods to songs \nthan do Canadian-born non-Chinese when given \nonly the recording of a song. \n Chinese-born Chinese will more consistently \nmatch Canadians when they are shown the lyrics \nto songs. \n Just reading the lyrics will be more helpful in \nmatching Canadians than just hearing the music \nfor Chinese-born Canadians. \n Canadian-born Chinese participants will be i n-\ndistinguishable from Canadian-born non-Chinese \nparticipants. \n A song does not have an inherent mood: its \n\"mood\" depends on the way it is perceived by \nthe listener, which is often listener-dependent. \n4.1 Lyrics and music mood perception between cu l-\ntures \nWe started this study with the hypothesis that difference \nin music mood perception between Chinese and Canadian \ncultures is partly caused by English language skills, and \nthat if participants are asked to assign mood to a song \nbased on its lyrics, we will see much more similarity in \njudgment between two different groups.  \n     We used the Kullback-Leibler distance between the \ndistribution of responses from one group and the distrib u-\ntion of responses from that group and another group to identify how similar the two groups' assignments of \nmoods to songs were, and we used a permutation test to \nidentify how significantly similar or different the two \ngroups were.  In Table 2, we show the number of songs \nfor which different population groups are surprisingly \nsimilar. What we find is that the three groups actually \nagree quite a bit in uncertainty of assigning mood to \nsongs when they are presented only with the recording: if \none song has uncertain mood assignment for Canadian \nlisteners, our Chinese listeners also typically did not co n-\nsistently assign a single mood to the same song. \n     Our original hypothesis was that adding presented ly r-\nics to the experience would make Chinese listeners agree \nmore with the Canadian listeners, due to reduced unce r-\ntainty in what they were hearing.  In actuality, this did not \nhappen at all: in fact, presence of both audio and lyrics \nresulted in both communities having both more uncertai n-\nty and disagreeing about the possible moods to assign to a \nsong. \n     This confusion in assigning a mood might be because \na lot of hit songs ( “Boulevard of Broken Dreams ”, “Viva \nLa Vida ”, “You’re Beautiful” , etc.) use depressing words \nwith very upbeat tunes. It could also be that by presenting \nboth lyrics and audio changes the way a song is perceived \nby the participants and leads to a completely new exper i-\nence. (We note parenthetically that this argues against u s-\ning lyrics only features in computer prediction of song \nmood.) \n     The number of songs with substantial agreement be-\ntween Chinese and Canadian, not of Chinese origin, pa r-\nticipants remains almost the same with lyrics only and a u-\ndio only, but falls drastically when both are presented t o-\ngether. (Note again: in this experiment, we are seeing how \nmuch the distribution of assignments differs for the two \ncommunities.) This contradicts our hypothesis that the \ndifference in music mood perception between Chinese \nand Canadians is because of their difference in English \nabilities. It could of course be the case that many Chinese \nparticipants did not understand the meaning of some of \nthe lyrics. \n    We had hypothesized that Canadians, of Chinese and \nnon-Chinese origin would have very similar mood jud g-\nments because of similar English language skills but they \ndo tend to disagree a lot on music mood. The mood \njudgment agreement between Chinese and Canadian, of \nChinese and non-Chinese origin seem to be similar and \nwe conclude that we can make no useful claims about the \nChinese-Canadian participants in our sample. \n     On the whole we conclude that the presence of lyrics \ndoes not significantly increase the music mood agreement \nbetween Chinese and Canadian participants: in fact, being \nable to read lyrics while listening to a recording seems to \nsignificantly decrease the music mood agreement between \nthe groups. \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5  \n \n  lyrics  audio  audio+lyrics  \nChinese  Canadians  25 22 14 \nChinese  Canadian -\nChinese  36 31 27 \nChinese  non-Chinese \nCanadians  31 32 23 \nnon-\nChinese \nCanadians  Canadian -\nChinese  36 29 31 \nTable 2. The number of statistically significantly similar \nresponses between the different cultures for the three di f-\nferent ways they interact with the songs. “Canadians” r e-\nfer to Canadians of both Chinese and non-Chinese origin. \n4.2 Stability across the three kinds of experiences \nWe analyze the response from participants when they are \nmade to listen to the lyrics, hear the audio or both simu l-\ntaneously across all the three groups. We calculate Sha n-\nnon entropy of this mood assignment for each of the 50 \nsongs for the three ways we presented a song to the pa r-\nticipants: some songs have much more uncertainty in how \nth\ne participants assign mood cluster to them.  We then see \nif this entropy is correlated across the three kinds of exp e-\nrience, using Spearman’s rank correlation coeff icient of \nthis entropy value between the groups. A rank correlation \nof 1.0 would mean that the song with the most entropy in \nits mood assignment in one experience category is also \nthe most entropic in the other, and so on.  \n \n Spearman’s rank correl ation \ncoefficient  \nonly lyrics & only a udio 0.050 4 \nonly lyrics & a udio+lyrics  0.1093  \nonly audio & a udio+lyrics  0.0771  \nTable 3.  Spearman’s rank correlation coeff icient between \nthe groups. The groups \"only lyrics\" and \"only audio\" \nidentify when participants had access to only lyrics and \naudio respectively while “audio+lyrics ” refers to when \nthey had access to both simultaneously. \n     The low value of the correlation analysis suggests that \nthere is almost no relationship be-tween \"certainty\" in \nmusic mood across the three different kinds of experien c-\nes: for songs like “Wake Up ” by Hillary Duff and “Maria \nMaria ” by Santana, listeners who only heard the song \nwere consistent in their opinion that the song was from \nthe second cluster , “cheerful”, while listeners who heard \nthe song and read the lyrics were far more uncertain as to \nwhich class to assign the song to. \n4.3 “Melancholy” lyrics  \nFor each song, we identify the mood cluster to which it \nwas most often assigned, and show these in Table 4. Mood Clu sters only lyrics  only audio  audio+lyrics  \nCluster 1  8 9 13 \nCluster 2  5 15 11 \nCluster 3  28 14 18 \nCluster 4  4 6 3 \nCluster 5  5 6 5 \nTable 4 . The most commonly assigned mood clusters for \neach experimental context. Most songs are assigned to the \nthird mood cluster when participants are shown only the \nlyrics. \n     Songs experienced only with the lyrics are most often \nassigned to the third mood cluster, which includes the \nmood tags similar to \"melancholy\". In the presence of a u-\ndio or both audio and lyrics there is a sharp decline in the \nnumber of songs assigned to that cluster; this may be a \nconsequence of \"melancholy\" lyrics being attached to \nsurprisingly cheery tunes that cause listeners to assign \nthem to the first two clusters. The number of songs a s-\nsigned to the fourth and fifth cluster remains more similar \nacross all experiential contexts.  Even between the two \ncontexts where the listener does hear the recording of the \nsong, there is a good deal of inconsistency in assignment \nof mood to songs: for 27 songs, the most commonly ide n-\ntified mood is different between the \"only audio\" and \n“audio+lyrics ” data. \n4.4 Rock songs \nWe explored different genres in our test set, to see if our \ndifferent cultural groups might respond in predictable \nways when assigning moods to songs.  \n     Things that might be considered loud to Chinese li s-\nteners could be perceived as normal to Canadian listeners. \nThus, we examined how responses differed across these \ntwo groups for rock songs, of which we had twelve in our \ndata set. We calculate the Shannon entropy of the r e-\nsponse of the participants and present the result in table 5 .   \n     We see that for many rock songs, there is high dive r-\ngence in the mood assigned to the song by our listeners \nfrom these diverse cultures.  For seven of the twelve rock \nsongs, the most diversity of opinion is found when liste n-\ners both read lyrics and hear the audio, while for three \nsongs, all participants who only read the lyrics agreed e x-\nactly on the song mood (zero entropy). \n     We see that for 3 of 12 cases all the participants tend \nto agree on the mood for the song when they are given \naccess to the lyrics. The data for lyrics only have lower \nentropy than audio for 5 of 12 cases and all five of these \nsongs are \"rebellious\" in style. For the five cases where \nthe audio-only set has lower entropy than lyrics-only, the \nsong has a more optimistic feel to it. This is consistent \nwith our finding in the last section about melancholy song \nlyrics. \n     For example, the lyrics of “Boulevard of Broken \nDreams”, an extremely popular Green Day song, evoke \nisolation and sadness, consistent with the third mood clu s-\nter. On the other hand the song's music is upbeat which \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6  \n \nmay give the increased confusion when the participant has \naccess to both the audio and lyrics for the song. \n \nSong  only ly r-\nics only \naudio  audio+lyrics  \n“Compl icated ” 1.0 0.918  1.148  \n“American Idiot ” 1.792  1.459  1.792  \n“Apologize ” 1.0 1.25 1.0 \n“Boulevard of Br o-\nken Dreams ” 0.0  1.792  2.155  \n“Bad Day ” 1.792  1.459  1.061  \n“In the End ” 0.65 1.459  1.061  \n“Viva La Vida ” 0.0 1.5849  1.75 \n“It’s My life ” 0.0 0.65 1.298  \n“Yellow ” 1.792  0.65 1.351  \n“Feel” 0.918  0.650  1.148  \n“Beautiful Day ” 1.584  1.459  1.836  \n“Numb ” 1.25 1.918  0.591  \nTable 5 . Entropy values for rock songs for the three di f-\nferent categories. \n4.5 Hip-Hop/ Rap \nLee et al. [8]  show that mood agreement among Chinese \nand American listeners is least for dance songs.  Our test \nset included five rap songs, and since this genre is often \nused at dance parties, we analyzed user response for this \ngenre. Again, we show the entropy of mood assignment \nfor the three different experiential contexts in Table 6 .  \n     What is again striking is that seeing the lyrics (which \nin the case of rap music is the primary creative element of \nthe song) creates more uncertainty among listeners as to \nthe mood of the song, while just hearing the audio recor d-\ning tends to yield more consistency. Perhaps this is b e-\ncause the catchy tunes of most rap music pushes listeners \nto make a spot judgment as to mood, while being remin d-\ned of lyrics pushes them to evaluate more complexity. \n     In general we see that there is high entropy in mood \nassignment for these songs, and so we confirm the prev i-\nous claim that mood is less consistent for “danceable ” \nsongs. \n5. DOES MUSIC MOOD EXIS T? \nFor music mood classification to be a well-defined task, \nthe implicit belief is th at songs have “inherent mood(s),”  \nthat are detectable by audio features. Our hypothesis is \nthat many songs have no inherent mood, but that the pe r-\nceived mood of a song depends on cultural and experie n-\ntial factors. The data from our study supports our hypot h-\nesis. \n     We have earlier shown that the mood judgment of a \nsong depends on whether it is heard to or its lyrics is read \nor both together, and that all three contexts produce mood \nassignments that are strikingly independent.       We have shown that participants are more likely to a s-\nsign a song to the “ melancholic”  mood cluster when only \nreading its lyrics, and we have shown genre-specific cu l-\ntural and experiential contexts that affect how mood a p-\npears to be perceived. Together, these findings suggest \nthat that the concept of music mood is fraught with unce r-\ntainty. \n     The result of the MIREX audio mood classification \ntask has had a maximum classification accuracy of less \nthan 70% [12], with no significant recent improvements. \nPerhaps, this suggests that the field is stuck at a plateau, \nand we need to redefine “music mood” and change our \napproach to the music mood classification problem. M u-\nsic mood is highly affected by external factors like the \nway a listener interacts with the song, the genre of the \nsong, the mood and personality of the listener, and future \nsystems should take these factors into account. \n \nSong  only ly r-\nics only \naudio  audio+lyrics  \n“London Bridge ” 1.459  0.918  1.405  \n“Don’t Phunk With \nMy Heart ” 1.459  1.251  1.905  \n“I Wanna Love \nYou” 0.918  1.459 1.905  \n“Smack That ” 1.918  1.792  1.905  \n“When I’m Gone ” 1.251  0.918  1.448  \nTable 6 . Entropy values for hip-hop/ rap songs for the \nthree different categories. \n6. CONCLUSION \nOur experiment shows that the presence of lyrics has a \nsignificant effect on how people perceive songs.  To our \nsurprise, reading lyrics alongside listening to a song does \nnot significantly reduce the differences in music mood \nperception between Canadian and Chinese listeners. Also, \nwhile we included two different sets of Canadian listeners \n(Canadian-Chinese, and Canadians not of Chinese origin), \nwe can make no useful conclusions about the Chinese-\nCanadian group. \n     We do consistently see that presence of both audio and \nlyrics reduces the consistency of music mood judgment \nbetween Chinese and Canadian listeners . This phenom e-\nnon may be because of irony caused by negative words \npresented in proximity to upbeat beats, or it could be that \npresenting both audio and lyrics together might be a co m-\npletely different experience for the listener . This is an o b-\nvious setting for further work. \n     We have shown that the mood of a song depends on its \nexperiential context. Interestingly, songs where listeners \nagree strongly about the mood of the song when only li s-\ntening to the recording are often quite uncertain in their \nmood assignments when the lyrics are shown alongside \nthe recording.  Indeed, there is little correlation between \nthe entropy in mood assignment between the different \nways we presented songs to participants .   \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n7  \n \n     We also show that many “melancholy” lyrics are found \nin songs assigned to a more cheerful mood by listeners, \nagain suggesting that for such songs, the extent to which \nlisteners focus on the lyrics may influence how sad they \nview a song to be. We analyzed the mood assignments of \nparticipants on rock and hip-hop/rap songs. We see that \npeople tend to agree much more to the mood of a hip-\nhop/rap song when they are made to listen to the song . \nWe found that for rebellious/negative rock songs lyrics \nleads to more agreement in music mood but audio is be t-\nter for positive songs. In both the genres we found that \nhearing audio while reading lyrics lead to less agreement \non music mood of songs. \n     Our results suggest that music mood is so dependent \non cultural and experiential context to make it difficult to \nclaim it as a true concept. With the classification accuracy \nof mood classification systems reaching a plateau with no \nsignificant improvements we suggest that we need to r e-\ndefine the term “music mood” and change our a pproach \ntoward music mood classification problem. \n     A possible extension to our work could be running a \nsimilar study using a larger set of songs and more partic i-\npants, possibly from more diverse cultures than the ones \nwe studied. Future studies could focus on multi-modal \nmusic mood classification where a song could belong to \nmore than one mood, to see if even in this more robust \ndomain there is a stable way to assign songs to clusters of \nmoods when they are experienced in different contexts.  \nWe also wonder if other contextual experiments can show \nother effects about mood: for example, if hearing music \nwhile in a car or on public transit, or in stores, makes the \n“mood” of a song more uncertain.  \n     We fundamentally also wonder if “mood” as an MIR \nconcept needs to be reconsidered.  If listeners disagree \nmore or less about the mood of a song when it is presen t-\ned alongside its lyrics, that suggests a general uncertainty \nin the concept of “mood”. We leave more evidence gat h-\nering about this concept to future work as well. \n \n7. ACKNOWLEDGEMENTS \nOur research is supported by a grant from the Natural \nSciences and Engineering Research Council of Canada to \nDGB . \n8. REFERENCES \n[1] L. Lu , D. Liu, and H. Zhang, “Automatic mood dete c-\ntion and tracking of music audio signals ”, in IEEE  Tran s-\nactions on Audio, Speech, and Language Processing , \nvolume 14, number 1, pages 5- 18, 2006 .  \n \n[2] K. Trohidis, G. Tsoumakas, G. Kalliris and I. Vla h-\nvas, “Multi-label classification of music into emotions ”, \nin Proceedings of the 9th International Conference on \nMusic Information Retrieval (ISMIR ’08), pages 325-330, \n2008 . [3] X. Hu and J.S. Downie, “When lyrics outperform a u-\ndio for music mood classification: a feature analysis ”, in \nProceedings of the 11th International Conference on M u-\nsic Information Retrieval (ISMIR ’10), pages 1 –6, 2010. \n[4] H. He, J. Jin , Y. Xiong, B. Chen, W. Sun, and L. \nZhao, “Language feature mining for music emotion class i-\nfication via supervised learning from lyrics”, in Procee d-\nings of Advances in the 3rd International Symposium on \nComputation and Intelligence (ISICA ‘08), pages 426-\n435, 2008 . \n[5] Y. Hu , X. Chen and D. Yang, “Lyric-based song emo-\ntion detection with affective lexicon and fuzzy clustering \nmethod ”, in Proceedings of the 10th International Confe r-\nence on Music Information Retrieval (ISMIR ’09), pages \n123-128,  2009 . \n[6] X. Hu and J. S. Downie, “ Improving mood classific a-\ntion in music digital libraries by combining lyrics and a u-\ndio”, i n Proceedings of Joint Conference on Digital L i-\nbraries (JCDL), pages 159 –168, 2010 . \n \n[7] C. Laurier , J. Grivolla, and P. Herrera, “Multimodal \nmusic mood classification using audio and lyrics ”, in \nProceedings of the International Conference on Machine \nLearning and Applications (ICMLA ’08), pages 688-693, \n2008 . \n \n[8] X . Hu and J.H. Lee, “A cross -cultural study of music \nmood perception between American and Chinese liste n-\ners,” in Proceedings of International Society for Music \nInformation Retrieval (ISMIR ‘12), pages 535-540, 2012 . \n[9] Y.H. Yang, Y.C. Lin , H.T. Cheng , I.B. Liao, Y.C. Ho, \nand H.H. Chen, “ Toward multi-modal music emotion \nclassification ”, in Proceedings of Pacific Rim Conference \non Multimedia (PCM ’08), pages 70-79, 2008 . \n[10] X. Hu. and J.S. Downie, “Exploring mood metad ata: \nrelationships with genre, artist and usage metadata”,  in \nProceedings of the 8th International Conference on M u-\nsic Information Retrieval, (ISMIR '07),  pages 67-72,  \n2007.  \n[11] K. Kosta, Y. Song, G. Fazekas and M. Sandler , “A \nstudy of cultural dependence of perceived mood in Greek \nmusic ”, in Proceedings of the 14th International Society \nfor Music Information Retrieval Conference , (ISMIR ' 13), \npages 317-322, 2013 . \n \n[12] X. Hu, J . S. Downie, C. Laurier, M. Bay, and A. F. \nEhmann: “The 2007 MIREX audio mood classific ation \ntask: Lessons learned,”  in Proceedings of the 9th Intern a-\ntional Society for Music Information Retrieval Confe r-\nence, (ISMIR ' 08) , pages 462–467, 2008. \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n8"
    },
    {
        "title": "Are Poetry and Lyrics All That Different?",
        "author": [
            "Abhishek Singhi",
            "Daniel G. Brown 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417004",
        "url": "https://doi.org/10.5281/zenodo.1417004",
        "ee": "https://zenodo.org/records/1417004/files/SinghiB14a.pdf",
        "abstract": "Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made man- ually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes in- creasingly critical. This paper describes the LIA music recommendation system for advertisements using both tex- tual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable sound- track from a list of candidate songs, given a TV commer- cial. The organizers provide a development dataset includ- ing multimedia features. The initial assumption of the pro- posed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this sub- set, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm.",
        "zenodo_id": 1417004,
        "dblp_key": "conf/ismir/SinghiB14a",
        "keywords": [
            "commercial advertisements",
            "song illustration",
            "economic impact",
            "commercial video",
            "acoustic features",
            "MediaEval 2013 Soundtrack task",
            "textual features",
            "automatic system",
            "predict the most suitable soundtrack",
            "TV commercial"
        ],
        "content": "A COMBINED THEMATIC AND ACOUSTIC APPROACH FOR A MUSIC\nRECOMMENDATION SERVICE IN TV COMMERCIALS\nMohamed Morchid, Richard Dufour, Georges Linar `es\nLIA - University of Avignon (France)\nfmohamed.morchid, richard.dufour, georges.linaresg@univ-avignon.fr\nABSTRACT\nMost of modern advertisements contain a song to illustrate\nthe commercial message. The success of a product, and\nits economic impact, can be directly linked to this choice.\nFinding the most appropriate song is usually made man-\nually. Nonetheless, a single person is not able to listen\nand choose the best music among millions. The need for\nan automatic system for this particular task becomes in-\ncreasingly critical. This paper describes the LIA music\nrecommendation system for advertisements using both tex-\ntual and acoustic features. This system aims at providing\na song to a given commercial video and was evaluated in\nthe context of the MediaEval 2013 Soundtrack task [14].\nThe goal of this task is to predict the most suitable sound-\ntrack from a list of candidate songs, given a TV commer-\ncial. The organizers provide a development dataset includ-\ning multimedia features. The initial assumption of the pro-\nposed system is that commercials which sell the same type\nof product, should also share the same music rhythm. A\ntwo-fold system is proposed: ﬁnd commercials with close\nsubjects in order to determine the mean rhythm of this sub-\nset, and then extract, from the candidate songs, the music\nwhich better corresponds to this mean rhythm.\n1. INTRODUCTION\nThe success of a product or a service essentially depends\nof the way to present it. Thus, companies pay much at-\ntention to choose the most appropriate advertisement that\nwill make a difference in the customer choice. The ad-\nvertisers have different media possibilities, such as journal\npaper, radio, TV or Internet. In this context, they can ex-\nploit the audio media (TV , radio...) to attract listeners using\na song related to the commercial. The choice of an appro-\npriate song is crucial and can have a signiﬁcant economic\nimpact [5,18]. Usually, this choice is made by a human ex-\npert. Nonetheless, while millions of musics exist, a human\nagent could only choose a song among a limited subset.\nThis choice could then be inappropriate, or simply not the\nbest one, since the agent could not search into a large num-\nc\rMohamed Morchid, Richard Dufour, Georges Linar `es.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Mohamed Morchid, Richard Dufour,\nGeorges Linar `es. “A Combined Thematic and Acoustic Approach for a\nMusic Recommendation Service in TV Commercials”, 15th International\nSociety for Music Information Retrieval Conference, 2014.ber of musics. For these reasons, the need for an automatic\nsong recommandation system, to illustrate advertisements,\nbecomes a critical subject for companies.\nIn this paper, an automatic system for songs recomman-\ndation is proposed. The proposed approach combines both\ntextual (web pages) and audio (acoustic) features to select,\namong a large number of songs, the most appropriate and\nrelevant music knowing the commercial content. The ﬁrst\nstep of the proposed system is to represent commercials\ninto a thematic space built from a Latent Dirichlet Alloca-\ntion (LDA) [4]. This pre-processing subtask uses the re-\nlated textual content of the commercial. Then, acoustic\nfeatures of each song are extracted to ﬁnd a set of the most\nrelevant songs for a given commercial.\nAn appropriate benchmark is needed to evaluate the ef-\nfectiveness of the proposed recommandation system. For\nthese reasons, the proposed system is evaluated in the con-\ntext of the challenging MediaEval 2013 Soundtrack task\nfor commercials [10]. Indeed, the MusiClef task seeks to\nmake this process automated by taking into account both\ncontext- and content-based information about the video,\nthe brand, and the music. The main difﬁculty of this task\nis to ﬁnd the set of relevant features that best describes the\nmost appropriate song for a video.\nNext section describes related work in topic space mod-\neling for information retrieval and music tasks. Section 3\npresents the proposed music recommandation system us-\ning both textual content and acoustic features related to\nmusics from commercials. Section 4 explains in details\nthe unsupervised Latent Dirichlet Allocation (LDA) tech-\nnique, while Section 4.2 describes how the acoustic fea-\ntures are used to evaluate the proximity of a music to a\ncommercial. Finally, experiments are presented in Sec-\ntion 5, while Section 6 gives conclusions and perspectives.\n2. RELATED WORKS\nLatent Dirichlet Allocation (LDA) [4] is widely used in\nseveral tasks of information retrieval such as classiﬁca-\ntion or keywords extraction. However, this unsupervised\nmethod is not much considered in the music processing\ntasks. Next sections describe related works using LDA\ntechniques with text corpora (Section 2.1) and in the con-\ntext of music tasks (Section 2.3).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4652.1 Topic modeling\nSeveral methods were proposed by Information Retrieval\n(IR) researchers to build topic spaces such as Latent Se-\nmantic Analysis or Indexing (LSA/LSI) [2, 6], that use a\nsingular value decomposition (SVD) to reduce the space\ndimension.\nThis method was improved by [11] which proposed a\nprobabilistic LSA/LSI (pLSA/pLSI). The pLSI approach\nmodels each word in a document as a sample from a mix-\nture model, where the mixture components are multino-\nmial random variables that can be viewed as representa-\ntions of topics. This method demonstrated its performance\non various tasks, such as sentence [3] or keyword [24] ex-\ntraction. In spite of the effectiveness of the pLSI approach,\nthis method has two main drawbacks. The distribution of\ntopics in pLSI is indexed by training documents. Thus, the\nnumber of these parameters grows with the training doc-\nument set size, and then, the model is prone to overﬁtting\nwhich is a main issue in an IR task such as document clus-\ntering. However, to address this shortcoming, a tempering\nheuristic is used to smooth the parameter of pLSI model for\nacceptable predictive performance. Nonetheless, authors\nshowed in [20] that overﬁtting can occur even if tempering\nprocess is used.\nAs a result, IR researchers proposed the Latent Dirichlet\nallocation (LDA) [4] method to overcome these two draw-\nbacks. Thus, the number of parameters of LDA does not\ngrow with the size of the training corpus and LDA is not\ncandidate for overﬁtting. LDA is a generative model which\nconsiders a document, seen as a bag-of-words [21], as a\nmixture of latent topics. In opposition to a multinomial\nmixture model, LDA considers that a theme is associated\nto each occurrence of a word composing the document,\nrather than associate a topic with the complete document.\nThereby, a document can change of topics from a word to\nanother. However, the word occurrences are connected by\na latent variable which controls the global respect of the\ndistribution of the topics in the document. These latent\ntopics are characterized by a distribution of word proba-\nbilities which are associated with them. pLSI and LDA\nmodels have been shown to generally outperform LSI on\nIR tasks [12]. Moreover, LDA provides a direct estimate\nof the relevance of a topic knowing a word set or a docu-\nment such as a web pages in the proposed system.\nα θ z wβ φ\nword topic N\nDtopic\ndistributionword\ndistribution\nFigure 1. LDA Formalism.\nFigure 1 presents the LDA formalism. For every docu-mentdof a corpus D, a ﬁrst parameter \u0012is drawn according\nto a Dirichlet law of parameter \u000b. A second parameter \u001e\nis drawn according to the same Dirichlet law of parameter\n\f. Then, to generate every word wof the document d, a\nlatent topiczis drawn from a multinomial distribution on\n\u0012. Knowing this topic z, the distribution of the words is\na multinomial of parameters \u001e. The parameter \u0012is drawn\nfor all the documents from the same prior parameter\u000b.\nThis allows to obtain a parameter binding the documents\nall together [4].\n2.2 Gibbs sampling\nSeveral techniques have been proposed to estimate LDA\nparameters, such as Variational Methods [4], Expectation-\nPropagation [17] or Gibbs Sampling [8]. Gibbs Sampling\nis a special case of Markov-chain Monte Carlo (MCMC) [7]\nand gives a simple algorithm for approximate inference\nin high-dimensional models such as LDA [9]. This over-\ncomes the difﬁculty to directly and exactly estimate param-\neters that maximize the likelihood of the whole data col-\nlection deﬁned as: P(Wj\u0000 !\u000b;\u0000 !\f) =QM\nm=1P(\u0000 !wmj\u0000 !\u000b;\u0000 !\f)\nfor the whole data collection W=f\u0000 !wmgM\nm=1 knowing\nthe Dirichlet parameters\u0000 !\u000band\u0000 !\f.\nThe ﬁrst use of Gibbs Sampling for estimating LDA is\nreported in [8] and a more comprehensive description of\nthis method can be found in [9]. One can refer to these pa-\npers for a better understanding of this sampling technique.\n2.3 Topic modeling and Music\nTopic modeling was already used in music processing, such\nas [13], where the authors presented a system which learns\nmusical key as a key-proﬁle. Thus, the proposed approach\nconsidered a song as a random mixture of key-proﬁles.\nIn [25], authors described a classiﬁcation method to assign\na label to an unseen music. The authors use LDA to build\na topic space from music-tags to get the probability of ev-\nery music-tag belonging to each music genre. Then, each\nmusic is labeled to a genre knowing its tags. The purpose\nof the proposed approach is to ﬁnd a set of relevant musics\nfor a TV commercial.\n3. PROPOSED APPROACH\nThe goal of the proposed automatic system is to recom-\nmend a set of musics given a TV commercial. The sys-\ntem uses external knowledge to ﬁnd these songs. These\nexternal resources are composed with a set of TV commer-\ncials associated, for each one, with a song and a set of web\npages (see [14] for more details about the MediaEval 2013\nSoundtrack task). The idea behind the proposed approach\nis to assume that two commercials sharing same subjects or\ninterests, also share the same kind of songs. The main issue\nin this approach is to ﬁnd commercials, from the external\ndataset, that have sets of subjects close to those in commer-\ncials from the test set. As described in Section 2.1, a doc-\nument can be represented as a set of latent topics. Thus,\ntwo documents sharing the same topics could be seen as\nthematically close.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n466{C ,S  }d∈Ddd{C ,S  }t∈Tt1TopicSpaceLDADevelopment setTopic vectors{V  }d∈DdTopic vectorV1MappingV  dV  1cos (α    )  -1d,1\nl commercial fromdevelopment set D{C ,S }with the highestsimilarity with Cfrom test set Tll1SS  tcos (α    )  -1l,tSA TV commercial C and the candidate songs S from test set T1t\n5 nearest soundtracks{S }with the commercial C1tt=1, ... , 5Cosine similarity α\nCosine similarity αMean rhythm patternFigure 2. Global architecture of the proposed system.\nBasically, the ﬁrst process of the proposed three step\nsystem is to map each TV commercial from the test and\ndevelopment sets, into a topic space learnt with a LDA al-\ngorithm. A TV commercial from the test set is then linked\nto TV commercials from development set sharing a set of\nclose topics. Moreover, each commercial of the develop-\nment set is related to a music. Thus, as a result, a commer-\ncial from the test set is related to a subset of songs from\nthe development set, considered as thematically close to\nthe commercial textual content.\nThe second step has the responsibility to estimate a list\nof candidate songs (see Figure 2) using song audio features\nfrom the subset of songs thematically close associated dur-\ning the ﬁrst step. This subset of songs is used to evaluate a\nrhythm pattern of the ideal song for this commercial.\nThe last step retrieves, from all candidate songs from\nthe test set, the closest song to the rhythm pattern estimated\nduring the previous step.\nIn details, the development set Dis composed of TV\ncommercials Cd, with for each, a soundtrack Sdand a vec-\ntor representation Vdrelated to the dthTV commercial. In\nthe same manner, the test set Tis composed of TV com-\nmercialsCt, with, for the tthone, a vector representation\nVtand a soundtrack Stto predict. Then a similarity score\nf\u000bd;tgt=1;:::; T\nd=1;:::;Dis computed for each commercial Cd\niof the\ndevelopment set given one from the test set Ct:\nD=fCd;VD;Sdgd=1;:::;D (1)\nT=fCt;VT;St\nkgk=1;:::;5000\nt=1;:::;T:\nIn the next sections, the topic space representation andthe mapping of a commercial in this topic representation\nto evaluate both VdandVtare described. Then, the com-\nputed similarity score is detailed. Finally, the soundtrack\nprediction process from a TV commercial is explained.\n4. TOPIC REPRESENTATION OF A TV\nCOMMERCIAL\nLet’s consider a corpus Dfrom the development set of TV\ncommercials with a word vocabulary V=fw1;:::;w Ng\nof sizeN. A topic representation from corpus Dis then\nperformed using a Latent Dirichlet Allocation (LDA) [4]\napproach. At the ﬁnal LDA analysis, a topic space mofn\ntopics is obtained with, for each theme z, the probability\nof each word wofVknowingz, and for the entire model\nm, the probability of each theme zknowing the model m.\nEach TV commercial from both development and test sets\nis mapped into the topic space (see Figure 3) to obtain a\nvector representation (VdandVt) of web pages related to\na commercial into the thematic space computed as follow:\nVd[i](Cd\nj) =P(zijCd\nj) (2)\nwhereP(zijCd\nj)is the probability of a topic zito be\ngenerated by the web pages from the commercial Cd\nj, esti-\nmated using Gibbs sampling as described in Section 2.2. In\nthe same way, Vtis estimated with the same topic space,\nand with the use of web pages of commercials of test set\nCt\nj(see Figure 3).\nTV Commercialz3\n. . .P(w1|z3)WORD WEIGHT\nw1\nP(w2|z3) w2\nP(w|V||z3) w|V|. . .z2\nP(w1|z2)WORD WEIGHT\nw1\nP(w2|z2) w2\nP(w|V||z2) w|V|. . .z1\nP(w1|z1)WORD WEIGHT\nw1\nP(w2|z1) w2\nP(w|V||z1) w|V|. . .\nz4\nP(w1|z4)WORD WEIGHT\nw1\nP(w2|z4) w2\nP(w|V||z4) w|V|. . .\nzn\nP(w1|zn)WORD WEIGHT\nw1\nP(w2|zn) w2\nP(w|V||zn) w|V|. . . . . .Vd[1]Vd[2]\nVd[3]\nVd[4]\nVd[n]\nFigure 3. Mapping of a TV commercial in the topic space.\n4.1 Similarity measure\nEach commercial from both development and test set, is\nmapped into the topic space to produce a vector represen-\ntation for each one, respectively VdandVtas outcomes.\nThen, given a TV commercial C1from the test set T, a\nsubset of other TV commercials from the development set\nDis selected knowing their thematic proximity with C1.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n467To estimate the similarity between C1and commercials\nfrom development set, the cosine metric \u000bis used. This\nsimilarity metric is expressed thereafter:\ncosine(Vd;Vt) =\u000bd;t\n=nP\ni=1Vd[i]\u0002Vt[i]\ns\nnP\ni=1Vd[i]2s\nnP\ni=1Vt[i]2(3)\nThis metric allows to extract a subset of commercials\nfrom Dthematically close to C1.\n4.2 Rhythm pattern\nThe cosine measure, presented in previous section, is also\nused to evaluate the similarity between a mean rhythm pat-\ntern vectorSdof a song, and all the candidate songs St\nkof\nthe test set.\n<?xml version=\"1.0\" ?><rhythmdescription>    <media>363445_sum.wav</media>    <description>        <bpm_mean>99.982723</bpm_mean>        <bpm_std>0.047869</bpm_std>        <meter>22.000000</meter>        <perc>47.023527</perc>        <perc_norm>1.910985</perc_norm>        <complex>29.630575</complex>        <complex_norm>0.652134</complex_norm>        <speed>2.660229</speed>        <speed_norm>1.201633</speed_norm>        <periodicity>0.900763</periodicity>        <rhythmpattern>0.124231 ... 0.098873</rhythmpattern>    </description></rhythmdescription>bmp_meanbmp_stdmeterpercperc_normcomplexcomplex_normspeedspeed_normpeiodicityrhythmpattern_1rhythmpattern_2...rhythmpattern_48{Rhythm pattern vectorRhythm pattern of a song\n(a)(b)\nFigure 4. Rhythm pattern of a song from the development\nset in xml (a) and vector (b) representations.\nIn details, each commercial from Dis related with a\nsoundtrack that is represented with a rhythm pattern vector.\nThe organizers provide for each song contained into the\nMusicClef 2013 dataset:\n\u000fvideo features (MPEG-7 Motion Activity and Scal-\nable Color Descriptor [15]),\n\u000fweb pages about the respective brands and music\nartists,\n\u000fmusic features:\n\u0000MFCC or BLF [22],\n\u0000PS209 [19],\n\u0000beat, key, harmonic pattern extracted with the\nIrcam software [1].\nIn our experiments, 10 rhythm features of songs are\nused (speed, percussion, . . . ,periodicity) as shown in Fig-\nure 4. These features of beat, key or harmonic pattern areextracted using the Ircam software available at [1]. More\ninformation about features extraction from songs are de-\ntailed in [14].\nAs an outcome, each commercial is represented by a\nrhythm pattern vector of size 58 (10 from song features and\n48 from rhythm pattern). From the subset of soundtracks\nof thelnearest commercials from D, a mean rhythm vector\nSis performed as:\nS=1\nlX\nd2lSd:\nFinally, the cosine measure between this mean rhythm\nSof thelnearest commercials from D, and each commer-\ncial (cosine( S;St)t2T), is used to ﬁnd, from the sound-\ntrackStof the test set T, the 5songs from all the candi-\ndates having the closest rhythm pattern.\n5. EXPERIMENTS AND RESULTS\nPrevious sections described the proposed automatic music\nrecommandation system for TV commercials. This sys-\ntem is decomposed into three sub-processes. The ﬁrst one\nmaps the commercials into a topic space to evaluate the\nproximity of a commercial from the test set and all com-\nmercials from the development set. Then, the mean rhythm\npattern of the thematically close commercials is computed.\nFinally, this rhythm pattern is computed with all ones from\nthe test set of candidate songs to ﬁnd a set of relevant mu-\nsics.\n5.1 Experimental protocol\nThe ﬁrst step of the proposed approach, detailed in pre-\nvious section, maps TV commercial textual content into a\ntopic space of size n(n= 500). This one is learnt from a\nLDA in a large corpus of documents. Section 4 describes\nthe corpus Dof web pages. This corpus contains 10;724\nWeb pages related to brands of the commercials contained\ninD. This corpus is composed of 44;229;747words for\na vocabulary of 4;476;153 unique words. More details\nabout this text corpus, and the way to collect it, is explained\ninto [14].\nThe ﬁrst step of the proposed approach is to map each\ncommercial textual content into a topic space learnt from a\nlatent Dirichlet allocation (LDA). During the experiments,\nthe MALLET tool is used [16] to perform a topic model.\nThe proposed system is evaluated in the MediaEval 2013\nMusiClef benchmark [14]. The aim of this task is to pre-\ndict, for each video of the test set, the most suitable sound-\ntrack from 5,000 candidate songs. The dataset is split into\n3 sets. The development set contains multimodal infor-\nmation on 392 commercials (various metadata including\nYoutube uploader comments, audio features, video fea-\ntures, web pages and text features). The test set is a set\nof 55 videos to which a song should be associated using\nthe recommandation set of 5,000 soundtracks (30 seconds\nlong excerpts).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4685.2 Experimental metrics\nFor each video in the test set, a ranked list of 5 candidate\nsongs should be proposed. The song prediction evaluation\nis manually performed using the Amazon Mechanical Turk\nplatform. This novel task is non-trivial in terms of “ground\ntruth”, that is why human ratings for evaluation are used.\nThree scores have been computed from our system output.\nLetVbe the full collection of test set videos, and let sr(v)\nbe the average suitability score for the audio ﬁle suggested\nat rankrfor the video v. Then, the evaluation measures\nare computed as follows:\n\u000fAverage suitability score of the ﬁrst-ranked song:\n1\nVjVjP\ni=1s1(vi)\n\u000fAverage suitability score for the full top-5:\n1\nVjVjP\ni=11\n5sr(vi)\n\u000fWeighted average suitability score of the full top-\n5. Here, we apply a weighted harmonic mean score\ninstead of an arithmetic mean:\n1\nVjVjP\ni=1P5\nr=1sr(vi)\nP5\nr=1sr(vi)\nr\nThe previously presented measures are used to study\nboth rating and ranking aspects of the results.\n5.3 Results\nThe measures deﬁned in the previous section are used to\nevaluate the effectiveness of songs selected to be associ-\nated to TV commercials from the test set. The proposed\ntopic space-based approach is evaluated in the same way,\nand obtained the results detailed thereafter:\n\u000fFirst rank average score: 2.16\n\u000fTop 5 average score (arithmetic mean): 2.24\n\u000fTop 5 average score (harmonic mean, taking rank\ninto account): 2.22\nConsidering that human judges rate the predicted songs\nfrom 1 (very poor) to 4 (very well), we can consider that\nour system is slightly better than the mean evaluation score\n(2) no matter the metric considered. While the system\nproposed in [23] is clearly different from ours, results are\nvery similar. This shows the difﬁculty to build an auto-\nmatic song recommendation system for TV commercials,\nthe evaluation being also a critical point to discuss.\n6. CONCLUSIONS AND PERSPECTIVES\nIn this paper, an automatic system to assign a soundtrack\nto a TV commercial has been proposed. This system com-\nbines two media: textual commercial content and audio\nrhythm pattern. The proposed approach obtains good re-\nsults in spite of the fact that the system is automatic and un-\nsupervised. Indeed, both subtasks are unsupervised (LDA\nlearning and commercials mapping into the topic space)and songs extraction (rhythm pattern estimation of the ideal\nsongs for a commercial from the test set). Moreover, this\npromising approach, combining thematic representation of\nthe textual content of a set of web pages describing a TV\ncommercial and acoustic features, shows the relevance of\ntopic-based representation in automatic recommandation\nusing external resources (development set).\nThe choice of a relevant song to describe the idea behind\na commercial, is a challenging task when the framework\ndoes not take into account relevant features related to:\n\u000fmood, such as harmonic content, harmonic progres-\nsions and timbre,\n\u000fmusic rhythm, such as musical style, texture, spec-\ntral centroid, or tempo.\nThe proposed automatic music recommendation system\nis limited by this small number (58) of features which not\ndescribe all music aspects. For these reasons, in future\nworks, we plan to use others features, such as the song\nlyrics or the audio transcription of the TV commercials,\nand evaluate the effectiveness of the proposed hybrid frame-\nwork into other information retrieval tasks such as classiﬁ-\ncation of music genre or music clustering.\n7. REFERENCES\n[1] Ircam. analyse-synthse: Software. In\nhttp://anasynth.ircam.fr/home/software., Accessed:\nSept. 2013.\n[2] J.R. Bellegarda. A latent semantic analysis framework\nfor large-span language modeling. In Fifth European\nConference on Speech Communication and Technol-\nogy, 1997.\n[3] J.R. Bellegarda. Exploiting latent semantic informa-\ntion in statistical language modeling. Proceedings of\nthe IEEE, 88(8):1279–1296, 2000.\n[4] D.M. Blei, A.Y . Ng, and M.I. Jordan. Latent dirichlet\nallocation. The Journal of Machine Learning Research,\n3:993–1022, 2003.\n[5] Claudia Bullerjahn. The effectiveness of music in tele-\nvision commercials. Food Preferences and Taste: Con-\ntinuity and Change, 2:207, 1997.\n[6] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-\ndauer, and R. Harshman. Indexing by latent semantic\nanalysis. Journal of the American society for informa-\ntion science, 41(6):391–407, 1990.\n[7] Stuart Geman and Donald Geman. Stochastic relax-\nation, gibbs distributions, and the bayesian restoration\nof images. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, (6):721–741, 1984.\n[8] Thomas L Grifﬁths and Mark Steyvers. Finding sci-\nentiﬁc topics. Proceedings of the National academy of\nSciences of the United States of America , 101(Suppl\n1):5228–5235, 2004.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n469[9] Gregor Heinrich. Parameter estimation for text anal-\nysis. Web: http://www. arbylon. net/publications/text-\nest. pdf, 2005.\n[10] Nina Hoeberichts. Music and advertising: The effect of\nmusic in television commercials on consumer attitudes.\nBachelor Thesis, 2012.\n[11] T. Hofmann. Probabilistic latent semantic analysis. In\nProc. of Uncertainty in Artiﬁcial Intelligence, UAI ’ 99,\npage 21. Citeseer, 1999.\n[12] Thomas Hofmann. Unsupervised learning by proba-\nbilistic latent semantic analysis. Machine Learning,\n42(1):177–196, 2001.\n[13] Diane Hu and Lawrence K Saul. A probabilistic\ntopic model for unsupervised learning of musical key-\nproﬁles. In ISMIR, pages 441–446, 2009.\n[14] Cynthia C. S. Liem, Nicola Orio, Geoffroy Peeters, and\nMarkus Scheld. MusiClef 2013: Soundtrack Selection\nfor Commercials. In MediaEval, 2013.\n[15] Bangalore S Manjunath, Philippe Salembier, and\nThomas Sikora. Introduction to MPEG-7: multimedia\ncontent description interface, volume 1. John Wiley &\nSons, 2002.\n[16] Andrew Kachites McCallum. Mallet: A machine learn-\ning for language toolkit. http://mallet.cs.umass.edu,\n2002.\n[17] Thomas Minka and John Lafferty. Expectation-\npropagation for the generative aspect model. In Pro-\nceedings of the Eighteenth conference on Uncertainty\nin artiﬁcial intelligence, pages 352–359. Morgan Kauf-\nmann Publishers Inc., 2002.\n[18] C Whan Park and S Mark Young. Consumer response\nto television commercials: The impact of involvement\nand background music on brand attitude formation.\nJournal of Marketing Research, pages 11–24, 1986.\n[19] Tim Pohle, Dominik Schnitzer, Markus Schedl, Peter\nKnees, and Gerhard Widmer. On rhythm and general\nmusic similarity. In ISMIR, pages 525–530, 2009.\n[20] Alexandrin Popescul, David M Pennock, and Steve\nLawrence. Probabilistic models for uniﬁed collabora-\ntive and content-based recommendation in sparse-data\nenvironments. In Proceedings of the Seventeenth con-\nference on Uncertainty in artiﬁcial intelligence, pages\n437–444. Morgan Kaufmann Publishers Inc., 2001.\n[21] G. Salton. Automatic text processing: the transforma-\ntion. Analysis and Retrieval of Information by Com-\nputer, 1989.\n[22] Klaus Seyerlehner, Gerhard Widmer, and Tim Pohle.\nFusing block-level features for music similarity esti-\nmation. In Proc. of the 13th Int. Conference on Digital\nAudio Effects (DAFx-10), pages 225–232, 2010.[23] Han Su, Fang-Fei Kuo, Chu-Hsiang Chiu, Yen-Ju\nChou, and Man-Kwan Shan. Mediaeval 2013: Sound-\ntrack selection for commercials based on content cor-\nrelation modeling. In MediaEval 2013, volume 1043 of\nCEUR Workshop Proceedings. CEUR-WS.org, 2013.\n[24] Y . Suzuki, F. Fukumoto, and Y . Sekiguchi. Keyword\nextraction using term-domain interdependence for dic-\ntation of radio news. In 17th international conference\non Computational linguistics, volume 2, pages 1272–\n1276. ACL, 1998.\n[25] Chao Zhen and Jieping Xu. Multi-modal music genre\nclassiﬁcation approach. In Computer Science and In-\nformation Technology (ICCSIT), 2010 3rd IEEE In-\nternational Conference on, volume 8, pages 398–402.\nIEEE, 2010.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n470"
    },
    {
        "title": "Panako - A Scalable Acoustic Fingerprinting System Handling Time-Scale and Pitch Modification.",
        "author": [
            "Joren Six",
            "Marc Leman"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.7185611",
        "url": "https://doi.org/10.5281/zenodo.7185611",
        "ee": "https://zenodo.org/records/7185611/files/Panako-joss.zip",
        "abstract": "A snapshot of the Panako acoustic fingerprinting system as described in the paper titled &#39;Panako: a scalable audio search system&#39; in the Journal of Open Source Software.\n\nPanako solves the problem of finding short audio fragments in large digital audio archives.The content based audio search algorithm implemented in Panako is able to identify a short audio query in a large database of thousands of hours of audio using an acoustic fingerprinting technique.",
        "zenodo_id": 7185611,
        "dblp_key": "conf/ismir/SixL14",
        "keywords": [
            "Panako",
            "audio search system",
            "audio fragments",
            "digital audio archives",
            "content based",
            "audio search algorithm",
            "acoustic fingerprinting",
            "large database",
            "thousands of hours",
            "audio query"
        ]
    },
    {
        "title": "Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera.",
        "author": [
            "Ajay Srinivasamurthy",
            "Rafael Caro Repetto",
            "Sundar Harshavardhan",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1285593",
        "url": "https://doi.org/10.5281/zenodo.1285593",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T078_134_Paper.pdf",
        "abstract": "The Beijing Opera Percussion Pattern (BOPP) dataset is a collection of audio examples of percussion patterns played by the percussion ensemble in Beijing Opera (Jingju, 京剧). The percussion ensemble in Jingju plays a set of pre-defined and labeled percussion patterns, which serve many functions.The percussion patterns can be defined as sequences of strokes played by different combinations of the percussion instruments, and the resulting variety of timbres are transmitted using oral syllables as mnemonics. More information on the percussion instruments used in Beijing Opera can be found at http://compmusic.upf.edu/examples-percussion-bo.\n\nThe dataset presented here was used as the training dataset in the referenced paper. A detailed description of percussion patterns inJingjucan also be found in it.\n\nDATASET\n\nThe dataset is a collection of 133 audio percussion patterns spanning five different pattern classes as described below. The scores for the patterns and additional details about the patterns are at:http://compmusic.upf.edu/bo-perc-patterns\n\nAudio Content\n\nThe audio files are short segments containing one of the above mentioned patterns. The audio is stereo, sampled at 44.1 kHz, and stored as wav files. The segments were chosen from the introductory parts of arias. The recordings of arias are from commercially available releases spanning various artists. The audio and segments were chosen carefully by a musicologist to be representative of the percussion patterns that occur inJingju. The audio segments contain diverse instrument timbres of percussion instruments (though the same set of instruments are played, there can be slight variations in the individual instruments across different ensembles), recording quality and period of the recording. Though these recordings were chosen from introductions of arias where only percussion ensemble is playing, there are some examples in the dataset where the melodic accompaniment starts before the percussion pattern ends.\n\nAnnotations\n\nEach of the audio patterns has an associated syllable level transcription of the audio pattern. The transcription is obtained from the score for the pattern and is not time aligned to the audio. The transcription is done using a reduced set of five syllables and is sufficient to computationally model the timbres of all the syllables. The annotations are stored as Hidden Markov Model Toolkit (HTK) label files. There is also a single master label file provided for batch processing using HTK (http://htk.eng.cam.ac.uk/).\n\nDataset organization\n\nThe dataset has wav files and label files. The files are named as\n\npIDInstID.extension\n\nThe pID is as in Table 1, instID is a three digit identifier for the specific instance of the pattern, and extension can be .wav for the audio file or .lab for the label file. pID ϵ{10, 11, 12, 13, 14}, InstID ϵ{1, 2, ..., NpID}. e.g. The audio file and the label file for the fifth instance of the pattern duotuo is named 12005.wav and 12005.lab, respectively. The master label file is called masterLabels.lab\n\nUsing this dataset\n\nIf you use the dataset in your work, please cite the following publication:\n\n\nAjay Srinivasamurthy, Rafael Caro Repetto, Harshavardhan Sundar, Xavier Serra, Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera, in Proceedings of the 15th International Society for Music Information Retrieval (ISMIR) Conference, Taipei, Taiwan, Oct 2014.\n\n\nhttp://hdl.handle.net/10230/25677\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nCONTACT\n\nIf you have any questions or comments about the dataset, please feel free to write to us.\n\nAjay Srinivasamurthy (ajays.murthy@upf.edu)\n\nRafael Caro Repetto (rafael.caro@upf.edu)\n\n\n\nhttp://compmusic.upf.edu/bopp-dataset",
        "zenodo_id": 1285593,
        "dblp_key": "conf/ismir/SrinivasamurthyRHS14",
        "keywords": [
            "Beijing Opera",
            "Percussion Patterns",
            "Jingju",
            "Dataset",
            "Timbres",
            "Oral Mnemonics",
            "Audio Examples",
            "Pattern Classes",
            "CompMusic",
            "Percussion Instruments"
        ]
    },
    {
        "title": "Classifying EEG Recordings of Rhythm Perception.",
        "author": [
            "Sebastian Stober",
            "Daniel J. Cameron",
            "Jessica A. Grahn"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415734",
        "url": "https://doi.org/10.5281/zenodo.1415734",
        "ee": "https://zenodo.org/records/1415734/files/StoberCG14.pdf",
        "abstract": "Electroencephalography (EEG) recordings of rhythm percep- tion might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms – half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme – were each repeated for 32 sec- onds. During presentation, the participants’ brain waves were recorded via 14 EEG channels. We applied stacked denois- ing autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we in- vestigated how far these techniques can be used to recognize the individual rhythms.",
        "zenodo_id": 1415734,
        "dblp_key": "conf/ismir/StoberCG14",
        "keywords": [
            "Electroencephalography",
            "EEG recordings",
            "rhythm perception",
            "distinguish different rhythm types",
            "identify rhythms",
            "deep learning techniques",
            "adults",
            "rhythmic tone sequences",
            "East African music",
            "Western music"
        ],
        "content": "CLASSIFYING EEG RECORDINGS OF RHYTHM PERCEPTION\nSebastian Stober, Daniel J. Cameron and Jessica A. Grahn\nBrain and Mind Institute, Department of Psychology, Western University, London, ON, Canada\nfsstober,dcamer25,jgrahn g@uwo.ca\nABSTRACT\nElectroencephalography (EEG) recordings of rhythm percep-\ntion might contain enough information to distinguish different\nrhythm types/genres or even identify the rhythms themselves.\nIn this paper, we present first classification results using deep\nlearning techniques on EEG data recorded within a rhythm\nperception study in Kigali, Rwanda. We tested 13 adults,\nmean age 21, who performed three behavioral tasks using\nrhythmic tone sequences derived from either East African\nor Western music. For the EEG testing, 24 rhythms – half\nEast African and half Western with identical tempo and based\non a 2-bar 12/8 scheme – were each repeated for 32 sec-\nonds. During presentation, the participants’ brain waves were\nrecorded via 14 EEG channels. We applied stacked denois-\ning autoencoders and convolutional neural networks on the\ncollected data to distinguish African and Western rhythms on\na group and individual participant level. Furthermore, we in-\nvestigated how far these techniques can be used to recognize\nthe individual rhythms.\n1. INTRODUCTION\nMusical rhythm occurs in all human societies and is related to\nmany phenomena, such as the perception of a regular empha-\nsis (i.e., beat), and the impulse to move one’s body. However,\nthe brain mechanisms underlying musical rhythm are not\nfully understood. Moreover, musical rhythm is a universal\nhuman phenomenon, but differs between human cultures, and\nthe influence of culture on the processing of rhythm in the\nbrain is uncharacterized.\nIn order to study the influence of culture on rhythm pro-\ncessing, we recruited participants in East Africa and Canada\nto test their ability to perceive and produce rhythms derived\nfrom East African and Western music. Besides behavioral\ntasks, which have already been discussed in [4], the East\nAfrican participants also underwent electroencephalography\n(EEG) recording while listening to East African and Western\nmusical rhythms thus enabling us to study the neural mech-\nanisms underlying rhythm perception. We were interested\nin differences between neuronal entrainment to the periodic-\nities in East African versus Western rhythms for participants\nfrom those respective cultures. Entrainment was defined as\nc\rSebastian Stober, Daniel J. Cameron and Jessica A. Grahn.\nLicensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Sebastian Stober, Daniel J. Cameron and Jessica\nA. Grahn. “Classifying EEG Recordings of Rhythm Perception”, 15th\nInternational Society for Music Information Retrieval Conference, 2014.the magnitudes of steady state evoked potentials ( SSEP s) at\nfrequencies related to the metrical structure of rhythms. A\nsimilar approach has been used previously to study entrain-\nment to rhythms [17,18].\nBut it is also possible to look at the collected EEG data\nfrom an information retrieval perspective by asking questions\nlikeHow well can we tell from the EEG whether a participant\nlistened to an East African or Western rhythm? orCan we\neven say from a few seconds of EEG data which rhythm some-\nbody listened to? Note that answering such question does\nnot necessarily require an understanding of the underlying\nprocesses. Hence, we have attempted to let a machine figure\nout how best to represent and classify the EEG recordings\nemploying recently developed deep learning techniques. In\nthe following, we will review related work in Section 2 , de-\nscribe the data acquisition and pre-processing in Section 3\npresent our experimental findings in Section 4 , and discuss\nfurther steps in Section 5 .\n2. RELATED WORK\nPrevious research demonstrates that culture influences per-\nception of the metrical structure (the temporal structure of\nstrong and weak positions in rhythms) of musical rhythms\nin infants [20] and in adults [16]. However, few studies have\ninvestigated differences in brain responses underlying the cul-\ntural influence on rhythm perception. One study found that\nparticipants performed better on a recall task for culturally fa-\nmiliar compared to unfamiliar music, yet found no influence\nof cultural familiarity on neural activations while listening to\nthe music while undergoing functional magnetic resonance\nimaging (fMRI) [15].\nMany studies have used EEG and magnoencephalogra-\nphy ( MEG ) to investigate brain responses to auditory rhythms.\nOscillatory neural activity in the gamma (20-60 Hz) frequency\nband is sensitive to accented tones in a rhythmic sequence and\nanticipates isochronous tones [19]. Oscillations in the beta\n(20-30 Hz) band increase in anticipation of strong tones in a\nnon-isochronous sequence [5,6,10]. Another approach has\nmeasured the magnitude of SSEP s (reflecting neural oscilla-\ntions entrained to the stimulus) while listening to rhythmic\nsequences [17,18]. Here, enhancement of SSEPs was found\nfor frequencies related to the metrical structure of the rhythm\n(e.g., the frequency of the beat).\nIn contrast to these studies investigating the oscillatory ac-\ntivity in the brain, other studies have used EEG to investigate\nevent-related potentials ( ERPs) in responses to tones occur-\nring in rhythmic sequences. This approach has been used to\nshow distinct sensitivity to perturbations of the rhythmic pat-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n649tern vs. the metrical structure in rhythmic sequences [7], and\nto suggest that similar responses persist even when attention\nis diverted away from the rhythmic stimulus [12].\nIn the field of music information retrieval ( MIR), retrieval\nbased on brain wave recordings is still a very young and un-\nexplored domain. So far, research has mainly focused on\nemotion recognition from EEG recordings (e.g., [3,14]). For\nrhythms, however, Vlek et al. [23] already showed that imag-\nined auditory accents can be recognized from EEG. They\nasked ten subjects to listen to and later imagine three sim-\nple metric patterns of two, three and four beats on top of a\nsteady metronome click. Using logistic regression to clas-\nsify accented versus unaccented beats, they obtained an av-\nerage single-trial accuracy of 70% for perception and 61%\nfor imagery. These results are very encouraging to further\ninvestigate the possibilities for retrieving information about\nthe perceived rhythm from EEG recordings.\nIn the field of deep learning, there has been a recent in-\ncrease of works involving music data. However, MIR is\nstill largely under-represented here. To our knowledge, no\nprior work has been published yet on using deep learning\nto analyze EEG recordings related to music perception and\ncognition. However, there are some first attempts to process\nEEG recordings with deep learning techniques.\nWulsin et al. [24] used deep belief nets ( DBN s) to de-\ntect anomalies related to epilepsy in EEG recordings of 11\nsubjects by classifying individual “channel-seconds”, i.e., one-\nsecond chunks from a single EEG channel without further\ninformation from other channels or about prior values. Their\nclassifier was first pre-trained layer by layer as an autoencoder\non unlabelled data, followed by a supervised fine-tuning with\nbackpropagation on a much smaller labeled data set. They\nfound that working on raw, unprocessed data (sampled at\n256Hz) led to a classification accuracy comparable to hand-\ncrafted features.\nLangkvist et al. [13] similarly employed DBN s combined\nwith a hidden Markov model ( HMM ) to classify different\nsleep stages. Their data for 25 subjects comprises EEG as\nwell as recordings of eye movements and skeletal muscle ac-\ntivity. Again, the data was segmented into one-second chunks.\nHere, a DBN on raw data showed a classification accuracy\nclose to one using 28 hand-selected features.\n3. DATA ACQUISITION & PRE-PROCESSING\n3.1 Stimuli\nAfrican rhythm stimuli were derived from recordings of tra-\nditional East African music [1]. The author (DC) composed\nthe Western rhythmic stimuli. Rhythms were presented as\nsequences of sine tones that were 100ms in duration with in-\ntensity ramped up/down over the first/final 50ms and a pitch\nof either 375 or 500 Hz. All rhythms had a temporal structure\nof 12 equal units, in which each unit could contain a sound\nor not. For each rhythmic stimulus, two individual rhythmic\nsequences were overlaid – each at a different pitch. For each\ncultural type of rhythm, there were 2 groups of 3 individual\nrhythms for which rhythms could be overlaid with the others\nin their group. Because an individual rhythm could be oneTable 1 . Rhythmic sequences in groups of three that pairings\nwere based on. All ‘x’s denote onsets. Larger, bold ‘ X’s\ndenote the beginning of a 12 unit cycle (downbeat).\nWestern Rhythms\n1Xx x x x x x x Xx x x x x x x\n2X x x x x x X x x x x x\n3X x x x x x x x x X x x x x x x x x\n4X x x x x x x X x x x x x x\n5Xx x x x x x Xx x x x x x\n6X x x x x x x x x X x x x x x x x x\nEast African Rhythms\n1X x x x x x x x x x X x x x x x x x x x\n2X x x x x x X x x x\n3X x x x X x x x\n4X x x x x x x x x X x x x x x x x x\n5X x x x x x x x X x x x x x x x\n6X x x x x x x X x x x x x x\nof two pitches/sounds, this made for a total of 12 rhythmic\nstimuli from each culture, each used for all tasks. Further-\nmore, rhythmic stimuli could be one of two tempi: having a\nminimum inter-onset interval of 180 or 240ms.\n3.2 Study Description\nSixteen East African participants were recruited in Kigali,\nRwanda (3 female, mean age: 23 years, mean musical train-\ning: 3.4 years, mean dance training: 2.5 years). Thirteen of\nthese participated in the EEG portion of the study as well as\nthe behavioral portion. All participants were over the age of\n18, had normal hearing, and had spent the majority of their\nlives in East Africa. They all gave informed consent prior to\nparticipating and were compensated for their participation, as\nper approval by the ethics boards at the Centre Hospitalier\nUniversitaire de Kigali and the University of Western Ontario.\nAfter completion of the behavioral tasks, electrodes were\nplaced on the participant’s scalp. They were instructed to\nsit with eyes closed and without moving for the duration of\nthe recording, and to maintain their attention on the auditory\nstimuli. All rhythms were repeated for 32 seconds, presented\nin counterbalanced blocks (all East African rhythms then all\nWestern rhythms, or vice versa), and with randomized order\nwithin blocks. All 12 rhythms of each type were presented\n– all at the same tempo (fast tempo for subjects 1–3 and 7–9,\nand slow tempo for the others). Each rhythm was preceded\nby 4 seconds of silence. EEG was recorded via a portable\nGrass EEG system using 14 channels at a sampling rate of\n400Hz and impedances were kept below 10k \n.\n3.3 Data Pre-Processing\nEEG recordings are usually very noisy. They contain artifacts\ncaused by muscle activity such as eye blinking as well as pos-\nsible drifts in the impedance of the individual electrodes over\nthe course of a recording. Furthermore, the recording equip-\nment is very sensitive and easily picks up interferences from\nthe surroundings. For instance, in this experiment, the power\nsupply dominated the frequency band around 50Hz. All these\nissues have led to the common practice to invest a lot of effort\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n650into pre-processing EEG data, often even manually rejecting\nsingle frames or channels. In contrast to this, we decided to\nput only little manual work into cleaning the data and just re-\nmoved obviously bad channels, thus leaving the main work to\nthe deep learning techniques. After bad channel removal, 12\nchannels remained for subjects 1–5 and 13 for subjects 6–13.\nWe followed the common practice in machine learning to\npartition the data into training ,validation (or model selec-\ntion) and testsets. To this end, we split each 32s-long trial\nrecording into three non-overlapping pieces. The first four\nseconds were used for the validation dataset. The rationale\nbehind this was that we expected that the participants would\nneed a few seconds in the beginning of each trial to get used\nto the new rhythm. Thus, the data would be less suited for\ntraining but might still be good enough to estimate the model\naccuracy on unseen data. The next 24 seconds were used for\ntraining and the remaining four seconds for testing.\nThe data was finally converted into the input format re-\nquired by the neural networks to be learned.1If the network\njust took the raw EEG data, each waveform was normalized\nto a maximum amplitude of 1 and then split into equally sized\nframes matching the size of the network’s input layer. No win-\ndowing function was applied and the frames overlapped by\n75% of their length. If the network was designed to process\nthe frequency spectrum, the processing involved:\n1.computing the short-time Fourier transform ( STFT ) with\ngiven window length of 64 samples and 75% overlap,\n2. computing the log amplitude,\n3. scaling linearly to a maximum of 1 (per sequence),\n4.(optionally) cutting of all frequency bins above the number\nrequested by the network,\n5.splitting the data into frames matching the network’s input\ndimensionality with a given hop size of 5 to control the\noverlap.\nHere, the number of retained frequency bins and the input\nlength were considered as hyper-parameters.\n4. EXPERIMENTS & FINDINGS\nAll experiments were implemented using Theano [2] and\npylearn2 [8].2The computations were run on a dedicated\n12-core workstation with two Nvidia graphics cards – a Tesla\nC2075 and a Quadro 2000.\nAs the first retrieval task, we focused on recognizing whe-\nther a participant had listened to an East African or Western\nrhythm ( Section 4.1 ). This binary classification task is most\nlikely much easier than the second task – trying to predict\none out of 24 rhythms ( Section 4.2 ). Unfortunately, due to\nthe block design of the study, it was not possible to train a\nclassifier for the tempo. Trying to do so would yield a clas-\nsifier that “cheated” by just recognizing the inter-individual\ndifferences because every participant only listened to stimuli\nof the same tempo.\n1Most of the processing was implemented through the librosa library\navailable at https://github.com/bmcfee/librosa/ .\n2The code to run the experiments is publicly available as supplemen-\ntary material of this paper at http://dx.doi.org/10.6084/m9.\nfigshare.1108287As the classes were perfectly balanced for both tasks, we\nchose the accuracy , i.e., the percentage of correctly classified\ninstances, as evaluation measure. Accuracy can be measured\non several levels. The network predicts a class label for\neach input frame. Each frame is a segment from the time\nsequence of a single EEG channel. Finally, for each trial,\nseveral channels were recorded. Hence, it is natural to also\nmeasure accuracy also at the sequence (i.e, channel) and trial\nlevel. There are many ways to aggregate frame label predic-\ntions into a prediction for a channel or a trial. We tested the\nfollowing three ways to compute a score for each class:\n\u000fplain: sum of all 0-or-1 outputs per class\n\u000ffuzzy: sum of all raw output activations per class\n\u000fprobabilistic: sum of log output activations per class\nWhile the latter approach which gathers the log likelihoods\nfrom all frames worked best for a softmax output layer, it\nusually performed worse than the fuzzy approach for the\nDLSVM output layer with its hinge loss (see below). The\nplain approach worked best when the frame accuracy was\nclose to the chance level for the binary classification task.\nHence, we chose the plain aggregation scheme whenever the\nframe accuracy was below 52% on the validation set and\notherwise the fuzzy approach.\nWe expected significant inter-individual differences and\ntherefore made learning good individual models for the partic-\nipants our priority. We then tested configuration that worked\nwell for individuals on three groups – all participants as well\nas one group for each tempo, containing 6 and 7 subjects\nrespectively.\n4.1 Classification into African and Western Rhythms\n4.1.1 Multi-Layer Perceptron with Pre-Trained Layers\nMotivated by the existing deep learning approaches for EEG\ndata (cf. Section 2 ), we choose to pre-train a MLP as an\nautoencoder for individual channel-seconds – or similar fixed-\nlength chunks – drawn from all subjects. In particular, we\ntrained a stacked denoising autoencoder ( SDA) as proposed\nin [22] where each individual input was set to 0 with a cor-\nruption probability of 0.2.\nWe tested several structural configurations, varying the\ninput sample rate (400Hz or down-sampled to 100Hz), the\nnumber of layers, and the number of neurons in each layer.\nThe quality of the different models was measured as the\nmean squared reconstruction error (MSRE). Table 2 gives\nan overview of the reconstruction quality for selected con-\nfigurations. All SDAs were trained with tied weights, i.e.,\nthe weight matrix of each decoder layer equals the transpose\nof the respective encoder layer’s weight matrix. Each layer\nwas trained with stochastic gradient descent ( SGD ) on mini-\nbatches of 100 examples for a maximum of 100 epochs with\nan initial learning rate of 0.05 and exponential decay.\nIn order to turn a pre-trained SDA into a multilayer percep-\ntron ( MLP ) for classification, we replaced the decoder part\nof the SDA with a DLSVM layer as proposed in [21].3This\nspecial kind of output layer for classification uses the hinge\n3We used the experimental implementation for pylearn2 provided by Kyle\nKastner at https://github.com/kastnerkyle/pylearn2/\nblob/svm_layer/pylearn2/models/mlp.py\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n651Table 2 . MSRE and classification accuracy for selected SDA (top, A-F) and CNN (bottom, G-I) configurations.\nneural network configuration MSRE MLP Classification Accuracy (for frames, channels and trials) in %\nid (sample rate, input format, hidden layer sizes) train test indiv. subjects fast (1–3, 7–9) slow (4–6, 10–13) all (1–13)\nA 100Hz, 100 samples, 50-25-10 (SDA for subject 2) 4.35 4.17 61.1 65.5 72.4 58.7 60.6 61.1 53.7 56.0 59.5 53.5 56.6 60.3\nB 100Hz, 100 samples, 50-25-10 3.19 3.07 58.1 62.0 66.7 58.1 60.7 61.1 53.5 57.7 57.1 52.1 53.5 54.5\nC 100Hz, 100 samples, 50-25 1.00 0.96 61.7 65.9 71.2 58.6 62.3 63.2 54.4 56.4 57.1 53.4 54.8 56.4\nD 400Hz, 100 samples, 50-25-10 0.54 0.53 51.7 58.9 62.2 50.3 50.6 50.0 50.0 51.8 51.2 50.1 50.2 50.0\nE 400Hz, 100 samples, 50-25 0.36 0.34 60.8 65.9 71.8 56.3 58.6 66.0 52.0 55.0 56.0 49.9 50.1 56.1\nF 400Hz, 80 samples, 50-25-10 0.33 0.32 52.0 59.9 62.5 52.3 53.9 54.9 50.5 53.5 55.4 50.2 51.0 50.3\nG 100Hz, 100 samples, 2 conv. layers 62.0 63.9 67.6 57.1 57.9 59.7 49.9 50.2 50.0 51.7 52.8 52.9\nH 100Hz, 200 samples, 2 conv. layers 64.0 64.8 67.9 58.2 58.5 61.1 49.5 49.6 50.6 50.9 50.2 50.6\nI 400Hz, 1s freq. spectrum (33 bins), 2 conv. layers 69.5 70.8 74.7 58.1 58.0 59.0 53.8 54.5 53.0 53.7 53.9 52.6\nJ 400Hz, 2s freq. spectrum (33 bins), 2 conv. layers 72.2 72.6 77.6 57.6 57.5 60.4 52.9 52.9 54.8 53.1 53.5 52.3\n*1 (J)*2 (J)*3 (J)4 (I)5 (H)6 (J)*7 (I)*8 (J)*9 (J)10 (J)11 (C) 12 (J)13 (J)\nsubjects (with best configuration, * = 'fast' group)5060708090frame accuracy (%)\nFigure 1 . Boxplot of the frame-level accuracy for each indi-\nvidual subject aggregated over all configurations.5\nloss as cost function and replaces the commonly applied soft-\nmax. We observed much smoother learning curves and a\nslightly increased accuracy when using this cost function for\noptimization together with rectification as non-linearity in\nthe hidden layers. For training, we used SGD with dropout\nregularization [9] and momentum, a high initial learning rate\nof 0.1 and exponential decay over each epoch. After train-\ning for 100 epochs on minibatches of size 100, we selected\nthe network that maximized the accuracy on the validation\ndataset. We found that the dropout regularization worked\nreally well and largely avoided over-fitting to the training\ndata. In some cases, even a better performance on the test\ndata could be observed. The obtained mean accuracies for\nthe selected SDA configurations are also shown in Table 2\nforMLP s trained for individual subjects as well as for the\nthree groups. As Figure 1 illustrates, there were significant\nindividual differences between the subjects. Whilst learning\ngood classifiers appeared to be easy for subject 9, it was much\nharder for subjects 5 and 13. As expected, the performance\nfor the groups was inferior. Best results were obtained for\nthe “fast” group, which comprised only 6 subjects including\n2 and 9 who were amongst the easiest to classify.\nWe found that two factors had a strong impact on the\nMSRE: the amount of (lossy) compression through the au-\ntoencoder’s bottleneck and the amount of information the\n5Boxes show the 25th to 75th percentiles with a mark for the median\nwithin, whiskers span to furthest values within the 1.5 interquartile range,\nremaining outliers are shown as crossbars.network processes at a time. Configurations A, B and D had\nthe highest compression ratio (10:1). C and E lacked the third\nautoencoder layer and thus only compressed at 4:1 and with a\nlower resulting MSRE. F had exactly twice the compression\nratio as C and E. While the difference in the MSRE was\nremarkable between F and C, it was much less so between\nF and E – and even compared to D. This could be explained\nby the four times higher sample rate of D–F. Whilst A–E\nprocessed the same amount of samples at a time, the input for\nA–C contained much more information as they were looking\nat 1s of the signal in contrast to only 250ms. Judging from the\nMSRE, the longer time span appears to be harder to compress.\nThis makes sense as EEG usually contains most information\nin the lower frequencies and higher sampling rates do not nec-\nessarily mean more content. Furthermore, with growing size\nof the input frames, the variety of observable signal patterns\nincreases and they become harder to approximate. Figure 2\nillustrates the difference between two reconstructions of the\nsame 4s raw EEG input segment using configurations B and\nD. In this specific example, the MSRE for B is ten times as\nhigh compared to D and the loss of detail in the reconstruc-\ntion is clearly visible. However, D can only see 250ms of the\nsignal at a time whereas B processes one channel-second.\nConfiguration A had the highest MSRE as it was only\ntrained on data from subject 2 but needed to process all other\nsubjects as well. Very surprisingly, the respective MLP pro-\nduced much better predictions than B, which had identical\nstructure. It is not clear what caused this effect. One ex-\nplanation could be that the data from subject 2 was cleaner\nthan for other participants as it also led to one amongst the\nbest individual classification accuracies.6This could have\nled to more suitable features learned by the SDA. In general,\nthe two-hidden-layer models worked better than the three-\nhidden-layer ones. Possibly, the compression caused by the\nthird hidden layer was just too much. Apart from this, it\nwas hard to make out a clear “winner” between A, C and E.\nThere seemed to be a trade-off between the accuracy of the\nreconstruction (by choosing a smaller window size and/or\nhigher sampling rate) and learning more suitable features\n6Most of the model/learning parameters were selected by training just\non subject 2.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6520s 1s 2s 3s 4s1.0\n0.5\n0.00.51.0\n0.000.25\n0s 1s 2s 3s 4s1.0\n0.5\n0.00.51.0\n0.000.25Figure 2 . Input (blue) and its reconstruction (red) for the same 4s sequence from the test data. The background color indicates\nthe squared sample error. Top: Configuration B (100Hz) with MSRE 6.43. Bottom: Configuration D (400Hz) with MSRE 0.64.\n(The bottom signals shows more higher-frequency information due to the four-times higher sampling rate.)\nTable 3 . Structural parameters of the CNN configurations.\ninput convolutional layer 1 convolutional layer 2\nid dim. shape patterns pool stride shape patterns pool stride\nG 100x1 15x1 10 7 1 70x1 10 7 1\nH 200x1 25x1 10 7 1 151x1 10 7 1\nI 22x33 1x33 20 5 1 9x1 10 5 1\nJ 47x33 1x33 20 5 1 9x1 10 5 1\nfor recognizing the rhythm type at a larger time scale. This\nled us to try a different approach using convolutional neural\nnetworks (CNNs) as, e.g., described in [11].\n4.1.2 Convolutional Neural Network\nWe decided on a general layout consisting of two convolu-\ntional layers where the first layer was supposed to pick up\nbeat-related patterns while the second would learn to recog-\nnize higher-level structures. Again, a DLSVM layer was used\nfor the output and the rectifier non-linearity in the hidden\nlayers. The structural parameters are listed in Table 3 . As\npooling operation, the maximum was applied. Configurations\nG and H processed the same raw input as A–F whereas I and\nJ took the frequency spectrum as input (using all 33 bins).\nAll networks were trained for 20 epochs using SGD with a\nmomentum of 0.5 and an exponential decaying learning rate\ninitialized at 0.1.\nThe obtained accuracy values are listed in Table 2 (bottom).\nWhilst G and H produced results comparable to A–F, the\nspectrum-based CNN s, I and J, clearly outperformed all other\nconfigurations for the individual subjects. For all but sub-\njects 5 and 11, they showed the highest frame-level accuracy\n(c.f.Figure 1 ). For subjects 2, 9 and 12, the trial classification\naccuracy was even higher than 90% (not shown).\n4.1.3 Cross-Trial Classification\nIn order to rule out the possibility that the classifiers just\nrecognized the individual trials – and not the rhythms – by\ncoincidental idiosyncrasies and artifacts unrelated to rhythm\nperception, we additionally conducted a cross-trial classifica-\ntion experiment. Here, we only considered all subjects with\nframe-level accuracies above 80% in the earlier experiments\n– i.e., subjects 2, 9 and 12. We formed 144 rhythm pairs by\ncombining each East African with each Western rhythm fromthe fast stimuli (for subjects 2 and 9) and the slow ones (for\nsubject 12) respectively. For each pair, we trained a classi-\nfier with configuration J using all but the two rhythms of the\npair.7Due to the amount of computation required, we trained\nonly for 3 epochs each. With the learned classifiers, the mean\nframe-level accuracy over all 144 rhythm pairs was 82.6%,\n84.5% and 79.3% for subject 2, 9 and 12 respectively. These\nvalue were only slightly below those shown in Figure 1 , which\nwe considered very remarkable after only 3 training epochs.\n4.2 Identifying Individual Rhythms\nRecognizing the correct rhythm amongst 24 candidates was\na much harder task than the previous one – especially as all\ncandidates had the same meter and tempo. The chance level\nfor 24 evenly balanced classes was only 4.17%. We used\nagain configuration J as our best known solution so far and\ntrained an individual classifier for each subject. As Figure 3\nshows, the accuracy on the 2s input frames was at least twice\nthe chance level. Considering that these results were obtained\nwithout any parameter tuning, there is probably still much\nroom for improvements. Especially, similarities amongst the\nstimuli should be considered as well.\n5. CONCLUSIONS AND OUTLOOK\nWe obtained encouraging first results for classifying chunks of\n1-2s recorded from a single EEG channel into East African or\nWestern rhythms using convolutional neural networks ( CNN s)\nand multilayer perceptrons ( MLP s) pre-trained as stacked\ndenoising autoencoders ( SDAs). As it turned out, some con-\nfigurations of the SDA (D and F) were especially suited to\nrecognize unwanted artifacts like spikes in the waveforms\nthrough the reconstruction error. This could be elaborated in\nthe future to automatically discard bad segments during pre-\nprocessing. Further, the classification accuracy for individual\nrhythms was significantly above chance level and encourages\nmore research in this direction. As this has been an initial and\nby no means exhaustive exploration of the model- and lean-\ning parameter space, there seems to be a lot more potential –\nespecially in CNN s processing the frequency spectrum – and\n7Deviating from the description given in Section 3.3 , we used the first\n4s of each recording for validation and the remaining 28s for training as the\ntest set consisted of full 32s from separate recordings in this special case.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n65305101520\nPredicted label0\n5\n10\n15\n20True labelsubject 1 2 3 4 5 6 7 8 9 10 11 12 13 mean\naccuracy 15.8% 9.9% 12.0% 21.4% 10.3% 13.9% 16.2% 11.0% 11.0% 10.3% 9.2% 17.4% 8.3% 12.8%\nprecision @3 31.5% 29.9% 26.5% 48.2% 28.3% 27.4% 41.2% 27.8% 28.5% 33.2% 24.7% 39.9% 20.7% 31.4%\nmean reciprocal rank 0.31 0.27 0.27 0.42 0.26 0.28 0.36 0.27 0.28 0.30 0.25 0.36 0.23 0.30\nFigure 3 . Confusion matrix for all subjects (left) and per-subject performance (right) for predicting the rhythm (24 classes).\nwe will continue to look for better designs than those consid-\nered here. We are also planning to create publicly available\ndata sets and benchmarks to attract more attention to these\nchallenging tasks from the machine learning and information\nretrieval communities.\nAs expected, individual differences were very high. For\nsome participants, we were able to obtain accuracies above\n90%, but for others, it was already hard to reach even 60%.\nWe hope that by studying the models learned by the classi-\nfiers, we may shed some light on the underlying processes\nand gain more understanding on why these differences occur\nand where they originate. Also, our results still come with a\ngrain of salt: We were able to rule out side effects on a trial\nlevel by successfully replicating accuracies across trials. But\ndue to the study’s block design, there remains still the chance\nthat unwanted external factors interfered with one of the two\nblocks while being absent during the other one. Here, the\nanalysis of the learned models could help to strengthen our\nconfidence in the results.\nThe study is currently being repeated with North America\nparticipants and we are curious to see whether we can repli-\ncate our findings. Furthermore, we want to extend our focus\nby also considering more complex and richer stimuli such\nas audio recordings of rhythms with realistic instrumentation\ninstead of artificial sine tones.\nAcknowledgments: This work was supported by a fellow-\nship within the Postdoc-Program of the German Academic\nExchange Service (DAAD), by the Natural Sciences and En-\ngineering Research Council of Canada (NSERC), through the\nWestern International Research Award R4911A07, and by an\nAUCC Students for Development Award.\n6. REFERENCES\n[1]G.F. Barz. Music in East Africa: experiencing music, expressing\nculture . Oxford University Press, 2004.\n[2]J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,\nG. Desjardins, J. Turian, D. Warde-Farley, and Y . Bengio.\nTheano: a CPU and GPU math expression compiler. In Proc. of\nthe Python for Scientific Computing Conference (SciPy) , 2010.\n[3]R. Cabredo, R.S. Legaspi, P.S. Inventado, and M. Numao. An\nemotion model for music using brain waves. In ISMIR , pages\n265–270, 2012.\n[4]D.J. Cameron, J. Bentley, and J.A. Grahn. Cross-cultural\ninfluences on rhythm processing: Reproduction, discrimination,\nand beat tapping. Frontiers in Human Neuroscience , to appear.\n[5]T. Fujioka, L.J. Trainor, E.W. Large, and B. Ross. Beta and\ngamma rhythms in human auditory cortex during musical beat\nprocessing. Annals of the New York Academy of Sciences ,\n1169(1):89–92, 2009.\n[6]T. Fujioka, L.J. Trainor, E.W. Large, and B. Ross. Inter-\nnalized timing of isochronous sounds is represented in\nneuromagnetic beta oscillations. The Journal of Neuroscience ,\n32(5):1791–1802, 2012.[7]E. Geiser, E. Ziegler, L. Jancke, and M. Meyer. Early electro-\nphysiological correlates of meter and rhythm processing in\nmusic perception. Cortex , 45(1):93–102, 2009.\n[8]I.J. Goodfellow, D. Warde-Farley, P. Lamblin, V . Dumoulin,\nM. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and Y . Bengio.\nPylearn2: a machine learning research library. arXiv preprint\narXiv:1308.4214 , 2013.\n[9]G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,\nand R.R. Salakhutdinov. Improving neural networks by\npreventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580 , 2012.\n[10] J.R. Iversen, B.H. Repp, and A.D. Patel. Top-down control of\nrhythm perception modulates early auditory responses. Annals\nof the New York Academy of Sciences , 1169(1):58–73, 2009.\n[11] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet\nclassification with deep convolutional neural networks. In\nAdvances in Neural Information Processing Systems (NIPS) ,\npages 1097–1105, 2012.\n[12] O. Ladinig, H. Honing, G. H ´aden, and I. Winkler. Probing atten-\ntive and preattentive emergent meter in adult listeners without ex-\ntensive music training. Music Perception , 26(4):377–386, 2009.\n[13] M. L ¨angkvist, L. Karlsson, and M. Loutfi. Sleep stage\nclassification using unsupervised feature learning. Advances\nin Artificial Neural Systems , 2012:5:5–5:5, Jan 2012.\n[14] Y .-P. Lin, T.-P. Jung, and J.-H. Chen. EEG dynamics during\nmusic appreciation. In Engineering in Medicine and Biology\nSociety, 2009. EMBC 2009. Annual Int. Conf. of the IEEE ,\npages 5316–5319, 2009.\n[15] S.J. Morrison, S.M. Demorest, E.H. Aylward, S.C. Cramer,\nand K.R. Maravilla. Fmri investigation of cross-cultural music\ncomprehension. Neuroimage , 20(1):378–384, 2003.\n[16] S.J. Morrison, S.M. Demorest, and L.A. Stambaugh. En-\nculturation effects in music cognition the role of age and\nmusic complexity. Journal of Research in Music Education ,\n56(2):118–129, 2008.\n[17] S. Nozaradan, I. Peretz, M. Missal, and A. Mouraux. Tagging\nthe neuronal entrainment to beat and meter. The Journal of\nNeuroscience , 31(28):10234–10240, 2011.\n[18] S. Nozaradan, I. Peretz, and A. Mouraux. Selective neuronal en-\ntrainment to the beat and meter embedded in a musical rhythm.\nThe Journal of Neuroscience , 32(49):17572–17581, 2012.\n[19] J.S. Snyder and E.W. Large. Gamma-band activity reflects the\nmetric structure of rhythmic tone sequences. Cognitive brain\nresearch , 24(1):117–126, 2005.\n[20] G. Soley and E.E. Hannon. Infants prefer the musical meter of\ntheir own culture: a cross-cultural comparison. Developmental\npsychology , 46(1):286, 2010.\n[21] Y . Tang. Deep Learning using Linear Support Vector Machines.\narXiv preprint arXiv:1306.0239 , 2013.\n[22] P. Vincent, H. Larochelle, I. Lajoie, Y . Bengio, and P.-A.\nManzagol. Stacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising\ncriterion. The Journal of Machine Learning Research ,\n11:3371–3408, Dec 2010.\n[23] R.J. Vlek, R.S. Schaefer, C.C.A.M. Gielen, J.D.R. Farquhar,\nand P. Desain. Shared mechanisms in perception and imagery of\nauditory accents. Clinical Neurophysiology , 122(8):1526–1532,\nAug 2011.\n[24] D.F. Wulsin, J.R. Gupta, R. Mani, J.A. Blanco, and B. Litt. Mod-\neling electroencephalography waveforms with semi-supervised\ndeep belief nets: fast classification and anomaly measurement.\nJournal of Neural Engineering , 8(3):036015, Jun 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n654"
    },
    {
        "title": "Formalizing the Problem of Music Description.",
        "author": [
            "Bob L. Sturm",
            "Rolf Bardeli",
            "Thibault Langlois",
            "Valentin Emiya"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414914",
        "url": "https://doi.org/10.5281/zenodo.1414914",
        "ee": "https://zenodo.org/records/1414914/files/SturmBLE14.pdf",
        "abstract": "The lack of a formalism for “the problem of music descrip- tion” results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the “ground truth” of a music dataset without attending to the music it contains. To address these issues, we formal- ize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formal- ism applied to three examples drawn from the literature.",
        "zenodo_id": 1414914,
        "dblp_key": "conf/ismir/SturmBLE14",
        "keywords": [
            "formalism",
            "ambiguity",
            "evaluation",
            "criteria",
            "success",
            "paradox",
            "system",
            "system",
            "evaluation",
            "illustrations"
        ],
        "content": "FORMALIZING THE PROBLEM OF MUSIC DESCRIPTION\nBob L. Sturm\nAalborg University\nDenmark\nbst@create.aau.dkRolf Bardeli\nFraunhofer IAIS\nGermany\nrolf.bardeli@iais.fraunhofer.deThibault Langlois\nLisbon University\nPortugal\ntl@di.fc.ul.ptValentin Emiya\nAix-Marseille Universit ´e\nCNRS UMR 7279 LIF\nvalentin.emiya@lif.univ-mrs.fr\nABSTRACT\nThe lack of a formalism for “the problem of music descrip-\ntion” results in, among other things: ambiguity in what\nproblem a music description system must address, how it\nshould be evaluated, what criteria deﬁne its success, and\nthe paradox that a music description system can reproduce\nthe “ground truth” of a music dataset without attending to\nthe music it contains. To address these issues, we formal-\nize the problem of music description such that all elements\nof an instance of it are made explicit. This can thus inform\nthe building of a system, and how it should be evaluated in\na meaningful way. We provide illustrations of this formal-\nism applied to three examples drawn from the literature.\n1. INTRODUCTION\nBefore one can address a problem with an algorithm (a ﬁ-\nnite series of well-deﬁned operations that transduce a well-\nspeciﬁed input into a well-speciﬁed output) one needs to\ndeﬁne and decompose that problem in a way that is com-\npatible with the formal nature of algorithms [17]. A very\nsimple example is the problem of adding any two posi-\ntive integers. Addressing this problem with an algorithm\nentails deﬁning the entity “positive integer”, the function\n“adding”, and then producing a ﬁnite series of well-deﬁned\noperations that applies the function to an input of two pos-\nitive integers to output the correct positive integer.\nA more complex example is “the problem of music de-\nscription.” While much work in music information re-\ntrieval (MIR) has proposed systems to attempt to address\nthe problem of music description [4, 12, 29], and much\nwork attempts to evaluate the capacity of these systems for\naddressing that problem [9, 20], we have yet to ﬁnd any\nwork that actually deﬁnes it. (The closest we have found is\nthat of [24].) Instead, there are many allusions to the prob-\nlem: predict the “genre” of a piece of recorded music [25];\nlabel music with “useful tags” [1]; predict what a listener\nwill “feel” when “listening” to some music [29]; ﬁnd mu-\nsic “similar” to some other music [26]. These allusions are\ndeceptively simple, however, since behind them lie many\nc\rBob L. Sturm, Rolf Bardeli, Thibault Langlois, Valentin\nEmiya.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Bob L. Sturm, Rolf Bardeli, Thibault\nLanglois, Valentin Emiya. “Formalizing the Problem of Music Descrip-\ntion”, 15th International Society for Music Information Retrieval Confer-\nence, 2014.problems and questions that have major repercussions on\nthe design and evaluation of any proposed system. For ex-\nample, What is “genre”? What is “useful”? How is “feel-\ning” related to “listening”? “Similar” in what respects?\nWith respect to the problem of music description, some\nwork in MIR discusses the meaningfulness, worth, and fu-\ntility of designing artiﬁcial systems to describe music [28];\nthe idea of and the difﬁculty in “ground truth” [3, 6, 15];\nthe size of datasets [5], a lack of statistics [10], the exis-\ntence of bias [16], and the ways such systems are evalu-\nated [21, 22, 27]. Since a foundational goal of MIR is to\ndevelop systems that can imitate the human ability to de-\nscribe music, these discussions are necessary. However,\nwhat remains missing is a formal deﬁnition of the problem\nof music description such that it can be addressed by algo-\nrithms, and relevant and valid evaluations can be designed.\nIn this work, we formalize the problem of music de-\nscription and try to avoid ambiguity arising from seman-\ntics. This leads to a rather abstract form, and so we illus-\ntrate its aspects using examples from the literature. The\nmost practical beneﬁt of our formalization is a speciﬁca-\ntion of all elements that should be explicitly deﬁned when\naddressing an instance of the problem of music description.\n2. FORMALISM\nWe start our formalization by deﬁning the domain of the\nproblem of music description. In particular, we discrimi-\nnate between the music that is to be described and a record-\ning of it since the former is intangible and the latter is data\nthat a system can analyze. We then deﬁne the problem\nof music description, a recorded music description system\n(RMDS), and the analysis of such a system. This leads to\nthe central role of the use case.\n2.1 Domain\nDenote a music universe, \n, a set of music, e.g., Vivaldi’s\n“The Four Seasons”, the piano part of Gershwin’s “Rhap-\nsody in Blue”, and the ﬁrst few measures of the ﬁrst move-\nment of Beethoven’s Fifth Symphony. A member of \nis\nintangible. One cannot hear, see or point to any member\nof\n; but one can hear a performance ofVivaldi’s “The\nFour Seasons”, read sheet music notating the piano part\nofGershwin’s “Rhapsody in Blue”, and point to a printed\nscore ofBeethoven’s Fifth Symphony. Likewise, a recorded\nperformance of Vivaldi’s “The Four Seasons” is notVi-\nvaldi’s “The Four Seasons”, and sheet music notating the\npiano part of Gershwin’s “Rhapsody in Blue” is notthe\npiano part of Gershwin’s “Rhapsody in Blue”.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n89In the tangible world, there may exist tangible record-\nings of the members of \n. Denote the tangible music record-\ning universe byR\n. A member ofR\nis a recording of an\nelement of !2\n. A recording is a tangible object, such\nas a printed CD or score. Denote one recording of !2\nasr!2R \n. There might be many recordings of an !in\nR\n. We say the music !isembedded inr!; it enables for\na listener an indirect sense of !. For instance, one can hear\na live or recorded performance of !, and one can read a\nprinted score of !. The acknowledgment of and distinc-\ntion between intangible music and tangible recordings of\nmusic is essential since systems cannot work with intangi-\nble music, but only tangible recordings.\n2.2 Music Description and the Use Case\nDenote a vocabulary,V, a set of symbols or tokens, e.g.,\n“Baroque”, “piano”, “knock knock”, scores employing com-\nmon practice notation, the set of real numbers R, other mu-\nsic recordings, and so on. Deﬁne the semantic universe as\nSV;A:=fs= (v1; : : : ; v n)jn2N;\n81\u0014i\u0014n[vi2V]^A(s)g (1)\nwhere A(\u0001) encompasses a semantic rule, for instance, re-\nstrictingSV;Ato consist of sequences of cardinality 1. Note\nthat the description sis a sequence, and not a vector or a\nset. This permits descriptions that are, e.g., time-dependent,\nsuch as envelopes, if VandA(\u0001) permit it. In that case,\nthe order of elements in scould be alternating time val-\nues with envelope values. Descriptions could also be time-\nfrequency dependent.\nWe deﬁne music description as pairing an element of\n\norR\nwith an element of SV;A. The problem of music\ndescription is to make the pairing acceptable with respect\nto a use case. A use case provides speciﬁcations of \nand\nR\n,VandA(\u0001), and success criteria. Success criteria de-\nscribe how music or a music recording should be paired\nwith an element of the semantic universe, which may in-\nvolve the sanity of the decision (e.g., tempo estimation\nmust be based on the frequency of onsets), the efﬁciency of\nthe decision (e.g., pairing must be produced under 100 ms\nwith less than 10 MB of memory), or other considerations.\nTo make this clearer, consider the following use case.\nThe music universe \nconsists of performances by Buck-\nwheat Zydeco, movements of Vivaldi’s “The Four Sea-\nsons”, and traditional Beijing opera. The tangible music\nrecording universe R\nconsists of all possible 30-second\ndigital audio recordings of the elements in \n. Let the vo-\ncabularyV=f“Blues”, “Classical”g; and deﬁne A(s) :=\n[jsj2f0; 1g]. The semantic universe is thus, SV;A=f();\n(“Blues”); (“Classical”)g. There are many possible suc-\ncess criteria. One is to map all recordings of Buckwheat\nZydeco to “Blues”, map all recordings of Vivaldi’s “The\nFour Seasons” to “Classical”, and map all recordings of\ntraditional Beijing opera to neither. Another is to map no\nrecordings of Buckwheat Zydeco and Vivaldi’s “The Four\nSeasons” to the empty sequence, and to map any recording\nof traditional Beijing opera to either non-empty sequence\nwith a probability less than 0:1.2.3 Recorded Music Description Systems\nArecorded music description system (RMDS) is a map\nfrom the tangible music recording universe to the semantic\nuniverse:\nS:R\n!SV;A: (2)\nBuilding an RMDS means making a map according to well-\nspeciﬁed criteria, e.g., using expert domain knowledge, au-\ntomatic methods of supervised learning, and a combina-\ntion of these. An instance of an RMDS is a speciﬁc map\nthat is already built, and consists of four kinds of com-\nponents [21]: algorithmic (e.g., feature extraction, classi-\nﬁcation, pre-processing), instruction (e.g., description of\nR\nandSV;A), operator(s) (e.g., the one inputting data and\ninterpreting output), and environmental (e.g., connections\nbetween components, training datasets). It is important to\nnote that Sis not restricted to map any recording to a sin-\ngle element ofV. Depending onVandA(\u0001),SV;Acould\nconsist of sequences of scalars and vectors, sets and se-\nquences, functions, combinations of all these, and so on. S\ncould thus map a recording to many elements of V.\nOne algorithmic component of an RMDS is a feature\nextraction algorithm, which we deﬁne as\nE:R\n!S F;A0 (3)\ni.e., a map fromR\nto a semantic universe built from the\nvocabulary of a feature space Fand semantic rule A0(\u0001).\nFor instance, if F:=CM,M2N, and A0(s) := [jsj = 1],\nthen the feature extraction maps a recording to a single\nM-dimensional complex vector. Examples of such a map\nare the discrete Fourier transform, or a stacked series of\nvectors of statistics of Mel frequency cepstral coefﬁcients.\nAnother algorithmic component of an RMDS is a classiﬁ-\ncation algorithm, which we deﬁne:\nC:SF;A0!SV;A (4)\ni.e., a map from one semantic universe to another. Ex-\namples of such a map are k-nearest neighbor, maximum\nlikelihood, support vector machine, and a decision tree.\nTo make this clearer, consider the RMDS named “RT\nGS” built by Tzanetakis and Cook [25]. Emaps sam-\npled audio signals of about 30-s duration to SF;A0, deﬁned\nby single 19-dimensional vectors, where one dimension is\nspectral centroid mean, another is spectral centroid vari-\nance, and so on. CmapsSF;A0toSV;A, which is deﬁned\nbyV=f“Blues”, “Classical”, “Country”, “Disco”, “Hip\nhop”, “Jazz”, “Metal”, “Pop”, “Reggae”, “Rock”g, and\nA(s) := [jsj = 1]. This mapping involves maximizing the\nlikelihood of an element of SF;A0among ten multivariate\nGaussian models created with supervised learning.\nSupervised learning involves automatically building com-\nponents of an S, or deﬁning EandC, given a training\nrecorded music dataset: a sequence of tuples of recordings\nsampled fromR\nand elements ofSV;A, i.e.,\nD:=f(ri; si)2R\n\u0002SV;Aji2Ig (5)\nThe setIindexes the dataset. We call the sequence (si)i2I\ntheground truth ofD. In the case of RT GS, its training\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n90recorded music dataset contains 900 tuples randomly se-\nlected from the dataset GTZAN [22,25]. These are selected\nin a way such that the ground truth of Dhas no more than\n100 of each element of SV;A.\n2.4 Analysis of Recorded Music Description Systems\nGiven an RMDS, one needs to determine whether it ad-\ndresses the problem of music description. Simple ques-\ntions to answer are: does \nandR\nof the RMDS encom-\npass those of the use case? Does the SV;Aof the RMDS\nencompass that of the use case? A more complex ques-\ntion could be, does the RMDS meet the success criteria\nof the use case? This last question involves the design,\nimplementation, analysis, and interpretation of valid ex-\nperiments that are relevant to answering hypotheses about\nthe RMDS and success criteria [21, 27]. Answering these\nquestions constitutes an analysis of an RMDS.\nAbsent explicit success criteria of a use case, a standard\napproach for evaluating an RMDS is to compute a vari-\nety of ﬁgures of merit (FoM) from its “treatment” of the\nrecordings of a testing Dthat exemplify the input/output\nrelationships sought. Examples of such FoM are mean\nclassiﬁcation accuracy, precisions, recalls, and confusions.\nAn implicit belief is that the correct output will be pro-\nduced from the input only if an RMDS has learned criteria\nrelevant to describing the music. Furthermore, it is hoped\nthat the resulting FoM reﬂect the real world performance\nof an RMDS. The real world performance of an RMDS\nare the FoM that result from an experiment using a testing\nrecording music dataset consisting of allmembers inR\n,\nrather than a sampling of them. If this dataset is out of\nreach, statistical tests can be used to determine signiﬁcant\ndifferences in performance between two RMDS (testing\nthe null hypothesis, “neither RMDS has ‘learned better’\nthan the other”), or between the RMDS and that of picking\nan element ofSV;Aindependent of the element from R\n(testing the null hypothesis, “The RMDS has learned noth-\ning”). These statistical tests are accompanied by implicit\nand strict assumptions on the measurement model and its\nappropriateness to describe the measurements made in the\nexperiment [2, 8].\nAs an example, consider the evaluation of RT GS dis-\ncussed above [25]. The evaluation constructs a testing D\nfrom the 100 elements of the dataset GTZAN not present in\nthe trainingDused to create the RMDS. They treat each\nof the 100 recordings in the testing Dwith RT GS, and\ncompare its output with the ground truth. From these 100\ncomparisons, they compute the percentage of outputs that\nmatch the ground truth (accuracy). Whether or not this is\na high-quality estimate of the real world accuracy of RT\nGS depends entirely upon the deﬁnition of \n,R\n,SV;A,\nas well as the testing Dand the measurement model of the\nexperiment.\nThere are many serious dangers to the interpretation of\nthe FoM of an RMDS as reﬂective of its real world per-\nformance: noise in the measurements, an inappropriate\nmeasurement model [2], a poor experimental design and\nerrors of the third kind [14], the lack of error bounds or\nerror bounds that are too large [8], and several kinds ofbias. One kind of bias comes from the very construction\nof testing datasets. For instance, if the testing dataset is the\nsame as the training dataset, and the set of recordings in the\ndataset is a subset of R\n, then the FoM of an RMDS com-\nputed from the treatment may not indicate its real world\nperformance. This has led to the prescription in machine\nlearning to use a testing dataset that is disjoint with the\ntraining dataset, by partitioning for instance [13]. This,\nhowever, may not solve many other problems of bias as-\nsociated with the construction of datasets, or increase the\nrelevance of such an experiment with measuring the extent\nto which an RMDS has learned to describe the music in \n.\n2.5 Summary\nTable 1 summarizes all elements deﬁned in our formaliza-\ntion of the problem of music description, along with exam-\nples of them. These are the elements that must be explic-\nitly deﬁned in order to address an instance of the problem\nof music description by algorithms. Central to many of\nthese are the deﬁnition of a use case, which speciﬁes the\nmusic and music recording universe, the vocabulary, the\ndesired semantic universe, andthe success criteria of an\nacceptable system. (Note that “use case” is not the same\nas “user-centered.”) If the use case is not unambiguously\nspeciﬁed, then a successful RMDS cannot be constructed,\nrelevant and valid experiments cannot be designed, and the\nanalysis of an RMDS cannot be meaningful. Table 1 can\nserve as a checklist for the extent to which an instance of\nthe problem of music description is explicitly deﬁned.\n3. APPLICATION\nWe now discuss two additional published works in the MIR\nliterature in terms of our formalism.\n3.1 Dannenberg et al. [7]\nThe use cases of the RMDS employed by Dannenberg et\nal. [7] are motivated by the desire for a mode of communi-\ncation between a human music performer and an accompa-\nnying computer that is more natural than physical interac-\ntion. The idea is for the computer to employ an RMDS to\ndescribe the acoustic performance of a performer in terms\nof several “styles.” Dannenberg et al. circumvent the need\nto deﬁne any of these “styles” by noting, “what really mat-\nters is the ability of the performer to consistently produce\nintentional and different styles of playing at will” [7]. As\na consequence, the use cases and thus system analysis are\ncentered on the performer.\nOne use case considered by Dannenberg et al. deﬁnes\nV=f“lyrical”, “frantic”, “syncopated”, “pointillistic”,\n“blues”, “quote”, “high”, “low”g, and the semantic rule\nA(s) := [jsj2f 1g]. The semantic universe SV;Ais then\nall single elements of V. The music universe \nis all possi-\nble music that can be played or improvised by the speciﬁc\nperformer in these “styles.” The tangible music record-\ning universeR\nis all possible 5-second acoustic record-\nings of the elements of \n. Finally, the success criteria of\nthis particular problem of music description includes the\nfollowing requirements: reliable for a speciﬁc performer\nin an interactive performance, classiﬁer latency of under\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n91Element (Symbol) Deﬁnition Example\nmusic universe (\n) a set of (intangible) music f“Automatic Writing” by R. Ashleyg\ntangible music recording\nuniverse (R\n)a set of tangible recordings of all members of \nfR. Ashley, “Automatic Writing”, LCD\n1002, Lovely Music, Ltd., 1996g\nrecording (r!) a member ofR\n a 1-second excerpt of the 46 minute\nrecording of “Automatic Writing” from\nLCD 1002\nvocabulary (V) a set of symbols f“Robert”, “french woman”, “bass in\nother room”, “Moog”g[ [0;2760]\nsemantic universe (SV;A)fs= (v 1; : : : ; v n)jn2N;81\u0014i\u0014n[vi2V]^A(s)g, i.e.,\nthe set of all sequences of symbols from Vpermitted by the\nsemantic rule A(\u0001)f(“Robert”, 1), (“Robert”, “Moog”,\n4.3), (“french woman”, 104.3), (“french\nwoman”, “Moog”, 459), : : :g\nsemantic rule (A(s)) a Boolean function that deﬁnes when sequence sis “permissi-\nble”A(s) :=\u0002\n(jsj 2 f2; 3;4;5g)^\n(fv1; : : : ; vjsj\u00001g \u0012 f“Robert”,\n“french woman”, “bass in other room”,\n“Moog”g[fg)^(vjsj2[0;2760])\u0003\nmusic description the pairing of an element of \norR\nwith an element ofSV;Alabel the events (character, time) in record-\ning LCD 1002 of “Automatic Writing” by\nR. Ashley\nthe problem of music de-\nscriptionmake this pairing acceptable with respect to the success criteria\nspeciﬁed by the use casemake this pairing such that F-score of\nevent “Robert” is at least 0.9\nuse case speciﬁcation of \n;R\n;V; A(s), and success criteria see all above\nsystem a connected set of interacting and interdependent components\nof four kinds (operator(s), instructions, algorithms, environ-\nment) that together address a use casesystem created in the Audio Latin Genre\nClassiﬁcation task of MIREX 2013 by or-\nganizer from submission “AP1” and fold 1\nof LMD [18]\noperators agent(s) that employ the system, inputting data, and interpret-\ning outputsAudio Latin Genre Classiﬁcation orga-\nnizer of MIREX 2013\ninstructions speciﬁcations for the operator(s), like an application program-\nming interfaceMIREX 2013 input/output speciﬁcations\nfor Train/Test tasks; “README” ﬁle in-\ncluded with “AP1”\nalgorithm a ﬁnite series of well-deﬁned ordered operations to transduce\nan input into an output“Training.m” and “Classifying.m” MAT-\nLAB scripts in “AP1”, etc.\nenvironment connections between components, external databases, the\nspace within which the system operates, its boundariesfolds 2 and 3 of LMD [18], MIREX com-\nputer cluster, local MATLAB license ﬁle,\netc.\nrecorded music description\nsystem (RMDS) (S)S:R\n!SV;A, i.e., a map fromR\ntoSV;A “RT GS” evaluated in [25]\nfeature extraction algorithm\n(E)E:R\n!S F;A0, i.e., a map fromR\nto an element of a\nsemantic universe based on the feature vocabulary Fand se-\nmantic rule A0(s)compute using [19] the ﬁrst 13 MFCCs\n(including zeroth coefﬁcient) from a\nrecording\nfeature vocabulary (F) a set of symbols R13\nclassiﬁcation algorithm (C)C:SF;A0!SV;A, i.e., a map fromSF;A0to the semantic\nuniversesingle nearest neighbor\nrecorded music dataset D:= (fr !2R \n; s2SV;Agi)i2I, i.e., a sequence of tuples\nof recordings and elements of the semantic universe, indexed\nbyIGTZAN [22, 25]\n“ground truth” ofD (si)i2I, i.e., the sequence of “true” elements of the semantic\nuniverse for the recordings in DinGTZAN:f“blues”, “blues”, : : :, “clas-\nsical”, : : :, “country”, : : :g\nanalysis of an RMDS answering whether an RMDS can meet the success criteria of\na use case with relevant and valid experimentsdesigning, implementing, analyzing and\ninterpreting experiments that validly an-\nswer, “Can RT GS [25] address the needs\nof user A?”\nexperiment principally in service to answering a scientiﬁc question, the\nmapping of one or more RMDS to recordings of D, and the\nmaking of measurementsapply RT GS to GTZAN, compare its out-\nput labels to “ground truth”, and compute\naccuracy\nﬁgure of merit (FoM) performance measurement of an RMDS from an experiment classiﬁcation accuracy of RT GS in\nGTZAN\nreal world performance of\nan RMDSthe ﬁgure of merit expected if an experiment with an RMDS\nuses all ofR\nclassiﬁcation accuracy of RT GS\nTable 1. Summary of all elements deﬁned in the formalization of the problem of music description, with examples.\n5 seconds. The speciﬁc deﬁnition of “reliable” might in-\nclude high accuracy, high precision in every class, or only\nin some classes.\nDannenberg et al. create an RMDS by using a training\ndataset of recordings curated from actual performances, as\nwell as collected in a more controlled fashion in a lab-\noratory. The ground truth of the dataset is created withinput from performers. The feature extraction algorithm\nincludes algorithms for pitch detection, MIDI conversion,\nand the computation of 13 low-level features from the MIDI\ndata. One classiﬁcation algorithm employed is maximum\nlikelihood using a naive Bayesian model.\nThe system analysis performed by Dannenberg et al. in-\nvolve experiments measuring the mean accuracy of all sys-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n92tems created and tested with 5-fold cross validation. Fur-\nthermore, they evaluate a speciﬁc RMDS they create in the\ncontext of a live music performance. From this they ob-\nserve three things: 1) the execution time of the RMDS is\nunder 1 ms; 2) the FoM of the RMDS found in the lab-\noratory evaluation is too optimistic for its real world per-\nformance in the context of live performance; 3) using the\nconﬁdence of the classiﬁer and tuning a threshold parame-\nter provides a means to improve the RMDS by reducing its\nnumber of false positives.\n3.2 Turnbull et al. [24]\nTurnbull et al. [24] propose several RMDS that work with\na vocabulary consisting of 174 unique “musically relevant”\nwords, such as “Genre–Brit Pop”, “Usage-Reading”, and\n“NOT-Emotion–Bizarre /Weird”. A(s) := [jsj= 10^\n8i6=j(vi6=vj)], and so the elements of SV;Aare tuples\nof ten unique elements of V. The music universe \ncon-\nsists of at least 502 songs (the size of the CAL500 dataset),\nsuch as “S.O.S.” performed by ABBA, “Sweet Home Al-\nabama” performed by Lynyrd Skynyrd, and “Fly Me to the\nMoon” sung by Frank Sinatra. The tangible music record-\ning universeR\nis composed of MP3-compressed record-\nings of entire music pieces. The RMDS sought by Turnbull\net al. aims “[to be] good at predicting all the words [in V]”,\nor “produce sensible semantic annotations for an acousti-\ncally diverse set of songs.” Since “good”, “sensible” and\n“acoustically diverse” are not deﬁned, the success criteria\nis ambiguous. \nis also likely much larger than 502 songs.\nThe feature extraction algorithm in the RMDS of Turn-\nbull et al. maps a music recording to a semantic universe\nbuilt from a feature vocabulary F:=R39, and the semantic\nruleA0(s) := [jsj= 10000]. That is, the algorithm com-\nputes from an audio recording 13 MFCC coefﬁcients on\n23ms frames, concatenates the ﬁrst and second derivatives\nin each frame, and randomly selects 10000 feature vec-\ntors from all those extracted. The classiﬁcation algorithm\nin the RMDS uses a a maximum a posteriori decision cri-\nterion, with conditional probabilities of features modelled\nby a Gaussian mixture model (GMM) of a speciﬁed order.\nOne RMDS uses expectation maximization to estimate the\nparameters of an 8-order GMM from a training dataset.\nTurnbull et al. build an RMDS using a training dataset\nof 450 elements selected from CAL500. They apply this\nRMDS to the remaining elements of CAL500, and mea-\nsure how its output compares to the ground truth. When\nthe ground truth of a recording in CAL500 does not have\n10 elements per the semantic rule of the semantic universe,\nTurnbull et al. randomly add unique elements of V, or\nrandomly remove elements from the ground truth of the\nrecording until it has cardinality 10.\nTurnbull et al. compute from an experiment FoM such\nas mean per-word precision. Per-word precision is, for a\nv2V and when deﬁned, the percentage of correct map-\npings of the system from the recordings in the test dataset\nto an element of the semantic universe that includes v.\nMean per-word precision is thus the mean of the jVjper-\nword precisions. Turnbull et al. compare the FoM of the\nRMDS to other systems, such as a random classiﬁer anda human. They conclude that their best RMDS is slightly\nworse than human performance on “more ‘objective’ se-\nmantic categories [like instrumentation and genre]” [24].\nThe evaluation, measuring the amount of ground truth re-\nproduced by a system (human or not) and not the sensi-\nbility of the annotations, has questionable relevance and\nvalidity to the ambiguous use case.\n4. CONCLUSION\nFormalism can reveal when a problem is not adequately de-\nﬁned, and how to explicitly deﬁne it in no uncertain terms.\nAn explicit deﬁnition of a problem shows how to evaluate\nsolutions in relevant and valid ways. It is in this direction\nthat we move with this paper for the problem of music de-\nscription, the spirit of which is encapsulated by Table 1.\nThe unambiguous deﬁnition of the use case is central for\naddressing an instance of the problem of music description.\nWe have discussed several published RMDS within this\nformalism. The work of Dannenberg et al. [7] provides\na good model since its use case and analysis are clearly\nspeciﬁed — both center on a speciﬁc music performer —\nand through evaluating the system in the real world they\nactually complete the research and development cycle to\nimprove the system [27]. The use cases of the RMDS built\nby Tzanetakis and Cook [25] and Turnbull et al. [24] are\nnot speciﬁed. In both cases, a labeled dataset is assumed to\nprovide sufﬁcient deﬁnition of the problem. Turnbull et al.\nsuggest a success criterion of annotations being “sensible,”\nbut the evaluation only measures the amount of ground\ntruth reproduced. Due to the lack of deﬁnition, we are thus\nunsure what problem either of these RMDS is actually ad-\ndressing, or whether either of them is actually considering\nthe music [23]. An analysis of an RMDS depends on an\nexplicit use case. The deﬁnition of the use case in Dan-\nnenberg et al. [7] renders this question irrelevant: all that\nis needed is that the RMDS meets the success criteria of a\ngiven performer, which is tested by performing with it.\nWhile we provide in this paper a formalization of the\nproblem of music description, and a checklist of the com-\nponents necessary to deﬁne an instance of such a problem,\nit does not describe how to solve any speciﬁc problem of\nmusic description. We do not derive restrictions on any\nof the components of the problem deﬁnition, or show how\ndatasets should be constructed to guarantee an evaluation\ncan result in good estimates of real world performance.\nOur future work aims in these directions. We will inco-\nprorate the formalism of the design and analysis of com-\nparative experiments [2,21], which will help deﬁne the no-\ntions of relevance and validity when it comes to analyzing\nRMDS. We seek to incorporate notions of learning and in-\nference [13], e.g., to specify what constitutes the building\nof a “good” RMDS using a training dataset (where “good”\ndepends on the use case). We also seek to explain more\nformally two paradoxes that have been observed. First,\nthough an RMDS is evaluated in a test dataset to repro-\nduce a large amount of ground truth, it appears to not be\na result of the consideration of characteristics in the music\nuniverse [20]. Second, though artiﬁcial algorithms have\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n93none of the extensive experience humans have in music lis-\ntening, description, and culture, they can reproduce ground\ntruth consisting of extremely subjective and culturally cen-\ntered concepts like genre [11].\n5. ACKNOWLEDGMENTS\nThe work of BLS and VE is supported in part by l’Institut\nfranc ¸ais du Danemark, and ARCHIMEDE Labex (ANR-\n11-LABX- 0033).\n6. REFERENCES\n[1] J.-J. Aucouturier and E. Pampalk. Introduction – from\ngenres to tags: A little epistemology of music in-\nformation retrieval research. J. New Music Research,\n37(2):87–92, 2008.\n[2] R. A. Bailey. Design of comparative experiments.\nCambridge University Press, 2008.\n[3] M. Barthet, G. Fazekas, and M. Sandler. Multidis-\nciplinary perspectives on music emotion recognition:\nImplications for content and context-based models. In\nProc. CMMR, 2012.\n[4] T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic\ntagging of audio: The state-of-the-art. In W. Wang, edi-\ntor,Machine Audition: Principles, Algorithms and Sys-\ntems. IGI Publishing, 2010.\n[5] T. Bertin-Mahieux, D. Ellis, B. Whitman, and\nP. Lamere. The million song dataset. In Proc. ISMIR,\n2011.\n[6] A. Craft, G. A. Wiggins, and T. Crawford. How many\nbeans make ﬁve? The consensus problem in music-\ngenre classiﬁcation and a new evaluation method for\nsingle-genre categorisation systems. In Proc. ISMIR,\npages 73–76, 2007.\n[7] R. B. Dannenberg, B. Thom, and D. Watson. A ma-\nchine learning approach to musical style recognition.\nInProc. ICMC, pages 344–347, 1997.\n[8] E. R. Dougherty and L. A. Dalton. Scientiﬁc knowl-\nedge is possible with small-sample classiﬁcation.\nEURASIP J. Bioinformatics and Systems Biology,\n2013:10, 2013.\n[9] J. Stephen Downie, Donald Byrd, and Tim Crawford.\nTen years of ISMIR: Reﬂections on challenges and op-\nportunities. In Proc. ISMIR, pages 13–18, 2009.\n[10] A. Flexer. Statistical evaluation of music informa-\ntion retrieval experiments. J. New Music Research,\n35(2):113–120, 2006.\n[11] J. Frow. Genre. Routledge, New York, NY , USA, 2005.\n[12] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of\naudio-based music classiﬁcation and annotation. IEEE\nTrans. Multimedia, 13(2):303–319, Apr. 2011.[13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements\nof Statistical Learning: Data Mining, Inference, and\nPrediction. Springer-Verlag, 2 edition, 2009.\n[14] A. W. Kimball. Errors of the third kind in sta-\ntistical consulting. J. American Statistical Assoc.,\n52(278):133–142, June 1957.\n[15] E. Law, L. von Ahn, R. B. Dannenberg, and M. Craw-\nford. Tagatune: A game for music and sound annota-\ntion. In Proc. ISMIR, pages 361–364, 2007.\n[16] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof audio-based music similarity and genre classiﬁca-\ntion. In Proc. ISMIR, pages 628–233, Sep. 2005.\n[17] R. Sedgewick and K. Wayne. Algorithms. Addison-\nWesley, Upper Saddle River, NJ, 4 edition, 2011.\n[18] C. N. Silla, A. L. Koerich, and C. A. A. Kaestner. The\nLatin music database. In Proc. ISMIR, 2008.\n[19] M. Slaney. Auditory toolbox. Technical report, Interval\nResearch Corporation, 1998.\n[20] B. L. Sturm. Classiﬁcation accuracy is not enough: On\nthe evaluation of music genre recognition systems. J.\nIntell. Info. Systems, 41(3):371–406, 2013.\n[21] B. L. Sturm. Making explicit the formalism underlying\nevaluation in music information retrieval research: A\nlook at the MIREX automatic mood classiﬁcation task.\nInPost-proc. Computer Music Modeling and Research,\n2014.\n[22] B. L. Sturm. The state of the art ten years after a state of\nthe art: Future research in music information retrieval.\nJ. New Music Research, 43(2):147–172, 2014.\n[23] B. L. Sturm. A simple method to determine if a music\ninformation retrieval system is a “horse”. IEEE Trans.\nMultimedia, 2014 (in press).\n[24] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\nsound effects. IEEE Trans. Audio, Speech, Lang. Pro-\ncess., 16, 2008.\n[25] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. Speech Audio Pro-\ncess., 10(5):293–302, July 2002.\n[26] J. Urbano. Evaluation in Audio Music Similarity . PhD\nthesis, University Carlos III of Madrid, 2013.\n[27] J. Urbano, M. Schedl, and X. Serra. Evaluation in\nmusic information retrieval. J. Intell. Info. Systems,\n41(3):345–369, Dec. 2013.\n[28] G. A. Wiggins. Semantic gap?? Schemantic schmap!!\nMethodological considerations in the scientiﬁc study\nof music. In Proc. IEEE Int. Symp. Mulitmedia, pages\n477–482, Dec. 2009.\n[29] Y .-H. Yang and H. H. Chen. Music Emotion Recogni-\ntion. CRC Press, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n94"
    },
    {
        "title": "The Kiki-Bouba Challenge: Algorithmic Composition for Content-based MIR Research and Development.",
        "author": [
            "Bob L. Sturm",
            "Nick Collins"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416364",
        "url": "https://doi.org/10.5281/zenodo.1416364",
        "ee": "https://zenodo.org/records/1416364/files/SturmC14.pdf",
        "abstract": "We propose the “Kiki-Bouba Challenge” (KBC) for the re- search and development of content-based music informa- tion retrieval (MIR) systems. This challenge is unencum- bered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imper- fect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irrepro- ducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition — a hitherto under-exploited resource for MIR.",
        "zenodo_id": 1416364,
        "dblp_key": "conf/ismir/SturmC14",
        "keywords": [
            "Kiki-Bouba Challenge",
            "content-based music information retrieval",
            "unencumbered by problems",
            "free data",
            "perfect ground truth",
            "well-specifiable characteristics",
            "open source algorithmic composition",
            "under-exploited resource",
            "imperfect copyrights",
            "irreproducibility"
        ],
        "content": "THE KIKI-BOUBA CHALLENGE: ALGORITHMIC COMPOSITION FOR\nCONTENT-BASED MIR RESEARCH & DEVELOPMENT\nBob L. Sturm\nAudio Analysis Lab, Aalborg University, Denmark\nbst@create.aau.dkNick Collins\nDept. Music, Durham University, UK\nnick.collins@durham.ac.uk\nABSTRACT\nWe propose the “Kiki-Bouba Challenge” (KBC) for the re-\nsearch and development of content-based music informa-\ntion retrieval (MIR) systems. This challenge is unencum-\nbered by several problems typically encountered in MIR\nresearch: insufﬁcient data, restrictive copyrights, imper-\nfect ground truth, a lack of speciﬁc criteria for classes (e.g.,\ngenre), a lack of explicit problem deﬁnition, and irrepro-\nducibility. KBC provides a limitless amount of free data, a\nperfect ground truth, and well-speciﬁable and meaningful\ncharacteristics deﬁning each class. These ideal conditions\nare made possible by open source algorithmic composition\n— a hitherto under-exploited resource for MIR.\n1. INTRODUCTION\nBefore attempting to solve a complex problem, one should\napproach it by ﬁrst demonstrably solving simpler, well-\ndeﬁned, and more restricted forms, and only then increase\nthe complexity. However, there are key problems of re-\nsearch in content-based music information retrieval (MIR)\n[8] where this has yet to be done. For example, much of\nthe enormous amount of research that attempts to address\nthe problem of music genre recognition (MGR) [26] has\nstarted with genre in the “real world” [30]. The same is\nseen for research in music mood recognition [28, 29, 37],\nand music autotagging [6]. On top of this, the problem of\ndescribing music using genre, mood, or tags in general, has\nrarely, if ever, been explicitly deﬁned [32].\nIn lieu of an explicit deﬁnition of the problem, the most\ncommon approach in much of this research is to implic-\nitly deﬁne it via datasets of real music paired with “ground\ntruth.” The problem then becomes reproducing as much\nof the “ground truth” as possible by pairing feature ex-\ntraction and machine learning algorithms, and comparing\nthe resulting numbers to those of other systems (includ-\ning humans). Thousands of numerical results and pub-\nlications have so far been produced, but it now appears\nas if most of it has tenuous relevance for content-based\nMIR [3, 27, 30, 31, 34]. The crux of the argument is that\nthe lack of scientiﬁc validity in evaluation in much of this\nc\rBob L. Sturm, Nick Collins.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Bob L. Sturm, Nick Collins. “The\nKiki-Bouba Challenge: Algorithmic composition for content-based MIR\nResearch & Development”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.work [3, 27, 30] has led to the development of many MIR\nsystems that appear as if they are “listening” to the music\nwhen they are actually just exploiting confounded charac-\nteristics in a test dataset [31]. Thus, in order to develop\nMIR systems that address the goal of “making music, or\ninformation about music, easier to ﬁnd” [8] in the real-\nworld, there is a need to ﬁrst demonstrably solve simple,\nwell-deﬁned and restricted problems.\nToward this end, this paper presents the “Kiki-Bouba\nChallenge” (KBC), which is essentially a simpliﬁcation\nof the problem of MGR. On a higher level, we propose\nKBC to refocus the goals in content-based MIR. We de-\nvise KBC such that solving it is unencumbered by six sig-\nniﬁcant problems facing content-based MIR research and\ndevelopment: 1) the lack of formal deﬁnition of retriev-\ning information in recorded music; 2) the large amount\nof data necessary to ensure representativeness and gen-\neralization for machine learning; 3) the problem of ob-\ntaining “ground truth”; 4) the stiﬂing affect of intellectual\nproperty (e.g., music copyright) on collecting and sharing\nrecorded music; 5) the lack of validity of standard evalua-\ntion approaches of systems; and 6) a lack of reproducible\nresearch. KBC employs algorithmic composition to gener-\nate a limitless amount of music from two categories, named\nKiki andBouba. Music from each category are thereby\nfree from copyright, are based in well-deﬁned programs,\nand have a perfect ground truth. Solving KBC represents a\nveritable contribution of content-based MIR research and\ndevelopment, and promises avenues for solving parallel\nproblems in less restricted and real-world domains.\nInstead of being merely the reproduction of a “ground\ntruth” of some dataset, the MIR “ﬂagship application” of\nMGR [4] — and that which KBC simpliﬁes — has as its\nprincipal goals the imitation of the human ability to or-\nganize, recognize, distinguish between, and imitate gen-\nres used by music [28]. To “imitate the human ability” is\nnot necessarily to replicate the physiological processes hu-\nmans use to hear, process and describe a piece of music, but\nmerely to describe as humans do a piece of music accord-\ning to its content, e.g., using such musically meaningful at-\ntributes as rhythm, instrumentation, harmonic progression,\nor formal structure. Solving the problem of MGR means\ncreating an artiﬁcial system that can work with music like\nhumans, but unencumbered by human limitations.\nThe concept of genre [12,13,16] is notoriously difﬁcult\nto deﬁne such that it can be addressed by algorithms [23].\nResearchers building MGR systems have by and large posed\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n21the problem, implicitly or explicitly, from an Aristotelean\nviewpoint, i.e., “genre” is a categorization of music just as\n“species” is a categorization of living things, e.g., [5,33].1\nThe problem then is to automatically learn the characteris-\ntics that place a piece of music on one branch of a taxon-\nomy, distinguish it from a piece of music on a different\nbranch, and avoid contradiction in the process [7]. Re-\nsearchers have combined signal processing and machine\nlearning with datasets of real music recordings in hopes\nthat the resulting system can discover Aristotelean criteria\nby which music can be categorized according to genre. The\nmajority of the resulting work, however, documents how\nmuch “ground truth” an algorithm replicates in benchmark\ndatasets [26], but rarely illuminates the criteria a system\nhas learned and is using to categorize music [27]. The for-\nmer quantity is meaningless when the latter is senseless.\nIn the next section, we discuss the use of algorithmic\nmusic composition for data generation. Then we present\nKBC in its most general form. We follow this with a con-\ncrete and speciﬁc realisation of KBC, available at the rele-\nvant webpage: http://composerprogrammer.com/\nkikibouba.html. We present an unacceptable solution\nto KBC, and discuss aspects of an acceptable solution. We\nconclude this paper with a discussion of KBC, and how it\nrelates to content-based MIR in the “real world.”\n2. ALGORITHMIC MUSIC COMPOSITION FOR\nGENERATING DATA\nAlgorithmic composition [1,9,19,21,22,25,36] has a long\nhistory back to mainframe computer experiments in the\nmid 1950s, predating by a decade MIR’s ﬁrst explicit pa-\nper [17]. Ames and Domino [2] differentiate empirical\nstyle modeling (of historic musical styles) and active style\nsynthesis (of novel musical style). In the practical work\nof this article we concentrate more on the latter, but there\nis a rich set of techniques for basing generation of music\non models trained on existing musical data. Many musi-\ncal models deployed to capture regularities in data sets are\ngenerative, in that a model trained from a corpus can gen-\neralise to production of new examples in that style [11].\nThough anticipated by some authors, it is surprising\nhow few studies in computer music have utilised algorith-\nmic composition to create the ground truth. Although [24]\npresent a four category taxonomy of algorithmic compo-\nsition, they do not explicitly discuss the option of using\nalgorithmic composition to produce data sets. The closest\ncategory is where “theories of a musical style are imple-\nmented as computer programs” [24], essentially empirical\nstyle modeling as above.\nSample CD data, especially meta-data on splices, have\nalso rarely been used. But the advantage of algorithmic\ncomposition techniques are the sheer volume of data which\ncan potentially be generated, and appropriately handled\nshould be free of the copyright issues that plague databases\nof music recordings and hinder research access.\nWe believe that algorithmic generation of datasets within\n1This of course belies the profound issues that biologists face in rec-\nognizing “speciation” events [10].a framework of open source software has the following po-\ntential beneﬁts to MIR and computer music analysis:\n\u000fLimitless data set generation, with perfect ground truth\n(the originating program is fully accessible, and can\nbe devised to log all necessary elements of the ground\ntruth during generation. Random seeds can be used to\nrecover program runs exactly as necessary)\n\u000fA fully controlled musical working space, where all as-\nsumptions and representational decisions are clear\n\u000fCopyright free as long as license free samples or pure\nsynthesis methods are utilised, under appropriate soft-\nware licensing\n\u000fEstablished data sets can be distributed free of the origi-\nnating software once accepted by the community, though\ntheir origins remain open to investigation by any inter-\nested researcher\nThe greatest issue with dependence on algorithmic gen-\neration of music is the ecological validity of the music be-\ning generated. A skeptic may question the provenance of\nthe music, especially with respect to the established cul-\ntural and economically proven quality of existing human\ndriven recorded music production. Nonetheless, humans\nare intimately involved in devising algorithmic composi-\ntion programs. We believe that there is place for expert\njudgement here, where experts in algorithmic composition\ncan become involved in the process of MIR evaluation.\nThe present paper serves as one humble example; but ul-\ntimately, a saving grace of any such position is that the\ngeneration code is fully available, and thus accessible to\nreproduction and evaluation by others.\n3. THE KIKI-BOUBA CHALLENGE\nWe now present KBC in its most general form: develop a\nsystem that can organize, recognize, distinguish between,\nand imitate Aristotelean categories of “music. ” We de-\nﬁne these in the subsections below, after we specify the\ndomain.\n3.1 Domain\nThe music universe of KBC is populated by “music” be-\nlonging to either one of two categories, Kiki andBouba.2\nIn KBC, music from either category is algorithmically com-\nposed such that there is available a limitless number of\nrecordings of music from both categories, and which are\nentirely unencumbered by copyrights. A music recording\nfrom this universe therefore embeds music from Kiki and\nnot from Bouba, or vice versa, for several reasons that are\nneither ambiguous nor disputable, and which can be com-\npletely garnered from the music recording. The ground\ntruth of a dataset of recordings of music from the music\nuniverse then is absolute. Note that a music recording need\nnot be an audio recording, but can be a notated score, or\nother kind of representation. Now, given that this is ideal\n2Shapes named “Kiki” and “Bouba” (the two are spiky and rounded,\nrespectively) were originally introduced in gestalt psychology to inves-\ntigate cross-cultural associations of visual form and language [18, 20].\nOur example realization of KBC involves two distinctive artiﬁcial musi-\ncal “genres” meant to illustrate in sonic terms a similar opposition.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n22Attribute Kiki Bouba\nForm Alternating accelerando rises and crazy section (“freak\nout”)Steady chorale\nRhythm Accelerando and complex “free” rhythm, fast Limited set of rhythmic durations, slow\nPitch Modulo octave tuning system Recursive subdivision tuning system\nDynamics Fade ins and outs during accelerando and close of\n“freak out” sectionsSingle dynamic\nVoicing All voices in most of the time Arch form envelope of voice density, starting and ending with\nsingle voice\nTimbre Percussive sounds alongside fast attack and decay\nbright pitched sounds. Second rise has an additional\nsiren sound.Slow attack and decay sounds with initial portamento and vi-\nbrato, with an accompanying dull thud\nHarmony Accidental coincidences only, no overall precepts System of tonality, with a harmonic sequence built from rela-\ntively few possible chords\nTexture More homophonic in accelerando, heterogenous with\nindependent voices in “freak out” sectionsHomophonic, homogenous\nExpression Ensemble timing loose on accelerando, independent\nduring “freak out” sectionsDetails of vibrato, portamento and “nervousness” (chance of\nsounding on a given chord) differ for each voice in the texture\nSpace Little or no reverb Very reverberant\nTable 1. Musical attributes of our realization of Kiki andBouba.\nfor toolboxes of algorithms in an Aristotelean world, we\npose the following tasks.\n3.2 The discrimination task (unsupervised learning)\nGiven an unlabelled collection of music recordings from\nthe music universe, build a system that determines there\nexist two categories in this music universe, andhigh-level\n(content) criteria that discriminate them. In machine learn-\ning, this can be seen as unsupervised learning, but ensuring\ndiscrimination is caused by content and not criteria that are\nirrelevant to the task.\n3.3 The identiﬁcation task (supervised learning)\nGiven a labelled collection of music recordings from the\nmusic universe, build a system that can learn to identify,\nusing high-level (content) criteria, recordings of music (ei-\nther from this music universe or from others) as being from\nKiki, Bouba, or from neither. In machine learning, this can\nbe seen as supervised learning, but ensuring identiﬁcation\nis caused by content and not criteria that are irrelevant to\nthe task.\n3.4 The recognition task (retrieval)\nGiven a labelled collection of music recordings from this\nmusic universe, build a system that can recognize content\ninreal world music recordings as being similar to contents\nin music from Kiki, Bouba, both, or neither. In information\nretrieval, this can be seen as relevance ranking.\n3.5 The composition task (generation)\nGiven a labelled collection of music recordings from this\nmusic universe, build a system that composes music hav-\ning content similar to music from Kiki, and/or music from\nBouba. The rules that the system uses to create the mu-\nsic must themselves be meaningful. For example, a mu-\nsic analyst would ﬁnd the program that generates the mu-\nsic to provide a high-level breakdown of the characteristics\nof a category. In one sense, this challenge is a necessary\nprecursor to those above, in that a human composer must\ndesign the ground truth of the music universe. The pro-\nduction of a dataset of music recordings with algorithmiccomposition necessitates creation in real musical terms.\nThe machine challenge here is to backwards engineer, or\nto learn in short, the compositional ability to work in the\npre-established music universe. However, backwards en-\ngineering the compositional mechanisms of such a system,\nas an expert human musician can potentially do when en-\ncountering a musical style unfamiliar to them, is itself an\nimportant challenge of high-level musical understanding.\n4. AN EXAMPLE REALIZATION OF KBC\nWe now present an example realization of KBC. We spec-\nifyKiki andBouba via computer programs for algorithmic\ncomposition, which we use to create unlimited recordings\nof music from Kiki andBouba, each varying subtly in the\nﬁne details (we discuss the practical range of this variation\nfurther below). Our computer program is written in the\nSuperCollider audio programming language [35], with Su-\nperCollider used here in non-realtime mode for fast synthe-\nsis of music recordings (which in this case are monophonic\ndigital audio ﬁles). We measure the speed of generation of\nmusic recordings to be around 60\u0002real-time, so that one\npiece of around one minute can be created every second\nby our code. With this we easily created a multi-gigabyte\ndataset of ten hours, and could very easily create far more.\nAsKiki andBouba are designed here by humans, they\nare not independent of “real” music, even though they are\nfully speciﬁed via open source code.3Table 3 outlines\nproperties of music from Kiki andBouba with respect to\nsome high and low level musical properties. This conveys\na sense of why Kiki andBouba are well-differentiated in\nmusically meaningful ways. Figure 1 further attempts to\nillustrate the formal structure of the two styles, again as a\ndemonstration of their distinctiveness. Although the musi-\ncal description is not as simple as the visual manifestation\nof the original shapes of “kiki” and “bouba” [18,20], it was\ndesigned to avoid too much overlap of musical character-\nistics. Each output piece is around 40-60 seconds, since\n3We make available this source code, as well as a few repre-\nsentative sound examples at the accompanying webpage: http://\ncomposerprogrammer.com/kikibouba.html.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n23Figure 1. Comparative musical forms of our realization of\nmusic from Kiki andBouba (labeled).\nthe actual length of sections is itself generative. It is be-\nyond the scope of this article to discuss every detail of the\ncode and the variability of output allowed, but this gives\nsome idea. To anthropomorphise and allow a little liter-\nary conceit, our realization envisages music from Kiki to\nbe ecstatic, chaotic and ritualistic, characterised by alter-\nnating build-ups (accelerando rises) and cathartic “freak-\nouts.” Our realization envisages music from Bouba as an\nabstract choral funeral march, steady and affected.\n4.1 An unacceptable solution\nA typical approach to attempt to address an identiﬁcation\ntask is by computing a variety of low-level and short-time\nfeatures from music recordings, modelling collections of\nthese by probability distributions (bags of frames), and spec-\nifying criteria for classiﬁcation, such as maximum likeli-\nhood. To this end, we use supervised learning to build\na single nearest neighbor classiﬁer trained with features\ncomputed from a dataset consisting of 250 recordings of\nmusic from Kiki and 250 from Bouba. As features, we ﬁrst\ncompute the number of zero crossings for 46.3 ms Hann-\nwindowed audio frames, overlapped 50% across the en-\ntire recording. We then compute the mean and variance\nof the number of zero crossings from texture windows of\n129 consecutive frames. Finally, we normalize the feature\ndimensions in the training dataset observations, and use\nthe same normalization parameters to transform input ob-\nservations. Figure 2 shows a scatter plot of these training\ndataset observations. To classify an input music recording\nas being of music from Kiki orBouba, we use majority\nvote from the nearest neighbor classiﬁcation of the ﬁrst 10\nconsecutive texture windows.\nWe test the system using a stratiﬁed test dataset of 500\nmusic recordings from Kiki orBouba. For each input, we\ncompare the system output to the ground truth. Our sys-\ntem produces a classiﬁcation error of 0.00! It has thus suc-\ncessfully labeled all observations in the test dataset with\nthe correct answer. However, this system is not a solu-\ntion to the identiﬁcation task of KBC, let alone the three\nother KBC tasks, simply because it is not using high-level\ncriteria (content). Of course, the statistics of low-level\nzero crossings across short-time frames has something to\ndo with content [15], but this relationship is quite far re-\nmoved and ambiguous. In other words, people listen to\nand describe music in terms related to key, tempo and tim-\nbre, but not zero crossings. Statistics of zero crossings are\n0 0.2 0.4 0.6 0.8 100.20.40.60.81\nNormalized Mean ZCNormalized Var ZC\n  \nKiki\nBoubaFigure 2. Scatter plot of features extracted from recordings\nof music from Kiki andBouba.\nnot meaningful musical information for solving any task\nof KBC. That this feature contributes to the perfect ﬁgure\nof merit of this system, it does not illuminate what makes\nmusic Kiki, and what makes music Bouba.\n4.2 An acceptable solution\nAs of the current time, we have yet to ﬁnd any acceptable\nsolution to our realization of KBC, or any of its tasks —\nwhich motivates this challenge. (Furthermore, as discussed\nbelow, the goal of KBC is not “a solution” but “solving.”)\nWe can, however, describe aspects of solutions acceptable\nfor our speciﬁc realization of KBC. An acceptable solution\nto the discrimination task determines that in a set of music\nrecordings from the music universe, there exist two differ-\nent kinds of music, which are discriminable by high-level\ncontent, some of which are listed in Table 3, and shown in\nFig. 1. An acceptable solution to the identiﬁcation task de-\ntermines for any given music recording whether its high-\nlevel contents are or are not consistent with all the musi-\ncal attributes of Kiki orBouba. An acceptable solution to\nthe recognition task might recognize as Bouba characteris-\ntics the slow plodding rhythm, wailing timbre, and homo-\nphonic texture of some jazz funeral music. It might recog-\nnize as Kikicharacteristics the glissando siren of some rave\nmusic, or the complex, unpredictable and ametrical rhythm\nof some free improvisation. It would recognize as not char-\nacteristic of either Kiki orBouba the form of 12-bar blues.\nFinally, an acceptable solution to the composition task gen-\nerates music that mimics particular characteristics of music\nfrom Kiki andBouba.\n5. DISCUSSION\nIn essence, KBC is a general exercise, of which we have\nprovided one realization. KBC simpliﬁes MGR — and\nmusic description in general — to the degree that many\nproblems typically encountered in MIR research are not\nan issue, i.e., lack of data, copyright restrictions, cost and\ninaccuracy of ground truth, poor problem deﬁnition, and\nevaluations that lack validity with respect to meaningful\nmusical understanding by machine. While most research\nin MGR searches for an Aristotelean categorization of real\nmusic (or the reverse engineering of the categorization used\nto create benchmark music datasets like GTZAN [30,33]),\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n24it sustains most of the complexity inherent to the problem\nof MGR. KBC simpliﬁes it to be Aristotelean and well-\ndeﬁned. Essentially, KBC deﬁnes categories of music as\nwell-speciﬁed and open-source programs, which comports\nwith an Aristotelean conception of music genre. This al-\nlows us to beneﬁt from algorithmic composition since we\ncan generate from these programs any quantity of data, free\nof copyright, and with a perfect ground truth and speciﬁed\nclassiﬁcation criteria.\nIt can be speculated that KBC is too much of a simpli-\nﬁcation of MGR, that deﬁning music using programs has\nlittle “ecological validity,” and thus that a solution to KBC\nwill be of little use for music in the “real world.” To the\nﬁrst claim, the tasks of KBC are much more complex than\nreproducing ground truth labels of datasets by any means\n— the implicit goal of the majority of work addressing\nMGR [27, 30] — because solving the tasks requires ma-\nchine listening, i.e., “intelligent, automated processing of\nmusic” [8]. To the second claim, our realizations of music\nfrom Kiki andBouba actually originate in higher-level mu-\nsical processes deﬁned by humans trained and practiced\nin music composition. Fundamentally, “algorithmic mu-\nsic” and “non-algorithmic music” is a false dichotomy; but\nthis is not to say all algorithms create equally “valid” mu-\nsic. One non-sensical realization of KBC is deﬁning music\nfrom Kiki andBouba as 50 ms long compositions, each\nconsisting of a single sine, but with frequencies separated\nby 1 Hz between the two categories. To the ﬁnal claim, we\nemphasize an important distinction between “a solution to\nKBC” and “solving KBC.” We are not claiming that, e.g.,\na system that has learned to discriminate between music\nfrom Kiki andBouba will be useful for discriminating be-\ntween “real” music using any two “real” genres. The sys-\ntem (the actual ﬁnished product and black box [29]) will\nlikely be useless. Rather, solving KBC is the goal because\nthis requires developing a system that demonstrates a ca-\npacity to listen to acoustic signals in ways that consider\nhigh level (musical) characteristics.\nIf one desires more complexity than KBC offers, one\ncan conceive of a music universe with more than two cate-\ngories, and/or various mixings of “base” categories, e.g.,\ngiving rise to cross-genres Bouki andKiba (the code at\nour link already has the capacity to generate these hybrid\nforms). However, we contend the best strategy is to ﬁrst\ndemonstrably solve the simplest problems before tackling\nones of increased difﬁculty. If the components of a pro-\nposed MGR system result in a system that does not solve\nKBC, then why should they be expected to result in a sys-\ntem that can discriminate between, or identify, or recog-\nnize, or compose music using “real” genres of music from\na limited amount of data having a ground truth output by\na complex culturally negotiated system that cannot be as\nunambiguously speciﬁed as Kiki andBouba?\n6. CONCLUSION\nSimply described, content-based MIR research and devel-\nopment aims to design and deploy artiﬁcial systems that\nare useful for retrieving, using or making music content.\nThe enormous number of published works [6, 14, 26, 38],not to mention the participation during the past ten years\nof MIREX,4show many researchers are striving to build\nmachine listening systems that imitate the human ability to\nlisten to, search for, and describe music. Examples of such\nresearch include music genre recognition, music mood recog-\nnition, music retrieval by similarity, cover song identiﬁca-\ntion, and various aspects of music analysis, such as rhyth-\nmic and harmonic analysis, melody extraction, and seg-\nmentation. These pursuits, however, are hindered by sev-\neral serious problems: a limited amount of data, the shar-\ning of which is restricted by copyright; the problematic na-\nture of obtaining “ground truth,” and explicitly deﬁning its\nrelationship to music content; and a lack of validity in the\nevaluation of content-based MIR systems with respect to\nthe task they are supposedly addressing. We are thus left to\nask: Have the simplest problems been demonstrably solved\nyet?\nIn this paper, we show how algorithmic music com-\nposition facilitates limitless amounts of data, with perfect\nground truth and no restricting copyright, thus holding ap-\npreciable potential for MIR research and development We\npropose the “Kiki-Bouba Challenge” (KBC) as a simpli-\nﬁcation of the problem of MGR, and produce an exam-\nple realization of it facilitated by algorithmic composition.\nWe do not present an acceptable solution to our realiza-\ntion of KBC, but discuss aspects of such a solution. We\nalso illustrate an unacceptable solution, which fails to re-\nveal anything relating to musical meaning even though it\nstill perfectly labels a test dataset. We emphasize, the goal\nof KBC is not the system itself, but in solving the challenge.\nSolving KBC changes the incentive of research and devel-\nopment in content-based MIR from one of developing sys-\ntems obtaining high ﬁgures of merit by any means, to one\nof developing systems obtaining high ﬁgures of merit by\nrelevant means.\n7. ACKNOWLEDGMENTS\nWe wish to acknowledge the anonymous reviewers who\nhelped signiﬁcantly in the revision of this paper. The work\nof BLS was supported in part by Independent Postdoc Grant\n11-105218 from Det Frie Forskningsr ˚ad.\n8. REFERENCES\n[1] C. Ames. Automated composition in retrospect: 1956-\n1986. Leonardo, 20(2):169–185, 1987.\n[2] C. Ames and M. Domino. Cybernetic Composer: an\noverview. In M. Balaban, K. Ebcio ˘glu, and O. Laske,\neditors, Understanding Music with AI: Perspectives\non Music Cognition, pages 186–205. The AAAI\nPress/MIT Press, Menlo Park, CA, 1992.\n[3] J.-J. Aucouturier and E. Bigand. Seven problems that\nkeep MIR from attracting the interest of cognition and\nneuroscience. J. Intell. Info. Systems, 41(3):483–497,\n2013.\n4http://www.music-ir.org/mirex\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n25[4] J.-J. Aucouturier and E. Pampalk. Introduction – from\ngenres to tags: A little epistemology of music in-\nformation retrieval research. J. New Music Research,\n37(2):87–92, 2008.\n[5] J. G. A. Barbedo and A. Lopes. Automatic genre classi-\nﬁcation of musical signals. EURASIP J. Adv. Sig. Pro-\ncess., 2007:064960, 2007.\n[6] T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic\ntagging of audio: The state-of-the-art. In W. Wang, edi-\ntor,Machine Audition: Principles, Algorithms and Sys-\ntems. IGI Publishing, 2010.\n[7] G. C. Bowker and S. L. Star. Sorting things out: Clas-\nsiﬁcation and its consequences. The MIT Press, 1999.\n[8] M. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges. Proc. IEEE, 96(4):668–696, Apr. 2008.\n[9] D. Cope, editor. Virtual Music : Computer Synthesis of\nMusical Style. MIT Press, Cambridge, MA, 2001.\n[10] D. C. Dennett. Darwin’s Dangerous Idea: Evolution\nand the Meanings of Life. Simon and Schuster, 1996.\n[11] S. Dubnov, G. Assayag, O. Lartillot, and G. Bejer-\nano. Using machine learning methods for musical style\nmodelling. Computer, pages 73–80, October 2003.\n[12] F. Fabbri. A theory of musical genres: Two applica-\ntions. In Proc. Int. Conf. Popular Music Studies, 1980.\n[13] J. Frow. Genre. Routledge, New York, NY , USA, 2005.\n[14] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of\naudio-based music classiﬁcation and annotation. IEEE\nTrans. Multimedia, 13(2):303–319, Apr. 2011.\n[15] F. Gouyon, F. Pachet, and O. Delrue. On the use of\nzero-crossing rate for an application of classiﬁcation\nof percussive sounds. In Proc. DAFx, 2000.\n[16] Scott Johnson. The counterpoint of species. In J. Zorn,\neditor, Arcana: Musicians on Music, pages 18–58.\nGranary Books, Inc., New York, NY , 2000.\n[17] M. Kassler. Towards musical information retrieval.\nPerspectives of New Music, 4(2):59–67, 1966.\n[18] W. K ¨ohler. Gestalt Psychology. Liveright, New York,\n1929.\n[19] J. McCormack and M. d’Inverno. Computers and Cre-\nativity. Springer-Verlag, Berlin Heidelberg, 2012.\n[20] E. Milan, O. Iborra, M. J. de Cordoba, V . Juarez-\nRamos, M. A. Rodr ´ıguez Artacho, and J. L. Rubio. The\nkiki-bouba effect a case of personiﬁcation and ideaes-\nthesia. J. Consciousness Studies, 20(1-2):1–2, 2013.\n[21] E. R. Miranda, editor. Readings in Music and Artiﬁcial\nIntelligence. Harwood Academic Publishers, Amster-\ndam, 2000.[22] G. Nierhaus. Algorithmic Composition: Paradigms of\nAutomated Music Generation. Springer-Verlag/Wien,\nNew York, NY , 2009.\n[23] F. Pachet and D. Cazaly. A taxonomy of musical gen-\nres. In Proc. Content-based Multimedia Information\nAccess Conference, Paris, France, Apr. 2000.\n[24] M. Pearce, D. Meredith, and G. Wiggins. Motivations\nand methodologies for automation of the composi-\ntional process. Musicae Scientiae, 6(2):119–147, 2002.\n[25] C. Roads. Research in music and artiﬁcial intelligence.\nComputing Surveys, 17(2), June 1985.\n[26] B. L. Sturm. A survey of evaluation in music genre\nrecognition. In Proc. Adaptive Multimedia Retrieval,\nOct. 2012.\n[27] B. L. Sturm. Classiﬁcation accuracy is not enough: On\nthe evaluation of music genre recognition systems. J.\nIntell. Info. Systems, 41(3):371–406, 2013.\n[28] B. L. Sturm. Evaluating music emotion recognition:\nLessons from music genre recognition? In Proc.\nICME, 2013.\n[29] B. L. Sturm. Making explicit the formalism underlying\nevaluation in music information retrieval research: A\nlook at the MIREX automatic mood classiﬁcation task.\nInPost-proc. Computer Music Modeling and Research,\n2014.\n[30] B. L. Sturm. The state of the art ten years after a state of\nthe art: Future research in music information retrieval.\nJ. New Music Research, 43(2):147–172, 2014.\n[31] B. L. Sturm. A simple method to determine if a music\ninformation retrieval system is a “horse”. IEEE Trans.\nMultimedia, 2014 (in press).\n[32] B. L. Sturm, R. Bardeli, T. Langlois, and V . Emiya.\nFormalizing the problem of music description. In Proc.\nISMIR, 2014.\n[33] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. Speech Audio Pro-\ncess., 10(5):293–302, July 2002.\n[34] J. Urbano, M. Schedl, and X. Serra. Evaluation in\nmusic information retrieval. J. Intell. Info. Systems,\n41(3):345–369, Dec. 2013.\n[35] S. Wilson, D. Cottle, and N. Collins, editors. The Su-\nperCollider Book. MIT Press, Cambridge, MA, 2011.\n[36] I. Xenakis. Formalized Music. Pendragon Press,\nStuyvesant, NY , 1992.\n[37] Y .-H. Yang, D. Bogdanov, P. Herrera, and M. Sordo.\nMusic retagging using label propagation and robust\nprincipal component analysis. In Proc. Int. Conf. Com-\npanion on World Wide Web, pages 869–876, 2012.\n[38] Y .-H. Yang and H. H. Chen. Music Emotion Recogni-\ntion. CRC Press, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n26"
    },
    {
        "title": "Sparse Cepstral, Phase Codes for Guitar Playing Technique Classification.",
        "author": [
            "Li Su 0002",
            "Li-Fan Yu",
            "Yi-Hsuan Yang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417215",
        "url": "https://doi.org/10.5281/zenodo.1417215",
        "ee": "https://zenodo.org/records/1417215/files/SuYY14.pdf",
        "abstract": "Automatic recognition of guitar playing techniques is chal- lenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our eval- uation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse represen- tations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos.",
        "zenodo_id": 1417215,
        "dblp_key": "conf/ismir/SuYY14",
        "keywords": [
            "Automatic recognition",
            "guitar playing techniques",
            "subtle nuances",
            "comparative study",
            "features extracted",
            "magnitude spectrum",
            "cepstrum",
            "phase derivatives",
            "group-delay function",
            "instantaneous frequency deviation"
        ],
        "content": "SPARSE CEPSTRAL AND PHASE CODES FOR GUITAR PLAYING\nTECHNIQUE CLASSIFICATION\nLi Su, Li-Fan Yu and Yi-Hsuan Yang\nResearch Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan\nlisu@citi.sinica.edu.tw, a999frank@gmail.com, yang@citi.sinica.edu.tw\nABSTRACT\nAutomatic recognition of guitar playing techniques is chal-\nlenging as it is concerned with subtle nuances of guitar\ntimbres. In this paper, we investigate this research problem\nby a comparative study on the performance of features\nextracted from the magnitude spectrum, cepstrum and\nphase derivatives such as group-delay function (GDF) and\ninstantaneous frequency deviation (IFD) for classifying\nthe playing techniques of electric guitar recordings. We\nconsider up to 7 distinct playing techniques of electric\nguitar and create a new individual-note dataset comprising\nof 7 types of guitar tones for each playing technique. The\ndataset contains 6,580 clips and 11,928 notes. Our eval-\nuation shows that sparse coding is an effective means of\nmining useful patterns from the primitive time-frequency\nrepresentations and that combining the sparse represen-\ntations of logarithm cepstrum, GDF and IFD leads to\nthe highest average F-score of 71.7%. Moreover, from\nanalyzing the confusion matrices we ﬁnd that cepstral and\nphase features are particularly important in discriminating\nhighly similar techniques such as pull-off, hammer-on\nand bending. We also report a preliminary study that\ndemonstrates the potential of the proposed methods in\nautomatic transcription of real-world electric guitar solos.\n1. INTRODUCTION\nThe use of various instrumental techniques is essential in\nmusic. A practical, interpretable automatic transcription\nsystem should provide information about playing tech-\nniques in addition to information about pitch or onset. For\nexample, various ﬁngering styles of the guitar, such as\npull-off, hammer-on or bending, are all important elements\nof a guitar performance. A novice guitar player might\nbe eager to learn the playing techniques employed in\na musical excerpt of interest. Similar to some popular\nonline automatic chord recognizer (e.g. Chordify1), a tool\ntranscribing the note-by-note playing techniques of a guitar\nrecording enhances the interactivity of music learning\n1http://chordify.net/\nc\rLi Su, Li-Fan Yu and Yi-Hsuan Yang.\nLicensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Li Su, Li-Fan Yu and\nYi-Hsuan Yang. “Sparse cepstral and phase codes for guitar\nplaying technique classiﬁcation”, 15th International Society for Music\nInformation Retrieval Conference, 2014.or listening experiences, and thereby offers important\neducational, recreational and even cultural values.\nWhile extracting the pitch, onset, chord and instru-\nment information from a musical excerpt has received\ngreat attention in the music information retrieval (MIR)\ncommunity [3, 5, 16–18, 24], relatively little effort has\nbeen invested in transcribing the playing technique of\ninstruments [23]. In addition, due to the use of various\nguitar tones (i.e. audio effects such as distortion, reverb,\ndelay, and chorus effect) in everyday guitar performances,\nconventional timbre descriptors extracted from the spec-\ntrum might not be enough in modeling the electric guitar\nplaying techniques. For instance, as the chorus effect is\nusually implemented by temporal delay [6], information\nabout the phase spectrum might be important. On the other\nhand, for distortions that involve a ﬁltering effect, cepstral\nfeatures might be useful to characterize the respective\nsource and ﬁlter components [8].\nMotivated by the above observations, we present in\nthis paper a comparative study evaluating the accuracy\nof playing technique classiﬁcation of electric guitar using\na variety of spectral, cepstral and phase features. The\ncontribution of the paper is three-fold. First, to investigate\nmore subtle variation of musical timbre, we compile an\nopen dataset of 7 playing techniques of electric guitar,\ncovering a variety of pitches and 7 tones (cf. Section 4).\nWe have made the full dataset and its detailed information\navailable online.2Second, as feature learning tech-\nniques such as dictionary learning and deep learning have\ngarnered increasing attention in audio signal processing\n[12, 18, 22, 25], we evaluate the performance of sparse\nrepresentations of audio signals using a dictionary adapted\nto the signals of interest (Section 5). Our evaluation shows\nthat, to better model the playing techniques, it is useful\nto combine the sparse representation of different types of\nfeatures, such as logarithm cepstrum and phase derivatives\n(Section 6). Finally, a preliminary study using a guitar\nsolo demonstrates the potential of the proposed methods\nin automatic guitar transcription (Section 7).\n2. RELATED WORK\nDesigning useful musical timbre descriptors has been a\nlong-studied topic, and has achieved high performance in\nsome fundamental problems such as instrument classiﬁca-\n2http://mac.citi.sinica.edu.tw/\nGuitarTranscription\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n9tion of monophonic signals [13]. Nowadays, researchers\nturn to more challenging problems like multiple instru-\nment recognition, which deals with a highly complicated\ntimbre space [10]. Besides the complexity of multiple\ninstruments, another challenge in timbre classiﬁcation is\nto identify all the styles of timbre that one instrument can\nproduce, such as to identify the playing techniques of an\ninstrument. For exmaple, Abeßer et al. and Reboursi `ere\net al. [1, 21] pioneered the problem of automatic guitar\nplaying technique classiﬁcation, and used timber descrip-\ntors such as spectral ﬂux, weighted phase divergence,\nspectral crust factors, brightness, and irregularity, amongst\nothers. Most of these features are physically related to\nthe characteristics of a plucked, vibrating string. However,\nthese studies were not evaluated using a dataset comprising\nof various playing techniques and guitar tones.\nIn addition to larger and more realistic datasets, novel\nfeature learning techniques might be helpful for modeling\nsubtle timbre variations. Recently, sparse coding (SC)\nas a feature learning technique has been shown effec-\ntive for MIR. This approach uses a predeﬁned dictionary\n(codebook) to encode the prominent information of a\ngiven low-level feature representation of an input signal.\nOne can encode any sensible audio representation by SC\nto capture different signal characteristics. For instance,\nNam et al. [17] applied SC on short-time mel-spectra\nfor music auto-tagging; Yu et al. [25] applied SC on\nlogarithm cepstra and power-scale cepstra for predominant\ninstrument recognition. Our work goes one step further\nand exploits phase information for SC.\n3. ELECTRIC GUITAR PLAYING TECHNIQUE\nTable 1 lists the 7 playing techniques we consider in\nthis work. Most guitar solos are constructed with these\ntechniques. For example, muting is widely used alterna-\ntively in place of normal in guitar riffs for rhythmic and\npunched phrases in rock and metal music, and bending is\ncommonly considered to be the most important technique\nfor expressing emotion.\nTo gain more insights into the signal-level properties of\nthe playing techniques, in Fig. 1 we show the spectrograms\n(the ﬁrst row) and the short-time cepstra (the second row)\nof the individual-note examples played with the 7 playing\ntechniques. The ﬁrst three columns are individual notes\nF4 of normal, vibrato and mute, the fourth column the\nconsecutive notes F4–E4 of pull-off, and the last three\ncolumns the consecutive notes F4–#F4 of hammer-on,\nsliding and bending. The length of all samples is 0.6s.\nThe window size is 46ms and the hop size is 10ms. From\nthe spectrograms and the short-time cepstra, we see that\nmuting has a ‘noisier’ attack and a faster decay comparing\nto normal. Moreover, hammer-on, sliding and bending\nhave quite different transition behaviors, although they\nhave the same note progression. The transition is sharp\nfor hammer-on; smooth for bending; and there is a two-\nstage transition for sliding. Therefore, it seems that both\nthe spectrogram and the cepstra contain useful information\nthat can be exploited for automatic classiﬁcation.Technique Description # clips\nNormal Normal sound 2,009\nMuting Sounds muted (by right hand) to\ncreate great attenuation385\nVibrato Trilled sound produced by twisting\nleft hand ﬁnger on the string637\nPull-off Sound similar to normal but with the\nsmoother attack created by pulling\noff the string by left hand ﬁnger525\nHammer-\nonSound similar to normal but with the\nsmoother attack created by hammer-\ning on the string by left hand ﬁnger581\nSliding Discrete change to the target note\nwith a smooth attack by left hand\nﬁnger sliding through the string1,162\nBending Continuous change to the target note\nwithout an apparent attack by bend-\ning the string by left hand ﬁngers1,281\nTable 1. Description of the playing techniques considered.\n4. DATASET\nWhile there is no publicly available dataset for guitar\nplaying technique classiﬁcation across different tones, we\nestablish our own one with the aforementioned 7 playing\ntechniques. The dataset is recorded by a professional\nguitarist using a recording interface, PreSonus’ AudioBox\nUSB, with bit depth of 24 bits and frequency response from\n14 Hz to 70 kHz. We directly line-in the guitar to recording\ninterface to catch every nuance of sound and exclude\nenvironmental noise. The guitar for recording is ESP’s MII\nwith Seymour Duncan’s pickup and Ebony ﬁnger board,\nwhich is a high-quality guitar especially for metal and\nrock music. To make the quality of the sound recordings\nakin to that of real-world performance, we augment the\nsingle clean tone source to different guitar tones, which is\ndone in the post-production stage using music production\nsoftware Cubase. In addition, we assign each audio clips\nto 7 different guitar tones, which involve different levels\nofdistortion, reverb, delay andchorus. Such tones may\nrepresent different genres such as rock, metal, funk, and\ncountry music solos. Moreover, the tones are carefully\ntuned to meet the quality for listening.\nBecause of the different characteristics of the tech-\nniques, the clips are recorded in slightly different ways.\nAll the clips of sliding and bending have 2 notes for each\nclip with both whole step (2 semitones) and half step (1\nsemitone); all the clips of hammer-on and pull-off have\n2 notes with only half step; and the clips of vibrato and\nmuting have only one note for each clip. As for normal, we\nrecord whole steps, one steps, and single notes to cover all\npossible cases which might occur in the other 6 techniques.\nFor sliding and bending, we record the clips only with the\nﬁrst three strings of the guitar since these techniques are\nless frequently applied on the last 3 strings. Similarly, we\nrecord muting clips with only the last 3 strings because it\nis commonly used in rhythm guitar with low pitch. Other\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n10Figure 1. Spectrograms (the ﬁrst row) and short-time cepstra (the second row) of the seven playing techniques considered\nin this study. From left to right: normal, muting, vibrato, pull-off, hammer-on, sliding, bending.\nplaying techniques are recorded with all the 6 strings. As a\nresult, we can see from Table 1 that the numbers of clips of\nthe 7 techniques are different, where normal has the largest\nnumber of 2,009 notes and muting has the smallest number\nof 385 notes. In total there are 6,580 clips.\n5. METHODS\n5.1 Feature representation\nOur feature processing procedures have two steps: low-\nlevel feature extraction and sparse coding. In low-level\nfeature extraction, we select spectrogram (SG), group-\ndelay function (GDF), instantaneous frequency deviation\n(IFD), logarithm cepstrum (CL) and power cepstrum (CP),\nall of which are derived quantities from the short-time\nFourier transformation (STFT):\nSh(t;!) =Z\nx(\u001c)h(\u001c\u0000t)e\u0000j!\u001cd\u001c= Mh(t;!)ej\bh(t;!);\n(1)\nwherex(t)2Ris the input signal, Sh(t;!)2C\nstands for the two-dimensional STFT representation on\ntime-frequency plane, and h(t)refers to the window\nfunction. SG is the magnitude part of the STFT repre-\nsentation: SGh(t;!) =jSh(t;!)j. Phase spctrum is the\nimaginary part of the logarithm spectrum: \bh(t;!) =\nIm\u0000\nlog Sh(t;!)\u0001\n. IFD and GDF are the derivative of\nphase \bover time and frequency, respectively:\nIFDh(t;!) =@\bh\n@t= Im\u0012SDh(t;!)\nSh(t;!)\u0013\n;(2)\nGDFh(t;!) =\u0000@\bh\n@!\u0000t= Re\u0012\n\u0000STh(t;!)\nSh(t;!)\u0013\n;(3)\nwhereDandTrepresent operators on window functions:\nDh(t) =h0(t)andTh(t) =t\u0001h(t). Detailed derivation\nprocedures of GDF and IFD can be found in [2]. On the\nother hand, CL and CP are calculated as\nCLh(t;q) = (Sh)\u00001\u0000\nlogjSh(t;!)j\u0001\n; (4)\nCPh(t;q) = (Sh)\u00001\u0010\njSh(t;!)j1=3\u0011\n; (5)\nwhere (Sh)\u00001(\u0001)denotes the inverse STFT and qdenotes\nquefrency [19]. Features derived from CL, such as theMel-frequency cepstral coefﬁcients (MFCCs), are often\nemployed in audio signal processing [8, 16].\n5.2 Sparse coding and dictionary learning\nFor any one of the aforementioned low-level features,\ndenoted as y2Rm, we further convert it to a sparse\nrepresentation \u000b2Rkby SC. Speciﬁcally, SC involves\nthe following l1-regularized LASSO problem [7] to encode\nyover a given dictionary D2Rm\u0002k.\n^\u000b=fSC(D;y) = arg min\n\u000bky\u0000D\u000bk2\n2+\u0015k\u000bk 1:(6)\nThe LASSO problem can be efﬁciently solved by for\nexample the least angle regression (LARS) algorithm [7].\nMoreover, the dictionary Dis learned by the online dictio-\nnary learning (ODL) [15] implemented by the open-source\npackage SPAMS (http://spams-devel.gforge.\ninria.fr/). The SC result when the input yis CL has\nbeen referred to as the sparse cepstral code [25].\n6. EXPERIMENT\n6.1 Experimental setup of individual notes\nAs Fig. 1 illustrates, the playing techniques can be better\nidentiﬁed around the onsets for most cases. Therefore,\nour system starts from detecting the onset of each clip\nand then extracts features from each segment starting from\nthe time before the onset by tasecond to the time after\nthe onset by tbsecond. We use the well-known spectral\nﬂux method [11] for onset detection, and empirically set\nta= 0:1 andtb= 0:2 for all the clips. For STFT, we use\nHanning window of window size 46 ms (1,024 samples)\nand hop size of 10 ms (441 samples). Under the sampling\nrate of 44.1 kHz, the dimension of all the low level features\nis 512 (i.e. considering only positive frequency).\nWe adopt a ﬁve-fold jack-knife cross-validation (CV)\nscheme for the evaluation. For all the fold partitions,\nthe distribution of clips over the playing techniques is\nbalanced. We learn both the classiﬁer and the ODL\ndictionary from the training folds only, without using the\ntest fold. The number of atoms kof each dictionary is set to\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n11Figure 2. Average accuracies (in F-scores) of playing\ntechnique classiﬁcation using various feature combination.\nLeft part: RAW features; right part: SC features.\n512.3After obtaining the frame-level sparse codeword \u000b,\na clip-level feature representation is constructed by mean\npooling. Finally, the features, either with or without sparse\ncoding, are fed into linear support vector machine (SVM)\n[9], with the parameter Coptimized through an inside CV\non the training data from the range f2\u000010;2\u00009;\u0001\u0001\u0001;210g.\nThe evaluation results on the test set are reported in terms\nof F-score, which is the harmonic mean of precision and\nrecall. All the evaluation is done at the clip-level.\nWe consider a number of baseline approaches for com-\nparison. First, we use the MIRtoolbox (version 1.3.4) [14]\nto compute a total number of 41 features covering the tem-\nporal, spectral, cepstral and harmonic aspects of music sig-\nnals (denoted as ‘TIMB’ in Fig. 2) as an implementation\nof a prior art on guitar playing technique classiﬁcation [1].\nSecond, the conventional MFCC, \u0001MFCC and \u0001\u0001MFCC\nare also used for their popularity (denoted as ‘MFCC’).\nThird, we try the early fusion of MFCC and TIMB (i.e. by\nconcatenating the corresponding clip-level representations\nto form a longer feature vector). Finally, for the features\nlearned by SC, we note that the sparse representation of\nthe mel-spectra (denoted as ‘MEL’) was used in [17],\nand the sparse representations of CL and CP were used\nin [25]. However, please note that the focus here is to\ncompare the performance of using different features for\nthe task, so our implementation does not faithfully follow\nthe ones described in the prior arts. For example, Nam\net al. uses automatic gain control as a pre-processing and\nuses multiple frame representation instead of frame-level\nfeatures as input to feature encoding [17]. For simplicity\nthe feature extraction and classiﬁcation pipelines have been\nkept simple in this study.\nWe apply SC to all the ﬁve low-level features described\nin Section 5.1 and consider a number of early fusion of\nthem. No normalization is performed for SC features.\nHowever, for non-SC features (referred to as ‘RAW’), it is\nuseful to apply a z-score normalization so that each feature\ndimension has zero mean and unit variance.\n3Using an over-complete dictionary (i.e. k\u001dm) usually improves\nthe performance of SC features [25], but we leave this as a future work.(a) SC+SG\npredicted classF-scorenor mut vib pul ham sli benactual classnor 92.6 3.26 1.07 1.01 0.85 0.90 0.38 55.2\nmut 44.0 43.7 6.94 1.13 0.32 0.97 2.90 56.1\nvib 31.0 4.93 63.8 0.27 0.00 0.00 0.00 74.1\npul 21.0 1.75 0.00 21.8 16.9 34.2 4.47 29.7\nham 31.4 0.36 0.18 12.6 25.8 25.6 4.14 33.1\nsli 11.9 0.94 0.00 7.92 10.9 52.7 15.6 46.1\nben 3.56 0.92 0.11 2.18 1.26 14.5 77.5 75.6\n(b) SC+CL\npredicted classF-scorenor mut vib pul ham sli benactual classnor 95.6 1.01 0.41 0.82 0.63 1.20 0.30 58.6\nmut 38.9 54.4 4.35 0.16 0.00 0.65 1.45 66.3\nvib 14.3 6.03 79.7 0.00 0.00 0.00 0.00 86.3\npul 27.2 0.58 0.19 28.2 14.6 25.6 3.69 38.9\nham 31.4 0.00 0.00 9.55 38.2 18.2 2.70 47.2\nsli 14.9 1.42 0.00 4.43 7.26 61.8 10.2 56.7\nben 3.79 0.69 0.00 1.84 1.03 10.5 82.2 81.9\n(c) SC+fCL,GDF,IFDg\npredicted classF-scorenor mut vib pul ham sli benactual classnor 95.6 1.59 0.33 0.55 0.79 0.79 0.36 64.1\nmut 35.0 57.9 4.52 0.32 0.16 0.32 1.77 68.7\nvib 12.3 6.85 80.8 0.00 0.00 0.00 0.00 86.9\npul 19.6 0.58 0.19 41.2 11.7 22.5 4.27 52.0\nham 24.3 0.18 0.00 10.5 45.8 17.5 1.80 55.2\nsli 10.2 1.13 0.19 5.66 6.60 70.4 5.85 65.0\nben 1.38 0.23 0.00 0.23 0.80 5.17 92.2 89.4\nTable 2. Confusion matrix (in %) of playing technique\nclassiﬁcation of electric guitar individual notes using\ndifferent feature combinations.\n6.2 Experiment results\nFrom the left hand side of Figure 2, we ﬁnd that both\nRAW+TIMB [1] and RAW+MFCC perform worse than\nRAW+SG, RAW+CL and RAW+CP, possibly because the\nfeature dimension of the latter three is larger. However,\nafter fusing TIMB and MFCC, the F-score is improved\nto 57.4%, which is not signiﬁcantly worse than the result\nof RAW+CL (i.e. 59.0%) under the two-tailed t-test. It\nturns out that using sophisticated features such as those\ncomputed by the MIRtoolbox does not offer gain for this\ntask. Note that the F-score of random guess would be\n1/7=14.3%, because each fold is balanced across the 7\ntechniques. The performance of most RAW features is\ngreatly better than the chance level.\nIn contrast, from the right hand side of Figure 2, we\nﬁnd that SC features usually performs much better than the\nnon-SC (i.e. RAW) counterparts. For example, SC+SG,\nSC+CL and SC+CP are better than RAW+SG, RAW+CL\nand RAW+CP, respectively. These improvements are all\nsigniﬁcant under the two-tailed t-test (p<0.01, d.f.=8).\nSimilar observations have been made in existing works that\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n12apply SC features to MIR tasks (e.g. [17, 25]). We also\nﬁnd that using SC+CL already leads to signiﬁcantly better\nF-score than RAW+fTIMB,MFCCg (p<0.0001, d.f.=8).\nMoreover, from the data of SC features we see that fusing\nGDF and IFD generally improves the accuracy, and that\nthe best F-score 71.7% is obtained by fusing sparse-coded\ncepstral and phase features (i.e. SC+fCL,GDF,IFDg).\nThe F-score of SC+fSG,GDF,IFDg is worse (66.1%) than\nSC+fCL,GDF,IFDg, but is still signiﬁcantly better than\nSC+SG. We also note that SC does not improve the per-\nformance for MEL, MFCC, TIMB and IFD. This implies\nthat sophisticated features like TIMB are not suitable for\nSC. Although SC+IFD is worse than IFD, its fusion with\nother SC features still results in better performance. In a\nnutshell, this evaluation shows that it is promising to use\nSC for playing technique classiﬁcation, especially when\nwe fuse multiple features derived from STFT.\nTable 2 displays the confusion matrices for three dif-\nferent feature combinations with sparse coding. Table\n2(a) shows the result of SC+SG, from which we see\nthat normal and bending have relatively high F-scores of\n74.1% and 75.6% (see the rightmost column), yet the\nother ﬁve techniques have F-scores lower. We see that\nmany playing techniques can be easily misclassiﬁed as\nnormal. We also see ambiguities between for example pull-\noff versus sliding and hammer-on versus sliding, showing\nthat such techniques are difﬁcult to be discriminated from\none another in the logarithm-scale spectrogram.\nIn contrast, we see from Table 2(b) that SC+CL leads\nto consistent improvement in F-score for all the playing\ntechniques, comparing to SC+SG. The largest performance\ngain (+14.1%) is obtained for hammer-on. We also see that\nthe ambiguity between normal and vibrato is mitigated.\nFinally, comparing Tables 2 (b) and (c) we see that\nSC+fCL,GDF,IFDg consistently improves the F-score for\nall the playing techniques. More interestingly, it seems\nthat adding phase derivatives effectively alleviate the afore-\nmentioned confusions without compromising the discrim-\ninability of other classes. The F-scores of all the playing\ntechniques are now above 50.0%.\n7. REAL-WORLD MUSIC\nThe automatic transcription ﬂow contains frame-level pitch\ndetection, onset detection, and playing technique classiﬁ-\ncation, one after another. We adopt the method proposed\nby Peeters [20] and use spectral and cepstral features for\npitch detection. For onset detection, we use again the\nspectral ﬂux method [4, 11]. Finally, we apply the playing\ntechnique classiﬁer trained from the individual note dataset\nto classify the playing techniques of the guitar solo.\nWe present a qualitative evaluation of a real-world\nelectric guitar solo excerpt performed by same professional\nguitarist. It is an interpretation of Sonata Artica’s Tallulah\nreleased in 2001, for the fragment 3:59–4:08. We show\nin the ﬁrst two subﬁgures of Fig. 3 its scoresheet and\nspectrogram. In the third subﬁgure we show the pitch and\nonset, using black horizontal bars, gray horizontal bars,\nand vertical dashed lines to denote the estimated frame-level pitches, ground truth pitches, and estimated onsets,\nrespectively. We see that the estimated pitches and onsets\nmatch the ground truth quite well, except for some cases\nsuch as the mismatch between the onset at 7.70s and the\nchange of pitch at 7.84s, which probably results from the\nambiguity of the onset of bending.\nThe last subﬁgure of Fig. 3 compares the result of\nSC+SG and SC+fCL,GDF,IFDg for playing technique\nclassiﬁcation. Since our classiﬁcation is performed with\nrespect to the detected onsets, the errors in the stage of on-\nset detection will fully propagate into the stage of playing\ntechnique classiﬁcation. Therefore, the techniques which\nare not characterized by onset (e.g., a long-sustaining\nvibrato) cannot be transcribed. A true positive of onset\nis deﬁned as an onset position which is detected within\n100ms of the ground truth onset time. A true positive\nof playing technique is accordingly deﬁned as a correct\nprediction of playing technique at a true positive of onset.\nWe can see that the performance of playing classiﬁcation\ndegrades a lot in comparison to the case of individual notes.\nSpeciﬁcally, we have 7 true positives (4 normal and 3\nbending) for SC+fCL,GDF,IFDg and 5 true positives (2\nsliding, 2 bending and 1 normal) for SC+SG, while there\nare in total 17 targets in the ground truth. The 2 muting at\n2.38s and 4.60s and the hammer-on at 9.24 second are not\nrecalled by both methods. Although SC+fCL,GDF,IFDg\nfails to recall sliding, SC+SG recalls 2 sliding. While\nSC+fCL,GDF,IFDg has many false positives of vibrato,\nSC+SG has many false positives of sliding. In general,\nSC+fCL,GDF,IFDg performs better.\nThe two estimated events at 4.11s and 5.80s are interest-\ning. Although the two events do not present in the ground\ntruth, the prediction of SC+fCL,GDF,IFDg is musically\ncorrect as the two false alarms of onset indeed occur in a\nlong-sustaining vibrato. In contrast, SC+SG misclassiﬁes\nthe two events as pull-off and sliding, respectively.\n8. CONCLUSION\nIn this study, we have reported a comparative study on the\nperformance of a number of timbre modeling methods for\nthe relatively unexplored task of guitar playing technique\nclassiﬁcation. The evaluation is performed on a large-\nscale individual-note dataset comprising of 6,580 clips and\na real-world guitar solo recording. Our evaluation shows\nthat sparse coding works well in learning features that\nare useful for the task, and that using features extracted\nfrom the cepstra and phase derivatives helps resolve the\nconfusion among similar playing techniques. We also\nreport a qualitative evaluation on guitar solo transcription.\nWe are currently collecting more individual notes and\nsolos to deeply understand the signal-level characteristics\nfor these playing techniques. Although the present study\nmight be at best preliminary, we hope it can call for more\nattention towards playing technique modeling.\n9. ACKNOWLEDGMENTS\nThis work was supported by the Academia Sinica Career\nDevelopment Award 102-CDA-M09.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n13Figure 3. Result of transcribing a real-world guitar solo excerpt. From top to bottom: scoresheet, guitar tab, spectrogram,\npitch and onset (gray bar: ground truth; black bar: estimated pitch; vertical dashed line: estimated onset), and result\nof playing technique classiﬁcation by using SC+SG and SC+fCL,GDF,IFDg. Abbreviation: N=normal, V=vibrato,\nM=muting, P=pull-off, H=hammer-on, S=sliding, B=bending.\n10. REFERENCES\n[1] J. Abeßer et al. Feature-based extraction of plucking and\nexpression styles of the electric bass guitar. In ICASSP, pages\n2290–2293, 2010.\n[2] F. Auger and P. Flandrin. Improving the readability of time-\nfrequency and time-scale representations by the method of\nreassigment. IEEE Trans. Sig. Proc., 43(5):1068–1089, 1995.\n[3] A. M. Barbancho et al. Automatic transcription of guitar\nchords and ﬁngering from audio. IEEE Trans. Audio, Speech,\nand Language Processing, 20(3):915–921, 2012.\n[4] J. P. Bello et al. A tutorial on onset detection in music signals.\nIEEE Speech Audio Process., 13(5-2):1035–1047, 2005.\n[5] E. Benetos et al. Automatic music transcription: challenges\nand future directions. J. Intelligent Information Systems,\n41(3):407–434, 2013.\n[6] J. Dattorro. Effect design, part 2: Delay line modulation and\nchorus. J. Audio engineering Society, 45(10):764–788, 1997.\n[7] B. Efron et al. Least angle regression. Annals of Statistics,\n32:407–499, 2004.\n[8] A. Eronen and A. Klapuri. Musical instrument recognition\nusing cepstral coefﬁcients and temporal features. In ICASSP,\npages 753–756, 2000.\n[9] R.-E. Fan et al. LIBLINEAR: A library for large linear\nclassiﬁcation. J. Machine Learning Research, 2008.\n[10] P. Hamel et al. Automatic identiﬁcation of instrument classes\nin polyphonic and pply-instrument audio. In ISMIR, 2009.\n[11] A. Holzapfel et al. Three dimensions of pitched instrument\nonset detection. IEEE Trans. Audio, Speech, Language\nProcess., 18(6):1517–1527, 2010.\n[12] E. J. Humphrey et al. Feature learning and deep architectures:\nnew directions for music informatics. J. Intelligent Informa-\ntion Systems, 41(3):461–481, 2013.[13] A. Klapuri and M. Davy, editors. Signal Processing Methods\nfor Music Transcription, chapter 6. Springer, 2006.\n[14] O. Lartillot and P. Toiviainen. A Matlab toolbox for musical\nfeature extraction from audio. In DAFx, 2007.\n[15] J. Mairal et al. Online dictionary learning for sparse coding.\nInInt. Conf. Machine Learning, pages 689–696, 2009.\n[16] M. M ¨uller et al. Signal processing for music analysis. IEEE\nJ. Sel. Topics Signal Processing, 5(6):1088–1110, 2011.\n[17] J. Nam et al. Learning sparse feature representations for\nmusic annotation and retrieval. In ISMIR, pages 565–560,\n2012.\n[18] K. O’Hanlon and M. D Plumbley. Automatic music transcrip-\ntion using row weighted decompositions. In ICASSP, 2013.\n[19] A. V . Oppenheim and R. W. Schafer. Discrete-Time Signal\nProcessing. Prentice Hall, 2010.\n[20] G. Peeters. Music pitch representation by periodicity\nmeasures based on combined temporal and spectral represen-\ntations. In ICASSP, 2006.\n[21] L. Reboursi `ere et al. Left and right-hand guitar playing\ntechniques detection. In NIME, 2012.\n[22] L. Su and Y .-H. Yang. Sparse modeling for artist identiﬁca-\ntion: Exploiting phase information and vocal separation. In\nISMIR, pages 565–560, 2013.\n[23] L. Su and Y .-H. Yang. Sparse modeling of subtle timbre: a\ncase study on violin playing technique. In WOCMAT, 2013.\n[24] K. Yazawa et al. Automatic transcription of guitar tablature\nfrom audio signals in accordance with player’s proﬁciency.\nInICASSP, 2014.\n[25] L.-F. Yu et al. Sparse cepstral codes and power scale for\ninstrument identiﬁcation. In ICASSP, 2014.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n14"
    },
    {
        "title": "Melody Extraction from Polyphonic Audio of Western Opera: A Method based on Detection of the Singer&apos;s Formant.",
        "author": [
            "Zheng Tang",
            "Dawn A. A. Black"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416714",
        "url": "https://doi.org/10.5281/zenodo.1416714",
        "ee": "https://zenodo.org/records/1416714/files/TangB14.pdf",
        "abstract": "Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer’s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera sing- ers [3]. In this paper we introduce a novel melody extrac- tion algorithm based on this feature for opera signals. At the front end, it automatically detects the singer’s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch sa- lience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic atten- uation is adopted to handle octave errors which are com- mon in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of- the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer’s formant detec- tion. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%).",
        "zenodo_id": 1416714,
        "dblp_key": "conf/ismir/TangB14",
        "keywords": [
            "opera",
            "singers formant",
            "Long-Term Average Spectrum (LTAS)",
            "Fan Chirp Transform (FChT)",
            "pitch salience",
            "octave errors",
            "subharmonic attenuation",
            "outliers in pitch detection",
            "voicing detection",
            "voicing false alarm"
        ],
        "content": "MELODY EXTRACTION FROM POLYPHONIC AUDIO OF \nWESTERN OPERA: A METHOD BASED ON DETECTION OF \nTHE SINGER’S FORMANT  \nZheng Tang  Dawn A. A. Black  \nUniversity of Washington , Department \nof Electrical Engineering  \nzhtang@uw.edu  Queen Mary University of London, Electronic Eng i-\nneering and Computer Science  \ndawn.black@qmul.ac.uk  \nABSTRACT \nCurrent melody extraction approaches perform poorly on \nthe genre of opera [1 , 2]. The singer’s formant is defined \nas a prominent spectral-envelope peak around 3 kHz \nfound in the singing of professional Western opera sin g-\ners [3]. In this paper we introduce a novel melody extra c-\ntion algorithm based on this feature for opera signals. At \nthe front end, it automatically detects the singer’s fo rmant \naccording to the Long-Term Average Spectrum (LTAS). \nThis detection function is also applied to the short-term \nspectrum in each frame to determine the melody. The Fan \nChirp Transform (FChT) [4] is used to compute pitch s a-\nlience as its high time-frequency resolution overcomes \nth\ne difficulties introduced by vibrato. Subharmonic atte n-\nuation is adopted to handle octave errors which are co m-\nm\non in opera vocals. We improve the FChT algorithm so \nthat it is capable of correcting outliers in pitch detection. \nThe performance of our method is compared to 5 state- of-\nthe-art melody extraction algorithms on a newly created \ndataset and parts of the ADC2004 dataset. Our algorithm \nachieves an accuracy of 87.5% in singer’s formant dete c-\ntion. In the evaluation of melody extraction, it has the \nbest performance in voicing detection (91.6%), voicing \nfalse alarm (5.3%) and overall accuracy (82.3%). \n1. INTRODUCTION \nSinging voice can be considered to carry the main melody \nin Western opera. Melody extraction from a polyphonic \nsignal including singing voice requires both of the fo l-\nlowing: estimation of the correct pitch of singing voice in \neach time frame and voicing detection to determine when \nthe singing voice is present or not. \nThe singer’s (or singing) formant was first introduced \nby Johan Sundberg [3] and described as a clustering of \nthe third, fourth, and fifth formants to form a prominent \nspectral-envelope peak around 3 kHz. It is purportedly \ngenerated by widening the pharynx and lowering the la r-\nynx. The existence of a singer’s formant has been co n-\nfirmed in the singing voices of classically trained male Western opera singers and some female singers, but it has \nnot yet been found in soprano singers [5] or Chinese \nopera singers [6]. It has been proposed that singers deve l-\nop the singer’s formant in order to be heard above the o r-\nchestra. In Western opera, orchestral instruments typica l-\nly occupy the same frequency range as the singers. Ther e-\nfore singers train their vocal equipment in order to raise \nthe amplitude of frequencies at this range. \nThe LTAS is the average of all short-term spectra in a \nsignal, has been shown to be an excellent tool to observe \nthe singer’s formant [7] as can be seen in Figure 1 . Cha r-\nacteristics of the singer’s formant in the spectral domain \ninclude a peak greater than 20 dB below the overall \nsound pressure level, a peak-location at 2.5-3.2 kHz, and \na bandwidth of around 500-800 Hz [5, 7]. However, to \ndate, there has been no method developed to automatica l-\nly detect the presence of a singer’s forma nt or to quantify \nits character istics. \n \nFigure  1. Normalized LTAS for 5 audio excerpts from \nthe ADC2004 test collection [1].  \n1.1 Related Work \nIn 2004, the Music Technology Group of the Universitat \nPompeu Fabra organized a melody extraction contest pr e-\nsented at the International Society for Music Information \nRetrieval Conference. The Music Information Retrieval \nEvaluation eXchange (MIREX) was set up in 2005 and \naudio melody extraction has been a highly competitive \nfield ever since. Currently, over 60 algorithms have been \n © Zheng Tang , Dawn A. A. Black . \nLicensed under a Creative Commons Attribution 4.0 International  \nLicense (CC BY 4.0). Attribution:  Zheng Tang , Dawn A. A. Black . \n“Melody Extraction From Polyphonic Audio Of Western Opera: A \nMethod Based On Detection Of The Singer’ s Formant ”, 15th \nInternational Society for Music Information Retrieval Conference, 2014.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n161  \n \nsubmitted and evaluated. So far, none of the approaches \nconsider the presence of the singer’s formant.  \nThe majority of algorithms presented at MIREX are \nsalience based [2]. These assume that the fundamental \nfrequency of the melody is equivalent to the most salient \npitch value in each frame. The Short-Time Fourier Tran s-\nform (STFT) is often chosen to compute pitch salience [7, \n8]. In 2008, Pablo Cancela proposed the Fan Chirp Tran s-\nform (FChT) method, combined with Constant Q Tran s-\nform (CQT) in music processing. The FChT is a time-\nwarped version of the Fourier Transform that provides \nbetter time-frequency resolution [4, 9]. Although the \nSTFT provides adequate resolution in the majority of \ncases, it fails to generate a satisfying outcome when dea l-\ning with Western opera signals. This is because opera \ntypically exhibits complex spectral characteristics due to \nvocal ornamentations such as vibrato [1]. Vibrato is a \nregular fluctuation of singing pitch produced by singers. \nThis increases the difficulty in tracking the melody. With \nbetter resolution, the fast change of pitch salience can be \nbetter observed and tracked by using FChT. \nIt has been proposed that the singer’s formant may \ncause octave errors [2]. The presence of a spectral peak \n(the singer’s formant) at a higher frequency may cause \nthe fundamental frequency to be confused with the fr e-\nquency at the centre of the singer’ s formant. To address \nthis, Can cela developed a method called ‘subharmonic \nattenuation’ that can minimize the negative effects of \nghost pitch values at the multiple and submultiple peaks \nof a certain fundamental frequency [2, 9]. \nVoicing detection typically receives much less atte n-\ntion than pitch detection, to the extent that some previous \nmelody extraction algorithms did not contain this proc e-\ndure [10]. The most common approach is to set an energy \nthreshold, which might be fixed or dynamic [9]. However, \nthis technique is too simplistic since the loudness of m u-\nsical accompaniment in Western opera may fluctuate \nconsiderably. It is therefore impossible to define an a p-\npropriate threshold. An alternative technique is to use a \ntrained classifier based on a Hidden Markov Model \n(HMM) [11] but it is time-consuming to create a large \ndataset for training and there are always exceptions b e-\nyond the scope of the training set. In 2009, Regnier and \nPeeters proposed a voicing detection algorithm based on \nextraction of vocal vibrato [12], but has not been applied \nto melody extraction. In general, the high rate of false \npositives when detecting voiced frames limits the overall \naccuracy of melody extraction algorithms and a reduction \nof this is beneficial [2, 13].  \nThis paper is organized as follows. In Section 2, we \ndescribe the design and implementation of our proposed \nalgorithm for melody extraction. Starting with a general \nworkflow of the system, the function and novelty of each \ncomponent is explained in detail. Section 3 explains the \nevaluation process and presents a comparison of existing \nalgorithms. The creation of the new dataset is also pr e-sented in this section. Finally, we draw conclusions from \nthe results and give suggestions for future work. \n2. DESCRIPTION OF THE ALGORITHM \n2.1 General Workflow \nFigure 2  shows an overview of our system. In order to ex-\ntract the pitch of singing voice from polyphonic audio, \nwe must first determine whether the audio contains sin g-\ning voice. The presence of a singer’s formant would ind i-\ncate the presence of a classically trained singer. The \nLTAS is used to determine whether a singer’s formant \nexists in the audio, and hence determines whether our \nmethod can be applied. \n \nFigure  2. System overview.  \nOnce the presence of a singer is confirmed the spe c-\ntrum is analysed on a frame- by-frame basis. Two dec i-\nsions are made for each frame: firstly, does the frame \ncontain singing and hence a salient pitch? Secondly, what \nis the salient pitch of that frame? \nWe examine the spectral content of each frame to e s-\ntablish the presence of a singer’ s formant in that frame. If \npresent, that frame is designa ted ‘voiced’ and assumed to \ncontain melody carried by the singer’s pitch.  \nEach frame is also transformed to the frequency d o-\nmain using the FChT and further processed by subha r-\nmonic attenuation to obtain the pitch. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n162  \n \n2.2 Singer’s Formant Detection and Voicing Dete c-\ntion \nBased on the characteristics of the singer’s formant (see \nSection 1) we introduce a novel algorithm to automatica l-\nly detect the presence of a singer’s formant (and hence \nthe presence of a classically trained singer). Using Mo n-\nson’s method to compute the LTAS of the input audio \nsignal [14] the presence of a singer’s formant would be \nconfirmed if the LTAS exhibited the following properties: \n1. There exists a spectral peak which has an amplitude \ngreater than 20 dB less than the overall sound pre s-\nsure level. \n2. The peak is located between 2.5 and 3.2 kHz. \n3. The peak has a bandwidth of around 500-800 Hz. \nHowever, these properties were observed through \nanalysis of singing voice in the absence of musical a c-\ncompaniment [7]. When analysing singing with acco m-\npaniment, these criteria had to be modified in the follo w-\ning ways: the amplitude threshold of the spectral peak \nwas found to be lower than the theoretical value and thus \nthe first criteria becomes: \n1. The spectral peak has an amplitude greater than 30 \ndB less than the overall sound pressure level. \nThe LTAS exhibited irregular fluctuations that made \naccurate identification of the singer’s formant peak pro b-\nlematic. We therefore smoothed the LTAS (20 point aver-\nage) and used polynomial fitting of degree 30. This \nsmoothing and polynomial fitting will shift the location \nof the spectral peak and hence the range of the peak must \nbe expanded. The second criteria is therefore modified to: \n2. The peak is located between 2.2 and 3.4 kHz. \nSimilarly, we observe that the polynomial bandwidth \nmay be slightly different from the LTAS curve. Therefore \nthe bandwidth of the singer’s formant is set to be larger \nthan the original value: \n3. The peak has a bandwidth larger than 600 Hz. \nWe must then add another criteria to ensure the signi f-\nicance of the peak. In order to measure the significance, \nwe employed the first-order and second-order derivatives \nof the LTAS to measure the LTAS curvature and, from \nempirical evidence, designate significance to be a peak \nwith a curvature greater than 0.01: \n4. The curvature exceeds 0.01 at the location of the \nspectral peak. \nIn order to illustrate the criteria, we present the follo w-\ning figures. Figure 3  shows the fitting polynomials of \nsmoothed LTAS for 5 samples from the MIREX \nADC2004 test collection [1]. The singer’s formant can be \nclearly observed for the male opera samples. Presented in \nFigure 4  is the second-order derivative of LTAS. This is \nnegative when the curve is convex and hence can be used \nto determine the formant bandwidth. Our constraint that \nthe bandwidth be at least 600 Hz is illustrated. In Figure 5 , \nwe show that the constraint on curvature can ensure the \ndegree of convexity of the curve. It is clear from all plots that the opera signals sung by male singers contain the \nsinger’s for mant but others do not. \n \nFigure  3. The fitting polynomials of smoothed LTAS \nfor 5 audio excerpts from ADC2004 [1].  \n \nFigure  4. The second-order derivatives of LTAS for 5 \naudio excerpts from ADC2004 [1].  \n \nFigure  5. The curvatures of LTAS for 5 audio excerpts \nfrom ADC2004 [1].  \n \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n163  \n \nIf the LTAS satisfies all four criteria the audio is pr e-\nsumed to contain a trained singer. Use of the same criteria \nto analyse the spectrum of a single audio frame can ind i-\ncate whether the frame is voiced (contains singing) or not. \nFor a single frame, only the second and third criteria are \napplied, as the other two criteria are more influenced by \nobserved amplitude variations in individual short-term \nspectra. The output of this stage is a two-value sequence \nwhose length is the number of frames, with ‘1’ indicating \na voiced frame and ‘ -1’ un voiced. Subsequently, when \nconsidering points of discontinuity causing false dete c-\ntions, single values within a sequence are removed. \n2.3 Pitch Detection \nIf a frame is classified as voiced it can be expected to \ncontain a clearly defined pitch. Vibrato in singing can \ncause pitch ambiguity. We therefor e adopt Cancela’s \nmethod to perform FChT since it exhibits optimal time-\nfrequency resolution. This chirp-based transform is based \non an FFT performed in a warped time domain. It is \ncombined with CQT in order to guarantee high resolution \neven when the fan chirp rate is not ideal. More details can \nbe found in [4] and [9].  \nIn Western opera the singer’s formant will cause peaks \nat frequencies higher than the fundamental [2]. The alg o-\nrithm from Cancela provides subharmonic attenuation - \nan effective solution to this problem. It will suppress mu l-\ntiple and submultiple pitch peaks of the fundamental fr e-\nquency. Then, we can perform salience computation to \ndetect the pitch in each frame.  \nIn the outlier correction stage, to improve Cancel a’s \nmethod, we compute two additional peaks per frame as \ncandidate substitutes for the wrong pitch. Firstly, the \nmost salient pitch peaks are compared with those from \nadjacent frames. If a difference of more than 2 semitones \noccurs on both sides, the estimated pitch in this frame is \nconsidered as a wrong detection. In this case we subst i-\ntute the pitch for this frame with the pitch among the \nthree candidates which is closest to the average of the two \nadjacent estimations. Due to subharmonic attenuation, the \ninfluence of the subharmonics of the top peak is reduced \nwhen calculating the other pitch candidates. \nOur method is novel to Cancela’s in the following \nways: (1) The algorithm designed by Cancela extracts \nmultiple salient peaks simultaneously and these are \nviewed as separate melodies. We introduce the correction \nblock so that the less salient peaks are taken as substitutes \nof wrong pitch detection in a single estimation of melody. \n(2) We improve the voicing detection by considering the \nsinger’s formant. (3) Cancela’s method is not specifically \ndesigned for opera items and its potential for dealing with \nvibrato and other spectrum characteristics has not been \nexplored. \nFinally, the estimated pitch sequence is multiplied by \nthe two-value voicing detection sequence. The output of \nour algorithm follows the standard format of MIREX and records the time-stamp and estimated frequency of each \nframe. \n3. EVALUATION \n3.1 The Dataset \nThe dataset we used for evaluation is a combination of \nthe ADC2004 test collection and our own dataset1. De-\ntails of the dataset can be found in Table 1 . \nAmong the existing test collections in MIREX, only \nADC2004 contains 2 excerpts in the genre of Western \nopera. In order to evaluate the performance of melody ex-\ntraction algorithms upon sufficient amount of opera sa m-\nples for meaningful comparison, we created a new dataset. \nNine students from the Central Academy of Drama in \nBeijing were recorded. All had received more than 5 \nyears of classical voice training except for an amateur \nWestern opera male singer . Their singing voices were \nrecorded in a practice room, about 10× 5× 5 m with mo d-\nerate reverberation. The equipment included a Sony \nPCM-D50 recorder and an AKG C5 microphone. The a c-\ncompaniments played by orchestra were recorded sep a-\nrately. All the signals were digitized at a sample rate of \n44.1 kHz with bit depth 16. We normalized the maximum \namplitude of the singing voices to be -1.25 dB. The si g-\nnal-to-accompaniment ratio is set to 0 dB. The ground \ntruth for melody extraction was generated by a mon o-\nphonic pitch tracker in SMSTools with manual adjus t-\nment [2] using the vocal track only. The frame size was \n2048 samples with a step size of 256 samples. \nWe conducted two evaluations based on this combined \ndataset. The test set for melody extraction consists of 18 \nexcerpts of 15s-25s duration sung by classically trained \nWestern opera tenors. For the evaluation of singer’s for-\nmant detection, we will compare them with 14 excerpts \nsung by trained Western opera sopranos, trained Peking \nopera singers, pop singers, and a single unprofessional \nWestern opera male singer. \n \nTest set  Singing type  No. of \nsongs  Expectation/  \ndetection of  \nsinger’s formant  \nADC2004  Tenor, Western  2 Yes/ Yes \nSoprano, Western  2 No/ No \nPopular music  4 No/ No \nThe  \ndataset  \nrecorded at  the  \nCentral  \nAcademy of \nDrama  Tenor, Western  16 Yes/ Yes \nSoprano, Western  2 No/ Yes \nAmateur, Western  2 No/ Yes \nLaosheng, Peking  2 No/ No \nQingyi, Peking  2 No/ No \nTable 1.  Test dataset for the evaluations of melody extraction \nand singer’s formant detection.  \n \n                                                           \n1This database is available for download under a creative commons l i-\ncense at http://c4dm.eecs.qmul.ac.uk/rdr/ all usage should cite this paper.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n164  \n \n3.2 Melody Extraction Comparison \nOf the many melody extraction algorithms submitted to \nMIREX, few are freely available. We present five alg o-\nrithms for comparison. We were limited in our choice by \navailability, but the methods are representative of the m a-\njority of algorithms submitted to MIREX in that they \ncover common approaches and best performance. Each \nmethod is briefly introduced next. \nCancela’s algorithm was submitted in 2008 [9]. He \nused FChT combined with CQT to estimate the pitch in \neach time frame. Voicing detection is conducted through \nthe calculation of an adaptive threshold, but this proc e-\ndure is not included in the open-source code provided \nonline. For the purposes of comparison, we added a \ncommon voicing detection function utilizing an adaptive \nenergy threshold as described in [9 ]. \nSalamon’s algorithm was introduced in 2011 [8]. It has \nbeen developed into a melody extraction vamp plug-in: \nMELODIA. This algorithm achieved the best score in \nMIREX 2011. It applies contour tracking to the salience \nfunction calculated by STFT to remove all the contours \nexcept for the melody. The voicing detection step is ca r-\nried out by removing the contours that are not salient. \nThe algorithm developed by Sutton in 2006 [11] inn o-\nvatively combines two pitch detectors based on the fe a-\ntures of singing voice including pitch instability and high-\nfrequency dominance. A modified HMM processes the \nestimated melodies and determines the voicing. \nThe final two algorithms were both proposed by Vi n-\ncent in 2005 [10]. One makes use of a Bayesian harmonic \nmodel to estimate the melody, and the other is achieved \nvia loudness-weighted YIN method. Vincent assumed \nthat the melody was continuous throughout the audio, and \nvoicing detection was not included in his algorithm. \n3.3 Results \nThe evaluation results of singer’s formant detection can \nbe found in Table 1 . Among the 32 audio files in the d a-\ntaset, the assumption is that only the 18 excerpts sung b y \nWest ern opera tenors possess the singer’s formant, while \nthe others do not. The results show that 28 of the files \n(87.5%) meet our expectation. The singer’s formant is \nalso detected in the excerpts of the Western opera am a-\nteur and sopranos in our dataset. The amateur singer is \nfrom the Acting Department at the Central Academy of \nDrama (Beijing) and declares that he has not received an y \nformal training in opera. However he used to take courses \nin vocal music due to a requirement of the school. Thus, \nthere is a p ossibility that the presence of singer’s formant \nonly requires a short period of training. Although sources \nstate that there is no singer’s formant present in soprano \nsinging [5, 7], the mean pitch of the two excerpts in our \ndataset is at the low end of the range for sopranos (550.43 \nHz). The presence of a singer’s formant  is pitch related. \nThe higher the pitch, the less likely a singer’s formant is \npresent. A precise study of this relationship is a topic for \nfuture work. Table 2  shows the melody extraction results of the 6 \nalgorithms. Voicing detection measures the probability of \ncorrect detection of voiced frames, while voicing false \nalarm is the probability of incorrect detection of unvoiced \nframes. Raw pitch accuracy and raw chroma accuracy \nboth measure the accuracy of pitch detection, with the \nlatter ignoring octave errors. The overall accuracy is the \nproportion of frames labeled with correct pitch and voi c-\ning. Since Vincent’s algorithms did not perform voicing \ndetection, their voicing metrics and overall accuracy are \ninapplicable. \n \nFirst author/ \ncompletion \nyear Voicing \ndetec-\ntion Voicing \nfalse \nalarm  Raw \npitch  \naccuracy  Raw \nchroma \naccuracy  Overall  \naccuracy  \nVincent \n(Bayes)/ \n2005  N/A N/A 64.8%  68.6%  N/A \nVincent \n(YIN)/ \n2005  N/A N/A 69.5%  72.2%  N/A \nSutton/ \n2006  89.3%  51.9%  87.0%  87.6%  76.9%  \nCancela/ \n20081 72.6%  39.3%  83.9%  84.8%  62.4%  \nSalamon/ \n2011  62.3%  21.8%  25.4%  30.1%  31.3%  \nOur method  91.6%  5.3%  84.3%  85.1%  82.3%  \nTable 2 . Results of the audio melody extraction evalu a-\ntion. \nOur algorithm ranks highest in overall accuracy. We \nalso achieve the highest voicing detection rate as 91.6% \nand the lowest voicing false alarm rate as 5.3%, which \nproves that voicing detection based on the singer’s fo r-\nmant is extremely effective for male Western opera. The \nimprovement in raw pitch accuracy by outlier correction \nwhen compared to Can cela’s method is not large. This \nallows us to hypothesise that the melody in Western \nopera may be so prominent that the influence of any a c-\ncompaniment can be disregarded. \nSutton’s method also has excellent performance on our \ndataset. That success might be attributed to his similar \nfocus on the characteristics of singing voice. He also \nmakes use of the vibrato feature to estimate the pitch of \nmelody. Due to the application of a high-frequency corr e-\nlogram, Sutton’s algorithm may i ndirectly benefit from \nthe pres ence of a singer’s form ant. However, the method \nwe propose for voicing detection is much more conve n-\nient than the use of an HMM. Moreover, Sutton’s alg o-\nrithm exhibits a much higher voicing false alarm rate. \nThe poor performance of Salamon’s algorithm on our \ndataset can be explained by the fact that it fails to est i-\nmate the pitch in detected unvoiced frames accurately. \nWe also evaluated the 4 audio files that contradicted \nour expectation in singer’s formant detection (two Wes t-\n                                                           \n1The voicing detection part of this algorithm is implemented by us and \ncannot represent the original design of the author. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n165  \n \nern soprano singers and one amateur male Western opera \nsinger). The performance of our algorithm declines si g-\nnificantly with a voicing detection rate of 53.1% and an \noverall accuracy of 53.7%. This may be due to the fact \nthat the singer’s formant, although present, is not as pr o-\nnounced or stable as the Western opera tenor’s . \n4. CONCLUSION AND FURTHER WORK \nIn this paper, we have presented a novel melody extra c-\ntion algorithm based on the detection of singer’s formant. \nThis detection relies on 4 criteria modified from prev i-\nously pro posed characteristics of the singer’s formant. \nThe pitch detection step of our algorithm is achieved u s-\ning FChT and subharmonic attenuation to overcome the \nknown difficulties when detecting the melody in opera. \nWe also improved the algorithm so it is capable of r e-\nmoving outliers in pitch detection. \nFrom the evaluation results, it can be seen that our a l-\ngorithm can detect the s inger’s formant accurately. Mel o-\ndy extraction evaluation on our dataset confirms that our \nalgorithm provides a clear improvement in voicing dete c-\ntion. Furthermore, its overall accuracy is comparable to \nstate- of-the-art methods when dealing with Western \nopera signals. \nIn the future, we plan to study the performance of this \nalgorithm on signals in other genres and expand its scope \nof application. Additionally, the possible effects of per-\nforming environments and accompaniment music to the \nusage of singer ’s formant will also be explored. \n5. ACKNOWLEDGMENTS \nThis paper is based upon a research collaboration with the \nDepartment of Peking Opera at the Central Academy of \nDrama. Thanks to Prof. Ma Li and his students for their \nrecording samples and professional advice on traditional \nopera. Besides, we would like to express our thanks to \nPablo Cancela, Justin Salamon, Emilia Gó mez, Christ o-\npher Sutton and Emmanuel Vincent for contributing their \nalgorithm codes. \n6. REFERENCES \n[1] E. Gó mez, S. Streich, B. Ong, R. P. Paiva, S. \nTappert, J. M. Batke, G. Poliner, D. Ellis, and J. P. \nBello: “A quantitative comparison of different \napproaches for melody extraction from polyphonic \naudio recordings,” Univ. Pompeu Fabra, Barcelona, \nSpain, 2006, Tech. Rep. MTG- TR-2006 -01. \n[2] J. Salamon, E. Gó mez, D. Ellis, and G. Richard: \n“Melody extraction from polyphonic music signals: \napproaches, applications and challenges, ” IEEE \nSignal Processing Magazine , Vol. 31, No. 2, pp. \n118-134, 2013.  \n[3] J. Sundberg: “Articulatory interpretation of the \n‘singing formant’,” The Journal of the Acoustical Society of America , Vol. 55, No. 4, pp. 838-844, \n1974.  \n[4] L. Weruaga, and M. Ké pesi: “The fan-chirp \ntransform for non-stationary harmonic signals, ” \nSignal Processing , Vol. 87, No. 6, pp. 1504-1522, \n2007.  \n[5] R. Weiss, Jr, W. S. Brown, and J. Moris: “Singer's \nformant in sopranos: fact or fiction? ” Journal of \nVoice , Vol. 15, No. 4, pp. 457-468, 2001. \n[6] J. Sundberg, L. Gu, Q. Huang, and P. Huang: \n“Acoustical study of  classical Peking Opera singing,” \nJournal of Voice , Vol. 26, No. 2, pp. 137-143, 2012. \n[7] J. Sundberg: “Level and center frequency of the \nsinger's formant,” Journal of voice , Vol. 15, No. 2, \npp. 176-186, 2001. \n[8] J. Salamon, and E. Gó mez: “Melody extraction from \npolyphonic music signals using pitch contour \ncharacteristics, ” IEEE Transactions on Audio, \nSpeech, and Language Processing , Vol. 20, No. 6, \npp. 1759-1770, 2012. \n[9] P. Cancela, E. Ló pez, and M. Rocamora: “Fan chirp \ntransform for music representation, ” Proceedings of \nthe 13th Int Conference on Digital Audio Effects \nDAFx10 Graz Austria , 2010. \n[10] E. Vincent, and M. D. Plumbley: “Predominant-F0 \nestimation using Bayesian harmonic waveform \nmodels,” 2005 Music Information Retrieval \nEvaluation eXchange (MIREX) , 2005. \n[11] C. Sutton: “Transcription of vocal melodies in \npopular music,” Report for the degree of MSc in \nDigital Music Processing, Queen Mary University of \nLondon, 2006. \n[12] L. Regnier, and G. Peeters: “Singing voice detection \nin music tracks using direct voice vibrato det ection,” \nIEEE International Conference on  Acoustics, Speech \nand Signal Processing , pp. 1685-1688, 2009. \n[13] G. E. Poliner, D. P. Ellis, A. F. Ehmann, E. Gó mez, \nS. Streich, and B. Ong: “Melody transcription from \nmusic audio: Approaches and evaluation,” IEEE \nTransactions on Audio, Speech, and Language \nProcessing , Vol. 15, No. 4, pp. 1247-1256, 2007.  \n[14] B. B. Monson: “High-frequency energy in singing \nand speech,” Doctoral dissertation, University of \nArizona, 2011.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n166"
    },
    {
        "title": "Drum Transcription via Classification of Bar-Level Rhythmic Patterns.",
        "author": [
            "Lucas Thompson",
            "Simon Dixon",
            "Matthias Mauch"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417341",
        "url": "https://doi.org/10.5281/zenodo.1417341",
        "ee": "https://zenodo.org/records/1417341/files/ThompsonDM14.pdf",
        "abstract": "We propose a novel method for automatic drum transcrip- tion from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognis- ing individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhyth- mic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six dif- ferent drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly be- low that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment.",
        "zenodo_id": 1417341,
        "dblp_key": "conf/ismir/ThompsonDM14",
        "keywords": [
            "drum transcription",
            "automatic method",
            "individual drums",
            "bar-level drum patterns",
            "support vector machines",
            "synthesised audio",
            "audio similarity",
            "measures of precision and recall",
            "real-world recordings",
            "polyphonic accompaniment"
        ],
        "content": "DRUM TRANSCRIPTION VIA CLASSIFICATION OF\nBAR-LEVEL RHYTHMIC PATTERNS\nLucas Thompson, Matthias Mauch and Simon Dixon\nCentre for Digital Music, Queen Mary University of London\ncontact@lucas.im, fm.mauch, s.e.dixong@qmul.ac.uk\nABSTRACT\nWe propose a novel method for automatic drum transcrip-\ntion from audio that achieves the recognition of individual\ndrums by classifying bar-level drum patterns. Automatic\ndrum transcription has to date been tackled by recognis-\ning individual drums or drum combinations. In high-level\ntasks such as audio similarity, statistics of longer rhyth-\nmic patterns have been used, reﬂecting that musical rhythm\nemerges over time. We combine these two approaches by\nclassifying bar-level drum patterns on sub-beat quantised\ntimbre features using support vector machines. We train\nthe classiﬁer using synthesised audio and carry out a series\nof experiments to evaluate our approach. Using six dif-\nferent drum kits, we show that the classiﬁer generalises to\npreviously unseen drum kits when trained on the other ﬁve\n(80% accuracy). Measures of precision and recall show\nthat even for incorrectly classiﬁed patterns many individual\ndrum events are correctly transcribed. Tests on 14 acoustic\nperformances from the ENST-Drums dataset indicate that\nthe system generalises to real-world recordings. Limited\nby the set of learned patterns, performance is slightly be-\nlow that of a comparable method. However, we show that\nfor rock music, the proposed method performs as well as\nthe other method and is substantially more robust to added\npolyphonic accompaniment.\n1. INTRODUCTION\nThe transcription of drums from audio has direct applica-\ntions in music production, metadata preparation for mu-\nsical video games, transcription to musical score notation\nand for musicological studies. In music retrieval, robust\nknowledge of the drum score would allow more reliable\nstyle recognition and more subtle music search by exam-\nple. Yet like related tasks such as polyphonic piano tran-\nscription [1], a versatile, highly reliable drum transcription\nalgorithm remains elusive.\nAudio drum transcription methods have been classiﬁed\ninto two different strategies [10, 18]: segment and classify\nc\rLucas Thompson, Matthias Mauch and Simon Dixon.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Lucas Thompson, Matthias Mauch and\nSimon Dixon. “Drum Transcription via Classiﬁcation of Bar-level Rhyth-\nmic Patterns”, 15th International Society for Music Information Retrieval\nConference, 2014.\nbar 1bar 2bar 3sub-beat-quantised MFCCsoverlapping bar-wise features for classiﬁcationMFCC extractionsub-beat quantisationaudio\nbar mapping\ntrainedpatternsSVM classiﬁcation pat #23pat #14pat #18map to transcriptionbeat/downbeat positions\ndrum event transcritptionclassiﬁed drum patternsdrum event transcriptionFigure 1. Overview of the system at prediction time.\nandseparate and detect. Systems in the ﬁrst category de-\ntect a regular or irregular event grid in the signal, segment\nthe signal according to the grid, extract features such as\nMFCCs [19] or multiple low-level features [23] and then\nclassify the segments using Gaussian Mixture Models [19],\nknearest neighbour classiﬁcation [21], or Support Vector\nMachines [23]. Systems in the second category ﬁrst de-\ntect multiple streams corresponding to drum types, usually\nvia a signal or spectral decomposition approach, e.g. [2,7],\nor simpler sub-band ﬁltering [15], and then identify onsets\nin the individual streams. Other methods combine aspects\nof both categories, via adaptation [24] or joint detection\nof onsets and drums [18]. To ensure temporal consistency\n(smoothness) many approaches make use of high-level sta-\ntistical models that encode some musical knowledge, e.g.\nhidden Markov models [18]. The methods greatly differ\nin terms of the breadth of instruments they are capable\nof detecting; most detect only bass drum, snare drum and\nhi-hat [7, 14, 18, 23] or similar variants, probably because\nthese instruments (unlike crash and ride cymbals) can be\nrepresented in few frames due to their very fast decay.\nDespite the evident diversity of strategies, all existing\nmethods aim directly at detecting individual or simultane-\nous drum events. As we will see later, our approach is qual-\nitatively different, using higher-level patterns as its classi-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n187ﬁcation target. One advantage of this is that the long decay\nof cymbals is naturally modelled at the feature level.\nSince drum transcription from polyphonic audio is only\npartially solved, music information retrieval tasks rely on\n“soft” mid-level audio features to represent rhythm. Fluc-\ntuation patterns [17] summarise the frequency content of\nsub-band amplitude envelopes across 3-second windows\nand were used to evaluate song similarity and to classify\npop music genres; they have also been used to describe\nrhythmic complexity [13]. Bar-wise rhythmic amplitude\nenvelope patterns have been shown to characterise ball-\nroom dance music genres [5] and bar-wise pseudo-drum\npatterns have been shown to correlate with popular music\ngenres [6]. Rhythmic patterns have also formed the basis\nof beat tracking systems [11] and been used for downbeat\ndetection [8]. These methods share a more musically holis-\ntic approach to rhythm, i.e. they summarise rhythmic com-\nponents in a longer temporal context. Drum tutorials, too,\nusually focus on complete rhythms, often a bar in length,\nbecause “with the command of just a few basic rhythms\nyou can make your way in a rock band” [22]. In fact, we\nhave recently shown that drum patterns are distributed such\nthat a small number of drum patterns can describe a large\nproportion of actual drum events [12].\nMotivated by this result and by the effectiveness of\nmore holistic approaches to rhythm description, we pro-\npose a novel drum transcription method based on drum\npattern classiﬁcation. Our main contribution is to show\nthat the classiﬁcation of bar-length drum patterns is a good\nproxy for predicting individual drum events in synthetic\nand real-world drum recordings.\n2. METHOD\nThe proposed method is illustrated in Figure 1. It can\nbroadly be divided into two parts: a feature extraction step,\nin which MFCC frame-wise features are calculated and\nformatted into a bar-wise, sub-beat-quantised representa-\ntion, and a classiﬁcation step, in which bar-wise drum pat-\nterns are predicted from the feature representation and then\ntranslated into the desired drum transcription representa-\ntion. For the sake of this study, we assume that correct\nbeat and bar annotations are given.\n2.1 Feature extraction\nFollowing Paulus and Klapuri [19], we choose Mel-\nfrequency cepstral coefﬁcients (MFCCs) as basis features\nfor our experiments. MFCCs are extracted from audio\nsampled at 44.1 kHz with a frame size of 1024 samples\n(23ms) and a hop size of 256 samples (6ms), using an\nadaptation of the implementation provided in the VamPy\nplugin examples.1We extract 14 MFCCs (the mentioned\nimplementation uses a bank of 40 Mel-ﬁlters) per frame,\nbut discard the 0thcoefﬁcient to eliminate the inﬂuence of\noverall signal level.\nIn order to obtain a tempo-independent representa-\ntion, we assume that we know the positions of musi-\n1http://www.vamp-plugins.org/vampy.htmlcal beats and quantise the feature frames into a metrical\ngrid. This is needed for subsequent bar-wise segmenta-\ntion. Whereas beat-quantised chroma representations usu-\nally summarise chroma frames within a whole inter-beat\ninterval [16], drum information requires ﬁner temporal res-\nolution. Hence, following [12] we choose 12 sub-beats per\nbeat, which is sufﬁcient to represent the timing of the most\ncommon drum patterns. The MFCC frames belonging to\neach sub-beat are summarised into a single value by taking\nthe mean over the sub-beat duration to give 12 quantised\nframes per beat.\nSince we assume we know which beat is the downbeat,\nit is now trivial to extract bar representations from sub-\nbeat-quantised MFCC features. For example, in a44time\nsignature, one bar corresponds to 4\u000212 = 48 sub-beat-\nquantised MFCC frames. However, slight deviations in\ntiming and natural decay times of cymbals and drum mem-\nbranes mean that information on a bar pattern will exist\neven outside the bar boundaries. For this reason we also\nadd an extra beat either side of the bar lines (further dis-\ncussion in Section 3), leading to the overlapping bar repre-\nsentations illustrated in Figure 1, each 6\u000212 = 72 frames\nlong. The features we are going to use to classify44bars\ninto drum patterns will therefore comprise 936 elements\n(72 frames\u000213 MFCCs).\n2.2 Classiﬁcation and transcription mapping\nAs our classiﬁer, we use the one-vs-one multi class\nSupport Vector Machine implementation provided in the\nsklearn.svm.SVC2package of the Python machine learn-\ning library, scikit-learn [20], with the default settings us-\ning a radial basis kernel, K(x;x0) =e\u0000\rjjx\u0000x0jj2, where\n\r=1\nNandN= 936 is the feature dimension. Once\nthe classiﬁer has predicted a drum pattern for a particular\nbar, we perform a simple mapping step to obtain a drum\ntranscription: using the information about the actual start\nand end time of the bar in the recording, each of the drum\nevents that constitute the pattern are assigned to a time\nstamp within this time interval, according to their position\nin the pattern.\n3. EXPERIMENTS AND RESULTS\nWe conducted three experiments to test the effectiveness\nof the proposed method, one with synthesised test data,\nand two with real recordings of human performances. In\nall experiments, the drum pattern data for training was en-\ncoded as MIDI and then synthesised using the FluidSynth\nsoftware. Our drum pattern dictionary contains the top 50\nmost common drum patterns, including the empty pattern,\nin a collection of 70,000 MIDI ﬁles (containing only bd-\nkick, sd- snare, hh- closed hi-hat, oh- open hi-hat, ri-\nride and cr- crash cymbals) [12].3Figure 2 details how\neach drum class is distributed. Data examples and further\n2http://scikit-learn.org/stable/modules/\ngenerated/sklearn.svm.SVC.html\n3http://www.eecs.qmul.ac.uk/ ˜matthiasm/ndrum/\npatternstats/full_1-2-3-4-5-6/patternvisual_\nreduced\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n188bd sd hh ho ri cr00.10.20.30.40.50.60.70.80.9relative occurance frequency  \n% of patterns% of overall hitsFigure 2. Relative occurrence of the drum classes in terms\nof overall number of drum events and the number of pat-\nterns containing each class. There are 50 patterns with an\naverage of 11 events per pattern.\ninformation can be found on the web page that accompa-\nnies this paper.4\nWe evaluate both the pattern classiﬁcation performance\nand the quality of transcription of drum events. Pattern\naccuracy is deﬁned as\nA=number of correctly classiﬁed bars\ntotal number of bars in test set: (1)\nThe transcription of drum events is evaluated using preci-\nsion, recall and the F-measure (their harmonic mean)\nP=Nc\nNd; R =Nc\nN; F =2PR\nP+R;(2)\nwhereNdis the number of detected drum hits, Ncis the\nnumber of correctly detected drum hits and Nthe number\nof drum hits in the ground truth. The individual drum hits\nare solely based on the presence or absence of a drum hit at\na particular discrete position in the pattern grid used in the\ndictionary. In Sections 3.2 and 3.3 the ground truth drum\nhits, given as onset times, are quantised to a position in the\ngrid. Tanghe’s method [23] (Sections 3.2 and 3.3) is eval-\nuated against the original ground truth with an acceptance\nwindow of 30ms, as in the original paper.\n3.1 Multiple Synthesised Drum Kits\nThe aim of this experiment is to see how well the proposed\nclassiﬁer performs on synthetic data generated using mul-\ntiple drum kits.\n3.1.1 Training and Test Data\nIn order to create varied training and test data, we ﬁrst\ngenerate 100 unique songs, each of which is simply a ran-\ndomly permuted list of the 50 drum patterns from our dic-\ntionary. These songs are encoded as MIDI ﬁles, and we\nintroduce randomised deviations in note velocity and on-\nset times (velocity range 67–127, timing range \u000620 ms) to\nhumanise the performances. All MIDI ﬁles are then ren-\ndered to audio ﬁles (WA V) using 6 drum kits from a set\nof SoundFonts we collected from the internet. In order to\navoid complete silence, which is unrealistic in real-world\nscenarios, we add white noise over the entirety of each\n4http://www.eecs.qmul.ac.uk/ ˜matthiasm/\ndrummify/overall drum events classiﬁcation\ndrum-kit R P F accuracy\n00 98.9 (98.5) 98.9 (98.7) 98.9 (98.6) 91.1 (88.8)\n01 97.1 (97.1) 97.6 (97.7) 97.4 (97.4) 89.2 (87.9)\n02 98.3 (97.9) 98.3 (98.0) 98.3 (98.0) 87.7 (86.5)\n03 84.8 (80.3) 82.3 (85.8) 83.6 (83.0) 50.1 (47.5)\n04 92.7 (92.2) 91.2 (90.8) 92.0 (91.5) 72.0 (66.4)\n05 97.2 (97.1) 98.5 (98.5) 97.9 (97.8) 91.6 (88.6)\nmean 94.8 (93.9) 94.5 (94.9) 94.7 (94.4) 80.3 (77.6)\nTable 1. Mean classiﬁcation accuracy (%) and overall\ndrum event R, P and F metrics for left out drum-kit from\nleave-one-out cross validation on 6 different drum-kits\n(see Section 3.1). Results for non-overlapping bars are in\nbrackets.\noverall drum events\ndrum-type R P F\nbd 96.2 (96.0) 95.4 (95.0) 95.8 (95.5)\nsd 96.5 (95.4) 99.3 (99.3) 97.8 (97.0)\nhh 96.5 (95.3) 93.7 (94.8) 95.0 (95.0)\nho 59.9 (57.1) 77.3 (77.3) 61.1 (56.8)\nri 86.3 (86.4) 98.3 (99.5) 88.2 (89.0)\ncr 84.4 (75.8) 97.0 (96.5) 89.3 (82.5)\nTable 2 . R, P and F for each drum type, taken over the\nwhole test set and all kits from leave-one-out cross valida-\ntion on 6 different drum-kits (see Section 3.1). Results for\nnon-overlapping bars are in brackets.\nsong at a SNR of 55 dB. We then calculate the bar-wise\nbeat-quantised MFCC features as described in section 2.1.\nThis yields a dataset of 6\u0002100 = 600 ﬁles.\nWe use a random 70:30 train/test split of the 100 songs,\nwhere each of the 70 training songs appears in ﬁve varia-\ntions synthesised from different drum kit SoundFonts. The\nremaining 30 songs, synthesised by the sixth drum kit, are\nused for testing. In order to assess performance on differ-\nent drum kits, we cycle the use of the test drum kit in a\nleave-one-out fashion.\n3.1.2 Results\nAs Table 1 shows, our method achieves a high average ac-\ncuracy of 80.3%, despite strong variation between drum\nkits. Irrespective of whether overlapping bar-wise features\nwere used, the accuracy on drum kits 00, 01, 02 and 05\nexceeds 85%. Performance is substantially worse on drum\nkits 03 and 04 (accuracies of 50.1% and 72.0%, respec-\ntively). Listening to a subset of the synthesised songs for\ndrum kits 03 revealed that the recording used for the closed\nhi-hat sounds contains hi-hats that are slightly open, which\nis likely to cause confusion between the two hi-hat sounds.\nTo demonstrate the beneﬁt of considering extra beats\neither side of the bar boundaries, Table 1 includes the re-\nsults for non-overlapping bars. In this case we can see that\nthe context given by the neighbouring beats increases clas-\nsiﬁcation accuracy (mean increase \u00193 percentage points).\nThe greatest increase in accuracy (\u00196 percentage points)\nis observed in drum-kit 04.\nTo gain an insight into the types of patterns being mis-\nclassiﬁed, we consider those patterns for each drum-kit\nthat are misclassiﬁed more than a quarter of the time. Fig-\nure 3 contains a few example cases. The single undetected\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n189A) kit 00 – 035 misclassiﬁed as 008, error-rate 26.7% E) kit 01 - 051 as 044, error-rate 73.3%bdsdhhhoricrbdsdhhhoricrB) kit 00 – 044 misclassiﬁed as 032, error-rate 26.7% F) kit 02 - 039 as 001, error-rate 100.0%bdsdhhhoricrbdsdhhhoricrC) kit 00 – 049 misclassiﬁed as 032, error-rate 33.3% G) kit 03 - 036 as 029, error-rate 70.0%bdsdhhhoricrbdsdhhhoricrD) kit 00 – 051 misclassiﬁed as 005, error-rate 80.0% H) kit 04 - 011 as 001, error-rate 26.7%bdsdhhhoricrbdsdhhhoricrground truth onlydetected onlyground truth and detected (correct)Figure 3. Examples of misclassiﬁed patterns (see Sec-\ntion 3.1.2).\nride or crash cymbals on the ﬁrst beat in the ground-truth\n(cases F and H) are likely to be caused by the system con-\nfusing them for remainders of the previous bar. For cases\nA, B, D and G, the differences are subtle. In case A, the\npatterns differ by one hi-hat on the ﬁrst beat. Cases B,\nD and G show that on occasions the classiﬁer chooses a\npattern where the majority of the drum events are correct,\napart from a few inserted bass or snare drum events.\nIf we compare the individual drum events of the pre-\ndicted pattern against the ground-truth and use precision\nand recall measures (see Table 2) we see that the system\nachieves high F-measures for the majority of drum classes\n(mean 0.88–0.97 for bd, sd, hh, ri, cr over all kits), but not\nfor the open hi-hat class (mean F-measure 0.61).\nUsing audio features with overlapping bars leads to a\nsubstantial increase of over 8 percentage points in the re-\ncall of crash cymbal hits (84.4%) with respect to using no\noverlap (75.8%). The majority of the crash hits in our pat-\ntern dictionary occur on the ﬁrst beat of the bar, and many\nof the patterns which were misclassiﬁed without the bene-\nﬁt of the overlapping neighbouring beats are such patterns,\nhighlighting that the added context helps distinguish the\npattern from those with a decaying hi-hat or other cym-\nbal at the end of the previous bar. Note that since crash\ncymbals usually occur no more than once per bar, the clas-\nsiﬁcation accuracy in Table 1 shows larger improvement\nthan the overall drum event precision and recall values.\n3.2 Real drums Without Accompaniment\nHaving evaluated the performance of our system on syn-\nthesised data, we now test its robustness to real acoustic\ndrum data.\n3.2.1 Training and test data\nWe use the set of 100 songs described in the previous ex-\nperiment (Section 3.1.1) synthesised on all 6 drum kits\n(6\u0002100 = 600 ﬁles). Since we have shown that over-\nlapping bar-wise features provide higher accuracy (Sec-\ntion 3.1.2), we use only this feature conﬁguration to traina re-usable model, which is used in the remainder of the\nexperiments.\nAs test data we use the ENST-Drums database [9],\nwhich contains a wide range of drum recordings and\nground-truth annotations of drum event onset times. We\nselected 13 phrase performances (15-25 s) which contain\na number of similar patterns to ones in our dictionary, with\nexpressional variations and ﬁlls, and one song from the\nminus-one category, a 60’s rock song, which contains ex-\ntensive variations and use of drum ﬁlls for which there are\nno similar patterns in our dictionary. In order to convert the\nprovided ground-truth annotations to bar length drum pat-\ntern representations of the same format as those in our pat-\ntern dictionary, we annotated the beat and downbeat times\nin a semi-automatic process using Sonic Visualiser [3] and\na Vamp-plugin implementation5of Matthew Davies’ beat-\ntracker [4].\n3.2.2 Results\nThe results for the ENST-Drums tracks are given in Ta-\nble 3. The system’s performance strongly varies by track.\nOur system performs particularly well on the disco and\nrock genre recordings (F-measure 0.479-0.924), for which\nour pattern dictionary contains very similar patterns. The\nshufﬂe-blues and hard-rock patterns perform much worse\n(F-measure 0.037–0.525), which is largely due to the fact\nthat they utilise patterns outside our dictionary, bringing\nthe mean F-measure down to 0.563. In order to under-\nstand the impact of out-of-dictionary patterns, Table 3 also\nprovides the maximum possible F-measure Fmaxcalculated\nfrom our dictionary by choosing the transcription that re-\nsults in the highest F-measure for each bar, and computing\nthe overall F-measure of this transcription.\nFor example, ENST recording 069 only achieves an F\nscore of 0.288, falling short of Fmax= 0:583, as it mostly\nconsists of a typical shufﬂe drum pattern utilising the ride\ncymbal which is outside of the dictionary. However, the\npattern which the system predicts is in fact one that con-\ntains a ride cymbal, from a total of ﬁve (see Figure 2). The\nhard rock recordings make extensive use of the open hi-hat,\nwhich is not utilised in the same fashion in our dictionary;\nhere, the classiﬁer most often predicts an empty bar (hence\nthe very low scores). Note that all scores are obtained on a\nvery diverse set of 6 drum and cymbal types.\nFor comparison, we obtained an implementation of an\nexisting drum transcription method by Tanghe [23] and ran\nit on the ENST recordings, using the default pre-trained\nmodel. Since Tanghe’s method only considers bass drum,\nsnare drum and hi-hat, we constrain the evaluation to those\ndrum types, and map the open and closed hi-hat events\nfrom our algorithm to single hi-hat events. Table 4 shows\nthat our system has an F-measure of 0.73; Tanghe’s system\nperforms better overall (0.82), which is largely due to ex-\ncellent bass drum detection. Note however that our system\nobtains better performance for the snare drum (F-measure\n0.74 vs 0.70) particularly with respect to precision (0.93 vs\n5http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html#qm-barbeattracker\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n190detected drum events\ngenre tempo R P F Fmax\n038 disco slow 72.6 84.9 78.3 86.7\n039 disco medium 90.2 94.8 92.4 95.8\n040 disco fast 93.1 87.1 90.0 100.0\n044 rock slow 48.1 47.6 47.9 59.8\n045 rock medium 52.7 47.5 50.0 58.5\n046 rock fast 54.5 52.5 53.5 63.6\n055 disco slow 75.9 63.8 69.3 98.3\n061 rock slow 93.8 84.3 88.8 92.5\n069 SB slow 25.6 32.8 28.8 58.3\n070 SB medium 50.0 55.4 52.5 59.5\n075 HR slow 1.9 50.0 3.7 58.7\n076 HR medium 3.8 100.0 7.4 53.2\n085 SB slow 49.5 49.0 49.2 79.5\n116 minus-one (60s rock) 76.8 77.1 77.0 81.7\nmean 56.3 66.2 56.3 74.7\nTable 3. Real drums without accompaniment: results in\npercent for ENST-Drums dataset. SB: shufﬂe-blues; HR:\nhard rock.\nmethod metric bd sd hh overall\nProposed R 70.2 62.0 73.1 69.9\nP 60.6 92.7 83.5 76.3\nF 65.1 74.3 77.9 73.0\nTanghe et al. R 87.0 65.0 89.8 83.8\nP 99.3 75.8 73.9 80.6\nF 92.8 70.0 81.1 82.1\nTable 4. Real drums without accompaniment: Results in\npercent for drum classes reduced to bd, sd, hh (including\nho) for comparison with Tanghe et al. [23].\n0.76). With a larger dictionary, our method would be able\nto capture more details, such as drum ﬁlls, so we expect a\nsimilar system with larger dictionary to perform better.\n3.3 Polyphonic Music\nFor the minus-one recording, the ENST-Drums database\nprovides additional non-percussive accompaniment, which\nallows us to test our system on polyphonic music.\n3.3.1 Training and Test Data\nAs in the previous experiment, we use the pre-trained\nmodel from all the synthesised drum data from the exper-\niment described in Section 3.1. The test data consists of\ntheminus-one recording considered in the previous exper-\niment. We add the polyphonic accompaniment at differ-\nent levels: 0dB (fully polyphonic, no attenuation), -6dB,\n-12dB, -18dB, -24dB and -30dB.\n3.3.2 Results\nThe overall F-measures obtained by the system for the var-\nious levels of attenuation are detailed in Figure 4. We pro-\nvide the performance of the system on the recording with\nno accompaniment as a baseline (overall F-measure 0.77,\nas in Table 3). The system’s performance on all drums de-\ncays rapidly between -24 dB and -18 dB, but then stays rel-\natively robust for the most difﬁcult levels considered (0dB\nto -18dB, overall F-measure scores of 0.48–0.58).\nWe compare the performance of our system to Tanghe’s\nmethod once more on the reduced drum type set (bd, sd,\nhh). It is interesting to observe that while the F-measure on\n● ●\n●\n●\n●\n●\n●−Inf −30 −24 −18 −12 −6 03640444852566064687276●\n●\n●\n●\n●\n●dB attenuationoverall F−measure (%)●Proposed − all drums\nProposed − bd,sd,hh combined\nTanghe et al.Figure 4. Overall drum events F-measure for ENST\nrecording 116, mixed in with accompaniment at various\nlevels of attenuation.\nthe pure drums is nearly the same (Tanghe: 0.76, proposed:\n0.77), susceptibility to additional instruments strongly dif-\nfers between the methods. The F-measure of Tanghe’s\nmethod ﬁrst increases for low levels of added polyphonic\nmusic (attenuation -30, -24 dB), due to the increased recall\nas a result of the accompaniment being detected as correct\ndrum hits. For increasing levels of added accompaniment,\nperformance rapidly decreases to an overall F-measure of\n0.35 for 0 dB. By direct comparison, the proposed method\nachieves an F-measure of 0.60 even at 0 dB, demonstrat-\ning its superior robustness against high levels of accom-\npaniment (-12, -6, 0 dB). Even for the more difﬁcult task\nof recognising all 6 drum types, the proposed method (F-\nmeasure 0.48) outperforms Tanghe’s.\n4. DISCUSSION\nOur results show not only that the proposed bar-wise drum\npattern classiﬁcation method is an effective, robust way to\ntranscribe drums, but also that the ﬁrst step for immediate\nimprovement should be to increase the dictionary size in\norder to obtain better coverage. In addition, relaxing the\nstrict holistic pattern approach by classifying patterns of\nindividual instruments would allow for the recognition of\ncombinations of patterns and hence of many new, unseen\npatterns. Another obvious route for improvement is to train\nour classiﬁer on drum data with added polyphonic music\ncontent, which is likely to further increase robustness in\npolyphonic conditions.\nThe general approach of bar-wise drum classiﬁcation is\nnot exhausted by our particular implementation, and we\nexpect to be able to gain further improvements by explor-\ning different classiﬁers, different amounts of neighbour-\nhood context or different basic features (e.g. non-negative\nmatrix factorisation activations). Furthermore, to use the\nmethod in an interactive annotation system, it would be in-\nteresting to investigate bar-wise conﬁdence scores for user\nguidance. Genre-speciﬁc training data could improve the\nperformance of such systems. Finally, using more holistic\nfeatures instead of single frames may also be applicable to\nother music informatics tasks such as chord transcription.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1915. CONCLUSIONS\nWe have presented a novel approach to drum transcrip-\ntion from audio using drum pattern classiﬁcation. Instead\nof detecting individual drums, our method ﬁrst predicts\nwhole drum patterns using an SVM classiﬁer trained on\na large collection of diverse synthetic data, and then maps\nthe drums from the recognised patterns to the relative time-\nstamps to achieve a transcription. The method performs\nvery well on synthetic data, even with tempo and velocity\nvariations on previously unseen sampled drum kits (mean\npattern accuracy: 80%). Even though the pattern accu-\nracy range differs between drum kits (50.1%–91.6%) many\ndrum events are still classiﬁed with high precision and re-\ncall (F-measure 0.836–0.989). Unlike existing techniques,\nour drum detection includes open hi-hat, closed hi-hat,\ncrash and ride cymbals, which are all reliably detected in\nmost cases. Extending the bar patterns by one beat either\nside and thus obtaining overlapping patterns leads to bet-\nter accuracy, mainly due to improved recognition of crash\ncymbals. On real drum recordings performance strongly\ndepends on genre (F-measure for rock and disco: 0.479–\n0.924; hard-rock and shufﬂe-blues: 0.037–0.525), mainly\ndue to the limited types of drum patterns in our current dic-\ntionary. This results in a performance slightly below that\nof a comparable method. However, we show that for rock\nmusic, the proposed method performs as well as the other\nmethod (F-measure: 0.77) and is substantially more robust\nto added polyphonic accompaniment.\n6. ACKNOWLEDGEMENTS\nMatthias Mauch is supported by a Royal Academy of En-\ngineering Research Fellowship.\n7. REFERENCES\n[1] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: Challenges and\nfuture directions. Journal of Intelligent Information Systems,\n41(3):407–434, 2013.\n[2] E. Benetos, S. Ewert, and T. Weyde. Automatic transcrip-\ntion of pitched and unpitched sounds from polyphonic music.\nInIEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP 2014), May 2014.\n[3] C. Cannam, C. Landone, M. B. Sandler, and J. P. Bello. The\nSonic Visualiser: A visualisation platform for semantic de-\nscriptors from musical signals. In Proceedings of the 7th In-\nternational Conference on Music Information Retrieval (IS-\nMIR 2006), pages 324–327, 2006.\n[4] M. E. P. Davies and M. D. Plumbley. Context-dependent\nbeat tracking of musical audio. IEEE Transactions on Audio,\nSpeech, and Language Processing, 15(3):1009–1020, 2007.\n[5] S. Dixon, F. Gouyon, and G. Widmer. Towards characteri-\nsation of music via rhythmic patterns. In Proceedings of the\n5th International Conference on Music Information Retrieval\n(ISMIR 2004), pages 509–516, 2004.\n[6] D. P. W. Ellis and J. Arroyo. Eigenrhythms: Drum pattern\nbasis sets for classiﬁcation and generation. In Proceedings of\nthe 5th International Conference on Music Information Re-\ntrieval (ISMIR 2004), pages 101–106, 2004.\n[7] D. FitzGerald, R. Lawlor, and E. Coyle. Prior subspace anal-\nysis for drum transcription. In Proceedings of the AES 114th\nInternational Convention, 2003.[8] D. G ¨artner. Unsupervised Learning of the Downbeat in Drum\nPatterns. In Proceedings of the AES 53rd International Con-\nference, pages 1–10, 2014.\n[9] O. Gillet and G. Richard. ENST-Drums: An extensive audio-\nvisual database for drum signals processing. In Proceedings\nof the 7th International Conference on Music Information Re-\ntrieval (ISMIR 2006), pages 156–159, 2006.\n[10] O. Gillet and G. Richard. Transcription and separation of\ndrum signals from polyphonic music. IEEE Transactions on\nAudio, Speech, and Language Processing, 16(3):529–540,\n2008.\n[11] M. Goto. An audio-based real-time beat tracking system for\nmusic with or without drum-sounds. Journal of New Music\nResearch, 30(2):159–171, 2001.\n[12] M. Mauch and S. Dixon. A corpus-based study of rhythm\npatterns. In Proceedings of the 13th International Conference\non Music Information Retrieval (ISMIR 2012), pages 163–\n168, 2012.\n[13] M. Mauch and M. Levy. Structural change on multiple time\nscales as a correlate of musical complexity. In Proceedings\nof the 12th International Conference on Music Information\nRetrieval (ISMIR 2011), pages 489–494, 2011.\n[14] M. Miron, M. E. P. Davies, and F. Gouyon. Improving the\nreal-time performance of a causal audio drum transcription\nsystem. In Proceedings of the Sound and Music Computing\nConference (SMC 2013), pages 402–407, 2013.\n[15] M. Miron, M. E. P. Davies, and Fabien Gouyon. An\nopen-source drum transcription system for Pure Data and\nMax MSP. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP 2013), pages 221–\n225. IEEE, 2013.\n[16] Y . Ni, M. McVicar, R. Santos-Rodriguez, and T. De Bie. An\nend-to-end machine learning system for harmonic analysis of\nmusic. IEEE Transactions on Audio, Speech, and Language\nProcessing, 20(6):1771–1783, 2012.\n[17] E. Pampalk, A. Flexer, and G. Widmer. Improvements of\naudio-based music similarity and genre classiﬁcaton. In Pro-\nceedings of the 6th International Conference on Music Infor-\nmation Retrieval, pages 634–637, 2005.\n[18] J. Paulus and A. Klapuri. Drum sound detection in poly-\nphonic music with hidden Markov models. EURASIP Journal\non Audio, Speech, and Music Processing, 2009:14, 2009.\n[19] J. K. Paulus and A. P. Klapuri. Conventional and periodic n-\ngrams in the transcription of drum sequences. In Proceed-\nings of the International Conference on Multimedia and Expo\n(ICME 2003), volume 2, pages II–737. IEEE, 2003.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and ´E. Duchesnay. Scikit-learn: Ma-\nchine learning in Python. The Journal of Machine Learning\nResearch, 12:2825–2830, 2011.\n[21] V . Sandvold, F. Gouyon, and P. Herrera. Percussion classiﬁ-\ncation in polyphonic audio recordings using localized sound\nmodels. In Proceedings of the 5th International Conference\non Music Information Retrieval (ISMIR 2004), pages 537–\n540, 2004.\n[22] J. Strong. Drums For Dummies. John Wiley & Sons, 2011.\n[23] K. Tanghe, S. Degroeve, and B. De Baets. An algorithm for\ndetecting and labeling drum events in polyphonic music. In\nProceedings of the 1st Annual Music Information Retrieval\nEvaluation Exchange (MIREX 2005), pages 11–15, 2005.\n[24] K. Yoshii, M. Goto, and H. G. Okuno. Automatic drum sound\ndescription for real-world music using template adaptation\nand matching methods. In Proceedings of the 5th Interna-\ntional Conference on Music Information Retrieval (ISMIR\n2004), pages 184–191, 2004.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n192"
    },
    {
        "title": "Design And Evaluation of Onset Detectors using Different Fusion Policies.",
        "author": [
            "Mi Tian 0001",
            "György Fazekas",
            "Dawn A. A. Black",
            "Mark B. Sandler"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415002",
        "url": "https://doi.org/10.5281/zenodo.1415002",
        "ee": "https://zenodo.org/records/1415002/files/TianFBS14.pdf",
        "abstract": "Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detec- tion methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achiev- ing better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the pa- rameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset de- tectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter set- tings.",
        "zenodo_id": 1415002,
        "dblp_key": "conf/ismir/TianFBS14",
        "keywords": [
            "Music Information Retrieval",
            "Note onset detection",
            "Fusion policies",
            "Existing algorithms",
            "Combining different algorithms",
            "Linear combination",
            "Late decision fusion",
            "Large scale evaluation",
            "Published datasets",
            "Chinese traditional instrument samples"
        ],
        "content": "DESIGN AND EV ALUATION OF ONSET DETECTORS USING\nDIFFERENT FUSION POLICIES\nMi Tian, Gy ¨orgy Fazekas, Dawn A. A. Black, Mark Sandler\nCentre for Digital Music, Queen Mary University of London\nfm.tian, g.fazekas, dawn.black, mark.sandlerg@qmul.ac.uk\nABSTRACT\nNote onset detection is one of the most investigated tasks\nin Music Information Retrieval (MIR) and various detec-\ntion methods have been proposed in previous research. The\nprimary aim of this paper is to investigate different fusion\npolicies to combine existing onset detectors, thus achiev-\ning better results. Existing algorithms are fused using three\nstrategies, ﬁrst by combining different algorithms, second,\nby using the linear combination of detection functions, and\nthird, by using a late decision fusion approach. Large scale\nevaluation was carried out on two published datasets and a\nnew percussion database composed of Chinese traditional\ninstrument samples. An exhaustive search through the pa-\nrameter space was used enabling a systematic analysis of\nthe impact of each parameter, as well as reporting the most\ngenerally applicable parameter settings for the onset de-\ntectors and the fusion. We demonstrate improved results\nattributed to both fusion and the optimised parameter set-\ntings.\n1. INTRODUCTION\nThe automatic detection of onset events is an essential part\nin many music signal analysis schemes and has various ap-\nplications in content-based music processing. Different ap-\nproaches have been investigated for onset detection in re-\ncent years [1,2]. As the main contribution of this paper, we\npresent new onset detectors using different fusion policies,\nwith improved detection rates relying on recent research in\nthe MIR community. We also investigate different conﬁg-\nurations of onset detection and fusion parameters, aiming\nto provide a reference for conﬁguring onset detection sys-\ntems.\nThe focus of ongoing onset detection work is typically\ntargeting Western musical instruments. Apart from using\ntwo published datasets, a new database is incorporated into\nour evaluation, collecting percussion ensembles of Jingju,\nalso denoted as Peking Opera or Beijing Opera, a major\nc\rMi Tian, Gy ¨orgy Fazekas, Dawn A. A. Black, Mark San-\ndler.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Mi Tian, Gy ¨orgy Fazekas, Dawn\nA. A. Black, Mark Sandler. “Design and Evaluation of Onset Detectors\nUsing Different Fusion Policies”, 15th International Society for Music\nInformation Retrieval Conference, 2014.genre of Chinese traditional music1. By including this\ndataset, we aim at increasing the diversity of instrument\ncategories in the evaluation of onset detectors, as well as\nextending the research to include non-Western music types.\nThe goal of this paper can be summarised as follows: i)\nto evaluate fusion methods in comparison with the baseline\nalgorithms, as well as a state-of-the-art method2;ii)to in-\nvestigate which fusion policies and which pair-wise com-\nbinations of onset detectors yield the most improvement\nover standard techniques; iii)to ﬁnd the best performing\nconﬁgurations by searching through the multi-dimensional\nparameter space, hence identifying emerging patterns in\nthe performances of different parameter settings, showing\ngood results across different datasets; iv)to investigate the\nperformance difference in Western and non-Western per-\ncussive instrument datasets.\nIn the next section, we present a review of related work.\nDescriptions of the datasets used in this experiment are\ngiven in Section 3. In Section 4, we introduce different fu-\nsion strategies. Relevant post-processing and peak-picking\nprocedures, as well as the parameter search process will\nbe discussed in Section 5. Section 6 presents the results,\nwith a detailed analysis and discussion of the performance\nof the fusion methods. Finally, the last section summarises\nour ﬁndings and provides directions for future work.\n2. RELATED WORK\nMany onset detection algorithms and systems have been\nproposed in recent years. Common approaches using en-\nergy or phase information derived from the input signal in-\nclude the high frequency content (HFC) and complex do-\nmain (CD) methods. See [1,6] for detailed reviews and [9]\nfor further improvements. Pitch contours and harmonic-\nity information can also be indicators for onset events [7].\nThese methods shows some superiority over energy based\nones in case of soft onsets.\nOnset detection systems using machine learning tech-\nniques have also been gaining popularity in recent years3.\nThe winner of MIREX 2013 audio onset detection task\nutilises convolutional neural networks to classify and dis-\ntinguish onsets from non-onset events in the spectrogram\n[13]. The data-driven nature of these methods makes the\n1http://en.wikipedia.org/wiki/Peking_opera\n2Machine learning-based methods are excluded from this study to\nlimit the scope of our work.\n3http://www.music-ir.org/mirex/wiki/2013:\nAudio_Onset_Detection\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n631detection less dependent on onset types, though a computa-\ntionally expensive training process is required. A promis-\ning approach for onset detection lies in the fusion of multi-\nple detection methods. Zhou et al. proposed a system inte-\ngrating two detection methods selected according to prop-\nerties of the target onsets [17]. In [10], pitch, energy and\nphase information are considered in parallel for the detec-\ntion of pitched onsets. Another fusion strategy is to com-\nbine peak score information to form new estimations of\nthe onset events [8]. Albeit fusion has been used in previ-\nous work, there is a lack of systematic evaluation of fusion\nstrategies and applications in the current literature. This\npaper focusses on the assessment of different fusion poli-\ncies, from feature-level and detection function-level fusion\nto higher level decision fusion.\nThe success of an onset detection algorithm largely de-\npends on the signal processing methods used to extract\nsalient features from the audio that emphasise the features\ncharacterising onset events as well as smoothing the noise\nin the detection function. Various signal processing tech-\nniques have been introduced in recent studies, such as vi-\nbrato suppression [3] and adaptive thresholding [1]. In\n[14], adaptive whitening is presented where each STFT\nbins magnitude is divided by the an average peak for that\nbin accumulated over time. This paper also investigates\nthe performances of some commonly used signal process-\ning modules within onset detection systems.\n3. DATASETS\nIn this study, we use two previously released evaluation\ndatasets and a newly created one. The ﬁrst published dataset\ncomes from [1], containing 23 audio tracks with a total du-\nration of 190 seconds and having 1058 onsets. These are\nclassiﬁed into four groups: pitched non-percussive (PNP),\ne.g. bowed strings, 93 onsets, pitched percussive (PP), e.g.\npiano, 482 onsets4, non-pitched percussive (NPP), e.g.\ndrums, 212 onsets, and complex mixtures (CM), e.g. pop\nsinging music, 271 onsets. The second set comes from [2]\nwhich is composed of 30 samples5of 10 second audio\ntracks, containing 1559 onsets in total, covering also four\ncategories: PNP (233 onsets in total), PP (152 onsets), NPP\n(115 onsets), CM (1059 onsets). The use of these datasets\nenables us to test the algorithms on a range of different in-\nstruments and onset types, and provides for direct compar-\nison with published work. The combined dataset used in\nthe evaluation of our work is composed of these two sets.\nThe third dataset consists of recordings of the four ma-\njor percussion instruments in Jingju: bangu (clapper- drum),\ndaluo (gong-1), naobo (cymbals), and xiaoluo (gong-2).\nThe samples are manually mixed using individual record-\nings of these instruments with possibly simultaneous on-\nsets to closely reproduce real world conditions. See [15]\nfor more details on the instrument types and the dataset.\nThis dataset includes 10 samples of 30-second excerpts\n4A 7-onset discrepancy(482 instead of 489) from the reference paper\nis reported by the original author due to revisions of annotations.\n5Only a subset of this dataset presented in the original paper is re-\nceived from the author for the evaluation in this paper.with 732 onsets. We also use NPP onsets from the ﬁrst two\ndatasets to form the fourth one, providing a direct com-\nparison with the Chinese NPP instruments. All stimuli are\nmono signals sampled at 44.1kHz6and 16 bits per sample,\nhaving 3349 onsets in total.\n4. FUSION EXPERIMENT\nThe aim of information fusion is to merge information from\nheterogeneous sources to reduce uncertainty of inferences\n[11]. In our study, six spectral-based onset detection al-\ngorithms are considered as baselines for fusion: high fre-\nquency content (HFC), spectral difference (SD) complex\ndomain (CD), broadband energy rise (BER), phase devia-\ntion (PD), outlined in [1], and SuperFlux (SF) from recent\nwork [4]. We also developed and included in the fusion a\nmethod based on Linear Predictive Coding [12], where the\nLPC coefﬁcients are computed using the Levinson-Durbin\nrecursion, and the onset detection function is derived from\nthe LPC error signal.\nThree fusion policies are used in our experiments: i)\nfeature-level fusion, ii)fusion using the linear combination\nof detection functions and iii)decision fusion by selecting\nand merging onset candidates. All pairwise combination\nof the baseline algorithms are amenable for the latter two\nfusion policies. However, not all algorithms can be mean-\ningfully combined using feature-level fusion. For example\nCD can be considered as an existing combination of SD\nand PD, therefore combining CD with either of these two\nat a feature level is not sensible. In this study, 10 feature-\nlevel fusion, 13 linear combination based fusion and 15\ndecision fusion based methods are tested. These are com-\npared to the 7 original methods, giving us 45 detectors in\ntotal. In the following, we describe speciﬁc fusion policies.\nWe assume familiarity with onset detection principles and\nrestrain from describing these details, please see [1] for a\ntutorial.\n4.1 Feature-level Fusion\nIn feature-level fusion, multiple algorithms are combined\nto compute fused features. For conciseness, we provide\nonly one example combining BER and SF, denoted BERSF,\nutilising the vibrato suppression capability of SF [4] for de-\ntecting soft onsets, as well as the good performance of BER\nfor detecting percussive onsets with sharp energy bursts\n[1]. Here, we use the BER to mask the SF detection func-\ntion as described by Equation (1). In essence, SF is used\ndirectly when there is evidence for a sharp energy rise, oth-\nerwise it is further smoothed using a median ﬁlter.\nODF (n) =\u001aSF(n) ifBER (n)>\r\n\u0015(SF(n)) otherwise;(1)\nwhere\ris an experimentally deﬁned threshold, \u0015is a weight-\ning constant set to 0.9 and SF(n)is the median ﬁltered\ndetection function with a window size of 3 frames.\n6Some audio ﬁles were upsampled to obtain a uniform dataset.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n6324.2 Linear Combination of Detection Functions\nIn this method, two time aligned detection functions are\nused and their weighted linear combination is computed to\nform a new detection function as shown in Equation 2:\nODF (n) =wODF 1(n) + (1\u0000w)ODF 2(n); (2)\nwhereODF 1andODF 2are two normalised detection func-\ntions andwis a weighting coefﬁcient (0\u0014w\u00141).\n4.3 Decision Fusion\nThis fusion method operates at a later stage and combines\nprior decisions of two detectors. Post-processing and peak\npicking are applied separately yielding two lists of onset\ncandidates. Onsets from the two lists occurring within a\nﬁxed temporal tolerance window will be merged and ac-\ncepted. Let TS1andTS2be the lists of onset locations\ngiven by two different detectors, iandjbe indexes of on-\nsets in the candidate lists and \u000ethe tolerance time window.\nThe ﬁnal onset locations are generated using the fusion\nstrategy described by Algorithm 1.\nAlgorithm 1 Onset decision fusion\n1:procedure DECISION FUSION (TS 1;TS 2)\n2:I;J 0 :len(TS 1)\u00001,0 :len(TS 2)\u00001\n3:TS empty list\n4: for alli;jinproduct(I;J )do\n5: ifabs(TS 1[i]\u0000TS2[j])<\u000ethen\n6: insert sorted: TS mean(TS 1[i];TS 2[j])\n7: returnTS\n5. PEAK PICKING AND PARAMETER SEARCH\n5.1 Smoothing and Thresholding\nPost-processing is an optional stage to reduce noise that in-\nterferes with the selection of maxima in the detection func-\ntion. In this study, three post-processing blocks are used: i)\nDC removal and normalisation, ii)zero-phase low-pass ﬁl-\ntering and iii)adaptive thresholding. In conventional nor-\nmalisation, data is scaled using a ﬁxed constant. Here we\nuse a normalisation coefﬁcient computed by weighting the\ninput exponentially. After removing constant offsets, the\ndetection function is normalised using the coefﬁcient Al-\nphaNorm calculated by Equation (3):\nAlphaNorm =\u0012P\nnjODF (n)j\u000b\nlen(ODF )\u00131\n\u000b\n(3)\nA low-pass ﬁlter is applied to the detection function to\nreduce noise. To avoid introducing delays, a zero phase ﬁl-\nter is employed at this stage. Finally, adaptive thresholding\nusing a moving median ﬁlter is applied following Bello [1],\nto avoid the common pitfalls of using a ﬁxed threshold for\npeak picking.\n5.2 Peak Picking\n5.2.1 Polynomial Fitting\nThe use of polynomial ﬁtting allows for assessing the shape\nand magnitude of peaks separately. Here we ﬁt a second-degree polynomial on the detection function around local\nmaxima using a least squares method, following the QM\nVamp Plugins7. The coefﬁcients aandcof the quadratic\nequationy=ax2+bx+care used to detect both sharper\npeaks, under the condition a > tha, and peaks with a\nhigher magnitude, when c>thc. The corresponding thresh-\nolds are computed from a single sensitivity parameter called\nthreshold usingtha= (100\u0000threshold )=1000 for the\nquadratic term and thc= (100\u0000threshold )=1500 for the\nconstant term. The linear term bcan be ignored.\n5.2.2 Backtracking\nIn case of many musical instruments, onsets have longer\ntransients without a sharp burst of energy rise. This may\ncause energy based detection functions to exhibit peaks\nafter the perceived onset locations. V os and Rasch con-\nclude that onsets are perceived when the envelope reaches\na level of roughly 6-15 dB below the maximum level of\nthe tones [16]. Using this rationale, we trace the onset lo-\ncations from the detected peak position back to a hypoth-\nesised earlier “perceived” location. The backtracking pro-\ncedure is based on measuring relative differences in the\ndetection function, as illustrated by Algorithm 2, where \u0012\nis the threshold used as a stopping condition. We use the\nimplementation available in the QM Vamp Plugins.\nAlgorithm 2 Backtracking\nRequire: idx: index of a peak location in the ODF\n1:procedure BACKTRACKING (idx;ODF;\u0012 )\n2:\u000e;\r 0\n3: whileidx> 1do\n4:\u000e ODF [idx]\u0000ODF [idx\u00001]\n5: if\u000e<\r\u0003\u0012then\n6: break\n7:idx idx\u00001\n8:\r \u000e\n9: returnidx\n5.3 Parameter Search\nAn exhaustive search is carried out to ﬁnd the conﬁgu-\nrations in the parameter space yielding the best detection\nrates. The following parameters and settings, related to the\nonset detection and fusion stages, are evaluated: i)adaptive\nwhitening (wht ) on/off; ii)detection sensitivity (thresh-\nold), ranging from 0.1 to 1.0 with an increment of 0.1; iii)\nbacktracking threshold (\u0012 ), ranging from 0.4 to 2.4 with 8\nequal subdivisions (the upper bound is set to an empirical\nvalue 2.4 in the experiment since the tracking will not go\nbeyond the previous valley); iv)linear combination coefﬁ-\ncient (w ), ranging from 0.0 to 1.0 with an increment of 0.1;\nv)tolerance window length (\u000e ) for decision fusion, rang-\ning from 0.01 to 0.05 (in second) having 8 subdivisions.\nThis gives a 5-dimensional space and all combinations of\nall possible values described above are evaluated. This re-\nsults in 180 conﬁgurations in case of standard detectors\nand feature-level fusion, 1980 in case of linear fusion and\n1620 for decision fusion. The conﬁgurations are described\n7http://www.vamp-plugins.org\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n633using the Vamp Plugin Ontology8and the resulting RDF\nﬁles are used by Sonic Annotator [5] to conﬁgure the de-\ntectors. The test result will thus give us not only the overall\nperformance of each onset detector, but also uncover their\nstrengths and limitations across different datasets and pa-\nrameter settings.\n6. EV ALUATION AND RESULTS\n6.1 Analysis of Overall Performance\nFigure 1 provides an overview of the results, showing the\nF-measure for the top 12 detectors in our study9. Detec-\ntors are ranked by the median showing the overall perfor-\nmance increase due to fusion across the entire range of pa-\nrameter settings. Due to space limitations, only a subset of\nthe results are reported in this paper. The complete result\nset for all tested detectors under all conﬁgurations on dif-\nferent datasets is available online10, together with Vamp\nplugins of all tested onset detectors. The names of the fu-\nsion algorithms come from the abbreviations of the con-\nstituent methods, while the numbers represent the fusion\npolicy: 0: feature-level fusion, 1: linear combination of\ndetection functions and2: decision fusion.\nCDSF-1 yields improved F-measure for the combined\ndataset by 3.06% and 6.14% compared to the two origi-\nnal methods SF and CD respectively. Smaller interquartile\nranges (IQRs) observed in case of CD, SD and HFC based\nmethods show they have less dependency on the conﬁgu-\nration. BERSF-2 and BERSF-1 vary the most in perfor-\nmance, also reﬂected from their IQRs. In case of BERSF-\n2, the best performance is obtained using the widest con-\nsidered tolerance window (0.05s), with modest sensitivity\n(40%). However, decreasing the tolerance window size has\nan adverse effect on the performance, yielding one of the\nlowest detection rates caused by the signiﬁcant drop of re-\ncall. In case of BERSF-1, a big discrepancy between the\nbest and worst performing conﬁgurations can be observed.\nThis is partly because the highest sensitivity setting has a\nnegative effect on SF causing very low precision.\nTable 1 shows the results ranked by F-measure, preci-\nsion and recall with corresponding standard deviations for\nthe ten best detectors as well as all baseline methods. Stan-\ndard deviations are computed over the results for all conﬁg-\nurations in each dataset. SF is ranked in the best perform-\ning ten, thus it is excluded from the baseline. Nine out of\nthe top ten detectors are fusion methods. CDSF-1 performs\nthe best for all datasets (including CHN-NPP and WES-\nNPP that are not listed in the table) while BERSF yields\nthe second best performance in the combined, WES-NPP\nand JPB datasets. Corresponding parameter settings for the\ncombined dataset are given in Table 2.\nFusion policies may perform differently in the evalu-\nation. In case of feature-level fusion, we compared how\ncombined methods score relative to their constituents. The\n8http://www.omras2.org/VampOntology\n9Due to different post-processing stages, the results reported here may\ndiverge from previously published results.\n10http://isophonics.net/onset-fusion\nCDSF-1BERSF-0SF\nBERSF-2 BERSF-1CD\nCDSD-1HFCCD-1CDSF-2SD\nHFCSD-1CDSD-20.450.500.550.600.650.700.750.800.85Average F measure\nFigure 1. F-meaure of all conﬁgurations for the top 12\ndetectors. (Min, ﬁrst and third quartile and max value of\nthe data are represented by the bottom bar of the whiskers,\nbottom and upper borders of the boxes and upper bar of the\nwhiskers respectively. Median is shown by the red line)\nmethod threshold\u0012 whtw\u000e(s)\nCDSF-1 10.0 2.15 off 0.20 n/a\nBERSF-1 10.0 2.40 off 0.30 n/a\nBERSF-2 40.0 2.15 off n/a 0.05\nBERSF-0 30.0 2.40 off n/a n/a\nCDSF-2 50.0 2.40 off n/a 0.05\nSF 20.0 2.40 off n/a n/a\nCDBER-1 10.0 2.40 off 0.50 n/a\nBERSD-1 10.0 2.40 off 0.60 n/a\nHFCCD-1 20.0 1.15 off 0.50 n/a\nCDBER-2 50.0 1.15 off n/a 0.05\nmean 25.90 2.100 - 0.4200 0.05\nstd 15.01 0.4848 - 0.1470 0.00\nmedian 20.0 2.15 - 0.50 0.05\nmode 10.0 2.40 off 0.50 0.05\nTable 2. Parameter settings for the ten best performing de-\ntectors, threshold: overall detection sensitivity; \u0012: back-\ntracking threshold; wht: adaptive whitening; w: linear\ncombination coefﬁcient; \u000e: tolerance window size.\nperformances vary between datasets, with only HFCBER-\n0 outperforming both HFC and BER on the combined and\nSB datasets in terms of mean F-measure. However, ﬁve\nperform better than their two constitutes on JPB, two on\nCHN-NPP and ﬁve on WES-NPP dataset (these results are\npublished online). A more detailed analysis of these per-\nformance differences constitutes future work.\nWhen comparing linear fusion of detection functions\nwith decision fusion, the former performs better across all\ndatasets in all but one cases, the fusion of HFC and BER.\nEven in this case, linear fusion yields close performance\nin terms of mean F-measure. Interesting observations also\nemerge for particular methods on certain datasets. The lin-\near fusion based detectors involving LPC and PD (SDPD-\n1 and LPCPD-1) show better performances in the case of\nthe CHN-NPP dataset compared to their performances on\nother datasets as well those given by their constituent meth-\nods (please see table online). Further analysis, for instance,\nby looking at statistical signiﬁcance of these observations\nis required to identify relevant instrument properties.\nWhen comparing BERSF-2, CDSF-2 and CDBER-2 to\nthe other detectors in Table 1, notably higher standard de-\nviations in recall andF-measure are shown, indicating this\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n634method F (combined) P (combined) R (combined) F (sb) P (sb) R (sb) F (jpb) P (jpb) R (jpb)\nCDSF-1 0.8580 0.0613 0.9054 0.1195 0.8153 0.0609 0.8194 0.0598 0.8455 0.1165 0.7949 0.0681 0.9286 0.0649 0.9748 0.1241 0.8865 0.0525\nBERSF-1 0.8559 0.0941 0.8857 0.1363 0.8280 0.0866 0.8126 0.0961 0.8191 0.1306 0.8062 0.0988 0.9283 0.0925 0.9718 0.1463 0.8885 0.0710\nBERSF-2 0.8528 0.1684 0.8901 0.1411 0.8186 0.2028 0.8088 0.1677 0.8729 0.1470 0.7536 0.2055 0.9230 0.1724 0.9637 0.1310 0.8856 0.2011\nBERSF-0 0.8451 0.0722 0.8638 0.1200 0.8272 0.0701 0.8025 0.0723 0.8185 0.1134 0.7870 0.0744 0.9175 0.0747 0.9712 0.1322 0.8694 0.0658\nCDSF-2 0.8392 0.1537 0.8970 0.1129 0.7884 0.1855 0.7892 0.1758 0.8336 0.1251 0.7493 0.2014 0.9165 0.1344 0.9642 0.1001 0.8732 0.1690\nSF 0.8274 0.0719 0.8313 0.1209 0.8234 0.0657 0.8126 0.0744 0.8191 0.1241 0.8063 0.0737 0.8488 0.0704 0.8290 0.1177 0.8694 0.0558\nCDBER-1 0.8145 0.0809 0.8210 0.1276 0.8080 0.0792 0.7877 0.0829 0.7972 0.1295 0.7785 0.0893 0.8560 0.0793 0.8678 0.1253 0.8446 0.0667\nBERSD-1 0.8073 0.0792 0.8163 0.1311 0.7986 0.0812 0.7843 0.0828 0.7985 0.1358 0.7707 0.0915 0.8420 0.0756 0.8310 0.1252 0.8532 0.0685\nHFCCD-1 0.8032 0.0472 0.8512 0.1179 0.7603 0.0734 0.7802 0.0448 0.8387 0.1239 0.7293 0.0765 0.8416 0.0511 0.8376 0.1101 0.8456 0.0705\nCDBER-2 0.7967 0.2231 0.8423 0.1404 0.7558 0.2398 0.7605 0.2279 0.8140 0.1607 0.7138 0.2384 0.8498 0.2291 0.8853 0.1273 0.8170 0.2494\nCD 0.7966 0.0492 0.8509 0.1164 0.7489 0.0672 0.7692 0.0467 0.8361 0.1191 0.7123 0.0709 0.8320 0.0535 0.8692 0.1128 0.7979 0.0636\nBER 0.7883 0.0942 0.7776 0.1184 0.7994 0.1001 0.7626 0.0974 0.7521 0.1166 0.7138 0.1119 0.8254 0.0920 0.7968 0.1226 0.8561 0.0851\nSD 0.7795 0.0466 0.8354 0.1269 0.7305 0.0733 0.7604 0.0450 0.8311 0.1326 0.7009 0.0785 0.8210 0.0491 0.8202 0.1190 0.8217 0.0676\nHFC 0.7712 0.0412 0.8011 0.1225 0.7436 0.0898 0.7411 0.0375 0.7818 0.1291 0.7044 0.0844 0.8159 0.0496 0.8082 0.1138 0.8236 0.1002\nLPC 0.7496 0.0658 0.7671 0.1103 0.7330 0.1061 0.7243 0.0657 0.7494 0.1069 0.7009 0.1019 0.7913 0.0662 0.8041 0.1164 0.7788 0.1118\nPD 0.6537 0.1084 0.5775 0.1008 0.7530 0.2235 0.6143 0.1093 0.5230 0.0688 0.7308 0.2302 0.7114 0.1115 0.6513 0.1536 0.7836 0.2158\nTable 1. F-measure (F), Precision (P) and Recall ( R) for dataset combined, SB,JPB for detectors under best performing\nconﬁgurations from the parameter search, with corresponding standard deviations over different conﬁgurations.\nstatistic Combined SB JPB CHN-NPP WES-NPP\nmean 0.7731 0.7438 0.8183 0.8527 0.8358\nstd 0.0587 0.0579 0.0628 0.1206 0.0641\nmedian 0.7818 0.7595 0.8226 0.8956 0.8580\nTable 3. Statistics for F-measure of the ten detectors with\ntheir best performances from Table 1 for different datasets\nfusion policy is more sensitive to the choice of parameters.\nA possible improvement in this fusion policy would be to\nmake the size of the tolerance window dependent on the\nmagnitude of relevant peaks of the detection functions.\nThe results also vary across different datasets. Table 3\nsummarises F-measure statistics computed over the detec-\ntors listed in Table 1 at their best setting for each datasets\nused in this paper. In comparison with SB, the JPB dataset\nexhibits higher F-measure. This dataset has larger diversity\nin terms of the length of tracks and the level of complex-\nity, while the SB dataset mainly consists of complex mix-\nture (CM) onsets type. Both the Chinese and Western NPP\nonset class provides noticeably higher detection rate com-\npared to the mix-typed datasets. Though the CHN-NPP set\nshows the largest standard deviation, suggesting a greater\nvariation in performance between the different detectors\nfor these instruments. Apart from aiming at optimal over-\nall detection results, it is also useful to consider when and\nhow a certain onset detector exhibits the best performance,\nwhich constitutes future work.\n6.2 Parameter Speciﬁcations\nFor general datasets a low detection sensitivity value is\nfavourable, which is supported by the fact that 30 out of\nthe 45 tested methods yield the best performances with a\nsensitivity lower than 50% (see online). In 23 out of all\ncases, the value of the backtracking threshold was the high-\nest considered in our study (2.4) when the detectors yield\nthe best performances for the combined dataset, and it was\nunanimously at a high value for all other datasets including\nthe percussive ones. This suggests that in many cases, the\nperceived onset will be better characterised by the valley of\nthe detection function prior to the detected peak. Note that\n0.10 0.12 0.14 0.16 0.18 0.20 0.22\nFP rate (FP / detected onsets)0.760.780.800.820.840.860.88TP rate (TP / targets)0.00.2\n0.4\n0.6\n0.8\n1.00.00.2\n0.4\n0.6\n0.8\n1.00.0 0.2\n0.4\n0.6\n0.8\n1.00.00.2\n0.4\n0.6\n0.8\n1.00.00.2\n0.4\n0.6\n0.8\n1.00.00.2\n0.4\n0.6\n0.8\n1.0\nthreshold=10.0\nthreshold=20.0\nthreshold=30.0\nthreshold=40.0\nthreshold=50.0\nthreshold=60.0Figure 2. Performances of CDSF-1 onset detector under\ndifferentw(labelled in each curve) and threshold (anno-\ntated in the side box) settings\neven at a higher threshold, the onset location would not be\ntraced back further than the valley preceding the peak de-\ntected in our algorithm. An interesting direction for future\nwork would thus be, given this observation, to take into\naccount the properties of human perception.\nAdaptive whitening had to be turned off for the majority\nof detectors to provide good performance for all datasets.\nThis indicates that the method does not improve onset de-\ntection performance in general, although it is available in\nmost onset detectors in the Vamp plugin library. The value\nof the tolerance window was always 0.05s for best per-\nformance in our study, suggesting that the temporal pre-\ncision of the different detectors varies signiﬁcantly, which\nrequires a fairly wide decision horizon for successful com-\nbination.\nFigure 2 shows how two parameters inﬂuence the per-\nformance of the onset detector CDSF-1. The ﬁgure illus-\ntrates the true positive rate (i.e., correct detections rela-\ntive to the number of target onsets) and false positive rate\n(i.e., false detections relative to the number of detected on-\nsets) and better performance is indicated by the curve shift-\ning upwards and leftwards. All parameters except the lin-\near combination coefﬁcient (w)anddetection sensitivity\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n635(threshold) are ﬁxed at their optimal values. We can ob-\nserve that the value of the linear combination coefﬁcient\nis around 0.2 for best performance. This suggests that the\ndetector works the best when taking the majority of the\ncontribution from SF. With the threshold increasing from\n10.0% to 60.0%, the true positive rate is increasing at the\ncost of picking more false onsets, thus a lower sensitiv-\nity is preferred in this case. Poorest performance in case\nof the linear fusion policy occurs in general when the lin-\near combination coefﬁcient overly favours one constituent\ndetector, or the sensitivity (threshold) is too high and the\nbacktracking threshold (\u0012 ) is at its lowest value.\n7. CONCLUSION AND FUTURE WORK\nIn this work, we applied several fusion techniques to aid\nthe music onset detection task. Different fusion policies\nwere tested and compared to their constituent methods,\nincluding the state-of-the-art SuperFlux method. A large\nscale evaluation was performed on two published datasets\nshowing improvements as a result of fusion, without extra\ncomputational cost, or the need for a large amount of train-\ning data as in the case of machine learning based methods.\nA parameter search was used to ﬁnd the optimal settings\nfor each detector to yield the best performance.\nWe found that some of the best performing conﬁgura-\ntions do not match the default settings of some previously\npublished algorithms. This suggests that in some cases,\nbetter performance can be achieved just by ﬁnding better\nsettings which work best overall for a given type of audio\neven without changing the algorithms.\nIn future work, a possible improvement in case of late\ndecision fusion is to take the magnitude of the peaks into\naccount when combining detected onsets, essentially treat-\ning the value as an estimation conﬁdence. We will investi-\ngate the dependency of the selection of onset detectors on\nthe type and the quality of the input music signal. We also\nintend to carry out more rigorous statistical analyses with\nsigniﬁcance tests for the reported results. More parameters\ncould be included in the search to study their strengths as\nwell as how they inﬂuence each other under different con-\nﬁgurations. Another interesting direction is to incorporate\nmore Non-Western music types as detection target and de-\nsign algorithms using instrument speciﬁc priors.\n8. REFERENCES\n[1] J.P. Bello, L. Daudet, S. Abdallan, C. Duxbury, and\nM. Davies. A tutorial on onset detection in music sig-\nnals. In IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, volume 13, 2005.\n[2] S. B ¨ock, F. Krebs, and M. Schedl. Evaluating the on-\nline capabilities of onset detection methods. In Proc.\nof the 13th Int. Soc. for Music Information Retrieval\nConf. (ISMIR), 2012.\n[3] S. B ¨ock and G. Widmer. Local group delay based vi-\nbrato and tremolo suppression for onset detection. InProc. of the 14th Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR), 2013.\n[4] S. B ¨ock and G. Widmer. Maximum ﬁlter vibrato sup-\npression for onset detection. In Proc. of the 16th Int.\nConf. on Digital Audio Effects (DAFx), 2013.\n[5] C. Cannam, M.O. Jewell, C. Rhodes, M. Sandler, and\nM. d’Inverno. Linked data and you: Bringing music re-\nsearch software into the semantic web. Journal of New\nMusic Research, 2010.\n[6] N. Collins. A comparison of sound onset detection al-\ngorithms with emphasis on psychoacoustically moti-\nvated detection functions. In Proc. of the 118th Con-\nvention of the Audio Engineering Society, 2005.\n[7] N. Collins. Using a pitch detector for onset detection.\nInProc. of the 6th Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR), 2005.\n[8] N. Degara-Quintela, A. Pena, and S. Torres-Guijarro.\nA comparison of score-level fusion rules for onset de-\ntection in music signals. In Proc. of the 10th Int. Soc.\nfor Music Information Retrieval Conf. (ISMIR), 2009.\n[9] S. Dixon. Onset detection revisited. In Proc. of the 9\nth Int. Conference on Digital Audio Effects (DAFx’06),\n2006.\n[10] A. Holzapfel, Y . Stylianou, A.C. Gedik, and\nB. Bozkurt. Three dimensions of pitched instrument\nonset detection. IEEE Transactions on Audio, Speech,\nand Language Processing, 2010.\n[11] L.A. Klein. Sensor and data fusion: a tool for informa-\ntion assessment and decision making. SPIE, 2004.\n[12] J. Makhoul. Linear prediction: A tutorial review. Pro-\nceedings of the IEEE, 63(4), 1975.\n[13] J. Schl ¨uter and S. B ¨ock. Musical onset detection with\nconvolutional neural networks. In 6th Int. Workshop on\nMachine Learning and Music (MML), 2013.\n[14] D. Stowell and M. Plumbley. Adaptive whitening for\nimproved real-time audio onset detection. In Proceed-\nings of the International Computer Music Conference\n(ICMC), 2007.\n[15] M. Tian, A. Srinivasamurthy, M. Sandler, and X. Serra.\nA study of instrument-wise onset detection in beijing\nopera percussion ensembles. In IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP), 2014.\n[16] J. V os and R. Rasch. The perceptual onset of musical\ntones. Perception & Psychophysics, 29(4), 1981.\n[17] R. Zhou, M. Mattavellii, and G. Zoia. Music onset de-\ntection based on resonator time frequency image. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n636"
    },
    {
        "title": "Boundary Detection in Music Structure Analysis using Convolutional Neural Networks.",
        "author": [
            "Karen Ullrich",
            "Jan Schlüter",
            "Thomas Grill"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1415886",
        "url": "https://doi.org/10.5281/zenodo.1415886",
        "ee": "https://zenodo.org/records/1415886/files/UllrichSG14.pdf",
        "abstract": "The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectro- grams. On a representative subset of the SALAMI struc- tural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at dif- ferent temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from",
        "zenodo_id": 1415886,
        "dblp_key": "conf/ismir/UllrichSG14",
        "keywords": [
            "Convolutional Neural Networks",
            "mel-scaled magnitude spectrograms",
            "boundary retrieval F-measure",
            "SALAMI structural annotation dataset",
            "audio signals",
            "music structure analysis",
            "temporal tolerances",
            "state-of-the-art",
            "±0.5 seconds",
            "±0.5 seconds"
        ],
        "content": "BOUNDARY DETECTION IN MUSIC STRUCTURE ANALYSIS\nUSING CONVOLUTIONAL NEURAL NETWORKS\nKaren Ullrich Jan Schlüter Thomas Grill\nAustrian Research Institute for Artiﬁcial Intelligence, Vienna\n{karen.ullrich,jan.schlueter,thomas.grill}@ofai.at\nABSTRACT\nThe recognition of boundaries, e.g., between chorus and\nverse, is an important task in music structure analysis. The\ngoal is to automatically detect such boundaries in audio\nsignals so that the results are close to human annotation.\nIn this work, we apply Convolutional Neural Networks to\nthe task, trained directly on mel-scaled magnitude spectro-\ngrams. On a representative subset of the SALAMI struc-\ntural annotation dataset, our method outperforms current\ntechniques in terms of boundary retrieval F-measure at dif-\nferent temporal tolerances: We advance the state-of-the-art\nfrom0:33 to0:46 for tolerances of\u00060:5 seconds, and from\n0:52 to0:62 for tolerances of\u00063seconds. As the algo-\nrithm is trained on annotated audio data without the need\nof expert knowledge, we expect it to be easily adaptable\nto changed annotation guidelines and also to related tasks\nsuch as the detection of song transitions.\n1. INTRODUCTION\nThe determination of the overall structure of a piece of au-\ndio, often referred to as musical form, is one of the key\ntasks in music analysis. Knowledge of the musical struc-\nture enables a variety of real-world applications, be they\ncommercially applicable, such as for browsing music, or\neducational. A large number of different techniques for au-\ntomatic structure discovery have been developed, see [16]\nfor an overview. Our contribution describes a novel ap-\nproach to retrieve the boundaries between the main struc-\ntural parts of a piece of music. Depending on the music\nunder examination, the task of ﬁnding such musical bound-\naries can be relatively simple or difﬁcult, in the latter case\nleaving ample space for ambiguity. In fact, two human an-\nnotators hardly ever annotate boundaries at the exact same\npositions. Instead of trying to design an algorithm that\nworks well in all circumstances, we let a Convolutional\nNeural Network (CNN) learn to detect boundaries from a\nlarge corpus of human-annotated examples.\nThe structure of the paper is as follows: After giving an\noverview over related work in Section 2, we describe our\nc\rKaren Ullrich, Jan Schlüter, and Thomas Grill.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Karen Ullrich, Jan Schlüter, and\nThomas Grill. “Boundary Detection in Music Structure Analysis using\nConvolutional Neural Networks”, 15th International Society for Music\nInformation Retrieval Conference, 2014.proposed method in Section 3. In Section 4, we introduce\nthe data set used for training and testing. After presenting\nour main results in Section 5, we wrap up in Section 6 with\na discussion and outlook.\n2. RELATED WORK\nIn the overview paper to audio structure analysis by Paulus\net al. [16], three fundamental approaches to segmentation\nare distinguished: Novelty-based, detecting transitions be-\ntween contrasting parts, homogeneity-based, identifying\nsections that are consistent with respect to their musical\nproperties, and repetition-based, building on the determi-\nnation of recurring patterns. Many segmentation algorithms\nfollow mixed strategies. Novelty is typically computed us-\ning Self-Similarity Matrices (SSMs) or Self-Distance Ma-\ntrices (SDMs) with a sliding checkerboard kernel [4], build-\ning on audio descriptors like timbre (MFCC features), pitch,\nchroma vectors and rhythmic features [14]. Alternative\napproaches calculate difference features on more complex\naudio feature sets [21]. In order to achieve a higher tempo-\nral accuracy in rhythmic music, audio features can be ac-\ncumulated beat-synchronously. Techniques capitalizing on\nhomogeneity use clustering [5] or state-modelling (HMM)\napproaches [1], or both [9, 11]. Repeating pattern discov-\nery is performed on SSMs or SDMs [12], and often com-\nbined with other approaches [13, 15]. Some algorithms\ncombine all three basic approaches [18].\nAlmost all existing algorithms are hand-designed from\nend to end. To the best of our knowledge, only two meth-\nods are partly learning from human annotations: Turn-\nbull et al. [21] compute temporal differences at three time\nscales over a set of standard audio features including chro-\nmagrams, MFCCs, and ﬂuctuation patterns. Training Boost-\ned Decision Stumps to classify the resulting vectors into\nboundaries and non-boundaries, they achieved signiﬁcant\ngains over a hand-crafted boundary detector using the same\nfeatures, evaluated on a set of 100 pop songs. McFee et al.\n[13] employ Ordinal Linear Discriminant Analysis to learn\na linear transform of beat-aligned audio features (including\nMFCCs and chroma) that minimizes the variance within a\nhuman-annotated segment while maximizing the distance\nacross segments. Combined with a repetition feature, their\nmethod deﬁnes the current state of the art in boundary re-\ntrieval, but still involves signiﬁcant manual engineering.\nFor other tasks in the ﬁeld of Music Information Re-\ntrieval, supervised learning with CNNs has already proven\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n417to outperform hand-designed algorithms, sometimes by a\nlarge margin [3, 6, 8, 10, 17]. In this work, we investigate\nwhether CNNs are effective for structural boundary detec-\ntion as well.\n3. METHOD\nWe propose to train a neural network on human annota-\ntions to predict likely musical boundary locations in audio\ndata. Our method is derived from Schlüter and Böck [17],\nwho use CNNs for onset detection: We also train a CNN\nas a binary classiﬁer on spectrogram excerpts, but we adapt\ntheir method to include a larger input context and respect\nthe higher inaccuracy and scarcity of segment boundary\nannotations compared to onset annotations. In the follow-\ning, we will describe the features, neural network, super-\nvised training procedure and the post-processing of the net-\nwork output to obtain boundary predictions.\n3.1 Feature Extraction\nFor each audio ﬁle, we compute a magnitude spectrogram\nwith a window size of 46 ms (2048 samples at 44.1 kHz)\nand 50% overlap, apply a mel ﬁlterbank of 80 triangular\nﬁlters from 80 Hz to 16 kHz and scale magnitudes loga-\nrithmically. To be able to train and predict on spectrogram\nexcerpts near the beginning and end of a ﬁle, we pad the\nspectrogram with pink noise at -70 dB as needed (padding\nwith silence is impossible with logarithmic magnitudes,\nand white noise is too different from the existing back-\nground noise in natural recordings). To bring the input val-\nues to a range suitable for neural networks, we follow [17]\nin normalizing each frequency band to zero mean and unit\nvariance. Finally, to allow the CNN to process larger tem-\nporal contexts while keeping the input size reasonable, we\nsubsample the spectrogram by taking the maximum over 3,\n6 or 12 adjacent time frames (without overlap), resulting in\na frame rate of 14.35 fps, 7.18 fps or 3.59 fps, respectively.\nWe will refer to these frame rates as high, std andlow.\nWe also tried training on MFCCs and chroma vectors\n(descriptors with less continuity in the ‘vertical’ feature\ndimension to be exploited by convolution), as well as ﬂuc-\ntuation patterns and self-similarity matrices derived from\nthose. Overall, mel spectrograms proved the most suitable\nfor the algorithm and performed best.\n3.2 Convolutional Neural Networks\nCNNs are feed-forward neural networks usually consist-\ning of three types of layers: Convolutional layers, pooling\nlayers and fully-connected layers. A convolutional layer\ncomputes a convolution of its two-dimensional input with\na ﬁxed-size kernel, followed by an element-wise nonlin-\nearity. The input may consist of multiple same-sized chan-\nnels, in which case it convolves each with a separate ker-\nnel and adds up the results. Likewise, the output may\nconsist of multiple channels computed with distinct sets\nof kernels. Typically the kernels are small compared to\nthe input, allowing CNNs to process large inputs with few\n30\n 20\n 10\n 0 10 20 300.00.20.40.60.81.0binary targets\n30\n 20\n 10\n 0 10 20 30\ntime frames0.00.20.40.60.81.0weightsFigure 1. The arrow at the top signiﬁes an annotated seg-\nment boundary present within a window of feature frames.\nAs seen in the upper panel, the target labels are set to one\nin the environment of this boundary, and to zero elsewhere.\nThe lower panel shows how positive targets far from the\nannotation are given a lower weight in training.\nlearnable parameters. A pooling layer subsamples its two-\ndimensional input, possibly by different factors in the two\ndimensions, handling each input channel separately. Here,\nwe only consider max-pooling, which introduces some trans-\nlation invariance across the subsampled dimension. Fi-\nnally, a fully-connected layer discards any spatial layout of\nits input by reshaping it into a vector, computes a dot prod-\nuct with a weight matrix and applies an element-wise non-\nlinearity to the result. Thus, unlike the other layer types,\nit is not restricted to local operations and can serve as the\nﬁnal stage integrating all information to form a decision.\nIn this work, we ﬁx the network architecture to a con-\nvolutional layer of 16 8\u00026kernels (8 time frames, 6 mel\nbands, 16 output channels), a max-pooling layer of 3\u00026,\nanother convolution of 32 6\u00023kernels, a fully-connected\nlayer of 128units and a fully-connected output layer of 1\nunit. This architecture was determined in preliminary ex-\nperiments and not further optimized for time constraints.\n3.3 Training\nThe input to the CNN is a spectrogram excerpt of Nframes,\nand its output is a single value giving the probability of\na boundary in the center of the input. The network is\ntrained in a supervised way on pairs of spectrogram ex-\ncerpts and binary labels. To account for the inaccuracy of\nthe ground truth boundary annotations (as observable from\nthe disagreement between two humans annotating the same\npiece), we employ what we will refer to as target smearing:\nAll excerpts centered on a frame within \u0006E frames from\nan annotated boundary will be presented to the network as\npositive examples, weighted in learning by a Gaussian ker-\nnel centered on the boundary. Figure 1 illustrates this for\nE= 10. We will vary both the spectrogram length Nand\nsmearing environment Ein our experiments. To compen-\nsate for the scarceness of positive examples, we increase\ntheir chances of being randomly selected for a training step\nby a factor of 3.\nTraining is performed using gradient descent on cross-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n418entropy error with mini-batches of 64 examples, momen-\ntum of 0.95, and an initial learning rate of 0.6 multiplied by\n0.85 after every mini-epoch of 2000 weight updates. We\napply 50% dropout to the inputs of both fully-connected\nlayers [7]. Training is always stopped after 20 mini-epochs,\nas the validation error turned out not to be robust enough\nfor early stopping. Implemented in Theano [2], training a\nsingle CNN on an Nvidia GTX 780 Ti graphics card took\n50–90 minutes.\n3.4 Peak-picking\nAt test time, we apply the trained network to each position\nin the spectrogram of the music piece to be segmented, ob-\ntaining a boundary probability for each frame. We then\nemploy a simple means of peak-picking on this boundary\nactivation curve: Every output value that is not surpassed\nwithin\u00066seconds is a boundary candidate. From each\ncandidate value we subtract the average of the activation\ncurve in the past 12 and future 6 seconds, to compensate\nfor long-term trends. We end up with a list of boundary\ncandidates along with strength values that can be thresh-\nolded at will. We found that more elaborate peak picking\nmethods did not improve results.\n4. DATASET\nWe evaluate our algorithm on a subset of the Structural\nAnalysis of Large Amounts of Music Information (SALAMI)\ndatabase [20]. In total, this dataset contains over 2400\nstructural annotations of nearly 1400 musical recordings\nof different genres and origins. About half of the annota-\ntions (779 recordings, 498 of which are doubly-annotated)\nare publicly available.1A part of the dataset was also\nused in the “Audio Structural Segmentation” task of the\nannual MIREX evaluation campaign in 2012 and 2013.2\nAlong with quantitative evaluation results, the organizers\npublished the ground truth and predictions of 17 different\nalgorithms for each recording. By matching the ground\ntruth to the public SALAMI annotations, we were able to\nidentify 487 recordings. These serve as a test set to evalu-\nate our algorithm against the 17 MIREX submissions. We\nhad another 733 recordings at our disposal, annotated fol-\nlowing the SALAMI guidelines, which we split into 633\nitems for training and 100 for validation.\n5. EXPERIMENTAL RESULTS\n5.1 Evaluation\nFor boundary retrieval, the MIREX campaign uses two\nevaluation measures: Median deviation andHit rate. The\nformer measures the median distance between each anno-\ntated boundary and its closest predicted boundary or vice\nversa. The latter checks which predicted boundaries fall\nclose enough to an unmatched annotated boundary (true\n1http://ddmal.music.mcgill.ca/datasets/salami/\nSALAMI_data_v1.2.zip, accessed 2014-05-02\n2Music Information Retrieval Evaluation eXchange, http://www.\nmusic-ir.org/mirex, accessed 2014-04-29\nFigure 2. Optimization of the threshold shown for model\n8s_std_3s at tolerance\u00060:5 seconds. Boundary re-\ntrieval precision, recall and F-measure are averaged over\nthe 100 validation set ﬁles.\npositives), records remaining unmatched predictions and\nannotations as false positives and negatives, respectively,\nthen computes the precision, recall and F-measure. Since\nnot only the temporal distance of predictions, but also the\nﬁgures of precision and recall are of interest, we opted for\nthe Hit rate at as our central measure of evaluation, com-\nputed at a temporal tolerance of \u00060:5 seconds (as in [21])\nand\u00063seconds (as in [9]). For accumulation over mul-\ntiple recordings, we follow the MIREX evaluation by cal-\nculating F-measure, precision and recall per item and av-\neraging the three measures over the items for the ﬁnal re-\nsult. Note that the averaged F-measure is not necessarily\nthe harmonic mean of the averaged precision and recall.\nOur evaluation code is publicly available for download.3\n5.2 Baseline and upper bound\nOur focus for evaluation lies primarily on the F-measure.\nTheoretically, the F-measure is bounded by F2[0;1], but\nfor the given task, we can derive more useful lower and up-\nper bounds to compare our results to. As a baseline, we use\nregularly spaced boundary predictions starting at time 0.\nChoosing an optimal spacing, we obtain an F-measure of\nFinf;3\u00190:33 for\u00063seconds tolerance, and Finf;0:5\u00190:13\nfor a tolerance of\u00060:5 seconds. Note that it is crucial to\nplace the ﬁrst boundary at 0seconds, where a large frac-\ntion of the music pieces has annotated segment bound-\naries. Many pieces have only few boundaries at all, thus\nthe impact can be considerable. An upper bound Fsupcan\nbe derived from the insight that no annotation will be per-\nfect given the fuzzy nature of the segmentation task. Even\nthough closely following annotation guidelines,4two an-\nnotators might easily disagree on the existence or exact po-\n3http://ofai.at/research/impml/projects/\naudiostreams/ismir2014/\n4cf. the SALAMI Annotator’s Guide: http://www.music.\nmcgill.ca/~jordan/salami/SALAMI-Annotator-Guide.\npdf, accessed 2014-04-30\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n419Figure 3. Comparison of different model parameters (con-\ntext length, resolution and target smearing) with respect\nto mean F-measure on our validation set at \u00060:5 seconds\ntolerance. Mean and minimum-maximum range of ﬁve in-\ndividually trained models for each parameter combination\nare shown, as well as results for bagging the ﬁve models.\nsitions of segment boundaries. By analyzing the items in\nthe public SALAMI dataset that have been annotated twice\n(498 pieces in total), we calculated Fsup;3\u00190:76 for\u00063\nseconds tolerance, and Fsup;0:5\u00190:67 for\u00060:5 seconds\ntolerance. Within our evaluation data subset (439 double-\nannotations), the results are only marginally different with\nFsup;0:5\u00190:68.\n5.3 Threshold optimization\nPeak-picking, described in Section 3.4, delivers the posi-\ntions of potential boundaries along with their probabilities,\nas calculated by the CNN. The application of a threshold\nto those probabilities rejects part of the boundaries, affect-\ning the precision and recall rates and consequently the F-\nmeasure we use for evaluation. Figure 2 shows precision\nand recall rates as well as the F-measure as a function of\nthe threshold for the example of the 8s_std_3s model (8\nseconds of context, standard resolution, target smearing 3\nseconds) at\u00060:5 seconds tolerance, applied to the 100 ﬁles\nof the validation data set. By locating the maximum of the\nF-measure we retrieve an estimate for the optimum thresh-\nold which is speciﬁc for each individual learned model.\nSince the curve for the F-measure is typically ﬂat-topped\nfor a relatively wide range of threshold values, the choice\nof the actual value is not very delicate.\n5.4 Temporal context investigation\nIt is intuitive to assume that the CNN needs a certain amount\nof temporal context to reliably judge the presence of a bound-\nary. Furthermore, the temporal resolution of the input spec-\ntra (Section 3.1) and the applied target smearing (Section 3.3)\nis expected to have an impact on the temporal accuracy of\nthe predictions. See Figure 3 and Figure 4 for comparisons\nof these model parameters, for tolerances \u00060:5 seconds\nFigure 4. Comparison of different model parameters (con-\ntext length, resolution and target smearing) with respect to\nmean F-measure on our validation set at \u00063seconds tol-\nerance. Mean and minimum-maximum range of ﬁve in-\ndividually trained models for each parameter combination\nare shown, as well as results for bagging the ﬁve models.\nand\u00063seconds, respectively. Each bar in the plots rep-\nresents the mean and minimum-maximum range of ﬁve in-\ndividual experiments with different random initializations.\nFor the case of only \u00060:5 seconds of acceptable error, we\nconclude that target smearing must also be small: A smear-\ning width of 1 to 1.5 seconds performs best. Low temporal\nspectral resolution tends to diminish results, and the con-\ntext length should not be shorter than 8 seconds. For \u00063\nseconds tolerance, context length and target smearing are\nthe most inﬂuential parameters, with the F-measure peak-\ning at 32 seconds context and 4 to 6 seconds smearing.\nLow temporal resolution is sufﬁcient, keeping the CNN\nsmaller and easier to train.\n5.5 Model bagging\nAs described in Section 5.4, for each set of parameters\nwe trained ﬁve individual models. This allows us to im-\nprove the performance on the given data using a statisti-\ncal approach: Bagging, in our case averaging the outputs\nof multiple identical networks trained from different ini-\ntializations before the peak-picking stage, should help to\nreduce model uncertainty. After again applying the above\ndescribed threshold optimization process on the resulting\nboundaries, we arrived at improvements of the F-measure\nof up to 0.03, indicated by arrow tips in Figures 3 and\n4. Tables 1 and 2 show our ﬁnal best results after model\nbagging for tolerances \u00060:5 seconds and\u00063seconds, re-\nspectively. The results are set in comparison with the al-\ngorithms submitted to the MIREX campaign in 2012 and\n2013, and the lower and upper bounds calculated from the\nannotation ground-truth (see Section 5.2).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n420Algorithm F-measure Precision Recall\nUpper bound (est.) 0.68\n16s_std_1.5s 0.4646 0.5553 0.4583\nMP2 (2013) 0.3280 0.3001 0.4108\nMP1 (2013) 0.3149 0.3043 0.3605\nOYZS1 (2012) 0.2899 0.4561 0.2583\n32s_low_6s 0.2884 0.3592 0.2680\nKSP2 (2012) 0.2866 0.2262 0.4622\nSP1 (2012) 0.2788 0.2202 0.4497\nKSP3 (2012) 0.2788 0.2202 0.4497\nKSP1 (2012) 0.2788 0.2201 0.4495\nRBH3 (2013) 0.2683 0.2493 0.3360\nRBH1 (2013) 0.2567 0.2043 0.3936\nRBH2 (2013) 0.2567 0.2043 0.3936\nRBH4 (2013) 0.2567 0.2043 0.3936\nCF5 (2013) 0.2128 0.1677 0.3376\nCF6 (2013) 0.2101 0.2396 0.2239\nSMGA1 (2012) 0.1968 0.1573 0.2943\nMHRAF1 (2012) 0.1910 0.1941 0.2081\nSMGA2 (2012) 0.1770 0.1425 0.2618\nSBV1 (2012) 0.1546 0.1308 0.2129\nBaseline (est.) 0.13\nTable 1. Boundary recognition results on our test set at\n\u00060:5 seconds tolerance. Our best result is emphasized and\ncompared with results from the MIREX campaign in 2012\nand 2013.\nAlgorithm F-measure Precision Recall\nUpper bound (est.) 0.76\n32s_low_6s 0.6164 0.5944 0.7059\n16s_std_1.5s 0.5726 0.5648 0.6675\nMP2 (2013) 0.5213 0.4793 0.6443\nMP1 (2013) 0.5188 0.5040 0.5849\nCF5 (2013) 0.5052 0.3990 0.7862\nSMGA1 (2012) 0.4985 0.4021 0.7258\nRBH1 (2013) 0.4920 0.3922 0.7482\nRBH2 (2013) 0.4920 0.3922 0.7482\nRBH4 (2013) 0.4920 0.3922 0.7482\nSP1 (2012) 0.4891 0.3854 0.7842\nKSP3 (2012) 0.4891 0.3854 0.7842\nKSP1 (2012) 0.4888 0.3850 0.7838\nKSP2 (2012) 0.4885 0.3846 0.7843\nSMGA2 (2012) 0.4815 0.3910 0.6965\nRBH3 (2013) 0.4804 0.4407 0.6076\nCF6 (2013) 0.4759 0.5305 0.5102\nOYZS1 (2012) 0.4401 0.6354 0.4038\nSBV1 (2012) 0.4352 0.3694 0.5929\nMHRAF1 (2012) 0.4192 0.4342 0.4447\nBaseline (est.) 0.33\nTable 2. Boundary recognition results on our test set at\n\u00063seconds tolerance. Our best result is emphasized and\ncompared with results from the MIREX campaign in 2012\nand 2013.6. DISCUSSION AND OUTLOOK\nEmploying Convolutional Neural Networks trained directly\non mel-scaled spectrograms, we are able to achieve bound-\nary recognition F-measures strongly outperforming any al-\ngorithm submitted to MIREX 2012 and 2013. The net-\nworks have been trained on human-annotated data, consid-\nering different context lengths, temporal target smearing\nand spectrogram resolutions. As we did not need any do-\nmain knowledge for training, we expect our method to be\neasily adaptable to different ‘foci of annotation’ such as,\ne.g., determined by different musical genres or annotation\nguidelines. In fact, our method is itself an adaption of a\nmethod for onset detection [17] to a different time focus.\nThere are a couple of conceivable strategies to improve\nthe results further: With respect to the three fundamen-\ntal approaches to segmentation described in Section 1, the\nCNNs in this work can only account for novelty and ho-\nmogeneity, which can be seen as two sides of the same\nmedal. To allow them to leverage repetition cues as well,\nthe vectorial repetition features of McFee et al. [13] might\nserve as an additional input. Alternatively, the network\ncould be extended with recurrent connections to yield a\nRecurrent CNN. Given suitable training data, the resulting\nmemory might be able to account for repeating patterns.\nSecondly, segmentation of musical data by humans is not a\ntrivially sequential process but inherently hierarchical. The\nSALAMI database actually provides annotations on two\nlevels: A coarse one, as used in the MIREX campaign, but\nalso a more ﬁne-grained variant, encoding subtler details of\nthe temporal structure. It could be helpful to feed both lev-\nels to the CNN training, weighted with respect to the sig-\nniﬁcance. Thirdly, we leave much of the data preprocess-\ning to the CNN, very likely using up a considerable part of\nits capacity. For example, the audio ﬁles in the SALAMI\ncollection are of very different loudness, which could be\nﬁxed in a simple preprocessing step, either on the whole\nﬁles, or using some dynamic gain control. Similarly, many\nof the SALAMI audio ﬁles start or end with noise or back-\nground sounds. A human annotator easily recognizes this\nas not belonging to the actual musical content, ignoring it\nin the annotations. The abrupt change from song-speciﬁc\nbackground noise to our pink noise padding may be mis-\ntaken for a boundary by the CNN, though. Therefore it\ncould be worthwhile to apply some intelligent padding of\nappropriate noise or background to provide context at the\nbeginnings and endings of the audio. And ﬁnally, we have\nonly explored a fraction of the hyperparameter space re-\ngarding network architecture and learning, and expect fur-\nther improvements by a systematic optimization of these.\nAnother promising direction of research is to explore\nthe internal processing of the trained networks, e.g., by vi-\nsualization of connection weights and receptive ﬁelds [19].\nThis may help to understand the segmentation process as\nwell as differences to existing approaches, and to reﬁne the\nnetwork architecture.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4217. ACKNOWLEDGMENTS\nThis research is funded by the Federal Ministry for Trans-\nport, Innovation & Technology (BMVIT) and the Austrian\nScience Fund (FWF): TRP 307-N23. Many thanks to the\nanonymous reviewers for your valuable feedback!\n8. REFERENCES\n[1] J.-J. Aucouturier and M. Sandler. Segmentation of mu-\nsical signals using hidden markov models. In Proc.\nAES 110th Convention, May 2001.\n[2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin,\nR. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,\nand Y . Bengio. Theano: a CPU and GPU math ex-\npression compiler. In Proc. of the Python for Scientiﬁc\nComputing Conference (SciPy), June 2010.\n[3] S. Dieleman, P. Braken, and B. Schrauwen. Audio-\nbased music classiﬁcation with a pretrained convolu-\ntional network. In Proc. of the 12th Int. Soc. for Mu-\nsic Information Retrieval Conf. (ISMIR), Miami, FL,\nUSA, October 2011.\n[4] J. Foote. Automatic audio segmentation using a\nmeasure of audio novelty. In Proceedings of IEEE\nInternational Conference on Multimedia and Expo\n(ICME’00), volume 1, pages 452–455 vol.1, 2000.\n[5] J. T. Foote and M. L. Cooper. Media segmentation us-\ning self-similarity decomposition. In Proc. of The SPIE\nStorage and Retrieval for Multimedia Databases, vol-\nume 5021, pages 167–175, San Jose, California, USA,\nJanuary 2003.\n[6] P. Hamel, S. Lemieux, Y . Bengio, and D. Eck. Tempo-\nral pooling and multiscale learning for automatic an-\nnotation and ranking of music audio. In Proc. of the\n12th Int. Soc. for Music Information Retrieval Conf.\n(ISMIR), October 2011.\n[7] G. E. Hinton, N. Srivastava, A. Krizhevsky,\nI. Sutskever, and R.R. Salakhutdinov. Improving\nneural networks by preventing co-adaptation of feature\ndetectors. arXiv:1207.0580, 2012.\n[8] E. J. Humphrey and J. P. Bello. Rethinking automatic\nchord recognition with convolutional neural networks.\nInProc. of the 11th Int. Conf. on Machine Learning\nand Applications (ICMLA), volume 2, Boca Raton, FL,\nUSA, December 2012. IEEE.\n[9] M. Levy and M. Sandler. Structural segmentation\nof musical audio by constrained clustering. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 16(2):318–326, Feb 2008.\n[10] T. LH. Li, A. B. Chan, and A. HW. Chun. Automatic\nmusical pattern feature extraction using convolutional\nneural network. In Proc. of the Int. MultiConf. of Engi-\nneers and Computer Scientists (IMECS), Hong Kong,\nMarch 2010.[11] Beth Logan and Stephen Chu. Music summarization\nusing key phrases. In In Proc. IEEE ICASSP, pages\n749–752, 2000.\n[12] Lie Lu, Muyuan Wang, and Hong-Jiang Zhang. Re-\npeating pattern discovery and structure analysis from\nacoustic music data. In MIR ’04: Proceedings of the\n6th ACM SIGMM international workshop on Multime-\ndia information retrieval, pages 275–282, New York,\nNY , USA, 2004. ACM.\n[13] B. McFee and D. P. W. Ellis. Learning to segment\nsongs with ordinal linear discriminant analysis. In In-\nternational conference on acoustics, speech and signal\nprocessing, ICASSP, 2014.\n[14] Jouni Paulus and Anssi Klapuri. Acoustic features for\nmusic piece structure analysis. In Conference: 11th In-\nternational Conference on Digital Audio Effects (Es-\npoo, Finland), 2008.\n[15] Jouni Paulus and Anssi Klapuri. Music structure analy-\nsis by ﬁnding repeated parts. In Proceedings of the 1st\nACM Workshop on Audio and Music Computing Mul-\ntimedia, AMCMM ’06, pages 59–68, New York, NY ,\nUSA, 2006. ACM.\n[16] Jouni Paulus and Anssi Klapuri. Music structure anal-\nysis using a probabilistic ﬁtness measure and a greedy\nsearch algorithm. Trans. Audio, Speech and Lang.\nProc., 17:12, 2009.\n[17] J. Schlüter and S. Böck. Improved musical onset detec-\ntion with convolutional neural networks. Proceedings\nof the 2014 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 2014.\n[18] Joan Serra, Meinard Müller, Peter Grosche, and\nJosep Ll. Arcos. Unsupervised detection of music\nboundaries by time series structure features. In Pro-\nceedings of the Twenty-Sixth AAAI Conference on Ar-\ntiﬁcial Intelligence, pages 1613–1619. Association for\nthe Advancement of Artiﬁcial Intelligence, 2012.\n[19] Karen Simonyan and Andrea Vedaldi and Andrew Zis-\nserman. Deep Inside Convolutional Networks: Visual-\nising Image Classiﬁcation Models and Saliency Maps.\nInCoRR, abs/1312.6034, 2013.\n[20] Jordan Bennett Louis Smith, John Ashley Burgoyne,\nIchiro Fujinaga, David De Roure, and J Stephen\nDownie. Design and creation of a large-scale database\nof structural annotations. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference, pages 555–560, 2011.\n[21] Douglas Turnbull and Gert Lanckriet. A supervised ap-\nproach for detecting boundaries in music using dif-\nference features and boosting. In In Proceedings of\nthe 5th International Conference on Music Information\nRetrieval (ISMIR), pages 42–49, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n422"
    },
    {
        "title": "What is the Effect of Audio Quality on the Robustness of MFCCs and Chroma Features?",
        "author": [
            "Julián Urbano",
            "Dmitry Bogdanov",
            "Perfecto Herrera",
            "Emilia Gómez",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416276",
        "url": "https://doi.org/10.5281/zenodo.1416276",
        "ee": "https://zenodo.org/records/1416276/files/UrbanoBHGS14.pdf",
        "abstract": "Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical appli- cations they are to be computed on music corpora contain- ing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may af- fect the computation of descriptors. This raises the ques- tion of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute sev- eral statistics to quantify the robustness of the resulting de- scriptors, and then estimate the practical effects for a sam- ple task like genre classification.",
        "zenodo_id": 1416276,
        "dblp_key": "conf/ismir/UrbanoBHGS14",
        "keywords": [
            "Music Information Retrieval",
            "descriptors computed from audio signals",
            "practical applications",
            "audio corpora",
            "audio signals",
            "lossy formats",
            "distort the original signal",
            "compute descriptors",
            "MFCCs",
            "chroma features"
        ],
        "content": "WHAT IS THE EFFECT OF AUDIO QUALITY ON THE\nROBUSTNESS OF MFCCs AND CHROMA FEATURES?\nJuli´an Urbano, Dmitry Bogdanov, Perfecto Herrera, Emilia G ´omez and Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra Barcelona, Spain\nfjulian.urbano,dmitry.bogdanov,perfecto.herrera,emilia.gomez,xavier.serrag@upf.edu\nABSTRACT\nMusic Information Retrieval is largely based on descriptors\ncomputed from audio signals, and in many practical appli-\ncations they are to be computed on music corpora contain-\ning audio ﬁles encoded in a variety of lossy formats. Such\nencodings distort the original signal and therefore may af-\nfect the computation of descriptors. This raises the ques-\ntion of the robustness of these descriptors across various\naudio encodings. We examine this assumption for the case\nof MFCCs and chroma features. In particular, we analyze\ntheir robustness to sampling rate, codec, bitrate, frame size\nand music genre. Using two different audio analysis tools\nover a diverse collection of music tracks, we compute sev-\neral statistics to quantify the robustness of the resulting de-\nscriptors, and then estimate the practical effects for a sam-\nple task like genre classiﬁcation.\n1. INTRODUCTION\nA signiﬁcant amount of research in Music Information Re-\ntrieval (MIR) is based on descriptors computed from au-\ndio signals. In many cases, research corpora contain mu-\nsic ﬁles encoded in a lossless format. In some situations,\ndatasets are distributed without their original music corpus,\nso researchers have to gather audio ﬁles themselves. In\nmany other cases, audio descriptors are distributed instead\nof the audio ﬁles. In the end, MIR research is thus based on\ncorpora that very well may use different audio encodings,\nall under the assumption that audio descriptors are robust\nto these variations and the ﬁnal MIR algorithms are not af-\nfected. This possible lack of robustness poses serious ques-\ntions regarding the reproducibility of MIR research and\nits applicability. For instance, whether algorithms trained\nwith lossless audio ﬁles can generalize to lossy encodings;\nor whether a minimum audio bitrate should be required in\ndatasets that distribute descriptors instead of audio ﬁles.\nIn this paper we examine the assumption of robust-\nness of music descriptors across different audio encod-\nings on the example of Mel-frequency cepstral coefﬁ-\ncients (MFCCs) and chroma features. They are among\nthe most popular music descriptors used in MIR research,\nas they respectively capture timbre and tonal information.\nc\rJ.Urbano, D.Bogdanov, P.Herrera, E.G ´omez and X.Serra.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: J. Urbano, D. Bogdanov, P. Herrera,\nE. G ´omez and X. Serra. “What is the Effect of Audio Quality on the Ro-\nbustness of MFCCs and Chroma Features?”, 15th International Society\nfor Music Information Retrieval Conference, 2014.Many MIR tasks such as classiﬁcation, similarity, autotag-\nging, recommendation, cover identiﬁcation and audio ﬁn-\ngerprinting, audio-to-score alignment, audio segmentation,\nkey and chord estimation, and instrument detection are at\nleast partially based on them. As they pervade the literature\non MIR, we analyzed the effect of audio encoding and sig-\nnal analysis parameters on the robustness of MFCCs and\nchroma. To this end, we run two different audio analysis\ntools over a diverse collection of 400 music tracks. We then\ncompute several indicators that quantify the robustness and\nstability of the resulting features and estimate the practical\nimplications for a general task like genre classiﬁcation.\n2. DESCRIPTORS\n2.1 Mel-Frequency Cepstrum Coefﬁcients\nMFCCs are inherited from the speech domain [18], and\nthey have been extensively used to summarize the spectral\ncontent of music signals within an analysis frame. MFCCs\nare widely used in tasks like music similarity [1,12], music\nclassiﬁcation [6] (in particular, genre), autotagging [13],\npreference learning for music recommendation [19, 24],\ncover identiﬁcation and audio segmentation [17].\nThere is no standard algorithm to compute MFCCs, and\na number of variants have been proposed [8] and adapted\nfor MIR applications. MFCCs are commonly computed as\nfollows. The ﬁrst step consists in windowing the input sig-\nnal and computing its magnitude spectrum with the Fourier\ntransform. We then apply a ﬁlterbank with critical (mel)\nband spacing of the ﬁlters and bandwidths. Energy val-\nues are obtained for the output of each ﬁlter, followed by\na logarithm transformation. We ﬁnally compute a discrete\ncosine transform to the set of log-energy values to obtain\nthe ﬁnal set of coefﬁcients. The number of mel bands and\nthe frequency interval on which they are computed may\nvary among implementations. The low order coefﬁcients\naccount for the slowly changing spectral envelope, while\nthe higher order coefﬁcients describe the fast variations of\nthe spectrum shape, including pitch information. The ﬁrst\ncoefﬁcient is typically discarded in MIR applications be-\ncause it does not provide information about the spectral\nshape; it reﬂects the overall energy in mel bands.\n2.2 Chroma\nChroma features represent the spectral energy distribution\nwithin an analysis frame, summarized into 12 semitones\nacross octaves in equal-tempered scale. Chroma captures\nthe pitch class distribution of an input signal, typically used\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n573for key and chord estimation [7, 9], music similarity and\ncover identiﬁcation [20], classiﬁcation [6], segmentation\nand summarization [5, 17], and synchronization [16].\nSeveral approaches exist for chroma feature extraction,\nincluding the following steps. The signal is ﬁrst analyzed\nwith a high frequency resolution in order to obtain its fre-\nquency domain representation. The main frequency com-\nponents (e.g. spectral peaks) are mapped onto pitch classes\naccording to an estimated tuning frequency. For most ap-\nproaches, a frequency value partially contributes to a set\nof “sub-harmonic” fundamental frequency (pitch) candi-\ndates. The chroma vector is computed with a given inter-\nval resolution (number of bins per octave) and is ﬁnally\npost-processed to obtain the ﬁnal chroma representation.\nTimbre invariance is achieved by different transformations\nsuch as spectral whitening [9] or cepstrum liftering [15].\n3. EXPERIMENTAL DESIGN\n3.1 Factors Affecting Robustness\nWe identiﬁed several factors that could have an effect on\nthe robustness of audio descriptors, from the perspective\nof their audio encoding (codec, bitrate and sampling rate),\nanalysis parameters (frame/hop size and audio analysis\ntool) and the musical characteristics of the songs (genre).\nSRate. The sampling rate at which an audio signal is\nencoded may affect robustness when using very high fre-\nquency rates. We study standard 44100 and 22050 Hz.\nCodec. Perceptual audio coders may also affect descrip-\ntors because they introduce perturbations to the original\naudio signal, in particular by reducing high-frequency con-\ntent, blurring the attacks, and smoothing the spectral enve-\nlope. In our experiments, we chose one lossless and two\nlossy audio codecs: WA V , MP3 CBR and MP3 VBR.\nBRate. Different audio codecs allow different bitrates\ndepending on the sampling rate, so we can not combine all\ncodecs with all bitrates. The following combinations are\npermitted and used in our study:\n\u000fWA V: 1411 Kbps.\n\u000fMP3 CBR at 22050 Hz: 64, 96, 128 and 160 Kbps.\n\u000fMP3 CBR at 44100 Hz: 64, 96, 128, 160, 192, 256\nand 320 Kbps.\n\u000fMP3 VBR: 6 (100-130 Kbps), 4 (140-185 Kbps), 2\n(170-210 Kbps) and 0 (220-260 Kbps).\nFSize. We considered a variety of frame sizes for spec-\ntral analysis: 23.2, 46.4, 92.9, 185.8, 371.5 and 743.0 ms.\nThat is, we used frame sizes of 1024, 2048, 4096, 8192,\n16384 and 32768 samples for signals with sampling rate of\n44100 Hz, and the halved values (512, 1024, 2048, 4096,\n8192 and 16384 samples) in the case of 22050 Hz.\nAudio analysis tool . The speciﬁc software used to com-\npute descriptors may have an effect on their robustness due\nto parameterizations (e.g. frequency ranges) and other im-\nplementation details. We use two state-of-the-art and open\nsource tools publicly available online: Essentia 2.0.11[2]\nandQM Vamp Plugins 1.7 for Sonic Annotator 0.72[3].\n1http://essentia.upf.edu\n2http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.htmlSince our goal here is not to compare tools, we refer to\nthem simply as Lib1 and Lib2 throughout the paper.\nLib1 and Lib2 provide by default two different implemen-\ntations of MFCCs, both of which compute cepstral coefﬁ-\ncients on 40 mel bands, resembling the MFCC FB-40 im-\nplementation [8, 22] but on different frequency intervals.\nLib1 covers a wider frequency range of 0-11000 Hz with\nmel bin centers being equally spaced on the mel scale in\nthis range, while Lib2 covers a frequency range of 66-\n6364 Hz. We compute the ﬁrst 13 MFCCs in both systems\nand discard the ﬁrst coefﬁcient. In the case of chroma,\nLib1 analyzes a frequency range of 40-5000 Hz based on\nFourier transform and estimates tuning frequency. Lib2\nuses a Constant Q Transform and analyzes the frequency\nrange 65-2093 Hz assuming tuning frequency of 440 Hz,\nbut it does not account for harmonics of the detected peaks.\nWe compute 12-dimensional chroma features.\nGenre. Robustness may depend as well on the music\ngenre of songs. For instance, as the most dramatic change\nthat perceptual coders introduce is that of ﬁltering out high-\nfrequency spectral content, genres that make use of very\nhigh-frequency sounds (e.g. cymbals and electronic tones)\nshould show a more detrimental effect than genres not in-\ncluding them (e.g. country, blues and classical).\n3.2 Data\nWe created an ad-hoc corpus of music for this study, con-\ntaining 400 different music tracks (30 seconds excerpts) by\n395 different artists, uniformly covering 10 music genres\n(blues, classical, country, disco/funk/soul, electronic, jazz,\nrap/hip-hop, reggae, rock and rock’n’roll). All 400 tracks\nare encoded from their original CD at a 44100 Hz sampling\nrate using the lossless FLAC audio codec.\nWe converted all lossless tracks in our corpus into var-\nious audio formats in accordance with the factors iden-\ntiﬁed above, taking into account all possible combina-\ntions of sampling rate, codec and bitrate. Audio conver-\nsion was done using the FFmpeg 0.8.33converter, which\nincludes the LAME codec for MP3 joint stereo mode\n(Lavf53.21.1 ). Afterwards, we analyzed the original loss-\nless ﬁles and their lossy versions using both Lib1 and Lib2.\nIn the case of Lib1, both MFCCs and chroma features were\ncomputed for all different frame sizes with the hop size\nequal to half the frame size. MFCCs were computed simi-\nlarly in the case of Lib2, but chroma features only allow a\nﬁxed frame size of 16384 samples (we selected a hop size\nof 2048 samples). In all cases, we summarize the frame-\nwise feature vectors with the mean of each coefﬁcient.\n3.3 Indicators of Robustness\nWe computed several indicators of the robustness of\nMFCCs and chroma, each measuring the difference be-\ntween the descriptors computed with the original lossless\naudio clips and the descriptors computed with their lossy\nversions. We blocked by tool, sampling rate and frame\nsize under the assumption that these factors are not mixed\nin practice within the same application. For two arbitrary\n3http://www.ffmpeg.org\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n574vectorsxandy(each containing n= 12 MFCC or chroma\nvalues) from a lossless and a lossy version, we compute\nﬁve indicators to measure how different they are.\nRelative error \u000e. It is computed as the average relative\ndifference across coefﬁcients. This indicator can be eas-\nily interpreted as the percentage error between coefﬁcients,\nand it is of especial interest for tasks in which coefﬁcients\nare used as features to train some model.\n\u000e(x;y) =1\nnPjxi\u0000y ij\nmax(jx ij;jy ij)\nEuclidean distance \". The Euclidean distance between\nthe two vectors, which is especially relevant for tasks that\ncompute distances between pairs of songs, such as in music\nsimilarity or other tasks that use techniques like clustering.\nPearson’sr. The common parametric correlation coef-\nﬁcient between the two vectors, ranging from -1 to 1.\nSpearman’s\u001a. A non-parametric correlation coefﬁ-\ncient, equal to the Pearson’s rcorrelation after transform-\ning all coefﬁcients to their corresponding ranks in x[y.\nCosine similarity \u0012. The angle between both vectors. It\nis is similar to \", but it is normalized between 0 and 1.\nWe have 400 tracks\u000219 BRate:Codec\u00026 FSize=45600\ndatapoints for MFCCs with Lib1, MFCCs with Lib2, and\nchroma with Lib1. For chroma with Lib2 there is just\noneFSize, which yields 7600 datapoints. This adds up to\n144400 datapoints for each indicators, 722000 overall.\n3.4 Analysis\nFor simplicity, we followed a hierarchical analysis for each\ncombination of sampling rate, tool, feature and robust-\nness indicator. We are ﬁrst interested in the mean of the\nscore distributions, which tells us the expected robustness\nin each case (e.g. a low \"mean score suggests that the de-\nscriptor is robust because it does not differ much between\nthe lossless and the lossy versions). But we are also inter-\nested in the stability of the descriptor, that is, the variance\nof the distribution. For instance, a descriptor might be ro-\nbust on average but not below 192 Kbps, or robust only\nwith a frame size of 2048.\nTo gain a deeper understanding of the variations in the\nindicators, we ﬁtted a random effects model to study the\neffects of codec, bitrate and frame size [14]. The spe-\nciﬁc models included the FSize andCodec main effects,\nand the bitrate was modeled as nested within the Codec\neffect (BRate:Codec); all interactions among them were\nalso ﬁtted. Finally, we included the Genre andTrack main\neffects to estimate the speciﬁc variability due to inherent\ndifferences among the music pieces themselves. We did\nnot consider any Genre orTrack interactions because they\ncan not be controlled in a real-world application, so their\neffects are all confounded with the residual effect. Note\nthough that this residual does not account for any random\nerror (in fact, there is no random error in this model); it\naccounts for high-order interactions associated with Genre\nandTrack that are irrelevant for our purposes. This re-\nsults in a Resolution V design for the factors of interest\n(main effects unconfounded with two- or three-factor in-\nteractions) and a Resolution III design for musical factorsrelated to genre (main effects confounded with two-factor\ninteractions) [14]. We ran an ANOV A analysis on these\nmodels to estimate variance components, which indicate\nthe contribution of each factor to the total variance, that is,\ntheir impact on the robustness of the audio descriptors.\n4. RESULTS\nTable 1 shows the results for MFCCs. As shown by the\nmean scores, the descriptors computed by Lib1 and Lib2\nare similarly robust (note that \"scores are not directly com-\nparable across tools because they are not normalized; ac-\ntual MFCCs in Lib1 are orders of magnitude larger than\nin Lib2). Both correlation coefﬁcients rand\u001a, as well as\ncosine similarity \u0012, are extremely high, indicating that the\nshape of the feature vectors is largely preserved. However,\nthe average error across coefﬁcients is as high \u000e\u00196:1% at\n22050 Hz and \u000e\u00196:7% at 44100 Hz.\nWhen focusing on the stability of the descriptors, we\nsee that the implementation in Lib2 is generally more sta-\nble because the distributions have less variance, except for\n\u000eand\u001aat 22050 Hz. The decomposition in variance com-\nponents indicates that the choice of frame size is irrelevant\nin general (low ^\u001b2\nFSize scores), and that the largest part of\nthe variability depends on the particular characteristics of\nthe music pieces (very high ^\u001b2\nTrack +^\u001b2\nresidual scores). For\nLib2 in particular, this means that controlling encodings\nor analysis parameters does not increase robustness signif-\nicantly when the sampling rate is 22050 Hz; it depends\nalmost exclusively on the speciﬁc music pieces. On the\nother hand, the combination of codec and bitrate has a quite\nlarge effect in Lib1. For instance, about 42% of the vari-\nability in Euclidean distances is due to the BRate:Codec\ninteraction effect. This means that an appropriate selection\nof the codec and bitrate of the audio ﬁles leads to signiﬁ-\ncantly more robust descriptors. At 44100 Hz both tools are\nclearly affected by the BRate:Codec effect as well, espe-\ncially Lib1. Figure 1 compares the distributions of \u000escores\nfor each tool. We can see that Lib1 has indeed large vari-\nance across groups, but small variance within groups, as\nopposed to Lib2. The robustness of Lib1 seems to con-\nverge to\u000e\u00193%at 256 Kbps, and the descriptors are\nclearly more stable with larger bitrates (smaller within-\ngroup variance). On the other hand, the average robustness\nof Lib2 converges to \u000e\u00195%at 160-192 Kbps, and stabil-\nCBR.64\nCBR.96\nCBR.128\nCBR.160\nCBR.192\nCBR.256\nCBR.320\nVBR.6\nVBR.4\nVBR.2\nVBR.00.000.100.200.30MFCCs Lib1 44100 Hzδ\nCBR.64\nCBR.96\nCBR.128\nCBR.160\nCBR.192\nCBR.256\nCBR.320\nVBR.6\nVBR.4\nVBR.2\nVBR.00.000.100.200.30MFCCs Lib2 44100 Hzδ\nFigure 1. Distributions of \u000escores for different combina-\ntions of MP3 codec and bitrate at 44100 Hz, and for both\naudio analysis tools. Blue crosses mark the sample means.\nOutliers are rather uniformly distributed across genres.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n57522050 Hz 44100 Hz\n\u000e \" r \u001a \u0012 \u000e \" r \u001a \u0012Lib1^\u001b2\nFSize1.08 3.03 1.73 0 1.74 0.21 0.09 0.01 0 0\n^\u001b2\nCodec0 0 0 0 0 0 0 0 0 0\n^\u001b2\nBRate :Codec31.25 42.13 21.61 8.38 21.49 46.98 41.77 22.52 24.03 21.51\n^\u001b2\nFSize\u0002Codec0 0 0 0 0 0 0.20 0.07 0.05 0.06\n^\u001b2\nFSize\u0002( BRate :Codec)4.87 11.71 12.36 1.23 13.21 7.37 18.25 17.98 10.85 18.02\n^\u001b2\nGenre0.99 4.53 3.92 0.08 3.80 1.12 0.52 0.90 0.32 0.89\n^\u001b2\nTrack19.76 5.84 6.46 11.59 5.73 10.12 3.91 2.65 5.23 2.59\n^\u001b2\nresidual42.05 32.75 53.92 78.72 54.03 34.19 35.26 55.87 59.52 56.92\nGrand mean 0.0591 1.6958 0.9999 0.9977 0.9999 0.0682 1.8820 0.9998 0.9939 0.9998\nTotal variance 0.0032 3.4641 1.8e-7 3.2e-5 1.5e-7 0.0081 11.44 1.6e-6 0.0005 1.4e-6\nStandard deviation 0.0567 1.8612 0.0004 0.0056 0.0004 0.0897 3.3835 0.0013 0.0214 0.0012Lib2^\u001b2\nFSize1.17 0.32 0.16 0.24 0.18 0.25 0 0 0 0\n^\u001b2\nCodec0 0 0 0 0 0 0 0 0 0\n^\u001b2\nBRate :Codec4.91 6.01 2.32 0.74 3.14 23.46 24.23 14.27 13.31 15.02\n^\u001b2\nFSize\u0002Codec0 0 0 0 0 0 0 0 0 0\n^\u001b2\nFSize\u0002( BRate :Codec)0.96 0.43 0.03 0.04 0.09 7.17 8.09 10.35 6.34 10.86\n^\u001b2\nGenre4.21 14.68 2.84 0.61 4.41 0.37 5.37 0.50 0 0.48\n^\u001b2\nTrack52.34 61.05 32.07 66.10 41.26 27.33 14.10 6.55 13.32 5.53\n^\u001b2\nresidual36.41 17.51 62.57 32.27 50.92 41.42 48.21 68.32 67.03 68.11\nGrand mean 0.0622 0.0278 0.9999 0.9955 0.9999 0.0656 0.0342 0.9998 0.9947 0.9999\nTotal variance 0.0040 0.0015 8.9e-8 0.0002 3.5e-8 0.0055 0.0034 6.4e-7 0.0002 4.8e-7\nStandard deviation 0.0631 0.0391 0.0003 0.0131 0.0002 0.0740 0.0587 0.0008 0.0150 0.0007\nTable 1. Variance components in the distributions of robustness of MFCCs for Lib1 (top) and Lib2 (bottom). Each\ncomponent represents the percentage of total variance due to each effect (eg. ^\u001b2\nFSize = 3:03 indicates that 3:03% of the\nvariability in the robustness indicator is due to differences across frame sizes; ^\u001b2\nx= 0when the effect is so extremely small\nthat the estimate is slightly below zero). All interactions with the Genre andTrack main effects are confounded with the\nresidual effect. The last rows show the grand mean, total variance and standard deviation of the distributions.\nity remains virtually the same beyond 96 Kbps. These plots\nconﬁrm that the MFCC implementation in Lib1 is nearly\ntwice as robust and stable when the encoding is homoge-\nneous in the corpus, while the implementation in Lib2 is\nless robust but more stable with heterogeneous encodings.\nTheFSize effect is negligible, indicating that the choice\nof frame size does not affect the robustness of MFCCs\nin general. However, in several cases we can observe\nlarge ^\u001b2\nFSize\u0002(BRate:Codec )scores, meaning that for some\ncodec-bitrate combinations it does matter. An in-depth\nanalysis shows that these differences only occur at 64 Kbps\nthough (small frame sizes are more robust); differences are\nvery small otherwise. Finally, the small ^\u001b2\nGenre scores in-\ndicate that robustness is similar across music genres.\nA similar analysis was conducted to assess the robust-\nness and stability of chroma features. Even though the\ncorrelation indicators are generally high as well, Table 2\nshows that chroma vectors do not preserve the shape as\nwell as MFCCs do. When looking at individual coefﬁ-\ncients, the relative errors are similarly \u000e\u00196%in Lib1, but\nthey are greatly reduced in Lib2, especially at 44100 Hz.\nIn fact, the chroma implementation in Lib2 is more robust\nand stable according to all indicators4. For Lib1, virtually\nall the variability in the distributions is due to the Track\nandresidual effects, meaning that chroma is similarly ro-\nbust across encodings, analysis parameters and genre. For\nLib2, we can similarly observe that errors in the correla-\ntion indicators depend almost entirely on the Track effect,\nbut\u000eand\"depend mostly on the codec-bitrate combina-\ntion. This indicates that, despite chroma vectors preserve\n4Even though these distributions include all frame sizes in Lib1 but\nonly 16384 in Lib2, the FSize effect is negligible in Lib1, meaning that\nthese indicators are still comparable across implementationstheir shape, the individual components vary signiﬁcantly\nacross encodings; we observed that increasing the bitrate\nleads to larger coefﬁcients overall. This suggests that nor-\nmalizing the chroma coefﬁcients could dramatically im-\nprove the distributions of \u000eand\". We tried the parameter\nnormalization=2 to have Lib2 normalize chroma vec-\ntors to unit maximum. As expected, the effects of codec\nand bitrate are removed after normalization, and most of\nthe variability is due to the Track effect. The correlation\nindicators are practically unaltered after normalization.\n5. ROBUSTNESS IN GENRE CLASSIFICATION\nThe previous section provided indicators of robustness that\ncan be easily understood. However, they can be hard to\ninterpret because in the end we are interested in the ro-\nbustness of the various algorithms that make use of these\nfeatures; whether \u000e= 5% is large or not depends on how\nMFCCs and chroma are used in practice. To investigate\nthis question we consider a music genre classiﬁcation task.\nFor each sampling rate, codec, bitrate and tool we trained\none SVM model with radial basis kernel using MFCCs and\nanother using chroma. For MFCCs we used a standard\nframe size of 2048, and for chroma we set 4096 in Lib1\nand the ﬁxed 16384 in Lib2. We did random sub-sampling\nvalidation with 100 random trials for each model, using\n320 tracks for training and the remaining 80 for testing.\nWe ﬁrst investigate whether a particular choice of en-\ncoding is likely to classify better when ﬁxed across train-\ning and test sets. Table 3 shows the results for a selec-\ntion of encodings at 44100 Hz. Within the same tool and\ndescriptor, differences across encodings are quite small,\napproximately 0.02. In particular, for MFCCs and Lib1\nan ANOV A analysis suggests that differences are signiﬁ-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n57622050 Hz 44100 Hz\n\u000e \" r \u001a \u0012 \u000e \" r \u001a \u0012Lib1^\u001b2\nFSize1.68 2.77 0.20 0.15 0.38 2.37 2.42 0.24 0.34 0.50\n^\u001b2\nGenre2.81 2.75 1.29 1.47 0.81 3.12 2.61 1.17 1.25 0.85\n^\u001b2\nTrack20.69 19.27 17.75 18.52 16.63 22.28 20.78 18.81 19.92 18.64\n^\u001b2\nresidual74.82 75.21 80.75 79.86 82.17 72.22 74.19 79.79 78.49 80.01\nGrand Mean 0.0610 0.0545 0.9554 0.9366 0.9920 0.0588 0.0521 0.9549 0.9375 0.9922\nTotal variance 0.0046 0.0085 0.0276 0.0293 0.0014 0.0048 0.0082 0.0286 0.0298 0.0013\nStandard deviation 0.0682 0.0924 0.1663 0.1713 0.0373 0.0695 0.0904 0.1691 0.1725 0.0355Lib2^\u001b2\nCodec63.62 34.55 0 0 0 32.32 21.59 0 0 0\n^\u001b2\nBRate :Codec0.71 0.23 0 0 0 61.80 39.51 0.01 0.03 0.04\n^\u001b2\nGenre0.25 15.87 2.90 4.05 7.95 0.62 9.98 3.43 1.33 3.66\n^\u001b2\nTrack19.29 32.77 96.71 92.75 91.80 3.27 13.79 94.24 93.04 77.00\n^\u001b2\nresidual16.14 16.58 0.38 3.20 0.25 1.98 15.13 2.32 5.60 19.30\nGrand mean 0.0346 0.0031 0.9915 0.9766 0.9998 2.6e-2 2.2e-3 0.9989 0.9928 1\nTotal variance 0.0004 5e-6 0.0002 0.0007 6.1e-8 4.6e-4 4.8e-6 3.7e-6 0.0001 1.8e-9\nStandard deviation 0.0195 0.0022 0.0135 0.0270 0.0002 0.0213 0.0022 0.0019 0.0122 4.2e-5\nTable 2. Variance components in the distributions of robustness of Chroma for Lib1 (top) and Lib2 (bottom), similar\nto Table 1. The Codec main effect and all its interactions are not shown for Lib1 because all variance components are\nestimated as 0. Note that the FSize main effect and all its interactions are omitted for Lib2 because it is ﬁxed to 16384.\n64 96 128 160 192 256 320 WA VLib1MFCCs .383 .384 .401 .403 .395 .402 .394 .393\nChroma .275 .281 .288 .261 .278 .278 .284 .291Lib2MFCCs .335 .329 .332 .341 .336 .336 .344 .335\nChroma .320 .325 .320 .323 .325 .319 .320 .313\nTable 3. Mean classiﬁcation accuracy over 100 trials when\ntraining and testing with the same encoding (MP3 CBR\nand WA V only) at 44100 Hz.\ncant,F(7;693) = 2:34;p = 0:023; a multiple comparisons\nanalysis reveals that 64 Kbps is signiﬁcantly worse than\nthe best (160 Kbps). In terms of chroma, differences are\nagain statistically signiﬁcant, F(7;693)=3:71;p< 0:001;\n160 Kbps is this time signiﬁcantly worse that most of\nthe others. With Lib2 differences are not signiﬁcant for\nMFCCs,F(7;693) = 1:07;p = 0:378. No difference is\nfound for chroma either, F(7;693) = 0:67;p = 0:702.\nOverall, despite some pairwise comparisons are signiﬁ-\ncantly different, there is no particular encoding that clearly\noutperforms the others; the observed differences are prob-\nably just Type I errors. There is no clear correlation either\nbetween bitrate and accuracy.\nWe then investigate whether a particular choice of en-\ncoding for training is likely to produce better results when\nthe target test set has a ﬁxed encoding. For MFCCs\nand Lib1 there is no signiﬁcant difference in any but\none case (testing with 160 Kbps is worst when training\nwith 64 Kbps). For chroma there are a few cases where\n160 Kbps is again signiﬁcantly worse than others, but we\nattribute these to Type I errors as well. Although not sig-\nniﬁcantly so, the best result is always obtained when the\ntraining set has the same encoding as the target test set.\nWith Lib2 there is no signiﬁcant difference for MFCCs or\nchroma. Overall, we do not observe a correlation either be-\ntween training and test encodings. Due to space constrains,\nwe do not discuss results for VBR or 22050 Hz, but the\nsame general conclusions can be drawn nonetheless.\n6. DISCUSSION\nSigurdsson et al. [21] suggested that MFCCs are sensi-\ntive to the spectral perturbations that result from low bi-trate compression, mostly due to distortions at high fre-\nquencies. They estimated squared Pearson’s correlation\nbetween MFCCs computed on original lossless audio and\nits MP3 derivatives, using 4 different MFCC implemen-\ntations. All implementations were found to be robust at\nbitrates of at least 128 Kbps, with r2>0:95, but a sig-\nniﬁcant loss in robustness was observed at 64 Kbps in\nsome of the implementations. The most robust MFCC im-\nplementation had a highest frequency of 4600 Hz, while\nthe least robust implementation included frequencies up to\n11025 Hz. Their music corpus contained only 46 songs\nthough, clearly limiting their results. In our experiments,\nall encodings show r2>0:99. However, we note that Pear-\nson’sris very sensible to outliers with such small samples.\nThis is the case of the ﬁrst MFCC coefﬁcients, which are\norders of magnitude larger than the last coefﬁcients. This\nmakesrextremely large simply because the ﬁrst coefﬁ-\ncients are remotely similar; most of the variability between\nfeature vectors is explained because of the ﬁrst coefﬁcient.\nThis is clear in our Table 1, where r\u00191and variance is\nnearly 0. To minimize this sensibility to outliers, we also\nincluded the non-parametric Spearman’s \u001acorrelation co-\nefﬁcient as well as the cosine similarity. In our case, the\ntool with the larger frequency range was shown to be more\nrobust under homogeneous encodings, while the shorter\nrange was more stable under heterogeneous conditions.\nHamawaki et al. [10] analyzed differences in the distri-\nbution of MFCCs for different bitrates using a corpus of\n2513 MP3 ﬁles of Japanese and Korean pop songs with bi-\ntrates between 96 and 192 Kbps. Following a music simi-\nlarity task, they compared differences in the top-10 ranked\nresults when using MFCCs derived from WA V audio, its\nMP3 encoded versions, and the mixture of MFCCs from\ndifferent sources. They found that the correlation of the re-\nsults deteriorates smoothly as the bitrate decreases, while\nranking on a set of MFCCs derived from different formats\nrevealed uncorrelated results. We similarly observed that\nthe differences between MFCCs of the original WA V ﬁles\nand its MP3 versions decrease smoothly with bitrate.\nJensen et al. [12] measured the effect of audio encoding\non performance of an instrument classiﬁer using MFCCs.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n577They compared MFCCs computed from MP3 ﬁles at only\n32-64 Kbps, observing a decrease in performance when\nusing a different encoder for training and test sets. In con-\ntrast, performance did not change signiﬁcantly when using\nthe same encoder. For genre classiﬁcation with MFCCs,\nour results showed no differences in either case. We note\nthough that the bitrates we considered are much larger. Ue-\nmura et al. [23] examined the effect of bitrate on chord\nrecognition using chroma features with an SVM classi-\nﬁer. They observed no obvious correlation between en-\ncoding and estimation results; the best results were even\nobtained with very low bitrates for some codecs. Our re-\nsults on genre classiﬁcation with chroma largely agree in\nthis case as well; the best results with Lib2 were also ob-\ntained by low bitrates. Casey et al. [4] evaluated the effect\nof lossy encodings on genre classiﬁcation tasks using au-\ndio spectrum projection features. They found a small but\nstatistically signiﬁcant decrease in accuracy for bitrates of\n32 and 96 Kbps. In our experiments, we do not observe\nthese differences, although the lowest bitrate we consider\nis 64 Kbps. Jacobson et al. [11] also investigated the ro-\nbustness of onset detection methods to lossy MP3 encod-\ning. They found statistically signiﬁcant changes in accu-\nracy only at bitrates lower than 32 Kbps.\nOur results showed that MFCCs and chroma features, as\ncomputed by Lib1 and Lib2, are generally robust and sta-\nble within reasonable limits. Some differences have been\nnoted between tools though, largely attributable to the dif-\nferent frequency ranges they employ. Nonetheless, it is\nevident that certain combinations of codec and bitrate may\nrequire a re-parameterization of some descriptors to im-\nprove or even maintain robustness. In practice, these pa-\nrameterizations affect the performance and applicability of\nalgorithms, so a balance between performance, robustness\nand generalizability should be sought. These considera-\ntions are of major importance when collecting audio ﬁles\nfor some dataset, as a minimum audio quality might be\nneeded for some descriptors.\n7. CONCLUSIONS\nIn this paper we have studied the robustness of two com-\nmon audio descriptors used in Music Information Re-\ntrieval, namely MFCCs and chroma, to different audio en-\ncodings and analysis parameters. Using a varied corpora\nof music pieces and two different audio analysis tools we\nhave conﬁrmed that MFFCs are robust to frame/hop sizes\nand lossy encoding provided that a minimum bitrate of\napproximately 160 Kbps is used. Chroma features were\nshown to be even more robust, as the codec and bitrates\nhad virtually no effect on the computed descriptors. This\nis somewhat expected given that chroma does not capture\ninformation as ﬁne-grained as MFCCs do, and that lossy\ncompression does not alter the perceived tonality. We did\nﬁnd subtle differences between implementations of these\naudio features, which call for further research on standard-\nizing algorithms and parameterizations to maximize their\nrobustness while maintaining their effectiveness in the var-\nious tasks they are used in. The immediate line for future\nwork includes the analysis of other features and tools.8. ACKNOWLEDGMENTS\nThis work is partially supported by an A4U postdoctoral\ngrant and projects SIGMUS (TIN2012-36650), Comp-\nMusic (ERC 267583), PHENICX (ICT-2011.8.2) and Gi-\nantSteps (ICT-2013-10).\n9. REFERENCES\n[1] J.J. Aucouturier, F. Pachet, and M. Sandler. “The way it\nsounds”: timbre models for analysis and retrieval of music\nsignals. IEEE Trans. Multimedia, 2005.\n[2] D. Bogdanov, N. Wack, et al. ESSENTIA: an audio analysis\nlibrary for music information retrieval. In ISMIR, 2013.\n[3] C. Cannam, M.O. Jewell, C. Rhodes, M. Sandler, and\nM. d’Inverno. Linked data and you: bringing music research\nsoftware into the semantic web. J. New Music Res., 2010.\n[4] M. Casey, B. Fields, et al. The effects of lossy audio encoding\non genre classiﬁcation tasks. In AES, 2008.\n[5] W. Chai. Semantic segmentation and summarization of mu-\nsic: methods based on tonality and recurrent structure. IEEE\nSignal Processing Magazine, 2006.\n[6] D. Ellis. Classifying music audio with timbral and chroma\nfeatures. In ISMIR, 2007.\n[7] T. Fujishima. Realtime chord recognition of musical sound:\na system using common lisp music. In ICMC, 1999.\n[8] T. Ganchev, N. Fakotakis, and G. Kokkinakis. Comparative\nevaluation of various MFCC implementations on the speaker\nveriﬁcation task. In SPECOM, 2005.\n[9] E. G ´omez. Tonal description of music audio signals. PhD the-\nsis, Universitat Pompeu Fabra, 2006.\n[10] S. Hamawaki, S. Funasawa, et al. Feature analysis and nor-\nmalization approach for robust content-based music retrieval\nto encoded audio with different bit rates. In MMM, 2008.\n[11] K. Jacobson, M. Davies, and M. Sandler. The effects of lossy\naudio encoding on onset detection tasks. In AES, 2008.\n[12] J.H. Jensen, M.G. Christensen, D. Ellis, and S.H. Jensen.\nQuantitative analysis of a common audio similarity measure.\nIEEE TASLP, 2009.\n[13] B. McFee, L. Barrington, and G. Lanckriet. Learning content\nsimilarity for music recommendation. IEEE TASLP, 2012.\n[14] D.C. Montgomery. Design and Analysis of Experiments. Wi-\nley & Sons, 2009.\n[15] M. M ¨uller and S. Ewert. Towards timbre-invariant audio fea-\ntures for harmony-based music. IEEE TASLP, 2010.\n[16] M. M ¨uller, H. Mattes, and F. Kurth. An efﬁcient multiscale\napproach to audio synchronization. In ISMIR, 2006.\n[17] J. Paulus, M. M ¨uller, and A Klapuri. Audio-based music\nstructure analysis. In ISMIR, 2010.\n[18] L.R. Rabiner and R.W. Schafer. Introduction to Digital\nSpeech Processing. Foundations and Trends in Signal Pro-\ncessing. 2007.\n[19] J. Reed and C. Lee. Preference music ratings prediction using\ntokenization and minimum classiﬁcation error training. IEEE\nTASLP, 2011.\n[20] J. Serr `a, E. G ´omez, and P. Herrera. Audio cover song identi-\nﬁcation and similarity: background, approaches, evaluation,\nand beyond. In Z. Ra ´s and A.A. Wieczorkowska, editors, Ad-\nvances in Music Information Retrieval. Springer, 2010.\n[21] S. Sigurdsson, K.B. Petersen, and T. Lehn-Schiler. Mel Fre-\nquency Cepstral Coefﬁcients: an evaluation of robustness of\nMP3 encoded music. In ISMIR, 2006.\n[22] M. Slaney. Auditory toolbox. Interval Research Corpora-\ntion, Technical Report, 1998. http://engineering.\npurdue.edu/ ˜malcolm/interval/1998-010/.\n[23] A. Uemura, K. Ishikura, and J. Katto. Effects of audio com-\npression on chord recognition. In MMM, 2014.\n[24] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and HG. Okuno.\nAn efﬁcient hybrid music recommender system using an in-\ncrementally trainable probabilistic generative model. IEEE\nTASLP, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n578"
    },
    {
        "title": "Vocal Separation using Singer-Vowel Priors Obtained from Polyphonic Audio.",
        "author": [
            "Shrikant Venkataramani",
            "Nagesh Nayak",
            "Preeti Rao",
            "Rajbabu Velmurugan"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1414974",
        "url": "https://doi.org/10.5281/zenodo.1414974",
        "ee": "https://zenodo.org/records/1414974/files/VenkataramaniNRV14.pdf",
        "abstract": "Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic- sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior sep- aration of the lead vocal from the instrumental background. Singer specific dictionaries learned from available poly- phonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF op- timization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral en- velope.",
        "zenodo_id": 1414974,
        "dblp_key": "conf/ismir/VenkataramaniNRV14",
        "keywords": [
            "harmonic-sinusoidal modeling",
            "matrix decomposition methods",
            "singer identity",
            "soft mask",
            "melodic instruments",
            "polyphonic recordings",
            "dictionary learning",
            "NMF optimization",
            "vocal signal bases",
            "sparseness constraints"
        ],
        "content": "VOCAL SEPARATION USING SINGER-VOWEL PRIORS OBTAINED\nFROM POLYPHONIC AUDIO\nShrikant Venkataramani1, Nagesh Nayak2, Preeti Rao1, and Rajbabu Velmurugan1\n1Department of Electrical Engineering , IIT Bombay , Mumbai 400076\n2Sensibol Audio Technologies Pvt. Ltd.\n1fvshrikant, prao, rajbabug@ee.iitb.ac.in\n2nageshsnayak@sensibol.com\nABSTRACT\nSingle-channel methods for the separation of the lead vocal\nfrom mixed audio have traditionally included harmonic-\nsinusoidal modeling and matrix decomposition methods,\neach with its own strengths and shortcomings. In this work\nwe use a hybrid framework to incorporate prior knowledge\nabout singer and phone identity to achieve the superior sep-\naration of the lead vocal from the instrumental background.\nSinger speciﬁc dictionaries learned from available poly-\nphonic recordings provide the soft mask that effectively\nattenuates the bleeding-through of accompanying melodic\ninstruments typical of purely harmonic-sinusoidal model\nbased separation. The dictionary learning uses NMF op-\ntimization across a training set of mixed signal utterances\nwhile keeping the vocal signal bases constant across the\nutterances. A soft mask is determined for each test mixed\nutterance frame by imposing sparseness constraints in the\nNMF partial co-factorization. We demonstrate signiﬁcant\nimprovements in reconstructed signal quality arising from\nthe more accurate estimation of singer-vowel spectral en-\nvelope.\n1. INTRODUCTION\nSource separation techniques have been widely applied in\nthe suppression of the lead vocal in original songs to ob-\ntain the orchestral background for use in karaoke and remix\ncreation. In stereo and multichannel recordings, spatial\ncues can contribute signiﬁcantly to vocal separation from\nthe original mixtures. However this separation is not com-\nplete, depending on the manner in which the multiple in-\nstruments are panned in the mix. Further, an important cat-\negory of popular music recordings, dating until the 1950s\nin the West and even later in the rest of the world, are\npurely monophonic. Single-channel methods for the sepa-\nration of the lead vocal from the instrumental background\nc\rShrikant Venkataramani, Nagesh Nayak, Preeti Rao, Ra-\njbabu Velmurugan.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Shrikant Venkataramani, Nagesh\nNayak, Preeti Rao, Rajbabu Velmurugan. “VOCAL SEPARATION US-\nING SINGER-VOWEL PRIORS OBTAINED FROM POLYPHONIC\nAUDIO”, 15th International Society for Music Information Retrieval\nConference, 2014.include harmonic sinusoidal modeling and matrix decom-\nposition methods. Of these, harmonic sinusoidal model-\ning has found success in situations where no clean data is\navailable for supervised learning [6], [10]. Based on the\nassumption that the vocal is dominant in the mixture, pre-\ndominant pitch detection methods are applied to obtain the\nvocal pitch and hence the predicted vocal harmonic loca-\ntions at each instant in time. Harmonic sinusoidal mod-\neling is then applied to reconstruct the vocal component\nbased on assigning a magnitude and phase to each recon-\nstructed harmonic from a detected sinusoidal peak in the\ncorresponding spectral neighborhood of the mixed signal\nshort-time Fourier transform (STFT). The vocal signal is\nreconstructed by the amplitude and phase interpolation of\nthe harmonic component tracks. The instrumental back-\nground is obtained by the subtraction of the reconstructed\nvocal from the original mixture. A high degree of vocal\nseparation is obtained when the assumption of vocal dom-\ninance holds for the mixture. However some well-known\nartifacts remain viz. (i) “bleeding through” of some of the\nmelodic instrumentation due to the blind assignment of the\ntotal energy in the mixed signal in the vocal harmonic lo-\ncation to the corresponding reconstructed harmonic; this\nartifact is particularly perceptible in the sustained vowel re-\ngions of singing, (ii) improper cancellation of the unvoiced\nconsonants and breathy voice components due to the lim-\nitations of sinusoidal modeling of noise and (iii) residual\nof vocal reverb if present in the original [14]. To address\nthe ﬁrst shortcoming, recent methods rely on the availabil-\nity of non-overlapping harmonics of the same source any-\nwhere in the entire audio [3]. We propose to replace the\nbinary mask (implicit in the harmonic-sinusoidal model-\ning) applied to the vocal harmonics before reconstruction\nby a soft-mask (a form of Wiener ﬁltering). An effective\nsoft mask would be based on an accurate estimate of the\nvocal signal spectrum at any time-instant [2], [14]. This\nwould improve the reconstructed vocal signal and lead to\nmore complete suppression in the estimated background.\nThe vocal signal spectrum depends on several factors\nsuch as the singer’s voice, the phone being uttered, the\npitch and the vocal effort. We cannot assume the avail-\nability of clean data for supervised training (i.e., unaccom-\npanied voice of the particular singer). However popular\nsingers typically have a large number of songs to their\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n283credit, and therefore a method for learning a dictionary of\nsoft masks for the singer from such a training data set could\nbe useful. The training set thus has original single-channel\npolyphonic songs where the vocal characteristics corre-\nspond to the singer but the background orchestration is di-\nverse. We apply non-negative matrix factorization (NMF)\nmethods to estimate the invariant set of basis vectors across\nmultiple instances of the singer’s phones in different songs.\nIn the recent past, several systems have been proposed that\nqualify as modiﬁcations of NMF for improved performance\nin various scenarios where speciﬁc prior knowledge about\nthe data are available [5] (and references therein). In the\npresent work, we attempt to formulate a NMF approach\nto obtain basis elements corresponding to the singer’s ut-\nterances by providing audio corresponding to a particu-\nlar singer. Given the very diverse spectra of the differ-\nent phones in a language, the quality of the decomposition\ncan be improved by restricting the optimization to within\na phone class [11]. We exploit the availability of song-\nsynchronized lyrics data available in karaoke applications\nto achieve this. Our main contribution is to combine the\nadvantages of harmonic-sinusoidal modeling in localizing\nthe vocal components in time-frequency with that of soft-\nmasking based on spectral envelope estimates from a NMF\ndecomposition on polyphonic audio training data. Prior\nknowledge about singer identity and underlying phone tran-\nscription of the training and test audio are incorporated in\nthe proposed framework. We develop and evaluate the con-\nstrained NMF optimization required for the training across\ninstances where a common basis function set corresponds\nto the singer-vowel. On the test data, partial co-factorization\nwith a sparseness constraint helps obtain the correct basis\ndecomposition for the mixed signal at any time instant, and\nthus a reliable spectral envelope estimate of the vowel for\nuse in the soft mask. Finally, the overall system is eval-\nuated based on the achieved vocal and orchestral back-\nground separation using objective measures and informal\nlistening. In the next sections, we present the overall sys-\ntem for vocal separation, followed by the proposed NMF-\nbased singer-vowel dictionary learning, estimation of the\nsoft mask for test mixed polyphonic utterances and exper-\nimental evaluation of system performance.\n2. PROPOSED HYBRID SYSTEM\nA block diagram of the proposed hybrid system for vocal\nseparation is shown in Figure 1. The single-channel audio\nmixture considered for vocal separation is assumed to have\nthe singing voice, when present, as the dominant source in\nthe mix. We assume that the sung regions are annotated at\nthe syllable level, as expected from music audio prepared\nfor karaoke use. A predominant pitch tracker [9] is applied\nto the sung regions to detect vocal pitch at 10ms intervals\nthroughout the sung regions of the audio. Sinusoidal com-\nponents are tracked in the computed short-time magnitude\nspectrum after biasing trajectory information towards the\nharmonic locations based on the detected pitch [8]. The\npitch salience and total harmonic energy are used to locate\nthe vowel region within the syllable. The vocal signal canMixture\n+\nPredominant f0\ndetection\nHarmonic\nsinusoidal\nmodelingMasking\nMFB, 40 ﬁlters\n130Hz - 8kHzMFB\ninversion\nBest MFS\nenvelope estimate\nPhoneme dictionary\nat multiplef0Spectral\nenvelope\nMFS envelopeSpectral\nenvelope\nestimate\u0000\nV ocalsInstr.\nFigure 1. Block diagram of the proposed vocal separation\nsystem.\nbe reconstructed from the harmonic sinusoidal component\ntrajectories obtained by amplitude and phase interpolation\nof the frame-level estimates from the STFT. An estimate\nof the instantaneous spectral envelope of the singer’s voice\nprovides a soft mask to re-shape the harmonic amplitudes\nbefore vocal reconstruction. The mel-ﬁltered spectral en-\nvelope (MFS) is computed by applying a 40-band mel-\nﬁlter bank to the log-linearly interpolated envelope of the\nmixture harmonic amplitudes. By using the spectral enve-\nlope, we eliminate pitch dependence in the soft mask to a\nlarge extent. The phoneme dictionary consists of a set of\nbasis vectors for each vowel, at various pitches. A linear\ncombination of these basis vectors may be used to estimate\nthe MFS envelope of the vocal component of the mixture,\nfrom the MFS envelope of the mixture. These spectral en-\nvelope vectors are learnt from multiple polyphonic mix-\ntures of the phoneme as explained in Section 3. The MFS\nis used as a low-dimensional perceptually motivated repre-\nsentation. The reconstructed vocal signal is subtracted in\nthe time-domain from the polyphonic mixture to obtain the\nvocal-suppressed music background.\n3. SPECTRAL ENVELOPE DICTIONARY\nLEARNING USING NMF\nTo obtain the singer speciﬁc soft mask mentioned in the\nprevious section, we create a dictionary of basis vectors\ncorresponding to each of the vowels of the language. This\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n284dictionary is created from polyphonic song segments, con-\ntaining the vowel, of the singer under consideration. While\nspectral envelope of a vowel depends on the vowel iden-\ntity, there are prominent dependencies on (i) the singer,\nwhose physiological characteristics and singing style af-\nfect the precise formant locations and bandwidths for a\ngiven vowel. This is especially true of the higher formants\n(4th and 5th), which depend primarily on the singer rather\nthan on the vowel; (ii) pitch, speciﬁcally in singing where\nthe articulation can vary with large changes in pitch due to\nthe “formant tuning” phenomenon [12]; (iii) loudness or\nvocal effort. Raising the vocal effort reduces spectral tilt,\nincreasing the relative amplitudes of the higher harmonics\nand consequently the brightness of the voice.\nIn the proposed dictionary learning, pitch dependence is\naccounted for by separate dictionary entries corresponding\nto 2 or 3 selected pitch ranges across the 2 octaves span\nof a singer. Since the pitch and vowel identity are known\nfor the test song segment, the correct dictionary can be se-\nlected at any time. The basis vectors for any pitch range of\nthe singer-vowel capture the variety of spectral envelopes\nthat arise from varying vocal effort and vowel context. We\nhave training data of several instances of a particular singer\nuttering a common vowel. These utterances have been ob-\ntained from different songs and hence, we may assume\nthat the accompaniments in the mixtures are different. The\nMFS envelopes, reviewed in the previous section, are ex-\ntracted for each training vowel instance in the polyphonic\naudio. Based on the assumption of additivity of the spectral\nenvelopes of vocal and instrumental background, there is a\ncommon partial factor corresponding to the singer-vowel\nacross the mixtures with changing basis vectors for the ac-\ncompaniment.\nWe use NMF to extract common features (singer-vowel\nspectra) across multiple song segments. The conventional\nuse of NMF is similar to the phoneme-dependent NMF\nused for speech separation in [7] where the bases are es-\ntimated from clean speech. We extend the scope of NMF\nfurther, using non-negative matrix partial co-factorization\n(NMPCF ) [4] equivalent to NMF for multiblock data [15]\ntechniques. NMPCF and its variants have been used in\ndrum source separation [4], where one of the training sig-\nnals is the solo drums audio. Here, we use NMPCF for\nmultiple MFS matrices of mixed signals across segments\nof the polyphonic audio of the singer, without the use of\nclean vocal signal. This will yield a common set of bases\nrepresenting the singer-vowel and other varying bases rep-\nresentative of the accompaniments.\nWe now describe the NMPCF algorithm for learning the\nsinger-vowel basis. The MFS representation for one spe-\nciﬁc annotated segment of a polyphonic music is repre-\nsented as Vl. This section has the vowel of interest and\ninstrumental accompaniments. We have MFS of Msuch\nmixtures for i= 1;:::;M represented as [15],\nVi=Vc;i+Va;i; i = 1;:::M: (1)\nwhereVc;iandVa;idenote the MFS of the common singer-\nvowel and accompaniment, respectively. Using NMF de-composition for the MFS spectra we have,\nVi=WcHc;i+Wa;iHa;i; i = 1;:::M: (2)\nwhereWc2RF\u0002Nc\n+ denotes the basis vectors correspond-\ning to the common vowel shared by the Mmixtures and\nWa;i2RF\u0002Na\n+ are the basis vectors corresponding to the\naccompaniments. Here Fis the number of mel-ﬁlters (40)\nused,NcandNaare the numer of basis vectors for the\nvowel and accompaniments, respectively. The matrices\nHc;iandHa;iare the activation matrices for the vowel\nand accompaniment basis vectors, respectively. Our objec-\ntive is to obtain the basis vectors Wccorresponding to the\ncommon vowel across these Mmixtures. We achieve this\nby minimizing the Frobenius norm k:k2\nFof the discrep-\nancy between the given mixtures and their factorizations,\nsimultaneously. Accordingly, the cost function,\nD=MX\ni=11\n2kVi\u0000WcHc;i\u0000Wa;iHa;ik2\nF+\n\u00151\n2kWa;ik2\nF;(3)\nis to be minimized with respect to Wc,Wa;i,Hc;i, and\nHa;i. The regularizer kWa;ik2\nFand\u00151the Lagrange\nmultiplier lead to dense Wa;iandWcmatrices [15]. The\nbasis vectors thus obtained are a good representation of\nboth the common vowel and the accompaniments, across\nthe mixture. In this work, we choose \u00151= 10 for our ex-\nperimentation as it was found to result in the sparsest Hc;i\nmatrix for varying values of \u00151. We solve (3) using the\nmultiplicative update algorithm. The multiplicative update\nfor a parameter Pin solving the NMF problem takes the\ngeneral form,\nP=P\fr\u0000\nP(D)\nr+\nP(D); (4)\nwherer\u0000\nX(D)andr+\nX(D)represent the negative and pos-\nitive parts of the derivative of the cost Dw.r.t. the param-\neterX, respectively,\frepresents the Hadamard (element-\nwise) product and the division is also element-wise. Cor-\nrespondingly, the multiplicative update for the parameter\nWcin (3) is,\nWc=Wc\fr\u0000\nWc(D)\nr+\nWc(D): (5)\nwhere,\nrWc(D) =MX\ni=1(WcHc;i+Wa;iHa;i\u0000Vi)HT\nc;i:(6)\nSimilarly, the update equation for other terms in (3) are,\nHc;i=Hc;i\fWT\ncVi\nWTcWcHc;i+WTcWa;iHa;i; (7)\nHa;i=Ha;i\fWT\na;iVi\nWT\na;iWcHc;i+WT\na;iWa;iHa;i;(8)\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n285Wa;i=\nWa;i\fViHT\na;i\nWcHc;iHT\na;i+Wa;iHa;iHT\na;i+\u00151\u0002Wa;i:\n(9)\nfori= 1;:::;M . The basis vectors Wcfor the vari-\nous phonemes form the dictionary and act as a prior in the\nspectral envelope estimation. Each dictionary entry is as-\nsociated with a vowel and pitch range. We denote each\nentry in the dictionary as Wc(=p=;f 0)for each vowel =p=\nat pitchf0.\n4. SOFT MASK ESTIMATION USING\nSINGER-VOWEL DICTIONARY\nIn this section, we describe the approach to estimate the\nframe-wise soft mask for a test polyphonic vowel mixture\nsegment. We ﬁrst obtain the MFS envelope for the mix-\nture as mentioned in Section 2. With the vowel label and\npitch range known, we obtain the corresponding set of ba-\nsis vectors Wc(=p=;f 0)from the dictionary. Given this\nMFS representation of the mixture and the basis vectors,\nour objective is to separate the vocal component from the\nmixture. We do this by minimizing the cost function\nDT=1\n2kVT\u0000WcHc;T\u0000Wa;THa;Tk2\nF+\n\u00152\n2kHc;Tk2\nF;(10)\nwhere the subscript Trefers to the test case. The minimiza-\ntion is done with the dictionary bases Wckept ﬁxed and\nusing multiplicative updates for Hc;T,Wa;TandHa;T.\nThe sparsity constraint on Hc;Tin (10) accounts for the\nfact that the best set of bases representing the vowel would\nresult in the sparsest temporal matrix Hc;T. Under this\nformulation, WcHc;Twill give an estimate of the vowel’s\nMFS envelope Vc(as in (1)) for the mixture. An alternate\nway is to use Wiener ﬁltering to estimate Vcas,\nbVc=WcHc;T\nWcHc;T+Wa;THa;T\fVT: (11)\nThis estimated vowel MFS can be used to reconstruct the\nspectral envelope of the vowel ^C. This is done by multi-\nplyingbVcwith the pseudoinverse of the DFT matrix Mof\nthe mel ﬁlter bank [1] as ^C=MybVc. A soft mask cor-\nresponding to this spectral envelope can be obtained using\nthe Gaussian radial basis function [2],\nGb(f;t) = exp0\nB@\u0000\u0010\nlogX(f;t )\u0000log^C(f;t )\u00112\n2\u001b21\nCA\n(12)\nwhere,\u001bis the Gaussian spread, Xis the magnitude spec-\ntrum of the mixed signal. The soft mask (12) is evaluated\nwith\u001b= 1, in a 50Hz band around the pitch (f 0) and its\nharmonics [14].\nHaving obtained the soft mask, the vocals track is re-\nconstructed by multiplying the soft mask with the harmonicamplitudes of the sinusoidally modeled signal. The resyn-\nthesized signal then corresponds to the reconstructed vo-\ncals. The accompaniment can be obtained by performing a\ntime-domain subtraction of the reconstructed vocals from\nthe original mixture.\n5. EXPERIMENTS AND PARAMETER CHOICES\nGiven a polyphonic vowel segment, the vocal is separated\nby applying the generated soft mask corresponding to the\ngiven mixture. We compare the separated vocal with the\nground truth to evaluate the performance. The performance\nevaluation of the proposed system is carried out in two\nsteps. The ﬁrst step is to choose the parameters of the sys-\ntem using the distance in the MFS space between the es-\ntimated and ground-truth MFS vectors obtained from the\nclean utterance. The second step is the computation of\nsignal-to-distortion (SDR) measure (in dB) on the sepa-\nrated vocal and instrumental time-domain signals which\nwill be given in Section 6. We present the training and\ntest data used in the experiments next.\n5.1 Description of the Dataset\nThe training dataset comprised of nine instances of three\nvowels viz., /a/, /i/, /o/ at two average pitches of 200 Hz\nand 300 Hz and sung by a male singer over three different\nsongs with their accompaniments, annotated at the phoneme\nlevel. The training data was chosen so as to have differ-\nent accompaniments across all the instances of a vowel.\nThe training audios thus contained the vowel utterances\nthroughout in the presence of background accompaniments.\nThe training mixtures were pre-emphasised using a ﬁlter\nwith a zero located at 0:7to better represent the higher for-\nmants. A dictionary of bases was created for all the vowels\nfor the two pitch ranges using the NMCPF optimization\nprocedure discussed in Section 3. The performance was\nevaluated over a testing dataset of 45 test mixtures with 15\nmixtures for each vowel over the two pitch ranges. The\nmixtures used for testing were distinct from the training\nmixtures. Since the audios were obtained directly from full\nsongs, there was a signiﬁcant variation in terms of the pitch\nof the vowel utterances around the average pitch ranges\nand in terms of coarticulation. The training and testing\nmixtures were created in a karaoke singing context and\nhence, we had available, the separate vocal and accom-\npaniment tracks to be used as ground truth in the perfor-\nmance evaluation. All the mixtures had durations in the\nrange of 400 ms - 2.2 s and were sampled at a frequency of\n16kHz. The window size and hop size used for the 1024\npoint STFT were 40ms and 10ms, respectively.\n5.2 Choice of Parameters\nThere are several training parameters likely to inﬂuence\nthe performance of the system. These include the ranks\nof the matrices in the decomposition (W c,Wa) and the\nnumber of mixtures M. We obtain these parameters exper-\nimentally using a goodness-of-ﬁt measure. The goodness-\nof-ﬁt is taken to be the normalised Frobenius norm of the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n286Figure 2. Goodness-of-ﬁt, averaged over all phoneme\nbases, with increasing number of mixtures (M) used in\nthe parallel optimization procedure for different decompo-\nsition ranks. The distance measure decreases indicating an\nimproving ﬁt as the number of mixtures and rank increase.\ndifference between the ideal envelope of a vowel in the\nMFS domain Vcand its best estimate bVcobtained as a\nlinear combination of the bases, for a mixture containing\nthe vowel and accompaniment. This estimate can be calcu-\nlated as explained in Section 4. Lower the value of this dis-\ntance measure, closer is the envelope estimate to the ideal\nestimate. The various bases may be compared by calculat-\ningDifor different bases Wciusing\nDi=kVc\u0000bVcik2\nF\nkVck2\nF\n=kVc\u0000WciHci\nWciHci+W aiHai\fVTk2\nF\nkVck2\nF;(13)\nand comparing the same. To account for variabilities in the\nevaluation mixtures, the distance measure is evaluated and\naveraged over a number of mixtures and combinations, for\neach set of bases Wci. The goodness-of-ﬁt is used only\nto choose the appropriate parameter values for the system.\nThe performance of the overall system, however, is evalu-\nated in terms of SDR.\nAs shown in Figure 2, the goodness-of-ﬁt measure de-\ncreases with increasing rank of the decomposition (num-\nber of vocal basis vectors) for a given M. The decreasing\ntrend ﬂattens out and then shows a slight increase beyond\nrank 35. For a ﬁxed rank, the goodness-of-ﬁt improves\nwith increasing number of mixtures. Of the conﬁgurations\ntried, the distance measure is minimum when four mix-\ntures (M = 4) are used in the NMPCF optimization to\nobtain the dictionary. Thus, a rank 35 decomposition with\nM= 4is chosen for each singer-vowel dictionary for sys-\ntem performance evaluation.\nAs for the rank of the accompaniment basis, it is ob-\nserved that the regularization term in the joint optimiza-\ntion (3) seems to make the algorithm robust to choice of\nnumber of basis vectors for the accompaniment. Eight\nbasis vectors were chosen for each mixture term in the\njoint optimization. Although the number of accompani-Separated Binary Soft mask\ntrack mask Original singer Alternate singer\nV ocal 8.43 9.06 8.66\nInstrumental 13.63 14.16 14.10\nTable 1. SDR values (in dB) for separated vocals and in-\nstruments obtained using a binary mask, soft mask from\nthe original singer’s training mixtures and soft mask from\nan alternate singer’s vocals.\nment bases seems to be comparatively low, eight bases\nwere sufﬁcient to reconstruct the accompaniment signals\nfrom the mixtures. A high value for \u00152in the test opti-\nmization problem of (10) results in a decomposition in-\nvolving a linear combination of the least number of bases\nper time frame. This sparse decomposition may not neces-\nsarily lead to the best reconstruction in more challenging\nscenarios involving articulation variations. Thus a small\nvalue of\u00152= 0:1 was chosen.\n6. RESULTS AND DISCUSSION\nWe evaluate the performance of the system using the SDR.\nThe SDR is evaluated using the BSS eval toolbox [13].\nThe SDR values averaged across 45 vowel test mixtures,\nseparately for the reconstructed vocals and instrumental\nbackground are given in Table 1. To appreciate the im-\nprovement, if any, the SDR is also computed for the har-\nmonic sinusoidal model without soft masking (i.e., binary\nmasking only). While the proposed soft masking shows an\nincrease in SDR, closer examination revealed that the im-\nprovements were particularly marked for those mixtures\nwith overlapping vocal and instrumental harmonics (ac-\ncompaniments) in some spectral regions. This was also\nborne out by informal listening. When we isolated these\nsamples, we observed SDR improvements of up to 4dB in\nseveral instances. This is where the selective attenuation of\nthe harmonic amplitudes in accordance with the estimated\nvowel spectral envelope is expected to help most. The har-\nmonics in the non-formant regions are retained in the in-\nstrumental background rather than being canceled out as\nin binary masking, contributing to higher SDR1.\nTo understand the singer dependence of the dictionary,\nwe carried out soft mask estimation from the polyphonic\ntest mixtures using the basis vectors of an alternate singer.\nThis basis was a set of clean vowel spectral envelopes ob-\ntained from another male singer’s audio with the same vow-\nels and pitches corresponding to our training dataset. We\nobserve from Table 1 that the alternate singer soft mask\ndoes better than the binary mask, since it brings in the\nvowel dependence of the soft mask. However, it does not\nperform as well as the original singer’s soft mask even\nthough the latter is obtained from clean vowel utterances.\nAs depicted in Figure 3 (for a sample case), the envelope\nobtained using the original singer’s data closely follows the\n1Audio examples are available at http://www.ee.iitb.ac.\nin/student/ ˜daplab/ISMIR_webpage/webpageISMIR.\nhtml.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n287Figure 3. Comparison of the reconstructed phoneme en-\nvelope of the phoneme /a/ obtained from training mixtures\nof the same singer and with that obtained from pure vocals\nof a different singer.\nideal envelope of the phoneme.\nAlthough the NMF optimization converges slowly, the\nnumber of iterations to be carried out to obtain the enve-\nlope is low, for both training and testing procedures. It\nis observed that the bases and envelopes attain their ﬁnal\nstructure after 4000 and 1000 iterations, respectively.\n7. CONCLUSION\nSoft masks derived from a dictionary of singer-vowel spec-\ntra are used to improve upon the vocal-instrumental music\nseparation achieved by harmonic sinusoidal modeling for\npolyphonic music of the particular singer. The main con-\ntribution of this work is an NMF based framework that ex-\nploits the amply available original polyphonic audios of the\nsinger as training data for learning the dictionary of singer\nspectral envelopes. Appropriate constraints are introduced\nin the NMF optimization for training and test contexts. The\navailability of lyric-aligned audio (and therefore phone la-\nbels) helps to improve the homogeneity of the training data\nand have a better model with fewer basis vectors. Signiﬁ-\ncant improvements in reconstructed signal quality are ob-\ntained over binary masking. Further it is demonstrated that\na vowel-dependent soft mask obtained from clean data of\na different available singer is not as good as the singer-\nvowel dependent soft mask even if the latter is extracted\nfrom polyphonic audio.\n8. ACKNOWLEDGEMENT\nPart of the work was supported by Bharti Centre for Com-\nmunication in IIT Bombay.\n9. REFERENCES\n[1] L. Boucheron and P. De Leon. On the inversion of mel-\nfrequency cepstral coefﬁcients for speech enhancement\napplications. In Int. Conf. Signals Electronic Systems,\n2008., pages 485–488, 2008.\n[2] D. Fitzgerald. V ocal separation using nearest neigh-\nbours and median ﬁltering. In Irish Signals Systems\nConf., 2012.[3] J. Han and B. Pardo. Reconstructing completely over-\nlapped notes from musical mixtures. In Proc. IEEE\nInt. Conf. Acoustics, Speech, Signal Process., 2011\n(ICASSP ’11.), pages 249 – 252, 2011.\n[4] M. Kim, J. Yoo, K. Kang, and S. Choi. Nonnegative\nmatrix partial co-factorization for spectral and tem-\nporal drum source separation. IEEE Journal Selected\nTopics Signal Process., 5(6):1192 – 1204, 2011.\n[5] A. Lefevre, F. Bach, and C. Fevotte. Semi-supervised\nNMF with time-frequency annotations for single-\nchannel source separation. In Proc. Int. Soc. Music\nInformation Retrieval (ISMIR 2012), pages 115–120,\n2012.\n[6] Y . Li and D. Wang. Separation of singing voice from\nmusic accompaniment for monaural recordings. IEEE\nTrans. Audio, Speech, Lang. Process., 15(4):1475–\n1487, 2007.\n[7] B. Raj, R. Singh, and T. Virtanen. Phoneme-dependent\nNMF for speech enhancement in monaural mixtures.\nInProc. Interspeech, pages 1217–1220, 2011.\n[8] V . Rao, C. Gupta, and P. Rao. Context-aware fea-\ntures for singing voice detection in polyphonic music.\nInProc. Adaptive Multimedia Retrieval, Barcelona,\nSpain, 2011.\n[9] V . Rao and P. Rao. V ocal melody extraction in the\npresence of pitched accompaniment in polyphonic\nmusic. IEEE Trans. Audio, Speech, Lang. Process.,\n18(8):2145–2154, 2010.\n[10] M. Ryynanen, T. Virtanen, J. Paulus, and A. Kla-\npuri. Accompaniment separation and karaoke applica-\ntion based on automatic melody transcription. In IEEE\nInt. Conf. Multimedia Expo, pages 1417–1420, 2008.\n[11] M. Schmidt and R. Olsson. Single-channel speech sep-\naration using sparse non-negative matrix factorization.\nInProc. of Interspeech, pages 2617–2614, 2006.\n[12] J. Sundberg. The science of the singing voice. Music\nPerception: An Interdisciplinary Journal, 7(2):187 –\n195, 1989.\n[13] E. Vincent, R. Gribonval, and C. Fevotte. Performance\nmeasurement in blind audio source separation. IEEE\nTrans. Audio, Speech, Lang. Process., 14(4):1462–\n1469, 2006.\n[14] T. Virtanen, A. Mesaros, and M. Ryyn ¨anen. Combining\npitch-based inference and non-negative spectrogram\nfactorization in separating vocals from polyphonic mu-\nsic. In ICSA Tutorial and Research Workshop on Sta-\ntistical and Perceptual Audition, Brisbane, Australia,\n2008.\n[15] G. Zhou, A. Cichocki, Q. Zhao, and S. Xie. Nonnega-\ntive matrix and tensor factorizations: An algorithmic\nperspective. IEEE Signal Process. Magazine, pages\n54–65, 2014.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n288"
    },
    {
        "title": "Towards Seamless Network Music Performance: Predicting an Ensemble&apos;s Expressive Decisions for Distributed Performance.",
        "author": [
            "Bogdan Vera",
            "Elaine Chew"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416186",
        "url": "https://doi.org/10.5281/zenodo.1416186",
        "ee": "https://zenodo.org/records/1416186/files/VeraC14.pdf",
        "abstract": "Internet performance faces the challenge of network la- tency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of dis- tributed or co-located ensembles. We evaluate the model by comparing its prediction ac- curacy to two others: one employing only linear condi- tional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent dis- tributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promis- ing, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predic- tive timing cues using our system.",
        "zenodo_id": 1416186,
        "dblp_key": "conf/ismir/VeraC14",
        "keywords": [
            "network latency",
            "music prediction",
            "expressive timing",
            "Bayesian network",
            "stylistic model selection",
            "linear conditional gaussian distributions",
            "rehearsals",
            "distributed ensembles",
            "custom-composed piece",
            "predictive timing cues"
        ],
        "content": "TOWARDS SEAMLESS NETWORK MUSIC PERFORMANCE:\nPREDICTING AN ENSEMBLE’S EXPRESSIVE DECISIONS FOR\nDISTRIBUTED PERFORMANCE\nBogdan Vera\nQueen Mary University of London\nCentre for Digital Music\nb.vera@qmul.ac.ukElaine Chew\nQueen Mary University of London\nCentre for Digital Music\nelaine.chew@qmul.ac.uk\nABSTRACT\nInternet performance faces the challenge of network la-\ntency. One proposed solution is music prediction, wherein\nmusical events are predicted in advance and transmitted\nto distributed musicians ahead of the network delay. We\npresent a context-aware music prediction system focusing\non expressive timing: a Bayesian network that incorporates\nstylistic model selection and linear conditional gaussian\ndistributions on variables representing proportional tempo\nchange. The system can be trained using rehearsals of dis-\ntributed or co-located ensembles.\nWe evaluate the model by comparing its prediction ac-\ncuracy to two others: one employing only linear condi-\ntional dependencies between expressive timing nodes but\nno stylistic clustering, and one using only independent dis-\ntributions for timing changes. The three models are tested\non performances of a custom-composed piece that is played\nten times, each in one of two styles. The results are promis-\ning, with the proposed system outperforming the other two.\nIn predictable parts of the performance, the system with\nconditional dependencies and stylistic clustering achieves\nerrors of 15ms; in more difﬁcult sections, the errors rise\nto 100ms; and, in unpredictable sections, the error is too\ngreat for seamless timing emulation. Finally, we discuss\navenues for further research and propose the use of predic-\ntive timing cues using our system.\n1. INTRODUCTION\nEnsemble performance between remote musicians playing\nover the Internet is generally made difﬁcult or impossi-\nble by high latencies in data transmission [3] [5]. While\nmany composers and musicians have chosen to treat la-\ntency as a feature of network music, performance of con-\nventional music, such as that of classical repertoire, re-\nmains extremely difﬁcult in network scenarios. Audio la-\ntency frequently results in progressively decreasing tempo\nc\rBogdan Vera, Elaine Chew.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Bogdan Vera, Elaine Chew. “Towards\nSeamless Network Music Performance: Predicting an Ensemble’s Ex-\npressive Decisions for Distributed Performance”, 15th International So-\nciety for Music Information Retrieval Conference, 2014.and difﬁculty in synchronizing.\nOne aspect that has received less attention than the la-\ntency is the lack of visual contact when performing over\nthe internet. Visual cues can be transmitted via video, but\nsuch data is at least as slow as audio, and was previously\nfound to not be of signiﬁcant use for transmitting synchro-\nnization cues even when the audio had an acceptable la-\ntency [6].\nSince the start of network music research, several re-\nsearchers have posited theoretically that music prediction\ncould be the solution to network latency (see, for example,\nChafe [2]). Ideally, if the music can be predicted ahead of\ntime with sufﬁcient accuracy, then it can be replicated at\nall connected end-points with no apparent latency. Recent\nefforts have made limited progress towards this goal. One\nexample is a system for predicting tabla drumming pat-\nterns [12], and recent proposals by Alexandraki [1]. Both\nassume that the tempo of the piece will be at least locally\nsmooth and, in the case Alexandraki’s system, timing al-\nterations are always based on one reference recording.\nIn many styles of music, such as romantic classical mu-\nsic, the tempo can vary widely, with musicians interacting\non ﬁne-scale note-to-note timing changes and using visual\ncues to synchronize. The tempo cannot be expected to al-\nways evolve in the exact same way as one previous perfor-\nmance, rather the musicians signiﬁcantly improvise timing\ndeviations to some constraints.\nIn this paper we propose a system for predicting timing\nin network performance in real time, loosely inspired by\nRaphael’s approach based on Bayesian networks [11]. We\npropose and test a way to incorporate abstract notions of\nexpressive context within a probabilistic framework, mak-\ning use of time series clustering. Flossman et al. [8] em-\nployed similar ideas when they extended the YQX model\nfor expressive ofﬂine rendering of music by using condi-\ntional gaussian distributions to link expressive predictions\nover time. Our model contains an extra layer of stylistic\nabstraction and is applied to modeling and real-time track-\ning of one performer or ensemble’s expressive choice at\nthe inter-onset interval level. We also describe how the\nmethod could be used for predicting musical timing in net-\nwork performance, and discuss ideas for further work.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4892. MOTIVATION\nOur goal is to use observable sources of information during\na live performance to predict the timing of future notes so\nas to counter the effects of network latency. The sources\nof information we can use include the timing of previous\nnotes and the intensity with which the notes are played.\nThe core idea is reminiscent of Raphael’s approach to\nautomatic accompaniment [11], which uses a Bayesian net-\nwork relating note onset times, tempo and its change over\ntime. In Raphael’s model, changes in tempo and local\nnote timing are represented as independent gaussian vari-\nables, with distributions estimated from rehearsals. Dur-\ning a performance, the system generates an accompani-\nment that emulates the rehearsals by applying similar al-\nterations of timing and tempo at each note event in the per-\nformance. The model has been demonstrated in live per-\nformances and proven to be successful, however as long as\nthe system generates musically plausible expression in the\naccompaniment, it is difﬁcult to determine an error value,\nas it is simply meant to follow a musician and replicate a\nperformance style established in rehearsals. An underlying\nassumption of this statistical model is that the solo musi-\ncian leading the performance tends to perform the piece\nwith the same expressive style each time.\nIn an ensemble performance scenario, two-way com-\nmunication exists between musicians. The requirement for\nthe system to simply ‘follow’ is no longer enough. As a\nstep towards tighter ensemble, we set as a goal a stringent\naccuracy requirement for our prediction system: to have\nerrors small enough\u0000no higher than 20-40ms\u0000as to be in-\ndistinguishable from the normal ﬂuctuations in ensemble\nplaying. Note that actual playing may have higher errors,\neven in ideal conditions, due to occasional mistakes and\nﬂuctuations in motor control.\nThe same ensemble might also explore a variety of ways\nto perform a piece expressively. When expressive possibil-\nities are explored during rehearsals, the practices establish\na common ‘vocabulary’ for possible variations in timing\nthat the musicians can then anticipate. Another goal of our\nsystem is to account for several distinct ways of applying\nexpression to the same piece. This is accomplished in two\nways. Like Flossman et al. [8], we deliberately encode the\ncontext of the local expression by introducing dependen-\ncies between the expressive tempo changes at each time\nstep. We additionally propose and test a form of model\nselection using discrete variables that represent the chosen\nstylistic mode of the expression. For example, given two\nsamples exhibiting the same tempo change, one may be\npart of a longer term tempo increase, while another may\nbe part of an elastic time-stretching gesture. Knowing the\nstylistic context for a tempo change will allow us to better\npredict its trajectory.\n3. CONTEXTUALIZING TIMING PREDICTION\nWe combine two techniques to implement ensemble per-\nformance prediction. First, we condition the expressive\n‘update’ distributions characterizing temporal expressionon those from preceding events, making the timing changes\ndependent on both musicians’ previous timing choices, while\nalso allowing the system to respond to the interplay be-\ntween the two musicians. Secondly, we abstract different\nways of performing the piece by summarizing these larger\nscale differences in an unsupervised manner in a new dis-\ncrete node in the network: a stylistic cluster node.\n3.1 Linear Gaussian Conditional Timing Prediction\nOur goal is to predict the timing of events such as notes,\nchords, articulations, and rests. In particular, we wish to\ndetermine the time until the next event given the score in-\nformation and a timing model. We collapse all chords into\nsingle events. Assume that the performance evolves ac-\ncording to the following equations,\ntn+1 =snln+tn;and\nsn+1 =sn\u0001\u000en; (1)\nwheretnis the onset time of the n-th event,snis the corre-\nsponding inter-beat period, lnis the length of the event in\nbeats, and\u000enis a proportional change in beat duration that\nis drawn from the gaussian distributions \u0001n. For simplic-\nity, there is no distinction between tempo and local timing\nin our model, though it could be extended to include this\nseparation.\nBecause\u000en’s reﬂect proportional change in beat dura-\ntion, prediction of future beat durations are done on a log-\narithmic scale:\nlog2sn+1= log2sn+ log2\u000en:\nlog(tempo) = log(1=s n), thus logsnas well, has been\nshown in recent research to be a more consistent measure\nof tempo variation in expressive performance [4].\nThe parameters of the \u0001ndistributions are predicted\nduring the performance from previous observations, such\nas\u000en\u00001. Thus, each inter-beat interval, sn, is shaped from\nevent to event by the random changes, \u000en. The conditional\ndependencies between the random variables are illustrated\nin Figure 1. The ﬁrst and last layers in the network, labeled\nP1 and P2 in the diagram, are the observed onset times.\nThe 3rd layer, labeled ‘Composite’ following Raphael’s\nterminology, embodies the time and tempo information at\neach event, regardless of which ensemble musician is play-\ning, and it is on this layer that our model focuses. The 2nd\nlayer, Expression, consists of the variables \u0001n.\nThe\u0001nvariables are conditioned upon their predeces-\nsors, using any number of previous timing changes as in-\nput; formally, they are represented by linear conditional\ngaussian distributions [9]. Let there be a Bayesian network\nnode with a normal distribution Y. We can condition Y\non itskcontinuous parents C=fC1;:::;C kgand dis-\ncrete parents D=fD1;:::;D kgby using a linear regres-\nsion model to predict the mean and variance of Ygiven the\nvalues ofCandD. The following equation describes the\nconditional probability of Ygiven only continuous parent\nnodes:\nP(YjC=c) =N(\f0+kX\ni=1\fici;\u001b2):\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n490Figure 1: A section of the graphical model. Round nodes\nare continuous gaussian variables, and the square node (S )\nis a discrete stylistic cluster node.\nThis is the equation for both continuous and discrete par-\nents:\nP(YjD=d;C =c) =N(\fd;0+kX\nj=1\fd;jcj;\u001b2\nd):\nSimply speaking, the mean and variance of each lin-\near conditional gaussian node is calculated from the values\nof its continuous and discrete parent nodes. The mean is\nderived through linear regression from its continuous par-\nents’ values with one weight matrix per conﬁguration of its\ndiscrete parents.\nThe use of conditional gaussian distributions means that\nrather than having ﬁxed statistics for how the timing should\noccur at each point, the parameters for the timing distri-\nbutions are predicted in real time from previous observa-\ntions using linear regression. This simple linear relation-\nship provides a means of predicting the extent of temporal\nexpression as an ongoing gesture. For example, if the per-\nformance is slowing down, the model can capture the rate\nof slowdown, or a sharp tempo turnaround if this occurred\nduring rehearsals.\nOur network music approach involves interaction be-\ntween two actual musicians rather than a musician and a\ncomputer. Thus, each event observed is a ‘real’ event,\nand we update the \u0001nprobability distributions at each step\nduring run-time with the present actions of the musicians\nthemselves. Unlike a system playing in automatic accom-\npaniment or an expressive rendering system, our system is\nnever left to play on its own, and its task is simply to con-\ntinue from the musicians’ choices, leaving less opportunity\nfor errors to accumulate. Additionally, we can correct the\nmusicians’ intended timing by compensating for latency\npost-hoc - this implies that we can make predictions that\nemulate what the musicians would have done without the\ninterference of the latency.\nWe may also choose the number of previous changes to\nconsider. Experience shows that adding up to 3 previous\ninputs improves the performance moderately, but the per-\nformance decreases thereafter with more inputs. For sim-\nplicity, we currently use only one previous input, which\nprovides the most signiﬁcant step improvement.\nIn constrast to a similar approach by Flossman et al. [8],\nwe do not attempt to link score features to the performance;\nwe only consider the local context of their temporal ex-\npression. Our goal is to capture the essence of one partic-\nular ensemble’s interpretation of a particular piece ratherthan attempting to construct a universal model for mapping\nscore to performance. As a result, the amount of training\ndata will generally be much smaller as we may only use\nthe most recent recorded and annotated rehearsals of the\nensemble. The next section describes a clustering method\nwe use to account for large-scale differences in timing.\n3.2 Unsupervised Stylistic Characterization\nAlthough we could add a large number of previous inputs\nto each of the \u0001nnodes, we cannot tractably condition\nthese variables’ distributions on potentially hundreds of\nprevious observations. This would require a large amount\nof training data to estimate the parameters in a meaning-\nful way. Instead, we propose to summarize larger-scale\nexpression using a small number of discrete nodes repre-\nsenting the stylistic mode. For example, a musician may\nplay the same section of music in few distinct ways, and\na listener may describe it as ‘static’, ‘swingy’ or ‘loose’.\nIf these playing styles could be classiﬁed in real time, pre-\ndiction could be improved by considering this stylistic con-\ntext. Our ultimate goal is to perform this segmentally on a\npiece of music, discovering distinct stylistic choices that\noccured in the ensemble’s rehearsals. In this paper, we\npresent the ﬁrst steps towards this goal: we characterize\nthe style of the entire performance using a single discrete\nstylistic node.\nThe stylistic node is shown at the top of Figure 1. In our\nmodel this node links to all of the \u0001nnodes in the piece, so\nthat each of the \u0001n’s is now linearly dependent on the pre-\nvious timing changes with weights that are dependent on\nthe stylistic node. Assuming that each \u0001nnode is linked\nto one previous one, the parameters of the \u0001ndistributions\nare then predicted at run-time using\nP(\u0001tjS=s;\u0001t\u00001=\u000e) =N(\fs;0+\fs;1\u000e;\u001b2\ns);\nwhereSis the style node.\nTo predict note events, we can simply take the means of\nthe\u0001ndistributions, and use Equation 1 to ﬁnd the onset\ntime of the next event given the current one.\nTo use this model, we must ﬁrst discover the distinct\nways (if any) in which the rehearsing musicians perform\nthe piece. We apply k-means clustering to the log(\u000en)time\nseries obtained from each rehearsal. We ﬁnd the optimal\nnumber of clusters by using the Bayes Information Crite-\nrion (BIC) as described by Pelleg and Moore [10]. Note\nthat other methods exist for estimating an optimal number\nof clusters. To train the Bayesian network, a training set is\ngenerated containing all of the \u000envalues for each rehearsal\nas well as the cluster to which each time series is allocated.\nWe then use the algorithm by Murphy [9] to ﬁnd all the\nparameters of the linear conditional nodes. Note that all of\nthe nodes are observable and we have training data for the\n\u0001n.\nDuring the performance, the system can update its be-\nlief about the stylistic node’s value from the note timings\nthat have been observed at any point; we do not need to\nre-cluster the performance, as the network has learned the\nrelationships between the \u0001n’s and the stylistic node. We\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n491use the message passing algorithm of Bayesian networks to\ninfer the most likely state of the node. As the performance\nprogresses, the belief about the state of the node is gradu-\nally established. Intuitively, the system arrives at a stable\nanswer after some observations, otherwise the overall style\nis ambiguous. The state of the node is then used to place\nfuture predictions into some higher level context. The next\nsection shows that the prediction performance is improved\nby using the stylistic node to select the best regression pa-\nrameters to predict the subsequent timing changes, which\ncan be thought of as a form of model selection.\n4. EVALUATION\n4.1 Methodology\nIn this section we present an evaluation of the basic form of\nour model. Evaluation of such predictive models remains a\nchallenge because testing in live performance requires fur-\nther work on performance tracking and optimization, while\nofﬂine testing necessitates a large number of annotated per-\nformances from the same ensemble. We present initial re-\nsults on a small dataset; in our future work we will study\nreal time performances of more complex pieces.\nWe evaluate the performance of three models: one uses\nlinear conditional nodes and a stylistic cluster node; the\nsecond uses only linear conditional nodes; and, the third\nhas independent gaussian distributions for the \u0001variables.\nOur dataset consists of 20 performances by one pianist\nof the short custom-composed piece shown in Figure 2.\nNotice that we have not added any dynamics or tempo-\nrelated markings - the interpretation is left entirely to the\nmusicians. While this is not an ensemble piece, the perfor-\nmances are sufﬁcient to test the prediction accuracy of our\nmodel in various conditions. In this simple example, we\nconsider only the composite layer in the model, without\nP1 and P2.\nFigure 2: Custom-composed piano test piece.\nThe piece was played on an M-Audio AXIOM MIDI\nkeyboard in one of two expressive styles decided before-\nhand, ten times for each style. We used IRCAM’s An-\ntescofo score follower [7] for live tracking of the perfor-\nmance in our system, and annotation of the note and chord\nevents. The log-period plots for every performance in the\ndataset are shown in Figure 4a. The changes in log-period\nper event are shown in Figure 4b, and we also show the\nsame changes but for the data in each cluster found, to\ndemonstrate the difference between the two playing styles.We evaluated the system using a ‘leave-one-out’ approach,\nwhere out of the 20 performances we always trained on\n19 of them and tested on the remaining one. We always\nused one previous input to the \u0001nnodes, using the actual\nobservations in the performances rather than our predic-\ntions (like the extended YQX), simulating the process of\nlive performance. We evaluated the prediction accuracy by\nmeasuring timing errors, which we deﬁne as the absolute\ndifference between the true event times and those predicted\nby the model (in seconds).\nThe training performances were clustered correctly in\nall cases, dividing the dataset into the two styles, with the\nﬁrst 10 performances being grouped with cluster 1 and the\nsecond 10 becoming part of cluster 2. Figure 3 shows the\nstylistic inference process. In the matrix, performances are\narranged as rows, with events on the x-axis. Recall that we\npredict the time between events rather than just notes. So,\nwe also consider the timing of rests, and chords are com-\nbined into single events rather than individual notes. The\ncolors indicate the inferred value of the style node: grey\nfor Style 1 and white for Style 2. We see that the system\ncorrectly infers the stylistic cluster of each performance\nwithin the ﬁrst 19 events. In many cases the classiﬁcation\nassigns the performance to the correct cluster after only\ntwo events.\nInferred Style per Event, per Performance\nEvent NumberPerformance Number 1020304050607080905\n10\n15\n20\nFigure 3: Matrix showing most likely style state after each\nevent’s observed \u000e. Performances 1-10 are in Style 1, and\n11-20 are in Style 2. Classiﬁcation result: grey = Style 1,\nwhite = Style 2.\nFigure 4 shows the tempo information for the dataset.\nFigure 4(a) shows the inter-beat period contours of all of\nthe performances, while Figure 4(b) shows boxplots (indi-\ncating the mean and variability) of the period at each mu-\nsical event, for the entire dataset and for the two clusters.\n4.2 Results\nFigure 5a and Figure 5b show the performance of the mod-\nels, measured using mean absolute error averaged over events\nin each performance, and over performances for each event,\nrespectively. We also show a detailed ‘zoomed in’ plot of\nthe errors between events 20-84 to make the different mod-\nels’ mean errors clearer in Figure 5c. For network mu-\nsic performance, we would want to predict at least as far\nforward as needed to counter the network (and other sys-\ntem) latency. As some inter-event time differences may be\nshorter than the latency, we may occasionally need to pre-\ndict more than one event ahead.\nThe model with stylistic clustering and linear condi-\ntional nodes performed best, followed by the one with only\nlinear conditional nodes, then the model with independent\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4920 20 40 60 80 100−2−101Log Period per Event for all PerformancesLog2(Period)\nEvent Number(a) Log-period per event for every performance in the dataset.\n−202\nEvent NumberEvent−wise Log−Period Change Boxplots for the Unclustered DataLog2(Period)\n−202\nEvent NumberEvent−wise Log−Period Change + Boxplots for K−Means Centroid 1Log2(Period)\n−2−101\nEvent NumberEvent−wise Log−Period Change + Boxplots for K−Means Centroid 2Log2(Period)\n(b) Boxplots showing median and variability for the log-period\nchange at each event. Top: unclustered data, Middle: ﬁrst cen-\ntroid, Bottom: second centroid.\nFigure 4: Tempo Data\n\u0001nnodes. In all cases the errors were higher for the second\nstyle (the latter 10 performances), which was much looser\nthan the ﬁrst. The mean absolute errors for each model,\nconsidering all of the events in all of the performances are\nsummarized in Table 1.\nObserve in Figure 5b that some parts of the performance\nwere very difﬁcult to predict. For example, we note high\nprediction errors in the ﬁrst 12 events of the piece and one\nlarge spike in the error at the end of the piece. These are\n1-bar and 2-bar long chords, for which musicians in an en-\nsemble would have to use visual gestures or other informa-\ntion to synchronize. We would not expect any prediction\nsystem to do better than a musician anticipating the same\ntiming without any form of extra-musical information. We\ndiscuss potential applications of music prediction for vir-\ntual cueing in the next section. The use of clustering and\nconditional timing distributions reduced the error rate for\nthe events which were poorly predicted with independent\ntiming distributions. For much of the piece the mean error\nwas as low as 15ms, but even for these predictable parts\nof the performance, the models with conditional distribu-\ntions and clustering lowered the error, as can be seen from\nFigure 5c.\n5. CONCLUSIONS AND FUTURE WORK\nWe have outlined a novel approach to network music pre-\ndiction using a Bayesian network incorporating contextual\ninference and linear gaussian conditional distributions. In\nan evaluation comparing the model with stylistic cluster-\ning and linear conditional nodes, one with only linear con-\nditional nodes without clustering, and one with indepen-\n0 5 10 15 200.030.040.050.060.070.080.090.10.11\nPerformance NumberMean Absolute Error\n  \nStylistic Clustering + Conditional\nConditional Only\nNo Clustering, Not Conditional(a) Mean absolute error for each performance.\n0 20 40 60 80 10000.20.40.60.81\nEvent NumberMean Absolute Error\n  \nStylistic Clustering + Conditional\nConditional Only\nNo Clustering, Not Conditional\n(b) Mean absolute error per event, over the whole performance.\n20 30 40 50 60 70 800.010.020.030.04\nEvent NumberMean Absolute Error\n(c) A ‘zoomed-in’ view of the error rates between events 20-84.\nFigure 5: Mean absolute error per event.\ndent nodes, we have shown that the proposed approach\nproduces promising results. Speciﬁcally, we have shown\nevidence that considering a notion of large scale expressive\ncontext, drawn from performance styles of a particular en-\nsemble, can intuitively increase the accuracy of timing pre-\ndiction. The model remains to be tested on more data. As\ncreative musicians are inﬁnitely diverse in their expressive\ninterpretations, the true test of the model would ultimately\nbe in live performances.\nThe end goal of this research is to implement and evalu-\nate network music performance systems based on the pre-\ndiction model. Whether music prediction can ever be pre-\ncise enough to allow seamless network performance re-\nmains an open question. Important questions arise in pur-\nModel Mean Abs. Error\nIndependent 69.8ms\nConditional 57.4ms\nClustering and Conditional 48.5ms\nTable 1: Overall Timing Errors for Each Model\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n493suit of this goal: how much should the system lead the\nmusicians to help them stay in time without making the\nperformance artiﬁcial? Predicting musical timing with suf-\nﬁcient accuracy will open up interesting avenues for net-\nwork music research, especially when we consider parallel\nresearch into predicting other information such as inten-\nsity and even pitch information, but whether any musician\nwould truly want to let a machine impersonate them ex-\npressively remains to be seen, which is why we propose\nthat a ‘minimally-invasive’ conductor-like approach to reg-\nulating tempo would be more appropriate than complete\naudio prediction.\n5.1 The Bayesian Network\nIt would be straightforward to extend our model by imple-\nmenting prediction of timing from other forms of expres-\nsion that tend to correlate with tempo. For example, using\nevent loudness in the prediction would simply require the\naddition of another layer of variables in the Bayesian net-\nwork and conditioning the timing variables on these nodes\nas well.\n5.2 Capturing Style\nMuch work remains to expand on the characterization of\nstylistic mode. As previously mentioned, we plan to ex-\nplore segmental stylistic characterization, considering dif-\nferent contextual information for each part of the perfor-\nmance. In our current model we use only one stylistic\nnode. This may be a plausible for a small segment of mu-\nsic, but in a longer performance the choice of performance\nstyle may vary over time. If the predicted performance\nstarts within one style but changes to another, the model is\nill-informed to predict the parameters. In our future work\nwe would like to extend the model to capture such stylis-\ntic tendencies over time. One approach would require pre-\nsegmentation of the piece based on the choice of expressive\nchoices during the reharsal stage, and introduction of one\nstylistic node per segment. The prediction context would\nthen be local to each part of the performance. We may\nthen, for example, have causal conditional dependencies\nbetween the stylistic nodes in each segment of the piece,\nwhich would allow the system to both infer the style within\na part of the performance from what is being played and\nfrom the previous stylistic choices.\nIn practice, a musician or ensemble’s rehearsals may\nnot comprise of completely distinct interpretations; how-\never, capturing expression contextually will likely offer a\nlarger degree of freedom to the musicians in an internet\nperformance, who may then explore a greater variety of\ntemporal and other articulations.\n5.3 Virtual Cueing\nVirtual cueing forms an additional application of interest.\nAs mentioned at the start of the paper, visual communi-\ncation is generally absent or otherwise delayed in network\nmusic performance. If we could predict with reasonableaccuracy the timing in sections of a piece requiring tem-\nporal coordination, then we could help musicians synchro-\nnize by providing them with perfectly simultaneous pre-\ndicted cues. We regard the use of predictive virtual cues as\nless invasive to networked ensembles than complete pre-\ndictive soniﬁcation. In situations where the audio latency\nis low enough for performance to be feasible but video la-\ntency is still too high for effective transmission of gestural\ncues, predictive soniﬁcation may be omitted completely,\nand virtual cues could be implemented as a regulating fac-\ntor.\n6. ACKNOWLEDGEMENTS\nThis research was funded in part by the Engineering and\nPhysical Sciences Research Council.\n7. REFERENCES\n[1] C. Alexandraki and R. Bader. Using computer accompani-\nment to assist networked music performance. In Proc. of the\nAES 53rd Conference on Semantic Audio, London, UK, 2013.\n[2] C. Chafe. Tapping into the internet as an acoustical/musical\nmedium. Contemporary Music Review, 28, Issue 4:413–420,\n2010.\n[3] C. Chafe and M. Gurevich. Network time delay and ensemble\naccuracy: Effects of latency, asymmetry. In Proc. of the 117th\nAudio Engineering Society Convention, 2004.\n[4] E. Chew and C. Callender. Conceptual and experiential rep-\nresentations of tempo: Effects on expressive performance\ncomparisons. In Proc. of the 4th International Conference on\nMathematics and Compution in Music, pages 76–87, 2013.\n[5] E. Chew, A. Sawchuk, C. Tanoue, and R. Zimmermann. Seg-\nmental tempo analysis of performances in user-centered ex-\nperiments in the distributed immersive performance project.\nInProc. of the Sound and Music Computing Conference,\n2005.\n[6] E. Chew, R. Zimmermann, A. Sawchuk, C. Kyriakakis, and\nC. Papadopolous. Musical interaction at a distance: Dis-\ntributed immersive performance. In Proc. of the 4th Open\nWorkshop of MUSICNETWORK, Barcelona, 2004.\n[7] A. Cont. Antescofo: Anticipatory synchronization and con-\ntrol of interactive parameters in computer music. In Proc. of\nthe International Computer Music Conference, 2008.\n[8] S. Flossmann, M. Grachten, and G. Widmer. Guide to Com-\nputing for Expressive Music Performance, chapter Expressive\nPerformance Rendering with Probabilistic Models, pages 75–\n98. Springer Verlag, 2013.\n[9] K. P. Murphy. Fitting a conditional linear gaussian distribu-\ntion. Technical report, University of British Columbia, 1998.\n[10] D. Pelleg and A. W. Moore. X-means: Extending k-means\nwith efﬁcient estimation of the number of clusters. In Proc. of\nthe Seventeenth International Conference on Machine Learn-\ning, pages 727–734, 2000.\n[11] C. Raphael. Music plus one and machine learning. In Proc.\nof the 27th International Conference on Machine Learning,\npages 21–28, 2010.\n[12] M. Sarkar. Tablanet: a real-time online musical collaboration\nsystem for indian percussion. Master’s thesis, MIT, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n494"
    },
    {
        "title": "Robust Joint Alignment of Multiple Versions of a Piece of Music.",
        "author": [
            "Siying Wang 0001",
            "Sebastian Ewert",
            "Simon Dixon"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416382",
        "url": "https://doi.org/10.5281/zenodo.1416382",
        "ee": "https://zenodo.org/records/1416382/files/WangED14.pdf",
        "abstract": "Large music content libraries often comprise multiple ver- sions of a piece of music. To establish a link between dif- ferent versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be signif- icant such that even state-of-the-art methods fail to iden- tify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state- of-the-art method, with significantly less outliers (standard deviation 53% lower).",
        "zenodo_id": 1416382,
        "dblp_key": "conf/ismir/WangED14",
        "keywords": [
            "automatic music alignment",
            "multiple versions",
            "correct alignment",
            "robustness",
            "joint alignment",
            "comparison",
            "significantly less outliers",
            "standard deviation",
            "state-of-the-art method",
            "Mazurka Project"
        ],
        "content": "ROBUST JOINT ALIGNMENT OF\nMULTIPLE VERSIONS OF A PIECE OF MUSIC\nSiying Wang Sebastian Ewert\nQueen Mary University of London, UK\nfsiying.wang,s.ewert,s.e.dixong@qmul.ac.ukSimon Dixon\nABSTRACT\nLarge music content libraries often comprise multiple ver-\nsions of a piece of music. To establish a link between dif-\nferent versions, automatic music alignment methods map\neach position in one version to a corresponding position in\nanother version. Due to the leeway in interpreting a piece,\nany two versions can differ signiﬁcantly, for example, in\nterms of local tempo, articulation, or playing style. For\na given pair of versions, these differences can be signif-\nicant such that even state-of-the-art methods fail to iden-\ntify a correct alignment. In this paper, we present a novel\nmethod that increases the robustness for difﬁcult to align\ncases. Instead of aligning only pairs of versions as done in\nprevious methods, our method aligns multiple versions in a\njoint manner. This way, the alignment can be computed by\ncomparing each version not only with one but with several\nversions, which stabilizes the comparison and leads to an\nincrease in alignment robustness. Using recordings from\nthe Mazurka Project, the alignment error for our proposed\nmethod was 14% lower on average compared to a state-\nof-the-art method, with signiﬁcantly less outliers (standard\ndeviation 53% lower).\n1. INTRODUCTION\nRecent years have seen signiﬁcant efforts to create large,\ncomprehensive music collections. Music content providers\n(e.g. Spotify, iTunes, Pandora) rely on their existence,\nwhile national libraries and charitable organizations cre-\nate and curate them in order to provide access to cultural\nheritage. For a given piece of music, large collections of-\nten contain various related recordings (cover songs, dif-\nferent interpretations), videos (ofﬁcial clip, live concert)\nand musical scores (in different formats such as MIDI and\nMusicXML, covering several editions). To identify and\nlink these different versions, various automatic alignment\nmethods have been proposed in recent years. Such syn-\nchronization methods have been used to facilitate naviga-\ntion in large collections [1], to implement score following\nin real-time [2–5], to compare different interpretations of\nc\rSiying Wang, Sebastian Ewert, Simon Dixon.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Siying Wang, Sebastian Ewert, Si-\nmon Dixon. “Robust Joint Alignment of Multiple Versions of a Piece of\nMusic”, 15th International Society for Music Information Retrieval Con-\nference, 2014.a piece [6], to identify cover songs [7] or to simplify com-\nplex audio processing tasks [8].\nIn general, the goal of music synchronization is, given\na position in one version of a piece of music, to locate the\ncorresponding position in another version. To compute a\nsynchronization, existing methods align two versions of a\npiece at a time, even if several relevant versions are avail-\nable. For example, in [9, 10] a score of a piece is automat-\nically aligned to a corresponding audio recording, while\nin [11] two acoustic realizations are being synchronized.\nAs shown previously, current methods yield in many cases\nalignments of high accuracy [9–11]. However, musicians\ncan interpret a piece in diverse ways, which can lead to sig-\nniﬁcant local differences in terms of articulation and note\nlengths, ornamental notes, or the relative loudness of notes\n(balance). If such differences are substantial, the alignment\naccuracy of state-of-the-art methods can drop signiﬁcantly.\nTo increase alignment robustness for difﬁcult cases, the\nmain idea in this paper is to exploit the fact that multiple\nversions of a piece are often available and can be aligned in\na joint way. This way, we can exploit the additional infor-\nmation that each version provides about how a certain po-\nsition in a piece can be realized by a musician. As a conse-\nquence, while two given recordings might be rather differ-\nent and hard to align, both of them might actually be more\nsimilar to a third recording and including such a record-\ning within the alignment process can lead to an increase in\noverall robustness. To compute our joint synchronization,\nwe modify a multiple sequence alignment method typically\nemployed in biological signal processing and combine it\nwith strategies developed in a musical context based on\nMultiscale-DTW (FastDTW) and chroma-based onset fea-\ntures for increased computational efﬁciency and synchro-\nnization accuracy. In the following, we describe technical\ndetails of this method in Section 2. Then, we report on\nsome of our experiments in Section 3. Conclusions and\nprospects for future work are given in Section 4.\n2. ALIGNMENT METHOD\nVarious methods have been proposed to align two given\ndata sequences, including Dynamic Time Warping (DTW)\nand Hidden Markov Models (HMM) [2], Conditional Ran-\ndom Fields (CRF) [9], and Particle Filter / Monte-Carlo\nSampling (MCS) based methods [4,5]. With the exception\nof MCS methods, which are online methods, the remaining\nthree methods operate in an ofﬂine fashion and are quite\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n83Figure 1. Alignment of two interpretations of Chopin Op. 24\nNo. 2, measures 115-120: (a)Score for the six measures. (b)/(c)\nChroma features for an interpretation by Magin and Indjic, re-\nspectively; chroma features with uniform energy distribution are\nthe result of silence in the recording. (d)Alignment results for\nour baseline pairwise (gray) and proposed method (black).\nsimilar from an algorithmic point of view. We describe our\nproposed method as an extension to DTW. However, the\nunderlying ideas are applicable in HMM and CRF contexts\nas well.\n2.1 Baseline Pairwise Alignment\nTo summarize DTW-based alignment, let X:= (x 1; x2;\n: : : ; x N)andY:= (y1; y2; : : : ; y M)be two feature se-\nquences with xn; ym2F, whereFdenotes a suitable fea-\nture space. Furthermore, let c:F\u0002F! Rdenote a local\ncost measure onF. We deﬁne a resulting (N\u0002M)cost\nmatrix CbyC(n; m) := c(xn; ym). An alignment be-\ntween XandYis deﬁned as a sequence p= (p 1; : : : ; p L)\nwithp`= (n `; m`)2[1:N]\u0002[1 :M]for`2[1:L]sat-\nisfying 1 =n1\u0014n2\u0014: : :\u0014nL=Nand1 =m1\u0014\nm2\u0014: : :\u0014mL=M(boundary and monotonicity con-\ndition), as well as p`+1\u0000p`2f(1;0);(0;1);(1;1)g(step\nsize condition). An alignment phaving minimal total cost\namong all possible alignments is called an optimal align-\nment. To determine such an optimal alignment, one recur-\nsively computes an (N\u0002M)-matrix D, where the matrix\nentry D(n; m) is the total cost of the optimal alignment\nbetween (x1; : : : ; x n)and(y1; : : : ; y m):\nD(n; m) := min8\n><\n>:D(n\u00001; m\u00001) +w1C(n; m);\nD(n\u00001; m) +w2C(n; m);\nD(n; m\u00001) +w3C(n; m);\nforn; m > 1. Furthermore, D(n;1) :=Pn\nk=1w2C(k;1)\nforn > 1,D(1; m) =PM\nk=1w3C(1; k)form > 1, and\nD(1;1) := C(1;1). The weights (w1; w2; w3)2R3\n+\ncan be used to adjust the preference over the three step\nsizes. By tracking the choice for the minimum starting\nfromD(N; M )back to D(1;1), an optimal alignment can\nbe derived in a straightforward way [2]. In a musical con-\ntext,Ftypically denotes the space of normalized chroma\nfeatures, cis usually a cosine (or Euclidean) distance withweights set to (w1; w2; w3) = (2 ;1;1)to remove a bias for\nthe diagonal direction [2, 11].\nA main difﬁculty in aligning music stems from the de-\ngree of freedom a musician has in interpreting a score,\nin particular regarding the local tempo, balance (relative\nloudness of concurrent notes), articulation and playing style.\nIf several differences occur together, standard alignment\nmethods sometimes fail to identify the musically correct\nalignment. In Fig. 1(b)/(c), we see chroma features for\ntwo interpretations of Chopin Op. 24 No. 2 measures 115-\n120 (Fig. 1(a)) as performed by Magin and by Indjic, re-\nspectively. Besides the tempo, we see differences in the\ninterpretation of pauses (the uniform energy distributions\nin the features correspond to silence), articulation and in\nthe balance (relative loudness of notes). In this case, the\ndifferences are signiﬁcant such that pairwise DTW-based\napproaches [10, 11] fail to compute the correct alignment,\nsee upper path in Fig. 1(d). The red dots indicate corre-\nsponding beat positions in the two versions.\n2.2 Joint Alignment of Multiple Versions\nComparing several versions of a piece, interpretations vary\nin different ways and to different extents. If several ver-\nsions of a piece are available, each version provides an\nexample of how a speciﬁc position in a piece can be real-\nized, and this additional information can be used to stabi-\nlize the alignment for difﬁcult sections. A straightforward\nstrategy to compute a joint alignment could be to extend\nDTW to allow for more than two versions. For example,\nto align three versions, one can deﬁne an order-3 cost ten-\nsor in a straightforward way and apply the same dynamic\nprogramming techniques as used in DTW [12] (note that\na cost matrix for two versions is an order-2 tensor). How-\never, assuming that each feature sequence to be aligned is\nroughly of length N, the time and memory requirement to\nalign Krecordings would be in O(NK), which prohibits\nthe alignment of more than a very few recordings.\nIn computational biology, multiple sequence alignment\nis a well-studied problem. Most popular are so called proﬁle-\nbased methods andprogressive alignment methods [12].\nProﬁle-based methods employ a speciﬁc type of HMM,\nwhich is trained via Expectation-Maximization (EM) on\nthe set of feature sequences to be aligned. Each state of\nthe resulting proﬁle-HMM corresponds to a position in a\nso called average-sequence: the sequence of means of the\nobservation probabilities of the HMM-states, see [12] for\ndetails. A multiple synchronization is then computed by\naligning each sequence to the average-sequence via the\nViterbi algorithm. This procedure has been attempted in\na musical context with limited success [13]. We believe\nthis is due to, using EM training, whereby aligned features\nare essentially averaged (with Gaussian observation proba-\nbilites), which results in a loss of information and can lead\nto a loss of alignment accuracy.\nUsing progressive alignment such averaging is not nec-\nessary. The underlying idea is to successively build a data\nstructure referred to as a template, which provides efﬁcient\naccess to several aligned feature sequences, see Fig. 2(a).\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n84Figure 2. Progressive alignment: Three aligned chroma se-\nquences contained in the template (a)are compared to the chroma\nsequence (b). The resulting individual cost matrices (c)are\nmerged into one (d), which is used to compute the alignment.\nThe white lines in (a) and (c) indicate the positions of gap sym-\nbols.\nBy comparing a given feature sequence (Fig. 2(b)) to the\nsequences contained in the template, the alignment can be\ncomputed not only using one cost matrix (as in pairwise\nalignment) but several matrices in parallel - one for each\nsequence in the template (Fig. 2(c)). By suitably combin-\ning the information provided by each individual cost ma-\ntrix, the inﬂuence of strong local differences on the align-\nment, that often only occur between speciﬁc pairs of ver-\nsions, can be attenuated. As shown in Section 3, this can\nlead to a signiﬁcant boost in alignment robustness.\nTo describe this procedure in more detail, we assume\nthat we have Kdifferent versions of a piece and that their\nfeature sequences are denoted by Xk= (xk\n1; : : : ; xk\nNk)for\nk2[1:K]. In each step of the progressive alignment, the\ntemplate Zcontains several of these feature sequences that\nhave been stretched to have the same length. Initially, Z\nonly consists of X1. The remaining feature sequences are\nthen successively aligned to Z, and after each alignment\nZis updated by adding one more sequence. To this end,\nleteZ= (ez1; : : : ;ezeL)denote the current template which\ncontains k\u00001sequences of length eL(i.e. eachez`con-\ntainsk\u00001features), Xkthe sequence to be aligned, and\np= (p 1; : : : ; p L) =\u0000\n(n1; m1); : : : ; (nL; mL)\u0001\nan align-\nment between eZandXk. Intuitively, to add XktoeZ, we\nusepto stretcheZandXksuch that corresponding fea-\ntures are aligned and become part of the same element\nofZ. However, whenever features need to be copied to\ndo the stretching (step sizes (1;0)and(0;1)), we rather\ninsert a special gapsymbol instead of the features them-\nselves. More precisely, let Z= (z 1; : : : ; z L)denote the\nupdated template, zn(k)denote the k-th feature in the n-\nth element of Z, and Gdenote the gap symbol1. Set\nz1= (ez1(1); : : : ;ez1(k\u00001); xk\n1), then for l= (2;3; : : : ; L ):\nz`=8\n><\n>:(ezn`(1); : : : ;ezn`(k\u00001); xk\nm`); p `\u0000p`\u00001= (1;1)\n(ezn`(1); : : : ;ezn`(k\u00001); G); p `\u0000p`\u00001= (1;0)\n(G; : : : ; G; xk\nm`); p`\u0000p`\u00001= (0;1)\n1Since chroma features contain only non-negative entries, the gap\nsymbol can often be encoded as a pseudo-feature having negative entries.The gap symbol and its inﬂuence will be further discussed\nin Section 3.\nThe alignment procedure itself is almost identical to\nstandard DTW; only the local cost measure has to be ad-\njusted to take the properties of the template into account.\nFor a template Zcomprising k\u00001feature sequences and a\nfeature sequence X, we deﬁne a template-aware cost func-\ntioncT: (F[G)k\u00001\u0002F! Ras\ncT(zn; xm) =k\u00001X\nr=1(\nc(zn(r); xm); z n(r)6=G;\nCG; z n(r) =G;\nwhereCG>0is a constant referred to as the gap penalty.\nThe inﬂuence a single additional recording can have us-\ning progressive alignment is illustrated in Fig. 1(d). Here,\nwe included a third performance by Poblocka in the align-\nment, which could be considered as being “between” the\ntwo versions shown in Fig. 1 in terms of articulation style\nand balance. As we can see, the resulting path (black) fol-\nlows the ground-truth markings (red dots) quite closely and\nimproves signiﬁcantly over the pairwise result.\n2.3 Order of Alignments and Iterative Processing\nThe alignment of the ﬁrst two versions in our progressive\napproach is equivalent to standard pairwise alignment. Er-\nrors in this ﬁrst step inﬂuence to some degree all subse-\nquent alignment steps. We discuss now two strategies that\ncan help to increase the reliability of the ﬁrst few align-\nments in our progressive approach. First, the order in which\nthe alignments are computed is of importance, and we should\nstart with recordings that are easy to align. In computa-\ntional biology, a common approach to identify a reasonable\norder is referred to as the guide tree approach [12]. While\nthere are various ways to implement such an approach, we\nconsider the following procedure. First, for each pair of\nrecordings, we compute the total cost of an optimal align-\nment between the pair to identify the pair having the low-\nestaverage cost, which is deﬁned as the total cost of the\nalignment divided by its length L. We call the feature se-\nquences for the recordings in this pair X1andX2. For the\nnext recording, we identify the one being jointly closest to\nX1andX2. To this end, we sum for each of the remaining\nrecordings the average cost of the alignments between the\nrecording and X1, and the recording and X2. We call the\nfeature sequence of the recording with the lowest sum X3.\nWe continue with this procedure until all recordings are in\norder. We refer to this strategy as DTW-cost-based order.\nWhile this strategy leads to a useful order, its computa-\ntional costs are signiﬁcant. In our experiments, we found\nan alternative based on a much simpler strategy: We sorted\nthe versions according to their length, starting with the\nshortest recordings. In the following, we refer to this strat-\negy as length-based order. In Section 3, we compare both\nordering strategies and discuss their behavior.\nA second strategy to improve the reliability of the ﬁrst\nalignments is referred to as iterative progressive alignment.\nThe idea behind this strategy is, after all versions are aligned\nand included in the template, to remove one version from\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n85ID Piece No. Rec. No. Pairs\nM17-4 Opus 17 No. 4 62 1891\nM24-2 Opus 24 No. 2 62 1891\nM30-2 Opus 30 No. 2 34 561\nM63-3 Opus 63 No. 3 81 3240\nM68-3 Opus 68 No. 3 49 1176\nTable 1. Chopin Mazurkas and their identiﬁers used in our ex-\nperiments. The last two columns indicate the number of perfor-\nmances available for the respective piece and the number of eval-\nuated unique pairs.\nthe template and realign it, starting with the ﬁrst version\nthat was aligned. This way, errors made early in the pro-\ngressive alignment can potentially be corrected. We imple-\nmented this extension as well and discuss it in Section 3.\n2.4 Increasing the Computational Efﬁciency and\nAlignment Accuracy\nSince progressive alignment shares its algorithmic roots\nwith standard DTW, we can incorporate extensions that\nwere successfully used with DTW-based methods. In par-\nticular, the methods described in [10, 14] employ a variant\nof DTW referred to as multiscale DTW (FastDTW) to in-\ncrease the computational efﬁciency. The general idea is\nto recursively project an alignment computed at a coarse\nfeature resolution level to a next higher resolution, and to\nreﬁne the projected alignment on that resolution. This way,\nthe matrix Donly has to be evaluated around the projected\npath. This multiscale approach typically leads to a signiﬁ-\ncant drop in runtime by up to a factor of 30, see [14].\nFurthermore, the authors in [10] introduce a type of\nfeatures that indicate onset positions separately for each\nchroma. These chroma-based onset features (DLNCO fea-\ntures) are then combined with normalized chroma features.\nAs shown by the experiments in [10], these combined fea-\ntures can lead to a signiﬁcant increase in alignment accu-\nracy for pairwise methods. In the following, we employ\nthe same features and cost measure as used in [10].\n3. EXPERIMENTS\nTo illustrate the performance of our proposed method as\nwell as the inﬂuence of certain parameters, we conducted\na series of experiments using recordings taken from the\nMazurka Project2, which compiled a database of over 2700\nrecorded performances by more than 130distinct pianists\nfor49Mazurkas composed by Fr ´ed´eric Chopin. The record-\nings are dated between 1902 and today, and were made\nunder strongly varying recording conditions. For our ex-\nperiments, we employ a subset of ﬁve Mazurkas and 288\nrecordings, for which manually annotated beat positions\nare available, see Table 1. Performances with structural\ndifferences compared to the majority of recordings (such\nas additional repetitions of a part of a piece) were excluded\nfrom our experiments.\n2http://www.mazurka.org.uk3.1 Evaluation Measure\nTo evaluate the accuracy of an alignment between two dif-\nferent versions of a piece, we employ the beat annotations\nas ground truth. To this end, we use the alignment to lo-\ncate for each annotated beat position in the one version\na corresponding position in the other version. Using the\nmanual beat annotations for the other version, we can then\ncompute the absolute difference between the correct beat\nposition and the one obtained from the alignment. By av-\neraging these differences for all beats, we obtain the aver-\nage beat deviation (ABD) for a given alignment, which we\nmeasure in milli-seconds. For our evaluation, we compute\nthis measure for each Mazurka and each pair of recordings.\nFor example, for M17-4 our setup contains 62recordings,\nwhich results in\u000062\n2\u0001\n= 1891 unique pairs and correspond-\ning average beat deviation values, see Table 1.\n3.2 Pairwise vs Progressive Alignment\nIn a ﬁrst experiment, we compare the alignment accuracy\nfor pairwise and progressive alignment. Since the pair-\nwise method described in [10] employs the same features\nand cost measure as our proposed progressive method, we\nuse [10] as a baseline (other pairwise methods [11] showed\na similar behavior). In particular, we use a temporal res-\nolution of 20ms for both chroma and onset-indicator (DL-\nNCO) features. The DTW weights are set to (w1; w2; w3) =\n(2;1:5;1:5). As proposed in [10], we use the cosine dis-\ntance for the chroma features and the Euclidean distance\nfor the DLNCO features. Moreover, for our proposed pro-\ngressive alignment, we use the length-based alignment or-\nder and set the gap penalty parameter to the highest value\nthe cost measure ccan assume. The distribution of the av-\nerage beat deviation (ABD) values for all pairs is summa-\nrized for each of the ﬁve Mazurkas separately in the box-\nplots3shown in Fig. 3, as well as in column A and B in\nTable 2.\nComparing the results for pairwise and progressive align-\nment, we can see that the mean ABD drops slightly us-\ning the progressive approach for most examples. For ex-\nample, the mean ABD for M17-4 drops from 68ms using\npairwise alignment to 59ms using our progressive method\n(decrease by 13%). On average, the mean ABD drops by\n14%. More importantly though, the progressive alignment\nis signiﬁcantly more stable. In particular, the inter-quartile\nrange is smaller for all ﬁve Mazurkas using the progres-\nsive alignment (Fig. 3). Further, the number of alignments\nwith a very high ABD is signiﬁcantly reduced. This can\nbe measured by the standard deviation (std), which for\nM17-4 using pairwise alignment is 19ms, while progres-\nsive alignment leads to an std of 12ms. This difference is\neven greater for other Mazurkas (M24-2 and M63-3). On\naverage, the std is reduced by more than 50%. So over-\nall, while our proposed procedure also led to an increase in\n3We use standard boxplots: the red bar indicates the median, the blue\nbox gives the 25thand75thpercentiles ( p25andp75), the black bars\ncorrespond to the smallest data point greater than p25\u00001:5(p 75\u0000p25)\nand the largest data point less than p75+ 1:5(p 75\u0000p25), and the red\ncrosses are called outliers.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n86Figure 3. Comparison of the baseline pairwise alignment method with our proposed progressive alignment method. The boxplots\nillustrate the distribution of the average beat deviation values for each Mazurka separately.\nM17-4 [A]\n[B] [C] [D] [E] [F] [G]\nmin 15\n15 17 15 15 15 19\nmean 68 59 68 63 76 80 91\nmax 210 102 118 116 789 129 252\nstd 19 12 13 13 94 13 22\nM24-2\nmin 12\n15 17 12 15 16 11\nmean 39 31 38 33 31 46 56\nmax 311 68 118 59 68 98 320\nstd 20 6 12 7 6 9 22\nM30-2\nmin 7\n7 7 7 16 6 6\nmean 30 30 31 29 31 40 43\nmax 61 46 49 53 46 64 80\nstd 8 5 6 6 5 7 9\nM63-3\nmin 11\n13 15 12 13 14 9\nmean 46 40 46 40 40 53 62\nmax 1000 97 99 99 97 109 1000\nstd 32 11 12 11 11 11 33\nM68-3\nmin 14\n17 21 15 17 21 12\nmean 58 46 57 53 46 71 86\nmax 172 89 144 105 89 179 335\nstd 23 13 18 15 13 21 34\nTable\n2.Statistics over the average beat deviation (ABD) values\nfor the ﬁve Mazurkas and for 7 different alignment approaches\n(see text). [A]: Pairwise alignment. [B]: Proposed progressive\nalignment. [C]: Proposed without gap symbols. [D]: Proposed\nusing DTW-cost-based alignment order. [E]: Proposed using it-\nerative alignment. [F]: Proposed without DLNCO features. [G]:\nPairwise without DLNCO features. All values in milli-seconds.\nalignment accuracy on average, the main effect is a gain in\nrobustness against strongly incorrect alignments.\n3.3 Gap Penalties\nIn the next experiment, we investigate the inﬂuence of the\ngap penalty parameter by testing a slightly modiﬁed ver-\nsion of our proposed method. To this end, we modify\nthe way the template is creating by setting z`= (ezn`(1);\n: : : ;ezn`(k\u00001); xk\nm`)for`2[1:L], i.e. we do not insert\ngap symbols but copy features as necessary to create the\nnew template (comparing to Section 2.2). The results us-\ning this modiﬁcation are shown in column C in Table 2.\nComparing these values to our proposed method (column\nB) and the reference pairwise method (column A), we seethat this gap-less version typically improves over pairwise\nalignment in terms of maximum ABD values and the stan-\ndard deviation, just as the proposed method. For example,\nfor M17-4, the max ABD in column A is 210ms, while the\nmax ABD in column C is 118ms. However, we do not ob-\nserve a decrease in the mean ABD compared to pairwise\nalignment. For example, for M17-4, while using gaps the\nmean ABD drops from 68ms (column A) to 59ms (column\nB), it stays on a similar level in column C (68ms). The\nreason could be that by copying the features to create the\ntemplate, some temporal precision is lost and this results\nin a minor loss of alignment accuracy.\n3.4 Alignment Order\nNext, we investigate the inﬂuence of the order in which we\ncompute the progressive alignment, comparing the length-\nbased and the DTW-cost-based strategy (see Section 2.3).\nThe results are given in columns B and D of Table 2, re-\nspectively. As we can see, there are no signiﬁcant differ-\nences between both strategies. For example, for M17-4,\nthe mean ABD using the length-based strategy is 59ms\n(column B), while using the DTW-cost-based strategy the\nABD slightly increases to 63ms. The other statistical val-\nues show a similar behavior. Since these results do not\ndisclose any obvious advantages for the DTW-cost-based\nstrategy, we therefore propose to simply use the length-\nbased strategy. Interestingly, using the length-based strat-\negy but starting with the longest recordings led to worse\nresults.\nSince (local) tempo differences can usually be handled\nquite well using DTW, it is not obvious why sorting by\nlength yields a useful order. However, the fact that it does\ncould indicate that there might be a correlation between\nthe chosen tempo and other expressive parameters, such\nas articulation or balance, as strong differences in these\nparameters typically lead to difﬁculties for the alignment.\nFurthermore, the fact that according to our evaluation the\nshorter recordings were easier to align, could indicate that\na high tempo could limit the range of possible realizations\nof expressive parameters in a performance. However, fur-\nther studies would be necessary to conﬁrm such theories.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n873.5 Iterative Alignment\nIn a further experiment, we investigate whether iterative\nprocessing could further improve the alignment accuracy,\ncompare Section 2.3. To this end, we use two iterations:\nthe ﬁrst iteration corresponds to progressive alignment, and\nin the second iteration, each version is removed from the\ntemplate once and is then realigned. The results for this\nextension are given in column E of Table 2. Overall, the\niterative variant led to a slight decrease in ABD in almost\nall examples, which is not even visible in Table 2 as we\nrounded all values. On the contrary, we observed a signif-\nicant increase in ABD for M17-4 using the iterative vari-\nant. Here, the realignment led to a misalignment of several\nshorter recordings. Therefore, the results do not indicate\nany signiﬁcant advantages of using iterative alignment.\n3.6 Inﬂuence of Onset-Indicator Features\nIn a ﬁnal experiment, we investigate the inﬂuence of the\nchroma-based onset-indicator (DLNCO) features [10] on\nthe alignment accuracy when using progressive alignment.\nTo this end, we disabled the DLNCO features in our pro-\nposed method, and computed the alignment only based on\nthe normalized chroma features. The results of this exper-\niment are given in column F in Table 2. As a further ref-\nerence, we disabled the DLNCO features in our baseline\npairwise method as well (column G).\nAs we can see, the minimum over the ABD values re-\nmains unaffected for most of the Mazurkas, which means\nthat easy to align pairs can be aligned with chroma fea-\ntures alone just as well. For example, for M17-4, the mini-\nmum value in column F is identical to the one in column B.\nHowever, we see a signiﬁcant increase in ABD in all other\nstatistical values. For example, the mean ABD for M17-\n4 for our proposed method including DLNCO features is\n59ms (column B), while disabling the DLNCO leads to a\nmean ABD of 80ms (column F). Similar observations can\nbe made comparing the pairwise results. Overall, the re-\nsults seem to indicate that including onset-indicator fea-\ntures indeed leads to a signiﬁcant increase in alignment ac-\ncuracy also for progressive alignments.\n4. CONCLUSION\nIn this paper, we introduced a method for aligning mul-\ntiple versions of a piece of music in a joint way. The\navailability of multiple versions to compare against during\nthe alignment, stabilized the comparison for hard-to-align\nrecordings and led to an overall increase in alignment ac-\ncuracy and, in particular, in alignment robustness. Our ex-\nperiments using real-world recordings from the Mazurka\nProject demonstrated that our proposed method can indeed\nbe used to raise the alignment accuracy compared to pre-\nvious methods that are limited to pairwise alignments. For\nthe future, we plan to further investigate the behaviour of\nour procedure. In particular, we plan to analyze how other\nordering strategies inﬂuence the alignment accuracy. We\nwill also further explore different strategies to implement\na cost for the gap symbol and to make it more adaptive.Acknowledgements: This work was partly funded by the China\nScholarship Council (CSC), EPSRC Grant EP/J010375/1, and the\nQueen Mary Postgraduate Research Fund (PGRF).\n5. REFERENCES\n[1] M. M ¨uller, M. Clausen, V . Konz, S. Ewert, and C. Fremerey,\n“A multimodal way of experiencing and exploring music,”\nInterdisciplinary Science Reviews (ISR), vol. 35, no. 2, pp.\n138–153, 2010.\n[2] R. B. Dannenberg and C. Raphael, “Music score align-\nment and computer accompaniment,” Communications of the\nACM, Special Issue: Music Information Retrieval, vol. 49,\nno. 8, pp. 38–43, 2006.\n[3] A. Arzt, S. B ¨ock, S. Flossmann, H. Frostel, M. Gasser, and\nG. Widmer, “The complete classical music companion v0.9,”\ninProceedings of the AES International Conference on Se-\nmantic Audio, London, UK, 18–20 2014, pp. 133–137.\n[4] N. Montecchio and A. Cont, “A uniﬁed approach to real\ntime audio-to-score and audio-to-audio alignment using se-\nquential Montecarlo inference techniques,” in Proceedings of\nthe IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), Prague, Czech Republic, 2011,\npp. 193–196.\n[5] Z. Duan and B. Pardo, “A state space model for online poly-\nphonic audio-score alignment,” in Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), Prague, Czech Republic, 2011, pp.\n197–200.\n[6] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and A. Tobu-\ndic, “In search of the Horowitz factor,” AI Magazine, vol. 24,\nno. 3, pp. 111–130, 2003.\n[7] J. Serr `a, E. G ´omez, P. Herrera, and X. Serra, “Chroma binary\nsimilarity and local alignment applied to cover song identiﬁ-\ncation,” IEEE Transactions on Audio, Speech and Language\nProcessing, vol. 16, pp. 1138–1151, 2008.\n[8] S. Ewert, B. Pardo, M. M ¨uller, and M. D. Plumbley, “Score-\ninformed source separation for musical audio recordings: An\noverview,” IEEE Signal Processing Magazine, vol. 31, no. 3,\npp. 116–124, May 2014.\n[9] C. Joder, S. Essid, and G. Richard, “A conditional random\nﬁeld framework for robust and scalable audio-to-score match-\ning,” IEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 19, no. 8, pp. 2385–2397, 2011.\n[10] S. Ewert, M. M ¨uller, and P. Grosche, “High resolution audio\nsynchronization using chroma onset features,” in Proceedings\nof the IEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), Taipei, Taiwan, 2009, pp.\n1869–1872.\n[11] S. Dixon and G. Widmer, “MATCH: A music alignment tool\nchest,” in Proceedings of the International Conference on\nMusic Information Retrieval (ISMIR), London, GB, 2005, pp.\n492–497.\n[12] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, Bio-\nlogical Sequence Analysis : Probabilistic Models of Proteins\nand Nucleic Acids. New York, USA: Cambridge University\nPress, 1999.\n[13] H. I. Robertson, “Testing a new tool for alignment of musical\nrecordings,” Master’s thesis, McGill University, 2013.\n[14] M. M ¨uller, H. Mattes, and F. Kurth, “An efﬁcient multiscale\napproach to audio synchronization,” in Proceedings of the In-\nternational Conference on Music Information Retrieval (IS-\nMIR), Victoria, Canada, 2006, pp. 192–197.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n88"
    },
    {
        "title": "Automatic Set List Identification and Song Segmentation for Full-Length Concert Videos.",
        "author": [
            "Ju-Chiang Wang",
            "Ming-Chi Yen",
            "Yi-Hsuan Yang",
            "Hsin-Min Wang"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417361",
        "url": "https://doi.org/10.5281/zenodo.1417361",
        "ee": "https://zenodo.org/records/1417361/files/WangYYW14.pdf",
        "abstract": "Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural ques- tions that arise include “what is the set list?” and “when does each song begin and end?” Indeed, many full con- cert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typ- ically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research prob- lem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identifica- tion, musical event detection, music alignment, and struc- tural segmentation. Moreover, we propose a greedy ap- proach that sequentially identifies a song from a database of studio versions and simultaneously estimates its prob- able boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm.",
        "zenodo_id": 1417361,
        "dblp_key": "conf/ismir/WangYYW14",
        "keywords": [
            "full concert videos",
            "set list",
            "timecodes",
            "uploaders",
            "viewers",
            "metadata",
            "automated solution",
            "research problem",
            "music information retrieval",
            "audio fingerprinting"
        ],
        "content": "AUTOMATIC SET LIST IDENTIFICATION AND SONG SEGMENTATION\nFOR FULL-LENGTH CONCERT VIDEOS\nJu-Chiang Wang1;2, Ming-Chi Yen1, Yi-Hsuan Yang1, and Hsin-Min Wang1\n1Academia Sinica, Taipei, Taiwan\n2University of California, San Diego, CA, USA\nasriver.wang@gmail.com; fymchiqq, yang, whmg@iis.sinica.edu.tw\nABSTRACT\nRecently, plenty of full-length concert videos have become\navailable on video-sharing websites such as YouTube. As\neach video generally contains multiple songs, natural ques-\ntions that arise include “what is the set list?” and “when\ndoes each song begin and end?” Indeed, many full con-\ncert videos on YouTube contain song lists and timecodes\ncontributed by uploaders and viewers. However, newly\nuploaded content and videos of lesser-known artists typ-\nically lack this metadata. Manually labeling such metadata\nwould be labor-intensive, and thus an automated solution\nis desirable. In this paper, we deﬁne a novel research prob-\nlem, automatic set list segmentation of full concert videos,\nwhich calls for techniques in music information retrieval\n(MIR) such as audio ﬁngerprinting, cover song identiﬁca-\ntion, musical event detection, music alignment, and struc-\ntural segmentation. Moreover, we propose a greedy ap-\nproach that sequentially identiﬁes a song from a database\nof studio versions and simultaneously estimates its prob-\nable boundaries in the concert. We conduct preliminary\nevaluations on a collection of 20 full concerts and 1,152\nstudio tracks. Our result demonstrates the effectiveness of\nthe proposed greedy algorithm.\n1. INTRODUCTION\nIn recent years, the practice of sharing and watching con-\ncert/performance footage on video sharing websites such\nas YouTube has grown signiﬁcantly [12]. In particular,\nwe have noticed that many concert videos consist of full-\nlength, unabridged footage, featuring multiple songs. For\nexample, the query “full concert” on YouTube returns a list\nof more than 2 million relevant videos. Before watching a\nfull concert video, a viewer might like to know if the artist\nhas performed the viewer’s favorite songs, and when are\nthose song played in the video. Additionally, after watch-\ning a concert video, a viewer may want to know the song\ntitles in order to locate the studio version.\nc\rJu-Chiang Wang, Ming-Chi Yen, Yi-Hsuan Yang, and\nHsin-Min Wang.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Ju-Chiang Wang, Ming-Chi Yen, Yi-\nHsuan Yang, and Hsin-Min Wang. “Automatic Set List Identiﬁcation and\nSong Segmentation for Full-Length Concert Videos”, 15th International\nSociety for Music Information Retrieval Conference, 2014.To satisfy such a demand, the uploader or some viewers\noften post the “set list” with the timecode for each song,1\nso that other viewers can easily fast-forward to the de-\nsired song. This metadata can help viewers to navigate\na long concert. From a technical point of view, it also\nhelps to extract the live version of a song to enrich a music\ndatabase. Such a database could be used to analyze perfor-\nmance style, to discover song transition [17], to train clas-\nsiﬁers for visual event detection [28], or to generate multi-\ncamera mashups and summaries of concert videos [22,27].\nHowever, newly uploaded videos and those performed\nby less known artists typically lack this metadata, because\nmanually identifying songs and song segmentation can be\ntime consuming even for an expert. One reason for this is\nbecause live performances can differ substantially from the\nstudio recordings. Another reason is that live performances\noften contain covers of songs by other artists. Even if the\nannotator can readily identify all songs, it is still necessary\nto go through the entire video to locate the precise times\nthat each song begins and ends. Therefore, an automated\nmethod is desirable to annotate the rapidly growing volume\nof full-length concert videos available online.\nThe aim of this paper is threefold. First, we deﬁne a\nnovel research problem, i.e. automatic set list segmenta-\ntion of full concert videos, and discuss its challenges. Sec-\nond, we propose a greedy approach to tackle the problem.\nThird, we construct a novel dataset designed for this task\nand suggest several evaluation methods.\n1.1 Task Deﬁnition and Challenges\nThere are two sub-tasks for this research problem: set list\nidentiﬁcation andsong segmentation. Given a full concert\nvideo, the former is to identify the sequence of song titles\nplayed in the concert based on a large collection of stu-\ndio version tracks, assuming that no prior knowledge on\nthe live performance of the artist(s) of the concert is avail-\nable. The latter task is to estimate the boundaries of each\nidentiﬁed song in the set list. This problem poses some\ninteresting challenges as follows:\n\u000fA live song can be played in many different ways,\ne.g., by changing its timbre, tempo, pitch and struc-\nture, comparing to the corresponding studio version.\n1A set list refers to a list of songs that a band/artist has played in a con-\ncert, and the timecode corresponds to the starting time of a song. Here is\nan example of full concert video with set list and timecodes on YouTube:\nhttps://www.youtube.com/watch?v=qTOjiniIltQ\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n239Therefore, certain robustness should be considered.\n\u000fLive performances often feature transitions between\nconsecutive songs, or even repeated oscillations be-\ntween the sections of different songs, suggesting that\none should identify songs on a small temporal scale.\n\u000fConcerts often feature sections with no reference in\nthe collection of studio versions, such as intros, out-\nros, solos, banter, transitions between songs, big rock\nendings, and applause, amongst others. Unexpected\nevents such as broken instruments, sound system mal-\nfunctions, and interrupted songs can also be found.\nAn ideal system should identify them or mark them\nas unknown songs/events, avoiding including them\nin a segmented song when appropriate.\n\u000fThe artist may play cover songs from other artists\npartially or entirely throughout the concert, resulting\nin a much larger search space in the music database.\n\u000fThe audio quality of user-contributed concert videos\ncan vary signiﬁcantly due to recording factors such\nas acoustic environment, position, device and user\nexpertise [14]. The quality degradation can amplify\nthe difﬁculty of the problem.\nTo tackle the above challenges, one may consider tech-\nniques for several fundamental problems in music informa-\ntion retrieval (MIR), such as audio ﬁngerprinting/matching\n[3, 7], cover song identiﬁcation [5, 24], audio quality as-\nsessment [14], musical event detection/tracking [32, 33],\nand music signal alignment and segmentation [18]. There-\nfore, automatic set list segmentation of full concert videos\nmay present a new opportunity for MIR researchers to link\nmusic/audio technology to real-world applications.\n1.2 Technical Contribution\nOur technical contribution lies in the development of a\ngreedy approach that incorporates three components: seg-\nmentation, song identiﬁcation, and alignment (see Section\n3). This approach provides a basic view as a baseline to-\nwards future advance. Starting from the beginning of the\nconcert, our approach ﬁrst identiﬁes the candidate songs\nfor a “probe excerpt” of the concert based on segmented\nmusic signals. Then, it estimates the most likely song title\nand boundaries of the probe excerpt based on dynamic time\nwarping (DTW) [18]. This sequential process is repeated\nuntil the entire concert video has been processed. To evalu-\nate the proposed algorithm, we collect 20 full concerts and\n1,152 studio tracks from 10 artists (see Section 4). More-\nover, we suggest three performance metrics for this task\n(see Section 5). Finally, we demonstrate the effectiveness\nof the proposed approach and observe that cover song iden-\ntiﬁcation works much better than audio ﬁngerprinting for\nidentifying the songs in a live performance (see Section 5).\n2. RELATED WORK\nAccording to a recent user study, YouTube was the second\nmost preferred online music streaming service by users in\n2012, just behind Pandora [12]. These community-contri-\nbuted concert videos have been extensively studied in themultimedia community. Most existing works focus on han-\ndling the visual content of the concert videos [1,10,22,27,\n28]. Relatively little attention, however, has been paid in\nthe MIR community to study the audio content of this type\nof data. Related work mainly focused on low-level audio\nsignal processing for tasks such as audio ﬁngerprint-based\nsynchronization and alignment for concert video organiza-\ntion [9, 11, 29], and audio quality ranking for online con-\ncert videos [14]. More recently, Raﬁi et al. proposed a\nrobust audio ﬁngerprinting system to identify a live music\nfragment [23], without exploring full-length concert videos\nand song segmentation. To gain deeper understanding of\nthe content and context of live performance, our work rep-\nresents an early attempt to use the full concert video data.\nWe note that our work is also related to PHENICX [6],\nan ongoing project which aims at enriching the user experi-\nence of watching classical music concerts via state-of-the-\nart multimedia and Internet technologies. With a system\nfor automatic set list segmentation of full concert videos,\none could index a large amount of online musical content,\nextracting information that helps link live performance to\nthe associated video content.\nAside from potential applications, the technical devel-\nopment of our work is highly motivated by several sig-\nnal matching-based music retrieval problems, which can\nbe categorized into audio ﬁngerprinting (AF) [3, 30], au-\ndio matching [21], and cover song identiﬁcation (CSID) [5,\n24], according to their speciﬁcities andgranularity [4, 7].\nAn AF system retrieves the exact audio piece that is the\nsource of a query audio fragment. Audio matching is de-\nﬁned as the task of retrieving from a database all the audio\nfragments that are musically relevant to a query fragment.\nIn contrast, CSID aims at identifying different renditions\nof a music piece in the track level (instead of fragment-\nlevel). Unlike AF which usually holds robustness to any\nnoises that may apply on the same rendition of a song, au-\ndio matching and CSID should handle the musically moti-\nvated variations occurring in different performances or ar-\nrangements of a music piece [7].\n3. PROPOSED GREEDY APPROACH\nThe proposed approach is outlined in Algorithm 1. It em-\nploys an intuitive greedy strategy that recursively probes an\nexcerptXfrom the beginning of the unprocessed concert\nZ, identiﬁesKsong candidates (K = 5) from the studio\ndatabaseD, selects the most probable song title s?, esti-\nmates the boundaries (i;j)ofs?inX, and ﬁnally removes\ns?fromDandX(1 :j)fromZ. The process stops when\nthe unprocessed portion of the input concert is shorter than\na pre-deﬁned threshold \u001c. We make the following assump-\ntions while developing Algorithm 1: 1) the performer plays\nnearly the entire part of a song rather than a certain small\nportion of the song, 2) a song in the studio database is per-\nformed at most once in a concert, and 3) the concert con-\ntains only songs from the same artist without covers. In\npractice, the artist of a concert can be easily known from\nthe video title. Therefore, we only take the studio tracks of\nthe artist to construct D. More details are given below.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n240Algorithm 1: Set list identiﬁcation & segmentation\nInput: A concert Z; studio track database D; probe\nlengthl; end length \u001c; candidate number K;\nOutput: Song listS; boundary setB;\n1S ; ;B ;;\n2while length(Z )>\u001cdo\n3X Z(1 :l), ifl>length(Z ),l=length(Z );\n4fskgK\nk=1 identify theKmost probable songs\nthat matchX, based on the thumbnails of D;\n5fs?;(i;j)g select the best song from fskgK\nk=1\nand estimate its boundaries on X, based on the\ncomplete track ofD;\n6S S +s?;B B + (i;j );\n7D D\u0000s?;Z Z\u0000X(1 :j);\n8end\n3.1 Segmentation\nIn our original design, we adopt music segmentation tech-\nniques to pre-process both the concert and every studio\ntrack in the database. This enhances the robustness to vari-\nation of song structure for the music matching and identiﬁ-\ncation processes. However, operating on ﬁne-grained seg-\nments of the concert signiﬁcantly increases the computa-\ntional time of the algorithm. Therefore, we make heuristic\nmodiﬁcations to gain more efﬁciency as follows.\nFirst, we segment a sufﬁciently long probe excerpt from\nthe beginning of an unprocessed concert that could include\nthe ﬁrst entire song played in the unprocessed concert, with-\nout involving any musically motivated segmentation. Ide-\nally, we hope the probe length lis longer than the exact\nsongs?plus the events prior to s?(e.g., banter, applause).\nIn the experiment, we will compare different settings of\nl=\u000b\u0002\u0016, where\u000bis the parameter and \u0016the mean length\nof all studio tracks in the database.\nSecond, each studio track in the database is represented\nby its thumbnail for better efﬁciency in the later song can-\ndidate identiﬁcation stage. Similar idea has been intro-\nduced by Grosche et al. [8]. We develop a simple method\nanalogous to [15] based on structural segmentation. Seg-\nmentino [2, 16] is utilized to discover the musically homo-\ngeneous sections marked by structure labels such as ‘A,’\n‘B,’ and ‘N’ for each studio track. We compute a weighted\nfactor\rthat jointly considers the repetition count and aver-\nage segment length for each label. The longest segment of\nthe label that has the largest \ris selected as the thumbnail.\n3.2 Song Candidate Identiﬁcation\nSong candidate identiﬁcation uses the probe excerpt as a\nquery and ranks the studio thumbnails in the database. We\nemploy two strategies for the identiﬁer: audio ﬁngerprint-\ning (AF) and cover song identiﬁcation (CSID). For sim-\nplicity, we employ existing AF and CSID methods in this\nwork. For future work, it might be more interesting to inte-\ngrate the identiﬁer with the subsequent boundary estimator.\nFor AF, we implement the identiﬁer using the widely-\nknown landmark-based approach proposed in [31]. It ex-tracts prominent peaks (a.k.a. landmarks) from the mag-\nnitude spectrogram of a reference track (e.g. a studio ver-\nsion) and characterizes each pair of landmarks by the fre-\nquencies of the landmarks and the time in between them,\nwhich provide indices to a hash table that allows fast re-\ntrieval of similarity information [30]. For a query (e.g. a\nprobe excerpt), we see whether there are sufﬁcient num-\nber of matched landmarks between the query and a refer-\nence track by looking up the hash table. If the query track\nis a noisy version of the reference track, this approach is\nlikely to perform fairly well, because the landmarks are\nmost likely to be preserved in noise and distortion.\nFor CSID, we implement the identiﬁer mainly based on\nthechroma DCT-reduced log pitch (CRP) features [19] and\nthecross recurrence quantiﬁcation (CRQ) approach [25],\nwhich correspond to two major components in a state-of-\nthe-art CSID system [26]. Speciﬁcally, we ﬁrst extract the\nframe-based CRP features for the probe excerpt and each\nstudio track by the Chroma Toolbox [20]. Then, we deter-\nmine the key transposition using the optimal transposition\nindex (OTI) [25]. To apply CRQ, we follow the standard\nprocedures [25], including constructing the delay coordi-\nnate state space vectors, computing the cross recurrence\nplot, deriving the Qmaxscore, and performing normaliza-\ntion on the scores across the database. This CSID system\n(cf. CYWW1) has led to performance comparable to the\nstate-of-the-art systems in the MIREX audio cover song\nidentiﬁcation task (e.g., on Sapp’s Mazurka Collection).2\n3.3 Song Selection and Boundary Estimation\nThe next step is to select the most probable song k?from\nthe topKstudio song candidates, fYkgK\nk=1, and at the\nsame time estimate its boundaries on the probe excerpt X.\nAccordingly, our goal is to ﬁnd a Ykand the correspond-\ning subsequence X?=X(i?:j?)that results in the best\nmatching between YkandX?, where 1\u0014i?< j?\u0014N.\nSuch process is based on the DTW alignment between X\nand eachYk, as presented in Algorithm 2.\nLetX=fx1;:::;x Ngand denote the complete track\nofYkasY0=fy1;:::;y Mg, wherexiandyirepresent the\nframe-based CRP vectors and N > M . We compute the\ncost by the negative cosine similarity of CRP between two\nframes after the OTI key transposition. One can observe\nthat Algorithm 2 includes two sub-procedures of one-side\nboundary estimation (cf. Algorithm 3). It ﬁrst executes Al-\ngorithm 3 to search for the end boundary j0onXand then\nreverses the search from j0for the start boundary i0using\nAlgorithm 3 with the cost matrix rotated by 180 degrees.\nWe follow the standard procedure to compute the accumu-\nlated cost matrix Din [18]. Then, Algorithm 3 searches\nfromD(N\n2+ 1;M )toD(N;M )for the minimum aver-\nage cost of DTW alignments, denoted by \u000e?\nk, where the\naverage cost is deﬁned as the accumulated cost divided by\nthe length of its optimal warping path (OWP). The frame\nindex of\u000e?\nkis set as the boundary.\nAfter theKcandidates are processed, we pick the one\n2http://www.music-ir.org/mirex/wiki/2013:\nAudio_Cover_Song_Identification_Results\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n241Algorithm 2: Boundaries & average cost estimation\nInput: Concert excerpt X; a studio track Y0;\nOutput: Boundary pair (i0;j0); average cost \u000e;\n1C N-by-M cost matrix between XandY0;\n2(j0;;) one-side boundary estimation on C;\n3C rotateC(1 :j0;1 :M)by 180 degrees;\n4(index;\u000e ) one-side boundary estimation on C;\n5i0 j0\u0000index + 1;\nAlgorithm 3: One-side boundary estimation\nInput: Cost matrix C;\nOutput: Boundary \f; average cost \u000e;\n1D accumulated cost matrix from C(1;1);\n2for1 itoN\n2do\n3p? compute the OWP of D(1 :N\n2+i;1 :M);\n4 \u0001(i) D(N\n2+i;M)=length(p?);\n5end\n6(\u000e;index) the minimum value and its index of \u0001;\n7\f index +N\n2;\nwith the lowest average cost, k?= arg minkf\u000ekgK\nk=1, and\nset the boundary pair as (i0\nk?;j0\nk?). In other words, we re-\nrank the top Kcandidates according to the results of Algo-\nrithm 2, based on the content of the complete studio tracks.\n4. DATA COLLECTION\nWe collect 20 popular full concert videos (from the ﬁrst\nfew responses to the query “full concert” to Youtube) and\nthe associated set lists and timecodes from YouTube. There-\nfore, the music genre is dominated by pop/rock. We man-\nually label the start and end boundaries of each song based\non the timecodes, as a timecode typically corresponds to\nthe start time of a song and may not be always accurate.\nThere are 10 artists. For each artist, we collect as many\nstudio tracks as possible including the songs performed in\nthe collected concerts to form the studio database. On aver-\nage, we have 115.2 studio version tracks for each artist, and\neach full concert video contains 16.2 live version tracks.\nTable 1 summarizes the dataset.\n5. EV ALUATION\n5.1 Pilot Study on Set List Identiﬁcation\nWe conduct a pilot study to investigate which strategy (i.e.,\nAF or CSID) performs better for set list identiﬁcation, as-\nsuming that the song segmentation is perfect. For simplic-\nity, we extract all the songs from the concert videos ac-\ncording to the manually labeled boundaries and treat each\nentire live song as a query (instead of thumbnail). We use\nmean average precision (MAP) and precision@1 with re-\nspect to the studio database as the performance metrics.\nWe also perform random permutation ten times for each\nquery to generate a lower bound performance, denoted by\n‘Random.’ One can observe from Table 2 that CSID per-\nforms signiﬁcantly better than AF in our evaluation, show-ID Artist Name Concerts Studio Tracks\n1 Coldplay 2 96\n2 Maroon 5 3 62\n3 Linkin’ Park 4 68\n4 Muse 2 100\n5 Green Day 2 184\n6 Guns N’ Roses 2 75\n7 Metallica 1 136\n8 Bon Jovi 1 205\n9 The Cranberries 2 100\n10 Placebo 1 126\nTable 1. The full concert dataset.\nMethod MAP Precision@1\nAF 0.060 0.048\nCSID 0.915 0.904\nRandom 0.046 0.009\nTable 2. Result for live song identiﬁcation.\ning that the landmark-based AF approach does not work\nwell for live version identiﬁcation. This conﬁrms our intu-\nition as live rendition can be thought of as a cover version\nof the studio version [5]. In consequence, we use CSID as\nthe song candidate identiﬁer in the following experiments.\n5.2 Performance Metrics\nWe use the following performance metrics for set list iden-\ntiﬁcation and song segmentation: edit distance (ED), boun-\ndary deviation (BD), and frame accuracy (FA). The ﬁrst\nmetric ED is originally used to estimate the dissimilarity\nof two strings and has been adopted in numerous MIR\ntasks [13]. We compute the ED between an output song\nsequence (a list of song indices) and the ground truth coun-\nterpart via dynamic programming. The weights for inser-\ntion, deletion, and substitution are all set to 1. ED can only\nevaluate the accuracy of set list identiﬁcation.\nThe second metric BD directly measures the absolute\ndeviation in second between the estimated boundary and\nthat of the ground truth for only each correctly identiﬁed\nsong, ignoring those wrongly inserted songs in the output\nset list, as they are not presented in the ground truth. There-\nfore, the average BD of a concert reﬂects the accuracy of\nsong segmentation but not set list identiﬁcation.\nThe last metric, FA, which has been used in tasks such\nas melody extraction, represents the accuracy at the frame-\nlevel (using non-overlapped frame with length 200 ms).\nThroughout the concert, we mark the frames between the\nstart and end boundaries of each song by its song index and\notherwise by ‘x’ (belonging to no song). Then, we calcu-\nlate the percentage of correct frames (the intersection rate)\nby comparing the output frame sequence with the ground\ntruth counterpart. Therefore, FA can reﬂect the accuracy\nof both set list identiﬁcation and song segmentation.\n5.3 Baseline Approach\nTo study the effectiveness of the song selection and bound-\nary estimation algorithms (see Section 3.3), we construct a\nbaseline approach using Algorithm 1 without Algorithms\n2 and 3. Speciﬁcally, we select the song s?with the largest\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n242ID A SG SO ED[sBD[eBD[FA\n1 7 20 15 17 6.5 89.1 0.317\n2 3 17 17 4 3.3 12.3 0.786\n3 1 15 15 3 27.2 33.2 0.744\n4 8 23 25 14 8.8 66.8 0.441\n5 10 19 18 5 11.5 27.8 0.641\n6 6 10 11 1 19.1 22.8 0.875\n7 2 10 10 6 28.2 39.1 0.428\n8 3 22 22 9 28.2 39.6 0.610\n9 6 20 21 7 30.7 35.9 0.653\n10 9 17 15 4 5.3 9.8 0.758\n11 9 22 21 3 6 8.7 0.860\n12 4 17 19 7 32.0 21.9 0.681\n13 2 9 12 5 110 155 0.509\n14 1 17 17 2 20.1 18.4 0.777\n15 2 11 11 7 50.9 72.9 0.393\n16 3 17 20 9 36.9 24.7 0.544\n17 4 13 11 4 48.1 94.3 0.626\n18 3 23 22 10 10 34.8 0.636\n19 5 7 7 3 42.4 13.6 0.584\n20 5 15 13 9 42.4 36.6 0.465\nA VG(\u000b=1.5) 16.2 16.1 6.5 23.4 42.9 0.616\nA VG(\u000b=1.2) 16.2 18 7.3 25.7 57.3 0.582\nA VG(\u000b=1.8) 16.2 14.6 8.4 29.3 45.3 0.526\nBaseline 16.2 19.7 8.9 229 241 0.434\nTable 3. Result of the greedy approach with \u000b=1.5 for\nthe 20 full concerts and their average (A VG) performance.\nWhile ‘A VG (\u000b =1.2 or\u000b=1.8)’ only shows the average per-\nformance with different lsettings. ‘Baseline’ represents\nthe average performance of the approach in Section 5.3.\nAdditional abbreviations: A (Artist ID), SG (number of\nSongs in the Ground truth set list), SO (number of Songs\nin the Output set list), sBD (start BD), and eBD (end BD).\nSymbol[marks the metrics that are the smaller the better.\nCSID score on a probe excerpt. The start boundary is the\nstart point of the probe excerpt, and the end boundary is\nthe length(s?). Then, we begin the next probe excerpt on a\nhop of 0.1\u0002length(s?).\n5.4 Result and Discussion\nTable 3 shows the quantitative result of each concert, the\naverage performance (A VG) with different values of l, and\nthe average performance of Baseline. Figure 1 depicts\nthe qualitative results of three concerts, including the best,\nmedium, and the worst cases according to FA in Table 3.\nThe following observations can be made. First, the A VG\nperformances of the complete approach are signiﬁcantly\nbetter than those of Baseline in all metrics, demonstrat-\ning the effectiveness of Algorithms 2 and 3. Second, fur-\nther comparison among A VG performances with different\nlsettings shows that \u000b=1.5 performs the best, revealing\nthat live versions are likely longer than studio ones, but\noverly large lcould yield more deletions, as observed by\nthe smaller SO of \u000b=1.8. Third, on average our approach\ngives similar number of songs of a concert as that of ground\ntruth (16.1 versus 16.2). Fourth, we ﬁnd an interesting\nlinkage between the result and the style of the live perfor-\nmance. For example, we ﬁnd that our approach performed\npoorly for ‘Maroon 5’ (A=2) and ‘Metallica’ (A=7). As\ncan be observed from the last two rows of Figure 1, Ma-roon 5 tends to introduce several non-song sections such\nas jam and banter, which cannot be accurately modeled by\nour approach. They also like to make the live renditions\ndifferent from their studio versions. On the other hand,\nwe conjecture that the riffs in the heavy metal music such\nas Metallica may be the main reason degrading the per-\nformance of matching thumbnails by CSID, because such\nriffs lack long-term harmonic progressions. Fifth, the per-\nformance for ‘Bon Jovi’ (A=8) is poor, possibly because of\nthe relatively large quantity of studio tracks in the search\nspace. Finally, owing to possible big rock endings or repet-\nitive chorus in the live performance, our approach rela-\ntively cannot estimate accurate end boundary of the songs\nin a concert, as reﬂected by larger eBD than sBD. Our ap-\nproach sometimes insert songs that are relatively short in\nlength, as can be observed in Figure 1. The above two\nobservations suggest that advanced methods (over Algo-\nrithm 3) for boundary estimation and regularizing the song\nlength might be needed.\nIn short, while there is still much room for improve-\nment, we ﬁnd that the result of the proposed greedy ap-\nproach is quite satisfactory in some cases (e.g., Concert 6\nin Figure 1). The greedy approach is preliminary in nature.\nWe believe that better result can be obtained by explicitly\naddressing the challenges described in Section 1.1.\n6. CONCLUSION AND FUTURE DIRECTION\nIn this paper, we have proposed a novel MIR research prob-\nlem with a new dataset and a new greedy approach to ad-\ndress the problem. We have also validated the effectiveness\nof the proposed approach via both quantitative and quali-\ntative results. We are currently expanding the size of the\ndataset and conducting more in-depth signal-level analy-\nsis of the dataset. Due to the copyright issue on the studio\ntrack collection, however, it is not likely to distribute the\ndataset. We will propose this task to MIREX to call for\nmore advanced approaches to tackle this problem.\n7. ACKNOWLEDGEMENT\nThis work was supported by Academia Sinica–UCSD Post-\ndoctoral Fellowship to Ju-Chiang Wang, and the Ministry\nof Science and Technology of Taiwan under Grants NSC\n101-2221-E-001-019-MY3 and 102-2221-E-001-004-MY3.\n8. REFERENCES\n[1] A. Bagri, F. Thudor, A. Ozerov, and P. Hellier. A scal-\nable framework for joint clustering and synchronizing multi-\ncamera videos. In Proc. EUSIPCO, 2013.\n[2] C. Cannam et al. MIREX 2013 entry: Vamp plugins from\nthe centre for digital music. In MIREX, 2013.\n[3] P. Cano et al. A review of audio ﬁngerprinting. J. Sign.\nProcess. Syst., 41(3):271–284, 2005.\n[4] M. A. Casey et al. Content-based music information retrieval:\nCurrent directions and future challenges. Proceedings of the\nIEEE, 96(4):668–696, 2008.\n[5] D. PW Ellis and G. E Poliner. Identifying ‘cover songs’ with\nchroma features and dynamic programming beat tracking. In\nProc. ICASSP, pages IV–1429, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n243The ground truth of Concert 6\n31 38 46 64 39 70 43 47 56 32\nThe output result of Concert 6\n31 38 46 64 39 70 43 47 56 32 58\nThe ground truth of Concert 8\n43 20 54 38 44 46 64 21 35 22 32 42 57 5 52 11 29 48 65 42 16 4\nThe output result of Concert 8\n43 20 54 38 44 46 61 21 30 42 57 5 52 11 29 56 36 25 16 4 6 33\nThe ground truth of Concert 1\n51 66 86 41 48 115 72 106 76 75 12432 134 117 52 89 35 14 77 90\nThe output result of Concert 1\n95 5 88 48 94 114 75 57 89 35 112 108 77 120Figure 1. Qualitative result of three concerts, which represent the best (Concert 6, ‘Guns N’ Roses’), medium (Concert\n8, ‘Linkin’ Park’), and worst (Concert 1, ‘Metallica’) output cases in the dataset. Black blocks correspond to no song.\nDifferent songs are marked by different colors. The number in a song block stands for the song index in the studio database.\nNote that Song 42 (‘Numb’) was sung twice in Concert 8, ﬁrstly by ‘Linkin’ Park’ and then by ‘featuring Jay-Z.’\n[6] E. G ´omez et al. PHENICX: Performances as highly enriched\nand interactive concert experiences. In Proc. SMC, 2013.\n[7] P. Grosche, M. M ¨uller, and J. Serr `a. Audio content-based\nmusic retrieval. Multimodal Music Processing, 3:157–174,\n2012.\n[8] P. Grosche, M. M ¨uller, and J. Serr `a. Towards cover group\nthumbnailing. In Proc. ACM MM, pages 613–616, 2013.\n[9] M. Guggenberger, M. Lux, and L. Boszormenyi. AudioAlign\n- Synchronization of A/V-streams based on audio data. In\nProc. IEEE ISM, pages 382–383, 2012.\n[10] L. Guimar ˜aes et al. Creating personalized memories from so-\ncial events: community-based support for multi-camera record-\nings of school concerts. In Proc. ACM MM, 2011.\n[11] L. Kennedy and M. Naaman. Less talk, more rock: Auto-\nmated organization of community-contributed collections of\nconcert videos. In Proc. WWW, pages 311–320, 2009.\n[12] J. H. Lee and N. M. Waterman. Understanding user require-\nments for music information services. In Proc. ISMIR, 2012.\n[13] Kjell Lemstr ¨om. String matching techniques for music re-\ntrieval. Ph.D. Thesis, University of Helsinki, 2000.\n[14] Z. Li, J.-C. Wang, J. Cai, Z. Duan, H.-M. Wang, and Y . Wang.\nNon-reference audio quality assessment for online live music\nrecordings. In Proc. ACM MM, pages 63–72, 2013.\n[15] B. Martin, P. Hanna, M. Robine, and P. Ferraro. Indexing\nmusical pieces using their major repetition. In Proc. JCDL,\npages 153–156, 2011.\n[16] M. Mauch, K. Noland, and S. Dixon. Using musical structure\nto enhance automatic chord transcription. In Proc. ISMIR,\npages 231–236, 2009.\n[17] B. McFee and G. Lanckriet. The natural language of playlists.\nInProc. ISMIR, pages 537–542, 2011.\n[18] M. M ¨uller. Information retrieval for music and motion. 2007.\n[19] M. Muller and S. Ewert. Towards timbre-invariant audio fea-\ntures for harmony-based music. IEEE Trans. Audio, Speech,\nand Lang. Process., 18(3):649–662, 2010.[20] M. M ¨uller and S. Ewert. Chroma Toolbox: MATLAB im-\nplementations for extracting variants of chroma-based audio\nfeatures. In Proc. ISMIR, pages 215–220, 2011.\n[21] M. M ¨uller, F. Kurth, and M. Clausen. Audio matching via\nchroma-based statistical features. In Proc. ISMIR, 2005.\n[22] S. U. Naci and A. Hanjalic. Intelligent browsing of concert\nvideos. In Proc. ACM MM, pages 150–151, 2007.\n[23] Z. Raﬁi, B. Coover, and J. Han. An audio ﬁngerprinting\nsystem for live version identiﬁcation using image processing\ntechniques. In Proc. ICASSP, pages 644–648, 2014.\n[24] J. Serra, E. G ´omez, and P. Herrera. Audio cover song identi-\nﬁcation and similarity: background, approaches, evaluation,\nand beyond. In Advances in Music Information Retrieval,\npages 307–332. 2010.\n[25] J. Serra, X. Serra, and R. G Andrzejak. Cross recurrence\nquantiﬁcation for cover song identiﬁcation. New Journal of\nPhysics, 11(9):093017, 2009.\n[26] J. Serr `a, M. Zanin, and R. G Andrzejak. Cover song retrieval\nby cross recurrence quantiﬁcation and unsupervised set de-\ntection. In MIREX, 2009.\n[27] P. Shrestha et al. Automatic mashup generation from multiple-\ncamera concert recordings. In Proc. ACM MM, pages 541–\n550, 2010.\n[28] C. GM Snoek et al. The role of visual content and style for\nconcert video indexing. In Proc. ICME, 2007.\n[29] K. Su, M. Naaman, A. Gurjar, M. Patel, and D. PW Ellis.\nMaking a scene: alignment of complete sets of clips based on\npairwise audio match. In Proc. ICMR, page 26, 2012.\n[30] A. Wang. An industrial strength audio search algorithm. In\nProc. ISMIR, pages 7–13, 2003.\n[31] C.-C. Wang, J.-S. R. Jang, and W. Li. Speeding up audio\nﬁngerprinting over GPUs. In Proc. ICALIP, 2014.\n[32] J.-C. Wang, H.-M. Wang, and S.-K. Jeng. Playing with tag-\nging: A real-time tagging music player. In ICASSP, 2012.\n[33] S.-Y . Wang, J.-C. Wang, Y .-H. Yang, and H.-M. Wang. To-\nwards time-varying music auto-tagging based on CAL500 ex-\npansion. In Proc. ICME, 2014.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n244"
    },
    {
        "title": "Emotional Predisposition of Musical Instrument Timbres with Static Spectra.",
        "author": [
            "Bin Wu 0013",
            "Andrew Horner",
            "Chung Lee"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417153",
        "url": "https://doi.org/10.5281/zenodo.1417153",
        "ee": "https://zenodo.org/records/1417153/files/WuHL14.pdf",
        "abstract": "Music is one of the strongest triggers of emotions. Re- cent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown sig- nificant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted follow- up listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition.",
        "zenodo_id": 1417153,
        "dblp_key": "conf/ismir/WuHL14",
        "keywords": [
            "emotions",
            "music",
            "timbres",
            "spectral centroid",
            "harmonic ratio",
            "brightness",
            "static tones",
            "emotional predisposition",
            "even/odd harmonic ratio",
            "spectral shape"
        ],
        "content": "EMOTIONAL PREDISPOSITION OF MUSICAL INSTRUMENT TIMBRES\nWITH STATIC SPECTRA\nBin Wu\nDepartment of Computer\nScience and Engineering\nHong Kong University\nof Science and Technology\nHong Kong\nbwuaa@cse.ust.hkAndrew Horner\nDepartment of Computer\nScience and Engineering\nHong Kong University\nof Science and Technology\nHong Kong\nhorner@cse.ust.hkChung Lee\nThe Information Systems\nTechnology and Design Pillar\nSingapore University\nof Technology and Design\n20 Dover Drive\nSingapore 138682\nim.lee.chung@gmail.com\nABSTRACT\nMusic is one of the strongest triggers of emotions. Re-\ncent studies have shown strong emotional predispositions\nfor musical instrument timbres. They have also shown sig-\nniﬁcant correlations between spectral centroid and many\nemotions. Our recent study on spectral centroid-equalized\ntones further suggested that the even/odd harmonic ratio is\na salient timbral feature after attack time and brightness.\nThe emergence of the even/odd harmonic ratio motivated\nus to go a step further: to see whether the spectral shape\nof musical instruments alone can have a strong emotional\npredisposition. To address this issue, we conducted follow-\nup listening tests of static tones. The results showed that\nthe even/odd harmonic ratio again signiﬁcantly correlated\nwith most emotions, consistent with the theory that static\nspectral shapes have a strong emotional predisposition.\n1. INTRODUCTION\nMusic is one of the most effective media for conveying\nemotion. A lot of work has been done on emotion recog-\nnition in music, especially addressing melody [4], har-\nmony [18], rhythm [23, 25], lyrics [15], and localization\ncues [11].\nSome recent studies have shown that emotion is also\nclosely related to timbre. Scherer and Oshinsky found\nthat timbre is a salient factor in the rating of synthetic\ntones [24]. Peretz et al. showed that timbre speeds up\ndiscrimination of emotion categories [22]. Bigand et al.\nreported similar results in their study of emotion similari-\nties between one-second musical excerpts [7]. It was also\nfound that timbre is essential to musical genre recognition\nand discrimination [3, 5, 27].\nEven more relevant to the current study, Eerola carried\nc\rBin Wu, Andrew Horner, Chung Lee.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Bin Wu, Andrew Horner, Chung\nLee. “Emotional Predisposition of Musical Instrument Timbres with\nStatic Spectra”, 15th International Society for Music Information Re-\ntrieval Conference, 2014.out listening tests to investigate the correlation of emotion\nwith temporal and spectral sound features [10]. The study\nconﬁrmed strong correlations between features such as at-\ntack time and brightness and the emotion dimensions va-\nlence and arousal for one-second isolated instrument tones.\nValence and arousal are measures of how pleasant and en-\nergetic the music sounds [31]. Asutay et al. also studied\nvalence and arousal responses to 18 environmental sounds\n[2]. Despite the widespread use of valence and arousal\nin music research, composers may ﬁnd them rather vague\nand difﬁcult to interpret for composition and arrangement,\nand limited in emotional nuance. Using a different ap-\nproach than Eerola, Ellermeier et al. investigated the un-\npleasantness of environmental sounds using paired com-\nparisons [12].\nRecently, we investigated the correlations between\nemotion and timbral features [30]. In our previous study,\nlistening test subjects compared tones in terms of emotion\ncategories such as Happy and Sad. We equalized the stim-\nuli attacks and decays so that temporal features would not\nbe factors. This modiﬁcation isolated the effects of spectral\nfeatures such as spectral centroid. Average spectral cen-\ntroid signiﬁcantly correlated for all emotions, and spectral\ncentroid deviation signiﬁcantly correlated for all emotions.\nThis correlation was even stronger than average spectral\ncentroid for most emotions. The only other correlation was\nspectral incoherence for a few emotions.\nHowever, since average spectral centroid and spectral\ncentroid deviation were so strong, listeners did not notice\nother spectral features much. This raised the question: if\naverage spectral centroid was equalized in the tones, would\nspectral incoherence be more signiﬁcant? Would other\nspectral characteristics emerge as signiﬁcant? We tested\nthis idea on spectral centroid normalized tones, and found\nthat even/odd harmonic ratio was signiﬁcant. This made\nus even more curious: if musical instruments tones only\ndiffered from one another in their spectral shapes, would\nthey still have strong emotional predispositions? To an-\nswer this question, we conducted the follow-up experiment\ndescribed in this paper using emotion responses for static\nspectra tones.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2532. LISTENING TEST\nIn our listening test, listeners compared pairs of eight in-\nstruments for eight emotions, using tones that were equal-\nized for attack, decay, and spectral centroid.\n2.1 Stimuli\n2.1.1 Prototype instrument sounds\nThe stimuli consisted of eight sustained wind and bowed\nstring instrument tones: bassoon (Bs), clarinet (Cl), ﬂute\n(Fl), horn (Hn), oboe (Ob), saxophone (Sx), trumpet (Tp),\nand violin (Vn). They were obtained from the McGill and\nProsonus sample libraries, except for the trumpet, which\nhad been recorded at the University of Illinois at Urbana-\nChampaign School of Music. The original of all these\ntones were used in a discrimination test carried out by\nHorner et al. [14], six of them were also used by McAdams\net al. [20], and all of them used in our emotion-timbre\ntest [30].\nThe tones were presented in their entirety. The tones\nwere nearly harmonic and had fundamental frequencies\nclose to 311.1 Hz (Eb4). The original fundamental fre-\nquencies deviated by up to 1 Hz (6 cents), and were syn-\nthesized by additive synthesis at 311.1 Hz.\nSince loudness is potential factor in emotion, amplitude\nmultipliers were determined by the Moore-Glasberg loud-\nness program [21] to equalize loudness. Starting from a\nvalue of 1.0, an iterative procedure adjusted an amplitude\nmultiplier until a standard loudness of 87:3\u00060:1phons\nwas achieved.\n2.2 Stimuli Analysis and Synthesis\n2.2.1 Spectral Analysis Method\nInstrument tones were analyzed using a phase-vocoder al-\ngorithm, which is different from most in that bin frequen-\ncies are aligned with the signal’s harmonics (to obtain ac-\ncurate harmonic amplitudes and optimize time resolution)\n[6]. The analysis method yields frequency deviations be-\ntween harmonics of the analysis frequency and the corre-\nsponding frequencies of the input signal. The deviations\nare approximately harmonic relative to the fundamental\nand within\u00062% of the corresponding harmonics of the\nanalysis frequency. More details on the analysis process\nare given by Beauchamp [6].\n2.2.2 Spectral Centroid Equalization\nDifferent from our previous study [30], the average spec-\ntral centroid of the stimuli was equalized for all eight in-\nstruments. The spectra of each instrument was modiﬁed to\nan average spectral centroid of 3.7, which was the mean\naverage spectral centroid of the eight tones. This modiﬁ-\ncation was accomplished by scaling each harmonic ampli-\ntude by its harmonic number raised to a to-be-determined\npower:\nAk(t) kpAk(t) (1)For each tone, starting with p= 0,pwas iterated using\nNewton’s method until an average spectral centroid was\nobtained within\u00060:1 of the 3.7 target value.\n2.2.3 Static Tone Preparation\nThe static tones were 0.5s in duration and were generated\nusing the average steady-state spectrum of each spectral\ncentroid equalized tone with linear 0.05s attacks and de-\ncays, and 0.4 sustains.\n2.2.4 Resynthesis Method\nStimuli were resynthesized from the time-varying har-\nmonic data using the well-known method of time-varying\nadditive sinewave synthesis (oscillator method) [6] with\nfrequency deviations set to zero.\n2.3 Subjects\n32 subjects without hearing problems were hired to take\nthe listening test. They were undergraduate students and\nranged in age from 19 to 24. Half of them had music train-\ning (that is, at least ﬁve years of practice on an instrument).\n2.4 Emotion Categories\nAs in our previous study [30], the subjects compared the\nstimuli in terms of eight emotion categories: Happy, Sad,\nHeroic, Scary, Comic, Shy, Joyful, and Depressed.\n2.5 Listening Test Design\nEvery subject made pairwise comparisons of all eight in-\nstruments. During each trial, subjects heard a pair of tones\nfrom different instruments and were prompted to choose\nwhich tone more strongly aroused a given emotion. Each\ncombination of two different instruments was presented in\nfour trials for each emotion, and the listening test totaled\nC8\n2\u00024\u00028 = 896 trials. For each emotion, the overall\ntrial presentation order was randomized (i.e., all the Happy\ncomparisons were ﬁrst in a random order, then all the Sad\ncomparisons were second, ...).\nBefore the ﬁrst trial, the subjects read online deﬁnitions\nof the emotion categories from the Cambridge Academic\nContent Dictionary [1]. The listening test took about 1.5\nhours, with breaks every 30 minutes.\nThe subjects were seated in a “quiet room” with less\nthan 40 dB SPL background noise level. Residual noise\nwas mostly due to computers and air conditioning. The\nnoise level was further reduced with headphones. Sound\nsignals were converted to analog by a Sound Blaster X-\nFi Xtreme Audio sound card, and then presented through\nSony MDR-7506 headphones at a level of approximately\n78 dB SPL, as measured with a sound-level meter. The\nSound Blaster DAC utilized 24 bits with a maximum sam-\npling rate of 96 kHz and a 108 dB S/N ratio.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n2543. RESULTS\n3.1 Quality of Responses\nThe subjects’ responses were ﬁrst screened for inconsis-\ntencies, and two outliers were ﬁltered out. Consistency\nwas deﬁned based on the four comparisons of a pair of in-\nstruments A and B for a particular emotion the same with\nour previous work [30]:\nconsistency A;B =max(v A;vB)\n4(2)\nwherevAandvBare the number of votes a subject\ngave to each of the two instruments. A consistency of 1\nrepresents perfect consistency, whereas 0.5 represents ap-\nproximately random guessing. The mean average consis-\ntency of all subjects was 0.74. Also, as in our previous\nwork [30], we found that the two least consistent subjects\nhad the highest outlier coefﬁcients using White et al.’s\nmethod [28]. Therefore, they were excluded from the re-\nsults.\nWe measured the level of agreement among the remain-\ning 30 subjects with an overall Fleiss’ Kappa statistic [16].\nFleiss’ Kappa was 0.026, indicating a slight but statisti-\ncally signiﬁcant agreement among subjects. From this, we\nobserved that subjects were self-consistent but less agreed\nin their responses than our previous study [30], since spec-\ntral shape was the only factor that could possibly affect\nemotion.\nWe also performed a \u001f2test [29] to evaluate whether\nthe number of circular triads signiﬁcantly deviated from\nthe number to be expected by chance alone. This turned\nout to be insigniﬁcant for all subjects. The approximate\nlikelihood ratio test [29] for signiﬁcance of weak stochas-\ntic transitivity violations [26] was tested and showed no\nsigniﬁcance for all emotions.\n3.1.1 Emotion Results\nSame with our previous work, we ranked the spectral cen-\ntroid equalized instrument tones by the number of positive\nvotes they received for each emotion, and derived scale val-\nues using the Bradley-Terry-Luce (BTL) model [8, 29] as\nshown in Figure 1. The likelihood-ratio test showed that\nthe BTL model describes the paired-comparisons well for\nall emotions. We observe that: 1) The distribution of emo-\ntion ratings were much narrower than the original tones\nin our previous study [30]. The reason is that spectral\nshape was the only factor that could possibly affect emo-\ntion, which made it more difﬁcult for subjects to distin-\nguish. 2) Opposite of our previous study [30], the horn\nevoked positive emotions. It was ranked as the least Shy\nand Depressed, and among the most Heroic and Comic. 3)\nThe clarinet and the saxophone were contrasting outliers\nfor all emotions (except Scary).\nFigure 2 shows BTL scale values and the correspond-\ning 95% conﬁdence intervals of the instruments for each\nemotion. The conﬁdence intervals cluster near the line\nof indifference since it was difﬁcult for listeners to make\nemotional distinctions. Table 1 shows the spectral char-\nacteristics of the static tones (time-domain spectral char-acteristics are omitted since the tones are static). With\nall time-domain spectral characteristics removed, spectral\nshape features such as even/odd harmonic ratio became\nmore salient. Speciﬁcally, even/odd ratio was calculated\naccording to Caclin et al.’s method [9]. Pearson correlation\nbetween emotion and spectral characteristics are shown in\nTable 2. Both spectral irregularity and even/odd harmonic\nratio are measures of spectral jaggedness, where even/odd\nharmonic ratio measures a particular, extreme type of spec-\ntral irregularity that is typical of the clarinet. In Table\n2, even/odd harmonic ratio signiﬁcantly correlated with\nnearly all emotions. The correlations were much stronger\nthan in the original tones [30], and indicate that spectral\nshape by itself can arouse strong emotional responses.\n4. DISCUSSION\nThese results are consistent with our previous results [30]\nand Eerola’s Valence-Arousal results [10]. All these stud-\nies indicate that musical instrument timbres carry cues\nabout emotional expression that are easily and consistently\nrecognized by listeners. They show that spectral cen-\ntroid/brightness is a signiﬁcant component in music emo-\ntion. Beyond Eerola’s and our previous ﬁndings, we have\nfound that spectral shape by itself can have strong emo-\ntional predispositions, and even/odd harmonic ratio is the\nmost salient timbral feature after attack time and brightness\nin static tones.\nIn hindsight, perhaps it is not so surprising that static\nspectra tones have emotional predispositions just as dy-\nnamic musical instrument tones do. It is somewhat anal-\nogous to viewers’ emotional dispositions to primary col-\nors [13, 17, 19].\nOf course, just because static tones have emotional pre-\ndispositions, it does not mean they are interesting to listen\nto. The dynamic spectra of real acoustic instruments are\nmuch more natural and life-like than any static tones, re-\ngardless of emotional predisposition. This is reﬂected in\nthe wider range of emotion rankings of the original dy-\nnamic tones compared to the static tones.\nFor future work, it will be fascinating to see how emo-\ntion varies with pitch, dynamic level, brightness, articula-\ntion, and cultural backgrounds.\n5. ACKNOWLEDGMENT\nThis work has been supported by Hong Kong Research\nGrants Council grants HKUST613112.\n6. REFERENCES\n[1] happy, sad, heroic, scary, comic, shy, joyful and\ndepressed. Cambridge Academic Content Dictionary,\n2013. Online: http://goo.gl/v5xJZ (17 Feb 2013).\n[2] Erkin Asutay, Daniel V ¨astfj¨all, Ana Tajadura-Jim ´enez,\nAnders Genell, Penny Bergman, and Mendel Kleiner.\nEmoacoustics: A Study of the Psychoacoustical\nand Psychological Dimensions of Emotional Sound\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n255Bs\nBsBs\nBsBs\nBsBs\nBs ClCl\nClCl\nClCl\nClCl\nFlFl\nFl FlFlFl\nFlFlHn\nHnHn HnHn\nHnHn\nHnObOb\nOb Ob\nObOb\nObObSx\nSxSxSxSx\nSxSx\nSxTpTp\nTp\nTp\nTpTp Tp\nTpVn\nVnVnVn\nVnVnVn\nVn\n0 .0 70 .0 90 .1 10 .1 30 .1 50 .1 70 .1 90 .2 10 .2 30 .2 5\nH ap p y Sa d* H e ro ic Sca ry* C om ic* Sh y* Joyfu l* D e pre sse d *Bs\nCl\nFl\nHn\nOb\nSx\nTp\nVnFigure 1. Bradley-Terry-Luce scale values of the static tones for each emotion.\nDesign. Journal of the Audio Engineering Society,\n60(1/2):21–28, 2012.\n[3] Jean-Julien Aucouturier, Franc ¸ois Pachet, and Mark\nSandler. The Way it Sounds: Timbre Models for Anal-\nysis and Retrieval of Music Signals. IEEE Transactions\non Multimedia, 7(6):1028–1035, 2005.\n[4] Laura-Lee Balkwill and William Forde Thompson. A\nCross-Cultural Investigation of the Perception of Emo-\ntion in Music: Psychophysical and Cultural Cues. Mu-\nsic Perception, 17(1):43–64, 1999.\n[5] Chris Baume. Evaluation of Acoustic Features for Mu-\nsic Emotion Recognition. In Audio Engineering Soci-\nety Convention 134. Audio Engineering Society, 2013.\n[6] James W Beauchamp. Analysis and Synthesis of Mu-\nsical Instrument Sounds. In Analysis, Synthesis, and\nPerception of Musical Sounds, pages 1–89. Springer,\n2007.\n[7] E Bigand, S Vieillard, F Madurell, J Marozeau, and\nA Dacquet. Multidimensional Scaling of Emotional\nResponses to Music: The Effect of Musical Exper-\ntise and of the Duration of the Excerpts. Cognition and\nEmotion, 19(8):1113–1139, 2005.\n[8] Ralph A Bradley. Paired Comparisons: Some Basic\nProcedures and Examples. Nonparametric Methods,\n4:299–326, 1984.\n[9] Anne Caclin, Stephen McAdams, Bennett K Smith,\nand Suzanne Winsberg. Acoustic Correlates of Tim-\nbre Space Dimensions: A Conﬁrmatory Study Using\nSynthetic Tones. Journal of the Acoustical Society of\nAmerica, 118:471, 2005.\n[10] Tuomas Eerola, Rafael Ferrer, and Vinoo Alluri. Tim-\nbre and Affect Dimensions: Evidence from Affect\nand Similarity Ratings and Acoustic Correlates of Iso-\nlated Instrument Sounds. Music Perception, 30(1):49–\n70, 2012.[11] Inger Ekman and Raine Kajastila. Localization Cues\nAffect Emotional Judgments–Results from a User\nStudy on Scary Sound. In Audio Engineering Society\nConference: 35th International Conference: Audio for\nGames. Audio Engineering Society, 2009.\n[12] Wolfgang Ellermeier, Markus Mader, and Peter Daniel.\nScaling the Unpleasantness of Sounds According to the\nBTL Model: Ratio-scale Representation and Psychoa-\ncoustical Analysis. Acta Acustica United with Acus-\ntica, 90(1):101–107, 2004.\n[13] Michael Hemphill. A note on adults’ color–emotion\nassociations. The Journal of genetic psychology,\n157(3):275–280, 1996.\n[14] Andrew Horner, James Beauchamp, and Richard So.\nDetection of Random Alterations to Time-varying Mu-\nsical Instrument Spectra. Journal of the Acoustical So-\nciety of America, 116:1800–1810, 2004.\n[15] Yajie Hu, Xiaoou Chen, and Deshun Yang. Lyric-\nBased Song Emotion Detection with Affective Lexicon\nand Fuzzy Clustering Method. Proceedings of ISMIR,\n2009.\n[16] Fleiss L Joseph. Measuring Nominal Scale Agree-\nment among Many Raters. Psychological Bulletin,\n76(5):378–382, 1971.\n[17] Naz Kaya and Helen H Epps. Relationship between\ncolor and emotion: A study ofcollege students. College\nstudent journal, 38(3), 2004.\n[18] Judith Liebetrau, Sebastian Schneider, and Roman\nJezierski. Application of Free Choice Proﬁling for the\nEvaluation of Emotions Elicited by Music. In Proceed-\nings of the 9th International Symposium on Computer\nMusic Modeling and Retrieval (CMMR 2012): Music\nand Emotions, pages 78–93, 2012.\n[19] Banu Manav. Color-emotion associations and color\npreferences: A case study for residences. Color Re-\nsearch & Application, 32(2):144–150, 2007.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n256[20] Stephen McAdams, James W Beauchamp, and\nSuzanna Meneguzzi. Discrimination of Musical In-\nstrument Sounds Resynthesized with Simpliﬁed Spec-\ntrotemporal Parameters. Journal of the Acoustical So-\nciety of America, 105:882, 1999.\n[21] Brian CJ Moore, Brian R Glasberg, and Thomas Baer.\nA Model for the Prediction of Thresholds, Loudness,\nand Partial Loudness. Journal of the Audio Engineer-\ning Society, 45(4):224–240, 1997.\n[22] Isabelle Peretz, Lise Gagnon, and Bernard Bouchard.\nMusic and Emotion: Perceptual Determinants, Imme-\ndiacy, and Isolation after Brain Damage. Cognition,\n68(2):111–141, 1998.\n[23] Magdalena Plewa and Bozena Kostek. A Study on Cor-\nrelation between Tempo and Mood of Music. In Audio\nEngineering Society Convention 133, Oct 2012.\n[24] Klaus R Scherer and James S Oshinsky. Cue Utiliza-\ntion in Emotion Attribution from Auditory Stimuli.\nMotivation and Emotion, 1(4):331–346, 1977.\n[25] Janto Skowronek, Martin McKinney, and Steven Van\nDe Par. A Demonstrator for Automatic Music Mood\nEstimation. Proceedings of the International Confer-\nence on Music Information Retrieval, 2007.\n[26] Amos Tversky. Intransitivity of Preferences. Psycho-\nlogical Review, 76(1):31, 1969.\n[27] George Tzanetakis and Perry Cook. Musical Genre\nClassiﬁcation of Audio Signals. IEEE Transactions on\nSpeech and Audio Processing, 10(5):293–302, 2002.\n[28] Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob\nBergsma, and Javier Movellan. Whose V ote Should\nCount More: Optimal Integration of Labels from La-\nbelers of Unknown Expertise. Advances in Neural In-\nformation Processing Systems, 22(2035-2043):7–13,\n2009.\n[29] Florian Wickelmaier and Christian Schmid. A Matlab\nFunction to Estimate Choice Model Parameters from\nPaired-comparison Data. Behavior Research Methods,\nInstruments, and Computers, 36(1):29–40, 2004.\n[30] Bin Wu, Simon Wun, Chung Lee, and Andrew Horner.\nSpectral Correlates in Emotion Labeling of Sustained\nMusical Instrument Tones. In Proceedings of the 14th\nInternational Society for Music Information Retrieval\nConference, November 4-8 2013.\n[31] Yi-Hsuan Yang, Yu-Ching Lin, Ya-Fan Su, and\nHomer H. Chen. A Regression Approach to Music\nEmotion Recognition. IEEE TASLP, 16(2):448–457,\n2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n257●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Happy\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Sad\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Heroic\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Scary\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Comic\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Shy\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Joyful\nBTL scale valueBsClFlHnObSxTpVn\n●●●●●●●●\n0.05 0.10 0.15 0.20 0.25 0.30Depressed\nBTL scale valueBsClFlHnObSxTpVnFigure 2. BTL scale values and the corresponding 95% conﬁdence intervals of the static tones for each emotion. The\ndotted line represents no preference.\n`````````````FeaturesInstrumentsBs Cl Fl Hn Ob Sx Tp Vn\nSpectral Irregularity 0.0971 0.1818 0.143 0.0645 0.119 0.1959 0.0188 0.1176\nEven/odd Ratio 1.2565 0.1775 0.9493 0.9694 0.4308 1.7719 0.7496 0.8771\nTable 1. Spectral characteristics of the static instrument tones.\nXXXXXXXXXXFeaturesEmotionHappy Sad Heroic Scary Comic Shy Joyful Depressed\nSpectral Irregularity -0.1467 0.1827 -0.4859 -0.0897 -0.3216 0.1565 -0.509 0.3536\nEven/odd Ratio 0.8901\u0003\u0003-0.8441\u0003\u00030.7468\u0003\u0003-0.3398 0.8017\u0003\u0003-0.7942\u0003\u00030.6524\u0003-0.7948\u0003\u0003\nTable 2. Pearson correlation between emotion and spectral characteristics for static tones.\u0003\u0003:p< 0:05;\u0003: 0:05<p< 0:1.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n258"
    },
    {
        "title": "Gender Identification and Age Estimation of Users Based on Music Metadata.",
        "author": [
            "Ming-Ju Wu",
            "Jyh-Shing Roger Jang",
            "Chun-Hung Lu"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417605",
        "url": "https://doi.org/10.5281/zenodo.1417605",
        "ee": "https://zenodo.org/records/1417605/files/WuJL14.pdf",
        "abstract": "Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users’ real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information.",
        "zenodo_id": 1417605,
        "dblp_key": "conf/ismir/WuJL14",
        "keywords": [
            "Music recommendation",
            "users real-world identity",
            "music metadata",
            "gender identification",
            "age estimation",
            "experimental results",
            "accuracy",
            "mean absolute error",
            "personal information",
            "improving music recommendation"
        ],
        "content": "GENDER IDENTIFICATION AND AGE ESTIMATION OF USERS BASED\nON MUSIC METADATA\nMing-Ju Wu\nComputer Science Department\nNational Tsing Hua University\nHsinchu, Taiwan\nbrian.wu@mirlab.orgJyh-Shing Roger Jang\nComputer Science Department\nNational Taiwan University\nTaipei, Taiwan\nroger.jang@mirlab.orgChun-Hung Lu\nInnovative Digitech-Enabled Applications\n& Services Institute (IDEAS),\nInstitute for Information Industry,\nTaipei, Taiwan\nenricoghlu@iii.org.tw\nABSTRACT\nMusic recommendation is a crucial task in the ﬁeld of\nmusic information retrieval. However, users frequently\nwithhold their real-world identity, which creates a negative\nimpact on music recommendation. Thus, the proposed\nmethod recognizes users’ real-world identities based on\nmusic metadata. The approach is based on using the tracks\nmost frequently listened to by a user to predict their gender\nand age. Experimental results showed that the approach\nachieved an accuracy of 78.87% for gender identiﬁcation\nand a mean absolute error of 3.69 years for the age\nestimation of 48403 users, demonstrating its effectiveness\nand feasibility, and paving the way for improving music\nrecommendation based on such personal information.\n1. INTRODUCTION\nAmid the rapid growth of digital music and mobile\ndevices, numerous online music services (e.g., Last.fm,\n7digital, Grooveshark, and Spotify) provide music\nrecommendations to assist users in selecting songs. Most\nmusic-recommendation systems are based on content- and\ncollaborative-based approaches [15]. For content-based\napproaches [2,8,9], recommendations are made according\nto the audio similarity of songs. By contrast,\ncollaborative-based approaches involve recommending\nmusic for a target user according to matched listening\npatterns that are analyzed from massive users [1, 13].\nBecause music preferences of users relate to their\nreal-world identities [12], several collaborative-based\napproaches consider identiﬁcation factors such as age\nand gender for music recommendation [14]. However,\nonline music services may experience difﬁculty obtaining\nsuch information. Conversely, music metadata (listening\nhistory) is generally available. This motivated us to\nrecognize users’ real-world identities based on music\nc\rMing-Ju Wu, Jyh-Shing Roger Jang, Chun-Hung Lu.\nLicensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Ming-Ju Wu, Jyh-Shing Roger\nJang, Chun-Hung Lu. “Gender Identiﬁcation and Age Estimation of\nUsers Based on Music Metadata”, 15th International Society for Music\nInformation Retrieval Conference, 2014.\nIdentity unknown\nTop-1 track Top-2 track Top-3 track …\nArtist name Paul Anka The Platters Johnny Cash …\nSong title You Are My \nDestinyOnly You I Love You \nBecause…\nOur system\nGender: male\nAge: 65Music metadata\nof the user\nInput\nOutputFigure 1. Illustration of the proposed system using a real\nexample.\nmetadata. Figure 1 illustrates the proposed system. In this\npreliminary study, we focused on predicting gender and\nage according to the most listened songs. In particular,\ngender identiﬁcation was treated as a binary-classiﬁcation\nproblem, whereas age estimation was considered a\nregression problem. Two features were applied for both\ngender identiﬁcation and age estimation tasks. The ﬁrst\nfeature, TF*IDF, is a widely used feature representation\nin natural language processing [16]. Because the music\nmetadata of each user can be considered directly as\na document, gender identiﬁcation can be viewed as a\ndocument categorization problem. In addition, TF*IDF is\ngenerally applied with latent semantic indexing (LSI) to\nreduce feature dimension. Consequently, this serves as the\nbaseline feature in this study.\nThe second feature, the Gaussian super vector (GSV)\n[3], is a robust feature representation for speaker\nveriﬁcation. In general, the GSV is used to model acoustic\nfeatures such as MFCCs. In this study, music metadata was\ntranslated into proposed hotness features (a bag-of-features\nrepresentation) and could be modeled using the GSV . The\nconcept of the GSV can be described as follows. First,\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n555a universal background model (UBM) is trained using a\nGaussian mixture model (GMM) to represent the global\nmusic preference of users. A user-speciﬁc GMM can\nthen be obtained using the maximum a posteriori (MAP)\nadaptation from the UBM. Finally, the mean vectors of the\nuser-speciﬁc GMM are applied as GSV features.\nThe remainder of this paper is organized as follows:\nSection 2 describes the related literature, and Section 3\nintroduces the TF*IDF; the GSV is explained in Section\n4, and the experimental results are presented in Section 5;\nﬁnally, Section 6 provides the conclusion of this study.\n2. RELATED LITERATURE\nMachine learning has been widely applied to music\ninformation retrieval (MIR), a vital task of which is\ncontent-based music classiﬁcation [5, 11]. For example,\nthe annual Music Information Retrieval Evaluation\neXchange (MIREX) competition has been held since 2004,\nat which some of the most popular competition tasks\nhave included music genre classiﬁcation, music mood\nclassiﬁcation, artist identiﬁcation, and tag annotation.\nThe purpose of content-based music classiﬁcation is to\nrecognize semantic music attributes from audio signals.\nGenerally, songs are represented by features with different\naspects such as timbre and rhythm. Classiﬁers are used\nto identify the relationship between low-level features and\nmid-level music metadata.\nHowever, little work has been done on predicting\npersonal traits based on music metadata [7]. Figure 2\nshows a comparison of our approach and content-based\nmusic classiﬁcation. At the top level, user identity provides\na basic description of users. At the middle level, music\nmetadata provides a description of music. A semantic gap\nexists between music metadata and user identity. Beyond\ncontent-based music classiﬁcation, our approach serves\nas a bridge between them. This enables online music\nservices to recognize unknown users more effectively and,\nconsequently, improve their music recommendations.\n3. TF*IDF FEATURE REPRESENTATION\nThe music metadata of each user can be considered a\ndocument. The TF*IDF describes the relative importance\nof an artist for a speciﬁc document. LSI is then applied for\ndimensionality reduction.\n3.1 TF*IDF\nLet the document (music metadata) of each user in the\ntraining set be denoted as\ndi=ft1;t2;\u0001\u0001\u0001;tng; di2D (1)\nwheretnis the artist name of the top-n listened to song of\nuseri.Dis the collection of all documents in the training\nset. The TF*IDF representation is composed of the term\nfrequency (TF) and inverse document frequency (IDF).\nTF indicates the importance of an artist for a particular\ndocument, whereas IDF indicates the discriminative power\nLow level Middle level Top level \nContent-based music classification Our approachTimbre\nRhythm…Artist \nMood \nGenreArtist Gender\nAge\n…\nFeaturesMusic\nmetadata…User identity\nMusic\nmetadataSemantic gap\nSemantic gapFigure 2. Comparison of our approach and content-based\nmusic classiﬁcation.\nof an artist among documents. The TF*IDF can be\nexpressed as\ntfidfi;n=tfi;n\u0002log\u0012jDj\ndfn\u0013\n(2)\nwheretfi;nis the frequency of tnindi, anddfnrepresents\nthe number of documents in which tnappears.\ndfn=jfd:d2Dandtn2dgj (3)\n3.2 Latent Semantic Indexing\nThe TF*IDF representation scheme leads to high feature\ndimensionality because the feature dimension is equal to\nthe number of artists. Therefore, LSI is generally applied\nto transform data into a lower-dimensional semantic space.\nLetWbe the TF*IDF reorientation of D, where each\ncolumn represents document di. The LSI performs\nsingular value decomposition (SVD) as follows:\nW\u0019U\u0006VT(4)\nwhereUandVrepresent terms and documents in the\nsemantic space, respectively. \u0006is a diagonal matrix\nwith corresponding singular values. \u0006\u00001UTcan be used\nto transform new documents into the lower-dimensional\nsemantic space.\n4. GSV FEATURE REPRESENTATION\nThis section introduces the proposed hotness features and\nexplains how to generate the GSV features based on\nhotness features.\n4.1 Hotness Feature Extraction\nWe assumed each artist tnmay exude various degrees of\nhotness to different genders and ages. For example, the\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n556count (the number of times) of Justin Bieber that occurs\nin users’ top listened to songs of the training set was 845,\nwhere 649 was from the female class and 196 was from\nthe male class. We could deﬁne the hotness of Justin\nBieber for females as 76.80% (649/845) and that for males\nas 23.20% (196/845). Consequently, a user tends to be a\nfemale if her top listened to songs related mostly to Justin\nBieber. Consequently, the age and gender characteristics of\na user can be obtained by computing the hotness features\nof relevant artists.\nLetDbe divided into classes Caccording to users’\ngenders or ages:\n\u001aC1[C2[\u0001\u0001\u0001[Cp=D\nC1\\C2\\\u0001\u0001\u0001\\Cp=;(5)\nwherepis the number of classes. Here, pis 2 for gender\nidentiﬁcation and 51 (the range of age) for age estimation.\nThe hotness feature of each artist tnis deﬁned as hn:\nhn=2\n6664cn;1\n\u000bcn;2\n\u000b...\ncn;p\n\u000b3\n7775(6)\nwherecn;pis the count of artist tninCp, and\u000bis the count\nof artisttnin all classes.\n\u000b=pX\nl=1cn;l (7)\nNext, each document in (1) can be transformed to a\np\u0002nmatrixx, which describes the gender and age\ncharacteristics of a user:\nx= [h1;h2;\u0001\u0001\u0001;hn] (8)\nBecause the form of xcan be considered a bag-of-features,\nthe GSV can be applied directly.\n4.2 GSV Feature Extraction\nFigure 3 is a ﬂowchart of the GSV feature extraction,\nwhich can be divided into ofﬂine and online stages. At\nthe ofﬂine stage, the goal is to construct a UBM [10] to\nrepresent the global hotness features, which are then used\nas prior knowledge for each user at the online stage. First,\nhotness features are extracted for all music metadata in\nthe training set. The UBM is then constructed through a\nGMM estimated using the EM (expectation-maximization)\nalgorithm. Speciﬁcally, the UBM evaluates the likelihood\nof a given feature vector xas follows:\nf(xj\u0012) =KX\nk=1wkN (xjmk;rk) (9)\nwhere\u0012= (w 1;:::;wK;m1;:::;mK;r1;:::;rK)is a set\nof parameters, with wkdenoting the mixture gain for\nthe kth mixture component, subject to the constraintPK\nk=1wk= 1, and N (xjmk;rk)denoting the Gaussian\ndensity function with a mean vector mkand a covariance\nML estimationTraining set \n(music metadata )Offline\nUBMHotness feature \nextraction Online\nMAP adaptationHotness feature \nextraction A user in the \ntraining or test sets \n(music metadata)\n12 k mm m   \nGSVFigure 3. Flowchart of the GSV feature extraction.\nmatrixrk. This bag-of-features model is based on the\nassumption that similar users have similar global artist\ncharacteristics.\nAt the online stage, the MAP adaptation [6] is used to\nproduce an adapted GMM for a speciﬁc user. Speciﬁcally,\nMAP attempts to determine the parameter \u0012in the\nparameter space \u0002that maximizes the posterior probability\ngiven the training data xand hyperparameter !, as follows:\n\u0012MAP = arg max\n\u0012f(xj\u0012)g(\u0012j!) (10)\nwheref(xj\u0012)is the probability density function (PDF) for\nthe observed data xgiven the parameter \u0012, andg(\u0012j!)is\nthe prior PDF given the hyperparameter !.\nFinally, for each user, the mean vectors of the adapted\nGMM are stacked to form a new feature vector called\nGSV . Because the adapted GMM is obtained using MAP\nadaptation over the UBM, it is generally more robust\nthan directly modeling the feature vectors by using GMM\nwithout any prior knowledge.\n5. EXPERIMENTAL RESULTS\nThis section describes data collection, experimental\nsettings, and experimental results.\n5.1 Data Collection\nThe Last.fm API was applied for data set collection,\nbecause it allows anyone to access data including albums,\ntracks, users, events, and tags. First, we collected\nuser IDs through the User.getFriends function. Second,\ntheUser.getInfo function was applied to each user for\nobtaining their age and gender information. Finally, the\nUser.getTopTracks function was applied to acquire at most\ntop-50 tracks listened to by a user. The track information\nincluded song titles and artist names, but only artist names\nwere used for feature extraction in this preliminary study.\nThe ﬁnal collected data set included 96807 users, in\nwhich each user had at least 40 top tracks as well as\ncomplete gender and age information. According to the\nusers’ country codes, they were from 211 countries (or\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n55730%\n18%\n6%5%3%3%3%2%2%2%2%2%2%2%1%18%\n  \nUnited Kingdom\nUnited States\nBrazil\nGermany\nRussia\nItaly\nPoland\nNetherlands\nBelgium\nCanada\nAustralia\nSpain\nFrance\nSweden\nMexico\nOthersFigure 4. Ratio of countries of the collected data set.\n66.21%33.79%\n  Male\nFemale\nFigure 5. Gender ratio of the collected data set.\nregions such as Hong Kong). The ratio of countries\nis shown in Figure 4. The majority were Western\ncountries. The gender ratio is shown in Figure 5, in which\napproximately one-third of users (33.79%) were female\nand two-thirds (66.21%) were male. The age distribution\nof users is shown in Figure 6. The distribution was a\nskewed normal distribution and most users were young\npeople.\nFigure 7 shows the count of each artist that occurred\nin the users’ top listened songs. Among 133938 unique\nartists in the data set, the ranking of popularity presents a\npow-law distribution. This demonstrates that a few artists\ndominate the top listened songs. Although the majority of\nartists are not popular for all users, this does not indicate\nthat they are unimportant, because their hotness could be\ndiscriminative over ages and gender.\n5.2 Experimental Settings\nThe data set was equally divided into two subsets, the\ntraining (48404) and test (48403) sets. An open source tool\nofPython, Gensim, was applied for the TF*IDF and LSI\nimplementation. followed the default setting of Gensim\nthat maintained 200 latent dimensions for the TF*IDF. A\nsupport vector machine (SVM) tool, LIBSVM [4], was\napplied as the classiﬁer. The SVM extension, support\nvector regression (SVR) was applied as the regressor,\nwhich has been observed in many cases to be superior\n15202530354045505560650100020003000400050006000\nAgeCountFigure 6. Age distribution of the collected data set.\n100102104106100101102103104105\nArtistCount\nFigure 7. Count of artists of users’ top listened songs.\nRanking of popularity presents a pow-law distribution.\nto existing regression approaches. The RBF kernel with\n\r= 8 was applied to the SVM and SVR. For the\nUBM parameters, two Gaussian mixture components were\nexperimentally applied (similar results can be obtained\nwhen using a different number of mixture components).\nConsequently, the numbers of dimensions of GSV features\nfor gender identiﬁcation and age estimation were 4 (2\u00022)\nand 102 (2\u000251), respectively.\n5.3 Gender Identiﬁcation\nThe accuracy was 78.87% and 78.21% for GSV and\nTF*IDF + LSI features, respectively. This indicates that\nboth features are adequate for such a task. Despite\nthe low dimensionality of GSV (4), it was superior to\nthe high dimensionality of TF*IDF + LSI (200). This\nindicates the effectiveness of GSV use and the proposed\nhotness features. Figures 8 and 9 respectively show\nthe confusion matrix of using GSV and TF*IDF + LSI\nfeatures. Both features yielded higher accuracies for\nthe male class than for the female class. A possible\nexplanation is that a portion of the females’ were similar to\nthe males’. The classiﬁer tended to favor the majority class\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n558(male), resulting in many female instances with incorrect\npredictions. The age difference can also be regarded for\nfurther analysis. Figure 10 shows the gender identiﬁcation\nresults of two features over various ages. Both features\ntended to have lower accuracies between the ages of 25 and\n40 years, implying that a user whose age is between 25 and\n40 years seems to have more blurred gender boundaries\nthan do users below 25 years and above 40 years.\n5.4 Age Estimation\nTable 1 shows the performance comparison for age\nestimation. The mean absolute error (MAE) was applied\nas the performance index. The range of the predicted ages\nof the SVR is between 15 and 65 years. The experimental\nresults show that the MAE is 3.69 and 4.25 years for GSV\nand TF*IDF + LSI, respectively. The GSV describes the\nage characteristics of a user and utilizes prior knowledge\nfrom the UBM; therefore, the GSV features are superior to\n86.40%\n(27690)13.60%\n(4358)\n35.89%\n(5870)64.11%\n(10485)Male\nFemale\nMale FemaleAccuracy 78.87%\nFigure 8. Confusion matrix of gender identiﬁcation by\nusing GSV features.\n92.14%\n(29529)7.86%\n(2519)\n49.10%\n(8030)50.90%\n(8325)Male\nFemale\nMale FemaleAccuracy 78.21%\nFigure 9. Confusion matrix of gender identiﬁcation by\nusing TF*IDF + LSI features.Method MAE MAE (male) MAE (female)\nGSV 3.69 4.31 2.48\nTF*IDF+LSI 4.25 4.86 3.05\nTable 1. Performance comparison for age estimation.\nthose of the TF*IDF + LSI.\nFor further analysis, gender difference was also\nconsidered. Notably, the MAE of females is less than\nthat of males for both GSV and TF*IDF + LSI features.\nIn particular, the MAE differences between males and\nfemales are approximately 1.8 for both features, implying\nthat females have more distinct age divisions than males\ndo.\n6. CONCLUSION AND FUTURE WORK\nThis study conﬁrmed the possibility of predicting users’\nage and gender based on music metadata. Three of the\nﬁndings are summarized as follows.\n\u000fGSV features are superior to those of TF*IDF +\nLSI for both gender identiﬁcation and age estimation\ntasks.\n\u000fMales tend to exhibit higher accuracy than females\ndo in gender identiﬁcation, whereas females are\nmore predictable than males in age estimation.\n\u000fThe experimental results indicate that gender\nidentiﬁcation is inﬂuenced by age, and vice versa.\nThis suggests that an implicit relationship may exist\nbetween them.\nFuture work could include utilizing the proposed\napproach to improve music recommendation systems. We\nwill also explore the possibility of recognizing deeper\nsocial aspects of user identities, such as occupation and\neducation level.\n15202530354045505560650.60.650.70.750.80.850.90.951\nAgeAccuracy\n  \nGSV\nTF*IDF+LSI\nFigure 10. Gender identiﬁcation results for various ages.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5597. ACKNOWLEDGEMENT\nThis study is conducted under the ”NSC\n102-3114-Y-307-026 A Research on Social Inﬂuence\nand Decision Support Analytics” of the Institute for\nInformation Industry which is subsidized by the National\nScience Council.\n8. REFERENCES\n[1] L. Barrington, R. Oda, and G. Lanckriet. Smarter than\ngenius? human evaluation of music recommender\nsystems. In Proceedings of the International\nSymposium on Music Information Retrieval, pages\n357–362, 2009.\n[2] D. Bogdanov, M. Haro, F. Fuhrmann, E. Gomez,\nand P. Herrera. Content-based music recommendation\nbased on user preference examples. In Proceedings of\nthe ACM Conf. on Recommender Systems. Workshop\non Music Recommendation and Discovery, 2010.\n[3] W. M. Campbell, D. E. Sturim, and D. A. Reynolds.\nSupport vector machines using GMM supervectors for\nspeaker veriﬁcation. IEEE Signal Processing Letters,\n13(5):308–311, May 2006.\n[4] C. C. Chang and C. J. Lin. Libsvm: A library for\nsupport vector machine, 2010.\n[5] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of\naudio-based music classiﬁcation and annotation. IEEE\nTrans. Multimedia., 13(2):303–319, Apr. 2011.\n[6] J. L. Gauvain and C. H. Lee. Maximum a posteriori\nestimation for multivariate gaussian mixture\nobservations of markov chains. IEEE Trans. Audio,\nSpeech, Lang. Process., 2(2):291–298, Apr. 1994.\n[7] Jen-Yu Liu and Yi-Hsuan Yang. Inferring personal\ntraits from music listening history. In Proceedings\nof the Second International ACM Workshop on\nMusic Information Retrieval with User-centered and\nMultimodal Strategies, MIRUM ’12, pages 31–36,\nNew York, NY , USA, 2012. ACM.\n[8] B. McFee, L. Barrington, and G. Lanckriet. Learning\ncontent similarity for music recommendation.\nIEEE Trans. Audio, Speech, Lang. Process.,\n20(8):2207–2218, Oct. 2012.\n[9] A. V . D. Orrd, S. Dieleman, and B. Benjamin. Deep\ncontent-based music recommendation. In Advances in\nNeural Information Processing Systems, 2013.\n[10] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn.\nSpeaker veriﬁcation using adapted gaussian mixture\nmodels. Digital Signal Process, 10(13):19–41, Jan.\n2000.\n[11] B. L. Sturm. A survey of evaluation in music\ngenre recognition. In Proceedings of the Adaptive\nMultimedia Retrieval, 2012.[12] A. Uitdenbogerd and R. V . Schnydel. A review\nof factors affecting music recommender success. In\nProceedings of the International Symposium on Music\nInformation Retrieval, pages 204–208, 2002.\n[13] B. Xu, J. Bu, C. Chen, and D. Cai. An exploration\nof improving collaborative recommender systems via\nuser-item subgroups. In Proceedings of the 21st\ninternational conference on World Wide Web, pages\n21–30, 2012.\n[14] Billy Yapriady and AlexandraL. Uitdenbogerd.\nCombining demographic data with collaborative\nﬁltering for automatic music recommendation.\nInKnowledge-Based Intelligent Information and\nEngineering Systems, volume 3684 of Lecture Notes\nin Computer Science, pages 201–207. 2005.\n[15] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkuno. Hybrid collaborative and content-based music\nrecommendation using probabilistic model with latent\nuser preferences. In Proceedings of the International\nSymposium on Music Information Retrieval, pages\n296–301, 2006.\n[16] W. Zhang, T. Yoshida, and X. Tang. A comparative\nstudy of tf*idf, lsi and multi-words for text\nclassiﬁcation. Expert Systems with Applications,\n38(3):2758–2765, 2011.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n560"
    },
    {
        "title": "Enhancing Collaborative Filtering Music Recommendation by Balancing Exploration and Exploitation.",
        "author": [
            "Zhe Xing",
            "Xinxi Wang",
            "Ye Wang 0007"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416776",
        "url": "https://doi.org/10.5281/zenodo.1416776",
        "ee": "https://zenodo.org/records/1416776/files/XingWW14.pdf",
        "abstract": "Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation al- gorithms work in a greedy way, invariably recommend- ing songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal per- formance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users’ musical tastes, we use a Bayesian graphical model that takes account of both CF latent fac- tors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance.",
        "zenodo_id": 1416776,
        "dblp_key": "conf/ismir/XingWW14",
        "keywords": [
            "Collaborative filtering",
            "greedy approach",
            "exploration",
            "Bayesian graphical model",
            "Bayesian inference",
            "recommendation performance",
            "user study",
            "simulation experiments",
            "novel reinforcement learning",
            "latent factors"
        ],
        "content": "ENHANCING COLLABORATIVE FILTERING MUSIC\nRECOMMENDATION BY BALANCING EXPLORATION AND\nEXPLOITATION\nZhe Xing, Xinxi Wang, Ye Wang\nSchool of Computing, National University of Singapore\nfxing-zhe,wangxinxi,wangyeg@comp.nus.edu.sg\nABSTRACT\nCollaborative ﬁltering (CF) techniques have shown great\nsuccess in music recommendation applications. However,\ntraditional collaborative-ﬁltering music recommendation al-\ngorithms work in a greedy way, invariably recommend-\ning songs with the highest predicted user ratings. Such a\npurely exploitative strategy may result in suboptimal per-\nformance over the long term. Using a novel reinforcement\nlearning approach, we introduce exploration into CF and\ntry to balance between exploration and exploitation. In\norder to learn users’ musical tastes, we use a Bayesian\ngraphical model that takes account of both CF latent fac-\ntors and recommendation novelty. Moreover, we designed\na Bayesian inference algorithm to efﬁciently estimate the\nposterior rating distributions. In music recommendation,\nthis is the ﬁrst attempt to remedy the greedy nature of CF\napproaches. Results from both simulation experiments and\nuser study show that our proposed approach signiﬁcantly\nimproves recommendation performance.\n1. INTRODUCTION\nIn the ﬁeld of music recommendation, content-based ap-\nproaches and collaborative ﬁltering (CF) approaches have\nbeen the prevailing recommendation strategies. Content-\nbased algorithms [1, 9] analyze acoustic features of the\nsongs that the user has rated highly in the past and recom-\nmend only songs that have high degrees of acoustic simi-\nlarity. On the other hand, collaborative ﬁltering (CF) algo-\nrithms [7, 13] assume that people tend to get good recom-\nmendations from someone with similar preferences, and\nthe user’s ratings are predicted according to his neighbors’\nratings. These two traditional recommendation approaches,\nhowever, share a weakness.\nWorking in a greedy way, they always generate “safe”\nrecommendations by selecting songs with the highest pre-\ndicted user ratings. Such a purely exploitative strategy may\nresult in suboptimal performance over the long term due to\nthe lack of exploration. The reason is that user preference\nc\rZhe Xing, Xinxi Wang, Ye Wang.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Zhe Xing, Xinxi Wang, Ye Wang.\n“Enhancing collaborative ﬁltering music recommendation by balancing\nexploration and exploitation”, 15th International Society for Music Infor-\nmation Retrieval Conference, 2014.is only estimated based on the current knowledge avail-\nable in the recommender system. As a result, uncertainty\nalways exists in the predicted user ratings and may give\nrise to a situation where some of the non-greedy options\ndeemed almost as good as the greedy ones are actually bet-\nter than them. Without exploration, however, we will never\nknow which ones are better. With the appropriate amount\nofexploration, the recommender system could gain more\nknowledge about the user’s true preferences before exploit-\ningthem.\nOur previous work [12] tried to mitigate the greedy prob-\nlem in content-based music recommendation, but no work\nhas addressed this problem in the CF context. We thus\naim to develop a CF-based music recommendation algo-\nrithm that can strike a balance between exploration and ex-\nploitation and enhance long-term recommendation perfor-\nmance. To do so, we introduce exploration into collabo-\nrative ﬁltering by formulating the music recommendation\nproblem as a reinforcement learning task called n-armed\nbandit problem. A Bayesian graphical model taking ac-\ncount of both collaborative ﬁltering latent factors and rec-\nommendation novelty is proposed to learn the user pref-\nerences. The lack of efﬁciency becomes a major chal-\nlenge, however, when we adopt an off-the-shelf Markov\nChain Monte Carlo (MCMC) sampling algorithm for the\nBayesian posterior estimation. We are thus prompted to\ndesign a much faster sampling algorithm for Bayesian in-\nference. We carried out both simulation experiments and a\nuser study to show the efﬁciency and effectiveness of the\nproposed approach. Contributions of this paper are sum-\nmarized as follows:\n\u000fTo the best of our knowledge, this is the ﬁrst work in\nmusic recommendation to temper CF’s greedy nature by\ninvestigating the exploration-exploitation trade-off using a\nreinforcement learning approach.\n\u000fCompared to an off-the-shelf MCMC algorithm, a\nmuch more efﬁcient sampling algorithm is proposed to speed\nup Bayesian posterior estimation.\n\u000fExperimental results show that our proposed approach\nenhances the performance of CF-based music recommen-\ndation signiﬁcantly.\n2. RELATED WORK\nBased on the assumption that people tend to receive good\nrecommendations from others with similar preferences, col-\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n445laborative ﬁltering (CF) techniques come in two categories:\nmemory-based CF and model-based CF. Memory-based\nCF algorithms [3, 8] ﬁrst search for neighbors who have\nsimilar rating histories to the target user. Then the target\nuser’s ratings can be predicted according to his neighbors’\nratings. Model-based CF algorithms [7, 14] use various\nmodels and machine learning techniques to discover latent\nfactors that account for the observed ratings.\nOur previous work [12] proposed a reinforcement learn-\ning approach to balance exploration and exploitation in\nmusic recommendation. However, this work is based on\na content-based approach. One major drawback of the per-\nsonalized user rating model is that low-level audio features\nare used to represent the content of songs. This purely\ncontent-based approach is not satisfactory due to the se-\nmantic gap between low-level audio features and high-level\nuser preferences. Moreover, it is difﬁcult to determine\nwhich underlying acoustic features are effective in mu-\nsic recommendation scenarios, as these features were not\noriginally designed for music recommendation. Another\nshortcoming is that songs recommended by content-based\nmethods often lack variety, because they are all acousti-\ncally similar to each other. Ideally, users should be pro-\nvided with a range of genres rather than a homogeneous\nset.\nWhile no work has attempted to address the greedy prob-\nlem of CF approaches in the music recommendation con-\ntext, Karimi et al. tried to investigate it in other recommen-\ndation applications [4, 5]. However, their active learning\napproach merely explores items to optimize the prediction\naccuracy on a pre-determined test set [4]. No attention is\npaid to the exploration-exploitation trade-off problem. In\ntheir other work, the recommendation process is split into\ntwo steps [5]. In the exploration step, they select an item\nthat brings maximum change to the user parameters, and\nthen in the exploitation step, they pick the item based on\nthe current parameters. The work takes balancing explo-\nration and exploitation into consideration, but only in an\nad hoc way. In addition, their approach is evaluated us-\ning only an ofﬂine and pre-determined dataset. In the end,\ntheir algorithm is not practical for deployment in online\nrecommender systems due to its low efﬁciency.\n3. PROPOSED APPROACH\nWe ﬁrst present a simple matrix factorization model for\ncollaborative ﬁltering (CF) music recommendation. Then,\nwe point out major limitations of this traditional CF algo-\nrithm and describe our proposed approach in detail.\n3.1 Matrix Factorization for Collaborative Filtering\nSuppose we have musers andnsongs in the music recom-\nmender system. Let R=frijgm\u0002n denote the user-song\nrating matrix, where each element rijrepresents the rating\nof songjgiven by user i.\nMatrix factorization characterizes users and songs by\nvectors of latent factors. Every user is associated with a\nuser feature vector ui2Rf;i= 1; 2;:::;m, and everysong a song feature vector vj2Rf;j= 1; 2;:::;n. For\na given song j,vjmeasures the extent to which the song\ncontains the latent factors. For a given user i,uimeasures\nthe extent to which he likes these latent factors. The user\nrating can thus be approximated by the inner product of the\ntwo vectors:\n^rij=uT\nivj (1)\nTo learn the latent feature vectors, the system minimizes\nthe following regularized squared error on the training set:\nX\n(i;j)2I(rij\u0000uT\nivj)2+\u0015(mX\ni=1nuikuik2+nX\nj=1nvjkvjk2)(2)\nwhereIis the index set of all known ratings, \u0015a regular-\nization parameter, nuithe number of ratings by user i, and\nnvjthe number of ratings of song j. We use the alternating\nleast squares (ALS) [14] technique to minimize Eq. (2).\nHowever, this traditional CF recommendation approach\nhas two major drawbacks. (I) It fails to take recommen-\ndation novelty into consideration. For a user, the novelty\nof a song changes with each listening. (II) It works greed-\nily, always recommending songs with the highest predicted\nmean ratings, while a better approach may be to actively\nexplore a user’s preferences rather than to merely exploit\navailable rating information [12]. To address these draw-\nbacks, we propose a reinforcement learning approach to\nCF-based music recommendation.\n3.2 A Reinforcement Learning Approach\nMusic recommendation is an interactive process. The sys-\ntem repeatedly choose among ndifferent songs to recom-\nmend. After each recommendation, it receives a rating\nfeedback (or reward ) chosen from an unknown probability\ndistribution, and its goal is to maximize user satisfaction,\ni.e., the expected total reward, in the long run. Similarly,\nreinforcement learning explores an environment and takes\nactions to maximize the cumulative reward. It is thus ﬁtting\nto treat music recommendation as a well-studied reinforce-\nment learning task called n-armed bandit.\nThen-armed bandit problem assumes a slot machine\nwithnlevers. Pulling a lever generates a payoff from the\nunknown probability distribution of the lever. The objec-\ntive is to maximize the expected total payoff over a given\nnumber of action selections, say, over 1000 plays.\n3.2.1 Modeling User Rating\nTo address drawback (I) in Section 3.1, we assume that\na song’s rating is affected by two factors: CF score, the\nextent to which the user likes the song in terms of each CF\nlatent factor, and novelty score, the dynamically changing\nnovelty of the song.\nFrom Eq. (1), we deﬁne the CF score as:\nUCF=\u0012Tv (3)\nwhere vector \u0012indicates the user’s preferences for dif-\nferent CF latent factors and vis the song feature vector\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n446learned by the ALS CF algorithm. For the novelty score,\nwe adopt the formula used in [12]:\nUN= 1\u0000e\u0000t=s(4)\nwheretis the time elapsed since when the song was last\nheard,sthe relative strength of the user’s memory, and\ne\u0000t=sthe well-known forgetting curve. The formula as-\nsumes that a song’s novelty decreases immediately when\nlistened and gradually recovers with time. (For more de-\ntails on the novelty deﬁnition, please refer to [12].) We\nthus model the ﬁnal user rating by combining these two\nscores:\nU=UCFUN= (\u0012Tv)(1\u0000e\u0000t=s) (5)\nGiven the variability in musical taste and memory strength,\neach user is associated with a pair of parameters \n=\n(\u0012;s), to be learned from the user’s rating history. More\ntechnical details will be explained in Section 3.2.2.\nSince the predicted user ratings always carry uncertainty,\nwe assume them to be random variables rather than ﬁxed\nnumbers. Let Rjdenote the rating of song jgiven by the\ntarget user, and Rjfollows an unknown probability distri-\nbution. We assume that the expectation of Rjis theUj\ndeﬁned in Eq. (5). Thus, the expected rating of song jcan\nbe estimated as:\nE[Rj] =Uj= (\u0012Tvj)(1\u0000e\u0000tj=s) (6)\nTraditional recommendation strategy will ﬁrst obtain the\nvjandtjof each song in the system to compute the ex-\npected rating using Eq. (6) and then recommend the song\nwith the highest expected rating. We call this a greedy rec-\nommendation as the system is exploiting its current knowl-\nedge of the user ratings. By selecting one of the non-\ngreedy recommendations and gathering more user feed-\nback, the system explores further and gains more knowl-\nedge about the user preferences. A greedy recommenda-\ntion may maximize the expected reward in the current it-\neration but would result in suboptimal performance over\nthe long term. This is because several non-greedy recom-\nmendations may be deemed nearly as good but come with\nsubstantial variance (or uncertainty), and it is thus possi-\nble that some of them are actually better than the greedy\nrecommendation. Without exploration, however, we will\nnever know which ones they are.\nTherefore, to counter the greedy nature of CF (draw-\nback II), we introduce exploration into music recommen-\ndation to balance exploitation. To do so, we adopt one of\nthe state-of-the-art algorithms called Bayesian Upper Con-\nﬁdence Bounds (Bayes-UCB) [6]. In Bayes-UCB, the ex-\npected reward Ujis a random variable rather than a ﬁxed\nvalue. Given the target user’s rating history D, the pos-\nterior distribution of Uj, denoted as p(UjjD), needs to be\nestimated. Then the song with the highest ﬁxed-level quan-\ntile value of p(UjjD)will be recommended to the target\nuser.\n3.2.2 Bayesian Graphical Model\nTo estimate the posterior distribution of U, we adopt the\nBayesian model (Figure 1) used in [12]. The correspond-\nτ \nNθ s\nv R t\n𝑎0 \n𝑑0 \n𝑒0 \n𝑏0 \n𝑐0 Figure 1: Bayesian Graphical Model.\ning probability dependency is deﬁned as follows:\nRjv;t;\u0012;s;\u001b2\u0018N (\u0012Tv(1\u0000e\u0000t=s);\u001b2) (7)\n\u0012j\u001b2\u0018N (0;a 0\u001b2I) (8)\ns\u0018Gamma (b0;c0) (9)\n\u001c= 1=\u001b2\u0018Gamma (d0;e0) (10)\nIis thef\u0002fidentity matrix.Nrepresents Gaussian\ndistribution with parameters mean and variance. Gamma\nrepresents Gamma distribution with parameters shape and\nrate.\u0012,s, and\u001care parameters. a0,b0,c0,d0, ande0are\nhyperparameters of the priors.\nAt current iteration h+ 1, we have gathered hobserved\nrecommendation history Dh=f(vi;ti;ri)gh\ni=1. Given\nthat each user in our model is described as \n= (\u0012;s),\nwe have according to the Bayes theorem:\np(\njDh)_p(\n)p(Dhj\n) (11)\nThen the posterior probability density function (PDF) of\nthe expected rating Ujof songjcan be estimated as:\np(UjjDh) =Z\np(Ujj\n)p(\njD h)d\n (12)\nSince Eq. (11) has no closed form solution, we are unable\nto directly estimate the posterior PDF in Eq. (12). We thus\nturn to a Markov Chain Monte Carlo (MCMC) algorithm\nto adequately sample the parameters \n= (\u0012;s). We then\nsubstitute every parameter sample into Eq. (6) to obtain a\nsample ofUj. Finally, the posterior PDF in Eq. (12) can\nbe approximated by the histogram of the samples of Uj.\nAfter estimating the posterior PDF of each song’s ex-\npected rating, we follow the Bayes-UCB approach [6] to\nrecommend song j\u0003that maximizes the quantile function:\nj\u0003=arg max\nj=1;:::;jSjQ(\u000b;p(UjjDh)) (13)\nwhere\u000b= 1\u00001\nh+1,jSjis the total number of songs in the\nrecommender system, and the quantile function Qreturns\nthe valuexsuch that Pr(U j\u0014xjDh) =\u000b. The pseudo\ncode of our algorithm is presented in Algorithm 1.\n3.3 Efﬁcient Sampling Algorithm\nBayesian inference is very slow with an off-the-shelf MCMC\nsampling algorithm because it takes a long time for the\nMarkov chain to converge. In response, we previously pro-\nposed an approximate Bayesian model using piecewise lin-\near approximation [12]. However, not only is the original\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n447Algorithm 1 Exploration-Exploitation Balanced Music\nRecommendation\nforh= 1!Ndo\nifh== 1 then\nRecommend a song randomly;\nelse\nDraw samples of \u0012andsbased onp(\njDh\u00001);\nforsongj= 1!jSjdo\nObtain vjandtjof songjand compute samples of\nUjusing Eq. (6);\nEstimatep(UjjDh\u00001)using histogram of the samples\nofUj;\nCompute quantile qh\nj=Q\u0000\n1\u00001\nh;p(UjjDh\u00001)\u0001\n;\nend for\nRecommend song j\u0003=argmaxj=1;:::;jSjqh\nj;\nCollect user rating rhand updatep(\njDh);\nend if\nend for\nBayesian model altered, tuning the numerous (hyper)para-\nmeters is also tedious. In this paper, we present a bet-\nter way to improve efﬁciency. Since it is simple to sam-\nple from a conditional distribution, we develop a speciﬁc\nGibbs sampling algorithm to hasten convergence.\nGivenNtraining samplesD=fvi;ti;rigN\ni=1, the con-\nditional distribution p(\u0012jD;\u001c;s )is still a Gaussian distri-\nbution and can be obtained as follows:\np(\u0012jD;\u001c;s)/p(\u001c)p(\u0012j\u001c)p(s)NY\ni=1p(rijvi;ti;\u0012;s;\u001c)\n/p(\u0012j\u001c)NY\ni=1p(rijvi;ti;\u0012;s;\u001c)_exp\u0012\n\u00001\n2\u0012T(a0\u001b2I)\u00001\u0012\u0013\n\u0002exp NX\ni=1\u00001\n2\u001b2\u0010\nri\u0000\u0012Tvi(1\u0000e\u0000ti=s)\u00112!\n_exp\u0012\n\u00001\n2\u0012T\u0003\u0012+\u0011T\u0012\u0013\n_N(\u0016;\u0006) (14)\nwhere \u0016and\u0006, respectively the mean and covariance of\nthe multivariate Gaussian distribution, satisfy:\n\u0006\u00001=\u0003=\u001c \n1\na0I+NX\ni=1(1\u0000e\u0000ti=s)2vivT\ni!\n(15)\n\u0016T\u0006\u00001=\u0011T=\u001c NX\ni=1ri(1\u0000e\u0000ti=s)vT\ni!\n(16)\nSimilarly, the conditional distribution p(\u001cjD;\u0012;s)re-\nmains a Gamma distribution and can be derived as:\np(\u001cjD;\u0012;s)/p(\u001c)p(\u0012j\u001c)p(s)NY\ni=1p(rijvi;ti;\u0012;s;\u001c)\n/p(\u001c)p(\u0012j\u001c)NY\ni=1p(rijvi;ti;\u0012;s;\u001c)\n_\u001cd0\u00001exp(\u0000e 0\u001c)\u0002exp\u0012\n\u00001\n2\u0012T(a0\u001b2I)\u00001\u0012\u0013\n\u0002\n\u0010\n\u001bp\n2\u0019\u0011\u0000N\nexp NX\ni=1\u00001\n2\u001b2\u0010\nri\u0000\u0012Tvi(1\u0000e\u0000ti=s)\u00112!\n_\u001c\u000b\u00001exp(\u0000\f\u001c )_Gamma (\u000b;\f ) (17)# Users # Songs # Observations % Density\n100,000 20,000 20,699,820 1.035%\nTable 1: Size of the dataset. Density is the percentage of entries\nin the user-song matrix that have observations.\nwhere\u000band\fare respectively the shape and rate of the\nGamma distribution and satisfy:\n\u000b=d0+f+N\n2(18)\n\f=e0+\u0012T\u0012\n2a0+1\n2NX\ni=1\u0010\nri\u0000\u0012Tvi(1\u0000e\u0000ti=s)\u00112\n(19)\nThe conditional distribution p(sjD;\u0012;\u001c)has no closed\nform expression. We thus adopt the Metropolis-Hastings\n(MH) algorithm [2] with a proposal distribution q(st+1jst) =\nN(st;1)to draw samples of s. Our detailed Gibbs sam-\npling process is presented in Algorithm 2.\nAlgorithm 2 Gibbs Sampling for Bayesian Inference\nInitialize\u0012,s,\u001c;\nfort= 1!MaxIteration do\nSample\u0012(t+1)sp(\u0012jD;\u001c(t);s(t));\nSample\u001c(t+1)sp(\u001cjD;\u0012(t+1);s(t));\nstmp=s(t);\nfori= 1!Kdo #MH Step\nDrawysN(stmp;1);\n\u000b=min\u0010\np(yjD;\u0012(t+1);\u001c(t+1))\np(stmpjD;\u0012(t+1);\u001c(t+1));1\u0011\n;\nDrawu\u0018Uniform(0; 1);\nifu<\u000b then\nstmp=y;\nend if\nend for\ns(t+1)=stmp;\nend for\n4. EV ALUATION\n4.1 Dataset\nThe Taste Proﬁle Subset1used in the Million Song Dataset\nChallenge [10] has over 48 million triplets (user, song,\ncount) describing the listening history of over 1 million\nusers and 380,000 songs. We select 20,000 songs with top\nlistening counts and 100,000 users who have listened to the\nmost songs. Since this collection of listening history is a\nform of implicit feedback data, we use the approach pro-\nposed in [11] to perform negative sampling. The detailed\nstatistics of the ﬁnal dataset are shown in Table 1.\n4.2 Learning CF Latent Factors\nFirst, we determine the optimal value of \u0015, the regular-\nization parameter, and f, the dimensionality of the latent\nfeature vectors. We randomly split the dataset into three\ndisjoint parts: training set (80%), validation set (10%),\nand test set (10%). Training set is used to learn the CF\nlatent factors, and the convergence criteria of the ALS al-\ngorithm is achieved when the change in root mean square\n1http://labrosa.ee.columbia.edu/millionsong/tasteproﬁle\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4480 100 200 300 400\nTraining set size2468Accuracy (Root Mean Square Error)MCMC\nGibbsFigure 2: Prediction accuracy of sampling algorithms.\nerror (RMSE) on the validation set is less than 10\u00004. Then\nwe use the learned latent factors to predict the ratings on\nthe test set. We ﬁrst ﬁx f= 55 and vary\u0015from 0.005 to\n0.1; minimal RMSE is achieved at \u0015= 0:025. We then\nﬁx\u0015= 0:025 and varyffrom 10 to 80, and f= 75\nyields minimal RMSE. Therefore, we adopt the optimal\nvalue\u0015= 0:025 andf= 75 to perform the ﬁnal ALS CF\nalgorithm and obtain the learned latent feature vector of\neach song in our dataset. These vectors will later be used\nfor reinforcement learning.\n4.3 Efﬁciency Study\nTo show that our Gibbs sampling algorithm makes Bayesian\ninference signiﬁcantly more efﬁcient, we conduct simula-\ntion experiments to compare it with an off-the-shelf MCMC\nalgorithm developed in JAGS2. We implemented the Gibbs\nalgorithm in C++, which JAGS uses, for a fair comparison.\nFor each data point di2f(vi;ti;ri)gn\ni=1in the simu-\nlation experiments, viis randomly chosen from the latent\nfeature vectors learned by the ALS CF algorithm. tiis\nrandomly sampled from uniform(50; 2592000), i.e. be-\ntween a time gap of 50 seconds and one month. riis calcu-\nlated using Eq. (6) where elements of \u0012are sampled from\nN(0;1)andsfromuniform(100; 1000).\nTo determine the burn-in and sample size of the two\nalgorithms and to ensure they draw samples equally effec-\ntively, we ﬁrst check to see if they converge to a similar\nlevel. We generate a test set of 300 data points and vary\nthe size of the training set to gauge the prediction accuracy.\nWe setK= 5 in the MH step of our Gibbs algorithm.\nWhile our Gibbs algorithm achieves reasonable accuracy\nwith burn-in = 20 and sample size = 100, the MCMC al-\ngorithm gives comparable results only when both parame-\nters are 10000. Figure 2 shows their prediction accuracies\naveraged over 10 trials. With burn-in and sample size de-\ntermined, we then conduct an efﬁciency study of the two\nalgorithms. We vary the training set size from 1 to 1000\nand record the time they take to ﬁnish the sampling pro-\ncess. We use a computer with Intel Core i7-2600 CPU\n@ 3.40Ghz and 8GB RAM. The efﬁciency comparison re-\nsult is shown in Figure 3. We can see that computation\ntime of both two sampling algorithms grows linearly with\nthe training set size. However, our proposed Gibbs sam-\npling algorithm is hundreds of times faster than MCMC,\n2http://mcmc-jags.sourceforge.net/\n0 200 400 600 800 1000\nTraining set size0102030405060Time (seconds)MCMC\nGibbsFigure 3: Efﬁciency comparison of sampling algorithms.\n(TimeMCMC = 538: 762s andTimeGibbs = 0:579s when\nTrainingSetSize = 1000 ).\nFigure 4: Online evaluation platform.\nsuggesting that our proposed approach is practical for de-\nployment in online recommender systems.\n4.4 User Study\nIn an online user study, we compare the effectiveness of\nour proposed recommendation algorithm, Bayes-UCB-CF,\nwith that of two baseline algorithms: (1) Greedy algo-\nrithm, representing the traditional recommendation strat-\negy without exploration-exploitation trade-off. (2) Bayes-\nUCB-Content algorithm [12], which also adopts the Bayes-\nUCB technique but is content-based instead of CF-based.\nWe do not perform ofﬂine evaluation because it cannot cap-\nture the effect of the elapsed time tin our rating model and\nthe interactiveness of our approach.\nEighteen undergraduate and graduate students (9 females\nand 9 males, age 19 to 29) are invited to participate in the\nuser study. The subject pool covers a variety of majors\nof study and nationalities, including American, Chinese,\nKorean, Malaysian, Singaporean and Iranian. Subjects re-\nceive a small payment for their participation. The user\nstudy takes place over the course of two weeks in April\n2014 on a user evaluation website we constructed (Figure\n4). The three algorithms evaluated are randomly assigned\nto numbers 1-3 to avoid bias. For each algorithm, 200 rec-\nommendations are evaluated using a rating scale from 1 to\n5. Subjects are reminded to take breaks frequently to avoid\nfatigue. To minimize the carryover effect, subjects can-\nnot evaluate two different algorithms in one day. For the\nuser study, Bayes-UCB-CF’s hyperparameters are set as:\na0= 10,b0= 3,c0= 0:01,d0= 0:001 ande0= 0:001.\nSince maximizing the total expected rating is the main\nobjective of a music recommender system, we thus com-\npare the cumulative average rating of the three algorithms.\nFigure 5 shows the average rating and standard error of\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n4490 50 100 150 200\nrecommendation iterations2.22.42.62.83.03.23.4cumulative average rating\nGreedy\nBayes-UCB-Content\nBayes-UCB-CFFigure 5: Recommendation performance comparison.\neach algorithm from the beginning till the n-th recommen-\ndation iteration. We can see that our proposed Bayes-UCB-\nCF algorithm signiﬁcantly outperforms Bayes-UCB-Content,\nsuggesting that the latter still fails to bridge the semantic\ngap between high-level user preferences and low-level au-\ndio features.\nT-tests show that Bayes-UCB-CF starts to signiﬁcantly\noutperform the Greedy baseline after the 46th iteration (p-\nvalue<0:0472). In fact, Greedy’s performance decays\nrapidly after the 60th iteration while others continue to\nimprove. Because Greedy solely exploits, it is quickly\ntrapped at a local optima, repeatedly recommending the\nfew songs with initial good ratings. As a result, the novelty\nof those songs plummets, and users become bored. Greedy\nwill introduce new songs after collecting many low ratings,\nonly to be soon trapped into a new local optima. By con-\ntrast, our Bayes-UCB-CF algorithm balances exploration\nand exploitation and thus signiﬁcantly improves the rec-\nommendation performance.\n5. CONCLUSION\nWe present a novel reinforcement learning approach to mu-\nsic recommendation that remedies the greedy nature of the\ncollaborative ﬁltering approaches by balancing exploita-\ntion with exploration. A Bayesian graphical model incor-\nporating both the CF latent factors and novelty is used to\nlearn user preferences. We also develop an efﬁcient sam-\npling algorithm to speed up Bayesian inference. In mu-\nsic recommendation, our work is the ﬁrst attempt to in-\nvestigate the exploration-exploitation trade-off and to ad-\ndress the greedy problem in CF-based approaches. Results\nfrom simulation experiments and user study have shown\nthat our proposed algorithm signiﬁcantly improves recom-\nmendation performance over the long term. To further im-\nprove recommendation performance, we plan to deploy a\nhybrid model that combines content-based and CF-based\napproaches in the proposed framework.\n6. ACKNOWLEDGEMENT\nWe thank the subjects in our user study for their participation.\nWe are also grateful to Haotian “Sam” Fang for proofreadingthe manuscript. This project is funded by the National Research\nFoundation (NRF) and managed through the multi-agency In-\nteractive & Digital Media Programme Ofﬁce (IDMPO) hosted\nby the Media Development Authority of Singapore (MDA) un-\nder Centre of Social Media Innovations for Communities (COS-\nMIC).\n7. REFERENCES\n[1] P. Cano, M. Koppenberger, and N. Wack. Content-based mu-\nsic audio recommendation. In Proceedings of the 13th annual\nACM international conference on Multimedia, pages 211–\n212. ACM, 2005.\n[2] S. Chib and E. Greenberg. Understanding the metropolis-\nhastings algorithm. The American Statistician, 49(4):327–\n335, 1995.\n[3] J. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An\nalgorithmic framework for performing collaborative ﬁltering.\nInProceedings of the 22nd annual ACM international con-\nference on SIGIR, pages 230–237. ACM, 1999.\n[4] R. Karimi, C. Freudenthaler, A. Nanopoulos, and L. Schmidt-\nThieme. Active learning for aspect model in recommender\nsystems. In Symposium on Computational Intelligence and\nData Mining, pages 162–167. IEEE, 2011.\n[5] R. Karimi, C. Freudenthaler, A. Nanopoulos, and L. Schmidt-\nThieme. Non-myopic active learning for recommender sys-\ntems based on matrix factorization. In International Confer-\nence on Information Reuse and Integration, pages 299–303.\nIEEE, 2011.\n[6] E. Kaufmann, O. Capp ´e, and A. Garivier. On bayesian upper\nconﬁdence bounds for bandit problems. In International Con-\nference on Artiﬁcial Intelligence and Statistics, pages 592–\n600, 2012.\n[7] N. Koenigstein, G. Dror, and Y . Koren. Yahoo! music recom-\nmendations: modeling music ratings with temporal dynamics\nand item taxonomy. In Proceedings of the ﬁfth ACM confer-\nence on Recommender systems, pages 165–172. ACM, 2011.\n[8] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker,\nL. R. Gordon, and J. Riedl. Grouplens: applying collabora-\ntive ﬁltering to usenet news. Communications of the ACM,\n40(3):77–87, 1997.\n[9] B. Logan. Music recommendation from song sets. In ISMIR,\n2004.\n[10] B. McFee, T. Bertin-Mahieux, D. P.W. Ellis, and G. R.G.\nLanckriet. The million song dataset challenge. In Proceed-\nings of international conference companion on World Wide\nWeb, pages 909–916. ACM, 2012.\n[11] R. Pan, Y . Zhou, B. Cao, N. N. Liu, R. Lukose, M. Scholz,\nand Q. Yang. One-class collaborative ﬁltering. In Eighth\nIEEE International Conference on Data Mining, pages 502–\n511. IEEE, 2008.\n[12] X. Wang, Y . Wang, D. Hsu, and Y . Wang. Exploration in in-\nteractive personalized music recommendation: A reinforce-\nment learning approach. arXiv preprint arXiv:1311.6355,\n2013.\n[13] K. Yoshii and M. Goto. Continuous plsi and smoothing tech-\nniques for hybrid music recommendation. In ISMIR, pages\n339–344, 2009.\n[14] Y . Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale\nparallel collaborative ﬁltering for the netﬂix prize. In Algo-\nrithmic Aspects in Information and Management, pages 337–\n348. Springer, 2008.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n450"
    },
    {
        "title": "Detecting Drops in Electronic Dance Music: Content based approaches to a socially significant music event.",
        "author": [
            "Karthik Yadati",
            "Martha A. Larson",
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417081",
        "url": "https://doi.org/10.5281/zenodo.1417081",
        "ee": "https://zenodo.org/records/1417081/files/YadatiLLH14.pdf",
        "abstract": "Electronic dance music (EDM) is a popular genre of mu- sic. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the num- ber of users who leave comments in the general neighbor- hood of drop events in music on online audio distribution platforms like SoundCloud. The variability that character- izes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage ap- proach to drop detection that first models the sound char- acteristics during drop events and then incorporates tem- poral structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the pro- posed method for combining content analysis and social activity to detect events in music recordings.",
        "zenodo_id": 1417081,
        "dblp_key": "conf/ismir/YadatiLLH14",
        "keywords": [
            "Electronic dance music",
            "automatic detection",
            "drop event",
            "SoundCloud",
            "two-stage approach",
            "sound characteristics",
            "temporal structure",
            "watershed moment",
            "social comments",
            "weak reference labels"
        ],
        "content": "DETECTING DROPS IN ELECTRONIC DANCE MUSIC: CONTENT\nBASED APPROACHES TO A SOCIALLY SIGNIFICANT MUSIC EVENT\nKarthik Yadati, Martha Larson, Cynthia C. S. Liem, Alan Hanjalic\nDelft University of Technology\nfn.k.yadati,m.a.larson,c.c.s.liem,a.hanjalicg@tudelft.nl\nABSTRACT\nElectronic dance music (EDM) is a popular genre of mu-\nsic. In this paper, we propose a method to automatically\ndetect the characteristic event in an EDM recording that is\nreferred to as a drop. Its importance is reﬂected in the num-\nber of users who leave comments in the general neighbor-\nhood of drop events in music on online audio distribution\nplatforms like SoundCloud. The variability that character-\nizes realizations of drop events in EDM makes automatic\ndrop detection challenging. We propose a two-stage ap-\nproach to drop detection that ﬁrst models the sound char-\nacteristics during drop events and then incorporates tem-\nporal structure by zeroing in on a watershed moment. We\nalso explore the possibility of using the drop-related social\ncomments on the SoundCloud platform as weak reference\nlabels to improve drop detection. The method is evaluated\nusing data from SoundCloud. Performance is measured\nas the overlap between tolerance windows centered around\nthe hypothesized and the actual drop. Initial experimental\nresults are promising, revealing the potential of the pro-\nposed method for combining content analysis and social\nactivity to detect events in music recordings.\n1. INTRODUCTION\nElectronic dance music (EDM) is a popular genre of dance\nmusic which, as the name suggests, is created using elec-\ntronic equipment and played in dance environments. Out-\nside of clubs and dance festivals, EDM artists and listeners\nactively share music on online social platforms. Central\nto the enjoyment of EDM is a phenomenon referred to as\n“The Drop”. Within the EDM community, a drop is de-\nscribed as a moment of emotional release, where people\nstart to dance “like crazy” [12]. There is no precise recipe\nfor creating a drop when composing EDM. Rather, a drop\noccurs after a build, a building up of tension, and is fol-\nlowed by the re-introduction of the full bassline [1]. A\ngiven EDM track may contain one or more drop moments.\nc\rKarthik Yadati, Martha Larson, Cynthia C. S. Liem,\nAlan Hanjalic.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Karthik Yadati, Martha Larson, Cyn-\nthia C. S. Liem, Alan Hanjalic. “Detecting Drops in Electronic Dance\nMusic: Content based approaches to a socially signiﬁcant music event”,\n15th International Society for Music Information Retrieval Conference,\n2014.The designation “The Drop” is generally reserved for the\noverall phenomenon rather than speciﬁc drop events.\nIn this paper we address the challenge of automatically\ndetecting a drop in a given EDM track. The social signiﬁ-\ncance of the drop in the EDM context can be inferred, for\ninstance, from the websites that compile a playlist of the\nbest drops1. It is also evident from vivid social activity\naround drop events on online audio distribution platforms\nsuch as SoundCloud2. We also mention here a documen-\ntary, scheduled to be released in 2014, tracking the evo-\nlution of EDM as a cultural phenomenon, and titled The\nDrop3. Ultimately, the drop detection approach proposed\nin this paper could serve both EDM artists and listeners.\nFor example, it would enable artists to compare drop cre-\nation techniques, and would also support listeners to better\nlocate their favorite drop moments.\nThe challenge of drop detection arises from the high\nvariability in different EDM tracks, which differ in their\nmusical content and temporal development. Our drop de-\ntection approach uses audio content analysis and machine\nlearning techniques to capture this variability. As an ad-\nditional source of reference labels for classiﬁer training,\nwe explore the utility of drop-related social data in the\nform of timed comments, comments associated with spe-\nciﬁc time codes. We draw our data from SoundCloud, a\nmusic distribution platform that supports timed comments\nand is representative of online social sharing of EDM. The\npaper makes three contributions:\n\u000fWe propose a two-step content-based drop detection\napproach.\n\u000fWe verify the ability of the approach to detect drops\nin EDM tracks.\n\u000fWe demonstrate utility of the social features (timed\ncomments on SoundCloud) to reduce the amount of\nhand-labeled data needed to train our classiﬁer.\nThe remainder of this paper is organized as follows.\nSection 2 discusses related work, and is followed by the\npresentation and evaluation of our method in sections 3 and\n4. Section 5 provides a summary and an outlook towards\nfuture work.\n1http://www.beatport.com/charts/top-10-edm-drops-feb1/252641\n2http://soundcloud.com\n3http://www.imdb.com/title/tt2301898/\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1432. RELATED WORK\nAlthough Electronic Dance Music is a popular music genre\nattracting large audiences, it has received little attention in\nthe music information retrieval research community. Re-\nsearch on EDM is limited to a small number of contribu-\ntions. Here, we mention the most notable. Hockman et\nal. [5] propose a genre-speciﬁc beat tracking system that is\ndesigned to analyze music from the following EDM sub-\ngenres: Hardcore, Jungle, Drum and Bass. Kell et al. in [6]\nalso apply audio content analysis to EDM in order to inves-\ntigate track ordering and selection, which is usually carried\nout by human experts, i.e., Disc Jockeys (DJ). The work re-\nport ﬁndings on which content features inﬂuence the pro-\ncess of ordering and selection. A musicological perspec-\ntive is offered by Collins in [3], who applies audio content\nanalysis and machine learning techniques to empirically\nstudy the creative inﬂuence of earlier musical genres on the\nlater ones using a date annotated database of EDM tracks,\nwith speciﬁc focus on the sub-genres Detroit techno and\nChicago house. Our work strives to redress the balance and\ngive more attention to EDM. It draws attention to Sound-\nCloud as an important source of music data and associated\nsocial annotations, and also to “The Drop”, a music event\nof key signiﬁcance for the audience of EDM.\nThe rise of social media has also seen the rise in avail-\nability of user-contributed metadata (e.g., comments and\ntags). Social tags have recently grown in importance in\nmusic information retrieval research. In [11], they were\nused to predict perceived or induced emotional responses\nto music. This work reports ﬁndings on the correlation be-\ntween the emotion tags associated with songs on Last.fm—\n“happy”, “sad”, “angry” and “relax”—and the user emo-\ntion ratings for perceived and induced emotions. Social\ndata is generally noisy, since generating precise labels is\nnot users’ primary motivation for tagging or commenting.\nHowever, this data can still prove useful as weak reference\nlabels, reducing the burden of producing ground-truth la-\nbels for a large set of music tracks, which is an expen-\nsive and time consuming task. Social tags available on\nLast.fm have been used to automatically generating tags\nfor songs [4]. An interesting direction of research is de-\nscribed in [13], where the authors use content-based anal-\nysis of the song to improve the tags provided by users. Ex-\nisting work makes use of social tags that users assign to\na song as a whole. In contrast, our work makes use of\ntimed comments that users contribute associated with spe-\nciﬁc time points during a song.\nObtaining time-code level ground-truth labels for a large\nset of music tracks is an expensive and time consuming\ntask. One way to obtain reference labels is to use crowd-\nsourcing, where users are explicitly offered a task (e.g., la-\nbel the type of emotion [9]). Our approach of using timed\ncomments spares the expense of crowdsourcing. It has the\nadditional advantage that users have contributed the com-\nments spontaneously, i.e., they have not been asked to ex-\nplicitly assign them, making them a more natural expres-\nsion of user reactions during their listening experience.3. PROPOSED APPROACH\nOur proposed two-step approach is based on general prop-\nerties of the “The Drop”. As previously mentioned drops\nare characterized by a build up towards a climax followed\nby reintroduction of the bassline. We hypothesize that the\nswitch will coincide with a structural segment that ends at a\ndrop moment. For this reason, the ﬁrst step in our approach\nis segmentation. However, not all segment boundaries are\ndrops. For this reason, the second step in our approach is\na content-based classiﬁcation of segments that eliminates\nsegments whose boundaries are not drop points. Figure 1\nillustrates the two-stage approach, where we ﬁrst segment\nto identify drop candidates and then classify in order to\nisolate candidates that are actually drop moments.\nSegment  \nClassification  \nSegment  \nboundaries  Detected drop  \nFigure 1. Two-stage approach to drop detection\nThe classiﬁcation framework we propose to ﬁnd drop\nevents is illustrated in Figure 2. At the heart of the frame-\nwork are the following modules: Segmentation, feature ex-\ntraction, classiﬁcation and evaluation.\nFeature \nExtraction  \nModel \nTraining  User \ncomments  Expert \nlabels  \nSegment \nClassification  Evaluation  Training Data \nTest Data  \nSegmentation  \nFeature \nExtraction  Segmentation  \nFigure 2. The proposed classiﬁcation framework\n3.1 Segmentation\nThe segmentation step carries out unsupervised segment\nboundary detection. Exploratory experiments revealed that\nthe segmentation method proposed in [10] gives a good\nﬁrst approximation of the drops in an EDM track, and we\nhave adopted it for our experiments. The method uses the\nchroma features computed from the audio track to identify\nthe segment boundaries. We use the same parameters as\nused in [10]: 12 pitch classes, a window length of 209 ms,\nand a hop size of 139 ms. We carried out an intermediate\nevaluation to establish the quality of the drop candidates\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n144generated by the segmentation step alone. The average\ndistance between the actual drop (ground-truth) and a seg-\nment boundary generated by our segmentation method is\n2.5 seconds, and less than 8% of the drops are missed in\nour training set (described in Section 4.1).\n3.2 Feature Extraction for Classiﬁcation\nAn overview of the feature extraction process is illustrated\nin Figure 3.\nFeature \nSelection  Spectrogram  \nRhythm  MFCC  \nSegment  \nboundaries  \nFigure 3. Feature extraction procedure\nAfter segmentation, we extract content-based features\nfrom a ﬁxed length window around the segment boundary.\nWe use the following features: Spectrogram, MFCC and\nfeatures related to rhythm. We adopt Mel-Frequency Cep-\nstral Coefﬁcients (MFCC) and features computed from the\nspectrogram because of their effectiveness. A unique fea-\nture of a drop is that it is preceded by a buildup or build.\nFigure 4 indicates that this buildup can be clearly observed\nin the spectrogram of an audio segment containing a drop.\nThis provides additional motivation to use features com-\nputed from the spectrogram in our approach. We use the\nstatistics computed from the spectrogram in our method\n(mean and standard deviations of the frequencies). For\nMFCC and spectrogram calculation, we use a window size\nof 50 msec with a 50% overlap with the subsequent win-\ndows. We use 13 coefﬁcients for the MFCC. Due to a\nFigure 4. Spectrogram of an audio segment indicating a\nbuild (red arrow) towards a drop at 10 seconds.\nswitch of rhythm at the drop moment, features related to\nrhythm are another important source of information. We\nuse the rhythm related features: rhythm patterns, rhythm\nhistogram, temporal rhythm histogram [8]. We concate-\nnate the rhythm features, MFCC and statistics computed\nfrom the spectrogram into a single feature vector. Featureselection, following the approach of [2], is performed on\nthe training data in order to reduce the dimensionality of\nthe feature vector and also to ensure that we use the most\ninformative features in the classiﬁcation step.\n3.3 Training and Classiﬁcation\nTo train the classiﬁer, we assign drop (1) vs. non-drop (0)\nlabels to time-points in the track using two sources of infor-\nmation: high ﬁdelity ground-truth (manual labels provided\nby an expert) and user comments (weak reference labels).\nPrior to training the model, we map the ground-truth la-\nbels to the nearest segment boundaries. We note that the\nsegmentation step reduces the search space for the drop,\nas we no longer search for it in the entire track, but fo-\ncus on features around the segment boundaries. We use a\nbinary SVM classiﬁer with a linear kernel as our training\nalgorithm.\n3.4 Evaluation\nOur method predicts time points in a track at which the\ndrop occurs. We consider each detected drop to be a dis-\ntinct drop. The fact that the drop can only be hypothesized\nat a segment boundary keeps detections from occurring\nclose together, given that the average length of segments\ngenerated by our segmentation algorithm is 16.5 seconds.\nIn order to report the performance in terms of accuracy\nand precision, we utilize the F1-score. Although the drop\nis annotated as a point in the track, it is characterized by\nthe music around the point. This aspect of the drop mo-\ntivates our choice of using a tolerance window of varying\ntemporal resolutions around the hypothesized drop and use\ntemporal overlap to compute the F1-score. We follow these\nsteps to compute the F1-score:\n1. Place a tolerance window of size tseconds centered\naround the hypothesized (from our approach) and\nthe reference drop (ground-truth).\n2. Compute the number of true positives (tp), false pos-\nitives (fp) and false negatives (fn) as illustrated in\nFigure 5 (the unit of measurement being seconds).\nNote that the numbers computed here are related to\nthe number of seconds of overlap between the win-\ndows placed over the actual drop and the predicted\ndrop. These are computed for every detected drop in\nthe track.\n3. Compute the F1-score using the following equation:\nF1 =2tp\n2tp+fn+fp.\n4. Repeat the above steps for different sizes of t. We\nuse windows sizes of t = 15 sec, 13 sec, 11 sec, 9\nsec, 7 sec, 5 sec, 3 sec to compute the F1 score.\n5. If there is more than one drop in the track, repeat all\nthe above steps and compute an average F1-score for\neach size of the window t.\n4. EXPERIMENTS\nWe have proposed a classiﬁcation framework for detecting\ndrops in an EDM track. We use MIRToolbox [7] to ex-\ntract features related to spectrogram and MFCC, while we\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n145Hypothesized  \ndrop  Actual  \ndrop  \nt seconds  t seconds  \nfp tp fn Figure 5. Illustration to compute true positive (tp), false\npositive (fp) and false negative (fn) using a rectangular\nwindow of size tseconds.\nuse the source code provided by the authors of [8] to ex-\ntract features related to rhythm. We carry out feature selec-\ntion with a mechanism, adopted from [2], that uses support\nvector machines to identify the most informative features.\nFor the binary classiﬁcation of drop vs. non-drop, we use\na support vector machine classiﬁer provided in LibSVM.\nThe experiments have been designed to address the two re-\nsearch questions of this paper:\n\u000fCan our proposed approach detect drops success-\nfully? (Section 4.3), and\n\u000fWhat is the utility of the timed comments in the lim-\nited presence of explicit ground-truth data? (Sec-\ntion 4.4)\n4.1 Dataset\nIn order to evaluate our method, we collect music and so-\ncial data from SoundCloud, which can be seen as a rep-\nresentative of modern online social audio distribution plat-\nforms. It allows users to upload, record and share their self-\ncreated music. One of the unique features of SoundCloud\nis that it allows users to comment at particular time-points\nin the sound. These comments are referred to as “timed\ncomments”. Figure 6 illustrates a screenshot of the audio\nplayer on SoundCloud along with the timed comments.\nFigure 6. Screenshot of the audio player on SoundCloud.\nThese comments offer a rich source of information as\nthey are associated with a speciﬁc time-point and could in-\ndicate useful information about the sound difﬁcult to infer\nfrom the signal. Table 1 illustrates a few example timed\ncomments, which provide different kinds of information\nabout the sound. These timed comments can be noisy with\nrespect to their timestamps due to discrepancies between\nwhen users hear interesting events, and when they com-\nment on them.\nSoundCloud provides a well-documented API that can\nbe used to build applications using SoundCloud data and\ninformation on select social features. In order to collect\nour dataset, we used the Python SDK to search for re-Timestamp Comment\n01:21 Dunno what it is about this song, inspires me to\nmake more tunes though! love it!\n00:28 Love the rhythm!!\n00:49 love that drop! nice bassline! nice vocals! epic!\nTable 1. Examples of timed comments on SoundCloud.\ncent sounds belonging to the following three sub-genres\nof EDM: dubstep, electro andhouse. Using the returned\nlist of track identiﬁcation numbers, we download the track\n(if its allowed by the user who uploaded the sound) and\nthe corresponding timed comments. We then ﬁlter out the\ncomments which do not contain the word “drop”. At the\nend of the data collection process, we have a set of tracks\nbelonging to the above mentioned genres, the associated\ntimed comments containing the word “drop”, and the cor-\nresponding timestamp of the comment. Table 2 provides\nsome statistics of the dataset.\nGenre # ﬁles Aver.\nDurationAver. #\ncommentsAver. #\ndrop\ncommentsAver. #\ndrops\nDubstep 36 4 min. 278 4 3\nElectro 36 3.6 min. 220 3 3\nHouse 28 3.9 min. 250 5 2\nTable 2. Statistics of the dataset\nAs we have ﬁltered out the non-drop comments and all\nthe tracks in the dataset have at least one drop comment,\nwe can assume that there is at least one drop in each track.\nWe use a dataset of 100 tracks with a split of 60–20–20 for\nthe training, development and testing respectively.\n4.2 Ground-truth annotations\nAs we are developing a learning framework to detect drops\nin an EDM track, we need reference labels for the time-\npoints at which drops occur in our dataset, as mentioned\npreviously. We utilize two sources of information: explicit\nground-truth (high ﬁdelity labels) and implicit ground-truth\n(user comments). In order to obtain high ﬁdelity drop la-\nbels, one of the authors has listened to the 100 tracks and\nmanually marked the drop points. The labeled points re-\nfer to the point where the buildup ends and the bassline is\nre-introduced. Instead of listening to the entire track, the\nauthor skips 30 seconds after he hears a drop as it is highly\nunlikely that a second drop would occur within 30 seconds.\nIt took approximately 6 hours for the author to label the en-\ntire dataset. When computing F1-score in the experiments,\nwe use the manual labels as ground-truth.\nExplicit ground-truth labels are expensive as creating\nthem requires experts to spend time and effort to listen\nto the tracks and mark the drop points. Relying on ex-\nplicit ground-truth data also hampers the scalability of the\ndataset, as it would require much more time and effort from\nthe annotators for a larger dataset. Keeping with the social\nnature of SoundCloud, users contribute comments, some\nwhich remark on the pretense or quality of a drop (Table\n1). We investigate the possibility of using these timed com-\nments as weak reference labels in predicting the drop. We\nrefer to timed comments as weak reference labels owing\nto their noisy nature. For example, only 20 % of the drop\ncomments in the training set are located at the actual drop\nin a track. Note that we treat each comment as a distinct\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n146drop. We have a total of 190 drops and 225 drop comments\nin our dataset. As we can see, there are more comments\nthan the actual drops. Mapping multiple drop comments,\nwhich are nearer to each other, to a single time point is a\nconsideration for the future.\n4.3 Detecting drop using content-based features\nIn this experiment, we evaluate the performance of the con-\ntent based features in detecting a drop using the explicit\nground-truth labels. We compute the F1-score for each\ntrack separately. The F1-score is averaged if there is more\nthan one drop in the track. In Table 3, we report three\nresults: (1) F1-score, averaged across the entire dataset;\n(2) Highest F1-score for a single track and (3) Lowest F1-\nscore for a single track. As mentioned before, we use win-\ndows of sizes t = 3, 5, 7, 9, 11, 13, 15 sec. The size of the\nwindow (t) represents the temporal precision to which the\nF1-score is reported. Observing the results for the average\nperformance (ﬁrst row of Table 3), we achieve a maximum\nF1 score of 0.71 for a 15 second tolerance window. How-\never, we already achieve an F1 score greater than 0.6 for\na tolerance window as small as 3 seconds. The second\nrow of Table 3 illustrates the F1 scores for one single track\nwhich has the best drop detection and we observe that the\nF1 scores are high and go up to 0.96 for a 15 second tol-\nerance window. The third row of Table 3 illustrates the F1\nscores for one single track which has the worst drop de-\ntection and we observe that the F1 scores are very low, as\nit has more false positives. Moreover, the structure seg-\nment boundaries do not capture the drops particularly well\nin this track.\n4.4 Utility of timed comments\nTimed comments are an important source of information\nas they could indicate the time point where a drop occurs.\nFigure 7 illustrates a pipeline for the experiment to assess\nthe utility of timed comments as weak reference labels. It\nis carried out in three stages labeled as (1), (2) and (3) in\nthe ﬁgure. The stages are explained here. We divide the\ncomplete training set of Ntracks into two mutually ex-\nclusive sets of nandN\u0000ntracks. Assuming that the n\ntracks have ground-truth labels, we train a model (1) and\nuse it to classify the unlabeled segment boundaries from\ntheN\u0000ntracks. We segment boundaries labeled positive\nby the classiﬁer, which will be of low ﬁdelity, and add them\nit to the training data. In the second stage (2), we use the\nexpanded training data (n tracks + low ﬁdelity positive seg-\nment boundaries) to predict the drop segments in the test\nset and compute the F1 score for evaluation. Then, the fea-\ntures computed from a window sampled around user drop\ncomments are added to the training data. The data now\nincludes features from the ntracks, and low ﬁdelity pre-\ndicted positive segment boundaries, and around sampled\nat user comments. We use this data to train a model (3)\nand use it to predict the drop segments in the test set and\ncompute the F1 score for evaluation.\nIn this experiment, we use the following training data\nsizes which are expressed in terms of the number of tracks:\nHigh fidelity \ndrop labels  \n(n songs)  Train classifier  Test on  \n(N-n) songs  Positive drop \nsegments \n(Low fidelity) Test set  F1 score  \n(1) (1) (1) \n(1) (2) (2) (2) \n(3) (3) \nUser \ncomments  (3) Expert \nlabels  Figure 7. Procedure to assess the utility of timed com-\nments in detecting drop.\nn= 5; 10;20;30;40;50. F1 scores over different win-\ndow sizes are computed to demonstrate the drop detection\nperformance. Figure 8 illustrates the performance of the\nbinary classiﬁer when we have increasing sizes of train-\ning data. Due to space constraints, we illustrate the results\nonly for one size of the tolerance window: 11 seconds.\nDifference in F1 scores when we add user comments is vi-\nsualized in Figure 8. Inspecting the ﬁgure, we can say that\nF1 score  \nNumber of songs for training (tolerance window size 11 seconds)  \nFigure 8. F1 scores for combining high ﬁdelity ground-\ntruth labels and user comments for a tolerance window size\nof 11 seconds and different training set sizes: 5 tracks, 10\ntracks, 20 tracks, 30 tracks, 40 tracks, 50 tracks. First bar\nin each group indicates the results of stage (3) of the ex-\nperiment and the second bar indicates the F1 score for the\nstage (2) of the experiment\nreasonable F1 scores are obtained when we use n= 30\nandn= 40 tracks as training set and a tolerance window\nsize of 11 seconds. We observe that the F1 scores are lower\nthan with explicit ground-truth annotations, which we at-\ntribute to the noise of user comments.\n5. CONCLUSION AND OUTLOOK\nWe have proposed and evaluated content-based approach\nthat detects an important music event in EDM referred to\nas a drop. To this end, we have made use of music and user-\ncontributed timed-comments from an online social audio\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n1473 sec 5 sec 7 sec 9 sec 11 sec 13 sec 15 sec\nAverage Perfermance 0.61 0.62 0.66 0.66 0.68 0.69 0.71\nTrack with Best Performanc 0.83 0.9 0.92 0.94 0.95 0.96 0.96\nTrack with Worst Performance 0.2 0.36 0.43 0.47 0.49 0.51 0.52\nTable 3. Experimental results indicating the average, best and worst F1 scores for increasing window sizes\ndistribution platform: SoundCloud. We reported perfor-\nmance in terms of F1, using a tolerance window of varying\ntime resolutions around the reference drop time-points and\nthe drop time-points hypothesized by our approach. With a\ntolerance window of 5 seconds, which we estimate to be an\nacceptable size to listeners, we obtain an F1 score greater\nthan 0.6. “Timed-comments”, contributed by users in as-\nsociation with speciﬁc time-codes were demonstrated to be\nuseful as weak labels to supplement hand-labeled reference\ndata. We achieved a reasonable accuracy using a standard\nset of music related features. One of the future steps would\nbe to come up with a set of features which can model the\nvariability and the temporal structure during drop events,\nwhich will in turn improve the accuracy. We concentrated\non a subset of genres: dubstep, electro and house in this pa-\nper as these were the more popular genres on SoundCloud\n(in terms of number of comments). An immediate direc-\ntion would be to expand the current dataset by including\nvarious sub-genres of EDM, e.g., techno and drum & bass.\nOur work demonstrates that musical events in popu-\nlar electronic music can be successfully analyzed with the\nhelp of time-level social comments contributed by users\nin online social sharing platforms. This approach to mu-\nsic event detection opens up new vistas for future research.\nOur next step is to carry out a user study with our drop\ndetector aimed at discovering exactly how it can be of use\nto EDM artists and listeners. Such a study could also re-\nveal the source of “noise” in the timed comments, allowing\nus to understand why users often comment about drops in\nneighborhoods far from where an actual drop has occurred.\nThis information could in-turn allow us to identify the most\nuseful drop comments to add to our training data. Further,\nwe wish to widen our exploration of information sources\nthat could possibly support drop detection to also include\nMIDI ﬁles that are posted by users online together with the\naudio. Currently, the availability of these ﬁles is limited,\nbut we anticipate that they might be helpful for bootstrap-\nping. Another source of information is a crowdsourcing,\nwhich could be used to identify drops directly, or to ﬁlter\ncomments directly related to the drop, from less-closely\nrelated or unrelated comments.\n6. ACKNOWLEDGEMENT\nThis research is supported by funding from the European\nCommission’s 7th Framework Program under grant agree-\nment no. 610594 (CrowdRec) and 601166 (PHENICX).\n7. REFERENCES\n[1] M.J. Butler. Unlocking the Groove: Rhythm, Meter,\nand Musical Design in Electronic Dance Music. Pro-\nﬁles in popular music. Indiana University Press, 2006.[2] Yi-Wei Chen and Chih-Jen Lin. Combining SVMs\nwith various feature selection strategies. In Feature Ex-\ntraction, volume 207 of Studies in Fuzziness and Soft\nComputing, pages 315–324. Springer Berlin Heidel-\nberg, 2006.\n[3] Nick Collins. Inﬂuence in early electronic dance mu-\nsic: An audio content analysis investigation. In ISMIR,\npages 1–6, 2012.\n[4] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux,\nand Stephen Green. Automatic generation of social\ntags for music recommendation. In NIPS, 2007.\n[5] Jason Hockman, Matthew E. P. Davies, and Ichiro Fu-\njinaga. One in the jungle: Downbeat detection in hard-\ncore, jungle, and drum and bass. In ISMIR, pages 169–\n174, 2012.\n[6] Thor Kell and George Tzanetakis. Empirical analy-\nsis of track selection and ordering in electronic dance\nmusic using audio feature extraction. In ISMIR, pages\n505–510, 2013.\n[7] Olivier Lartillot and Petri Toiviainen. Mir in matlab\n(ii): A toolbox for musical feature extraction from au-\ndio. In ISMIR, pages 127–130, 2007.\n[8] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiﬁcation. In ISMIR, pages 34–41,\n2005.\n[9] Erik M. Schmidt and Youngmoo E. Kim. Modeling\nmusical emotion dynamics with conditional random\nﬁelds. In ISMIR, pages 777–782, 2011.\n[10] Joan Serr `a, Meinard M ¨uller, Peter Grosche, and\nJosep LLuis Arcos. Unsupervised music structure an-\nnotation by time series structure features and seg-\nment similarity. IEEE Transactions on Multimedia ,\nPP(99):1–1, 2014.\n[11] Yading Song, Simon Dixon, Marcus Pearce, and An-\ndrea R. Halpern. Do online social tags predict per-\nceived or induced emotional responses to music? In\nISMIR, pages 89–94, 2013.\n[12] John Steventon. DJing for Dummies. –For dummies.\nWiley, 2007.\n[13] Yi-Hsuan Yang, Dmitry Bogdanov, Perfecto Herrera,\nand Mohamed Sordo. Music retagging using label\npropagation and robust principal component analysis.\nInWWW, pages 869–876, 2012.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n148"
    },
    {
        "title": "Bayesian Singing-Voice Separation.",
        "author": [
            "Po-Kai Yang",
            "Chung-Chien Hsu",
            "Jen-Tzung Chien"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417373",
        "url": "https://doi.org/10.5281/zenodo.1417373",
        "ee": "https://zenodo.org/records/1417373/files/YangHC14.pdf",
        "abstract": "This paper presents a Bayesian nonnegative matrix fac- torization (NMF) approach to extract singing voice from",
        "zenodo_id": 1417373,
        "dblp_key": "conf/ismir/YangHC14",
        "keywords": [
            "Bayesian",
            "nonnegative",
            "matrix",
            "factorization",
            "Singing",
            "voice",
            "extract",
            "fac- torization",
            "NMF",
            "approach"
        ],
        "content": "BAYESIAN SINGING-VOICE SEPARATION\nPo-Kai Yang, Chung-Chien Hsu and Jen-Tzung Chien\nDepartment of Electrical and Computer Engineering, National Chiao Tung University, Taiwan\nfniceallen.cm01g, chien.cm97g, jtchieng@nctu.edu.tw\nABSTRACT\nThis paper presents a Bayesian nonnegative matrix fac-\ntorization (NMF) approach to extract singing voice from\nbackground music accompaniment. Using this approach,\nthe likelihood function based on NMF is represented by\na Poisson distribution and the NMF parameters, consist-\ning of basis and weight matrices, are characterized by the\nexponential priors. A variational Bayesian expectation-\nmaximization algorithm is developed to learn variational\nparameters and model parameters for monaural source sep-\naration. A clustering algorithm is performed to establish\ntwo groups of bases: one is for singing voice and the other\nis for background music. Model complexity is controlled\nby adaptively selecting the number of bases for different\nmixed signals according to the variational lower bound.\nModel regularization is tackled through the uncertainty\nmodeling via variational inference based on marginal like-\nlihood. The experimental results on MIR-1K database\nshow that the proposed method performs better than var-\nious unsupervised separation algorithms in terms of the\nglobal normalized source to distortion ratio.\n1. INTRODUCTION\nSinging voice conveys important information of a song.\nThis information is practical for many music-related ap-\nplications including singer identiﬁcation [11], music emo-\ntion annotation [21], melody extraction, lyric recognition\nand lyric synchronization [6]. However, singing voice is\nusually mixed with background accompaniment in a mu-\nsic signal. How to extract the singing voice from a single-\nchannel mixed signal is known as a crucial issue for mu-\nsic information retrieval. Some approaches have been pro-\nposed to deal with single-channel singing-voice separation.\nThere are two categories of approaches to source sep-\naration: supervised learning [2] and unsupervised learn-\ning [8,9,13,22]. Supervised approach conducts the single-\nchannel source separation given by the labeled training\ndata from different sources. In the application of singing-\nvoice separation, the separate training data of singing voice\nc\rPo-Kai Yang, Chung-Chien Hsu and Jen-Tzung Chien.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Po-Kai Yang, Chung-Chien Hsu\nand Jen-Tzung Chien. “BAYESIAN SINGING-VOICE SEPARATION”,\n15th International Society for Music Information Retrieval Conference,\n2014.and background music should be collected. But, it is\nmore practical to conduct the unsupervised learning for\nblind source separation by using only the mixed test data.\nIn [13], the repeating structure of the spectrogram of the\nmixed music signal was extracted and applied for sep-\naration of music and voice. The repeating components\nfrom accompaniment signal were separated from the non-\nrepeating components from vocal signal. A binary time-\nfrequency masking was applied to identify the repeating\nbackground accompaniment. In [9], a robust principal\ncomponent analysis was proposed to decompose the spec-\ntrogram of mixed signal into a low-rank matrix for accom-\npaniment signal and a sparse matrix for vocal signal. Sys-\ntem performance was improved by imposing the harmonic-\nity constraints [22]. A pitch extraction algorithm was in-\nspired by the computational auditory scene analysis [3] and\nwas applied to extract the harmonic components of singing\nvoice.\nIn general, the issue of singing-voice separation is seen\nas a single-channel source separation problem which could\nbe solved by using the learning approach based on the\nnonnegative matrix factorization (NMF) [10, 19]. Using\nNMF, a nonnegative matrix is factorized into a product\nof a basis matrix and a weight matrix which are nonneg-\native [10]. NMF can be directly applied in Fourier spec-\ntrogram domain for audio signal processing. In [7], the\nnonnegative sparse coding was proposed to conduct sparse\nlearning for overcomplete representation based on NMF.\nSuch sparse coding provides efﬁcient and robust solution\nto NMF. However, how to determine the regularization pa-\nrameter for sparse representation is a key issue for NMF. In\naddition, the time-varying envelopes of spectrogram con-\nvey important information. In [16], one dimensional con-\nvolutive NMF was proposed to extract the bases, which\nconsidered the dependencies across successive columns of\ninput spectrogram, and was applied for supervised single-\nchannel speech separation. In [14], two dimensional NMF\nwas proposed to discover fundamental bases for blind mu-\nsical instrument separation in presence of harmonic varia-\ntions from piano and trumpet. Number of bases was empir-\nically determined. Nevertheless, the selection of the num-\nber of bases is known as a model selection problem in sig-\nnal processing and machine learning. How to tackle this\nregularization issue plays an important role to assure gen-\neralization for future data in ill-posed condition [1].\nBasically, uncertainty modeling via probabilistic frame-\nwork is helpful to improve model regularization for NMF.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n507The uncertainties in singing-voice separation may come\nfrom improper model assumption, incorrect model order\nand possible noise interference, nonstationary environ-\nment, reverberant distortion. Under probabilistic frame-\nwork, nonnegative spectral signals are drawn from proba-\nbility distributions. The nonnegative parameters are also\nrepresented by prior distributions. Bayesian learning is in-\ntroduced to deal with uncertainty decoding and build a ro-\nbust source separation by maximizing the marginal likeli-\nhood over the randomness of model parameters. In [15],\nBayesian NMF (BNMF) was proposed for image feature\nextraction based on the assumption of Gaussian likelihood\nand exponential prior. In the BNMF [4], an approximate\nBayesian inference based on variational Bayesian (VB) al-\ngorithm using Poisson likelihood for observation data and\nGamma prior for model parameters was proposed for im-\nage reconstruction. Implementation cost was demanding\ndue to the numerical calculation of shape parameter. Al-\nthough NMF was presented for singing-voice separation\nin [19, 23], the regularization issue was ignored and the\nsensitivity of system performance due to uncertain model\nand ill-posed condition was serious.\nThis paper presents a new model-based singing-voice\nseparation. The novelties of this paper are twofold. The\nﬁrst one is to develop Bayesian approach to unsupervised\nsinging-voice separation. Model uncertainty is compen-\nsated to improve the performance of source separation of\nvocal signal and background accompaniment signal. Num-\nber of bases is adaptively determined from the mixed signal\naccording to the variational lower bound of the logarithm\nof a marginal likelihood over NMF basis and weight ma-\ntrices. The second one is the theoretical contribution in\nBayesian NMF. We construct a new Bayesian NMF where\nthe likelihood function in NMF is drawn from Poisson dis-\ntribution and the model parameters are characterized by ex-\nponential distributions. A closed-form solution to hyperpa-\nrameters using the VB expectation-maximization (EM) [5]\nalgorithm is derived for ease of implementation and com-\nputation. This BNMF is connected to standard NMF with\nsparseness constraint. But, using the BNMF, the regular-\nization parameters or hyperparameters are optimally esti-\nmated from training data without empirical selection from\nvalidation data. Beyond the approaches in [4, 15], the pro-\nposed BNMF completely considers the dependencies of\nthe variational objective on hyperparameters and derives\nthe analytical solution to singing-voice separation.\n2. NONNEGATIVE MATRIX FACTORIZATION\nLee and Seung [10] proposed the standard NMF where no\nprobabilistic distribution was assumed. Given a nonnega-\ntive data matrix X2RM\u0002N\n+ , NMF aims to decompose\ndata matrix Xinto a product of two nonnegative matrices\nB2RM\u0002K\n+ andW2RK\u0002N\n+ . The (m;n)-th entry of X\nis approximated by Xmn\u0019[BW ]mn=P\nkBmkWkn.\nNMF parameters \u0002 =fB;Wgconsist of basis matrix B\nand weight matrix W. The approximation based on NMF\nis optimized by minimizing the Kullback-Leibler (KL) di-\nvergenceDKL(XkBW)between the observed data Xandthe approximated data BW\nX\nm;n(XmnlogXmn\n[BW ]mn+ [BW]mn\u0000Xmn) (1)\n2.1 Maximum Likelihood Factorization\nNMF approximation is revisited by introducing the prob-\nabilistic framework based on maximum likelihood (ML)\ntheory. The nonnegative latent variable Zmkn is embedded\nin data entry XmnbyXmn=P\nkZmkn and is repre-\nsented by a Poisson distribution with mean BmkWkn, i.e.\nZmkn\u0018Pois(Zmkn;BmkWkn)[4]. Log likelihood func-\ntion of data matrix Xgiven parameters \u0002is expressed by\nlogp(XjB; W) = logY\nm;nPois(X mn;X\nkBmkWkn)\n=X\nm;n(Xmnlog[BW ]mn\u0000[BW ]mn\u0000log \u0000(Xmn+ 1))(2)\nwhere \u0000(\u0001) is the gamma function. Maximizing the log\nlikelihood function in Eq. (2) based on Poisson distribution\nis equivalent to minimizing the KL divergence between X\nandBW in Eq. (1). This ML problem with missing vari-\nablesZ=fZmkngcan be solved according to EM algo-\nrithm. In E step, the expectation function of the log likeli-\nhood of data Xand latent variable Zgiven new parameters\nB(\u001c+1)andW(\u001c+1)is calculated with respect to Zunder\ncurrent parameters B(\u001c)andW(\u001c). In M step, we maxi-\nmize the resulting auxiliary function to obtain the updating\nof NMF parameters which is equivalent to that of standard\nNMF in [10].\n2.2 Bayesian Factorization\nML estimation is prone to ﬁnd an over-trained model [1].\nTo improve model regularization, Bayesian approach is in-\ntroduced to establish NMF for single-source separation.\nML NMF was improved by considering the priors of ba-\nsis matrix Band weight matrix Wfor Bayesian NMF\n(BNMF). Different speciﬁcations of likelihood function\nand prior distribution result in different solutions with dif-\nferent inference procedures. In [15], the approximation\nerror ofXmnusingP\nkBmkWknis modeled by a zero-\nmean Gaussian distribution\nXmn\u0018N(Xmn;X\nkBmkWkn;\u001b2) (3)\nwith the variance parameter \u001b2which is distributed by an\ninverse gamma prior. The priors of nonnegative Bmkand\nWknare modeled by the exponential distributions\nBmk\u0018Exp(B mk;\u0015b\nmk); W kn\u0018Exp(W kn;\u0015w\nkn) (4)\nwhere Exp(x; \u0012) =\u0012exp(\u0000\u0012x), with means (\u0015b\nmk)\u00001and\n(\u0015w\nkn)\u00001, respectively. Typically, the larger the exponen-\ntial hyperparameter \u0012is involved, the sparser the expo-\nnential distribution is shaped. The sparsity of basis pa-\nrameterBmkand weight parameter Wknis controlled by\nhyperparameters \u0015b\nmkand\u0015w\nkn, respectively. In [15], the\nhyperparametersf\u0015b\nmk;\u0015w\nkngwere ﬁxed and empirically\ndetermined. The Gaussian likelihood does not adhere to\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n508the assumption of nonnegative data matrix X. The other\nweakness in the BNMF [15] is that the exponential dis-\ntribution is not conjugate prior to the Gaussian likelihood\nfunction for NMF. There was no closed-form solution. The\nparameters \u0002 =fB;W;\u001b2gwere accordingly estimated\nby Gibbs sampling procedure where a sequence of poste-\nrior samples of \u0002was drawn by the corresponding condi-\ntional posterior probabilities.\nCemgil [4] proposed the BNMF for image reconstruc-\ntion based on the Poisson likelihood function as given in\nEq. (2) and the gamma priors for basis and weight matri-\nces. The gamma distribution, represented by a shape pa-\nrameter and a scale parameter, is known as the conjugate\nprior to Poisson likelihood function. Variational Bayesian\n(VB) inference procedure was developed for NMF im-\nplementation. However, the shape parameter was imple-\nmented by the numerical solution. The computation cost\nwas relatively high. Some dependencies of variational\nlower bound on model parameters were ignored in [4]. The\nresulting parameters did not reach true optimum of varia-\ntional objective.\n3. NEW BAYESIAN FACTORIZATION\nThis study aims to ﬁnd an analytical solution to full\nBayesian NMF by considering all dependencies of varia-\ntional lower bound on regularization parameters. Regular-\nization parameters are optimally estimated.\n3.1 Bayesian Objectives\nIn accordance with the Bayesian perspective and the spirit\nof standard NMF, we adopt the Poisson distribution as like-\nlihood function and the exponential distribution as conju-\ngate prior for NMF parameters BmkandWknwith hyper-\nparameters\u0015b\nmkand\u0015w\nkn, respectively. Maximum a pos-\nteriori (MAP) estimates of parameters \u0002 =fB;Wgare\nobtained by maximizing the posterior distribution or min-\nimizing\u0000logp(B;WjX)which is arranged as a regular-\nized KL divergence between XandBW\nDKL(XjjBW ) +X\nm;k\u0015b\nmkBmk+X\nk;n\u0015w\nknWkn (5)\nwhere the terms independent of BmkandWknare treated\nas constants. Notably, the regularization terms (2nd and\n3rd terms) in this objective are nonnegative and seen as the\n`1regularizers [18] which are controlled by hyperparame-\ntersf\u0015b\nmk;\u0015w\nkng. These regularizers impose sparseness in\nthe estimated MAP parameters.\nHowever, MAP estimates are seen as point estimates.\nThe randomness of parameters is not considered in model\nconstruction. To conduct full Bayesian treatment, BNMF\nis developed by maximizing the marginal likelihood\np(Xj\u0002) over latent variables Zas well as NMF parame-\ntersfB;Wg\nZX\nZp(XjZ; B;W)p(ZjB; W)p(B;Wj\u0002)dBdW (6)\nand estimating the sparsity-controlled hyperparameters or\nregularization parameters \u0002 =f\u0015b\nmk;\u0015w\nmkg. The resultingevidence function is meaningful to act as an objective for\nmodel selection which balances the tradeoff between data\nﬁtness and model complexity [1]. In the singing-voice sep-\naration based on NMF, this objective is used to judge which\nnumber of bases Kshould be selected. The selected num-\nber is adaptive to ﬁt different experimental conditions with\nvarying lengths and the variations from different singers,\ngenders, songs, genres, instruments and music accompani-\nments. Model regularization is tackled accordingly. But,\nusing NMF without Bayesian treatment, the number of\nbases was ﬁxed and empirically determined.\n3.2 Variational Bayesian Inference\nThe exact Bayesian solution to optimization problem in\nEq. (6) does not exist because the posterior probability of\nthree latent variables fZ;B;Wggiven the observed mix-\nturesXcould not be factorized. To deal with this issue, the\nvariational Bayesian expectation-maximization (VB-EM)\nalgorithm is developed to implement Poisson-Exponential\nBNMF. VB-EM algorithm applies the Jensen’s inequal-\nity and maximizes the lower bound of the logarithm of\nmarginal likelihood\nlogp(Xj\u0002)\u0015ZX\nZq(Z;B;W) logp(X;Z;B;Wj\u0002)\nq(Z;B;W)\n\u0002dBdW=Eq[logp(X;Z;B;Wj\u0002)] +H[q(Z;B;W)](7)\nwhereH[\u0001]is an entropy function. The factorized vari-\national distribution q(Z;B;W) =q(Z)q(B)q(W)is\nassumed to approximate the true posterior distribution\np(Z;B;WjX;\u0002).\n3.2.1 VB-E Step\nIn VB-E step, a general solution to variational distribution\nqjof an individual latent variable j2fZ; B;Wgis ob-\ntained by [1]\nlog ^qj/Eq(i6=j )[logp(X;Z;B;Wj\u0002)]: (8)\nGiven the variational distributions deﬁned by\nq(Bmk) =Gam(B mk;\u000bb\nmk;\fb\nmk)\nq(Wkn) =Gam(W kn;\u000bw\nkn;\fw\nkn)\nq(Zmkn) =Mult(Z mkn;Pmkn)(9)\nthe variational parameters f\u000bb\nmk;\fb\nmk;\u000bw\nkn;\fw\nkn;Pmkngin\nthree distributions are estimated by\n^\u000bb\nmk= 1 +X\nnhZmkni;^\fb\nmk= X\nnhWkni+\u0015b\nmk!\u00001\n^\u000bw\nkn= 1 +X\nmhZmkni;^\fw\nkn= X\nkhBmki+\u0015w\nkn!\u00001\n^Pmkn =exp(hlogBmki+hlogWkni)P\njexp(hlogBmji+hlogWjni)(10)\nwhere the expectation function Eq[\u0001]is replaced byh\u0001ifor\nsimplicity. By substituting the variational distribution into\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n509Eq. (7), the variational lower bound is obtained by\nBL=\u0000X\nm;n;khBmkihWkni\n+X\nm;n(\u0000log \u0000(Xmn+ 1)\u0000X\nkhZmknilog^Pmkn)\n+X\nm;khlogBmkiX\nnhZmkni+X\nk;nhlogWkniX\nmhZmkni\n+X\nm;k(log\u0015b\nmk\u0000\u0015b\nmkhBmki) +X\nk;n(log\u0015w\nkn\u0000\u0015w\nknhWkni)\n+X\nm;k(\u0000(^\u000bb\nmk\u00001)\t(^\u000bb\nmk) + log ^\fb\nmk+ ^\u000bb\nmk+ log \u0000(^\u000bb\nmk))\n+X\nk;n(\u0000(^\u000bw\nkn\u00001)\t(^\u000bw\nkn) + log ^\fw\nkn+ ^\u000bw\nkn+ log \u0000(^\u000bw\nkn))\n(11)\nwhere \t(\u0001) is the derivative of the log gamma function,\nand is known as a digamma function.\n3.2.2 VB-M Step\nIn VB-M step, the optimal regularization parameters \u0002 =\nf\u0015b\nmk;\u0015w\nkngare derived by maximizing Eq. (11) with re-\nspect to \u0002and yielding\n@BL\n@\u0015b\nmk=1\n\u0015b\nmk\u0000hBmki+@log\fb\nmk\n@\u0015b\nmk= 0\n@BL\n@\u0015w\nkn=1\n\u0015w\nkn\u0000hWkni+@log\fw\nkn\n@\u0015w\nkn= 0:(12)\nAccordingly, the solution to BNMF hyperparameters is de-\nrived by solving a quadratic equation where nonnegative\nconstraint is considered to ﬁnd positive values of hyperpa-\nrameters by\n^\u0015b\nmk=1\n2 \n\u0000X\nnhWkni+s\n(X\nnhWkni)2+ 4P\nnhWkni\nhBmki!\n^\u0015w\nkn=1\n2 \n\u0000X\nmhBmki+s\n(X\nmhBmki)2+ 4P\nmhBmki\nhWkni!\n(13)\nwherehBmki=\u000bb\nmk\fb\nmkandhWkni=\u000bw\nkn\fw\nknare ob-\ntained as the means of gamma distributions. VB-E step\nand VB-M step are alternatively and iteratively performed\nto estimate BNMF parameters \u0002with convergence. It is\nmeaningful to select the best number of bases (K ) with the\nlargest lower bound of the log marginal likelihood which\nintegrates out the parameters of weight and basis matrices.\n3.3 Poisson-Exponential Bayesian NMF\nTo the best of our knowledge, this is the ﬁrst study where a\nBayesian approach is developed for singing-voice separa-\ntion. The uncertainties in singing-voice separation due to\na variety of singers, songs and instruments could be com-\npensated. Model selection problem is tackled as well. In\nthis study, total number of basis vectors Kis adaptively\nselected for individual mixed signal according to the vari-\national lower bound in Eq. (11) with the converged varia-\ntional parametersf^\u000bb\nmk;^\fb\nmk;^\u000bw\nkn;^\fw\nkn;^Pmkngand model\nparametersf^\u0015b\nmk;^\u0015w\nkng.\nConsidering the pairs of likelihood function and prior\ndistribution in NMF, the proposed method is also called\nthe Poisson-Exponential BNMF which is different fromthe Gaussian-Exponential BNMF in [15] and the Poisson-\nGamma BNMF in [4]. The superiorities of the proposed\nmethod to the BNMFs in [15, 4] are twofold. First, as-\nsuming the exponential priors provides a BNMF approach\nwith tractable solution as given in Eq. (13). Gibbs sam-\npling in [15] and Newton’s solution in [4] are computation-\nally expensive. Second, the dependencies of three terms of\nthe variational lower bound in Eq. (11) on hyperparame-\nters\u0015b\nmkor\u0015w\nknare all considered in ﬁnding the true op-\ntimum while some dependencies were ignored in the solu-\ntion to Poisson-Gamma BNMF [4]. Also, the observations\nin Gaussian-Exponential BNMF [15] were not constrained\nto be nonnegative.\n4. EXPERIMENTS\n4.1 Experimental Setup\nWe used the MIR-1Kdataset [8] to evaluate the proposed\nmethod for unsupervised singing-voice separation from\nbackground music accompaniment. The dataset consisted\nof 1000 song clips extracted from 110 Chinese karaoke pop\nsongs performed by 8 female and 11 male amateurs. Each\nclip recorded at 16 KHz sampling frequency with the dura-\ntion ranging from 4 to 13 seconds. Since the music accom-\npaniment and the singing voice were recorded at left and\nright channels, we followed [8, 9, 13] and simulated three\ndifferent sets of monaural mixtures at signal-to-music-\nratios (SMRs) of 5, 0, and -5 dB where the singing-voice\nwas treated as signal and the accompaniment was treated\nas music. The separation problem was tackled in the short-\ntime Fourier transform (STFT) domain. The 1024-point\nSTFT was calculated to obtain the Fourier magnitude spec-\ntrograms with frame duration of 40 ms and frame shift of\n10 ms. In the implementation of BNMF, ML-NMF was\nadopted as the initialization and 50 iterations were run to\nﬁnd the posterior means of basis and weight parameters.\nTo evaluate the performance of singing-voice separation,\nwe measure the signal-to-distortion ratio (SDR) [20] and\nthen calculate the normalized SDR (NSDR) and the global\nNSDR (GNSDR) as\nNSDR( ^V;V;X) = SDR( ^V;V)\u0000SDR(X; V)\nGNSDR( ^V;V;X) =P~N\nn=1lnNSDR( ^Vn;Vn;Xn)\nP~N\nn=1ln(14)\nwhere ^V;V;Xdenote the estimated singing voice, the\noriginal clean singing voice, and the mixture signal, re-\nspectively, ~Nis the total number of the clips and lnis the\nlength of the nth clip. NSDR is used to measure the im-\nprovement of SDR between the estimated singing voice ^V\nand the mixture signal X. GNSDR is used to calculate\nthe overall separation performance by taking the weighted\nmean of the NSDRs.\n4.2 Unsupervised Singing-Voice Separation\nWe implemented the unsupervised singing-voice separa-\ntion where total number of bases (K ) and the grouping of\nthese bases into vocal source and music source were both\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n510HsuHuang YangRafii[12]Rafii[13] BNMF1BNMF2−101234\n−0.511.512.1\n1.32.42.813.38SMR:−5dBGNSDR(dB)\nHsuHuang YangRafii[12]Rafii[13] BNMF1BNMF201234\n0.912.372.76\n1.72.72.923.25SMR:0dBGNSDR(dB)\nHsuHuang YangRafii[12]Rafii[13] BNMF1BNMF201234\n0.172.57 2.58\n1.32.12.122.57SMR:5dBGNSDR(dB)Figure 1. Performance comparison using BNMF1 (K-\nmeans clustering) and BNMF2 (NMF-clustering) and ﬁve\ncompetitive methods (Hsu [8], Huang [9], Yang [22], Raﬁi\n[12], Raﬁi [13]) in terms of GNSDR under various SMRs.\nlearned from test data in an unsupervised way. No training\ndata were required. Model complexity based on Kwas de-\ntermined in accordance with the variational lower bound of\nlog marginal likelihood in Eq. (11) while the grouping of\nbases for two sources was simply performed via the clus-\ntering algorithms using the estimated basis vectors in B\nor equivalently from the estimated variational parameters\nf\u000bb\nmk;\fb\nmkg. Following [17], we conducted the K-means\nclustering algorithm based on the basis vectors Bin Mel-\nfrequency cepstral coefﬁcient (MFCC) domain. Each basis\nvector was ﬁrst transformed to the Mel-scaled spectrum by\napplying 20 overlapping triangle ﬁlters spaced on the Mel\nscale. Then, we took the logarithm and applied the discrete\ncosine transform to obtain nine MFCCs. Finally, we nor-\nmalized each coefﬁcient to zero mean and unit variance.\nThe K-means clustering algorithm was applied to partition\nthe feature set into two clusters through an iterative pro-\ncedure until convergence. However, it is more meaningful\nto conduct NMF-based clustering for the proposed BNMF\nmethod. To do so, we transformed the basis vectors Binto\nMel-scaled spectrum to form the Mel-scaled basis matrix.\nML-NMF was applied to factorize this Mel-scaled basis\nmatrix into two matrices ~Bof sizeN-by-2 and ~Wof size\n2-by-K . The soft mask scheme based on Wiener gain was\napplied to smooth the separation of Binto basis vectors\nfor vocal signal and music signal. This same soft mask\nwas performed for the separation of mixed signal Xinto\nvocal signal and music signal based on the K-means clus-\ntering and NMF clustering. Finally, the separated singing\nvoice and music accompaniment signals were obtained by\nthe overlap-and-add method using the original phase.NMF NMF NMF BNMF\n(30) (40) (50) (adaptive)\nK-means clustering 2.69 2.58 2.47 2.92\nNMF clustering 3.15 3.13 2.97 3.25\nTable 1. Comparison of GNSDR at SMR = 0 dB using\nNMF with ﬁxed number of bases f30, 40, 50g and BNMF\nwith adaptive number of bases.\nFigure 2. Histogram of the selected number of bases using\nBNMF under various SMRs.\n4.3 Experimental Results\nThe unsupervised single-channel separation using BNMFs\n(BNMF1 using K-means clustering and BNMF2 using\nNMF clustering) and the other ﬁve competitive systems\n(Hsu [8], Huang [9], Yang [22], Raﬁi [12], Raﬁi [13])\nis compared in terms of GNSDR as depicted in Figure\n1. Using K-means clustering in MFCC domain, the re-\nsulting BNMF1 outperforms the other ﬁve methods under\nSMRs of 0 dB and -5 dB while the results using Huang [9]\nand Yang [22] perform better than BNMF1 under 5 dB\ncondition. This is because the methods in [9, 22] used\nadditional pre- and/or post-processing techniques as pro-\nvided in [13, 22] which were not applied in BNMF1 and\nBNMF2. Nevertheless, using BNMF factorization with\nNMF clustering (BNMF2), the overall evaluation consis-\ntently achieves around 0.33\u00180.57 dB relative improvement\nin GNSDR compared with BNMF1 including the SMR\ncondition at 5dB. In addition, we evaluate the effect on the\nadaptive basis selection using BNMF. Table 1 reports the\ncomparison of BNMF1 and BNMF2 with adaptive basis\nselection and ML-NMF with ﬁxed number of bases under\nSMR of 0 dB. Two clustering methods were also carried\nout for NMF with different K. BNMF factorization com-\nbined with NMF clustering achieves the best performance\nin this comparison. Figure 2 shows the histogram of the\nselected number of bases Kusing BNMF. It is obvious\nthat this adaptive basis selection plays an important role to\nﬁnd suitable amount of bases to ﬁt different experimental\nconditions.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n5115. CONCLUSIONS\nWe proposed a new unsupervised Bayesian nonnegative\nmatrix factorization approach to extract the singing voice\nfrom background music accompaniment and illustrated the\nnovelty on an analytical and true optimum solution to the\nPoisson-Exponential BNMF. Through the VB-EM infer-\nence procedure, the proposed method automatically se-\nlected different number of bases to ﬁt various experimen-\ntal conditions. We conducted two clustering algorithms to\nﬁnd the grouping of bases into vocal and music sources.\nExperimental results showed the consistent improvement\nof using BNMF factorization with NMF clustering over\nthe other singing-voice separation methods in terms of\nGNSDR. In future works, the proposed BNMF shall be\nextended to multi-layer source separation and applied to\ndetect unknown number of sources.\n6. REFERENCES\n[1] C. M. Bishop. Pattern Recognition and Machine\nLearning. Springer Science, 2006.\n[2] N. Boulanger-Lewandowski, G. J. Mysore, and\nM. Hoffman. Exploiting long-term temporal depen-\ndencies in NMF using recurrent neural networks with\napplication to source separation. In Proc. of ICASSP,\npages 337–344, 2014.\n[3] A. S. Bregman. Auditory Scene Analysis: the Percep-\ntual Organization of Sound. MIT Press, 1990.\n[4] A. T. Cemgil. Bayesian inference for nonnegative ma-\ntrix factorisation models. Computational Intelligence\nand Neuroscience, (Article ID 785152), 2009.\n[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-\nimum likelihood from incomplete data via the em al-\ngorithm. Journal of the Royal Statistical Society (B),\n39(1):1–38, 1977.\n[6] H. Fujihara, M. Goto, J. Ogata, and H. G. Okuno.\nLyricsynchronizer: automatic synchronization system\nbetween musical audio signals and lyrics. IEEE Jour-\nnal of Selected Topics in Signal Processing, 5(6):1252–\n1261, 2011.\n[7] P. O. Hoyer. Non-negative matrix factorization with\nsparseness constraints. The Journal of Machine Learn-\ning Research, 5:1457–1469, 2004.\n[8] C.-L. Hsu and J.-S. R. Jang. On the improvement of\nsinging voice separation for monaural recordings us-\ning the MIR-1K dataset. IEEE Transactions on Audio,\nSpeech, Language Processing, 18(2):310–319, 2010.\n[9] P.-S. Huang, S. D. Chen, P. Smaragdis, and\nM. Hasegawa-Johnson. Singing-voice separation from\nmonaural recordings using robust principal component\nanalysis. In Proc. of ICASSP, pages 57–60, 2012.[10] D. D. Lee and H. S. Seung. Algorithms for non-\nnegative matrix factorization. Advances in Neural In-\nformation Processing Systems, pages 556–562, 2000.\n[11] A. Mesaros, T. Virtanen, and A. Klapuri. Singer iden-\ntiﬁcation in polyphonic music using vocal separation\nand pattern recognition methods. In Proc. of Annual\nConference of International Society for Music Infor-\nmation Retrieval, pages 375–378, 2007.\n[12] Z. Raﬁi and B. Pardo. A simple music/voice separa-\ntion method based on the extraction of the repeating\nmusical structure. In Proc. of ICASSP, pages 221–224,\n2011.\n[13] Z. Raﬁi and B. Pardo. Repeating pattern extraction\ntechnique (REPET): A simple method for music/voice\nseparation. IEEE Transactions on Audio, Speech, Lan-\nguage Processing, 21(1):73–84, Jan. 2013.\n[14] M. N. Schmidt and M. Morup. Non-negative ma-\ntrix factor 2-D deconvolution for blind single chan-\nnel source separation. In Proc. of ICA, pages 700–707,\n2006.\n[15] M. N. Schmidt, O. Winther, and L. K. Hansen.\nBayesian non-negative matrix factorization. In Proc. of\nICA, pages 540–547, 2009.\n[16] P. Smaragdis. Convolutive speech bases and their\napplication to speech separation. IEEE Transactions\non Audio, Speech, Language Processing, 15(1):1–12,\n2007.\n[17] M. Spiertz and V . Gnann. Source-Filter based clus-\ntering for monaural blind source separation. In Proc.\nof International Conference on Digital Audio Effects,\npages 1–4, 2009.\n[18] R. Tibshirani. Regression shrinkage and selection via\nthe lasso. Journal of the Royal Statistical Society. Se-\nries B, 58(1):267–288, 1996.\n[19] S. Vembu and S. Baumann. Separation of vocals from\npolyphonic audio recordings. In Proc. of ISMIR, pages\n375–378, 2005.\n[20] E. Vincent, R. Gribonval, and C. Fevotte. Performance\nmeasurement in blind audio source separation. IEEE\nTransaction on Audio, Speech and Language Process-\ning, 14(4):1462–1469, 2006.\n[21] D. Yang and W. Lee. Disambiguating music emotion\nusing software agents. In Proc. of ISMIR, pages 52–57,\n2004.\n[22] Y .-H. Yang. On sparse and low-rank matrix decompo-\nsition for singing voice separation. In Proc. of ACM\nInternational Conference on Multimedia, pages 757–\n760, 2012.\n[23] B. Zhu, W. Li, R. Li, and X. Xue. Multi-stage non-\nnegative matrix factorization for monaural singing\nvoice separation. IEEE Transactions on Audio, Speech,\nLanguage Processing, 21(10):2096–2107, 2013.\n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n512"
    },
    {
        "title": "Singing Voice Separation Using Spectro-Temporal Modulation Features.",
        "author": [
            "Frederick Z. Yen",
            "Yin-Jyun Luo",
            "Tai-Shih Chi"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1417695",
        "url": "https://doi.org/10.5281/zenodo.1417695",
        "ee": "https://zenodo.org/records/1417695/files/YenLC14.pdf",
        "abstract": "An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational audito- ry scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time- frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three catego- ries in music audio signals: vocal, harmonic, and percus- sive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algo- rithm was tested using the MIR-1K dataset and demon- strated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures.",
        "zenodo_id": 1417695,
        "dblp_key": "conf/ismir/YenLC14",
        "keywords": [
            "auditory-perception",
            "singing voice separation",
            "monaural music recordings",
            "computational auditory scene analysis",
            "spectrograms",
            "spectral-temporal modulation",
            "music audio signals",
            "vocal category",
            "time-frequency masking",
            "MIR-1K dataset"
        ],
        "content": "SINGING VOICE SEPARATION USING                       \nSP\nECTRO-TEMPORAL MODULATION FEATURES \nFrederick Yen        Yin-Jyun Luo                                          Tai-Shih Chi \nMaster Program of SMIT \n National Chiao-Tung University, Taiwan  \n{fredyen.smt01g,fredom.smt02g} \n@\nnctu.edu.tw     Dept. of Elec. & Comp. Engineering \n N\national Chiao-Tung University, Taiwan  \ntschi@mail.nctu.edu.tw  \nABSTRACT \nA\nn auditory-perception inspired singing voice separation \nalgorithm for monaural music recordings is proposed in \nthis paper. Under the framework of computational audito-\nry scene analysis (CASA), the music recordings are first \ntransformed into auditory spectrograms. After extracting \nthe spectral-temporal modulation contents of the time-\nfrequency (T-F) units through a two-stage auditory model, \nwe define modulation features pertaining to three catego-\nries in music audio signals: vocal, harmonic , and percus-\nsive. The T-F units are then clustered into three categories \nand the singing voice is synthesized from T-F units in the \nvocal category via time-frequency masking. The algo-\nrithm was tested using the MIR-1K dataset and demon-\nstrated comparable results to other unsupervised masking \napproaches. Meanwhile, the set of novel features gives a \npossible explanation on how the auditory cortex analyzes \nand identifies singing voice in music audio mixtures.  \n1. INTRODUCTION \nO\nver the past decade, the task of singing voice separation \nhas gained much attention due to improvements in digital \naudio technologies. In the research field of music infor-\nmation retrieval (MIR), separated vocal signals or ac-\ncompanying music signals can be of great use in many \napplications, such as singer identification, pitch extrac-\ntion, and music genre classification. During the past few \nyears, many algorithms have been proposed for this chal-\nlenging task. These algorithms can be categorized into \nunsupervised and supervised approaches. \nThe unsupervised approaches do not contain any \ntraining mechanism in the algorithms. For instance, \nDurrieu et al. used a source/filter signal model with non-\nnegative matrix factorization (NMF) to perform source \nseparation [5] and Fitzgerald et al. used median filtering \nand factorization techniques to separate harmonic and \npercussive components in audio signals [7]. Some other \nunsupervised methods considered structural characteris-\ntics of vocals and music accompaniments in several do-\nmains for separation. For example, Pardo and Rafii pro-\nposed REPET which views the accompaniments as re-\npeating background signals and vocals as the varying in-\nformation lying on top of them [16]. Tachibana et al. pro-posed the separation technique, HPSS, to remove the \nharmonic and percussive instruments sequentially in a \ntwo-stage framework by considering the nature of fluctu-\nations of audio signals [19]. Huang et al. used RPCA to \npresent accompaniments in low-rank subspace and vocal \nin sparse representation [8]. In addition, some unsuper-\nvised CASA-based systems were proposed for singing \nvoice separation by finding singing dominant regions on \nthe spectrograms using pitch and harmonic information. \nFor instance, Li and Wang proposed a CASA system ob-\ntaining binary masks using pitch-based inference [13]. \nHsu and Jang extended the work and proposed a system \nfor separating both voiced and unvoiced singing segments \nfrom the music mixtures [9]. Although training mecha-\nnisms were seen in these two systems, they were only for \ndetecting voiced and unvoiced segments, but not for sepa-\nration. \nIn contrast, there were approaches based on super-\nvised learning techniques. For example, Vembu et al. \nused vocal/non-vocal SVM and neural-network (NN) \nclassifiers for vocal-nonvocal segmentation [20]. Ozerov \net al. used a vocal/non-vocal classifier based on Bayesian \nmodeling [15]. Another group of methods combined \nRPCA with training mechanisms. For instance, Yang’s \nlow-rank representation method decomposed vocals and \naccompaniments using pre-trained low-rank matrices [22] \nand Sprechmann et al. proposed a real-time method using \nlow-rank modeling with neural networks [17]. Although \nthese supervised learning methods demonstrated very \nhigh performance, they usually offer a weaker conception \nof generality.  \nMusic instruments produce signals with various kinds \nof fluctuations such that they can be briefly categorized \ninto two groups, percussive  and harmonic . Signals pro-\nduced by percussive instruments are more consistent \nalong the spectral axis and by harmonic instruments are \nmore consistent along the temporal axis with little or no \nfluctuations. These two categories occupy a large propor-\ntion of a spectrogram with mainly vertical and horizontal \nlines. To extend this sense into a more general form, the \nfluctuations can be viewed as a sum of sinusoid modula-\ntions along the spectral axis and the temporal axis. If a \nsignal has nearly zero modulation along one of the two \naxes, its energy is smoothly distributed along that axis. \nConversely, if a signal has a high frequency of modula-\ntion along one axis, then its energy becomes scattered \nalong that axis. Therefore, if one can decipher the modu-\nlation status of a signal, one may be able to identify the \ninstrument type of the signal. An algorithm utilizing mo-  © Frederick Yen, Yin-Jyun Luo, Tai-Shih Chi. \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Frederick Yen, Yin-Jyun Luo, Tai-\nShih Chi. “Singing Voice Separation using Spectro- Temporal \nModulation Features ”, 15th International Society for Music Information \nRetrieval Conference, 2014. \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n617  \n \n \n \nFigure 1 . Stages of the cochlear module, adopted from \n[2]. \n \ndulation information can be seen in [1], where Barke r  et \nal. combined the modulation spectrogram (MS) with non-\nnegative tensor factorization (NTF) to perform speech \nseparation from mixtures of speech and music. \nAlthough the above mentioned engineering approach-\nes produce promising results, human’s tremendous ability \nin sound streams separation makes a biomimetic ap-\nproach interesting to investigate. Based on neuro-\nphysiological evidences, it is suggested that neurons of \nthe auditory cortex (A1) respond to both spectral modula-\ntions and temporal modulations of the input sounds. Ac-\ncordingly, a computational auditory model was proposed \nto model A1 neurons as spectro-temporal modulation fil-\nters [2]. This concept of spectro-temporal modulation de-\ncomposition has inspired many approaches in various en-\ngineering topics, such as using spectro-temporal modula-\ntion features for speaker recognition [12], robust speech \nrecognition [18], voice activity detection [10], and sound \nsegregation [6].  \nSince modulations are important for music signal cat-\negorization, this modulation-decomposition auditory \nmodel is used as a pre-processing stage for singing voice \nseparation in this paper. Our proposed unsupervised algo-\nrithm adapts this two-stage auditory model, which de-\ncodes the spectro-temporal modulations of a T-F unit, to \nextract modulation based features and performs singing \nvoice separation under the CASA framework. This paper \nis organized as follows. A brief review of the auditory \nmodel is presented in Section 2. Section 3 describes the \nproposed method. Section 4 shows evaluation and results. \nLastly, Section 5 draws the conclusion. \n2. SPECTRO-TEMPORAL AUDITORY MODEL \nA neuro-physiological auditory model is used to extract \nthe modulation features. The model consists of an early \ncochlear (ear) module and a central auditory cortex (A1) \nmodule. \n2.1 Cochlear Module \nAs shown in Figure 1, the input sound goes through 128 \noverlapping asymmetric constant-Q band-pass filters \n(Q\u0002\u0003\u0004≫ 4 ) whose center frequencies are uniformly dist- r\nibuted over 5.3 octaves with the 24 filters/octave fre-\nquency resolution. These constant-Q filters mimic the \nfrequency selectivity of the cochlea. Outputs of these fil-\nters are then transformed through a non-linear compres-\nsion stage, a lateral inhibitory network (LIN), and a half-\nwave rectifier cascaded with a low-pass filter. The non-\nlinear compression stage models the saturation caused by \ninner hair cells, the LIN models the spectral masking ef-\nfect, and the following stage serves as an envelope ex-\ntractor to model the temporal dynamic reduction along \nthe auditory pathway to the midbrain. Outputs of the \nmodule from different stages are formulated below: \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty \t\nt,ω\u000e= \u0010\n\u0011\u000e∗\u0013ℎ\n\u0011;\u0016\u000e                      (1) \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty \u0017\nt,ω\u000e= g\u0019∂\u001by\t\nt,ω\u000e\u001c∗\u001bℓ\nt\u000e             (2) \n                                                                      \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty \u0002\nt,ω\u000e= max\t\n∂ !y\u0017\nt,ω\u000e,0\u000e\t\t            (3) \n \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty #\nt,ω\u000e= y\u0002\nt,ω\u000e∗\u001bμ\nt;τ\u000e\t\t                 (4) \n \nwhere  s\nt\u000e is the input signal; \tℎ\n\u0011;\u0016\u000e is the impulse re-\nsponse of the cochlear filter with center frequency  \u0016\t; \t∗\u0013 \nd\nenotes convolution in time; \tg\n．\u000e is the nonlinear com-\npression function; \t∂\u001b is the partial derivative of  t  ;  ℓ\nt\u000e \nis the membrane leakage low-pass filter; μ\nt;τ\u000e=\te(\u001b/*∙\nu\n\nt\u000e is the integration window with the time constant  τ \nto model current leakage of the midbrain; u\nt\u000e is the step \nfunction. Detailed descriptions of the cochlear module \ncan be found in [2].  \nThe output y#\nt,ω\u000e of the module is the auditory \nspectrogram, which represents the neuron activities along \ntime and log-frequency axis. In this work, we bypass the                                      \nnon-linear compression stage by assuming input sounds \nare properly normalized without triggering the high-\nvolume saturation effect of the inner hair cells. \n2.2 Cortical Module \nThe second module simulates the neural responses of the \nauditory cortex (A1). The auditory spectrogram \ty#\nt,ω\u000e \nis analyzed by cortical neurons which are modeled by \ntwo-dimensional filters tuned to different spectro-\ntemporal modulations. The rate parameter (in Hz) char-\nacterizes the velocity of local spectro-temporal envelope  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n618  \n \n \nF\nigure 2. Rate-scale outputs of the cortical module to \ntwo T-F units of the auditory spectrogram of the \n'Ani_2_03.wav' vocal track in MIR-1K [9]. \nvariation along the temporal axis. The scale parameter \n(in cycle/octave) characterizes the density of the local \nspectro-temporal envelope variation along the log-\nfrequency axis. Furthermore, the cortical neurons are \nfound sensitive to the direction of the spectro-temporal \nenvelope. It is characterized by the sign of the rate para- \nmeter in this model, with negative for the upward direc-\ntion and positive for the downward direction.   \nFrom functional point of view, this module performs \na spectro-temporal multi-resolution analysis on the input \nauditory spectrogram in various rate-scale combinations. \nOutputs of various cortical neurons to a single T-F unit of \nthe spectrogram demonstrate the local spectro-temporal \nmodulation contents of the unit in terms of the rate, scale \nand directionality parameters. \nFigure 2 shows rate-scale outputs of two T-F units in \nan auditory spectrogram of a vocal clip. The rate-scale \noutput is referred to as the rate-scale plot in this paper. \nThe rate and scale indices are ±2(\u0017~±20\tand\t2−2~23, re-\ns\npectively. The strong responses of the plots correspond to \nt\nhe variations of singing pitch envelopes resolved by the \nrate and scale parameters and the moving direction of the \npitch. Detailed description of the cortical module is avail-\nable in [3]. \n3. PROPOSED METHOD \nA schematic diagram of the proposed algorithm is shown \nin Figure 3. The following sections will discuss each part \nin details. \n3.1 Feature Extraction \nAccording to the spectral and temporal behaviors ob-\nserved on the auditory spectrogram, components of a \nmusical piece are characterized into three categories,   \nFigure 3. Block diagram of the proposed algorithm. \nharmonic , percussive  and vocal. Harmonic components \nhave steady energy distributions over time and have clear \nformant structures over frequency. Each percussive com-\nponent has impulsive energy concentrated in a short pe-\nriod of time and has no obvious harmonic structure. Vo-\ncal components possess harmonic structure and their en-\nergy is distributed along various time periods. Interpret-\ning the above statements from the rate-scale perspective, \nseveral general properties can be drawn. Harmonic com-\nponents can be usually regarded as having low rate and \nhigh scale modulations. It means that they have relative-\nly slow energy change along time and rapid energy \nchange along the log-frequency axis due to the harmonic \nstructures. In contrast, percussive components typically \nshow quick energy change along time and energy spread-\ning along the whole log-frequency axis, such that they \npossess high rate and low scale modulations. Vocal \ncomponents are often recognized as a mix version of the \nharmonic and percussive components with characteris-\ntics sometimes considered more similar to harmonics. \nDifferent types of singing or vocal expression can result \nin various values of rate and scale. Figure 4 shows some \nexamples of rate-scale plots of components from the \nthree categories. \nGiven an auditory spectrogram y4∈ ℛ5×7 t rans-\nfo\nrmed from an input music signal \ts\nt\u000e, the rate-scale \nplots of the T-F units are generated. As a pre-process, in \norder to prevent extracting trivial data from nearly inau-\ndible T-F units of the auditory spectrogram, we leave out \nthe T-F units that have energy less than 1% of the maxi-\nmum energy of the whole auditory spectrogram. With the \nrest of the T-F units, we obtain the rate-scale plot of each \nunit and proceed to the feature extraction stage.  \nFor each rate-scale plot, the total energies of the nega-\ntive and positive rate side are compared. The side with \ngreater energy is determined as the dominant plot. From \nthe dominant plot, we extract 11 features as shown in Ta-\nble 1. The features are selected by observing the rate-\nscale plots with some intuitive assumptions of the physic-\nal properties which distinguish between harmonic, per-\ncussive and vocal. The first 10 features are obtained by \ncomputing the energy ratio of two different areas on the \nrate-scale plot. For example, as shown in Table 1, the first \nfeature is the ratio of the total modulation energy of scale \n= 1 to the total modulation energy of scale = 0.25. The \nlow scales, such as 0.25 and 0.5, capture the degree of the  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n619  \n \n \nF\nigure 4. (a) Rate-scale plot from the vocal track of \n‘Ani_4_07’ in MIR-1K. The modulation energy is mostly \nconcentrated in the middle and high scales for a unit with \na clear harmonic structure. (b) Rate-scale plots from the \naccompanying music track of ‘Ani_4_07’. The upper plot \nshows energy concentrating at low rates for a sustained \nunit. The lower plot shows energy concentrating at high \nrates for a transient unit.  \n \nflatness of the formant structure while the high scales, \nsuch as 1, 2, 4 and 8, capture the harmonicity with differ-\nent frequency spacing between harmonics. Therefore, the \nfirst four features can be thought as descriptors which dis-\ntinguish harmonic from percussive using spectral infor-\nmation. The fifth to the seventh features capture temporal \ninformation which can distinguish sustained units from \ntransient units. \nThe feature values are saved as feature vectors and \nthen grouped as a feature matrix F\t ∈\tℛ9\t×:\tf or clustering, \nwhere 9 is the number of features and :\tis the number of \ntotal valid units in the auditory spectrogram.  \n3.2 Unsupervised Clustering \nIn the unsupervised clustering stage, a spectrogram is di-\nvided into three parts and clustering is performed for each \npart. Based on hearing perception, the frequency resolu-\ntion is higher at lower frequencies while the temporal \nresolution is higher at higher frequencies [14]. Due to the \nfrequency resolution of the constant-Q cochlear fil-\nters/channels in the auditory model, the auditory spectro-\ngram can only resolve about ten harmonics [11]. To han-\ndle different resolutions, the spectrogram is separated into \nthree sub-spectrograms with overlapped frequency ranges. \nThe three sub-spectrograms consist of channel 1 to chan-\nnel 60, channel 46 to channel 75, and channel 61 to chan-\nnel 128, respectively, with overlaps of 15 channels. Table 1.  Eleven extracted modulation energy features \nT\nhe clustering step is performed using the EM algo-\nrithm to group data into three unlabelled clusters. The \nEM algorithm assigns a probability set to each T-F unit \nshowing its likelihood of belonging to each cluster. Note \nthat in spectrogram representations, the sound sources are \nsuperimposed on top of each other. It implies that one T-\nF unit may contain energy from more than one source. \nTherefore, in this work, if one T-F unit has a probability \nset in which the second highest probability is higher than \n5%, that particular T-F unit will also be labelled to the \nsecond high probability cluster. It means one unit may \neventually appear in more than one cluster. The parame-\nter 5% was empirically determined. Each of the three \nsub-spectrograms is clustered into three groups. Total of \nnine groups are generated and merged back into three \nwhole spectrograms by comparing the correlations of the \noverlapped channels between different groups. Each of \nthe three whole spectrograms represents the extracted \nharmonic, percussive, and vocal part of the music mixture. \nWith no prior information about the labels  of the three \nwh\nole spectrograms, the effective mean rate-scale plot of \neach spectrogram is examined. The effective mean rate-\nscale plot is the mean of rate-scale plots of the T-F units \nwith energy higher than 20% of the maximum energy in \nthat spectrogram. The total modulation energy of rate = 1, \n2 Hz and scale = 0.25, 2, 4 cycle/octave is calculated \nfrom the effective mean rate-scale plot and referred to as \nEv, which is used as the criterion  to select the vocal spec-\ntr\nogram. The one with the maximum Ev value is picked \nas the vocal spectrogram since Ev catches modulations \nrelated to the formant structure (scale = 0.25), the har-\nmonic structure (scale = 2 and 4) and the singing rate \n(rate = 1 and 2) of singing voices. \nThe vocal spectrogram is then synthesized to an esti-\nmated signal using the auditory model toolbox  [24]. The \nno\nnlinear operation of the envelope extractor in the coch-\nlear module makes perfect synthesis impossible, thus \ncausing a general result of loss of higher frequencies of \nthe signal. Detailed computations are shown in [2]. \n4. EVALUATION RESULTS \nThe MIR-1K [9] is used as the evaluation dataset. It cont- \nScale Rate \n1 : 0.25 all \n2 : 0.25 all \n4 : 0.25 all \n8 : 0.25 all \n(0.25, 2, 4) (1, 2) : (0.25, 0.5, 1, 2, 16, 32) \n(0.25, 2, 4) (0.25, 0.5) : (0.25, 0.5, 1, 2, 16, 32) \n(0.25, 2, 4) (16, 32) : (0.25, 0.5, 1, 2, 16, 32) \n(0.25, 0.5) : all  all \n(1, 2) : all all \n(4, 8) : all all \n(0.25) all \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n620  \n \n \nF\nigure 5. GNSDR comparison at voice-to-music ratio of \n-5, 0, and 5 dB with existing methods.  \nains 1000 WAV files of karaoke clips sung by amateur \nsingers. The length of each clip is around 4~13 seconds. \nThe vocal and music accompaniment parts were recorded \nin the right and the left channels separately. In this exper-\niment, we mixed two channels in -5, 0, 5 dB SNR (signal \nto noise ratio, i.e., vocal to music accompaniment ratio) \nfor test. To assess the quality of separation, the source-to- \ndistortion ratio (SDR) [21] is used as the objective meas-\nure. The ratios are computed by the BSS Eval toolbox \nv3.0 [23]. Following [9], we compute the normalized \nSDR (NSDR) and the weighted average of NSDR, the \nglobal NSDR (GNSDR), with the weighting proportional \nto the length of each file. To have a fair comparison, we \ncompare our method with other unsupervised methods, \nwhich extract vocal clips only through one major stage. \nThe compared algorithms are listed below: \nI. Hsu: the approach proposed in [9] that performs \nunvoiced sound separation combined with the \npitch-based inference method in [13]. \nII. R (REPET with soft masking): the approach pro-\nposed in [16] that computes a repeating background \nstructure and extract vocal with soft time-frequency \nmasking. \nIII. RPCA: a matrix decomposition method applying \nrobust principal component analysis proposed by \nHuang et al. [8].  \nFrom Figure 5, we can observe that the proposed \nmethod has the highest performance tied with RPCA in \nthe -5 dB SNR condition. In 0 and 5 dB SNR conditions, \nthe performance of the proposed method is comparable to \nthe performance of REPET.   \n5. CONCLUSION \nI\nn this paper, we propose a singing voice separation \nmethod utilizing the spectral-temporal modulations as \nclustering features. Based on the energy distributions on \nthe rate-scale plots of T-F units, the vocal signal is ex-\ntracted from the auditory spectrogram and the separation \nperformance is evaluated using the MIR-1K dataset. Our \nproposed CASA-based masking method outperforms the \nCASA-based system in [9] and has comparable perfor-mance to the masking-based REPET in all SNR condi-\ntions. When compared with the subspace RPCA method, \nour proposed method has comparable performance only \nin the -5 dB SNR condition. These results demonstrate \nthe effectiveness of the spectral-temporal modulation fea-\ntures for analyzing music mixtures. As this proposed \nmethod only applies a simple EM algorithm for clustering, \nharmonic mismatches and artificial noises are yet to be \ndiscussed. \nThe future work will be focused on applying more \nadvanced classifiers for more accurate separations and \nadopting a two-stage mechanism like HPSS to discard \npercussive and harmonic components sequentially. The \nother potential work is to implement the proposed \nspectro-temporal modulation based method in the Fourier \nspectrogram domain [4] to mitigate synthesis errors in-\njected by the projection-based reconstruction process of \nthe auditory model. \n6. ACKNOWLEDGEMENTS \nThis research is supported by the National Science Coun-\ncil, Taiwan under Grant No NSC 102-2220-E-009-049 \nand the Biomedical Electronics Translational Research \nCenter, NCTU. \n7. REFERENCES  \n[1] T. Barker and T. Virtanen, \"Non-negative tensor fac-\ntorization of modulation spectrograms for monaural \nsound source separation, \" Proc. of I nterspeech , pp. \n827-831, 2013. \n[2] T. Chi, P. Ru, and S. A. Shamma, \"Multiresolution \nspectrotemporal analysis of complex sounds,\" J. \nAcoust. Soc. Am. , Vol. 118, No. 2, pp. 887-906, \n2005. \n[3] T. Chi, Y. Gao, M. C. Guyton, P. Ru, and S. \nShamma, \"Spectro-temporal modulation transfer \nfunctions and speech intelligibility,\" J. Acoust. Soc. \nAm., Vol. 106, No. 5, pp. 2719-2732, 1999.  \n[4] T.-S. Chi and C.-C. Hsu, \"Multiband analysis and \nsy\nnthesis of spectro-temporal modulations of Fourier \nspectrogram, \" J. Acoust. Soc. Am. ,  Vol. 129, No. 5, \npp. EL190-EL196, 2011. \n[5] J.-L. Durrieu, B. David, and G. Richard, \"A musical-\nly motivated mid-level representation for pitch esti-\nmation and musical audio source separation,  \"IEEE \nJ. of Selected Topics on Signal Process. ,\" Vol. 5, No. \n6, pp. 1180-1191, 2011.  \n[6] M. Elhilali and S. A. Shamma, \"A cocktail party \nwi\nth a cortical twist: how cortical mechanisms con-\ntribute to sound segregation,  \" J. Acoust. Soc. Am. ,  \nVol. 124, No. 6, pp. 3751-3771, 2008. \n[7] D. FitzGerald and M. Gainza, \"Single channel vocal \nse\nparation using median filtering and factorization \ntechniques, \" ISAST Trans. on Electron. and Signal \nP\nrocess., Vol. 4, No. 1, pp. 62-73 (ISSN 1797-2329), \n2010.  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n621  \n \n[\n8] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. \nHasegawa-Johnson, \"Singing-voice separation from \nmo\nnaural recordings using robust principal \ncomponent analysis, \" Porc. IEEE Int. Conf. on \nA\ncoust., Speech and Signal Process. , pp. 57-60, \n2012.  \n[9] C.-L. Hsu and J.-S. R. Jang, \"On the improvement of \nsinging voice separation for monaural recordings \nusing the MIR-1K dataset,\" IEEE Trans. on Audio, \nSpeech, and Language Process. , Vol. 18, No. 2, pp. \n310-319, 2010. \n[10] C.-C. Hsu, T.-E. Lin, J.-H. Chen, and T.-S. Chi, \n\"Voice activity detection based on frequency modu-\nlation of harmonics, \" IEEE Int. Conf. on Acoust. , \nSpeech and Signal Process. , pp. 6679-6683, 2013. \n[11] D. Klein, and S. A. Shamma, \"The case of the \nmissing pitch templates: how harmonic templates \nemerge in the early auditory system,\" J. Acoust. Soc. \nAm., Vol. 107, No. 5, pp. 2631-2644, 2000. \n[12] H. Lei, B. T. Meyer, and N. Mirghafori, \"Spectro-\ntemporal Gabor  features  for  speaker  recognition,\"  \nIEEE Int. Conf. on Acoust., Speech and Signal \nProcess., pp. 4241-4244, 2012. \n[13] Y. Li and D. Wang, \"Separation of singing voice \nfr\nom music accompaniment for monaural \nrecordings, \" IEEE Trans. on Audio, Speech, and \nLanguage Process. , Vol. 15, No. 4, pp. 1475-1487, \n2007. \n[14] B. C. J. Moore: An Introduction to the Psychology of \nHearing 5th Ed.,  Academic Press, 2003. \n[15] A. Ozerov, P. Philippe, F. Bimbot, and R. \nGribonval, “Adaptation of Bayesian models for sin-\ngle channel source separation and its application to \nvoice / music separation in popular songs, \"IEEE \nTrans. on Audio, Speech, and Language Process. ,\" \nspecial issue on Blind Signal Proc. for Speech and \nAudio Applications, Vol. 15, No. 5, pp. 1564-1578, \n2007. \n[16] Z. Rafii and B. Pardo, \"REpeating Pattern Extraction \nTechnique (REPET): A Simple Method for \nMusic/Voice Separation,\" IEEE Trans. on Audio, \nSpeech, and Language Process. , Vol. 21, No. 1, pp. \n73-84, 2013. \n[17] P. Sprechmann, A. Bronstein, and G. Sapiro, \"Real-\nti\nme online singing voice separation from monaural \nrecordings using robust low-rank modeling, \" Proc. \nof the Int. Soc. for Music Inform. Retrieval Conf. , pp. \n67–72, 2012. \n[18] R. M. Stern and N. Norgan, \"Hearing is believing: \nbiologically inspired methods for robust automatic  \nspeech recognition,\" IEEE Signal Process. Mag. , \nVol. 29, No. 6, pp. 34–43, 2012. \n[19] H. Tachibana, N. Ono, and S. Sagayama, \"Singing \nVoice Enhancement in Monaural Music Signals Based on Two-stage Harmonic/Percussive Sound \nSeparation on Multiple Resolution Spectrograms,\" \nIEEE/ACM Trans. on Audio, Speech, and Language \nProcess., Vol. 22, No. 1, pp.  228-237, 2014. \n[20] S. Vembu and S. Baumann, \"Separation of vocals \nfrom polyphonic audio recordings, \" Proc. of the Int. \nSoc. for Music Inform. Retrieval Conf. , pp. 337–344, \n2005. \n[21] E. Vincent, R. Gribonval, and C. Févotte, \n\"Performance measurement in blind audio source \nseparation,\" IEEE Trans. on Audio, Speech, and \nLanguage Process.,  Vol. 14, No. 4, pp. 1462-1469, \n2006. \n[22] Y. Yang, \"Low-rank representation of both singing \nvoice and music accompaniment via learned \ndictionaries,\" Proc. of the Int. Soc. for Music Inform. \nRetrieval Conf. , pp. 427-432, 2013. \n[23] http://bass-db.gforge.inria.fr/bss_eval/ \n[24] http://www.isr.umd.edu/Labs/NSL/nsl.html  \n15th International Society for Music Information Retrieval Conference (ISMIR 2014)\n622"
    },
    {
        "title": "Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing.",
        "author": [
            "Shuo Zhang",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1416249",
        "url": "https://doi.org/10.5281/zenodo.1416249",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/proceedings/T062_292_Paper.pdf",
        "abstract": "Features of linguistic tone contours are important factorsthat shape the distinct melodic characteristics of differentgenres of Chinese opera. In Beijing opera, the presence ofa two-dialectal tone system makes the tone-melody rela-tionship more complex.  In this paper, we propose a noveldata-driven approach to analyze syllable-sized tone-pitchcontour similarity in a corpus of Beijing Opera (381 arias)with  statistical  modeling  and  machine learning  methods.A total number of 1,993 pitch contour units and attributeswere extracted from a selection of 20 arias. We then buildSmoothing  Spline  ANOVA  models  to  compute  matrixesof average melodic contour curves by tone category andother  attributes.   A  set  of  machine  learning  and  statisti-cal analysis methods are applied to 30-point pitch contourvectors as well as dimensionality-reduced representationsusing Symbolic Aggregate approXimation(SAX). The re-sults indicate an even mixture of shapes within all tone cat-egories,  with the absence of evidence for a predominantdialectal tone system in Beijing opera. We discuss the keymethodological issues in melody-tone analysis and futurework on pair-wise contour unit analysis.",
        "zenodo_id": 1416249,
        "dblp_key": "conf/ismir/ZhangRS14",
        "keywords": [
            "Beijing Opera",
            "Linguistic Tone Contours",
            "Melodic Pitch Contours",
            "Tone-Melody Relationship",
            "Chinese Opera",
            "Tone System",
            "Pitch Contour Units",
            "Machine Learning",
            "Statistical Modeling",
            "Smoothing Spline ANOVA"
        ]
    },
    {
        "title": "Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014, Taipei, Taiwan, October 27-31, 2014",
        "author": [
            "Hsin-Min Wang",
            "Yi-Hsuan Yang",
            "Jin Ha Lee 0001"
        ],
        "year": "2014",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://www.terasoft.com.tw/conf/ismir2014/",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2014"
    }
]