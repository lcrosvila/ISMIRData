[
    {
        "title": "Content-based Identification of Audio Material Using MPEG-7 Low Level Description.",
        "author": [
            "Eric Allamanche"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417853",
        "url": "https://doi.org/10.5281/zenodo.1417853",
        "ee": "https://zenodo.org/records/1417853/files/Allamanche01.pdf",
        "abstract": "Along with investigating similarity metrics between audio material, the topic of robust matching of pairs of audio content has gained wide interest recently. In particular, if this matching process is carried out using a compact representation of the audio content (\"audio fingerprint\"), it is possible to identify unknown audio material by means of matching it to a database with the fingerprints of registered works. This paper presents a system for reliable, fast and robust identification of audio material which can be run on the resources provided by today's standard computing platforms. The system is based on a general pattern recognition paradigm and exploits low level signal features standardized within the MPEG-7 framework, thus enabling interoperability on a world-wide scale. Compared to similar systems, particular attention is given to issues of robustness with respect to common signal distortions, i.e. recognition performance for processed/modified audio signals. The system's current performance figures are benchmarked for a range of real-world signal distortions, including low bitrate coding and transmission over an acoustic channel. A number of interesting applications are discussed.",
        "zenodo_id": 1417853,
        "dblp_key": "conf/ismir/Allamanche01",
        "keywords": [
            "audio fingerprint",
            "robust matching",
            "audio material",
            "database",
            "signal features",
            "pattern recognition",
            "MPEG-7 framework",
            "computing platforms",
            "real-world signal distortions",
            "interesting applications"
        ],
        "content": "Content-based Identification of Audio Material Using\nMPEG-7 Low Level Description\nEric Allamanche\nFraunhofer IIS-A\nAm Weichselgarten 3\nD - 91058 Erlangen, Germany\n+49 9131 776 322\nalm@iis.fhg.de\nBernhard Fröba\nFraunhofer IIS-A\nAm Weichselgarten 3\nD - 91058 Erlangen, Germany\n+49 9131 776 535\nbdf@iis.fhg.deJürgen Herre\nFraunhofer IIS-A\nAm Weichselgarten 3\nD - 91058 Erlangen, Germany\n+49 9131 776 353\nhrr@iis.fhg.de\nThorsten Kastner\nFraunhofer IIS-A\nAm Weichselgarten 3\nD - 91058 Erlangen, Germany\n+49 9131 776 348\nksr@iis.fhg.deOliver Hellmuth\nFraunhofer IIS-A\nAm Weichselgarten 3\nD - 91058 Erlangen, Germany\n+49 9131 776 372\nhel@iis.fhg.de\nMarkus Cremer\nFraunhofer IIS-A / AEMT\nAm Ehrenberg 8\nD - 98693 Ilmenau, Germany\n+49 3677 69 4344\ncre@emt.iis.fhg.de\nABSTRACT\nAlong with investigating similarity metrics between audio\nmaterial, the topic of robust matching of pairs of audio content\nhas gained wide interest recently. In particular, if this matching\nprocess is carried out using a compact representation of the audio\ncontent (\"audio fingerprint\"), it is possible to identify unknown\naudio material by means of matching it to a database with the\nfingerprints of registered works. This paper presents a system for\nreliable, fast and robust identification of audio material which can\nbe run on the resources provided by today's standard computing\nplatforms. The system is based on a general pattern recognition\nparadigm and exploits low level signal features standardized\nwithin the MPEG-7 framework, thus enabling interoperability on\na world-wide scale.\nCompared to similar systems, particular attention is given to\nissues of robustness with respect to common signal distortions,\ni.e. recognition performance for processed/modified audio signals.\nThe system's current performance figures are benchmarked for a\nrange of real-world signal distortions, including low bitrate\ncoding and transmission over an acoustic channel. A number of\ninteresting applications are discussed.\n1. INTRODUCTION\n Stimulated by the ever-growing availability of musical material to\nthe user via new media and ways of distribution (e.g. the Internet,\nefficient audio compression schemes) an increasing need to\nidentify and classify audio data has emerged. Given the enormous\namount of available audio material it has become more and more\ndifficult for the consumer to locate music that fits his or her\npersonal tastes.\nDescriptive information about audio data which is delivered\ntogether with the actual content would be one way to facilitate this\nsearch immensely . This so-called metadata  (\"data about data \")could e.g. describe the performing artist, composer or title of the\nsong and album, producer, date of release, etc.. Examples of de-\nfacto and formal standards for metadata are the widely used ID3\ntags attached to MP3 bitstreams [ 1] and the forthcoming MPEG-7\nstandard [ 2].\nAnother way of retrieving these information resides in the\ncharacteristics of the medium on which the audio data is\ncomprised. This kind of services are provided by e.g. Gracenote,\nformerly CDDB , [3] where the Table Of Content (TOC) of an\naudio CD is compared against a vast database. Obviously, this\nkind of mechanism fails when the CD is a self made compilation,\nor when commercially not available.\nA lot of different approaches have addressed the automatic\nanalysis of audio content, be it speech/music classification\n[4, 5, 6], retrieval of similar sounds ( \"sounds like \" data base\nsearch) [ 7, 8, 9], or music genre classification [ 10].\nThe main topic of this paper, however, is to present a system\nwhich performs an automated identification of audio signals rather\nthan assigning them to predefined categories. The essential\nproperty of the introduced system lies in the fact that it does not\nrely on the availability of metadata information that is attached to\nthe audio signal itself. It will, however, identify all incoming\naudio signals by means of a database of works that are known to\nthe system. This functionality can be considered the algorithmic\nequivalent of human recognition of a song from the memory of\nthe recognizing person.\nThis observation yields the key criteria for the performance\nrequirements of an audio identification system. It should be able\nto identify the song as long as a human being is able to do so. To\ncome as close as possible to this aim, the system should be robust\nagainst alteration commonly applied to musical material, like\nfiltering, dynamic range processing, audio coding, and so on.\nAdditionally, arbitrary excerpts of the music signal should be\nsufficient for the recognition.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.The segment size needed for recognition by an ideal system\nshould not be longer than a few seconds, with other words as long\nas it would take a human listener to identify a piece of music\ncorrectly.\nOn top of that the system should be able to operate with large\ndatabases of registered works while reliably discriminate between\nthe items, and computational complexity should stay within\nacceptable limits ( \"System Scalability \").\nWhile the task of music recognition may appear easy to human\nlisteners, lately introduced technologies definitely fall short of\nreaching these high goals, e.g. in terms of robustness of\nrecognition [ 11] or computational complexity [ 12].\nThe system presented in this paper has been designed to meet\nmany of the requirements mentioned above. The system's\ncomplexity is low enough to allow operation on today's personal\ncomputers and other cost-effective computing platforms and the\ndescribed algorithm is based on well-known feature\nextraction/pattern recognition concepts [ 13]. It includes extraction\nof a set of robust features with a psychoacoustic background. The\nextraction process itself is based on so called Low Level\nDescriptors  that will be part of the upcoming MPEG-7 standard.\nIn the following chapters an overview of the presented system is\nprovided first. The architecture of the system as well as the basic\nunderlying concepts are explained. Subsequently, the system\nrequirements for robust recognition are discussed by identifying a\nsuite of typical alterations of the original audio material. The\ninfluence of the audio feature selection on the recognition\nperformance is addressed thereafter based on test results using\ndifferent sets of test data . In the following two chapters potential\napplications of the proposed system are identified and the\ncompliance to the upcoming MPEG-7 standard is accounted for.Finally, a conclusion section will present promising future\nimprovements and directions for further enhancement of the\noverall system performance.\n2. SYSTEM OVERVIEW\nThe audio identification system presented here follows a general\npattern recognition paradigm as described in [1 3]. From the block\ndiagram shown in Figure 1, two distinct operating modes can be\nidentified, namely the training  mode and the classification\n(recognition) mode.  During training a condensed \"fingerprint \" of\neach item from the training sample is created which is used in the\nrecognition phase to identify the item under test.  In a\npreprocessing step a signal preprocessor converts the audio input\nsignal into a fixed target format with predefined settings. In the\npresent configuration, the signal is converted to a mono signal\nusing common downmix techniques and then, if necessary,\nresampled to a sampling frequency of 44.1 kHz.\n2.1 Feature Extraction\nFeature extraction is a central processing step which has a high\ninfluence on the overall system performance. The chosen feature\nset should be robust under a wide class of distortions (see Section\n3.2) and the computational burden should be low enough to allow\nfor real-time calculation. In the present configuration the audio\ntime signal is segmented by a windowing function and each\nwindow is mapped to a spectral representation by means of a DFT\n(Discrete Fourier Transform). A set of psychoacoustic features is\nextracted from the spectrum of each analysis window to form a\nfeature vector. This vector is regarded as an elementary feature at\na discrete time instant t and undergoes further processing.\nThe elementary features are then normalized to have component-\nwise unit variance. Note that no removal of the mean is necessary\nFigure 1: System architecture overviewprior to normalization, as suggested in [1 4], since only the\ndifference between the feature vectors to be classified and the\nreference vectors from the \"fingerprint \" are considered. Through\nthis normalization step, a balanced feature vector is generated\nwhich can be filtered optionally.\nNormalized features from subsequent time steps are then grouped\ntogether to form a composite feature vector of higher dimension.\nIn addition, the feature statistics of the single vectors are\nestimated.\n2.2 Vector Quantization for Pattern\nRecognition\nThe identification system uses a linear classifier which is based on\na compact representation of the training vectors, the above\nmentioned fingerprint. The classification is performed using a\nstandard NN (Nearest Neighbor) rule. To obtain a compact class\nrepresentation a VQ (Vector Quantization) algorithm is applied\nfor training. This method approximates the training data for each\nclass with a so-called vector codebook by minimizing a RMSE\n(Root Mean Square Error) criterion. The codebook consists of a\ncertain number of code vectors depending on the maximum\npermitted RMSE. An upper limit of the number of code vectors\nmay be specified. The VQ clustering algorithm is an iterative\nalgorithm which approximates a set of vectors by a much lower\nnumber of representative code vectors, forming a codebook. Such\na codebook is needed for each class (audio item). In Figure 2 an\nexample of the representation of a set of 2-D feature vectors by 6\ncode vectors is shown.\nThe code vectors are obtained using a simple k-means  clustering\nrule. The code vectors computed during training phase are stored\nin a database together with other associated descriptive\ninformation of the music items, such as title and composer of the\nitem.\nIn Figure 3 the approximation error of a feature vector set is\nshown, depending on the number of code vectors used for the\ncodebook. The training set can be approximated ideally if the\nnumber of code vectors reaches the number of training vectors.\nFor distorted versions of the training vectors, on the other hand,\nthe approximation error does not converge toward zero.\n2.3 Classification\nThe music identification task here is an N-class  classification\nproblem. For each of the music items in the database one class,\ni.e. the associated codebook, is generated. To identify an unknown\nmusic item which is included in the reference database, a\nsequence of feature vectors is generated from the unknown item\nand these features are compared to the codebooks stored in the\ndatabase.\nIn more detail, during the identification process each vector is\nsubsequently approximated by all stored codebooks using some\nstandard distance metric. For each of the classes the\napproximation errors are accumulated and, as a result, the music\nitem is assigned to the  class which yields the smallest\naccumulated approximation error.\nIn a more recent version of the system, the statistics of the features\nis used for the classification task instead of the features\nthemselves. The extracted features are collected over a certain\nperiod of time and short-time statistics are calculated.\nFurthermore, the temporal dependencies between the features are\ntaken into account. This results in both higher recognition\nperformance and lower processing time.\n3. SYSTEM REQUIREMENTS\n3.1 Robustness Requirements\nFor a human listener, just a few seconds, even in noisy\nenvironments, may be sufficient to identify a song. In order to\ndesign a prototype system which approximates this behavior,\nspecial attention has to be paid to the alterations an audio signal\ncan be subjected to and to measure the impact of these\ndegradations on the recognition performance. It is therefore of\ngreat importance for an audio identification system to handle \"real\nworld \" audio signals and distortions. Some of these types ofFigure 2. Example of 2-D feature set and it's\napproximation using 6 code vectors.Figure 3. RMS error as a function\nof the number of code vectors.\n.distortions are discussed subsequently, forming the basis of the\ndevelopment process of a robust identification system.\nA basic type of signal \"degradation \" which exists in real world are\ntime shifted signals. If a feature turns out to be very sensitive\ntowards this kind of signal modification, it is likely that this\nfeature will also yield a poor recognition performance when faced\nwith \"real world \" signals.\nAnother essential aspect is the sensitivity of the identification\nsystem against level changes. This is particularly important when\nthe level of the input signal is unknown, or even worse, may\nslowly vary over time. Such situations arise when, for example, a\nsong is recorded via a microphone. When considering this kind of\ndistortion, the selected features should be invariant to scaling.\nThis is, for instance, the case for energy envelopes and loudness.\nHowever, appropriate post  processing of such features can avoid\nthis dependency. A simple example could be the calculation of the\ndifference of two consecutive feature vectors (these are the so-\ncalled delta features ). Other transforms may be applicable as well\nto overcome this deficiency.\nThe following list enumerates a selection of signal distortions\nwhich were used during the development process of the\nidentifications system to form a test suite of typical \"reference\ndistortions \", each representing a different aspect of robustness.\n• Time shift: Tests the system’s robustness against arbitrary\ntime shifts of the input signal. This can be performed very\neasily by accessing the original audio signal randomly. Care\nshould be taken that the entry points do not correspond to a\nblock boundary used during training.\n• Cropping: It is desirable that an audio identification system\nmay be able to identify a small excerpt from a musical item\nwith sufficient accuracy. In this way, identification of an\nentire song would be possible when only parts (such as the\nintroduction or chorus) are used for recognition. As a\nconsequence, the duration of a song to be entered in the base\nclass database cannot be used as a feature.\n• Volume change: By scaling the input signal by a constant or\nslightly time varying factor, the signal amplitude (volume)\nmay be varied within a reasonable range. In order to counter\nlevel dependency, all features/post  processing chosen for the\nidentification system were designed to be level independent.\nThus, no separate test results will be listed for this type of\nrobustness test.\n• Perceptual audio coding: An ever-increasing amount of\naudio is available in various compressed audio formats (e.g.\nMP3). It is therefore important for an identification system to\nmaintain high recognition performance when faced with this\nkind of signals. The bitrate should be selected within a\nreasonable range, so that the degradation of subjective audio\nquality is not excessive. A bitrate of 96kbps for an MPEG-\n1/2 Layer-3 coded stereo signal is considered to be\nappropriate for general testing.\n• Equalization: Linear distortion may e.g. result from applying\nequalization which is widely used  to adapt the frequency\ncharacteristics to the users personal taste. For robustness\ntesting of the audio identification system, octave band\nequalization has been used with adjacent band attenuations\nset to -6dB and +6dB in an alternating fashion.• Bandlimiting: Bandlimited signals occur when the signal was\nrepresented at a low sample rate or, simply, if a low pass\nfilter has been applied. This can be regarded as a special case\nof equalization.\n• Dynamic range compression: Dynamic range compression is\nfrequently used in broadcast stations. In order to identify\naudio signals from these stations, robustness against this\ntime-variant  type of processing must be considered.\n• Noise addition: White noise  or pink noise  with a reasonable\nSNR (like e.g. 20-25 dB) was added to the item with a\nconstant level in order to simulate effects such as analog\nbackground noise.\n• Loudspeaker-microphone transmission (Ls- Mic): This kind\nof distortion appears when a musical item is played back\nover a loudspeaker and the emitted sound is recorded via a\nmicrophone. The resulting analog signal is then digitized by\nmeans of an A/D converter and presented to the input of the\nsystem. Such a setup provides a realistic combination of both\nsevere linear and non-linear distortions and has been found\nto be one of the most challenging types of distortions with\nrespect to automatic audio recognition. A system exhibiting\nrobustness with respect to such a scenario is perfectly\nsuitable for a wide range of applications. The test setup used\nin the presented work consists of a pair of small multimedia\nPC speakers and a standard PC microphone, which is\ndirected towards the speakers at a distance of around 10cm.\nWhile this list is by far not exhaustive, it should be sufficient for a\ngeneral assessment of a system’s robustness qualities. In\nparticular, the robustness of each feature with respect to these\ndistortion types can be quantified effectively by such a test suite\nand then taken into account for the final feature selection process.\n3.2 Computational Requirements\nWhen investigating the necessary computational resources of all\nthe software components involved in the identification process, it\nbecomes apparent that that there exists a clear asymmetry between\nthe feature extractor and the classifier in terms of processing\npower and memory space (both RAM and disk space). More\nprecisely, the extraction process ( \"fingerprint generation \") can be\nperformed several times faster than real-time, since it only\nconsists of a signal analysis followed by a feature calculation.\nThis processing step is independent from the used classification\nscheme and from the database size, and thus only requires a small\namount of CPU processing power and RAM storage.\nIn contrast, the required resources for the classification task are\ndirectly related to the underlying matching algorithm, the size of\nthe database (i.e. the number of trained reference items) and the\nsize and type of the fingerprint information.\nWhile there is a trade-off between the degree of tolerable\ndistortions, the fingerprint size and the computational complexity\nof the matching algorithm, it was the goal of the work described\nin this paper to find efficient configurations which would allow\nfor both reliable recognition of real-world audio signals and real-\ntime operation on today’s standard PC computing platforms.4. RECOGNITION PERFORMANCE\nThis section discusses the recognition performance achieved by\nthe prototype system depending on the choice of features. More\nspecifically, the performance of the system is investigated when\nfaced with distorted audio signals like the ones listed in the\nprevious section. Figures are provided for different configurations\nof the system, including three features and different sizes of the\ntest database.\n4.1 Features\nA decisive factor in the performance of the identification system is\nthe selection of features. An extensive review of potentially\ninteresting features led to the selection of the following candidate\nfeatures which have been used for further experimentation.\n• An important part in the perception of sound is represented\nby the so-called Loudness. Loudness belongs to the category\nof intensity sensations [ 15]. It seems intuitive that this basic\naspect of an audio signal could serve as a robust feature for\naudio identification. Simple computational models of\nloudness are known, including both the calculation of the\nsignal’s total loudness and partial loudness in different\nfrequency bands. This provides plenty of flexibility for\ndefining a loudness-based feature set. For the following\ninvestigations a multi-band loudness feature was used.\n• Besides the loudness sensation, another important\ncharacteristics of the audio signal relates to the distinction\nbetween more tone-like and more noise-like signal quality.\nThe so-called SFM (Spectral Flatness Measure) [ 16] is a\nfunction which is related to the tonality aspect of the audio\nsignal and can therefore be used as a discriminating criterion\nbetween different audio signals. Similar to loudness, the\nSFM can be used to describe the signal in different frequency\nbands. Such a multi-band version of the SFM features was\nused for the following evaluations.\n• Similar to SFM, a so-called SCF ( \"Spectral Crest Factor \")\nfeature was investigated which is related to the tonality\naspect of the audio signal as well. Instead of calculating the\nmean value for the numerator the maximum is computed, i.e.\nthe ratio between the maximum spectral power within a\nfrequency band and its mean power is determined. In the\nsame way as for SFM, a multi-band version is used.\nThe next sections present classification results based on different\nsetups. Each setup consists of a data base holding an increasing\nnumber of music items.\nFor each setup, a few tables are provided which reflect the\nrecognition performance of the identification system. The\nperformance is characterized by a pair of numbers, where the first\nstands for the percentage of items correctly identified (top 1),\nwhile the second expresses the percentage for which the item was\nplaced within the first ten best matches (top 10).\n4.2 1,000 Items Setup\nAn experimental setup of 1,000 musical items was chosen first,\neach item stored in the compressed MPEG-1/2 Layer 3 format (at\na data rate of 192 kbit/s for a stereo signal). The items were\nchosen from the combined genre rock/pop, to mak e a distinction\nbetween the items more demanding than if material with a wider\ndiversity of characteristics would have been used.  To achieve a\nfast classification of the test items the processed length was set to20 seconds while training was limited to 30 seconds, i.e. the data\nhad to be recognized based on an excerpt of the sequence only.\nThe feature extractor uses a block size of 1,024 samples. Both the\nLoudness and the SFM feature were using 4 frequency bands.\nAfter feature extraction, temporal grouping and subsequent\ntransformation techniques were applied prior further processing.\nThe generation of the base classes was conducted as described\nabove (VQ clustering algorithm). The setup described here\nallowed a classification time of 1 second per item (measured on a\nPentium III 500 MHz class PC). A selection of the recognition\nperformance for this setup of the system is reported in Table 1.\nTable 1. Recognition performance of Loudness and SFM\nfeatures (1,000 item setup , top 1/ top 10 )\nFeature Loudness SFM\nNo distortion 100.0% / 100.0% 100.0% / 100.0%\nCropping 15s 51.0% / 75.5% 92.3% / 99.6%\nEqualization 99.6% / 100.0% 14.1% / 29.8%\nDynamic\nRange\nCompression89.5% / 94.9% 99.0% / 99.3%\nMPEG-1/2\nLayer 3 @ 96\nkbit/s19.0% / 33.3% 90.0% / 98.6\nLoudspeaker\n/ Microphone\nChain38.3% / 61.7% 27.2% / 59.7%\nAs can be seen from these figures, the Loudness feature provides\na rather low recognition performance for the case of cropping\neffects (further restriction to 15s length) or MPEG-1/2 Layer-3\nrobustness. In contrast to this, SFM shows very good performance\nconcerning these robustness tests. Both features do not perform\nvery well in this configuration for the loudspeaker /microphone\nchain experiment.\n4.3 15,000 Items Setup\nThis setup represents one significant step on the way to a \"real\nworld \" scenario. A set of 15,000 items was chosen as a database\nfor the classification system, representing a clearly more\ndemanding task. Again the chosen test items belong mostly to the\nrock/pop genre. To cope with the two main points of interest\n(namely speed and discrimination) while handling this amount of\ndata, some improvements were made compared to the previous\nsetup. To realize an even faster classification speed with a larger\nnumber of items, the statistical analysis of the features was\nexploited and used for classification instead of the raw features\nthemselves. Furthermore, the number of frequency bands was\nincreased from 4 to 16 bands in order to achieve a more precise\ndescription of the audio signal.\nA further difference compared to the previous setup is the fact that\nthe features were implemented in accordance with the\ntime/frequency resolution as specified for the extraction of Low\nLevel Descriptors ( LLDs) by the MPEG-7 audio standard [ 2] (i.e.\nsame window/DFT and shift length).\nTables 2 and 3 show the recognition performance achieved for\nthis experimental setup, now investigating the behavior of the\npromising features which are related to the signal’s spectralflatness properties (and thus \"tone-likeness \"). Table 2 reports the\nclassification results of a standard Vector Quantization approach,\nwhereas Table 3 shows the results for a more advanced matching\nalgorithm including aspects of temporal relationship between\nsubsequent feature vectors. As can be seen from the figures, both\nfeatures (SFM and SCF) perform extremely well even under\nsevere distortion conditions, such as the loudspeaker/microphone\nchain. It can be observed that the SFM feature performs very good\nwhile using a standard VQ classifier. This is further increased to\nrecognition rates above 97% with the more sophisticated\nmatching algorithm. In both cases, SCF shows an even better\nrecognition performance. Being at some kind of \"saturation level \"\nfurther tests with an increased amount of items and additional\nrobustness requirements are mandatory for a better discrimination\nof the two features. Classification time is 7.5 seconds for standard\nand 2.5 seconds for advanced matching (per item).\nTable 2. Recognition performance of SFM  and SCF  features\nusing standard  matching (15,000 item setup)\nFeature SFM SCF\nNo distortion 100.0% / 100.0% 100.0% / 100.0%\nCropping 100.0% / 100.0% 100.0% / 100.0%\nMPEG-1/2\nLayer 3 @ 96\nkbit/s96.1% / 97.2% 99.4% / 99.6%\nMPEG-1/2\nLayer 3 @ 96\nkbit/s &\nCropping92.2% / 94.3% 98.8% / 99.3%\nTable 3. Recognition performance of SFM  and SCF  features\nusing advanced  matching (15,000 item setup)\nFeature SFM SCF\nNo distortion 100.0% / 100.0% 100.0% / 100.0%\nCropping 100.0% / 100.0% 100.0% / 100.0%\nMPEG-1/2\nLayer 3 @ 96\nkbit/s99.6% / 99.8% 100.0% / 100.0%\nMPEG-1/2\nLayer 3 @ 96\nkbit/s &\nCropping97.9% / 99.9% 99.7% / 100.0%\nLoudspeaker\n/ Microphone\nChain &\nCropping98.0% / 99.0% 98.8% / 99.5%\n5. APPLICATIONS\nThe identification of audio content based on matching to a\ndatabase of known works has many attractive applications, some\nof which are presented in the following:\n• Audio Fingerprinting: Matching of audio signals as\ndescribed in this paper is closely related to the much-\ndiscussed topic of \"Audio Fingerprinting \". A compactrepresentation of the signal features for matching (e.g. the\nVQ codebooks) resembles the condensed \"essence \" of the\naudio item and is thus usable as a fingerprint of the\ncorresponding item.\n• Identification of music and linking to metadata:\nAutomated identification of audio signals is a universal\nmechanism for finding associated descriptive data ( metadata)\nfor a given piece of audio content. This is especially useful\nwhen the format the content has been delivered in is\nirrelevant for the identification process and when\nfurthermore this format does not support the transport of\nassociated metadata or reference thereto. Under these\npremises recognition of the song will also serve to provide\nlinks to the corresponding metadata. Since the metadata is\nnot necessarily embedded in the audio content, access to a\nremote database could carry updated information on the\nartist, concerts, new releases and so on.\n• Broadcast monitoring:  A system for automatic audio\nrecognition can identify and protocol transmitted audio\nprogram material on broadcasting stations. With a system\nlike the one introduced in this paper this can be achieved\nwithout the need for special processing of the transmitted\naudio material, as would otherwise be required when using\nbranding methods like watermarking. Applications that\nrequire monitoring of radio programs would include\nverification of scheduled transmission of advertisement\nspots, securing the composer’s royalties for broadcast\nmaterial or statistical analysis of program material (charts\nanalysis).\n• Music Sales:  Automatic audio identification can also be\nused to retrieve ordering and pricing information of the\nidentified material and additionally offer similar material.\nThe recording of sound/music and storage of the signature on\nsmall handheld devices (such as Personal Digital Assistants)\nwill enable the customer to find the recorded music item in\nthe music store or by connecting to the Internet.\n6. MPEG-7 AND ROBUST\nIDENTIFICATION OF AUDIO\nDue to the ever-increasing amount of multimedia material which\nis available to users, efficient management of such material by\nmeans of so-called content-related techniques is of growing\nimportance. This goal can be achieved by using pre-computed\ndescriptive data ( \"metadata \") which is associated with the content.\nOne example of a number of upcoming metadata standards for\naudiovisual data is the MPEG-7 [ 2] process which is planned to\nbe finalized in a first version in late 2001.\nMPEG-7 defines a wide framework for the description of audio,\nvisual and generic properties of multimedia content, covering both\nhigh level semantic concepts as well as low level features (the\nlatter can be extracted directly from the signal itself) [ 17].\nThe basic descriptive entities in MPEG-7 are called Descriptors\n(D) and represent specific content properties or attributes by\nmeans of a defined syntax and semantics. Description Schemes\n(DS) are intended to combine components with view towards\napplication and may comprise both Descriptors and other\nDescription Schemes. Both Descriptors and Description Schemesare syntactically defined by a so-called Description Definition\nLanguage  (DDL) which also provides the ability for future\nextension /modification of existing elements. The MPEG-7 DDL is\nbased on XML Schema as the language of choice for the textual\nrepresentation of content description and for allowing\nextensibility of description tools.\nIn the area of audio signal description, MPEG-7 provides a set of\nLow Level Descriptors  (LLDs) which are defined in terms of both\nsyntactic format and semantics of the extraction process. While\nthese descriptors can be considered to form a universal toolbox\nfor many future applications, a number of concrete functionalities\nhave already been envisaged during the development process of\nthe standard [ 2]. These include \"Query by humming \"-type search\nfor music, sound effects recognition, musical instrument timbre\ndescription, annotation of spoken content and robust matching of\naudio signals.\nSpecifically, the functionality of content-related identification of\naudio signals is supported within MPEG-7 audio by means of  the\nAudioSpectrumFlatness  low level descriptor which is\ndesigned to support robust matching of a pair of audio signals,\nnamely the unknown signal and the known reference signal. The\nAudioSpectrumFlatness  descriptor specifies the flatness\nproperty of the signal's power spectrum within a certain number of\nfrequency bands, i.e. the underlying feature of the recognition\nsystem, as described previously. Using the Scalable Series\nconcept, this data can be delivered with varying temporal\ngranularity to achieve different trade offs between descriptive\naccur acy and compactness.\nThis standardized descriptor design forms the basis for achieving\nan open, interoperable platform for automatic audio identification:\n• Identification relies on a published, open feature format\nrather than proprietary solutions. This allows all potential\nusers to easily produce descriptive data for the audio works\nof interest (e.g. descriptions of newly released songs).\n• Due to the exact standardized specification of the descriptor,\ninteroperability is guaranteed on a worldwide basis, i.e. every\nsearch engine relying on the MPEG-7 specification will be\nable to use compliant descriptions, wherever they may have\nbeen produced.\nIn this sense, MPEG-7 provides a point of interoperability for\nthese applications at the feature level. Since textual descriptions\nbased on an XML representation are not designed to provide\nextremely compact representations, applications may choose to\ntranscode the MPEG-7 compliant description into a smaller,\ncompressed representation for storage in an internal database\n(\"fingerprint \", \"signature \"). Still, the \"un-packed \" representation\nwill remain to be available as a point of interoperability with other\nschemes.\n7. CONCLUSIONS AND OUTLOOK\nThis paper discussed methods for achieving automatic content-\nbased identification of audio material by means of robust\nmatching to a set of known reference items. Particular attention\nwas paid to aspects of robustness with respect to common types of\nsignal alterations, including both linear and non-linear distortions,\naudio compression and cropping to a reasonably-sized excerpt.\nThe ability to handle these types of distortions is vital to theusability of systems for content-based processing in many real-\nworld application scenarios.\nRelying on a general feature extraction/pattern recognition\nparadigm, a prototype system for automatic identification of audio\nmaterial was described in its architecture and background.\nClearly, the selection of appropriate robust features can be\nconsidered crucial for achieving a good recognition performance\nunder a wide range of possible distortions.\nRecognizing the importance of the application, the upcoming\nMPEG-7 audio standard defines a descriptor designed to provide\nthe functionality of robust matching of pairs of audio signals\nwhich relates to the \"un-flatness \" of the signal’s power spectrum\nand thus the tone-like quality of the signal in a number of\nfrequency bands.\nUsing this (and related) features, the recognition performance of\nthe identification system was assessed in a number of\nexperiments. The system configuration used showed excellent\nmatching performance for a test set comprising 15,000 songs. A\ncorrect identification rate of better than 98% was achieved even\nfor severe distortion types, including an acoustic transmission\nover a loudspeaker/microphone chain. The system runs  about 80\ntimes real-time performance on a Pentium III 500MHz class PC.\nClearly, there is still a long way to go until such an automatic\nsystem will be able to match the recognition performance of a\nhuman listener. Nonetheless, the current level of performance\nalready opens the door for a number of very interesting\napplications, including finding associated metadata for a given\npiece of audio content, broadcast monitoring and music sales.\n8. REFERENCES\n[1] S. Hacker. MP3: The Definitive Guide . O’Reilly, 2000.\n[2] ISO-IEC/JTC1 SC29 WG11 Moving Pictures Expert Group.\nInformation technology – multimedia content descri ption\ninterface – part 4: Audio. Comittee Draft 15938-4, ISO/IEC,\n2000.\n[3] Gracenote homepage: http://www.gracenote.com\n[4] E. Scheirer, and M. Slaney. Construction and evaluation of a\nrobust multifeature speech  music discriminator . In ICASSP,\n1997.\n[5] R. M. Aarts, and R. T. Dekkers. A real-time speech-music\ndiscriminator . J. Audio Eng. Soc., 47(9), 1999.\n[6] K. El- Maleh, M. Klein, G. Petrucci, and P. Kabal. Speech\nmusic  discrimination for multimedia applications . In\nICASSP, vol. IV, pages 2445-2448, 2000.\n[7] Cantametrix homepage: http://www.cantametrix.com\n[8] Musclefish homepage: http://www.musclefish.com\n[9] E. Wold, T. Blum, D. Keislar, and J. Wheaton. Content-\nbased classification, search, and retrieval of audio . In IEEE\nMultimedia, vol. 3, pages 27-36, 1996.\n[10] D. Pye. Content-based methods for the management of\ndigital music . In ICASSP, vol. IV, pages 2437-2440, 2000.\n[11] Relatable homepage: http://www.relatable.com\n[12] C. Papaodysseus, G. Roussopoulos, D. Fragoulis, T.\nPanagopoulos, and C. Alexiou. A new approach to theautomatic recognition of musical recordings . J. Audio Eng.\nSoc., 49(1/2), 2001.\n[13] A. K. Jain, R. P. W. Duin, and J. Mao. Statistical Pattern\nRecognition: A Review . IEEE Transaction in Pattern\nAnalysis and Machine Intelligence, 2(1), 2000.\n[14] D. Kil, and F. Shin. Pattern Recognition and Prediction with\nApplications to Signal Characterization . American Institute\nof Physics, 1996.[15] E. Zwicker, and H. Fastl. Psychoacoustics . Springer, Berlin,\n2nd edition, 1999.\n[16] N. Jayant, and P. Noll. Digital Coding of Waveforms .\nPrentice-Hall, Englewood Cliffs, NJ, 1984.\n[17] ISO/IEC JTC1/SC29/WG11 (MPEG): \"Introduction to\nMPEG-7\", available from http://www.cselt.it/mpeg ."
    },
    {
        "title": "Figured Bass and Tonality Recognition.",
        "author": [
            "Jérôme Barthélemy"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417161",
        "url": "https://doi.org/10.5281/zenodo.1417161",
        "ee": "https://zenodo.org/records/1417161/files/Barthelemy01.pdf",
        "abstract": "In the course of the WedelMusic project [15], we are currently implementing retrieval engines based on musical content automatically extracted from a musical score. By musical content, we mean not only main melodic motives, but also harmony, or tonality. In this paper, we first review previous research in the domain of harmonic analysis of tonal music. We then present a method for automated harmonic analysis of a music score based on the extraction of a figured bass. The figured bass is determined by means of a template-matching algorithm, where templates for chords can be entirely and easily redefined by the end-user. We also address the problem of tonality recognition with a simple algorithm based on the figured bass. Limitations of the method are discussed. Results are shown and compared to previous research. Finally, potential uses for Music Information Retrieval are discussed.",
        "zenodo_id": 1417161,
        "dblp_key": "conf/ismir/Barthelemy01",
        "keywords": [
            "harmonic analysis",
            "tonal music",
            "automated harmonic analysis",
            "figured bass",
            "template-matching algorithm",
            "tonality recognition",
            "Music Information Retrieval",
            "research limitations",
            "results comparison",
            "potential uses"
        ],
        "content": "Figured Bass and Tonality Recognition \nJerome Barthélemy \nIrcam \n1 Place Igor Stravinsky \n75004 Paris France \n33 01 44 78 48 43 \njerome.barthelemy@ircam.fr Alain Bonardi \nIrcam \n1 Place Igor Stravinsky \n75004 Paris France \n33 01 44 78 48 43 \nalain.bonardi@ircam.fr\n  \nABSTRACT \n \nIn the course of the WedelMusic project [15], we are currently \nimplementing retrieval engines based on musical content automatically extracted from a musical score. By musical content, \nwe mean not only main melodic motives, but also harmony, or \ntonality.  \nIn this paper, we first review previous research in the domain of \nharmonic analysis of tonal music. \nWe then present a method for automated harmonic analysis of a \nmusic score based on the extraction of a figured bass. The figured bass is determined by means of a template-matching algorithm, \nwhere templates for chords can be entirely and easily redefined by \nthe end-user. We also address the problem of tonality recognition with a simple algorithm based on the figured bass. \nLimitations of the method are discussed. Results are shown and \ncompared to previous research. \nFinally, potential uses for Music Information Retrieval are \ndiscussed. \n \nKEYWORDS  \n \nMusic analysis, automatic extraction of musical features, figured \nbass and tonality recognition. \n1. INTRODUCTION \nAs stated by Ian Bent in his article ”Analysis” of the New Grove’s Dictionary, musical analysis is “the resolution of a musical structure into relatively simpler constituent elements, and the \ninvestigation of the functions of these elements within that \nstructure”. \nHarmonic analysis is one of the principal means to achieve this \ngoal through the production of a figured bass and the analysis of \nthe function of chords based on the relationship of their root to the main tonality. In this paper, we describe a technique for the \nautomated extraction of the figured bass. \nThe figured bass is a very old principle, described in several \ntreatises, starting from “Del sonare sopra il basso” by Agazzari \n(1607).  The aim of the figured bass was, in principle, oriented towards \ninterpretation. Rameau turned it into a genuine theory of tonality \nwith the introduction of the fundamental concept of root. \nSuccessive refinements of the theory have been introduced in the 18\nth, 19th (e.g., by Reicha and Fetis) and  20th  (e.g., Schoenberg \n[10, 11]) centuries. For a general history of the theory of \nharmony, one can refer to Ian Bent [1] or Jacques Chailley [2] \n \nSeveral processes can be build on the top of a harmonic reduction \n• detection of tonality, \n• recognition of cadence, \n• detection of similar structures \nFollowing a brief review of systems addressing the problem of \ntonal and harmonic analysis, we first point out the problems raised \nby harmonic reduction. We then describe our algorithm, and show \nits use in some examples. In the subsequent section, we show the application of a simple process of tonality detection on top of \nharmonic reduction.\n \nThe analysis tools that are described here are part of the \nWedelMusic project, which is funded by the European \nCommission [15]. Its aim is the development of a system of \ndistribution of music scores over the Internet while preserving the owner’s rights. This project includes a cataloguing system. \nIndexes are built from such metadata as name of composer, date \nof composition and so on. Indexes are also built on the basis of musical content, as extracted from the score by analysis tools \ndeveloped at Ircam. They include such elements as main motives, \ndescriptions of tonalities and their relation with the main tonality, etc. \nThese elements can be used in a more general strategy of Music \nInformation Retrieval, which would be based not only just on motives, but also on tonal style, harmony and so on. \n2. A BRIEF TOUR OF MUSIC ANALYSIS \nSYSTEMS \nIn the past, a number of systems have been developed to address \nthe problem of automatic tonal harmonic analysis. Only a few \ntackle the difficult problem of chord generation - that is, \ngeneration of root and encoding of the nature of the chord  - directly from the score. \nMaxwell’s expert system for harmonic analysis of tonal music [6] \nis a rule-based system, consisting of more than 50 rules. The first \nphase performs a reduction of the vertical sonorities of the piece into a chord sequence, by recognizing dissonances and \nconsonances. Maxwell’s complex set of decision rules for consonance and dissonance is difficult to adapt to situations \nwhere the notion of dissonance is slightly different, such as music \nof the 19th century. In addition, as noticed by David Temperley [12], Maxwell ’s algorithm appears not to be capable of correctly \nhandling situations where notes of the chord are stated in \nsequence. \nTemperley ’s approach to harmonic analysis [12] consists of a set \nof preference rules, as described in Lerdahl ’s and Jackendoff ’s \ngenerative theory of tonal music [5]. As in Maxwell ’s system, the \nfirst phase of Temperley ’s algorithm leads to the production of the \nroots of chords. Despite the strongly encouraging results he \nachieved, the author himself pointed out several problems with \nthe algorithm, especially in the analysis of the Gavotte from the French Suite n ° 5 by J.-S. Bach.  \nPardo and Birmingham [8] developed HarmAn, a system that \npartitions tonal music into harmonically significant segments \ncorresponding to single chords. It also tags these segments with \nthe proper chord label. A strength of the system is that it is independent of rhythm. New templates for chords can be \nintroduced, but this requires a rethinking of both the preferences \nrules and the scoring method for a single template, as stated by the authors. A numerical method is used for scoring elements, with \nknown drawbacks: as stated by Francois Pachet [7], “numerical \nvalues are difficult to justify, difficult to maintain, and have poor explanatory capacity ”. The system works with a MIDI-like \nrepresentation of notes, and no enharmonic spelling algorithm is \nimplemented. The system thus suffers from a number of drawbacks by not recognizing the difference between, for \nexample, F# and Gb. This will certainly lead to a number of \nproblems in passages belonging to tonalities with several accidentals. In addition, some aggregations used in the late 18th \ncentury and in the 19th century, such as the augmented sixth (C – \nE – G – A#) cannot be distinguished from other chords (in this \ncase, from a seventh on the fifth degree). \nOther systems have been developed, which don ’t address the first \ndifficulty of chord recognition and segmentation of the score. \nWinograd [14], in a pioneering work, addressed the analysis of \nmusical scores by using systemic grammars. His method needs a \npreliminary hand-made conversion of the original score into a score expressed as a sequence of four-part perfect chords. During \nthis operation, ornamental notes, like suspensions, passing notes \nand the like, are eliminated. \nUlrich [13] developed a process of functional analysis, this term \nreferring to the identification of the function of each chord in a song, and the grouping together of measures that move the tune \nfrom one key center to another one. Similarly to Winograd, the \ninput to the program consists of a sequence of chords, each of \nthem consisting of a set of musical notes. An interesting part of \nthe system is an algorithm for detection of keys, described as an \n“island-growing ” mechanism. \nFrançois Pachet ’s approach to computer analysis of jazz chord \nsequences [7] can be seen as an extension of Ulrich ’s island \ngrowing mechanism, as stated by the author himself. The input of \nthe system is a chord sequence, already explicitly mentioned on \nthe score. The most important improvement to Ulrich ’s \nmechanism is that the system outputs a hierarchical description of \nmodulations. Hoffmann and Birmingham [4] use a constraint satisfaction \napproach in order to solve the problem of tonal analysis. Similarly \nto Winograd ’s method, a preliminary hand-made conversion of \nthe score is necessary. \n3. PROBLEM STATEMENT \nThe issues raised by harmonic reduction are the following: \n• ornamental notes and incomplete harmony, \n• ambiguities, \n• non-regularity of harmony, \n• non-universality of harmony rules. \n \nWe shall now address each of these points. \n \n3.1 Ornamental notes and incomplete \nharmonies \nIn the process of harmonic reduction, some notes are extraneous \nto the harmony - these are ornamental notes, like appoggiaturas, \nsuspensions, passing notes and so on. On the other hand, harmony is frequently incomplete – i.e., some notes may be missing. \nThis is illustrated in the following example: \n \nFigure 1. Ornamental notes and incomplete harmonies \n(Trio of the Clarinet Quintet by Mozart, KV 581) \nThe circled A and C# (written transposed C and E) in the clarinet \npart in the first measure are not part of the harmony, thus they are \nto be considered as ornamental notes. \nIn the second measure, the harmony – a fifth chord on F# - is \nnever complete anywhere in the measure. \nTo cope with these problems, we must apply a fundamental rule \nof analysis, as described by Cook [3] in his treatise: “Essentially \nthere are two analytical acts: the act of omission and the act of relation ”. In order to decide if a note is an ornamental, we use the \nrule handling the resolution: in general, resolution of an \nornamental note such as a suspension, a passing note, an \nappoggiatura is performed with a conjunct degree. \nIn some cases, however, the resolution of an ornamental will be \ndone through disjoint motion: for example, a suspension can be resolved by first playing a harmonic note before playing the resolution. For now, we only apply “natural ” resolution, and we \nwill extend our rule to handle more cases. \nAnother rule for deciding if a note is an ornamental is based on \nthe relative duration (weight) of the note as compared to the other \nnotes of the chord. \n \n3.2 Ambiguities \nSome ambiguities have to be resolved, since certain vertical \naggregations are not “true harmony ”, as shown in the following \nexample: \n \nFigure 2. Ambiguous harmony. \n(Trio of the Clarinet Quintet by Mozart, KV 581) \nThe harmony found on the third beat, surrounded here by a \nrectangle, looks like a sixth. If it is so analysed, its root would \nthen be C#,  the third degree of A Major. But this  is a nonsense in this context. \n3.3 Non-regularity of harmony \nIn traditional harmony, one cannot assume that the harmonic \nrhythm is regular. In other words, a harmonic reduction process \ncannot be based on the assumption that harmony is the same for a beat and for a measure. \n3.4 Non-universality of harmony rules \nThe “theory of harmony ” is not to be considered as a genuine, \nuniversal and well-defined set of rules. As Fran çois Pachet states \n[7], it is “a theory without theorems ”. Rules of harmony have \nevolved through history. As noticed by Hoffmann [4], “the rules \nfor tonal harmony are not specifically stated, but are conventions \ndrawn from centuries of musical experience ”. \nTo cope with this problem, we must let the user define his own \nsets of “harmonic rules ”, and choose which  set of “right ” rules to \napply. \n \n4. DESCRIPTION OF HARMONIC \nREDUCTION \nThe harmonic reduction process includes two main phases: \n• a clusterisation , which is composed of a first phase of \nquantization  of each measure, followed by a vertical \naggregation of notes belonging to the same \nquantization, • an iterative horizontal aggregation, in which \nunnecessary notes are eliminated, and successive \nclusters are merged into chords. \nThe quantization  of the measure is simply the computation of the \nduration of the shortest note in the measure. \nFor each quantization, we store the result of a vertical aggregation \nas a cluster . We use this term here to designate an aggregate of \nnotes which has not yet reached the status of a chord; it is \nrepresented as a list of notes, each of them stored with its diatonic value (pitch, accidental, octave) and its melodic function (interval \nto the following note in the same voice). The information about \nthe melodic function is used to decide whether the note is an ornamental note or a harmonic note: a note with an interval of a \nsecond to the following note is considered to be a possible \nornamental note. A note with an interval greater than a second (a third, a fourth and so on) is considered to be a harmonic note. \nThe iterative horizontal aggregation uses a set of user-defined \nchord templates, i.e., a list of chords together with their figures. For the analysis presented in this paper, we have used a set of 33 \nchords, including some seventh and ninth chords, which can be \nconsidered as representative of classical harmony as  used by composers at the end of the 18th century. \nFor other styles, the user can choose another pre-defined set of \nchords, or to redefine entirely his own set of chords, and store it in the database. The definition of the set of chords is easily input \nthrough the Wedel score editor, which is also used for displaying \nthe score being analysed. The process of horizontal aggregation extensively uses the set of chords that the user has selected. \nWe begin the process of aggregation by comparing two \nconsecutive clusters. They are considered the same if the sounds composing the two clusters are the same, regardless of their \noctave, i.e., each sound of the first cluster belongs to the second, \nand each sound of the second belongs to the first. In this case, the two clusters are merged in one. \nIf they have not been merged, the process performs a union of \nboth clusters and compares the result against the each chord in the set of chords: \n• If the union, except for the possible ornamental notes, \ncan be exactly mapped to a chord, the two clusters are merged into one. \n• If the union, including the possible ornamental notes, \ncan be exactly mapped to a chord, the two clusters are merged into one, and the ornamental notes are now \nconsidered to be harmonic notes. \nThe merge is first applied beat by beat, and then measure by \nmeasure, and is iteratively repeated until no more merge can be \nachieved. \nWhen no further merge can be accomplished, an attempt is made \nto turn each cluster into a chord, by mapping it to the nearest \nchord possible.  \nFirst, we try to find a chord containing all the harmonic notes of \nthe cluster and conversely. If this attempt fails, we then search for a chord containing all the notes of the cluster (this assumes that \nthe cluster can be an incomplete chord). If this fails, we try to find \na chord such that the duration of those cluster's notes which \ncannot be mapped to any note of this chord, is significantly \nshorter (actually by a factor of 6) than the total duration of the notes of the cluster (this assumes that these notes are really \nornamental notes, but were not previously detected as being so). \n \n5. EVALUATION \n5.1 Limitations \nSome very special cases are not taken into account in our \nalgorithm, notably pedals. Another limitation is due to the \noversimplicity of our rule for detection of ornamental notes: some ornamentals can be followed by a disjoint interval, and these can \nonly be detected by the last attempt of turning a cluster into a \nchord, as described above. \nA further limitation is due to the fact that our algorithm doesn ’t \ntake sufficiently into account the context. Some problems of \ncontext dependencies are handled, as shown below in fig. 5, but the resolution of ambiguities is not sufficiently strong. Let us \nexamine this example extracted from the Gavotte from the French \nSuite n ° 5 by J.-S. Bach: \n \nFigure 3. Gavotte from French Suite n°5 by J.-S. Bach, \nmeasure 8 \nIn this Gavotte, whose figured bass is given below (see Figure \n11), the harmony is a seventh chord on the dominant of D (A - C# \n-  E - G). But in some other contexts, it can be a sixth chord on the \nroot of F# (this analysis being the one produced by our algorithm). \nMore generally, we must limit the scope of our harmonic analysis \nto accompanied melody, even if in some limit cases of monophonic voice, a good result can be obtained (as shown below \nwith Mozart ’s example). We think also that these results can be \napplied to some music of the 20th century, for example Bartok ’s \nworks, by redefining the set of chords, but we are aware that this \nmethod cannot be applied to contrapuntal work. \n5.2 Examples \nThese examples show the process of harmonic reduction applied to the Trio of Mozart ’s Clarinet  Quintet. \nThe first example\n1 shows elimination of ornamental notes and \nreconstruction of incomplete chords  \n                                                                 \n1 The notation of figures follows the conventions of figured bass as stated \nin the treatise, with the following exceptions:  figures are written from left to right and not from top to bottom, and a slash following a figure indicates that this figure is diminished. \n7+ is for \n, 65/ is for \n . \n \nFigure 4. Elimination of ornamental notes \n and reconstruction of incomplete harmony. \n(Trio of the Clarinet Quintet by Mozart, KV 581) \n \nThe figure 5 shows the resolution of ambiguities: \n \nFigure 5. Resolution of ambiguities \n(Trio of the Clarinet Quintet by Mozart, KV 581) \nThe harmony on the third beat is not analysed as being a 6th \nchord, as the C# in the clarinet part is determined as a potential \nornamental note (an appoggiatura), and thus, the harmony is \nmerged with the following one, giving as a result a correct analysis of a 7th chord on the fifth degree. \nThe following example shows that the algorithm can produce \ncorrect results even in the case of a simple monophonic voice:  \nFigure 6. Detection  of the root for a monophonic voice \n(Trio of the Clarinet Quintet by Mozart, KV 581) \nThe root is correctly detected as being a B. \n This last example shows that detection of figured bass is not \nconstrained by rhythm: \n \nFigure 7. Measures 6 –  7 , \nSarabande in D minor by J.-S. Bach \n \n6. Application to tonality detection \nOn top of this harmonic reduction, we have developed a simple \nalgorithm of tonality detection. This algorithm is based on the fact that each chord can belong to a limited number of tonalities. \nThe possible tonalities are derived from the figured bass as \npreviously obtained, and a process of elimination is then applied by successively merging regions where there is at least one \ntonality in common, eliminating tonalities not common to the \nregions being merged. Where there is no common tonality, a change of tonality is therefore detected. \nThis algorithm, proceeding as an “island-growing ” mechanism, is \nvery near to the system implemented by Ulrich.  \nThe result of this operation for the Trio of the Clarinet Quintet by \nMozart is shown here, together with the complete figured bass \ngenerated by the system: \n \nFigure 8: Figured Bass and Tonalities detected for the Trio of \nthe Clarinet Quintet by Mozart, KV 581 \nThe figured bass presented here is totally consistent with an \nanalysis done by a human analyst, with a small exception (in measures 31 and 32). \nThe detected tonalities are written below the figured bass. When a \nchange of tonality is detected, it is written on the score, the tonality is determined to be the same until the next change of \ntonality. If a tonality is not recognized, it is denoted bys “?”. \nThe tonalities are correctly detected as being A Major, B Minor, \nA Minor, E Major and D Major, with the exception of measures \n31 and 32 where the tonality is unrecognised. \nThe advantage of this approach is that, due to the harmonic \nreduction process, a number of problems related to tonality \nrecognition are easily solved.  \nIn particular, certain notes “out of the tonality ”, that is, notes \nwhich are not really part of the tonality, are eliminated from the process. One can notice, using the original score, that a B# in \nmeasure 5 or a E# in measure 51 are completely ignored and do \nnot interfere with the process of tonality recognition.  \nHowever, some problems are raised by this simplification. \nIn the following example from “Eine Kleine Nachtmusik ” by \nMozart, measures 24-28, a main tonality is simply ignored: Figure 9. Mozart ’s “Eine Kleine Nachtmusik ” \n \nThe musicologist easily recognizes in measure 28 the main entry \nof the second theme, in D Major. \nUnfortunately, the G natural is ignored by the process of harmonic \nreduction,  being a passing note, even if the root harmony is \ncorrectly recognized as D. So, between the (short) modulation in A found at the end of measure 25, and the (short) modulation in E \nminor correctly recognized at the end of measure 28, the main \ntonality of D Major is not recognized. \nA possible solution to this problem can be a refinement of the \nmodel of tonality recognition by adding a rule recognizing some \nmodulations as being embedded modulations (in some French treatises of Harmony, such modulations are called “emprunts ”, \ni.e., “loans ”). To this end, a derivation of the model of Fran çois \nPachet can be applied. \n7. COMPARISON \nFor the purpose of comparing our models with other work, we \nshow here the result of the production of figured bass applied to a \nfragment of a Sarabande in D minor by J.-S. Bach, whose analysis can be found in the papers of Maxwell [6] and Pardo [8]: \n \n \nFigure 10.  Sarabande in D minor by J.-S. Bach \nThe result of the production of figured bass is shown here on the \nthird staff, marked “FB”, together with the recognized tonalities. \nThe results of Pardo and Maxwell are shown on the following \nlines. \nThe results of Maxwell are identical to ours, with a (very little) \nexception at the beginning of measure 5: the reason is that chord \nBb – D - F# - A is part of our templates. In measure 8, Maxwell ’s \nsystem doesn ’t recognize the sixth-fourth chord on the root of D. \nPardo ’s result suffers from several drawbacks: the system \nproduces an A Major chord on the second eighth note of measure 2, and a G Major chord on the second eighth note of measure 4, \nthis last one being quite  annoying since the correct tonality in this \ncontext is G Minor. Incorrect analysis of augmented chords on the first beat of measure 5 and on the third beat of measure 6 are \ncertainly due to the MIDI-like representation of notes. In addition, \none cannot understand the analysis of the last chord (A7), the seventh - G - being not in the chord. \nWe have also applied the Figured Bass to the Gavotte already \nanalysed by Temperley [12].  \nFigure 11. Gavotte from French Suite n °5 by J.-S. Bach \nThere are several drawbacks in this Figured Bass: \n• measure 6 is incorrectly analysed as a seventh of \ndominant on the tonic, this chord being part of our templates, \n• the last chord of measure 8 is incorrectly analysed as a \nsixth and fifth chord, \n• the root of measure 5 is correctly detected, but \nincorrectly figured as a seventh chord. \nTemperley ’s analysis of the same Gavotte also suffers from \nseveral drawbacks: \n• measure 8 is incorrectly analysed as entirely based on \nthe root of D, \n• a incorrect root of A is detected for the second beat of \nmeasure 4, \n• the second half of measure 5 is incorrectly detected as \nbeing based on the root of E, \n• incorrect roots of D and E are detected in measure 6. \nTemperley ’s analysis of measure 6 can be considered better than \nour Figured Bass, but our analysis of measure 4 can be considered \nbetter. The definite mistake made in both cases in measure 8 is \ndue to the same fact: our models are not able to analyse correctly \nthe last F# as an ornamental.  \n8. Conclusion and perspectives \nIn this paper, we described an algorithm for production of a figured bass.  \nThis algorithm allows the musicologist to redefine “harmony \nrules ” entirely, merely by redefining the chord templates. It is thus \nmuch more general than algorithms found in the literature. We have also shown that our results can be considered at least as good as the best results previously found. We are currently trying to \nmake improvements to the algorithm. \nWe have shown that higher-level processes, for example tonality \nrecognition, can be build on the top of the figured bass. As stated \nin the introduction, several processes can be build on the top of a figured bass: detection of cadence, of tonality, of similar \nstructures, and so on. Results of these processes can be stored and \nindexed in database for the purpose of Music Information Retrieval. \nOne can notice that the Figured Bass, as the result of a \nstandardized process, can be used as a retrieval criterion. It is a \nuseful criterion for a teacher, for example, in the retrieval of \nscores using of the same fragment of Figured Bass (the Figured \nBass of Sarabande in D minor by J.-S. Bach is an interesting one). \nTo this end, a transposition independent encoding of the Figured Bass must be developed, and we are currently working on it. \nOther applications for Music Information Retrieval are possible, \nsuch as classification of style based upon the frequency of chords, \nor upon the relationship between the recognized tonalities and the \nmain tonality, assuming that the complexity of tonal relations is characteristic of a given style. Some techniques actually used to \nclassify melody, such as the Hidden Markov Model (Pollastri, \n[9]), or techniques issued from Graph Theory can be also applied on the description of the score generated by the harmonic \nanalysis. \n9. REFERENCES \n[1] Bent, I. (1987), Analysis, Macmillan Press, London, 1987. \n[2] Chailley, J., (1977), Trait é historique d ’analyse musicale, \nAlphonse Leduc, Paris, 1977. \n[3] Cook, N., (1987), A Guide to Musical Analysis, Oxford University Press, 1987. \n[4] Hoffmann, T., Birmingham, W. P.,(2000), A Constraint \nSatisfaction Approach To Tonal Harmonic Analysis, \nTechnical report, Electrical Engineering and Computer \nScience Department, University of Michigan. \n[5] Lerdahl, F., Jackendoff, R., (1983) A Generative Theory of Tonal Music, MIT press, London, 1985 \n[6] Maxwell, H.J. (1992) “An Expert System for Harmonic \nAnalysis of Tonal Music ”, Understanding Music with AI: \nPerspectives on Music Cognition, M. Balaban, K. Ebcioglu, and O. Laske, eds., AAAI Press, 335-353. \n[7] Pachet, F., (1997) “Computer Analysis of Jazz Chord \nSequences: Is Solar a Blues? ”, in Readings in Music and \nArtificial Intelligence, Miranda, E. Ed, Harwood Academic \nPublishers, February 2000. \n[8] Pardo, B, Birmingham, W. P., (1999) Automated \nPartitioning of Tonal Music, Technical report, Electrical \nEngineering and Computer Science Department, \nUniversity of Michigan.  \n[9] Pollastri, E., Simoncelli, G., (2001), Classification of \nMelodies by Composer with Hidden Markov Models, to be published in Proceedings of the WedelMusic conference, \nFlorence, 2001  [10] Schoenberg, A., Structural Functions of Harmony, London, \nFaber and Faber, 1969. \n[11] Schoenberg, A., Theory of Harmony, London, Faber and \nFaber, 1978. \n[12] Temperley, D., (1996) “An Algorithm for Harmonic \nAnalysis ”, Music Perception 15/1 (1997), 31-68. [13] Ulrich, W., (1977) “The Analysis and Synthesis of Jazz by \nComputer ”, Fifth International Joint Conference on Artificial \nIntelligence, MIT, Cambridge, MA, 865-872. \n[14] Winograd, T., (1968) “Linguistic and Computer Analysis of \nTonal Harmony ”, Journal of Music Theory;, 12, 2-49.  \n[15] Wedelmusic, http://www.wedelmusic.org"
    },
    {
        "title": "MUSART: Music Retrieval Via Aural Queries.",
        "author": [
            "William P. Birmingham"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415810",
        "url": "https://doi.org/10.5281/zenodo.1415810",
        "ee": "https://zenodo.org/records/1415810/files/Birmingham01.pdf",
        "abstract": "MUSART is a research project developing and studying new techniques for music information retrieval. The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling, melodic contour, and phonetic streams for music retrieval. To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music. Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.",
        "zenodo_id": 1415810,
        "dblp_key": "conf/ismir/Birmingham01",
        "keywords": [
            "MUSART",
            "music information retrieval",
            "Markov modeling",
            "melodic contour",
            "phonetic streams",
            "musical abstraction",
            "theme extraction",
            "structure analysis",
            "audio representations",
            "single architecture"
        ],
        "content": "MUSART : Music Retrieval Via Aural Queries \nWilliam P. Birmingham, Roger B. Dannenberg, Gregory H. Wakefield, Mark Bartsch,  \nDavid Bykowski, Dominic Mazzoni, Colin Meek, Maureen Mellody, William Rand \n \nUniversity of Michigan \n128 ATL Building \n1101 Beal Avenue \nAnn Arbor, MI 48109-2110 \n(734) 936-1590 \nwpb@eecs.umich.edu  \nCarnegie Mellon University \nSchool of Computer Science \nPittsburgh, PA 15213 \n(412) 268-3827 \nrbd@cs.cmu.edu \n  \nABSTRACT \nMUSART  is a research project developing and studying new \ntechniques for music information retrieval. The M USART  \narchitecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling, melodic contour, and phonetic streams for music retrieval. To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music. Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality. \n1. INTRODUCTION \nWe are integrating components for music search and retrieval into \na comprehensive architecture called M USART , an acronym for \nMUSic Analysis and Retrieval Technology . Like several other \nmusic-retrieval systems, M USART  takes as input an aural query, \nwhich is typically a theme, hook, or riff, of the pi ece for which the \nuser is searching. Unlike other systems, however, M USART  \nautomatically builds a thematic index of the pieces in its database. Since users generally remember the theme of a piece of music, and the theme can occur anywhere in a piece, indexing by theme can greatly improve both the precision and recall of the retrieval system. \nMoreover, M\nUSART  uses a variety of representations to support \nmultiple search modes. These representations run from a Markov model, to phonetic streams, to strings. This allows us, for example, to easily compute approximate matches and to search based on stylistic similarity or the lyrics in a popular song. Our representation can capture harmony and rhythm, should the user decide to query based on harmonic progression or rhythmic pattern, or both, in addition to or in place of melody. The current version of the system contains hundreds of pi eces \nfrom different Western genres (all are tonal pieces). From these pieces, we have automatically i nduced about 2000 themes. \nM\nUSART  is able to effectively retrieve pieces from either the \ntheme or full piece databases. The system is relatively r obust \nagainst queries that contain some classes of errors (e.g.,  rhythmic \nchanges). We measure performance by rank (the target  piece is in \nthe top ten pieces retrieved), yielding measures from 100%1 for \nqueries without errors and degrading from there based on the type of error in the query. \nIn this paper, we describe the M\nUSART  system architecture2. The \ndiscussion concentrates mostly on searching themes, as we believe this will be the primary method most users will employ. We will mention, however, extensions to full pieces and other music-analysis strategies. We begin by placing our work in the context of current systems. \n2. RELATED RESEARCH \nThere are varieties of approaches described in the database and \ninformation-retrieval (IR) literature on retrieval of music. Some of these approaches, such as Variations [1], are primarily based on retrieving either musical scores or sound recordings using traditional categorization schemes, where the musical items are treated in much the same way as text-based media.  \nA number of other systems [2-8] have focused on sound and \nMIDI [9] input. These systems generally take as input a melody that is “hummed” or played on some type of musical input device, such as a MIDI keyboard. The hummed melodies are converted to text strings, usually with a representation of intervallic distance or simply relative pitch contour [10]. For the melody “Happy Birthday,” a user may hum the pitches “G G A G C B,” which may then be more simply categorized as ascending (U), descending (D), or the same (S) to yield the pitch contour “S U D U D.” A commonly used variation to the SUD approach is to divide jumps into large (U or D) and small (u or d), where large is, for example, a jump greater than a minor 3\nrd. This SUuDd \nalphabet provides more information than a string composed from the SUD alphabet, but does not substantially change the retrieval process. \n                                                                \n \n1 That is, in 100% of the cases the target was in the top-ten rank. \n2 See musen.engin.umich.edu for more information on the \nMUSART  project and related projects. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.   While hummed input is a natural approach, there are some \nproblems with using it as the sole input. Pitch contour and related representations (such using actual or modified pitch normalized to some key) are not necessarily unique: the hummed input is sometimes fraught with a number of distortions inherent in the way that humans remember and produce (sing) melodies [6]. Such distortions include raising or lowering the pitch of various notes, “going flat” over time, losing the beat, or humming “off key.” To account for these distortions, researchers treat the string as imprecise. In other words, regions of uncertainty are added. The retrieval process on imprecise strings employs a string-matching [11], N-gram [12], or other algorithm where the input string is matched against abstractions stored in the database.  \nWhile this approach has clearly led to some impressive results, we \nargue that the “string approach” is fundamentally limited for the following reasons: \n• The assumption that a melodic fragment exists and forms a \nsuitable search key is not always true for some types of music such as rap, electronic dance music, and even some modern orchestral music. Users may want to search for non-melodic attributes. \n• Given a large, but not unrealistic, corpus of music, pitch \ncontour or intervallic representations will not uniquely identify pieces. In other words, there will be a great many pieces with similar string representations; this problem is exacerbated if the strings are modeled as imprecise. \n• In some genres, harmony, homophony or polyphony may be \npredominant musical features. Representing any of these as simple strings is fraught with significant problems. For example, would a two-voice piece be represented as two “concurrent” strings? We are not arguing against melodic search per se ; we only want to \nrecognize that current practice must be augmented with new techniques to achieve better performance and richer functionality. For example, current research suggests using genre to narrow searches, but in many cases users may want to search for orchestration, themes, or rhythms that span many genres. Using genre to narrow searches is similar to MARC-record searching in that pre-established categories are selected using traditional database and query techniques. We look to implement more general and more powerful searching techniques. \nSeveral researchers have described systems based on various non-\nstring methods to represent and classify music [8, 13, 14]. These projects have not yet demonstrated how to integrate general query mechanisms and methods for returning results. Nor do these systems integrate abstraction with a search mechanism. \nRetrieval by melody depends on the “hook” of a piece of music. \nThe hook is usually what a person uses as a query when humming. The problem is that although the hook may be contained in the opening melodic line of a song, often it is not. For example, it is common in popular songs for a familiar chorus (such as “Take me out to the ballgame…” by Jack Norworth & Albert Von Tilzer) to follow an unfamiliar verse (such as “Nelly Kelly loved baseball games…,” the actual opening line from the 1927 version of “Take Me Out to the Ballgame”). In classical music, the main theme (or melody) may not be stated for several measures following the start of the piece, or it may be disguised in a variation, and the variation may be better known than the main theme.  \nThus, there is a significant problem in determining what part of a \nmelody, or even which melody, should be indexed. Some abstraction of a piece is clearly needed, as many pieces of music are too long to be reasonably searched. Faced with these problems, music librarians have developed thematic indexes which highlight significant themese in staff notation, thus preserving major musical features (harmony, rhythm, etc.) [13]. \nCorpusMusical\nAbstraction\nProcessingRepresentation\nTranslation\nSearch\nEnginesTheme\nFinding\nChroma\nAnalysis\nMelodic\nPattern\nStyle\nClassifier\nVowel\nClassifier...Markov\nRepresentation\nFrame\nRepresentation...Markov\nDistance\nContour\nDistance\nViterbi\nSearch...\nQuery\nInterfaceBrowsing\nInterface\nUserDatabase\n \nFigure 1. MUSART Architecture. \n   We know of no other researchers who are addressing the problem \nof abstraction using automated mechanisms. \n3. MUSART  ARCHITECTURE \nThis section illustrates the conceptual organization of our system \n(see Figure 1). The corpus is preprocessed using a collection of tools to build abstract representations of the music (i.e., our data). The extracted information is then translated to multiple representations. Queries are also translated, and a search engine is selected to search the database. The important idea of this framework is that various modules and representations can work together in parallel or in sequence to achieve more refined searches than any single search technique can offer. We can also compare different techniques on the same set of data. \nWith this architecture, the user interface can offer a rich set of \noptions to the user. Because music is abstracted in various ways, users can explore musical spaces along many dimensions. In addition, abstract representations lend themselves to music generation and synthesis. We have just begun to explore this possibility, but this promises to be a powerful tool for users to refine queries and to explore different musical dimensions. \nSo far, we have implemented several new techniques for \nsearching, and we have investigated several techniques for musical abstraction. These are described in the following sections. We are in the process of integrating these subsystems to form a more integrated whole. \n4. MARKOV REPRESENTATION AND \nTHE CONCURRENCY \nOne way we model music is as a stochastic process, where we cast \na musical performance as a Markov model. Recently, we have experimented with Markov models where we use simple state features of pitch (or concurrent set of pitches) and duration [14]. The transition table is the likelihood of going from one state to another, where the priors are determined by counting transitions \noccurring in some set of music (e.g., the piano concerti of Mozart). \nWe define the states of the Markov model as concurrencies . A \nconcurrency consists of all the notes that are sounding at the same time. These notes need not all have the same duration, onset, or offset. Hence, a different concurrency exists at each point a new note begins or terminates in a piece. For the purposes of simplification, the notes in a concurrency are modeled by pitch class, thereby ignoring octave information. By representing concurrencies in this manner, we are able to store and use them as 12-tuple bit vectors. It is easy to extend the concurrency to include octave information, simply by increasing the size of the bit vector. For example, a two-octave range is represented by a 24-tuple and corresponding bit vector. \nThe concurrency can represent up to twelve simultaneous pitches \n(every pitch class); elements of the harmony or polyphony are naturally represented. Moreover, this represent is well suited for automated harmonic analysis [15].  \nWe use Figure 2 and Table 1 as examples of a set of concurrencies \nand the corresponding state and transition tables for the Markov model representing the piece. Figure 2 shows the piano-roll notation for Wilson’s Wilde , an anonymous 16\nth-century work. \nTable 1 shows the corresponding concurrencies (given in the table as Markov states) based solely on this excerpt. \nAlthough relatively simple, the Markov representation has several \ninteresting properties that allow us to use it as one of the primary representations in our system. First, our results from inducing Markov models from the pieces in our corpus indicate that composers occupy (mostly) unique regions in the state space implied by the Markov model. Secondly, we found that the transition matrices are relatively sparse (more so for some works than others). Thirdly, we can impose an order on the states implied by the Markov model.  \nThe ordering is formed as follows: we quantize note duration to \nsome minimum (e.g., 16\nth-note) and use a pitch-class vector \nwhere, for example, <100000000000> represents the pitch class C as the single pitch sounding, <110000000000> represents the pitch classes C and C#/D-flat sounding concurrently, and so forth. A state is represented by a duple (duration, pitch vector). We can simply order the state space as follows: (16\nth-note, \n<100000000000>), (8th-note, <100000000000>), … (whole note, \n \nFigure 2: Piano roll for  Wilson’s Wilde , mm. 1 – 4. State # Notes in State\n1E\n2A3E ,  A4C # ,  A5A ,  B6C # ,  A7D8C # ,  D9 E, G#, B\n10 REST\n \nTable 1: Concurrencies for Wilson’s Wilde, mm. 1-4.   <111111111111>), where durations are ordering by increasing \npowers of 2 of a 16th-note, and the maximum duration is a whole \nnote.3 \nBecause of these three properties, we can assess similarity among \npieces of music, or even composers using a variety of techniques. Initial results indicate a strong correspondence between similarity as computed by the M\nUSART  system and educated musical \nopinion. Consider that once the query is converted to a Markov chain, it can be easily correlated with pieces in the database (See Section 6.) While at worst case this is a linear-time operation, the correlation operation is fast and, with clever indexing, we can significantly reduce this time. Finally, the induction of the Markov model and correlation computation can be done off line. \n5. Thematic Abstraction \nWe are interested in extracting the major themes from a musical \npiece: recognizing patterns and motives in the music that a human listener would most likely retain. Extracting themes is an important problem to solve. In addition to aiding music librarians and archivists, exploiting musical themes is key to developing efficient music retrieval systems. The reasons for this are twofold. First, it appears that themes are a highly attractive way to query a music-retrieval system. Second, b ecause themes are much smaller \nand less redundant than the full pi ece, by searching a database of \nthemes rather than full pieces, we simultaneously get faster retrieval (by searching a smaller space) and get increased relevancy. Relevancy is increased as only crucial elements, variously named motives, themes, melodies or hooks are searched, thereby reducing the chance that less important, but frequently occurring, elements are assigned undue relevancy. \nOur theme abstraction subsystem, MME [16], exploits \nredundancy that is found in music. Thus, by breaking up a pi ece \ninto note sequences and seeing how often these sequences repeat, we identify the themes. Breaking up a piece involves examining all note sequence lengths of one to some constant. Moreover, because of the problems listed earlier, we must examine the entire piece and all voices. This leads to very large numbers of sequences, thus we must use a very efficient algorithm to compare these sequences. \nOnce repeating sequences have been identified, we must further \ncharacterize them with respect to various perceptually important features in order to evaluate their thematic value. It has been noted, for instance, that the frequency of a pattern is a stronger indication of thematic importance than pattern register. We implement hill-climbing techniques to learn weights across features, i.e., MME learns relative to a training set the relative importance of the features it uses. The resulting evaluation function is then used to rate the sequence patterns we uncover in a piece. A greedy algorithm is used to identify a subset of the piece, consisting of some pre-determined number of note events, containing top-ranked patterns. \nOur formal evaluation of MME on over 30 training trials, with 30-\npiece training and test sets ra ndomly selected across 60 pi eces for \neach trial, MME returns Barlow's A Dictionary of Musical Themes  \n[17] “1\nst theme” in 98.7% of cases (see Figure 4). Figure 3 shows \n                                                                 \n3 We can choose any range of durations, and any method to order \nthe durations. sample output from MME, two slightly different versions of the \npassage Barlow identifies as the “1st theme”. \n \n \nFigure 3: Sample MME output, Smetana's Moldau.  \nBecause of the large number of patterns that MME may find in a \ncomplex piece of music, it must be computationally efficient. The \nsystem’s overall complexity is ) (23nmΘ  time, where m is the \nmaximum pattern length under consideration, and n is the number \nof note events in the input pi ece. In practice, however, we observe \nsub-linear performance, and reasonable running times on even the largest input pi eces.  \nOverall Performance\n051015202530\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\nTrial NumberNumber of Primary Themes Returned Correctly out of 3 0\n \nFigure 4: MME test results. \n6. Retrieval Methods \nThe pieces in the MusArts’ database are converted by MME into a \nset of themes, where up to a certain number of themes are identified for each piece. These themes are then converted to the Markov models described in Section 4. Thus, the “Markov Distance” retrieval engine takes a query, converts it to a Markov model, and then computes a variety of correlations measures between the query model and all “theme” models [18]. \nCurrently, we use two correlation methods to examine how the \nquery relates to a theme. Both are based on a standard correlation coefficient. One technique, however, uses the first-order Markov model, while the other simply uses the frequency count of each \nstate observed, which is a zero-order Markov model. Once a comparison has been made to each database entry the results are sorted and presented as a score out of 1000 to the user. \nThe results so far are promising. We have taken some of the \nthemes extracted automatically and manipulated them to simulate typical errors we expect to see in queries. The three classes of errors that we are investigating are duration change, note drop, and pitch change. In our experiments, we set an error rate for each type of error that indicates what percentage of the MIDI events we will manipulate when comparing the query to the database entries.   We define a successful trial as one in which the piece that was \nbeing sought is presented as one of the top ten themes returned (a \nrank measure). The system is very robust to duration-change \nerrors, having as high as a 95% success rate even when the error rate is 100%. When it comes to note-drop errors, the system performs at a 75% success rate with error-rates approaching 50%. However the system is not robust to pitch-change errors. It appears that a pitch-change error rate of 10% the system does not return the sought piece in the top rank. We are investigating several solutions to this problem.  \nWe have recently implemented a Viterbi algorithm for retrieval. \nWith this algorithm, we can find which Markov model is most likely to cover the input query. Rather than calculate correlation measures, this approach calculates true posterior probabilities (given the input query as evidence). To account for errors, we \ninsert “error states” in the Markov model for each theme. The distributions for these error states are currently based on our intuition; however, we are conducting an extensive set of experiments to get better error models.  \nOur initial results with the Viterbi approach are encouraging. \nUsing the same experimental setup as we used for the correlation experiments, we have recorded better results for all error classes. In particular, the Viterbi approach appears to be more robust to pitch-change errors. \nThe description of both the correlation and Viterbi approaches \ngiven in this paper has relied on searching a database of monophonic themes (which do include rhythmic features). Given that both approaches are based on concurrencies, it is very simple to apply both approaches to homophonic or polyphonic music. In fact, we are experimenting with searching for harmonic progressions using the Viterbi approach.  \n7. FRAME REPRESENTATION AND \nMELODIC CONTOUR \nWe are exploring another representation and search strategy to \naddress some of the problems of conventional “event-based” searching, where events are typically musical notes and searching is performed on the basis of note sequences. Event-based searches suffer from at least two problems. First, it is difficult for music transcription systems to segment audio correctly into discrete notes. This is a problematic even when skilled musicians sing queries. Secondly, efficient string-matching algorithms, when given enough leeway to ignore music transcription and performance errors, can often time-align two perceptually dissimilar melodic strings, leading to false positives. \nAn alternative is to ignore the concept of note and perform a \ndirect comparison of musical contours, representing melody as a function of pitch versus time. This function is discretized by segmenting the melody into time frames. Thus, we call this a “frame-based” approach. Figure 5 illustrates a pitch contour from an audio query and the resulting query string. In this approach, there is no need to quantize pitch, nor is there a problem if pitch varies during a “note” because notes are not represented explicitly. Furthermore, this approach can be extended to incorporate transcription uncertainty by representing pitch as a probability distribution. 464748495051525354\n00 . 511 . 522 . 53\nTime (seconds)Pitch (semitones)\n \nQuery string : 47.1, 47.4, 47.8, 47.8, 47.8, 49.4, 49.2, 49.6, \n49.2, 48.4, 47.4, 47.7, 47.7, 47.7, 51.5, 52.6, … \nFigure 5. An audio query is segmented into frames \nrepresenting a melodic contour. Each frame corresponds \nto the pitch of a 100 ms timeslice. \nTo deal with pitch transposition, we transpose queries by many \ndifferent pitch offsets; to deal with tempo variation, we “stretch” the target melodies in the database by different scale factors. In addition, we use a constrained form of time warping to align queries with the database entries. Constraints prohibit large deviations that would distort the overall contour shape. \nThis approach is obviously very slow and impractical for \nsearching a large database. However, we believe it would be foolish to limit our research to the existing body of fast search algorithms. Instead, we hope to characterize some limitations of fast search algorithms and then try to overcome them. For example, a slow precise search might be used to refine the results of a fast, imprecise search. To evaluate the frame-based approach, we are replicating various event-based search engines from the literature so that we can compare different approaches using identical queries and databases. Preliminary results show that the frame-based approach is giving substantially better precision. For example, the frame-based approach ranked the correct match as the closest match in 13 out of 20 queries on a collection of 77 big band arrangements, and it ranked the correct match in the top 3 on 16 out of 20 queries. In contrast, an event-based search found the correct match in 5 out of 20 queries, with only 6 out of 20 queries ranked in the top 3 results [19]. \n8. PHONETIC-STREAM ANALYSIS \nIn addition to pitch and rhythm, singing as a natural form of query \npossesses time-varying acoustic-phonetic information, e.g., one sings the \nwords  of a song according to the pitch and duration of \neach note. While all three streams may provide useful information for searching the musical database, only the pitch stream has been studied in any detail, and almost no work has been done on the acoustic-phonetic stream. In the ideal case of errorless queries, the acoustic-phonetic stream is likely to be highly redundant with the rhythm and pitch streams, and, therefore, is expected to provide little additional information. In the practical case, where the rhythm and pitch streams may contain a significant number of   errors, the acoustic-phonetic stream may be the only reliable \nsource of information. \nIn its most general form, extracting the stream of phonetic \ninformation from the query is a problem in speaker-independent continuous sp eech recognition, for which a sizable body of \nresearch literature exists, all of which suggests that we should expect little success in the case of sung passages wit hout \nsubstantial effort. Besides independence across speakers, the problem of speech recognition for singing is further exacerbated by the fact that non-vocalic segments of the stream are generally poorly represented, e.g., one cannot “sing” the fricative /f/. Furthermore, singing extends the pitch range upwards from the normal range of speaking to fundamental frequencies that generally cause problems for many standard recognition systems. \nOur work [20] focuses on a reduced version of phonetic-stream \nanalysis. Rather than attempting to transcribe the word that is sung into standard phonetic units, we have studied coarser quantizations of the phonetic stream, which trade robustness to production variations within and across singer against the information-bearing capacity of the stream. The algorithm we have developed extracts a symbol stream consisting of the Cartesian product of a “phonetic” alphabet of four vowel types (front, neutral, back, non-vocalic) and a duration alphabet of long and short.  \nAmong the several approaches we have studied for segmenting \nthe phonetic stream into the 8-element symbol stream, the most promising appears to be based on a self-referential model, as opposed to an absolute-referential one. In an absolute-referential model, queries are segmented based on templates constructed for the four vowel types over the entire population of singers. A self-referential model segments each query individually and then maps these segments to the most likely “universal” alphabet of front, neutral, back, and non-vocalic. Of the two approaches, the self-referential model appears in our preliminary studies to be more robust to such sources of variation in production as gender, training, song, and register. \nThe self-referential model utilizes nearest-mean reclassification \n(NMRA) to segment the query into four categories based on properties of the query’s short-time Fourier transform. NMRA is performed on the entire set of short-time Fourier transforms to assign one of four raw categories to each time slice. Aggregation is performed across time slices to yield short and long classification of each vowel type. Finally, the raw categories are mapped into front, neutral, back, and non-vocalic labels based on features of the spectral distributions within and across the raw categories. The string for each query is then compared with the database to find the best matches. \nResults from pilot studies suggest that the approach outlined \nabove may be useful in music retrieval. A database of sung queries was  constructed by having subjects sing one verse from seven familiar songs: “Happy Birthday”, “Yankee Doodle”, “America the Beautiful”, the Beatles’ “Yesterday”, “Row, Row, Row Your Boat”, “Somewhere Over the Rainbow”, and “My Bonnie Lies Over the O cean”. Ten subjects were recruited from \namong staff, faculty, and graduate students at the Advanced Technologies Laboratory at the University of Michigan. Each subject sang four instances of each song. They were allowed to pace themselves and to c hoose whatever pitch range and tempo \nthey felt most comfortable for each of the songs. Informal classification of the quality of singing ranged from very poor to excellent across the ten subjects.  \nError! Reference source not found.  shows the percent correct in \nthe nearest neighbor matches as a function of query song. For each query, the nearest neig hbor is found as the vowel stream that \nrequires the fewest number of (weighted) edits to be transformed into the desired query. If that selected vowel stream is generated from the same song as the query, then the selection is deemed correct. There are a total of 40 queries for each song, for a total of 279 possible neighbors from which to select, as we do not count the query itself as a possible neighbor.  Therefore, a 14% chance exists of a correct answer occurring at random. \nAcross all songs, the nearest neighbor selection is correctly \nchosen from the same song 60% of the time, and varies by song between 42% (for “Yankee Doodle”) and 80% (for “America the Beautiful”).  We interpret these results as supporting the general hypothesis that some representation of vowel stream is useful for music information retrieval. \namer bon hb rain row yd yes00.20.40.60.81Fraction correct\nSong  \nFigure 6. Fraction of correct nearest-neighbor choices as a \nfunction of the query song. A correct choice means that the \nnearest neighbor to the vowel stream is a vowel stream from \nthe same song. There are 40 queries for each song. \n9. STRUCTURAL ANALYSIS AND \nABSTRACTION \nIn parallel with our work on melodic search, we are investigating \nmethods by which we can deduce musical structure and genre. Information about structure has many uses. Since repetition is common in music, identifying repetition can aid in music processing and transcription as well as eliminate unnecessary searching. Listeners often remember the most-often repeated parts of a song, so search engines can give extra weight to these, and audio browsers can begin playback at these memorable moments. While a great deal of work has focused on the analysis of symbolic representations of music, we also wish to consider applications to databases of audio waveforms. Both of the approaches described below represent early efforts to derive structure from music audio.   9.1 Chroma-Based Search for Structure \nOne approach in this direction is our “audio thumbnailing” \nalgorithm [21]. Prior work in this area includes Logan and Chu, [22] who developed algorithms for finding key phrases in selections of popular music. Their work focused on the use of Hidden Markov Models and clustering techniques for mel-frequency cepstral coefficient (MFCC) representations of the acoustic waveform. Their system was subjectively evaluated on a relatively small selection of Beatles songs. In another work, Foote [23, 24] talks about audio “gisting” as an application of his proposed measure of audio novelty. This audio novelty score is based on a similarity matrix, which compares frames of audio based on features extracted from the audio. Foote leaves details such as the similarity metric and feature class as design decisions; however, he does recommends the use of MFCCs as a feature class for computing audio novelty. \nOur approach to “audio thumbnailing” draws upon two key \nconcepts: the chromagram and recurrent state. The chromagram is an abstraction of the time-varying spectrum of the audio signal which is based on the perceptual organization of pitch [25]. For \neach time frame, the chromagram maps the linear-frequency spectrum onto pitch-chroma spectrum, which ranges in semitones over the octave [26]. Among the mathematical properties of the chromagram is that it discounts octave relationships among the components of the frequency spectrum, which are highly redundant in harmonic sources, and pl aces greater emphasis on \npitch-class relationships, which bear information about harmony.  \nRecurrent state abstracts the concept of structural organization in \na piece of music. The refrain in a song, for example, is distinguished from the rest of the piece only by virtue of the fact that it, alone, recurs throughout the pi ece. Thus, in searching for a \nrefrain, or other such structures that repeat often in a piece of music, relatively little can be assumed about the structure save some basic unit of time, which establishes the scale of the structure.  \nOur system performs audio thumbnailing by examining the audio \nsignal for recurrent states over time scales from 5 to 60 seconds in duration. To reduce redundancies in the representation of \nLAG (SECONDS)TIME (SECONDS)\n50 100 150 20050100150200\nFigure 5. High correlation of chroma at a given lag indicates similarity. Therefore, vertical line \nsegments represent repetition of musical material.  \n   harmonic sources, each time-slice of the chromagram is quantized \ninto twelve equal semitone bins. The 12-dimensional feature vectors are correlated across time and the correlation values are aggregated over windows of time to identify recurrent structures at a particular time scale. \nFoote’s similarity matrix is a convenient way to represent the \nfeature-correlation space. Wi ndowing, in this case, transforms the \nsimilarity matrix into a time-lag surface Figure 7 presents a time-lag surface for Jimmy Buffet’s \nMargaritaville . A thumbnail for \nthe piece of music is selected by locating the maximum element of the time-lag matrix subject to two constraints. To prevent the selection of quick repetitions and fading repeats, we require that the location have a lag greater than one-tenth the length of the song and occur less than three-fourths of the way into the song. The thumbnail is then defined by the time position of this maximum, which corresponds to the time that the first of the pair of sections begins, and the length of the window used for aggregating the data. \nOur primary quantitative studies of the thumbnailing algorithm \nhave been applied to a database of 93 selections of popular music, with styles including rock, folk, dance, country-western, and others [21]. Each song was hand-scored to identify the refrain or chorus segments of the song. The thumbnailing algorithm was then used to identify highly similar segments of music. Our general observations include the following. With respect to frame-level recall and precision rates, the thumbnail algorithm performs as high as 0.9, for proper choice of window. When it fails, it is often the case that the chorus or refrain is repeated, but there is some change, either in instrumentation or in the musical structure of the repeat. These cases violate our initial assumption of high correlation between instances of the chorus and indicate that this assumption would need to be relaxed under such circumstances. It is also interesting to note that thumbnailing based on the chromagram reduction of the audio stream clearly outperforms a comparable system based on MFCC’s. We interpret this outcome to reflect the fact that MFCC’s provide a low-order representation of the wideband spectrum, whereas the chromagram provides a low-order representation of the “wideband” harmonic content of the signal, by folding harmonically redundant regions of the spectrum into each other. \n9.2 Melodic Pattern Analysis \nAnother effort in the direction of structural analysis also starts \nwith audio but uses conventional autocorrelation-based pitch estimation to extract melodic contour. The top of Figure 8 shows audio taken directly from a commercial recording of a ballad, “Naima,” by John Coltrane and performed by his j azz quartet  \n[27]. Below the audio is a piano-roll display of a pitch transcription, accomplished using a straightforward autocorrelation algorithm for pitch estimation. At the bottom of the figure is the analysis, discussed below. \nTaking inspiration from the frame-based melodic contour \ncomparison described in Section 7 and the chroma-based correlation analysis of Section 9.1, the analysis procedure computes the length of similar melodic contours starting at all pairs of locations \ni and j, which index note position. The matrix \nM(i, j) is defined as the duration of similar contours starting at \nlocations i and j. (M(i, j) is mostly zero.) For example, there is a \nrepeated 4-bar phrase at the beginning of the piece, starting at the first and seventh notes. Note that where M(i, j) is non-zero, there \nwill tend to be a slightly shorter duration at M(i+1, j+1). For \nexample, there are similar phrases starting at notes 2 and 8. These “implied” entries are “removed” by setting them to zero.\n \nAfter eliminating these implied entries, clusters of similar melodic contours are formed. For example, similar lengths at \ni,j, j,k, and \nk,i imply that three similar contours are located at i, j, and k. In the \nsaxophone solo, there is a third repetition of the opening four bars near end of the excerpt shown in Figure 8.  \nAfter simplifying the matrix \nM and forming clusters from the \nremaining melodic fragments, a greedy algorithm attempts to “explain” all notes of the transcription in terms of these clusters. The shaded bars at the bottom of Figure 8 locate similar fragments. It can be seen from this that the melody consists of a repeated phrase followed by a shorter repeated phrase and a third phrase. This is followed by a return to the opening phrase. These phrases return after a piano solo (not shown). \n \nFigure 8. Audio from jazz quartet (top), automatic \ntranscription (middle), and analysis (bottom). Similar \nshading indicates similar melodic fragments. \nThis work is at an early stage. It has produced an excellent \nanalysis of a performance of “Naima.” This example was chosen because of the clear, simple lines and structure and the dominance of the saxophone in the j azz quartet recording (and also because it \nis a wonderful ballad). Work is underway to adapt these methods to more challenging examples. \n9.3 Genre Classification \nAlthough we have argued that not all searches should be \nconducted within a given genre, there are certainly many cases where users can narrow their search by specifying music categories. We have investigated the use of machine learning techniques to perform music classification. We trained a neural network classifier on audio power spectra measured within windows of several seconds of duration of different genres. To classify a piece, we divide the piece into many wi ndow-sized \nchunks and run the classifier on each c hunk. The overall class is \nthe one reported for the greatest number of chunks. This approach correctly classified all 80 pieces in a database of digital audio as rock, jazz, c ountry, or classical. Much finer classification is \ndesirable, and we hope to incorporate new methods into our architecture as they become available. \n10. SUMMARY \nWe have presented an architecture for music retrieval. The \nMUSART  architecture is motivated by the need to explore and   ultimately rely on multiple mechanisms for representation, \nabstraction and search. We have made progress toward more robust and flexible music databases. Our work on Markov models provides a new an approach to musical abstraction and retrieval. Our frame-based melodic search and phonetic-stream search deal specifically with problems of audio-based queries. Additional work addresses the problems of audio analysis, the identification of musical structure, and music classification. \nIn the future, we will refine all of these techniques and integrate \nthem into the architecture we have presented. We also need to explore many user interface issues. The benefits will include the ability to combine multiple search strategies and a more formal approach to the evaluation and comparison of music retrieval techniques. \n11. ACKNOWLEDGMENT \nThis material is supported by NSF Award #0085945 and by an \nNSF Fellowship awarded to Mark Bartsch. The views expressed here are those of the authors. We thank Jonah Shifrin for implementing tphe Viterbi algorithm and generating the data reported in this paper. We also thank Bryan Pardo and Erica Schattle for comments on previous versions of this paper. \n12. REFERENCES \n[1] Dunn, J.W., C.A. Mayer. VARIATIONS: A digital music \nlibrary system at Indiana University . in Digital Libraries. \n1999: ACM. \n[2] Kornstadt, A., Themefinder: A Web-based Melodic Search \nTool, in Melodic Similarity Concepts, Procedures, and \nApplications , W. Hewlett and E. Selfridge-Field, Editors. \n1998, MIT Press: Cambridge. \n[3] McNab R.J., L.A.Smith, D. Bainbridge and I. H. Witten, The \nNew Zealand Digital Library MELody inDEX.  D-Lib \nMagazine , 1997. May. \n[4] Bainbridge, D. The role of Music IR in the New Zealand \nDigital Music Library project. in Proceedings of the \nInternational Symposium on Music Information Retrieval, 2000\n. \n[5] Tseng, Y.H. Content-based retrieval for music collections . in \nSIGIR . 1999: ACM. \n[6] McNab R.J., L.A.S., Ian H. Witten, C.L. Henderson, S.J. \nCunningham. Towards the digital music library: tune \nretrieval from acoustic input . in Digital Libraries . 1996: \nACM. \n[7] Blum, T., D. Keislar, J. Wheaton, E. Wold, Audio databases \nwith content-based retrieval , in Intelligent multimedia \ninformation retrieval , M.T. Mayberry, Editor. 1997, AAAI \nPress: Menlo Park. \n[8] Rolland, P.Y., G. Raskinis, J.G. Ganascia. Musical content-\nbased retrieval: an overview of the Melodiscov approach and system\n. in Multimedia . 1999. Orlando, FL: ACM. \n[9] Rothstein, J., MIDI: A Comprehensive Introduction . 1992, \nMadison, WI: A-R Editions. \n[10] Hewlett, W. and E. Selfridge-Field, eds. Melodic Similarity \nConcepts, Procedures, and Applications . Computing in \nMusicology. Vol. 11. 1998, MIT Press: Cambridge. [11] Sankoff, D. and J.B. Kruskal, eds. Time Warps, String Edits, \nand Macromolecules: The Theory and Practice of Sequence Comparison\n. 1983, Addison-Wesley: Reading, MA. \n[12] Downie, J. S. Informetrics and music information retrieval: \nan informetric examination of a folksong database . in \nProceedings of the C anadian Association for Information \nScience, 1999 Annual Conference, 1999. Ottawa, Ontario.  \n[13] Zhang, T., C.C.J. Kuo, Hierarchical system for content-\nbased audio classification and retrieval , University of \nSouthern California: Los Angeles. \n[14] Alamkan, C., W. Birmingham, and M. Simoni, Stochastic \nGeneration of Music, 1999, University of Michigan. \n[15] Pardo, B. and W. Birmingham. Automated Partitioning of \nTonal Music . FLAIRS , 2000. Orlando, FL. \n[16] Meek, C. and W. Birmingham. Thematic Extractor. \nInternational Symposium on Music Information Retrieval  in \nSecond  International Symposium on Music Information \nRetrieval. 2001. \n[17] Barlow, H. A Dictionary of Musical Themes.  Crown \nPublishers. 1983. \n[18] Rand, W. and W. Birmingham. Statistical Analysis in Music \nInformation Retrieva l in Second  International Symposium on \nMusic Information Retrieval. 2001. \n[19] Mazzoni, D., and R. B. Dannenberg, Melody Matching \nDirectly from Audio  in Second International Symposium on \nMusic Information Retrieval. 2001. \n[20] Mellody, M., Bartsch, M., and G. H. Wakefield. Analysis of \nVowels in Sung Queries for a Music Information Retrieval System\n submitted for publication in Journal of Intelligen \ntInformation Systems . 2001. \n[21] Bartsch, M., and G. H. Wakefield. To Catch a Chorus: Using \nChroma-Based Representations For Audio Thumbnailing  in \nProceedings of the Works hop on Applications of Signal \nProcessing to Audio and Acoustics , 2001. \n[22] Logan, B., and S. Chu. Music summarization using key \nphrases in International Conference on Acoustics, Speech, \nand Signal Processing , 2000. \n[23] Foote, J. Automatic audio segmentation using a measure of \naudio novelty  in Proceedings of IEEE International \nConference on Multimedia and Expo , 1999. \n[24] Foote, J. Viusalizing music and audio using self-similarity  in \nProceedings of ACM Multimedia , 1999. \n[25] Shepard, R. Circularity in judgements of relative pitch  in \nJournal of the Acoustical Society of America, 36, 2346-2353, \n1964. \n[26] Wakefield, G.H. Mathematical Representation of Joint Time-\nChroma Distributions . in Intl. Symp. on Opt. Sci., Eng., and \nInstr., SPIE’99 . 1999. Denver. \n[27] Coltrane, J. Naima on the album Giant Steps. Atlantic \nRecords. 1960."
    },
    {
        "title": "Building a Platform for Performance Study of Various Music Information Retrieval Approaches.",
        "author": [
            "Arbee L. P. Chen"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1414822",
        "url": "https://doi.org/10.5281/zenodo.1414822",
        "ee": "https://zenodo.org/records/1414822/files/Chen01.pdf",
        "abstract": "In this paper, we describe the Ultima project which aims to construct a platform for evaluating various approaches of music information retrieval. Three approaches with the corresponding tree-based, list-based, and (n-gram+tree)-based index structures are implemented. A series of experiments has been carried out. With the support of the experiment results, we compare the performance of index construction and query processing of the three approaches and give a summary for efficient content-based music information retrieval.",
        "zenodo_id": 1414822,
        "dblp_key": "conf/ismir/Chen01",
        "keywords": [
            "Ultima project",
            "music information retrieval",
            "platform construction",
            "index structures",
            "experiments",
            "query processing",
            "index construction",
            "performance comparison",
            "efficient retrieval",
            "summary"
        ],
        "content": "Building a Platform for Performance Study of Various \nMusic Information Retrieval Approaches \n \n \n \n \n Jia-Lien Hsu and Arbee L.P. Chen \nDepartment of Computer Science \nNational Tsing Hua University \nHsinchu, Taiwan 300, R.O.C. \nalpchen@cs.nthu.edu.tw  \n \n \n \n \n \nABSTRACT  \nIn this paper, we describe the Ultima project which aims to \nconstruct a platform for evaluating various approaches of music \ninformation retrieval. Three approaches with the corresponding \ntree-based, list-based, and ( n-gram+tree)-based index structures are \nimplemented. A series of experiments has been carried out. With \nthe support of the experiment results, we compare the performance \nof index construction and query processing of the three approaches \nand give a summary for efficient content-based music information \nretrieval.  \n1. Introduction \nWith the growth of music objects available, it is getting more \nattention on the research of constructing music information \nretrieval systems. To provide an efficient and effective content-\nbased retrieval of music objects, various approaches have been \nproposed in which the music representations, index structures, \nquery processing methods, and similarity measurement are key \nissues. \nRegarding the issue of music representation, several approaches \nare introduced to model various features of music content, such as \npitch, rhythm, interval, chord, and contour. To efficiently resolving \nuser queries, different kinds of techniques are proposed, including \nstring matching methods, dynamic programming methods, n-gram \nindexing methods, and list-based and tree-based indexing \nstructures with the corresponding traversal procedures. Most \nresearchers present their solutions to the key issues separately. \nHowever, the research work focusing on the quantitative and \nqualitative comparison of various techniques used in music \ninformation retrieval is still limited. \nOn the contrary, in the traditional information retrieval, the \nproblems and techniques involved in the evaluation of retrieval \nsystems and procedures have been investigated. The most common \nevaluation criteria have also been identified, such as precision and \nrecall, response time, user effort, form of presentation, and \ncollection coverage [18]. \nDue to the multi-faceted properties of music, there exist intrinsic \ndifficulties for content-based music information retrieval (MIR). \nThe framework of a formal MIR evaluation mechanism becomes \nnecessary. The point is emphasized in [7], which states “a \nformalized set of MIR evaluation standards must become part of \nthe MIR researcher toolkit” and “a set of music test databases of \nsubstantial size and varied content must be formed so that all MIR \nresearchers can properly compare and contrast techniques under a \nvariety of scenarios.” From the database point of view, we initiate the project of building \na platform for the evaluation of music information retrieval \nsystems. Considering the retrieval efficiency and effectiveness, we \nfocus on the performance study of music representations, indexing \nand query processing which involve a wide range of techniques \nused in content-based music information retrieval. \nThe rest of this paper is organized as follows. In Section 2, we \ndescribe our project for evaluating music information retrieval \napproaches. The issues of system design, data set, query set \ngeneration, and efficiency and effectiveness study are also \nintroduced in this section. The three approaches implemented in \nour platform are described in Section 3. We perform a series of \nexperiments and illustrate the experiment results and performance \nstudy in Section 4. Section 5 concludes this paper and points out \nour future directions. \n1.1 Related Work \nSelfridge-Field [19] provides a survey of clarifying and resolving \nconceptual and representational issues in melodic comparison. \nResearch work on MIR systems are introduced as follows. Ghias, \net al.  [10] propose an approach for modeling the content of music \nobjects. A music object is transformed into a string which consists \nof three kinds of symbols, ‘U’, ‘D’, and ‘S’ which represent a note \nis higher than, lower than, or the same as its previous note, \nrespectively. The problem of music data retrieval is then \ntransformed into that of approximate string matching. \nIn [1][6], a system supporting the content-based navigation of \nmusic data is presented. A sliding window is applied to cut a music \ncontour into sub-contours. All sub-contours are organized as an \nindex structure for the navigation. Tseng [20] proposes a content-\nbased retrieval model for music collections. The system uses a \npitch profile encoding for music objects and an n-gram indexing \nfor approximate matching. A framework is also proposed in which \nthe music objects are organized as an n-gram structure for efficient \nsearching [22]. Different techniques of local alignment and local \ncommon subsequences have also been applied for comparison. \nSimilar techniques of n-gram indexing have also been employed in \n[8][24][25]. Furthermore, Downie and Nelson [8] provide an \neffectiveness evaluation of an n-gram based MIR system by using \nstatistical analysis. \nThe work [17] focuses on music retrieval from a digital library in \nwhich dynamic programming is used to match melodic phrases. \nThe issues of melody transcription and matching parameters are \ndiscussed and the trade-off between the matching criteria and \nretrieval effectiveness is shown. Also using dynamic programming, \nLemstrom and Perttu [14] present a bit-parallel algorithm for efficiently searching melodic excerpts. In the bit-parallel \nprocessing, the whole table for dynamic programming need not be \ncreated, and thus it leads to a better performance. Clausen, et al.  [5] \ndesign a web-based tool for searching polyphonic music objects. \nThe applied algorithm is a variant of the classic inverted file index \nfor text retrieval. A prototype is implemented and its performance \nis investigated. \nTo develop a content-based MIR system, we have implemented a \nsystem called Muse  [3][4][13]. In this system, various methods are \napplied for content-based music data retrieval. The rhythm, melody, \nand chords of a music object are treated as music feature strings \nand a data structure called 1D-List  i s  d e v e l o p e d  t o  e f f i c i e n t l y  \nperform approximate string matching [13]. Moreover, we consider \nmusic objects and music queries as sequences of chords [4] and \nmubol  s t r i n g s  [ 3 ] .  A  t r e e - b a s e d  i n d e x  s t r u c t u r e  i s  d e v e l o p e d  f o r  \neach approach to provide efficient matching capability. In [3], we \npropose an approach for retrieving music objects by rhythm. \nInstead of using only melody [1][4][6][10][13] or rhythm of music \ndata, we consider both pitch and duration information plus the \nmusic contour, coded as music segment , to represent music objects \n[2]. Two index structures, called one-dimensional augmented \nsuffix tree  a n d  two-dimensional augmented suffix tree , are \nproposed to speed up the query processing. By specifying the \nsimilarity thresholds, we provide the capability of approximate \nmusic information retrieval. When considering more than one \nfeature of music objects for query processing, we propose multi-\nfeature index structures [12]. With the multi-feature index, both \nexact and approximate search functions on various music features \nare provided. \n2. The Ultima Project  \nThe Ultima project is established with the goal to make a \ncomprehensive and comparative assessment of various MIR \napproaches. Under the same environment and real data sets, a \nseries of experiments can be performed to evaluate the efficiency \nand effectiveness of the MIR systems. Issues such as the threshold \nsetting and the identification of most influential factors which \ndominates the system performance can be explored. Furthermore, \nheuristics for choosing appropriate representation schemes, \nindexing structures, and query processing methods when building \nan MIR system can be provided based on the performance study. \nThe Ultima platform will be continuously maintained and served \nas the testbed whenever new approaches of content-based music \ninformation retrieval are proposed. \n2.1 System Design and Implementation \nThe system is implemented as a web server, which runs on the \nmachine of Intel Pentium III/800 with 1GB RAM on MS Windows \n2000 by JDK 1.3. For posing queries at the client end, we provide \nthe ways of humming songs, playing the piano keyword, uploading \nMIDI files, and using the computer keyboard and mouse. The \nserver end consists of a mediator, four modules, and a data store, \nas shown in Figure 1. The mediator receives user queries and \ncoordinates with other modules. The music objects and the \ncorresponding information, such as title, composer, and genre, are \norganized as standard MIDI files and relational tables, respectively. \nThe summarization module aims to resemble and visualize query \nresults. The query generation module aims to generate \nparameterized user queries for performance evaluation, as \ndiscussed in Section 2.3. The implementations of the two modules \nare not finished yet. The report module aims to monitor and assess the performance of the system, such as the elapsed time of query \nprocessing, space of indices, and precision and recall of the \nretrieved results. The query processing module aims to resolve \nqueries from the client end or the query generation module. The \nquery processing module is designed as a “container” to which \neach query processing met hods can be “plugged-in”. Whenever a \nnew method is proposed, it can be easily plugged into the module \nfor performing experiments under the same environment. Currently, \nthree methods are considered, i.e., 1D-List [13], APS [2] and APM \n[3] which will be further discussed in Section 3. \nData Store (MS Access)SMF\nMediatorQuery Processing Module\nTableto the Internet Summarization Module1D-List APS APM\nReport Module\nQuery Generation Module\nFigure 1: The function blocks of the server in the Ultima \nproject. \n2.2 Data Set \nThe testing data of music objects, from CWEB Technology, Inc., is \na collection of 3500 single track and monophonic MIDI files. Most \nof them are pop music of Chinese and English songs in various \ngenres. \nThe average object size is 328.05 notes. When coding these objects \nin the mubol and music segment representations, the average \nobject size is 78.34 (mubol) and 272 (segment), respectively. \nBased on the statistics of the CWEB dataset, we estimate that one \nmubol corresponds to 4.19 notes, and one music segment \ncorresponds to 1.21 notes. The note count  is defined as the number \nof distinct notes appearing in a music object. According to the \nMIDI standard, the alphabet size is 128. Therefore, the note count \nof every melody string is between 1 and 128. For the CWEB data \nset, the average note count is 13.46. Due to the sp ace limitation, \nthe histograms of the object size and note count of the CWEB data \nset are skipped. \nMoreover, when coding music objects by music segment, the \ndistribution of segment pitch is shown in Figure 2. For the segment \nduration, most of its value is between 0 and 20 beats without any \nobvious clustering. \n2.3 Query Set Generation \nIn the traditional information retrieval, there exist standard testing \ndata, queries and the associated answers [9][23]. 0306090120150\n-87~-78\n-67~-58\n-47~-38\n-27~-18\n-7~2\n13~22\n33~42\n53~62\n73~82\n93~102\nsegment pitch value# of note (x 1000)type Atype Btype Ctype D\n \nFigure 2: The distribution of segment pitch values of CWEB \ndata set. \nTherefore, a fair performance evaluation can be performed. \nHowever, there is no such kind of benchmarks dedicated to the \nMIR systems. In this project, we will also investigate a standard \nprocedure for generating parameterized queries and the associated \nanswers from a data set. With the variety of queries, the \nperformance study will be more accurate. \n2.4 Efficiency and Effectiveness Study \nWe design a series of experiments to evaluate the methods of \nindexing and query processing. Factors influencing the system \nperformance are identified, such as query length, database size, and \nquery approximation degree. The measurement of performance is \nbased on memory usage, retrieved candidates and elapsed time for \nefficiency, and precision and recall for effectiveness. \n3. Description of the Three Approaches \nInstead of detailed procedures and algorithms, each approach is \nillustrated by an example to show the basic idea. The three \napproaches cover various methods of music representation, \nindexing, and query processing, as summarized in Table 1. Note \nthat all the approaches support the functionality of exact, partial, \nand approximate matching. For simplicity, we only show an \nexample of exact query processing. \nTable 1: The representations and indexing structures of the \nthree approaches. \nApproach Representation Index structure \nAPM Mubol (rhythm) (N-gram +tree)-based\n1D-List Melody (pitch) List-based \nAPS Music segment \n(pitch+duration) Suffix tree-based \n3.1 The APM Approach \nThe rhythm information of music objects is coded as mubol strings. \nA mubol is a rhythmic pattern of a measure in a music object. A \nmubol string of a music object is the string of mubols which are \ndetermined by each measure of the music objects. For example, as \nshown in Figure 3(a), R1 is a mubol string of eight measures. The \nn-grams of R1, where n = 1, 2, 3, with the associated positions are \nlisted in Figure 3(b). For example, the position “R1: 1,4,7” of the \nmubol in the first row of Figure 3(a) indicates the mubol appears in \nthe first, forth, and seventh measure of R1. All the prefixes of an n-\ngram can be found in the ( n−1)-grams, ( n−2)-grams, …, and 1-grams. To efficiently process queries, the n-grams of mubol strings \nare organized as a tree structure, called L-tree. The tree height h of \nL-tree is the maximal n, i.e., h = 3 in our e x a m ple . A s  s how n in \nFigure 3(c), the nodes in level 1 of the tree indicate the first \nmubols of 1-grams, the nodes in level 2 indicate the second mubols \nof 2-grams, etc. Note that there are two kinds of links in L-tree, \nnamely, solid link and dotted link. The internal nodes are \nconnected with solid links, while the leaf nodes with the associated \ninformation are indicated by dotted links. \nThe exact query processing is performed by a tree traversal of the \nL-tree. Suppose the query is \n  for exact searching. The \nquery will be processed by traversing the L-tree in level-wise \nmanner. When processing the first mubol of the query at level 1 of \nthe L-tree, the node containing \n  is matched and its children \nwill be reached for processing the next m ubol. When processing \nthe second mubol at level 2, those children (only one node in this \nexample) will be compared to the second mubol. Since the node \ncontaining \n  is matched to the second mubol of the query \nstring, the two children of this node will be reached for further \nprocessing. Moreover, all the mubols of the query string have been \nprocessed and only (R1:2,5) is the answer. \nThe L-tree is a ( n-gram+tree)-based index structure. In the \napproach of n-gram indexing, if the length of the query string is \nlarger than n, the false match may happen. For the L-tree of tree \nheight h, if the query length is larger than h, the query will be \ndivided into subqueries and the intermediate answers with \nrespective to each s ubquery will be merged and confirmed by the \njoin processing. \n3.2 The 1D-List Approach \nIn this approach, music objects are coded as melody strings. For \nexample, there are two music objects M1 and M2 in the database. \nThe melody strings of M1 and M2 are “so-mi-mi-fa-re-re-do-re-\nmi-fa-so-so-so” and “do-mi-so-so-re-mi-fa-fa-do-re-re-mi”, \nrespectively. \nTo support efficient string matching, melody strings are organized \nas linked lists, as shown in Figure 4(a). For the notes of the same \npitch in the melody strings, they are linked in an increasing order. \nEach node in the linked lists is of the form ( x:y) which denotes the \ny-th note of the melody string of the x-th music object in the \ndatabase. \nWhen a query Q = “do-re-mi” is specified, the lists involved in Q \nare retrieved with the two dummy nodes start and end as shown in \nFigure 4(b). Then, the exact query processing goes as follows. Let \nAx be the first element of node A, and Ay be the second element of \nnode A. For each pair of nodes ( A, B) taken from two adjacent \nlinked lists, if Ax = Bx and Ay+1 = By, we build an exact link from A \nto B, as shown in Figure 4(c). Also, we build an exact link from \nstart to node F of the first list if F has an outgoing link, and from \nnode L o f  t h e  l a s t  l i s t  t o  end i f  L h a s  a n  i n c o m i n g  l i n k .  B y  \ntraversing the exact links from start to end, each path indicates a \nsubstring appearing in the melody string of the database and will \nbe considered as a result. In our example shown in Figure 4(c), \nthere exists only one path, which is denoted in bold-faced links, \ni.e., “start-(1:7)-(1:8)-(1:9)- end”.  \nFigure 3: (a) A sample mubol string R1. (b) The table of n-grams associated with the corresponding positions. (c) The L-tree of the \nmubol string R1. \n \n3.3 The APS Approach \nFor better readability, the representation, indexing, and query \nprocessing are separately described as follows. \n3.3.1 Representation of Music Objects \nTaking into account of music contour with note duration and pitch, \nthe APS approach represents music objects by sequences of music \nsegments. A music segment  i s  a  t r i p l e t  w h i c h  c o n s i s t s  o f  t h e  \nsegment type  a n d  t h e  a s s o c i a t e d  d u r a t i o n  a n d  p i t c h  i n f o r m a t i o n .  \nThere are four segment types defined to model the music contour, \ni.e., \n (type A), \n (type B), \n (type C), and \n (type \nD). Define the segment base  a s  t h e  h o r i z o n t a l  p a r t  o f  a  m u s i c  \nsegment. The beat information of a music segment is represented \nby the segment duration  w h i c h  i s  t h e  n u m b e r  o f  b e a t s  i n  t h e  \ncorresponding segment base. The pitch information of a music \nsegment is represented by the segment pitch  w h i c h  i s  t h e  note \nnumber  in the  MID I s ta nda rd of  the  c orre s ponding  s e g m e nt ba s e  \nminus the note number of the segment base of the previous \nsegment base. For example, for the piece of music shown in Figure \n5, the corresponding representation as a sequence of music \nsegments is shown in Figure 6. The music segment (A, 1, +1) \nindicates that it is a type A segment with the segment duration and \nsegment pitch being 1 and +1, respectively. When coding by music segments, the first music segment and the last music segment are \nignored due to lack of information to assign the segment type. \nTherefore, the music object of Figure 5 is represented by the \nsequence of (B,3,-3) (A,1,+1) (D,3,-3) (B,1,-2) (C,1,+2) (C,1,+2) \n(C,1,+1). In priori to describing the dedicated indexing structures \nfor APS, we introduce a data structure named suffix tree . A suffix \ntree is originally developed for substring matching [11][15]. \nFor example, Figure 7 shows the suffix tree of the string S \n= ”ABCAB”. Each leaf node (denoted by a box) corresponds to a \nsubstring starting at the position indicated in the node in S, and \neach link is labeled with a symbol α, where α ∈ ∑ ∪ {$}, ∑ is the \nalphabet of S and ‘$’ is a special symbol denoting end-of-string. \nAs a result, all the suffixes, i.e., “ABCAB”, “BCAB”, “CAB”, \n“AB”, and “B”, are organized in the tree. For a query string, the \nmatching processing is a typical tree traversal. For example, \nsuppose that the query string is “AB”. We follow the leftmost path \nto the black node, and all leaf nodes descending from the black \nnode are the results, i.e., the first and the forth position. \n \nFigure 5: A piece of music. (b) R1 = (a) \n(c)\n1:7 1:1 1:4 1:2 1:5\n2:1 1:3 1:6\n2:9 2:7 1:9 1:8\n2:8 2:2 2:5\n2:3 2:6\n2:42:10\n2:11 2:121:10 1:11\n1:12\n1:13do re mi fa so la si\n1:7 1:2 1:5\n2:1 1:3 1:6\n2:9 1:9 1:8\n2:2 2:5\n2:6 2:10\n2:11 2:12do re mi\nstart end1:7 1:2 1:5\n2:1 1:3 1:6\n2:9 1:9 1:8\n2:2 2:5\n2:6 2:10\n2:11 2:12do re mi\nstart end\n(a) (b) (c)\n \nFigure 4: (a) The index structure of M1 and M2 for the 1D-List approach. (b) An example of exact query Q = “do-re-mi”. (c) The \nexact link and result of the query Q. \n \nnote number\nbeat606265\n6467\n(B, 3, -3)(A, 1, +1)\n(D, 3, -3)\n(B, 1, -2)(C, 1, +2)(C, 1, +2)(C, 1, +1)\n \nFigure 6: The corresponding sequence of music segments of the \nmusic object in Figure 5. \n3.3.2 Index Structures for Sequences of Music \nSegments \nIn the following, we introduce two index structures for efficiently \nprocessing queries of music segments, i.e., the one-dimensional \nand two-dimensional augmented suffix trees. \nA one-dimensional augmented suffix tree (1-D AST, in short) is a \nsuffix tree with the segment duration information being added to \nthe edges. First, a suffix tree based on the sequences of the \nsegment types is constructed. Each edge of the suffix tree refers to \na symbol appearing in one or more positions in the sequence. For \nexample, let the sequence of music segments be (A,2,+1) (B,5,-1) \n(C,1,+1) (A,3,+1) (B,3,-2). Using only the segment types, the \nsuffix tree can be constructed as shown in Figure 7. The bold-faced \nedge in Figure 7 refers to the ‘B’ in the second and fifth position. \nSince the corresponding segment durations are 5 and 3, we attach \nthe range of segment duration < min, max> = < 3, 5>  to the  e dg e . \nThis range can be used to filter out some results which cannot be \nanswers during query processing. Figure 8 shows an example of a \n1-D AST. To exploit the filtering effect, the range < min, max > should be as \ncompact as possible. For a given population of segment durations, \nsuch as {1, 2, 2, 3, 7, 8, 8}, two ranges <1, 3> and <7, 8> are \nbetter than one range <1, 8>. Thus, the edge should be split into \ntwo edges labeled with <1, 3> and <7, 8>. This method is called \ndynamic splitting . In some cases, however, if it is hard to find \ncompact ranges from a given population, we may apply static \nsplitting  method by splitting a range into some predefined smaller \nranges which can be obtained from the statistics of data set. \nThe 2-D AST is an extension of the 1-D AST by attaching both \nsegment duration and segment pitch information to the edge. \n3.3.3 Query Processing \nThe query processing for the augmented suffix tree is called the \nthresholding-based matching , which is able to deal with both exact \nand approximate queries [2]. The approximation degree of the \nquery is specified by means of thresholds. The exact queries can be \nconsidered as a special case with the thresholds being set to zero. \nFor ease of illustration, we only show the processing of exact \nqueries in the following. \nBased on the 1-D AST in Figure 8(b), given the query Q = (A,1, −) \n(C,2, −) (A,5, −), we find the music objects whose sequences of \nmusic segments contain Q. When processing queries against a 1-D \nAST, the segment pitch in the queries is not needed and denoted \nby ‘−‘. \nThe tree traversal starts from the root node and goes as follows. \nWhen processing the first music segment (A,1, −), the edge A<1, \n1> is matched such that we reach the node N 1. Then, when \nprocessing the second music segment (C,2, −), the edge C<1, 3> is \nsatisfied because the duration of the music segment is covered by \nthe range of the edge. For the last music segment (A,5, −), although \nthe segment type of the two edges from node N 2 is m a tc he d, the  \ntwo edges are filtered out because the duration of the music \nsegment is not covered by any ranges of the edges. Therefore, the \nprocessing terminates without any answer. Note that the results \nderived from this tree traversal are not necessary the answers to the \nquery. Further verification of the results is required. 1 4A\nB C\nB C\nC $$\n253\n \nFigure 7: The suffix tree of the string S ==== “ABCAB”. \n(a) root  A \nC A \n(b) root  A<1,1>\nC<7, 8>C<1, 3> A<7,8>  A<3,4>  N1 \nN2 \n \nFigure 8: (a) An example of suffix tree. (b) The 1-D augmented \nsuffix tree. \n4. The Efficiency Study \nIn this section, we show the experiment results on the efficiency of \nthe three approaches described in Section 3. For the APS approach, \nboth 1-D AST and 2-D AST are implemented. For comparison, we \nalso construct a suffix tree, denoted as ST, based on the segment \ntypes of music segment sequences. \n4.1 Index Construction \nThe elapsed time and memory usage for constructing indices of the \nthree approaches are illustrated as follows. For the APM approach, \nthe tree height of L-tree is set to 6 in our experiments. As shown in \nFigure 9 and Figure 10, both the elapsed time and the memory \nusage of 1D-List are less than those of L-tree. This is because the \nconstruction of 1D-List is a simple process of transforming the \nmelody strings to the linked lists, and the number of nodes in the \n1D-List is linear to the database size. \nThe suffix tree-like data structures including the augmented suffix \ntrees in the APS approach suffer from the space consumption. It is \nnot reasonable to construct a full and complete augmented suffix \ntree just for handling the rare cases of extremely long-length \nqueries. On the contrary, an augmented suffix tree with longer tree \nheights is beneficial to the efficiency of query processing. In our \nexperiments, the tree height of augmented suffix tree is set to 4, 6, \n8, 10, and 12. We construct three indices of APS, i.e., ST, 1-D \nAST, and 2-D AST. As described in Section 3.3, we apply the \nstatic splitting method to divide the domain of duration into three \nranges and the domain of pitch into two ranges. Obviously, the \nelapsed time and memory usage of the three indices ST, 1-D AST, \nand 2-D AST are increasing. We only show the construction of the \n2-D AST in Figure 11 and Figure 12, where ‘h_4’ indicates the \ntree height of four, ‘h_6’ indicates tree height of six, and so on. \nThe elapsed time and memory usage in the cases of smaller tree \nheights are much less than the cases of larger tree heights.  4.2 Exact Query Processing \nIn the following, we discuss the efficiency of processing exact \nqueries for the APM, 1D-Lst, and APS approaches. Factors of \nquery length, number of objects, and tree height of indices of APS \nwill be investigated. \n015304560\n01 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0\n# of objectselapsed time (second)1D-List\nAPM\n \nFigure 9: Elapsed time vs. # of objects for index construction of \nAPM (L-tree, h ====6) and 1D-List. \n075150225300\n500 1000 1500 2000 2500 3000 3500\n# of objectsmemory usage (MB)1D-List\nAPM\n \nFigure 10: Memory usage vs. # of objects for index \nconstruction of APM (L-tree, h ====6) and 1D-List. \n0255075100\n05 0 0 1 0 0 0 1 5 0 0 2 0 0 0 2 5 0 0 3 0 0 0 3 5 0 0 4 0 0 0\n# of objectselaspsed time (second)h_4\nh_6\nh_8\nh_10\nh_12\n \nFigure 11: Elapsed time vs. # of objects for index construction \nof APS (2-D AST).  0200400600800\n500 1000 1500 2000 2500 3000 3500\n# of objectsmemory usage (MB)h_4\nh_6\nh_8\nh_10\nh_12\n \nFigure 12: Memory usage vs. # of objects for index \nconstruction of APS (2-D AST). \nFor the APM and 1D-List approaches, the factors of query length \nand number of objects are explored, as shown in Figure 13, Figure \n14, and Figure 15. Figure 13 shows the scalability of two \napproaches. The query length for 1D-List, denoted by |Qn|, is of \ntwelve notes. Accordingly, the query length for APM, denoted by \n|Qm|, is of three mubols. Compared to the APM approach, the 1D-\nList approach scales well as the number of music objects. \nFigure 14 and Figure 15 show the elapsed time versus query length \nof APM and 1D-List, where ‘obj_0.5K’ indicates five hundred \nmusic objects in the experiment, ‘obj_1.0K’ indicates one \nthousand objects, and so on. For the APM approach, as shown in \nFigure 14, the elapsed time decreases rapidly when processing \nqueries of length from one to six. As processing queries of length \nseven, the elapsed time rises up substantially. In the experiment \nsetting, the tree height of L-tree is six. As in Section 3.1, if the \nquery is of length seven, it will be divided into two subqueries. As \na result, two times of the L-tree traversal are required. In addition, \nthe join processing also contributes extra elapsed time. For the \nqueries of length from seven to thirteen, similar behavior can be \nobserved. When processing queries of length from seven to twelve, \nthe elapsed time decreases. As processing the queries of length \nthirteen, the elapsed time rises up again, and so on. For the 1D-Lsit \napproach, Figure 15 shows the elapsed time versus the query \nlength. The elapsed time increases slightly for query lengths \nranging from 1 to 10, and remains almost the same for longer \nqueries. Since only the lists involved in the query are retrieved for \nbuilding exact links, the elapsed time is linear to the query length.  \nThe elapsed time consists of the time for building links and \ntraversing links. When dealing with shorter queries, the number of \nlists to be processed is small and the elapsed time increases slightly. \nWhen dealing with longer queries, although the number of lists to \nbe processed increases, the number of answers to the query \ndramatically reduces such that the elapsed time remains almost the \nsame. In our experiment, the number of answers is less than 2 for \nthe query of lengths ranging from 16 to 64. \nFor APS, factors of query length, number of objects, and tree \nheight of the three indices are explored as follows. \nFigure 16 shows the scalability of APS with 1-D AST and 2-D \nAST of tree height of eight. The APS with ST is not included \nbecause of a much larger elapsed time under the same condition. Compared to the 1-D AST, the 2-D AST performs well as the \nnumber of objects increases. \n0481216\n01 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0\n# of objectselapsed time (millisec.)1D-List\nAPM\n \nFigure 13: Elapsed time vs. # of objects for query processing of \nAPM (|Qm| ====3, L-tree, h ====6) and 1D-List (|Qn| ====12). \n0204060\n02468 1 0 1 2 1 4 1 6 1 8 2 0 2 2 2 4 2 6 2 8 3 0 3 2 3 4\nquery length (mubol)elapsed time (millisec.)obj_0.5K\nobj_1.0K\nobj_1.5K\nobj_2.0K\nobj_2.5K\nobj_3.0K\nobj_3.5K\n \nFigure 14: Elapsed time vs. query length for query processing \nof APM (L-tree, h ====6). \n0.001.002.003.004.005.006.007.008.00\n01 0 2 0 3 0 4 0 5 0 6 0 7 0\nquery length (note)elapsed time (millisec.)obj_0.5Kobj_1.0K\nobj_1.5K obj_2.0K\nobj_2.5K obj_3.0K\nobj_3.5K\n \nFigure 15: Elapsed time vs. query length for query processing \nof 1D-List. 00.250.50.751\n01 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0\n# of objectselapsed time (second)2-D AST1-D AST\n \nFigure 16: Elapsed time vs. # of objects for query processing of \nAPS (1-D AST and 2-D AST, |Qs| ====8, h====8). \nFigure 17, Figure 18, and Figure 19 show the elapsed time versus \nquery length for ST, 1-D AST, and 2-D AST, respectively. The \ncurves in Figure 19 have a similar trend to the curves in Figure 14. \nHowever, for shorter queries ranging from one to eight music \nsegments, such kind of trend is not obvious in APS. Two reasons \nare given as follows. In APM, leaf nodes are regarded as results, \nwhile leaf nodes of APS are just candidates for further \nconfirmation. In addition, the number of leaf nodes retrieved in \nAPS is much more than the one in APM. For example, after the \ntree traversal, there are four leaf nodes for four-mubol queries in \nAPM, while there are 16968, 5429, 105 nodes for four-segment \nqueries in APS with the index of ST, 1-D AST, and 2-D AST of \ntree height twelve, respectively. Post processing of a large number \nof candidates results in extra computation which smoothes the \ncurves. \nThe total elapsed time of query processing in APS consists of three \nparts, i.e., tree traversal, joining processing (if the query length is \nlonger than the tree height), and post processing (for similarity \ncomputation). Among the three parts, the post processing \nconsumes most of the elapsed time. For example, with the 2-D \nAST of tree height ten, the total elapsed time of processing a ten-\nsegment query is 811 milliseconds, where 10 milliseconds for tree \ntraversal and 801 milliseconds for computing similarity. When \nprocessing queries whose length is longer than the tree height, the \nquery will be divided into subqueries. The number of candidates \nwill be reduced after the joining processing. However, our \ndatabase of 3500 music objects is only of moderate size. No matter \nwhat the tree height is, the number of candidates does not change \nmuch. Therefore, the difference of the performance with various \ntree heights is not obvious in our experiments, as shown in Figure \n17, Figure 18, and Figure 19. We believe that, when dealing with \nmuch more music objects in databases, the influence of tree height \nwill be revealed. \nFor comparison, we show the elapsed time for different indices in \nFigure 20. The performance gain of 2-D AST is obvious because \nof substantial edge pruning and candidate reduction. \nIn the following, we show the filtering effect of APS by applying \n1-D AST and 2-D AST. The number of candidates is the number \nof leaf nodes retrieved after tree traversal. The filtering effect is \nmeasured by the candidate reduction rate (CRR ), which is defined \nas the ratio of the number of reduced candidates using 1-D AST or \n2-D AST to the number of candidates using ST. STAST STNN NCRR  −=  \nwhere NST denotes the number of candidates by applying ST and \nNAST denotes the one by 1-D AST or 2-D AST. \n010203040\n02468 1 0 1 2 1 4 1 6 1 8 2 0 2 2 2 4 2 6 2 8 3 0 3 2 3 4\nquery length (segment)elapsed time (second)h_4\nh_6\nh_8\nh_10\nh_12\n \nFigure 17: Elapsed time vs. query length for query processing \nof APS (ST, 3.5K objects). \n010203040\n02468 1 0 1 2 1 4 1 6 1 8 2 0 2 2 2 4 2 6 2 8 3 0 3 2 3 4\nquery length (segment)elapsed time (second)h_4\nh_6\nh_8\nh_10\nh_12\n \nFigure 18: Elapsed time vs. query length for query processing \nof APS (1-D AST, 3.5K objects). \n03691215\n02468 1 0 1 2 1 4 1 6 1 8 2 0 2 2 2 4 2 6 2 8 3 0 3 2 3 4\nquery length (segment)elapsed time (second)h_4\nh_6\nh_8\nh_10\nh_12\n \nFigure 19: Elapsed time vs. query length for query processing \nof APS (2-D AST, 3.5K objects). 010203040\n02468 1 0 1 2 1 4 1 6 1 8 2 0 2 2 2 4 2 6 2 8 3 0 3 2 3 4\nquery length (segment)elapsed time (second)ST\n1-D AST2-D AST\n \nFigure 20: Elapsed time vs. query length for comparison of \nquery processing of APS using various indices (h ====12, 3.5K \nobjects). \nHigher reduction rates suggest better filtering effects. As shown in \nFigure 21, there are two kinds of curves with respect to the \ncorresponding y-axis. The ‘ST’, ‘1-D AST’, and ‘2-D AST’ \nindicate the number of candidates applying the corresponding \nindices. The ‘R1D’ and ‘R2D’ indicate the CRR of the \ncorresponding indices. \nFor the 1-D AST, the CRR increases as the query length is less \nthan 14, while the ratio decreases as the query length ranges from \n15 to 32. For the 2-D AST, since there are much fewer candidates, \nthe CRR for the query lengths ranging from 1 to 24 is at least 80%. \nFor the longer queries, the CRR is decreased to 67%. \nFor shorter queries, the APS approaches with 1-D AST and 2-D \nAST get benefits through attaching the beat and pitch information. \nHowever, for longer queries, all the methods have fewer candidates \nsuch that the filtering effect decreases slightly. For example, as the \nquery lengths range from 24 to 32, the number of candidates for \nST, 1-D AST, and 2-D AST is 3, 1, and 1, respectively. In general, \nthe filtering effect of 2-D AST is better than that of 1-D AST. \nMoreover, a significant reduction of the candidates can be \nachieved using our approaches as the query length is less than 14. \n050100150200\n13579 1 1 1 3 1 5 1 7 1 9 2 1 2 3 2 5 2 7 2 9 3 1\nquery length (segment)# of candidates (x 1000)00.20.40.60.81\nCRR (%)ST\n1-D AST\n2-D AST\nR1D\nR2D\n \nFigure 21: Reduction rate vs. query length for comparison of \nquery processing of APS using various indices (h ====12, 3.5K \nobjects). \n4.3 Summary of the Experiment Results \nFollowing the comprehensive illustrations of the performance with \nrespect to each approach, we summarize the experiment results for \na comparison in Table 2. Four sets of query lengths for query processing are selected, i.e., 1, 2, 3, and 4 mubols for APM, 4, 7, \n10, and 12 notes for 1D-List, and 4, 8, 12, and 14 music segments \nfor APS. \nFor reference, we also implement the string matching methods, \nnamely, STR_MAT_n, and STR_MAT_ms. STR_MAT_n is a \nstandard string matching method using the indexOF  function in \nJava, which can be used to compare melody strings. On the other \nhand, STR_MAT_ms is for comparing sequences of music \nsegments, which match segment types, followed by a checking for \nsegment duration and segment pitch.  \nWe summarize the experiment results as follows.  \nFirst, the 1D-List approach is superior in terms of indexing and \nquery processing. However, the melody string of 1D-List approach \nis coded as the string of pitch values ( i.e., the note number in MIDI \nstandard). If the MIR system is designed for end users and the \nquery approximation is one of major concerns, 1D-List may not \nresult in good effectiveness. If it is the case of exact searching from \nthe bibliographic catalog, the 1D-List approach is suggested. \nSecond, the APM outperforms the APS family. Two reasons are \ngiven as follows. The APS family needs an extra cost for post \nprocessing. In addition, the average number of branches of a tree \nnode in L-tree is much more than that of AST. It results in fewer \ncandidates of APM. Therefore, the elapsed time of APS family is \nmore than that of APM. \nThird, constructing indices for the APS family is not always \nbeneficial to query processing, especially when the query length is \nless than four music segments. For longer query lengths, the \nperformance of 2-D AST is impressive, as shown in Figure 20 and \nTable 2. In addition, the performance difference between the 2-D \nAST with various tree heights is limited, as shown in Figure 17, \nFigure 18, and Figure 19. Therefore, for the APS family, we \nsuggest using the 2-D AST of smaller tree heights. This is because \nthe index size of 2-D AST substantially reduces when the tree \nheight is smaller. For example, as shown in Figure 12, the index \nsize of 2-D AST of tree height 12 is 774.46 MB, while that of tree \nheight 10 is 461.57 MB.  \n5. Conclusion \nIn this paper, we describe the Ultima project which aims at \nbuilding a platform for evaluating the performance of various \napproaches for music information retrieval. The issues of system \ndesign, query set generation, and performance study are discussed. \nThe list-based, tree-based, ( n-gram +tree)-based approaches are \nconsidered. Concerning the efficiency study, a series of \nexperiments are conducted. The factors of database size, query \nlength, tree height are investigated. We also provide a comparative \nstudy and summarization of the three approaches.  \nFuture work include a performance evaluation of retrieval \neffectiveness among these approaches. Also, various input \nmethods, the summarization module, and the query generation \nmodule will be implemented. The dynamic programming-based \napproaches, which are not covered in this project yet, will be \nconsidered in the next stage. While more and more polyphonic \nmusic retrieval methods are proposed, we also plan to extend our \nproject to build a database of polyphonic music objects for \nevaluating these methods. \n Table 2: The comparison of various approaches. \nIndex Exact query processing(1)(2) (millisec.) \nApproach  \n(|DB| = 3500) Size (MB)Time \n(sec.) |Qm| = 1 mubol \n|Qs| = 4 notes \n|Qn| = 4 segments|Qm| = 2  \n|Qs| = 7  \n|Qn| = 8 |Qm| = 3  \n|Qs| = 10 \n|Qn| = 12 |Qm| = 4  \n|Qs| = 12  \n|Qn| = 14 In average\nAPM (L-tree, h=6) 289.0 52.5 50.6 23.8 13.6 10.1 24.5 \n1D-List 48.3 33.7 4.0 4.0 4.1 4.0 4.0 \nSTR_MAT_n N/A N/A 861.0 852.0 852.0 851.0 854.0 \nAPS (ST, h=12) 48.3 39.9 23767.0 9239.0 2899.0 1271.0 9294.0 \nAPS (1-D AST, h=12) 290.7 54.0 10882.0 1630.0 416.0 195.0 3280.1 \nAPS (2-D AST, h=12) 774.5 90.0 1570.0 244.0 96.0 9.0 479.8 \nSTR_MAT_ms N/A N/A 2974.0 2814.0 2794.0 2814.0 2849.0 \nNote:  \n(1) Qn, Qm, Qs indicate that queries are coded as melody strings for the 1D-List approach, mubol strings for the APM approach, \nand sequences of music segments for the APS approach, respectively.  \n(2) |Qn|, |Qm|, and |Qs| indicate the length of queries in note, mubol, and music segment, respectively.  \n \nAcknowledgment \nWe would like to thank the CWEB Technology, Inc., for sharing us \nthe data set used in our experiments. \nReferences:  \n[1] Blackburn, S. & DeRoure, D. (1998). A tool for content-based \nnavigation of music. In Proc. of ACM Multimedia. \n[2] Chen, A. L. P., Chang, M., Chen, J., Hsu, J. L., Hsu, C. H. & \nHua, S. Y. S. (2000). Query by music segments: An efficient \napproach for song retrieval. In Proc. of IEEE Intl. Conf. on \nMultimedia and Expo (ICME). New York. \n[3] Chen, J. C. C. & Chen, A. L. P. (1998). Query by rhythm: An \napproach for song retrieval in music databases. In Proc. of the \n8th Intl. Workshop on Research Issues in Data Engineering, \n(pp. 139-146).  \n[4] Chou, T. C., Chen, A. L. P., & Liu, C. C. (1996). Music \ndatabases: Indexing techniques and implementation. In Proc. \nof IEEE Intl. Workshop on Multimedia Data Base \nManagement System.  \n[5] Clausen, M., Engelbrecht, R., Mayer, D. & Smith, J. (2000). \nPROMS: A web-based tool for searching in polyphonic music.  \n[6] DeRoure, D. & Blackburn, S. (2000). Content-based \nnavigation of music using melodic pitch contours. Multimedia \nSystems, 8(3), Springer. (pp. 190-200).  \n[7] Downie, S. (2000). Thinking about formal MIR system \nevaluation: Some prompting thoughts. Available on \nhttp://www.lis.uiuc.edu/~jdownie/mir_papers/downie_mir_eva\nl.html. \n[8] Downie, S. & Nelson, M. (2000). Evaluation of a simple and \neffective music information retrieval method. In Proc. of ACM \nSIGIR, (pp. 73-80). \n[9] Frakes, W. B. & Baeza-Yates, R. ( 1992). Information retrieval: \nData structures and algorithms, Prentice-Hall. \n[10] Ghias, A., Logan, H., Chamberlin, D., & Smith, B. C. (1995). \nQuery by humming: Musical information retrieval in an audio \ndatabase. In Proc. of ACM Multimedia, (pp. 231-236). \n[11] Gusfield, D. (1997). Algorithms on strings, trees, and \nsequences. Cambridge University Press. \n[12] Lee, W. & Chen, A. L. P. (2000). Efficient multi-feature index \nstructures for music data retrieval. In Proc. of SPIE Conference on Storage and Retrieval for Image and Video \nDatabases. \n[13] Liu, C. C., Hsu, J. L., & Chen, A. L. P. (1999). An \napproximate string matching algorithm for content-based \nmusic data retrieval. In Proc. of Intl. Conference on \nMultimedia Computing and Systems (ICMCS’99). \n[14] Lemstrom, K. & Perttu, S. (2000). SEMEX: An efficient \nmusic retrieval prototype. In Proc. of Intl. Symposium on \nMusic Information Retrieval. \n[15] McCreight, E. M. (1976). A sp ace ec onomical suffix tree \nconstruction algorithm. Journal of Assoc. Comput. Mach., 23, \n262-272. \n[16] MIDI Manufactures Association (MMA), MIDI 1.0 \nSpecification, http://www.midi.org/. \n[17] McNab, R. J., Smith, L. S., Witten, I. H., & Henderson, C. L. \n(2000). Tune retrieval in the multimedia library. Multimedia \nTools and Applications, 10(2/3), Kluwer Academic Publishers. \n[18] Salton, G. & McGill, M. (1983). Introduction to modern \ninformation retrieval. MaGraw-Hill Book Company.  \n[19] Selfridge-Field, E. (1998). Conceptual and representational \nissues in melodic comparison. In Hewlett, W. B. & Selfridge-\nField E. (Ed.), Melodic similarity: Concepts, procedures, and \napplications (Computing in Musicology: 11), The MIT Press. \n[20] Tseng, Y. H. (1999). Content-based retrieval for music \ncollections. In Proc. of ACM SIGIR. \n[21] Uitdenbogerd, A. & Zobel, J. (1998). Manipulation of music \nfor melody matching. In Proc. of the 6th ACM Intl. \nMultimedia Conference, (pp. 235-240). \n[22] Uitdenbogerd, A. & Zobel, J. (1999). Melodic matching \ntechniques for large music databases. In Proc. of the 7th ACM \nIntl. Multimedia Conference, (pp. 57-66). \n[23] Witten, I. H., Moffat, A., & Bell, T. C. (1994). Managing \ngigabytes: compressing and indexing documents and images, \nInternational Thomson Publishing company. \n[24] Yanase, T. & Takasu, A. (1999). Phrase based feature \nextraction for musical information retrieval. In Proc. of IEEE \nPacific Rim Conf. on Communications, Computers, and Signal \nProcessing. \n[25] Yip, C. L. & Kao, B. (2000). A study on n-gram indexing of \nmusical features. In Proc. of IEEE ICME."
    },
    {
        "title": "Computer Analysis of Musical Allusions.",
        "author": [
            "David Cope"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.7679672",
        "url": "https://doi.org/10.5281/zenodo.7679672",
        "ee": "https://zenodo.org/records/7679672/files/word-art-2023-1-3.pdf",
        "abstract": "The article imparts the information on religiously marked allusions, their symbolic representations in English literary texts and analysis from the perspectives of cognitive linguistics. Among its other aspects, the research places particular emphasis on symbolic functions outperformed by allusions and in order to reveal the symbolic significance generated in the semantic layer of religiously marked allusions, the methods of conceptual analysis and conceptual blending were applied. The results of the analysis show that religiously marked allusions activate religious knowledge structures, implicitly symbolize abstract phenomena, convey conceptual meaning and express the author&#39;s individual world picture.",
        "zenodo_id": 7679672,
        "dblp_key": "conf/ismir/Cope01",
        "keywords": [
            "religiously marked allusions",
            "symbolic representations",
            "English literary texts",
            "cognitive linguistics",
            "symbolic functions",
            "concepts",
            "conceptual analysis",
            "conceptual blending",
            "semantic layer",
            "religious knowledge structures"
        ],
        "content": "dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n1 \n   \ndǩZ\u0003dS`mSe[\u0003hS^ ȉSca\u0003Yfc`S^[ \u0003\n \u0003Y[^W\u0003\u001b\u0003da`\u0003\n\u0003\n_XYWf`ScaW`n\\\u0003Yfc`S^\u0003[d]fddeUa\u0003d^aUS \u0003\nea_\u0003 \u0003`a_Xc\u0003\u001b\u0003\n\u0003\n\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b\u0003\t\u000e\u0014\u0011\rɮ\u000b\u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013 \u0003\n\u0015\u000e\u000b\u0014\f\u0004\u0003  \u0003\b\u0012\u0012\u0014\u0004\u0003\u001b\u0003\n\u0003ISSN 2181 -9297  \n \nDoi Journal 10.26739/2181 -9297  \nTОШКЕНТ -202 3 \n  dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n2 \n Контакт редакций журналов. www.tadqiqot.uz  \nООО Tadqiqot город Ташкент,  \nулица Амира Темура пр.1, дом -2. \nWeb: http://www.tadqiqot.uz/; E -mail: info@tadqiqot.uz  \nТел: (+998 -94) 404 -0000    \nБош му ҳаррир:  \nТўҳтасинов Ил ҳом  \nп.ф.д.,  профессор  (Ўзбекистон)   \n \nБош му ҳаррир ўринбосари:  \n \n \n \nТаҳрир ҳайъати:  \n \nНазаров Бахтиёр  \nакадемик. (Ўзбекистон)  \n \nЯкуб Умарў ғли \nф.ф.д., профессор (Туркия)  \n \nАлмаз Улви Биннатова  \nф.ф.д., профессор ( Озарбайжон)  \n \nБоқиева Гуландом  \nф.ф.д., профессор (Ўзбекистон)  \n \nМиннуллин Ким  \nф.ф.д., профессор (Татаристон)  \n \nМаҳмудов Низомиддин  \nф.ф.д., профессор (Ўзбекистон)  \n \nКеримов Исмаил  \nф.ф.д., профессор (Россия)  \n \nЖўраев Мамат қул \nф.ф.д., профессор ( Ўзбекистон)  \n \nKуренов Рахиммамед  \nк.ф.н. (Tуркманистон)  \n \nКристофер Жеймс Форт  \nМичиган университети (А ҚШ) \n \nУмархўжаев Мухтор  \nф.ф.д., профессор (Ўзбекистон)  \n \nМирзаев Ибодулло  \nф.ф.д., профессор (Ўзбекистон)  \n \nБолтабоев Ҳамидулла  \nф.ф.д., профессор (Ўзбекистон)  \n \nДўстму ҳаммедов Хуршид  \nф.ф.д., профессор (Ўзбекистон)  \n \nЛиходзиевский А.С.  \nф.ф.д., профессор (Ўзбекистон)  \n \nСидди қова Ирода  \nф.ф.д., профессор (Ўзбекистон)  \n \nШиукашвили Тамар  \nф.ф.д. (Грузия)  \n \nЮсупов Ойбек  \nмасьул котиб, доцент (Ўзбекистон)  \n\u000f+1/7+5/<\u0003\u0003¬\u0003Ux}s\u0003¬\u0003dsȘ{s~u{\u0003h{w\u0003_{zsxwu \u0003\nEditorial staff of the journals of www.tadqiqot.uz  \nTadqiqot LLC The city of Tashkent,  \nAmir Temur Street pr.1, House 2.  \nWeb: http://www.tadqiqot.uz/; E -mail: info@tadqiqot.uz  \nPhone: (+998 -94) 404 -0000  Editor in Chief:  \nTuhtasinov Ilhom  \nDSc. Professor (Uzbekistan)  \n \nDeputy Chief Editor  \n \n \n \nEditorial Board:  \n \nBakhtiyor Nazarov  \nacademician. (Uzbekistan)  \n \nYakub Umarogli  \nDoc. of philol. scien., prof. (Turkey)  \n \nAlmaz Ulvi Binnatova  \nDoc. of philol. scien., prof. (Azerbaijan)  \n \nBakieva Gulandom  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nMinnulin Kim  \nDoc. of philol. scien., prof. (Tatarstan)  \n \nMahmudov Nizomiddin  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nKerimov Ismail  \nDoc. of philol. scien., prof. (Russia)  \n \nJuraev Mamatkul  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nKurenov Rakhimmamed  \nPh.D. Ass. Prof. (Turkmenistan)  \n \nChristopher James Fort  \nUniversity of Michigan (USA)  \n \nUmarkhodjaev Mukhtar  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nMirzaev Ibodulla  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nBoltaboev Hamidulla  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nDustmuhammedov Khurshid  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nLixodzievsky A.S.  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nSiddiqova Iroda  \nDoc. of philol. scien., prof. (Uzbekistan)  \n \nShiukashvili Tamar  \nDoc. of philol. scien.  (Georgia)  \n \nYusupov Oybek  \nAss. prof. (Uzbekistan) - Senior Secretary  Главный редактор:  \nТухтасинов Илхом  \nд.п.н., профессор  (Узбекистан)  \n \nЗаместитель главного редактора:  \n \n \n \nРедакционная коллегия:  \n \nНазаров Бахтиёр  \nакадемик. (Узбекистан)  \n \nЯкуб Умар оглы  \nд.ф.н., профессор (Туркия)  \n \nАлмаз Улви Биннатова  \nд.ф.н., профессор (Азербайджан)  \n \nБакиева Гуландом  \nд.ф.н., профессор (Узбекистан)  \n \nМиннуллин Ким  \nд.ф.н., профессор (Татарстан)  \n \nМахмудов Низомиддин  \nд.ф.н., профессор (Узбекистан)  \n \nКеримов Исмаил  \nд.ф.н., профессор (Россия)  \n \nДжураев Маматкул  \nд.ф.н., профессор (Узбекистан)  \n \nKуренов Рахыммамед  \nк.ф.н. (Туркменистан)  \n \nКристофер Джеймс Форт  \nУниверситет Мичигана (США)  \n \nУмархаджаев Мухтар  \nд.ф.н., профессор (Узбекистан)  \n \nМирзаев Ибодулло  \nд.ф.н., профессор (Узбекистан)  \n \nБалтабаев Хамидулла  \nд.ф.н., профессор (Узбекистан)  \n \nДустмухаммедов Хуршид  \nд.ф.н., профессор (Узбекистан)  \n \nЛиходзиевский А.С.  \nд.ф.н., профессор (Узбекистан)  \n \nСиддикова Ирода  \nд.ф.н., профессор (Узбекистан)  \n \nШиукашвили Тамар  \nд.ф.н. (Грузия)  \n \nЮсупов Ойбек  \nотв. секретарь , доцент  (Узбекистан)  \n dǩZ\u0003dS`mSe[\u0003hS^ ȉSca\u0003Yfc`S^[\u0003\u0003\u0003\n_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa\u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b\u0003\t\u000e\u0014\u0011\rɮ\u000b\u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013 \u0003\n№1 (2023) DOI http://dx.doi.org/10.26739/2181 -9297 -2023-1 \neSȗc[c[\\\u0003_Sd^SȗSe\u0003]X`VSk[\u0003\u0003 cXWS]i[a``n\\\u0003daUXe \u0003 \u0004ɖ\b\u0013\u000e\u0011\bɮ\u000b\u0003ɸ\u000eɮ\u0011ɖ \u0003dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n3 \n  \n1. Нурмуратов Анвар Яхшибаевич  \nМАСОФАВИЙ ТАЪЛИМ ТАШКИЛ ЭТИШНИНГ ИМКОНИЯТ ВА ХУСУСИЯТЛАРИ ………………………... 5 \n \n2. Yusuf Nurmuhammedov Shakarboyevich, Bahodir Nurmuhammedov Shakarboyevich  \nQO‘SHRABOT TUMANIDAGI AYRIM TOPONIMLARNING ETIMOLOGIK VA SINTAKTIK TALQINI ……… 12 \n \n3. Mukhayyo Fayzullaeva  \nSYMBOLIC REPRESENTATIONS OF RELIGIOUSLY MARKED ALLUSIONS  \nIN THE LITERARY TEXT ……………………………………………………………………………………………... 18 \n \n4. Сапарова  Мохира  Файзуллаевна  \nПЕРЕВОДЧЕСКОЕ ИСКУССТВО, В ЧАСТНОСТИ, ТЕОРЕТИЧЕСКИЕ ОСНОВЫ ЗНАНИЙ О \nХУДОЖЕСТВЕННОМ ПЕРЕВОДЕ ……… …… ……………………… ……………… …………… ……… …… …...24 \n \n5. Dilnoza Baxodirovna Buronova  \nMUALLIF BADIIY USLUBINI O‘RGANISHNING NAZARIY METODOLOGIK ASOSLARI ……………………. 32  \n \n6. Abduraxmonova Nilufar Zaynobiddin qizi, Raximov Xasanboy Komiljonovich  \nO‘ZBEK TILI SENTIMENT ANALIZNING NAZARIY MASALALARI ……………………………………………. 36 \n \n7. Elov Botir Boltayevich, Hamroyeva Shahlo Mirjonovna, Xusainova Zilola Yuldashevna  \nNLPNING ZAMONAVIY ALGORITMLARI VA KONSE PSIYALARI ……………………………………………… 42 \n \n8. Xurshida Narxodjayeva  \nRASMIY MULOQOT MATNINING LINGVOMADANIY XUSUSIYATLARI ……………………………………... 58 \n \n9. Ulikova Mavluda  \nTILSHUNOSLIKDA KONSEPT TUSHUNCHASINI AKS ETTIRISHNING MILLIY -MADANIY \nXUSUSIYATLARI ……………………………………………………………………………………………………… 63 \n \n10. Абдувалиев  Маҳаматжон  Арабович   \nКОНЦЕПТЛАР  ТАД ҚИҚИДА  ЛИНГВИСТИК  КОНТЕКСТНИНГ  ЎРНИ ………………………………………... 68 \n \n11. Эргашева Зиёда Абдурасуловна  \nКАУЗАЛЛИК ВА СУБЪЕКТИВЛИК МУНОСАБАТЛАРИНИНГ ВО ҚЕЛАНИШИ ………………… …………..74 \n \n12. Мардонова Ситора Мардоновна  \nСЕМАНТИЧЕСКИЙ АНАЛИЗ ГЛАГОЛОВ, ВХОДЯЩИХ В 4 КАТЕГОРИИ  \nВ ДРЕВНЕАНГЛИЙСКОМ ЯЗЫКЕ ………………………………………………………………………………….. 78 \n \n13. Аманов Рахмон Аслонович, Шарипова Латифа Хакимовна  \nБАЛО ҒАТ ЁШИДАГИ Ў ҚУВЧИ ҚИЗЛАРДА ТЕМИР ТАН ҚИСЛИК КАМ ҚОНЛИГИ  \nВА ЯШИРИН ТЕМИР ТАНКИСЛИК КАМКОНЛИК КЛИНИКАСИ ВА ДИАГНОСТИКАСИ… …………… …84 \n \n14. Ҳамидов Лутфулло Паизуллаевич  \nБАНК МАТНЛАРИНИНГ ТИПОЛОГИЯСИНИ ЯРАТИШ МУАММОСИ…………………………………… …..88 \n \n15. Туракулова Зарина Мардонкуловна  \nУСТАРЕВШАЯ БИБЛЕЙСКАЯ ФРАЗЕОЛОГИЯ КАК ЧАСТЬ ПАССИВНОГО  \nФРАЗЕОЛОГИЧЕСКОГО СОСТАВА РУССКОГО ЯЗЫКА ……………………………………………………….. 92 \n \n16. Хaлимoвa Фиpузa Pуcтaмoвнa  \nКOНЦЕПТ МAЙДOН ТИЗИМИ CИФAТИДA ………………………………………………………………………. 97 \n \n17. Воҳидов Абдува ҳоб \nТОЖИК ТИЛИНИНГ САМАР ҚАНД ШЕВАСИДА  ЙАК(БИР) СОНИ БИЛАН ҲОСИЛ ҚИЛИНГАН \nФРАЗЕОЛОГИК БИРЛИКЛАР Т ӮҒРИСИДА .................................................................................................... ........102 \n \n18. Азимова Сайёра Хусанбоевна  \nИЗУЧЕНИЕ ИНОСТРАННЫХ ЯЗЫКОВ С ИСПОЛЬЗОВАНИЕМ ЛИНГВОПРАГМАТИЧЕСКОГО  \nПОДХОДА НА ОСНОВЕ КОММУНИКАТИВНОЙ ТЕОРИИ ЯЗЫКА ……………………… …………………..108 \n _f`WSc[YS\u0003 ¬\u0003daWXcYS`[X\u0003 ¬\u0003ɹ\u000e\r\u0013\u0004\r\u0013\u0003\n dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n4 \n 19. Tair Akimov  \nXITOY TILIDAGI IBORA, MAQOL VA MATALLARNING QIYОSI TAHLILI ...................................................... 114  \n \n20. Maftuna Ibodullayeva Habibullayevna  \nTEACHING INTERACTIVE WAYS OF CONDUCTING RESEARCH TO THE STUDENTS  \nOF HIGHER EDUCATION ....................................................................... ...................................................................... 122 \n \n21. Fayzullayeva Dilnoza Ubaydullayevna  \nINCREASING THE ACTIVITY OF STUDENTS IN EDUCATION THROUGH  \nTHE USE OF DIGITAL TECHNOLOGIES .................................................................................................... ............... 129 \n \n22. Nurislom Iskandarovich Xursanov  \nDRAMATIK DISKURSNING SOTSIOLINGVISTIK XUSUSIYATLARI ................................................. .................13 3\n           \n   dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n18 \n Mukhayyo Fayzullaeva  \nTeacher, Department of Linguistics and English literature  \nUzbekistan State World Languages University  \ne-mail: muhayyo.fayzullayeva.93@mail.ru  \n \nSYMBOLIC REPRESENTATIONS OF RELIGIOUSLY MARKED ALLUSIONS  \nIN THE LITERARY TEXT  \n \n \nhttps://doi.org/ 10.5281/zenodo.7679672  \n \n \nANNOTATION  \nThe article imparts the information on religiously marked allusions, their symbolic \nrepresentations in English literary texts and analysis from the perspectives of cognitive linguistic s. \nAmong its other aspects, the rese arch places particular emphasis on symbolic functions outperformed \nby allusions and in order to reveal the symbolic significance generated in the semantic layer of \nreligiously marked allusions, the methods of conceptual analysis and conceptual blending wer e \napplied. The results of the analysis show that religiously marked allusions activate religious \nknowledge structures, implicitly symbolize abstract phenomena, convey conceptual meaning and \nexpress the author's individual world picture.  \nKey words: allusion , religiously marked allusion, symbol, conceptual integration, implicit \nmessage, precedent text, recipient text  \n   \nMuhayyo Fayzullayeva  \nLingvistika va ingliz adabiyoti kafedrasi o’qituvchisi  \nO’zbekiston davlat jahon tillari universiteti  \ne-mail: muhayyo.fayzullayeva.93@mail.ru  \n \nDINIY MARKERLANGAN ALLYUZIYALARNING BADIIY MATNDA  \nSIMVOLIK TASVIRLARI  \nANNOTATSIYA  \nMaqolada diniy markerlangan allyuziyalar, ularning ing liz badiiy matnlaridagi ramziy \nko'rinishlari haqida ma'lumot hamda ushbu til birliklarining kognitiv tilshunoslik nuqtai nazaridan \ntahlili berilgan. Tadqiqotda allyuziyalarning aynan simvolik funksiyasiga katta e ʼtibor qaratilgan \nboʻlib, diniy markerlangan  allyuziyalarning semantik qatlamida hosil bo ʻlgan ramziy maʼnoni ochib \nberish uchun konseptual tahlil va konseptual blending tahlil usullari qo ʻllanilgan. Tadqiqot natijalari \nshuni ko'rsatadiki, diniy markerlangan allyuziyalar diniy bilim tuzilmalarini fa ollashtiradi, mavhum \nhodisalarni simvol sifatida aks ettiradi, konseptual ma'noni bildiradi va muallifning individual dun yo \nqarashini ifodalaydi.  \nKalit so'zlar:  allyuziya, diniy markerlangan allyuziya, simvol, konseptual integratsiya, yashirin \nma'lumot, p retsedent matn, retsipient matn  \n \n \ndǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n19 \n Мухайё Файзуллаева  \nпреподаватель кафедры лингвистики и английской литературы  \nУзбекский государственный университет мировых языков,  \nmuhayyo.fayzullayeva.93@mail.ru  \n \nСИМВОЛИЧЕСКИЕ П РЕДСТАВЛЕНИЯ РЕЛИГИОЗНО МАРКИРОВАННЫХ \nАЛЛЮЗИЙ  В ХУДОЖЕСТВЕННОМ ТЕКСТЕ  \nАННОТАЦИЯ  \nВ статье представлена информация о религиозно маркированных аллюзиях, их \nсимволической репрезентации в англоязычных художественных текстах и анализе с позиций \nкогнитивной лингвистики. Среди других  аспектов в исследовании особое внимание уделяется \nсимволическим функциям аллюзий и для выявления символической значимости, \nгенерируемой в семантическом слое религиозно маркированных аллюзий, были применены \nметоды концептуального анализа и концептуального интеграция. Результаты анализа \nпоказывают, что религиозно маркированные аллюзии активизируют структуры религиозного \nзнания, имплицитно символизируют абстрактные явления, передают концептуальный смысл \nи выражают индивидуально -авторскую картину мира.  \nКлючевы е слова:  аллюзия, религиозно маркированная аллюзия, символ, концептуальная \nинтеграция, имплицитное сообщение, прецедентный текст, реципиентный текст.  \n   \nIntroduction.  Allusion is delineated as a multifaceted linguistic phenomenon and thus is \ninterpreted quite broadly, but largely known as a stylistic tool that bears a deliberate action of a n \nauthor to implicitly refer to a preceding phenomenon (person, object, event) ( Ashurova D.Yu., 2005; \n8). However, with the development of the sciences of anthropocentric paradigm, many stylistic \nphenomena were reconsidered. Being among those stylistic devices, allusion is now on the spotlight \nof such sciences as cognitive, pragmatic and cultural linguistics, thus is being scrutinized from \ndifferent perspectives. The study over allusions can be found in the works of Galperin (1981), Fatee va \n(2000), Kusmina (2004), Gornakova (2010), Vasileva (2011), Ashurova (2012, 2016), Djusupov \n(2006 , 2011), Arnold (2014), Galieva (2018) and in many more scholarly works of other linguists.  \nAllusion is defined as a common and frequently used intertextual marker, since it establishes a bond  \nbetween the precedent and recipient literary texts (Fateeva, 1 998, 2000; Kusmina, 2004; Piego -Gro, \n2008; Arnold, 2014). It is of significance to note that allusion partakes in the formation of author ’s \neither trite or genuine image/style of writing, and immensely assists in the revelation of the autho r’s \nmodality inh erit in the literary text. In other words, allusion explicates the personality of the author \nand helps to decode his/her individual conceptual world picture (Tukhareli, 1984; Dronova, 2006). \nImportant of all, allusion embraces conceptual information, corre ct interpretation of which explicates \ndeep semantic layer of the text in its relation to the preceding source [Kristeva, 1980; Fateeva, 20 00; \nMolchanova, 2007; Piege -Gro, 2008; Dusabaeva, 2009; Ashurova, Galieva, 2016, 2018] (cited in \nGalieva M.R., 2018; 1 58). Allusion is characterized by polyfunctionality, for it fulfills stylistic, \npragmatic, cognitive and socio -cultural functions. Stylistic functions of allusions are aimed to express \nimagery, emotiveness, evaluation and expressiveness. Pragmatic function s are targeted at producing \na certain emotional or intellectual impact on the reader. Cognitive functions deal with the \nrepresentation of knowledge structures and conceptual picture of world. Socio -cultural functions \nentail the expression of cultural and a esthetic values and national specifics of linguistic units and \nliterary texts as a whole.  The aim of our research is to dwell upon one of the essential functions - \nsymbolic representations of allusions, in particular its religious type, in the literary te xt. Religiously \nmarked allusion itself (henceforth RA), the object of our investigation, is denoted as an indirect \nreference to a religious source and as an extended transmitter of the knowledge about mythological, \nreligious heroes, objects and events to t hose actions, facts and personages addressed to in the text \nrecipient.  \nMain part.  As has been remarked above, of huge significance is to discuss symbolism created \nby RA in the literary text. It is worthy of note that the two stylistic devices, symbol and allusion , have dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n20 \n much in common, as the structure and set of functions outperformed by symbols interweave with \nthose of allusions: a) both represent knowledge structures; b) both function in the literary text \nhighlighting the most salient features of an object or phenomena; c) both symbol and allusion detect ed \nin the literary text should be conceptualized and interpreted; d) both can be of universally known and \nnationally specific. The commonality between allusion and symbol is also manifested by the fact that  \nin the literary text allusion can acquire a symbolic meaning: it can represent a  symbol, it can be \nregarded as a symbol. In other words, allusions symbolize abstract phenomena in the literary text, \nand as a result, symbolic perception of the sequences of events leads to the decipherment of the hid den \nconceptual information conveyed vi a allusions.  \nAs a method of analyzing symbolic significance of RA and interpreting conceptual \ninformation embodied in the semantic construct of RA, cognitive mechanisms of conceptual analysis \nand conceptual blending (integration) can effectively be implem ented. The method of conceptual \nanalysis is intended to study functional and conceptual significance of allusions, to explicate the \nsymbolic meaning of this stylistic device and also to decode the conceptual information of the text \nand author's individual world picture. Conceptual integration theory was first pushed forward by G. \nFauconnier and M. Turner in 2002. They asserted that conceptual integration implies a mental activit y \nin which \"new meanings\" are made \"out of old” (Fauconnier, Turner, 2002; 18). To attain this, a set \nof deliberate actions are to be performed: \".. setting up mental spaces, matching across spaces, \nprojecting selectively to a blend, locating shared structures, projecting backwards to inputs, recru iting \nnew structure to the inputs or the blend, and running various operations in the blend” (Fauconnier, \nTurner, 2002; 18). In other words, conceptual blending entails generating mental spaces and \ninteraction between input domains and creating new conceptual meaning. So those indispensable \nmental spaces partaking in the process of integration are conceptual domains, generic space and the \nblend itself. In the process of conceptual integration, generic space incorporates and conjoins the two \ndomains by deducing the commonalities between them, t his, in its turn, results in the emergence of \nthe blend, a new conceptual structure. More specifically, the source domain provides background \nknowledge, which is synthesized and transferred to describe the target domain. It is of expedient \nimportance to re mark that the blended space originated from the joint domains bears the most salient \nelements of both inputs, howbeit during the running of the integration process it obtains novel and \nunique assets too.  \nAll in all, allusions representing and symbolizing a particular object, personage or event anew \nin the text recipient are regarded as the linguistic units that activate the mechanism of conceptual  \nintegration: they accumulate the knowledge structures belonging to the precedent text and represent \nthe old in formation with either totally new or additional shades in the recipient text.  \nAccording to Galperin, literary texts bear factual, subtextual and conceptual information \n(Galperin, 2007). So, first of all, the successful implementation of conceptual blendin g requires the \nexposition of factual, subtextual and conceptual information embedded in the literary text. To attai n \nthe research aim, the following steps should be taken in gradual succession: 1. Factual information \nwhich exists on the surface layer of th e text should be divulged. To attain this, the plot of the text, the \nmain sequences of events should be apprehended; 2. Subtextual information which is implicitly \ndelivered via different language means (particularly such stylistic devices as cognitive meta phor and \nallusion) should be detected and the essence formed by them is to be explicated; 3. Conceptual \ninformation created via symbolism should be revealed. In this respect, the method of conceptual \nintegration can be addressed to and the symbolic meaning  of allusion can be revealed.   \nTo justify our claim we intend to present the analysis of the religious allusion represented in the short \nstory \"Gift of the Magi\" by O’ Henry.  \nInterpretation of the factual information  \n\"Gift of the Magi\" by O' Henry is a f amous short story most profoundly known as the hymn \nof unconditional love. Mr. and Mrs. Dillinghams live not in favorable conditions with lower family \nbudget. How pure and unconditional their love is, can be witnessed in the end of the story when they  \nat last hand in their Christmas gifts to each other: Della's present is just the thing that she longed f or \nso long - \"Beautiful combs, with jewels, perfect for her beautiful hair \" , but alas they turn out to be dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n21 \n useless now as her hair is cut off and sold to bu y \"Something nearly good enough. Something almost \nworth the honor of belonging to Jim\".  Meanwhile Jim is also deprived from the delight of possessing \na gold chain presented to him to use with his gold watch since it is sold to make his beloved happy.  \nSo th e most valuable and precious things belonging to the Dellingham Youngs, with which they used \nto feel extreme pride, Jim's heirloom, a gold watch and Della's hair, like a brown cascade are \nsacrificed for the sake of love. Therefore, O' Henry confesses and c laims that Jim and Della are the \ntrue magi.  \nInterpretation of the subtextual information  \nThe main concept around which the sequences of events are constructed is “gift”, as it stands \nout as the recurrent element in the story forming the main ideology pushe d forward by the author: the \nart of giving a gift. So the “gift” in the story can be regarded as a symbol of a “spiritual” offer rather \nthan a “material” object. Further denotative meanings of the concept will be outlined.  \n1. Dictionary meaning of the term  \"gift\":  \nIn the dictionary of Britannica, gift is defined as \"something that is given to another person or to a \ngroup or organization\" . According to the conception made in the dictionary of Merriam Webster \ngift can largely be apprehended as \"something voluntarily transferred by one person to another \nwithout compensation\".  Roget's Thesaurus singles out the following synonymous line of the  word \n\"gift\": allowance, award, benefit, bonus, contribution, donation, endowment, favor, grant, legacy, \noffering, present, benefaction, bequest, bestowal, charity, courtesy .  \nAll in all, denotative meaning of the word \"gift\" signifies a present voluntaril y offered to someone \nas a sign of affection, recognition and appreciation. To reveal additional connotative meaning \nattached to the primary meaning of the word is of paramount importance in the complete explication \nof the concept. As a matter of fact, the lexeme “gift” is a religiously marked allusion as it indirectly \nrefers to the biblical event when Christ, being a newborn, is visited and offered presents by the th ree \nwise men (Matthew, 2:1 –12). Besides that, the lexeme “gift” can be encountered in severa l other \nbiblical verses too.  \n2. Biblical origin of the term \"gift\":  \nThe analysis of the biblical verses instigates that gift is perceived as something unbelievably valu able \npresented by God:  \n\"For the wages of sin is death, but the free gift of God is eter nal life in Christ Jesus our Lord \" \n(Romans 6:23  ).  \n\"If you then, who are evil, know how to give good gifts to your children, how much  more will your \nFather who is in heaven give good things to those who ask him!\" ( Matthew 7:11 ) \n\"If any of you lacks wisdom, let him ask God, who gives generously to all without reproach, and it \nwill be given him\" ( James 1:5 ). \n\"Also that everyone should eat and drink and  take pleasure in all his toil —this is God's gift to man\" \n(Ecclesiastes 3:13  ). \n\"Thanks be to God for his inexpressible gift!\" ( 2 Corinthians 9:15 )  \n\"Every good and perfect gift is from above, coming down from the Father of the heavenly lights, \nwho does not change like shifting shadows \"(James 1:17)  \n\"You will be enriched in every way so that you can be generous on every occasion, and through us \nyour generosity will result in thanksgiving to God\" (Corinthians 9:12)  \nOverall, biblical verses testify that gi ft is believed to be a heavenly award given to men by God. The \ngift shared by God is unique for a) it has no equation at all ( eternal life ), b) God's gift is so tremendous \nthat no any living creature can bear the same ( how much more will your Father who is  in heaven give \ngood things to those who ask him ), c) God is the most generous to give what is asked ( gives generously \nto all without reproach ). \nInterpretation of the conceptual information  \nThe conceptual and symbolic significance that the RM “Gift of the Magi” obtains in the story is \nexposed via the implementation of conceptual integration mechanism in our research.  dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n22 \n  \n \nAs it can be seen, the RA “Gift” appears to be the main concept of the story. The conceptual analysi s \nof the story and conceptual blending analysis reveal the cognitive structure of this concept includi ng \nsuch constituents as “sacrifice” and “wisdom”. So i n the author’s reflection true love means Wisdom \n(like the wisdom of the Magi)  and Sacrifice (like young men’s sacrifice of their precious things).   \nConclusion. To sum up, the following inferences were deduced from the research:  \n Allusion as an intertextua l marker fulfills various strylistic, pragmatic, socio -cultural and \nsymbolic functions in the literary text;  \n One of the most essential functions of allusions is to represent symbolic meanings i.e. \nsymbolic characterization of particular phenomena;  \n The con ceptual analysis of the whole story in general and allusion in particular reveals the \nsymbolic meaning of the RA and at the same time the author's evaluation of the described \nevent. In other words, the RA in the literary text is aimed a) to represent relig ious knowledge \nstructures, b) to express its symbolic meaning; c) to construct a concept of the whole story; d) \nto explicate the author's modality and his individual world picture.  \n \nReferences  \n   \n1. Fauconnier, Gilles; Mark Turner (2002). The Way We Think: Conceptual Blending and the \nMind’s Hidden Complexities. New York: Basic Books  \n2. Ашурова  Д.Ю. Новые  тенденции  в развитии  стилистики  // til va nutq sistem -sath talqinida: \nМатериал қ науч -теор . конф . - Самарканд : СамГИИЯ , 2005. - C. 7-8 \n3. Галиева М.Р. Теоли нгвистика: истоки, направления, перспективй. — Ташкент: \nВнешИнвестПром, 2018. — 50 c.  \n4. Гальперин И.Р. Текст как объект лингвистического исследования / И.Р. Гальперин. – М. \n: КомКнига, 2007. – 144 с  \n5. https://www.britannica.com/dictionary/gift   \n6. https://www.merriam -webster.com/dictionary/gift   \ndǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n23 \n 7. https://www.thesaurus.com/browse/gift   \n8. https://dailyverses.net/search/gift   \n9. https://www.openbible.info/topics/gifts_from_god   \n10. 10.https://americanenglish.state.gov/files/ae/resource_files/1the_gift_of_the_magi_0.pdf   \n \n  dǩZ\u0003dS`mSe[ \u0003hS^ȉSca\u0003Yfc`S^[ \u0003¬\u0003_XYWf`ScaW`n\\ \u0003Yfc`S^\u0003[d]fddeUa \u0003d^aUS\u0003¬\u0003\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b \u0003\t\u000e\u0014\u0011\rɮ\u000b \u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003№1 | 202 3                   \n \n142 \n Контакт редакций журналов. www.tadqiqot.uz  \nООО Tadqiqot город Ташкент,  \nулица Амира Темура пр.1, дом -2. \nWeb: http://www.tadqiqot.uz/; E -mail: info@tadqiqot.uz  \nТел: (+998 -94) 404 -0000   \nISSN 2181 -9297  \nDoi Journal 10.26739/2181 -9297  \nEditorial staff of the journals of www.tadqiqot.uz  \nTadqiqot LLC The city of Tashkent,  \nAmir Temur Street pr.1, House 2.  \nWeb: http://www.tadqiqot.uz/; E -mail: info@tadqiqot.uz  \nPhone: (+998 -94) 404 -0000  \n dǩZ\u0003dS`mSe[\u0003hS^ ȉSca\u0003Yfc`S^[ \u0003\n \u0003Y[^W\u0003\u001b\u0003da`\u0003\n\u0003\n_XYWf`ScaW`n\\\u0003Yfc`S^\u0003[d]fddeUa\u0003d^aUS \u0003\nea_\u0003 \u0003`a_Xc\u0003\u001b\u0003\n\u0003\n\b\r\u0013\u0004\u0011\rɮ\u0013\b\u000e\rɮ\u000b\u0003\t\u000e\u0014\u0011\rɮ\u000b\u0003\u000e\u0005\u0003\u0016\u000e\u0011ɖ\u0003ɮ\u0011\u0013 \u0003\n\u0015\u000e\u000b\u0014\f\u0004\u0003  \u0003\b\u0012\u0012\u0014\u0004\u0003\u001b\u0003\n\u0003"
    },
    {
        "title": "Music Information Retrieval as Music Understanding.",
        "author": [
            "Roger B. Dannenberg"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1418263",
        "url": "https://doi.org/10.5281/zenodo.1418263",
        "ee": "https://zenodo.org/records/1418263/files/Dannenberg01.pdf",
        "abstract": "Much of the difficulty in Music Information Retrieval can be traced to problems of good music representations, understanding music structure, and adequate models of music perception. In short, the central problem of Music Information Retrieval is Music Understanding, a topic that also forms the basis for much of the work in the fields of Computer Music and Music Perception. It is important for all of these fields to communicate and share results. With this goal in mind, the author’s work on Music Understanding in interactive systems, including computer accompaniment and style recognition, is discussed.",
        "zenodo_id": 1418263,
        "dblp_key": "conf/ismir/Dannenberg01",
        "keywords": [
            "Music Information Retrieval",
            "Music representations",
            "Music structure",
            "Music perception",
            "Music Understanding",
            "Computer Music",
            "Music Perception",
            "Interactive systems",
            "Computer accompaniment",
            "Style recognition"
        ],
        "content": "139 ISMIR 2001 Invited Address Music Information Retrieval as Music Understanding1 \n                                                                    1 Originally published as: Roger B. Dannenberg, “Music Information Retrieval as Music Understanding,” in ISMIR 2001 2nd Annual International Symposium on Music Information Retrieval, Bloomington: Indiana University, (2001), pp. 139-142. Roger B. Dannenberg Carnegie Mellon University School of Computer Science Pittsburgh, PA 15213 USA 1-412-268-3827 rbd@cs.cmu.edu  ABSTRACT Much of the difficulty in Music Information Retrieval can be traced to problems of good music representations, understanding music structure, and adequate models of music perception. In short, the central problem of Music Information Retrieval is Music Understanding, a topic that also forms the basis for much of the work in the fields of Computer Music and Music Perception. It is important for all of these fields to communicate and share results. With this goal in mind, the author’s work on Music Understanding in interactive systems, including computer accompaniment and style recognition, is discussed. 1. INTRODUCTION One of the most interesting aspects of Music Information Retrieval (MIR) research is that it challenges researchers to form a deep understanding of music at many levels. While early efforts in MIR were able to make impressive first steps even with simple models of music, it is becoming clear that further progress depends upon better representations, better understanding of music structure, and better models of music perception. As MIR research progresses, the community will undoubtedly find more and closer ties to other music research communities, including “Computer Music,” probably best represented by the International Computer Music Association and its annual conference [22], and “Music Perception” as represented by the Society for Music Perception and Cognition [25]. While MIR is not the main focus of either of these communities, there is considerable overlap in terms of music processing, understanding, perception, and representation. The goal of this presentation is to survey some work (mostly my own) in Music Understanding and to describe work that is particularly relevant to MIR. Most of my work has focused on interactive music systems. Included in this work is extensive research on computer accompaniment systems, in which melodic search and comparison are essential components. Other efforts include beat-tracking, listening to and accompanying traditional jazz performances, and style classification of free improvisations. Along with the preparation of this presentation, I am placing many of the cited papers on-line so they will be more accessible to the MIR community. My thesis is that a key problem in many fields is the understanding and application of human musical thought and processing; this drives much of the research in all fields related to music, science, and technology. This is not to say that these fields are equivalent, but it is important to understand how and why they are related. The work that I describe here shares many underlying problems with MIR. I hope this overview and the citations will be of some benefit to the MIR community. 2. COMPUTER ACCOMPANIMENT The general task of computer accompaniment is to synchronize a machine performance of music to that of a human. I introduced the term computer accompaniment in 1984, but others terms have been used including synthetic performer [26], artificially intelligent performer [2] and intelligent accompanist [5]. In computer accompaniment, it is assumed that the human performer follows a composed score of notes and that both human and computer follow a fully notated score. In any performance, there will be mistakes and tempo variation, so the computer must listen to and follow the live performance, matching it to the score. Computer accompaniment involves the coordination of signal processing, score matching and following, and accompaniment generation. Because of the obvious similarity of score matching to music search, I will focus on just this aspect of computer accompaniment. See the references for more detail [9, 12]. 2.1 Monophonic Score Following My first computer accompaniment systems worked with acoustic input from monophonic instruments. The system is note-based: the sequence of performed pitches is compared to the sequence of pitches in the score. Times and durations are ignored for the purposes of matching and comparison, although timestamps must be retained for tempo estimation and synchronization. Originally, I tried to apply the algorithm from the Unix diff command, which, viewed from the outside, seems to be perfect for comparing note sequences. Unfortunately, diff does not work here because it assumes that lines of text are mostly unique. This led to the exploration and application of dynamic programming, inspired by longest common substring (LCS) and dynamic Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  140 timewarp algorithms [24]. To my knowledge, this is the first use dynamic programming for melodic comparison. Recall that LCS computes a matrix of size m·n for strings of length m and n. An important refinement for real-time music recognition is the introduction of a sliding window centered around the current score position. This reduces the computation cost per note to a constant. This windowing idea could be used in music search applications, especially to compare long query strings to stored strings. The window only affects the result in cases where the match is poor, but presumably these cases are not of interest anyway. Dynamic programming algorithms typically look at the final m·n matrix to determine the result, but this is not possible in real time. As a heuristic, my score follower reports a match when a “match score” is computed that is higher than any previous value computed so far. The “match score” is essentially the number of notes matched so far minus the number of notes skipped in the score. This formulation compensates for the tendency to skip over notes in order to find a match. It is perhaps worth noting that matching the performance (a prefix of the score) to the score is a bit like matching a query, which may be a melodic fragment, to a complete melody. Since a fragment may start and end anywhere in a complete melody, we want to compute the least distance from any contiguous fragment of the melody, ignoring a certain (but unknown) prefix and suffix of the melody. Dynamic programming, as formulated for score following, can find this best match with a cost of m·n, where m and n are the lengths of the melody and query. (Unfortunately, the windowing idea does not seem to apply here because we do not know where the best match will start in the melody.) Monophonic score following works very well. The original implementation ran on a 1MHz 8-bit processor that performed pitch estimation, score following, accompaniment, and synthesizer control in real time, and fit under an airline seat in 1984! As a historical note, I suggested in a talk given in 1985 [7] that these matching algorithms could be used to quickly search a database of songs. Unfortunately, I missed the opportunity to mention this in my patent [8] or create the first such implementation. 2.2 Polyphonic Score Following The logical next step in this research was to consider accompanying keyboard performances, and two algorithms were developed for polyphonic score following [3]. Rather than repeat their full descriptions here, I will simply try to give the main ideas and properties of the algorithms. One approach, developed by Josh Bloch, generalizes the idea of “note” to “compound event.” A compound event is set of simultaneous note onsets, i.e. a chord. The score and performance are regarded as sequences of compound events, and we are essentially looking for the best match. The quality of the match is determined by the number of pitches that match within corresponding compound events minus the number of pitches that are skipped. This is easily solved using dynamic programming, where rows and columns correspond to compound events. One problem with the preceding algorithm is that it relies upon some process to group events into compound events. We form compound events by grouping notes whose onsets are separated by less than 50 to 100ms. Another algorithm for polyphonic score following was created that forms compound events dynamically. In this algorithm, score events are initially grouped into compound events, but performed events are processed one-at-a-time. What amounts to a greedy algorithm is used to associate performed notes with compound events. Unfortunately, this algorithm does not always find the optimal match because of the heuristic nature of its grouping. In practice, both algorithms work very well, failing only in (different) contrived pathological cases. Both use the same windowing technique introduced in the monophonic matcher and therefore run in constant time per performed note. The precision of a MIDI keyboard compared to acoustic input, combined with the additional information content of a polyphonic score, makes computer accompaniment of keyboard performances very robust. These algorithms could be used for music search, but they rely on matching notes as opposed to something more abstract such as harmony. If an improviser plays correct harmonies but in different rhythms or voicings, the match rating might be low. On the other hand, the algorithm can be used as a sort of diff on MIDI files, for example to compare different performances [21, 23] or editions. Another interesting application of this technology is in intelligent piano tutoring systems [4, 6, 13]. 2.3 Ensemble Accompaniment With keyboard performance, the right and left hands are generally synchronized, but this is not so true of ensembles.  Following and accompanying an ensemble can be accomplished by following each musician separately and then integrating the results [10, 15, 16]. One of the interesting problems encountered here is that different performers may have more or less relevance at any given time. Usually, performers that have performed a note more recently and that are synchronized with other performers are better sources of timing information. The situation changes constantly in a performance as one part assumes prominence and another plays a background role or rests. In MIR research, it is common to assume music is a totally ordered sequence of notes or features. It might be useful to consider that, in performance, individuals are not always synchronized. Instead, each performer has a separate notion of time and has a strong goal to produce coherent musical gestures. The synchronization of all these independent lines and gestures is a quasi-independent task performed as each performer listens to the others. 2.4 Vocal Accompaniment In spite of the success of monophonic and polyphonic matchers for score following, these techniques do not work well for vocal soloists. The main problem is that vocal melodies are difficult to segment into discrete notes, so the data seen by the matcher has a high error rate. Similar problems occur in MIR systems, and a more detailed analysis can be found in Lorin Grubb’s thesis [19]. Given that discrete string matching methods cannot be applied to vocal music, Grubb’s solution [18, 20] is based on the idea of using probability theory to form a consistent view based on a large number of observations that, taken individually, are unreliable. The probabilistic framework allows the system to be trained on actual performance data; thus, typical performance errors and signal processing errors are all integrated into the framework and accounted for.  141 The system effectively matches pitch as a function of time to the score, but rather than use dynamic time warping, Grubb’s system represents score position as a probability density function. This density function is updated using a model of tempo variation, accounting for natural variations in performed tempo, and a model of pitch observations, accounting for the natural distribution of pitch around the one notated in the score. In addition, phonetic information and note onset information can be integrated within the probabilistic framework [17]. This work forms an interesting basis for MIR using vocal queries. 3. Listening to Jazz  It would be wrong to assume every MIR query can be formulated as a melodic fragment. Similarly, it is restrictive to assume accompanists can only follow fully notated music. What about jazz, where soloists may follow chord progressions, but have no predetermined melody? Working with Bernard Mont-Reynaud, I developed a real-time blues accompaniment system that analyzed a 12-bar blues solo using supervised learning to characterize typical pitch distributions and a simple correlation strategy to identify location [11]. This work also included some early beat induction techniques [1]. It seems unlikely that these techniques will be directly applicable to MIR systems, but the general idea that improvised solos (or even stylized interpretations of melodies) can be understood in terms of harmonic and rhythmic structure is important for future MIR research. 4. Style Classification An underlying structure of beats, measures, harmony and choruses supports traditional jazz solos. I am interested in interactive improvisations with computers where this structure is absent. Instead, I want the computer to recognize different improvisational styles, such as “lyrical,” “syncopated,” and “frantic” so that the improviser can communicate expressive intentions to the computer directly through the music, much as human musicians communicate in collective improvisations. This goal led to work in style classification using supervised machine learning [14]. This work has obvious applications to music search where the object is to retrieve music of a certain genre or style. We were able to obtain good classification rates on personal styles using quite generic features obtained from a real-time pitch analyzer. Recognition was based on only 5 seconds of music to minimize latency in a real-time performance. 5. Conclusions Music Understanding is a critical part of Music Information Retrieval research as well as a central topic of Computer Music and Music Perception. The similarities between score following and style classification to problems in MIR are striking. I hope that this paper will introduce some pioneering work in Music Understanding to a broader audience including especially MIR researchers.  REFERENCES [1] Allen, P. and Dannenberg, R.B., Tracking Musical Beats in Real Time. in 1990 International Computer Music Conference, (1990), International Computer Music Association, 140-143. http://www.cs.cmu.edu/~rbd/bib-beattrack.html#icmc90 (note that an extended version of the paper is available online). [2] Baird, B., Blevins, D. and Zahler, N. Artificial Intelligence and Music: Implementing an Interactive Computer Performer. Computer Music Journal, 17 (2). 73-79. [3] Bloch, J. and Dannenberg, R.B., Real-Time Accompaniment of Polyphonic Keyboard Performance. in Proceedings of the 1985 International Computer Music Conference, (1985), International Computer Music Association, 279-290. http://www.cs.cmu.edu/~rbd/bib-accomp.html#icmc85. [4] Capell, P. and Dannenberg, R.B. Instructional Design and Intelligent Tutoring. Journal of Artificial Intelligence in Education, 4 (1). 95-121. [5] Coda. SmartMusic Studio 6.0, Coda Music Technology, Inc., Eden Prarie, MN, 2000. http://www.codamusic.com/coda/ sm.asp. [6] Dannenberg, R., Sanchez, M., Joseph, A., Saul, R., Joseph, R. and Capell, P., An Expert System for Teaching Piano to Novices. in 1990 International Computer Music Conference, (1990), International Computer Music Association, 20-23. http://www.cs.cmu.edu/~rbd/bib-ptutor.html#icmc90. [7] Dannenberg, R.B. Computer Accompaniment (oral presentation), STEIM Symposium on Interactive Music, Amsterdam, 1985.  [8] Dannenberg, R.B. Method and Apparatus for Providing Coordinated Accompaniment for a Performance, US Patent #4745836, 1988.  [9] Dannenberg, R.B. and Bookstein, K., Practical Aspects of a Midi Conducting Program. in Proceedings of the 1991 International Computer Music Conference, (1991), International Computer Music Association, 537-540. http://www.cs.cmu.edu/~rbd/subjbib2.html#icmc91. [10] Dannenberg, R.B. and Grubb, L.V. Automated Musical Accompaniment With Multiple Input Sensors, US Patent #5521324, 1994.  [11] Dannenberg, R.B. and Mont-Reynaud, B., Following an Improvisation in Real Time. in Proceedings of the International Computer Music Conference, (1987), International Computer Music Association, 241-248. http://www.cs.cmu.edu/~rbd/bib-accomp.html#icmc87. [12] Dannenberg, R.B. and Mukaino, H., New Techniques for Enhanced Quality of Computer Accompaniment. in Proceedings of the International Computer Music Conference, (1988), International Computer Music Association, 243-249. http://www.cs.cmu.edu/~rbd/bib-accomp.html#icmc88. [13] Dannenberg, R.B., Sanchez, M., Joseph, A., Capell, P., Joseph, R. and Saul, R. A Computer-Based Multimedia Tutor for Beginning Piano Students. Interface - Journal of New Music Research, 19 (2-3). 155-173. [14] Dannenberg, R.B., Thom, B. and Watson, D., A Machine Learning Approach to Style Recognition. in 1997 International Computer Music Conference, (1997), International Computer Music Association. http://www.cs.cmu.edu/~rbd/bib-styleclass.html#icmc97. [15] Grubb, L. and Dannenberg, R.B., Automated Accompaniment of Musical Ensembles. in Proceedings of the Twelfth National Conference on Artificial Intelligence, (1994), AAAI, 94-99.  142 [16] Grubb, L. and Dannenberg, R.B., Automating Ensemble Performance. in Proceedings of the 1994 International Computer Music Conference, (1994), International Computer Music Association, 63-69. http://www.cs.cmu.edu/~rbd/bib-accomp.html#icmc94. [17] Grubb, L. and Dannenberg, R.B., Enhanced Vocal Performance Tracking Using Multiple Information Sources. in Proceedings of hte International Computer Music Conference, (1998), International Computer Music Association, 37-44. http://www.cs.cmu.edu/~rbd/bib-accomp.html#icmc88. [18] Grubb, L. and Dannenberg, R.B., A Stochastic Method of Tracking a Vocal Performer. in 1997 International Computer Music Conference, (1997), International Computer Music Association. http://www.cs.cmu.edu/~rbd/bib-accomp.html# icmc97. [19] Grubb, L.V. A Probabilistic Method for Tracking a Vocalist. Carnegie Mellon University, Pittsburgh, PA, 1998. http://reports-archive.adm.cs.cmu.edu/anon/1998/abstracts/ 98-166.html. [20] Grubb, L.V. and Dannenberg, R.B. System and Method for Stochastic Score Following, US Patent #5913259, 1997.  [21] Hoshishiba, T., Horiguchi, S. and Fujinaga, I., Study of Expression and Individuality in Music Performance Using Normative Data Derived from MIDI Recordings of Piano Music. in International Conference on Music Perception and Cognition, (1996), 465-470. http://www.jaist.ac.jp/~hoshisi/ public/papers/icmpc96.pdf. [22] ICMA. http://www.computermusic.org/, International Computer Music Association, 2001.  [23] Large, E.W. Dynamic programming for the analysis of serial behaviors. Behavior Research Methods, Instruments, and Computers, 25 (2). 238-241. [24] Sankoff, D. and Kruskal, J.B. Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. Addison-Wesley, Reading, MA, 1983. [25] SMPC. http://psyc.queensu.ca/~smpc, Society for Music Perception and Cognition, 2001. http://psyc.queensu.ca/ ~smpc. [26] Vercoe, B., The Synthetic Performer in the Context of Live Performance. in Proceedings of the International Computer Music Conference 1984, (1984), International Computer Music Association, 199-200."
    },
    {
        "title": "An Approach Towards A Polyphonic Music Retrieval System.",
        "author": [
            "Shyamala Doraisamy"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415622",
        "url": "https://doi.org/10.5281/zenodo.1415622",
        "ee": "https://zenodo.org/records/1415622/files/Doraisamy01.pdf",
        "abstract": "Most research on music retrieval systems is based on monophonic musical sequences.  In this paper, we investigate techniques for a full polyphonic music retrieval system.  A method for indexing polyphonic music data files using the pitch and rhythm dimensions of music information is introduced.  Our strategy is to use all combinations of monophonic musical sequences from polyphonic music data.  ‘Musical words’ are then obtained using the n-gram approach enabling text retrieval methods to be used for polyphonic music retrieval. Here we extend the n-gram technique to encode rhythmic as well as interval information, using the ratios of onset time differences between two adjacent pairs of pitch events.  In studying the precision in which intervals are to be represented, a mapping function is formulated in dividing intervals into smaller classes.  To overcome the quantisation problems that arise with using rhythmic information from performance data, an encoding mechanism using ratio bins is also adopted.  We present results from retrieval experiments with a database of 3096 polyphonic pieces.",
        "zenodo_id": 1415622,
        "dblp_key": "conf/ismir/Doraisamy01",
        "keywords": [
            "polyphonic music retrieval system",
            "pitch and rhythm dimensions",
            "indexing polyphonic music data",
            "n-gram approach",
            "text retrieval methods",
            "musical words",
            "interval information",
            "mapping function",
            "quantisation problems",
            "encoding mechanism"
        ],
        "content": "An Approach Towards A Polyphonic Music Retrieval \nSystem \nShyamala Doraisamy \nDept. of Computing \nImperial College, \nLondon SW7 2BZ \n+44-(0)20-75948230 \nsd3@doc.ic.ac.uk \n  Stefan M Rüger \nDept. of Computing \nImperial College \nLondon SW7 2BZ \n+44-(0)20-75948355 \nsrueger@doc.ic.ac.uk \n \n \nABSTRACT  \nMost research on music retrieval systems is based on monophonic \nmusical sequences.  In this paper, we investigate techniques for a \nfull polyphonic music retrieval system.  A method for indexing \npolyphonic music data files using the pitch and rhythm dimensions \nof music information is introduced.  Our strategy is to use all \ncombinations of monophonic musical sequences from polyphonic \nmusic data.  ‘Musical words’ are then obtained using the n-gram \napproach enabling text retrieval methods to be used for polyphonic \nmusic retrieval. Here we extend the n-gram technique to encode \nrhythmic as well as interval information, using the ratios of onset \ntime differences between two adjacent pairs of pitch events.  In \nstudying the precision in which intervals are to be represented, a \nmapping function is formulated in dividing intervals into smaller \nclasses.  To overcome the quantisation problems that arise with \nusing rhythmic information from performance data, an encoding \nmechanism using ratio bins is also adopted.  We present results \nfrom retrieval experiments with a database of 3096 polyphonic \npieces.   \n \n1. INTRODUCTION \nMusic documents encoded in digital formats have rapidly been \nincreasing in number with the advances in computer and network \ntechnologies.  Managing large collections of these documents can \nbe difficult and this has consequently motivated research towards \ncomputer-based music information retrieval (IR) systems.  Music \ndocuments encompass documents that contain any music-related \ninformation such as music recordings, musical scores, manuscripts \nor sketches and so on [1].  Many studies have been carried out in \nusing the music-related information contained in these documents \nfor the development of content-based music IR systems.  Such \nsystems retrieve music documents based on information such as \nthe incipits, themes and instrument families.  However, most of \nthese content-based IR systems are still research prototypes.  \nMusic IR systems that are currently in wide-spread use are systems \nthat have been developed using meta-data such as file-names, titles \nand catalogue references.   \n \n One common approach in developing content-based music IR \nsystems is with the use of pitch information.  Examples of such \nsystems are Themefinder [2] and Meldex [3].  However, these \nsystems were developed using monophonic musical sequences, \nwhere a single musical note is sounded at one time, as opposed to \npolyphonic music where more than one note is sounded \nsimultaneously at any one point in time.  With vast collections of \npolyphonic music data available, research on polyphonic music IR \nis on the rise [4].   \n \nOur aim is the development of a polyphonic music IR system for \nretrieving a title and performance of a musical composition given \nan excerpt from a musical performance as a query.  For content-\nbased indexing, we use the pitch and rhythm dimensions of music \ninformation and propose an approach for indexing full polyphonic \nmusic data. In this paper we present our approach and evaluate it \nusing a database of polyphonic pieces. \n \nThe paper is structured as follows: Section 2 highlights some of \nthe issues and challenges in content-based indexing. Section 3 \npresents the approach taken in using the pitch and duration \ninformation for indexing. The steps in constructing n-grams from \npolyphonic music data and the mechanism of extending the \nrepresentation to include rhythm information are outlined.  The \nempirical analysis performed and approach for encoding patterns \nderived from n-gramming is presented.  Section 4 reports the \nretrieval experiments using ranked retrieval and evaluation results \nusing the mean reciprocal rank measure of our polyphonic music \nIR system. \n \n2.  ISSUES IN CONTENT-BASED \nINDEXING AND RETRIEVAL OF \nMUSICAL DATA \nThe problem of varying user requirements is common to most IR \nsystems.  Music IR systems are no exception.  Music librarians, \nmusicologists, audio engineers, choreographers and disc-jockeys \nare among the wide variety of music IR users with a wide range of \nrequirements [1]. For example, with a musical query where the \nuser plays a recording or hums a tune, one user could possibly \nrequire all musical documents with the same key to be retrieved \nwhile another user's requirement might be to obtain all documents  \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n of the same tempo.  Looking at another example where a musical \ncomposition’s title is queried, one user could require the \ncomposer’s full-name and another user might need to know the \nnumber of times the violin had a solo part in the composition.   \nKnowledge of user requirements is an important aspect in \ndeveloping useful indexes, and with music IR systems this \nchallenge is compounded with others such as the multiple \ndimensions of music data and digital music data formats. \n \nMusic data are multi-dimensional; musical sounds are commonly \ndescribed by their pitch, duration, dynamics and timbre.  Most \nmusic IR systems use one or two dimensions and these vary based \non types of users and queries.  Selecting the appropriate dimension \nfor indexing is an important aspect in developing a useful music IR \nsystem.  Indexing a system based on its genre class would be useful \nfor a system that retrieves music based on mood but not for a \nsystem where a user needs to identify the title of a music piece \nqueried by its theme.    \n \nThe multiple formats in which music data can be digitally encoded \npresent a further challenge.  These formats are generally \ncategorised into a) highly structured formats such as Humdrum [5] \nwhere every piece of musical information on a piece of musical \nscore is encoded, b) semi-structured formats such as MIDI in \nwhich sound event information is encoded and c) highly un-\nstructured raw audio which encodes only the sound energy level \nover time.   Most current music IR systems adopt a particular \nformat and therefore queries and indexing techniques are based \nupon the dimensions of music information that can be extracted or \ninferred from that particular encoding method.  \n \nThere are many approaches for the development of music IR \nsystems.  Some of these include the use of approximate matching \ntechniques in dealing with challenges such as recognising melodic \nsimilarity [6], the use of standard principles of text information \nretrieval and exact matching techniques that demand less retrieval \nprocessing time [7,8].  \n \n3.  A TECHNIQUE FOR INDEXING \nPOLYPHONIC MUSICAL DATA \n3.1 Pattern extraction \nThe approach we take for indexing is full-music indexing, similar \nto full-text indexing in text IR systems.  This approach was studied \nby Downie [8], where a database of folksongs was converted to an \ninterval-only representation of monophonic ’melodic strings’.  \nUsing a gliding window, these strings were fragmented into length-\nn subsections or windows called n-grams for music indexing.   \n \nWith polyphonic music data, a different approach to obtaining n-\ngrams would be required since more than one note can be sounded \nat one point in time (known as the onset time in this context).  In \nsorting polyphonic music data with ascending onset times and \ndividing it into windows of n different adjacent onset times, one or \nmore possible monophonic ’melodic string(s)’ can be obtained \nwithin a window.  The term melodic string used in this context \nmay not be a melodic line in the musical sense.  It is simply a monophonic sequence extracted from a sequence of polyphonic \nmusic data.   \n \nVarious approaches in deriving patterns from unstructured \npolyphonic music for computer-based music analysis have been \ninvestigated in a study by Crawford et. al. [9].  The approach taken \nfor our study would be a musically unstructured but an exhaustive \nmechanism in obtaining all possible combinations of monophonic \nsequences from a window for the n-gram construction.  Each n-\ngram on its own is unlikely to be a musical pattern or motif but a \npattern amenable for digital string-matching.  The n-grams \nencoded as musical words using text representations would be \nused in indexing, searching and retrieving a set of sequences from \na polyphonic music data collection. \n \nThe summary of steps taken in obtaining these monophonic \nmusical sequences is as follows: \n \nGiven a polyphonic piece in terms of ordered pairs of onset time \nand pitch  sorted by onset time, \n \n1. Divide the piece using a gliding window approach into \noverlapping windows of n different adjacent onset times \n2. Obtain all possible combinations of melodic strings from each \nwindow \n \nN-grams are constructed from the interval sequence(s) of one or \nmore monophonic sequence(s) within a window.  Intervals (the \ndistance and direction between adjacent pitch values) are a \ncommon mechanism for deriving patterns from melodic strings, \nbeing invariant to transpositions [10].  For a sequence of n pitches, \nan interval sequence is derived with n-1 intervals by Equation (1). \n \ni i i Pitch Pitch Interval − = +1   (1) \n \nTo illustrate the pattern extraction mechanism for polyphonic \nmusic data, the first few bars of Mozart’s Alla Turca, as shown in \nFigure 1, is used.  The performance data of the first two bars of the \npiece was extracted from a MIDI file and converted into a text \nformat, as shown in Figure 2(a). The left column contains the onset \ntimes sorted in ascending order, and the corresponding notes \n(MIDI semitone numbers) are on the right column.  The \nperformance visualised on a time-line is shown in Figure 2 (b). \n \n \n \n \nFigure 1. Excerpt from Mozart’s Alla Turca  \n     \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n                      \n                  2(a) \n01020304050607080\n0 500 1000 1500 2000\nTime (ms)Pitch number\n \n                                         2(b) \n \n   \nFigure 2.  (a) Onset times and pitch events for Mozart’s  \n   All Turca (b) performance visualized on a time-line \n \nFollowing the steps outlined in obtaining the n-grams and applying \nEquation (1) in pattern derivation, the interval sequences from the \nfirst 3 windows of length-3 onset times of the performance data in \nFigure 2 are: \nWindow 1: [-2 -1]  \nWindow 2: [-1 1] \nWindow 3: [1 –12]  and  [1 3]  \n To add to the information content of the n-grams constructed \nusing interval sequences, the duration dimension of music \ninformation is used.  Numerous studies have been carried out with \nthe use of patterns generated from various combinations of the \npitch and duration dimensions.  These studies either used pitch \ninformation [8, 11], rhythm information [12] or both pitch and \nrhythm information simultaneously [4, 13].  In using the duration \ndimension for pattern derivation, a common mechanism is to use \nthe relative duration of a note to a designated base duration such \nas the quarter or the sixteenth note.  Relative durations are widely \nused as they are invariant to changes of tempo [10].  However, the \nchoice of base durations such as the quarter or the sixteenth note \ncould pose quantisation problems with performance data compared \nto data obtained from score encodings.  With performance data, \none option for the selection of a base duration could be the time \ndifference between the first two notes of a given performance.  \nHowever, with errors such as timing deviations of these two notes \nor recordings being slightly trimmed off at the beginning, this error \nwould be duplicated in obtaining rhythmic information of the \nwhole performance data.   \n                                    \nIn our approach, we look at the onset times pattern based on the \ntimeline - the times at which pitch events occur.  The approach in \nusing time between consecutive note onsets has been studied by I. \nShmulevich et. al. [14].  For pattern derivation using rhythm \ninformation, the ratios of time difference between adjacent pairs of \nonset times form a rhythmic ratio sequence.  With this approach, it \nis not necessary to quantise on a predetermined base duration, to \nuse the duration length of a note (which can be difficult to \ndetermine from audio performances) and we do not assume any \nknowledge of beat and measure information.  For a sequence of n \nonset times, a rhythmic ratio sequence is derived with n-2 ratios \nobtained by Equation (2). \n \n\n\n\n−−=\n++ +\ni ii i\niOnset OnsetOnset OnsetRatio\n11 2  (2) \n \nIn obtaining n-grams that incorporate interval and rhythmic ratio \nsequences using n onset times and pitches, the n-gram would be \nconstructed in the pattern form of:  \n \n[ Interval1 Ratio1 … Intervaln-2 Ration-2 Intervaln-1  ] \n \nUsing the example of Figure 2, the combined interval and ratio \nsequences from the first 3 windows of length 3-onset are: \n \nWindow 1: [-2  1  -1] \nWindow 2: [-1 1 1] \nWindow 3: [1 1 -12] and [1 1 3] \n \nNote that the first and last number of each tuple are intervals while \nthe middle number is a ratio. \n 0 71 \n150 69 \n300 68 \n450 69 \n600 57 \n600 72 \n900 64 \n900 60 \n1200 74 \n1200 64 \n1200 60 \n1350 72 \n1500 71 \n1500 64 \n1500 60 \n1650 72 Window3 Window 1 \nWindow 2 \nWindow 4 3.2 Pattern encoding \n \nIn order to be able to use text search engines we need to encode \nour n-gram patterns with text characters.  One challenge that arises \nis to find an encoding mechanism that reflects the pattern we find \nin musical data.  With large numbers of possible interval values and \nratios to be encoded, and a limited number of possible text \nrepresentations, classes of intervals and ratios that clearly represent \na particular range of intervals and ratios without ambiguity had to \nbe identified.  For this, the frequency distribution for the directions \nand distances of pitch intervals and ratios of onset time differences \nthat occur within the data set were obtained.  A data collection of \n3096 MIDI files of a classical music collection was used in \nobtaining these frequencies. These were mostly classical music \nperformances obtained from the Internet.  \n[http://www.classicalarchives.com] \n \nFor the pitch encoding, firstly the data set was analysed for the \nrange and interval distances that occur within the data set and the \nfrequency at which these occur.  The frequency distribution versus \ninterval (in units of semitones) graph obtained is shown in Figure \n3. \n \nAccording to Figure 3, the vast bulk of pitch changes occurs \nwithin one octave (i.e., -12 to +12 semitones). A good encoding \nshould be more sensitive in this area than outside of it.  We chose \nthe code to be the integral part of a differentiable continuously \nchanging mapping function (3), the derivative of which \napproximately matches the empirical distribution of intervals in \nFigure 3.   \n \n\n\n\n\n\n=−\nYIntervaX Code1nltanhint   (3) \n \nIn Equation (3), X is a constant set to 27 for our experiments as a \nmechanism to limit the codes range to the 26 text letters.  Y is set \nto 24 to obtain a 1-1 mapping of semitone differences in the range \n[-13, 13].  In accordance with the empirical frequency distribution \nof Figure 3, less frequent semitone differences (which are bigger in \nsize) are squashed and have to share codes. Based on the property \nof the tanh curve, Y determines the rate at which class sizes \nincrease as interval sizes increase.  This is a trade-off between \nclasses of small (and frequent) versus large (and rare) intervals.  \nThe codes obtained are then mapped to the ASCII character values \nfor letters.  In encoding the interval direction, positive intervals are \nencoded as uppercase A-Z and negative differences are encoded \nwith lower case a-z and in the centre code 0 being represented by \nthe numeric character 0. \nIn using duration ratios, most studies have assumed quantised \nrhythms, i.e., rhythm as notated in the score [14] owing to \nsimplicity and timing deviations that could occur with performance \ndata.  To deal with performance data, we adopt ratio bins for our \nstudy.   \n 050000100000150000200000250000300000350000400000450000\n-100-75-50-250255075100\nIntervalFrequency\n \n \n \n \nFigure 4 shows the frequency versus the log of the ratios (onset \ntimes were obtained in units of milliseconds).  We analysed the \nfrequency distribution of ratio values of the data collection in order \nto provide quantisation ranges for the bins that reflect the data set.  \nThe peaks clearly discriminate ratios that are frequent and bins for \nratio values for encoding can be established.  Mid-points between \nthese peak ratios were then used to construct bins which provided \nappropriate quantisation ranges in encoding the ratios.  Ratio 1 has \nthe highest peak as expected and other peaks occur in a \nsymmetrical fashion where for every peak ratio identified, there is y,i-a                            Z                  A-I,Y \n         z-a                        0  A-Z \n \nFigure 3. Interval Histogram \n \nFigure 4. Ratio Histograms and Ratio Bins \n a symmetrical peak value of 1/peak ratio.  From our data analysis, \nthe peaks identified as ratios greater than 1 are 6/5, 5/4, 4/3, 3/2, \n5/3, 2, 5/2, 3, 4 and 5.  \n \nThe ratio 1 is encoded as Z.  The bins for ratios above 1 as listed \nabove are encoded with uppercase alphabets A-I and any ratio \nabove 4.5 is encoded as Y.  The various bins for ratios smaller than \n1 as listed above are encoded with lowercase alphabets a-i and y \nrespectively.  The ranges identified with this symmetry and \ncorresponding codes assigned are visualised in Figure 4. \n \n4 IMPLEMENTATION \n4.1 Database development \nOne of the main aims of this study is to examine the retrieval \neffectiveness of the musical words obtained from n-grams based on \nthe pitch and duration information.  The experimental factors \ninvestigated for this initial study were a) the size of interval classes \nand bin ranges for ratios, b) the query length and c) the window \nsize used for the n-gram construction.  We use the same data \ncollection of 3096 classical MIDI performances for the database \ndevelopment as in Section 3.   \n6 databases P4, R4, PR3, PR4, PR4CA and PR4CB were \ndeveloped.  The minimum window size is 3, as at least 3 unique \nonset times would be required in obtaining one onset time \ndifference ratio.  A description of each database and its \nexperimental factors follows:   \n \nP4:  Only the pitch dimension is used for the n-gram construction \nwith the window size of 4 onset times.  Each n-gram is encoded as \na string of 3 characters corresponding to 3 intervals.  Y is set to 24 \nto enable a 1-1 mapping of codes to most of the intervals within a \ndistance of 20. The theoretical maximum of possible index terms is \n148,877 = (26*2+1)3. \n \nR4:  Only the rhythm dimension is used for the n-gram \nconstruction with the window size of 4 onset times.  All bin ranges \nidentified as significant ratio ranges were used in encoding.  The \ntheoretical maximum of possible index terms is 441 = (10*2+1)3. \n \nPR3: The pitch and rhythm dimensions are used for the n-gram \nconstruction in the combined pattern form stated in Section 3 with \nthe window size of 3 onset times.  Y is assigned 24 to enable \nsimilar interval class encoding as P4.  All bin ranges identified as \nsignificant ratio ranges are used in encoding.  The theoretical \nmaximum of possible index terms is 58,989 = (53*21*53). \n \nPR4: The pitch and rhythm dimensions are used for the n-gram \nconstruction as above but with the window size of 4 onset times.  \nAll bin ranges identified as significant ratio ranges are used in \nencoding.  The theoretical maximum of possible index terms is \n65,654,757 = (533*212). \n \nPR4CA:  The pitch and rhythm dimensions are used for the n-gram \nconstruction as above.  To study the effects of the interval class \nsizes within the range of 2 octaves for a 2-1 mapping for most \nintervals for most intervals smaller than 20 semitones, Y is set to \n48.  Although one character now covers at least 2 semitones (as opposed to 1 semitone above), still all alphabets are used with this \nencoding, i.e. 26 uppercase and 26 lowercase letters and 0 for no \nchange.  The encoding for the ratios was made coarser as well : \nwhere we previously used the codes A-I,Y and a-I,y we now use \nthe codes A-D, Y and a-d,y respectively, now A covers what used \nto be represented by A and B, B covers what used to be C and D , \nC covers what used to be E and F etc.  The theoretical maximum \nof possible index terms is 18,014,177 = (533*112).  \n \nPR4CB:  The pitch and rhythm dimensions are used for the n-gram \nconstruction as above.  To study the effects of the interval class \nsizes within the range of 2 octaves for a 3-1 mapping for most \nintervals up to around 20 semitones, Y is set to 72. Coarse ratio \nencoding with bins used as in PR4CA.   \n \nThe summary of databases and experimental factors are shown in \nTable 1.  \n \n \nTable 1. Databases and experimental factors \n \nDatabase Pitch Rhythm n Y # R.Bins #Terms \nP4 Y  4 24  148,877 \nR4  Y 4  21 441 \nPR3 Y Y 3 24 21 58,989 \nPR4 Y Y 4 24 21 65,654,757 \nPR4CA Y Y 4 48 11 18,014,117 \nPR4CB Y Y 4 72 11 18,014,117 \n \n \n4.2 Retrieval Experiments \nIn examining the retrieval effectiveness of the various formats of \nmusical words and to evaluate the various experimental factors, an \ninitial run, R1, was performed on the 6 databases.  For query \nsimulation, polyphonic excerpts are extracted from randomly \nselected musical documents of the data collection.  Query locations \nwere set to be the beginning of the file.  In simulating a variety of \nquery lengths, lengths of the excerpts extracted from the randomly \nselected files were of 10, 30 and 50 onset times.  These excerpts \nwere then pre-processed and encoded to generate musical words \nwith similar formats to the corresponding 6 databases: P4, R4, \nPR3, PR4, PR4CA and PR4CB.  The ranked retrieval method was \nused for run R1 averaged over 30 queries.  In ranking the \ndocuments retrieved, the cosine rule used by the MG system was \nadopted [15] and in evaluating our retrieval using the known item \nsearch of our query excerpt, the Mean Reciprocal Rank (MRR) \nmeasure was used.  The reciprocal rank is equal to 1/r where r is \nthe rank of the music piece the query was taken from.  In using the \nknown item search, the rank position of the document that the \nquery was extracted from was used in obtaining the reciprocal rank \nmeasure.  These were averaged over the 30 queries.  This MRR \nmeasure is between 0 and 1 where 1 indicates perfect retrieval.  \nThe retrieval results are shown in Table 2. \n Table 2. MRR measures for run R1 \n 10 30 50 \nP4 0.60 0.77 0.81 \nR4 0.03 0.11 0.15 \nPR3 0.46 0.74 0.81 \nPR4 0.74 0.90 0.95 \nPR4CA 0.71 0.83 0.71 \nPR4CB 0.47 0.68 0.73 \n \n \nThe results clearly indicate that using n-grams with polyphonic \nmusic retrieval is a promising approach with the best retrieval \nmeasure 0.95 being obtained by musical words of the PR4 format \nand a query length of 50 onset times. Comparing the retrieval \nmeasures of P4 and PR4 for all 3 query lengths, it can be said that \nthe addition of rhythm information to the n-gram is a definite \nimprovement to widening the scope of n-gram usage in music \ninformation retrieval. \n \nThe length of a window for n-gram construction would require \nfurther study, as there are clear improvements of measures \nbetween PR3 and PR4 for all query lengths.  Further experiments \nwill be needed to obtain the optimal length.  In looking at the class \nsize of the intervals and bin range of ratios, measures clearly \ndeteriorate from smaller class sizes of PR4 to larger sizes of \nPR4CA and PR4CB.  The class sizes require further investigation \nto determine its usefulness in providing allowances for more fault-\ntolerant retrieval.   \n \nIn general, and as expected, the measure improves with the length \nof the query for all databases although retrieval using only ratio \ninformation with R4 is almost insignificant.  Clearly, the 441 \npossible different index terms are insufficient to discriminate music \npieces. \n \n4.3 Error Simulation \nA second run, R2, was performed by simulating errors in the \nqueries to study the retrieval behaviour under error conditions. \nError models used in monophonic music described in [3, 8] were \nnot adopted for this study as the range of intervals was \nsignificantly different.  As there were no error models available \nwith polyphonic music, we adopted the Gaussian error model for \nintervals as shown in Equation (4) and for ratios as shown in \nEquation (5).  ε is the Gaussian standard random variable and Di is \nthe mean deviation for an interval error and Dr is the mean \ndeviation for an error in the ratio. \n \n*(i k k D Intervall NewInterva + = ε ) (4) \n*.(exp* r k k D Ratio NewRatio = ε ) (5) \n As an initial attempt to investigate retrieval with error conditions, \nwe arbitrarily selected two sets of error deviation values D1 and \nD2.  With D1,  Di was assigned 3 and Dr assigned as 0.3.  For the \nsecond set of mean error deviation values, Di was assigned 2 and \nDr was retained as 0.3.  Dr was left unchanged, as the ratio bin \nrange was not varied between PR4CA and PR4CB. All musical \nwords generated for the similar queries used in R1 and with length \n30 were modified by incorporating the error deviation for the pitch \nand duration dimensions correspondingly for the 3 databases PR4, \nPR4CA and PR4CB. The MRR measures are shown in Table 3. \n \nTable 3. MRR measures for run R2 \n D1 D2 \nPR4 0.24 0.50 \nPR4CA 0.30 0.65 \nPR4CB 0.27 0.50 \n \n \nThe results clearly indicate that musical words encoded with a \nwider interval class size perform better with error conditions.  A \ncompromise between musical words encoded using larger interval \nclass sizes and wider ratio bin ranges and smaller ones is clearly \nrequired.  This can be seen from the improvement in measures \nobtained with run R2 and deviation set D2 of Table 3 where the \nmeasure of PR4CA is 0.65 and PR4 only 0.50. For the counterpart \nrun, R1, with no query errors, it indicates deterioration in measure \nwith the wider encoding (where a measure of 0.90 was obtained \nwith PR4 and only 0.83 for PR4CA with query length 30). \n \nThis initial experiment under error conditions clearly identifies the \nneed for a detailed analysis in obtaining optimal values for interval \nclass size and effective retrieval in using n-grams in polyphonic \nmusic retrieval.  \n \n5 FUTURE WORK \nBased on the experimental results and initial experimental factors \ninvestigated, this study will be continued with an in-depth study of \nthe following experimental factors: a) query length b) window \nlength c) ration bin range d) Y value for interval classification e) \nerror model.  Further issues for investigation are a) the \ndevelopment of error models with polyphonic music, b) a \nrelevance judgment investigation in assessing the documents and \nfiner retrieval measures, c) suitability of the ranking mechanism for \nmusical words, d) an analysis of the search complexity of the \nalgorithm in extracting all possible patterns \n \n6 CONCLUSIONS \nThis study has proven the usefulness of using n-grams in \npolyphonic music data retrieval.  An interval mapping function was \nutilised and proved useful in mapping interval classes over the text \nalphabetical codes.  Onset time ratios have proven useful for \nincorporating rhythm information.  With the use of bins for ranges \nof significant ratios, the rhythm quantisation problem in music performance data has been overcome.   The results presented so \nfar for polyphonic retrieval are qualitatively comparable to \npublished successful monophonic retrieval experiments [8] and, \nhence, very promising.  \n \n7 ACKNOWLEDGEMENTS \nThis work is partially supported by the EPSRC, UK. \n \n8 REFERENCES \n[1] David Huron, Perceptual and Cognitive Applications in \nMusic Information Retrieval, International Symposium on \nMusic Information Retrieval, Music IR 2000, Oct 23rd - 25th, \n2000, Plymouth, Massachussetts. \n[2] Andreas Kornstadt, Themefinder: A Web-Based Melodic \nSearch Tool, Computing in Musicology 11, 1998, MIT Press \n[3] Rodger J. MacNab, Lloyd A.Smith, David Bainbridge and Ian \nH. Witten, The New Zealand Digital Library MELody \ninDEX, D-Lib Magazine, May 1997 \n[4] M. Clausen, R. Engelbrecht, D. Meyer, J. Schmitz, PROMS: \nA Web-based Tool for Searching Polyphonic Music, \nInternational Symposium on Music Information Retrieval, \nMusic IR 2000, Oct 23rd - 25th, 2000, Plymouth, \nMassachussetts. \n[5] David Huron, Humdrum and Kern: Selective Feature \nEncoding, Beyond MIDI: The Handbook of Musical Codes, \npp 375-40. \n[6] Eleanor Selfridge-Field, Conceptual and Representational \nIssues In Melodic Comparison, Computing in Musicology 11, \n1998, pp 1-64 [7] Massimo Melucci and Nicola Orio, Music Information \nRetrieval using Melodic Surface, The Fourth ACM \nConference on Digital Libraries ’99, Berkeley, USA,pp 152- \n160 \n[8] Stephen Downie and Michael Nelson, Evaluation of A Simple \nand Effective Music Information Retrieval Method, SIGIR \n2000, Athens, Greece, pp 73-80 \n[9] Tim Crawford, Costas S. Iliopoulus and Rajeev Raman, \nString-Matching Techniques for Musical Similarity and \nMelodic Recognition, Computing in Musicology 11, 1998, \nMIT Press, pp 73-100 \n[10] Kjell Lemström, Atso Haapaniemi, Esko Ukkonen, Retrieving \nMusic - To Index or not to Index, ACM Multimedia '98, -Art \nDemos, Technical Demos - Poster Papers, September 1998, \nBristol, UK \n[11] Steven Blackburn and David DeRoure, A Tool for Content-\nBased Navigation of Music, ACM Multimedia '98, Bristol, \nUK, pp 361 – 368 \n[12] Chen, J.C.C. and A.L.P. Chen, Query by Rhythm: An \nApproach for Song Retrieval in Music Databases, In proc. \nOf IEEE Intl. Workshop on Research issues in Data \nEngineering, pp 139-146, 1998 \n[13] Shyamala Doraisamy, Locating Recurring Themes in Musical \nSequences, M. Info. Tech. Thesis, 1995, University Malaysia \nSarawak. \n[14] I. Shmulevich, O. Yli-Harja, E. Coyle, D.-J. Povel, and K. \nLemström, Perceptual Issues in Music Pattern Recognition – \nComplexity of Rhythm and Key Finding, In Proceedings of \nthe AISB ’99 Symposium on Musical Creativity, pages 64-69, \nEdinburgh, 1999 \n[15] Ian H. Witten, Alistair Moffat and Timothy C. Bell, \nManaging Gigabytes: Compressing and Indexing Documents \nand Images, 2nd edition, 1999, Morgan Kaufmann Publishers"
    },
    {
        "title": "A Technique for Regular Expression Style Searching in Polyphonic Music.",
        "author": [
            "Matthew J. Dovey"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1416140",
        "url": "https://doi.org/10.5281/zenodo.1416140",
        "ee": "https://zenodo.org/records/1416140/files/Dovey01.pdf",
        "abstract": "This paper discussed some of the ongoing investigative work on integrating these two systems conducted as part of the NSF/JISC funded OMRAS (Online Music Retrieval and Searching) project into polyphonic searching of music. It describes a simple and efficient “piano-roll” based algorithm for locating a polyphonic query within a large polyphonic text. It then describes ways in which this algorithm can be modified without affecting the performance to allow more freedom in the how a match is made, allowing queries which involve something akin to polyphonic regular expressions to be located in the text.",
        "zenodo_id": 1416140,
        "dblp_key": "conf/ismir/Dovey01",
        "keywords": [
            "ongoing investigative work",
            "integration of systems",
            "OMRAS project",
            "polyphonic searching",
            "piano-roll based algorithm",
            "large polyphonic text",
            "polyphonic query",
            "efficient algorithm",
            "freedom in match making",
            "polyphonic regular expressions"
        ],
        "content": "A technique for “regular expression” style searching in \npolyphonic music \nMatthew J. Dovey \nVisiting Research Fellow \nDept. of Computer Science  \nKings College, London \n+44 1865 278272 \nmatthew.dovey@las.ox.ac.uk \n \nABSTRACT \nThis paper discussed some of the ongoing investigative work on \nintegrating these two systems conducted as part of the NSF/JISC \nfunded OMRAS (Online Music Retrieval and Searching) project \ninto polyphonic searching of music. It describes a simple and \nefficient “piano-roll” based algorithm for locating a polyphonic \nquery within a large polyphonic text. It then describes ways in \nwhich this algorithm can be modified without affecting the \nperformance to allow more freedom in the how a match is made, \nallowing queries which involve something akin to polyphonic \nregular expressions to be located in the text. \n1. INTRODUCTION \nThe OMRAS (Online Music Retrieval And Searching) project is \na three year collaborative project between Kings College London \nand the Center for Intelligent Information Retrieval, University of \nMassachusetts. Its primary aim is to look at various issues \nsurrounding content based searching of polyphonic music; \ncurrent research in content based music searching has primarily \nconcentrated on monophonic music, that is to say music \nconsisting of only a single melodic line and ignoring the \ncomplexities find in a more complex music texture for example \nas found in an say an orchestral symphony. \nDifferent computer representations of music roughly fall into two \nbasic categories: those representing the audio of the music such \nas a typical wave or aiff file (or more topically MP3), and those \nrepresenting the symbolic structure of the music as indicated in a \ntypically written musical score. The audio file formats typically \nrepresent an actual performance of a piece, whilst the symbolic \nformats represent the composer’s instructions and guidelines to \nthe performer. In practice, as described in Byrd and Crawford \n(2001) [1], these are two extremes of a spectrum with various \nformats falling in between which contain elements of both such \nas MPEG7 and MIDI. MIDI, for example, was originally \ndesigned for representing music performances but is closer to a \nsymbolic notation than an audio representation (and is used to \nindicate instructions for a performance rather than record a \nperformance). (Byrd and Crawford actually talk of three distinct \ntypes – the two referred to above plus MIDI representing the \nmiddle ground) \nOne aspect of OMRAS is to look at conversion of formats \nmoving along this spectrum. Moving from symbolic notations to \naudio is fairly straightforward but this is to be expected since the \nrole of the symbolic notation is to describe how to enact a \nperformance of the music. Going from the audio to a symbolic \ndescription of how to perform that audio is far more difficult. A \ngood comparison would be between a performance of a play, and \nthe script and stage-directions for performing that play. A second aspect on OMRAS is to consider the searching of music \nby content. As indicated above we are concentrating on \npolyphonic music since this is currently neglected in the existing \nwork. One of the groups in OMRAS has been looking at audio \nwork, but this paper concentrates on searching symbolic formats \nwhich represent Common Music Notation (CMN) of the Western \ntradition of music. Most work on music searching has \nconcentrated on searching within monophonic, single voice \nmusic, often applying techniques derived from the text-retrieval \nworld (e.g. Downie (1999) [5]). There are been some approaches \nat polyphonic searching (e.g. Uitdenbogerd and Zobel (1998) [9], \nHolub, Iliopoulos, Melichar and Mochard (1999) [6] and \nLemström and Perttu (2000) [7]). This paper will outline some of \nthe algorithms we have developed for this purpose, which we \nbelieve are more versatile for regular expression style searching \nthan previous work in this area. \nA key mission statement of the joint JISC/NSF International \nDigital Library Initiative, which is funding the OMRAS work is \nthat the projects should make existing digital collections more \naccessible. We feel that the work of OMRAS makes digital \ncollections of music more accessible by providing content based \nsearching possible, in addition to standard metadata searched \nsuch as by composer or title. OMRAS is collaborating with the \nJISC funded JAFER project at Oxford University to provide \nintegration with existing music library catalogues1. \n2. THE OMRAS TEST FRAMEWORK \nWithin the OMRAS project we have written a test framework \nusing the Java programming language for testing the performance \nof various search algorithms and techniques. The framework is \ncommand line driven (and not really designed for novice users). \nFrom the command line we can load a music file into the \nsystem), can load in different algorithms and can load in different \nuser interface components for displaying and editing queries and \nresults. The framework allows us to experiment with a number of \ndifferent algorithms and a number of different representations of \nmusic. A screenshot of the command line framework in use is \ngiven in figure 1. \nThe framework has been designed to take advantage of Java’s \nobject-oriented nature: all the components such as file format \nmodules, user interface widgets and search algorithms are \n                                                             \n1 http://www.lib.ox.ac.uk/jafer and http://www.jafer.org  \nFigure 1 \n \nimplemented as independent java objects, so that the entire \nframework is modular. These java objects can be manipulated in \nthe framework using a scripting interface for java such as \nBeanShell2, but selected components can be used in other \nsoftware. In particular, some engineering undergraduates have \nbeen working with OMRAS to build a GUI interface to the \nframework in addition to the current command line interface. \nCollaborating with the JISC funded JAFER Project at Oxford \nUniversity3 we have reused the components to build a prototype \ndemonstrating how our algorithms for content based searching \ncan be integrated into existing bibliographic oriented music \nlibrary catalogue systems (Dovey, M (2001) [4]). \nAt the moment we only have modules for handling a small \nnumber of file formats (including MIDI) but are working on \nothers most notably GUIDO and Kern4. The user interface \ncomponents are based on piano roll type displays, but we are \nworking on incorporating better CMN software for displaying \nscores into the framework in the near future. We also have \nobjects for rendering scores into audio, using the Java Media \nFramework. \n                                                             \n2 http://www.beanshell.org \n3 http://www.jafer.org and http://www.lib.ox.ac.uk/jafer \n4 A good reference of various music file formats can be found in \nSelfridge-Field (1997) [8]. 3. BASE ALGORITHM FOR SEARCHING \nPOLYPHONIC MUSIC \n3.1 “Piano Roll” Model of CMN \nCommon Music Notation (CMN) is a very complex pictorial \nlanguage for describing music with a number of cues as to how it \nshould be performed. Its pictorial nature proved very difficult for \nin the history of the printing press; in many cases the most \nefficient means to produce a printed score was to create an \nengraving rather than attempt to produce a generic type-set for \nmusic. It is not surprising, therefore, that CMN produces \ncomplex problems for computer based representations. We are \nworking with a very simple model for representing music, but \none, which can provide a skeleton for re-introducing some of the \ncomplexities of CMN.  \nThe representational model of music we are currently using can \nbe compared to that of a piano roll, where punched holes indicate \nwhether a note should be playing. The horizontal position in the \nroll indicates the time of the note, whilst the vertical position \nindicates the pitch of the note. In our base model we only concern \nourselves with the beginnings of notes (in MIDI terms with the \nNote On events). We consider a musical “event” to be a list of \nnotes which begin to play at the same time (in some ways this is \nsimilar to a chord but is more generalized). Whilst not an ideal \nterminology, we will use the term “event” in this manner, for the \npurposes of this paper. We only introduce a new musical event \nwhere needed, i.e. because a new note has begun to play. In this \nmodel we can regard a piece of music to be a sequence of \nmusical events. For example the extract in Figure 2,  \nFigure 2 \n \ncould be represented as the following list of musical events: \n1. F, C \n2. G, D \n3. A, E \n4. C \n5. E \n6. E \n7. G, C \nThis leads an obvious representation as a matrix of zeros and \nones indicating whether a note begins to play at that time and \npitch and this is used as an effective way to display queries and \nresults. For example the more complex extract in Figure 3 \n \nFigure 3 \n \ncan be represented in this piano roll format as in Figure 4 \n \n 79   •        77           76     •      74         • C5 72 •          71           69           67 •  •  •      65        •   64 •   •     •  62          C4 60  •       •  59   •    •    57    •  •     55     •     Figure 4 \n \nIn the OMRAS framework, we represent as an XML structure \nsuch as: \n<score> \n  <event> \n    <note pitch=”72”/> \n    <note pitch=”67”/> \n    <note pitch=”64”/>   </event> \n    <note pitch=”60”/> \n  <event> \n  </event> \netc… \n \nBy representing this structure as an XML Schema document we \ncan generate the Java object model for this format from the XML \nSchema description. Representing CMN in this manner allows us \nto add additional music information. For example onset time (in \nmilliseconds as for MIDI, or metrical information such as bar \nnumber of beat within the bar) can be added as additional \nattributes for the event element; duration of notes, voicing, \ninstrumentation etc. can be added as additional attributes to the \nnote element. \n3.2 Searching in a “piano roll” model \nIn the model, described above the typical search problem can be \nexpressed as follows: given a query as a sequence of musical \nevents and a text to perform the query on as a sequence of \nmusical events, we are trying to find a sequence of musical \nevents in the text such that each musical event in the query is a \nsubset of the musical event in the text. This again is best \nillustrated by an example. Consider the musical extract in figure \n5. \n \n \nFigure 5 \n \n \nAs a piano roll, this would be represented as in Figure 6. \n \n \n 79   •        77           76     •      74         • C5 72 •         Figure 6 \n \n \nA potential match is illustrated in Figure 7. As can be seen, there \nis some allowance in that intervening musical events are allowed \nbetween matched events in the text. The freedom of this can be \nlimited to avoid too many spurious matches. \n \n  \n 79   •        77           76     •      74         • C5 72 •          71           69           67 •  •  •      65        •   64 •   •     •  62          C4 60  •       •  59   •    •    57    •  •     55     •     Figure 7 \n \nIn musical terms, it is also necessary to allow transpositions of \npitch. Figure 8 gives a second match at a different transposition. \n \n 79   •        77           76     •      74         • C5 72 •          71           69           67 •  •  •      65        •   64 •   •     •  62          C4 60  •       •  59   •    •    57    •  •     55     •     Figure 8 \n \nThere are other degrees of freedom that need to be allowed in \nsearching, in order to accommodate inaccurate recall of the user, \nand also inaccuracies in the data, for example if the musical \ninformation has been created by automatic transcription of either \naudio data or via optical recognition of the printed score. A fuller \ndescription of these areas of fuzziness is given in Crawford and \nDovey (1999) [2]. \n3.3 A “piano roll” based algorithm \nIn Dovey (1999) [3], we presented a matrix-based algorithm for \nsearching the music using the piano roll abstract model described \nabove. That algorithm has underpinned much of our subsequent \nwork, however there were some serious performance issues with \nthe algorithm described there in pathological cases. The \nalgorithm described here is a more refined version of that \nalgorithm which performs in effectively linear time for a given \ntext. \nLet the text be represented by the sequence <Tm> and the query \nby the sequence <Sn> and consider the case when we can allow k \nintervening musical events in the text between the matches for \nconsecutive events in the query. \nIn essence, we are performing a pass over the text for each \nmusical event in the query. In the first case we are looking for \nthe matches for the first event in the query to occur in the text. \nFor all subsequent passes we are looking for an occurrence of the \nnth  term of the query occurring within k musical events of an \noccurrence of the n-1th term found in the previous pass. \nMathematically speaking, we construct the matrix M, where \nMi j =  k+1 iff Sj ⊆ Ti  and (Mi-1 j-1 ≠ 0 or i = 0) \n Mi j-1-1 iff (not Sj ⊆ Ti ) and (Mi j-1 ≠ 0) \n 0 otherwise \n A result can be found by traversing the last row of the matrix \nconstructed for the value k+1, this indicates by it horizontal \nposition the match in the text for the last event in the query, \nreading to the left from this position in the row above for the \nvalue k gives the match for the penultimate event in the query \nand so on. We then repeat the process for each pitch \ntransposition of the query sequence (i.e. where the value pitch of \nnote in the sequence <Sn> is transposed up or down by the same \namount). We clearly need not consider any transposition such \nthat the transposed sequence of <Sn> and the sequence <Tm> \nhave not pitches in common. \nIn the worse case this algorithm can never take more that m x n \nsteps to locate each pitch transposition, and the number of \ntranspositions will never exceed the range of pitches which occur \nin <Tm>. \nThe following worked example would make this clearer. Let use \nconsider the text in figure 4. Then our text is \nT0 = 72, 76, 64 \nT1 = 60 \nT2 = 79, 67, 59 \netc. \nLet us consider a simple query \nS0 = 67, 59 \nS1 = 67, 55 \nS2 = 59 \nFor this case we allow one intervening gap, i.e. k=1. \nWe build the first row of the matrix by writing k+1 (i.e. 2) where \nS0 contains all the pitches in Ti, otherwise if the value of the \npreceding square is non-zero we write that value less one, \notherwise zero. So the first line becomes \n T0 T1 T2 T3 T4 T5 T6 T7 T8 \nS0 0 0 2 1 0 0 0 0 0 \n \nFor the second line we perform a similar operation for S1, \nhowever we only write k+1 if all the pitches of S1 are in Ti and \nthe value of the preceding square in the row above is non-zero. \nSo we now have \n T0 T1 T2 T3 T4 T5 T6 T7 T8 \nS0 0 0 2 1 0 0 0 0 0 \nS1 0 0 0 0 2 1 0 0 0 \n \n \nWe then repeat for S2 giving \n T0 T1 T2 T3 T4 T5 T6 T7 T8 \nS0 0 0 2 1 0 0 0 0 0 \nS1 0 0 0 0 2 1 0 0 0 \nS2 0 0 0 0 0 0 2 1 0 \n \nNote that although S2 occurs within T2 the preceding square \nabove in the second row is zero, so we do not write k+1 (i.e. 2) \nhere. The results can now be found in linear time by reading the \nmatrix backwards looking for the occurrence of the value 2 i.e. here T2 T4 T6 is our match. We can then repeat for the other 15 \npossible pitch transpositions of the query.  \nThere is an optimization to avoid unnecessary comparisons of \nmusical events. For the row i of the matrix there exist a maximal \ns such that Mi j = 0 for all j < s. Similarly there exists a minimal t \nsuch that Mi j = 0 for all j > t. From this we can deduce that Mi+1 j \n= 0 for all j < s and j > t+k, i.e. we can limit our attention when \nconstructing the next row of the matrix to the subsequence Ts, \nTs+1 … Tt+k of the text. Of course when t < s there can be no \nmatches for the query in the text. This optimization can cut \nprocessing time dramatically. \nThe above method forms the basis for more complex searching \ntechniques where we have a query which in many ways \nresembles a regular expression for a musical phrase.  \n4. EXTENSIONS OF BASE ALGORITHM \nTO HANDLE REGULAR EXPRESSION \nTYPE QUERIES \n4.1 Comparisons of Music Events \nIn the algorithm described in section 3, we considered only one \nway in which a musical event in the query can match a musical \nevent in the text, namely that of inclusion. i.e. we say that a \nmusical event Sj matches Ti if all the notes in Sj are in Ti (i.e Sj ⊆ \nTi  ). Dovey (1999) [3] considers four such comparison operators: \n• Sj ⊆ Ti – as described here.   \n• Sj = Ti – for exact matching \n• Sj \n\u0000\ni  \u0001Ø –  Sj here represents a disjunctive lists of \nnotes we wish to match \n• Ti ⊆  Sj – for symmetry of the operators \n \nThis is generalized in Crawford and Dovey (1999) [2], so that \nthese become just four of the most typical comparison operators \nin a lattice of all possible comparison operators with equals \nbeing the Top of this lattice and always matches the Bottom. In \ngeneral we have some relationship R which defines whether two \nevents “match” or not. Other typical relationships may include: \n• Rh : Sj and Ti are harmonically similar. \n• Rr : For each note in Sj there is a note in Ti whose pitch \nis within a given range of the pitch of the note in Sj \n• Rr* : For all but x notes in Sj there is a note in Ti whose \npitch is within a given range of the pitch of the note in \nSj \n \nOur base algorithm described in section 3.3 can easily be \nextended to accommodate any such relationship R without any \nlose of performance. In this case given our relationship R we \nconstruct the matrix M where \nMi j =  k+1 iff Sj R Ti  and (Mi-1 j ≠ 0 or i = 0) \n Mi j-1-1 iff (not Sj R Ti ) and (Mi j-1 ≠ 0) \n 0 otherwise \n \nWe can further generalize this. Given that we perform a pass \nover <Tm> for each Sj in <Sn>, we can use a different \nrelationship for each pass again without any loss in performance. \ni.e. we now have a sequence <Rn> where Rj describes the comparison operation for locating matches of Sj in <Tm>. In this \ncase we construct the matrix M where \nMi j =  k+1 iff Sj Rj Ti  and (Mi-1 j-1 ≠ 0 or i = 0) \n Mi j-1-1 iff (not Sj Rj Ti ) and (Mi j-1 ≠ 0) \n 0 otherwise \n \nThis allows use for each event in the query to specify how the \nmatch will be found in the text using a polyphonic comparison \nrange from exact match to more fuzzy matching comparisons. \n4.2 Adding additional dimensions to notes \nSo far we have considered a note only to have a single value \nindicating its pitch. Clearly the algorithm described in section \n3.3 and the enhancements described in section 4.1 would apply \nto any property. In the case of duration we would clearly be more \ninterested in a comparison operator of the type Rr or Rr*. For \ninstrumentation or voicing we would be more interested in strict \nequality. If we define each note to be an n-tuple of the properties \nwe are interested in we can perform a match against all these \nproperties by defining a suitable composite comparison operator \nR without affecting the efficiency of our algorithm. \nFor example, in the case of three properties pitch, duration and \ninstrument then R might behave as follows \nSj R Ti if for almost all notes in Sj there is a note in Tj \nwith a similar pitch, a similar duration and in the same \nvoice (with some suitable parameters defining the \nranges for similar and almost all). \n4.3 Varying gaps between event matches \nIn section 4.1, we showed that we could have a different \ncomparison operator for each term in the query. However, we \nstill only have a single value of k, determining the allowed “gap” \nbetween matched events in the text, for the entire query. This \nalso need not be the case. We can instead consider the sequence \n<kn> where kj indicates the number of allowed gaps that can \noccur in the text after a match of Sj before a match of Sj+1 occurs. \nkn clearly has no meaning but must be non-zero for the algorithm \nto work properly. Modifying the generic form of the algorithm \nfrom section 3.3 gives \nMi j =  kj+1 iff Sj ⊆ Ti  and (Mi-1 j-1 ≠ 0 or i = 0) \n Mi j-1-1 iff (not Sj ⊆ Ti ) and (Mi j-1 ≠ 0) \n 0 otherwise \nWhen reading the matrix for results we now look for the value \nkj+1 to indicate matches where j is the current row of the matrix. \nModifying the form of the algorithm given at the end of section \n4.1 gives \nMi j =  kj+1 iff Sj Rj Ti  and (Mi-1 j-1 ≠ 0 or i = 0) \n Mi j-1-1 iff (not Sj Rj Ti ) and (Mi j-1 ≠ 0) \n 0 otherwise \nSo far we have considered the “gap” to be measured in terms of \nthe number of events that can occur. Clearly in a piece of music \nthe time between two consecutive events can vary. We can \nincorporate this into the algorithm by allowing the “gap” to be \nspecified in terms of the time between matched events rather \nthan the number of intervening events. We define an monotonic \nincreasing function O on <Tm> where \nO(Ti) is the time at which Ti occurs.  The units could be milliseconds as in MIDI or could be based on \na musical metric such as number of quarter notes. \nIn this case we set k to be the maximum allowed duration \nbetween matched events in the text. The base algorithm from \nsection 3.3 now becomes \nMi j =  k iff Sj ⊆ Ti  and ((Mi-1 j > 0 and Mi-1 j < k) or i = 0)) \n Mi j-1- (O(Ti) – O(Ti-1)) \n    iff (not Sj ⊆ Ti ) and (Mi j-1- (O(Ti) – O(Ti-1)) > 0) \n 0 otherwise \nReading the results matrix now involves looking for the \noccurrences of the value of k. The modifications described in \nsections 4.1 and 4.2 can be applied to this form. \n4.4 Omission of events in the query \nThe final modification to the base algorithm is to allow a match \nto be found even if some of the events in the query are not \npresent in the matched subsequence of the text. Essentially when \nparsing the matrix we consider non-zero values in not only the \nline immediately above but also previous lines. To avoid \nexcessive parsing of the matrix which would degrade the \nperformance of the algorithm we can build a matrix of ordered \npairs. For notational purposes, given an ordered pair p, we will \ndenote the first value as p[0] and the second as p[1]. \nWorking with the base algorithm from section 3.3 and allowing a \nsequences of up to l events from the query to be omitted from the \nmatch we build the matrix \nMi j =  (k+1, l) iff Sj ⊆ Ti  and (Mi-1 j-1 ≠ (0, 0) or i = 0) \n (Mi j-1[0] - 1, Mi j-1[1])  iff (not Sj ⊆ Ti ) and (Mi j-1[0]≠ \n0) \n (Mi-1 j[0], Mi-1 j[1] - 1)  iff (not Sj ⊆ Ti ) and (Mi-1 j[1]≠ \n0) \n (0, 0) otherwise \nParsing the matrix for matches is a matter of looking for the \nvalue k+1 as the first member of any ordered pairs. Limiting the \nnumber of sequences omitted can be performed when parsing the \nmatrix for results. Again the modifications described in sections \n4.1, 4.2 and 4.3 can also be applied in conjunction with this \nmodification. \n4.5 An XML query structure \nCombined these modifications allow us to search a musical text \ngiven a polyphonic query and a number of degrees of freedom. \nConsidering just note pitches and durations, we can use the \nfollowing XML structures such as the following to write a query \nallowing some of these degrees of freedom \n<query omissions=”0”> \n  <event \n    following-gap=2 \n    following-gap.units=”events” \n    content-omissions.max=”0” \n    content-omissions.min=”0”> \n     <note \n       pitch.min=”60” \n       pitch.max=”65” \n       duration.min=”1” \n       duration.max=”1”/> \n  </event> \netc… \n Here the omissions attribute of the query element tells us that \nwe do not allow any events of the query to be omitted in the \nmatch. The following-gap attribute of the event element tells us \nthe “gap” that can occur after this event in the text before the \nnext event must occur; the following-gap.units whether this is \nmeasure in events or durations. The content-omissions.min and \ncontent-omissions.max tell us how many notes can be omitted \nfrom the match in order for it still to be classified as a match. \nThe pitch.min, pitch.max, duration.min and duration.max \nattributes of the note element define ranges for a note in the text \nto match.  \nWhilst the algorithms described here can efficiently cope with \nthis sort of query, there are other queries which can be handled \nwhich cannot be articulated in this XML structure. \n5. FURTHER WORK \nAt present the algorithms described here merely locate matches \ngiven a number of degrees of freedom. There is no attempt to \nrank these matches. The calculation of a rank could be made as \nthe matrix is parsed for matches and some pre-parsing could also \nbe performed as the matrix is built. Crawford and Dovey (1999) \n[2] outline a number of similarity measures which could be used \nin creating a ranking algorithm such as completeness, \ncompactness, musical salience, harmonic progression, rhythmic \nsimilarity, metrical congruity. This ranking process is essential \nbefore we can fully evaluate the effectiveness of this type of \napproach to music information retrieval. \nGiven the amount of freedom these algorithms allow in the \nspecification of a query there is a need for query languages and \nGUI query interfaces to allow users to easily express such \nqueries. Some work has already been undertaken in this area. \nThe XML structure above is very much a working structure and \nnot intended for general use. \n6. ACKNOWLEDGEMENTS \nI would like to thank Tim Crawford of Kings College London \nand Don Byrd of the University of Massachusetts at Amherst for \ntheir assistance in working on this topic and their patience in the \namount of time it has taken to put together a description of these \nalgorithms. I would also like to acknowledge the NSF/JISC IDLI \nand JISC DNER programmes which are funding the OMRAS and \nJAFER projects upon which this work is based. \n \n7. REFERENCES \n[1] Byrd, D. and Crawford, T. (2001) Problems of Music \nInformation Retrieval in the Real World. To appear in \nInformation Processing and Management. \n[2] Crawford, T and Dovey, M. \"Heuristic Models of \nRelevance Ranking in Musical Searching\", \nProceedings of the Fourth Diderot Mathematical \nForum., Vienna, 1999. \n[3] Dovey, M, ‘An algorithm for locating polyphonic \nphrases within a polyphonic piece’, Proceedings of the \nAISB’99 Symposium on Musical Creativity, \nEdinburgh, April, 1999. Pages 48-53. \n[4] Dovey, M, ‘Adding content-based searching to a \ntraditional music library catalogue server’, Proceedings \nof the Joint Conference on Digital Libraries, \nRoanoake, VA, 2001. [5] Downie, J. S., ‘Music retrieval as text retrieval: simple \nyet effective’, Proceedings of the 22nd International \nConference on Research and Development of \nInformation Retrieval, Berkeley, CA, 1999.  Pages \n297-298. \n[6] Holub, J., Iliopoulos, C. S., Melichar, B. and \nMouchard, L. ‘Distributed String matching using finite \nautomata’,  Proceedings of the 10th Australiasian \nWorkshop on Combinatorial Algorithms’, Perth, 1999. \nPages 114-128. [7] Lemström, K. and Perttu, S., ‘SEMEX – an efficient \nmusic retrieval protoype’, First International \nSymposium on Music Information Retrieval, \nUniversity of Massachusetts, Plymouth 2000. \n[8] Selfridge-Field, E. (editor), Beyond MIDI, CCARH \n1997. ISBN 0262193949. \n[9] Uidebogerg, A. L. and Zobel, J. ‘Manipulation of music \nfor melody matching’, ACM Multimedia 98 \nProceedings, Bristol 1998. Pages 235-240."
    },
    {
        "title": "Whither MIR Research: Thoughts about the Future.",
        "author": [
            "J. Stephen Downie"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.4739972",
        "url": "https://doi.org/10.5281/zenodo.4739972",
        "ee": "https://zenodo.org/records/4739972/files/Vidyankur_2018 _January_XX.1_Varghese_StemCell.pdf",
        "abstract": "Human beings have participated in the evolution process by selectively breeding and domesticating certain kinds of plants and animals while crossbreeding others. Although it was never termed so, we were practising genetic engineering by keeping the ones that were desirable and eventually eliminating the other. Thanks to the many advances in genetics and genetic engineering today, we are in a position to treat or eliminate a disease from the very root itself, the genes. This is called gene therapy where the corrected genes are introduced into the affected cells either using a viral vector or nanoparticle. Depending on the target cell type, gene therapy can be divided into somatic cell gene therapy and germline gene therapy, which are non-transferable and transferable respectively, to future generations. There are many obstacles to the use of viral vectors, like the unnecessary immunogenic response that it stimulates in the patient and the potential uncertainties or the outcome of this novel therapy. Ethical issues involve sourcingembryonic cells for research, taking consent of the individual who is an embryo, clinical trials, the risk of misuse of this technology to create a superior human class, the affordability of such treatments and the regulations that will govern such therapies. In this essay we take a closer look at the various issues surrounding the use of stem cells and how we see it evolving in the future and if we could ever fully reap the benefits of this revolutionizing technology without compromising our humanness.",
        "zenodo_id": 4739972,
        "dblp_key": "conf/ismir/Downie01",
        "keywords": [
            "Evolutionary breeding",
            "Genetic engineering",
            "Gene therapy",
            "Somatic cell gene therapy",
            "Germline gene therapy",
            "Viral vector",
            "Ethical concerns",
            "Stem cells",
            "Regulations",
            "Revolutionizing technology"
        ],
        "content": "54 S. Varghese : Gene Therapy  \n \n \n   \nVidyankur: Journal of Philosophical and Theological Studies  \nXX/1 Jan 2018   | ISSN P -2320 -9429 |  54-64 \nhttps://www.vidyankur.in  | DOI: 10.5281/ zenodo.  4739971  \nStable URL: http://doi.org/10.5281/ zenodo.  4739971  \n \n \nWhither Gene Therapy?  \nSeby Varghese SJ  \nIII BPh, Jnana Deepa, Pune 411014, India  \n \nAbstract:  Human beings have participated in the evolution \nprocess by selectively breeding and domesticating certain kinds \nof plants an d animals while crossbreeding others. Although it \nwas never termed so, we were practising genetic engineering by \nkeeping the ones that were desirable and eventually eliminating \nthe other. Thanks to the many advances in genetics and genetic \nengineering toda y, we are in a position to treat or eliminate a \ndisease from the very root itself, the genes. This is called gene \ntherapy where the corrected genes are introduced into the \naffected cells either using  a viral vector or nanoparticle. \nDepending on the target cell type, gene therapy can be divided \ninto somatic cell gene therapy and germline gene therapy, \nwhich are non -transferable and transferable respectively, to \nfuture generations. There are many obstacles to the use of viral \nvectors, like the unnecessary imm unogenic response that it \nstimulates in the patient and the potential uncertainties or the \noutcome of this novel therapy. Ethical issues involve sourcing \nCite as: Varghese, Seby . (201 8). Whither Gene Therapy? . (Version \n1.0) Vidyankur: Journal of Philosophical and Theological Studies.  \nJan-June 20 18 XX/1 www.doi.org/10.5281/zenodo. 4739971  54-64.  \nVidyankur XX/1 Ja nuary-June 20 18 55 embryonic cells for research, taking consent of the individual who \nis an embryo, clinical trials, the risk of misuse of this technology to \ncreate a superior human class, the affordability of such treatments \nand the regulations that will govern such therapies.  In this essay we \ntake a closer look at the various issues surrounding the use of stem \ncells and h ow we see it evolving in the future and if we could ever \nfully reap the benefits of this revolutionizing technology without \ncompromising our “humanness ”. \n \nKeywords:  Gene Therapy , Stem cell, Genetic Disease , Risk -\nBenefit Ratio , Ethical Issues of Gene Editin g \n \nIntroduction  \n“Many people say they are worried about the changes in our genetic \ninstructions. But these (genetic instructions) are merely a product of \nevolution, shaped so we can adapt to certain conditions which might \nno longer exist. We all know how imperfect we are. Why not \nbecome a little better apt to survive? ”, said James Watson in 1991 \n(cited in Gonçalves and Paiva, 2017).  Inheritance of hereditary \ncharacteristics ha s always been of great interest to mankind from the \nvery beginning. In fact, it was in the very process of evolution that \nwas anyways happening. Man too participated in the evolution \nprocess by selectively breeding and domesticating certain kinds of \nplants and animals while crossbreeding others. Although it was \nnever termed so, we wer e actually practising genetic engineering by \nkeeping the ones that were desirable and eventually eliminating the \nother. Genetic Engineering is thus not something new to \nhumankind.  \nIn the recent past , we have made further progress in understanding \nthe myst ery of genetics. It all began with the pea experiment by \nGregor Mendel paving the way to the theory of inheritance. Years \nlater, the chemical nature and the double -helical spiral structure of \nthe DNA was proposed by James Watson and Francis Crick in 1950.  \n56 S. Varghese : Gene Therapy  \n \n \n Further research helped identify the enzymes responsible for \nduplication, separation and reinsertion of genes at specific \nlocations along  with the DNA. This knowledge opened the door \nto a whole new field creating genetically modified bacteria and \nfungi to produce drugs, chemicals and antibodies (Gonçalves \nand Paiva, 2017). Today, we are at a point when we can treat or \neliminate a disease from the very root itself, the genes. This is \ncalled gene therapy.  \nBut is gene therapy a foolproof technology? There are many \nissues concerning the delivery system, efficiency and ethics \nwhen it comes to the actual use of the therapy. To top it all is \nthe question of cost and the potential benefits as compared to \nthe currently available therapies.  \nGene Therapy  \nBy 2003, the h uman genome project had mapped the human \ngenome. This was a breakthrough event in understanding \ngenetic information across populations. With such high -end \ntechniques at disposal, scientists are now able to identify the \ngenes responsible for any particular disease and hence find \nways to rectify it. This local modification using  correction of \nmutated genes or site -specific modifications is called gene \ntherapy. Though currently limited to research laboratories, it \npromises to treat diseases such as sickle cell  anaemia, \nhaemophilia, cancer etc. (Gonçalves and Paiva, 2017).   \nThe corrected genes are introduced into the affected cells either \nemploying  a viral vector or nanoparticles. Virus is most widely \nused because of their ability to infect and introduce geneti c \nmaterial into the cell. Viral vectors are however difficult to use \nin therapy because of our immune response that quickly \nneutralizes them. Therefore, nanotechnology using \nnanoparticles to deliver site -specific siRNA ha s proven to be a \nsuccessful alterna tive (Bulaklak and Gersbach, 2020).  \nVidyankur XX/1 Ja nuary-June 20 18 57 Depending on the target cell type, gene therapy can be divided into \nsomatic cell gene therapy and germline gene therapy. When \ntherapeutic genes are transferred to the somatic cells of the patients \nit is called somatic ge ne therapy and therefore the genes are \nrestricted to the patient and are non-transferable to subsequent \ngenerations. However, in germline therapy , the stem cells are \nmodified by introducing functional genes which are integrated into \nthe genome. These are p assed on to the next generations (Gonçalves \nand Paiva, 2017).   \nScientific Obstacles in Gene Therapy  \nViral vectors are often hindered by issues related to the patient ’s \nimmune response, the specificity of delivery and insertional \nmutagenesis. As previously  mentioned, the major obstacle in the use \nof viral vectors is our immune response, which  attacks any foreign \nagent and neutralizes it, even the ones that are meant to deliver \ngenes. In 1999, Jesse Gelsinger, a young man died during an \nexperimental gene the rapy due to his immune response to an \nadenoviral vector that was used in the study. The other concern of \nviral vector is its uptake by non -target cells in other organs that can \nlead to irreversible mutagenesis and other ill -effects (Hunt, 2008).  \nSince gene  therapy involves using a living drug it has a lot of \nintrinsic variability in its behaviour and predictability in terms of \nefficiency. This is further exaggerated by the gene modification \nprocess and the environment in which it interacts with the living \ncells. Therefore, it is difficult to get a clear understanding of the \ncellular level interaction of these therapies. Unlike the drugs that are \neliminated from the system through metabolic processes the effects \nof gene therapy are irreversible (Riva and Petr ini, 2019). Gene \ntherapy is still in its early stage of development and ongoing \nresearch has a lot to do to allay the fears of any unintended effect.  \nDespite these limitations, over the years many strategies to address \nthese issues have evolved and have be en tried in clinical trials. There \nis only an increase in the number of these trials. There have been  \n58 S. Varghese : Gene Therapy  \n \n \n instances of success such as the curing of three children from a \nfatal immunodeficiency disorder in 2000. Another study \nreported partial restoration of ey esight to four young adults who \nwere born blind, with the insertion of a single curative gene \n(Hunt, 2008). With new developments, gene therapy is going to \nrevolutionize how we look at a disease and the human person.  \nSome Significant Ethical Issues  \nAs we h ave seen, gene therapy is way different from \nconventional therapies based on drugs and other biologicals. \nThis science is in its nascent stage and so are our understanding \nof its many complexities. Since genes involve alteration at the \ngenetic level, it ra ises several other questions related to ethics \nnot only those related to the very use of stem cells but also the \nviability of such therapy with the heavy price tag vis-a-vis the \nrisks. We shall have a look at some of these issues.  \na. Use of Embryonic Stem  Cells  \nAdult stem cells are commonly used in cancer treatment even \nwithout the general awareness that we are using stem cells for \ntherapy. Stem cell research involving germ lines need human \nembryos for research and development of potential therapies. \nThis raises several moral and religious objections among \npeople. No doubt embryonic stem cells have created a lot of \ndebate not only regarding the consciousness in the embryo but \nalso the source of these cells. Conception is considered as the \nbeginning of life and therefore use of embryo s is morally \nobjectionable. Scientists counter this by saying that these \nembryos are outside a woman ’s body and is therefore not \nconceived in the usual sense. And to counter the argument of \nconsciousness, they point out the lack of a nervous system in \nthe embryo until implantation (Hunt, 2008).   \nThe use of in vitro  fertilization techniques has always been \nunacceptable practi se on religious grounds. Therefore, claiming  \nVidyankur XX/1 Ja nuary-June 20 18 59 that conception can take place in the womb alone and thus just ifying \nthe use of embryos seems very absurd. On the other hand, linking \nthe nervous system to consciousness and thus justifying that an \nembryo outside a woman ’s womb lacks consciousness to use in \nresearch is equally debatable. As a matter of fact, we are s till far \nfrom understanding what is consciousness. While the nervous \nsystem is an essential part of consciousness it is not the only thing \nthat defines it. On the other hand, they are conveniently forgetting \nthat the embryo has the potential to develop the  nervous system if \nimplanted in a womb. Thus, by depriving the embryo of what it \nrightfully deserves to grow into an individual, we cannot claim any \nmoral ground to carry out our research for the wellbeing of \nhumanity at the cost of anyone ’s life without e ven having an \ninformed consent of that “potential individual ” just because we \nclaim that it lacks consciousness.    \nCurrently , in the US, federal funds cannot be used for any research \nthat creates or destroys embryos. In addition, NIH does not fund any \nuse of gene editing in human embryos. While in some countries \ngenome -editing research on non -viable embryos is allowed, in \nothers  there are approved genome -editing research studies with \nviable embryos. Each of these will have its own moral and ethical \nconsiderations to be made (NHGRI, 2017).  \nb. Identification of the Genetic Disorder  \nWith the advances in the field of genetics, we can get a genetic map \nof potential diseases we are likely to be vulnerable to and the ones \nthat could be passed on to the next generations. Being a genetic \ndisorder, the disease is likely to dominate our famil y tree for many \ngenerations to come. Therefore, it would be highly desirable to fix \nthis defect and ensure a better life for future generations. All said \nand done it may not be a welcome strategy to alleviate human \nsusceptibility to genetic disorders.   \nThe first issue is the usage of the type of stem cell for therapy. As \ndiscussed earlier only corrections in the genes of germline will be  \n60 S. Varghese : Gene Therapy  \n \n \n inherited. But such an alteration seems to be highly debated \nbecause of the potential ly irreversible threat to other gen etic \ntraits that may or may not be associated with the gene that is \nbeing treated. Moreover, it is also known that a trait is expressed \nby the interaction of several chromosomal and non -\nchromosomal genetic material. This increases the risk of \nuntoward even ts that put the future progenies at risk. Thus, \nsomatic cell therapy may seem a relatively easier option for the \ntime being. But before that , we need to identify the disorder at \na very early stage of embryonic development.  \nPrenatal screening for genetic di sorders is a common practice \ntoday. The correct and timely identification of such diseases \nplays a decisive role for the parents to decide the next steps to \nbe taken. Accordingly, the parents would choose either for \ntermination of pregnancy or let the chil d be born with the \nanomaly. The decision for the treatment would also depend on \nthe culture of the parents. In an Indian context , a male child is \npreferred over a female one, and therefore a female neonate is \nlikely to be terminated despite the possibility  of a cure.  \nIn prenatal screening we also need to look at the individual ’s \nautonomy and the cost involved. In this case the individual ’s \nautonomy is out of question as we are talking of the embryo that \ndoesn ’t have the capability to understand, reflect or  reason to \nmake an informed decision. It may be argued that the parents \ntake the decision on behalf of the “unconsulted ” foetus. On the \nother hand, is the moral question of passing on a genetic disease \nto the future generations even when a cure is availabl e on the \ngrounds of autonomy and consent of the individual (D and GA \n2014).  \nc. Clinical Trials  \nOnce a potential therapy is proven to be effective in the \nlaboratory it needs to be taken the next level of first in human  \nVidyankur XX/1 Ja nuary-June 20 18 61 trials. This poses ethical issues suc h as difficulty in evaluating \npreclinical research; difficulty in assessing the risk -to-benefit ratio; \nconceptualisation and estimation of patient benefits and/or social \nbenefits; application of the principle of justice; criteria for \ninclusion/exclusion of  participants; the process of information and \nconsent; and risk of therapeutic misconception.  \nThe general practice is to have basic laboratory and animal research \nat the preclinical stage before undertaking any human trials. The \nNuremberg Code states that  “The experiment should be so designed \nand based on the results of animal experimentation and knowledge \nof the natural history of the disease or other problem under study \nthat the anticipated results will justify the performance of the \nexperiment ” (Nurembe rg Military Tribunals, 1948 –1953). The \nDeclaration of Helsinki also states a similar requirement (WMA, \n1964 –2013, article 18) and provides that: “Every medical research \nstudy involving human subjects must be preceded by careful \nassessment of predictable ri sks and burdens to the individuals and \ncommunities involved in the research in comparison with \nforeseeable benefits to them and to other individuals or communities \naffected by the condition under investigation ”.  \nAs is known, every experimental study by its nature involves \nuncertainty and risks especially, when it comes to first -time human \nresearch. Therefore, a risk -benefit study is required before any \nclinical trial, to save the interests of the subjects in the trial. \nHowever, we still lack a quantitative  technique to weigh the risk to \nbenefit ratio. The three conditions that must be met include (i) the \npotential risks to individual subjects must be minimised; (ii) the \npotential benefits to individual subjects must be enhanced; and (iii) \nthe potential bene fits to individual subjects and society must be \nproportionate to or outweigh the risks. Thus, even if there are no \npotential benefits to the subject, the potential risks should be \nminimal to justify a potential benefit to society in the long run.   \n62 S. Varghese : Gene Therapy  \n \n \n Coming to  the point of subject selection for clinical trials, \nseriously ill patients who have exhausted the therapeutic \npossibilities are considered for gene therapy. Even in such \ncases, all risks may not be justified given that a viable \nalternative therapy is unav ailable. Moreover, the patient must \nbe aware of the uncertain nature of such therapies and the \npotential risks. Stem cell -based approaches are beginning to be \ntested in clinical trials on neurodegenerative disorders. These \ncould also include first -in-human  intracerebral transplantation \nof cells derived from human embryonic stem cells and inducible \npluripotent cells. This involves inserting the cells into the brains \nof the patients and thus exposing non -target cells to potential \nrisks and permanent impairmen t of brain functions. On the other \nhand, it is not possible to get informed consent from a \ncognitively impaired patient (Riva and Petrini, 2019).  \nd. Safety  \nThere is always a likelihood of off -target effects or mosaicism \n(when some cells carry the edit but others do not), which brings \nthe safety of patients as the primary concern. Some researchers \nare of the opinion that there may never be a time when genome \nediting in embryos will offer a benefit greater than that of \nexisting technologies. Once proven succe ssful, there is a \nlikelihood that genome editing would also be used for non -\ntherapeutic purposes such as the creation of individuals with \ncertain behaviours or characters. There is also the likelihood of \ncreating new human species with superior qualities ( NHGRI, \n2017).    \nVidyankur XX/1 Ja nuary-June 20 18 63 e.  Justice and Equity  \nGenome editing, as has been known, is an \nexpensive affair and nothing less than an \nunrealistic dream for ordinary people, let \nalone the poorer nations. Thus, even \namong the wealthy a few are going to \navail such a therapy. This would create an \nall-new class  of genetically engineered \nwho will claim superiority, not only based \non wealth but also in the very essence of \ntheir genetic being (NHGRI, 2017).  \nConclusion  \nHaving looked at various aspects of gene therapy and the obstacles in this \nemerging field we sti ll have some unanswered questions. Firstly, is it even \nethical to modify the human genome? What is the definition of a disease \nand who decides? Wouldn ’t an undesirable behaviour be termed as the \ndisease for economical exploitation? How are we to address th e issues of \neventual mishaps in gene editing? Is it really a boon when we think of \ncreating a whole new superior human race? Who will take the onus of \nensuring that it is used ethically without any ill intent to dominate or \nsubjugate the other?  \nAny scient ific discovery is to be welcomed. But when it comes to gene \ntherapy or editing, we are putting at risk our human liberty, autonomy and \nthe future of the entire human race. In the gene therapy ethics debate, \nscience provides us with the facts. The facts are  necessary for us to make \ninformed decisions. But science cannot tell us what our choices ought to \nbe (van Bogaert, and Ogunbanjo, 2014). Looking at the cost, ethical and \nscientific issues involved in gene therapy, currently the risks outweigh the \npotentia l benefits. Moreover, like nuclear energy, gene therapy too can be \npotentially misused against humanity.   \nReferences  \nBulaklak, Karen, and Charles A. Gersbach. (2020 ). The once and future \ngene therapy. 16 November. https: //www.nature . com/  \narticles/s41467 -020-19505 -2. Is it even ethical to \nmodify the human \ngenome? What is the \ndefinition of a disease \nand who decides? \nWouldn ’t an \nundesirable behaviour \nbe termed as disease \nfor economical \nexploitation?   \n64 S. Varghese : Gene Therapy  \n \n \n van Bogaert, D, Knapp and Ogunbanjo GA. 2014. The Human \nGenome and Gene “Therapy ”:. 15 August. https://www . \ntandfonline.com/doi/pdf/10.1080/20786204.2010.10873929.  \nGonçalves, Giulliana Augusta Rangel, and Raquel de Melo Alves \nPaiva. (2017). Gene therapy: advances, challenges and \nperspectives. July. https://www.ncbi . nlm.nih.gov/  pmc/  \narticles/  PMC5823056/.  \nHunt, Sonia Y. (2008). Controversies in  treatment approaches: Gene \ntherapy, IVF, stem cells, and pharmacogenomics. . https://www. \nnature.com/scitable/ topicpage/ controversies -in-treatment - \napproaches -gene -therapy -ivf-792/.  \nNHGRI, (2017). What are the Ethical Concerns of Genome E diting? \n3 August. Natio nal Human Genome Research Insti tute. \nhttps://www . genome.gov  /about - genomics/  policy -issues  \n/Genome  -Editing/ethical -concerns.  \nRiva, Luciana, and Carlo Petrini. (2019). A few ethical issues in \ntranslation al research for gene and cell therapy. 28 November. \nhttps://translational -medicine .biomed central . com/articles/  \n10.1186  /s 12967 - 019-02154 -5. \nSeby Varghese SJ  is a Jesuit scholastic belonging \nto the Bombay Province. He is a student of \nphilosophy at Jnana Deepa, Pune. He holds a \nMaster ’s degree in Pharmaceutical Sciences \n(Pharmaceutics) from Mumbai University. He is an \navid reader of subjects pertaining to scien ce and \nreligion. Email: Sebysebastian1@gmail.com \nORCID : 0000 -0003 -2793 -4826  \n \nRecd: Jan 12, 2018    Accepted: Feb 4, 2021 Words : 3010  \nby the authors. This is an open -access \narticle distributed under the terms and \nconditions of the Creative Commons \nAttribution (CC BY) license. \n(http://creativecommons .org/ \nlicenses/ by /4.0/)"
    },
    {
        "title": "Expressive and Efficient Retrieval of Symbolic Musical Data.",
        "author": [
            "Michael Droettboom"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417741",
        "url": "https://doi.org/10.5281/zenodo.1417741",
        "ee": "https://zenodo.org/records/1417741/files/Droettboom01.pdf",
        "abstract": "The ideal content-based musical search engine for large corpora must be both expressive enough to meet the needs of a diverse user base and efficient enough to perform queries in a reasonable amount of time. In this paper, we present such a system, based on an existing advanced natural language search engine. In our design, musically meaningful searching is simply a special case of more general search techniques. This approach has allowed us to create an extremely powerful and fast search engine with minimal effort.",
        "zenodo_id": 1417741,
        "dblp_key": "conf/ismir/Droettboom01",
        "keywords": [
            "musically meaningful searching",
            "advanced natural language search engine",
            "diverse user base",
            "reasonable amount of time",
            "powerful and fast search engine",
            "minimal effort",
            "corpora",
            "expressive enough",
            "efficient enough",
            "special case"
        ],
        "content": "Expressive and efﬁcient retrieval of symbolic musical data\nMichael Droettboom, Ichiro Fujinaga,\nKarl MacMillan\nThe Peabody Institute\nThe Johns Hopkins University\nfmdboom,ich,karlmac g@peabody.jhu.eduMark Patton, James Warner,\nG. Sayeed Choudhury, Tim DiLauro\nDigital Knowledge Center\nMilton S. EisenhowerLibrary\nThe Johns Hopkins University\nfsayeed,timmo,mpatton,jwarner g@jhu.edu\nABSTRACT\nTheidealcontent-basedmusicalsearchengineforlargecor-\npora must be both expressive enough to meet the needs of\na diverse user base and efﬁcient enough to perform queries\nin a reasonable amount of time. In this paper, we present\nsuch a system, based on an existing advanced natural lan-\nguage search engine. In our design, musically meaningful\nsearching is simply a special case of more general search\ntechniques. This approach has allowed us to create an ex-\ntremelypowerfulandfastsearchenginewithminimaleffort.\n1. INTRODUCTION\nThispaperdescribesasystemformusicsearchingthatisex-\npressive enough to perform both simple and sophisticated\nsearches that meet a broad range of user needs. It is also\nefﬁcientenoughtosearchthroughalargecorpusinareason-\nable amount of time. The music search system was created\nby extending an existing advanced natural language search\nengine with simple ﬁltersand user-interface elements.\nThis paper will describe the search engine in the context of\nour larger sheet music digitization project, and relate it to\nothermusicalsearchenginesalreadyavailableforuseonthe\nweb. Then,thecapabilitiesofthenon-music-speciﬁccoreof\nthe search engine will be described, followed by the exten-\nsions necessary to adaptit to music.\n2. BACKGROUND\nTheLesterS.LevyCollectionofSheetMusicrepresentsone\nof the largest collections of sheet music available online.\nThe Collection, part of the Special Collections of the Mil-\nton S. Eisenhower Library at The Johns Hopkins University,\ncomprises nearly 30,000 pieces of music (Choudhury et al.\n2000). It provides a rich, multi-facetted view of life in late\n19th and early 20th century America. Scholars from vari-\nous disciplines have used the Collection for both research\nand teaching. All works in the public domain are currently\navailable online as JPEG images. The user can browse the\ncollection by category or search based on metadata, such as\nauthor, title, publisher, and date. Musical searches, such as\nﬁndingaparticularmelodicorrhythmicpattern,willsoonbepossible once the collection has been converted to symbolic\nmusical data.\nTo convert this data, an optical music recognition (OMR)\nsystem is being developed (Choudhury et al. 2001). We\nchose GUIDO as the target representation language due\nto its simplicity and extensibility (Hoos and Hamel 1997).\nHaving music in a symbolic format opens the collection to\nsound generation, musicological analysis and, the topic of\nthe present paper, musicalsearching.\n3. PRIOR ART\nNone of the available musical search engines we evaluated\nmet the needs of the diverse user base of the collection, or\ncould handle the large quantity of data in the complete Levy\ncollection. In particular, we evaluated two projects in detail:\nThemeﬁnder (Huron et al. 2001) and MELDEX (McNab et\nal. 1997).\n3.1 Themeﬁnder\nThemeﬁnder’s goal is to retrieve works by their important\nthemes. These themes are manually determined ahead of\ntime and placed in an incipitdatabase.\nOne can query the database using ﬁve different kinds of\nsearch queries: pitch, interval, scale degree, gross contour,\nand reﬁned contour. These ﬁve categories served as the in-\nspiration for a subset of our basic query types. The user can\nquery within an arbitrary subset of these categories and then\nintersect the results. However, Themeﬁnder does not allow\nthe user to combine these query types within a single query\nin arbitrary ways. For instance, a user may know the begin-\nningofamelodicphrase,whiletheendingismoreuncertain.\nTherefore,theusermaywanttospecifyexactintervalsatthe\nbeginning and use gross contours or wild-cards at the end.\nUnfortunately, in Themeﬁnder, the user must have the same\nlevel of certainty about all of the notes in the query. Un-\nfortunately, this is not consistent with how one remembers\nmelodies (McNab et al. 2000).\nInaddition,Themeﬁnderdoesnothaveanotionofrhythmic\nsearching. While its invariance to rhythm can be an asset, it\ncan also be cumbersome when it provides too many irrele-\nvant matches. Figure 1 shows the results of a query where\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or\ndistributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page.one result is more relevant than the other. Such queries may\nreturnfewerfalsematchesiftheycouldincluderhythmicin-\nformation.\nThe searches themselves are executed in Themeﬁnder us-\ning a brute-force method. The entire database is linearly\nsearched for the given search query string. While this is\nacceptable for the 18,000 incipits in Themeﬁnder’s largest\ndatabase, it may not scale well for searching across a full-\ntext database such as theLevy collection.\nBeethoven, Ludwig Van. Quartet in E Minor, Op. 59, No. 2\n“Rasoumowsky”, 4th Movement.\nBeethoven, Ludwig Van. Sonata No. 4, in A Minor, Op. 23, Violin\nand Pianoforte, 1st Movement.\nFigure 1: These two incipits start with the identical set\nof pitches, [cded ], but with different rhythmic content.\nWith better rhythmic speciﬁcity, irrelevant results could\nbe eliminated. (http://www.themefinder.org/ )\n3.2 MELDEX\nThesimpletext-basedquerystringsinThemeﬁnderareeasy\nto learn and use by those with moderate musical training.\nMELDEX, however, has a more natural interface for non-\nmusicians. The user sings a melody using a syllable with a\nstrongattacksuchas“tah.” Thepitchesofthemelodyarede-\ntermined using pitch-tracking, and the rhythm is quantized.\nTheresultsareusedasthesearchquery. Thequeryisapprox-\nimately matched to melodies in the database using a fast-\nmatchingalgorithmrelatedtodynamicprogramming. While\nthis approach is highly effective for non-musicians and sim-\nple queries, it is limiting to those wanting more ﬁne-grained\ncontrol.\n4. CAPABILITIES\nOurmusicalsearchenginesupportsbothmelodicandrhyth-\nmic searches. Search queries can also include the notion of\nsimultanaeity. That is, events can be constrained to occur\nat the same time as other events. The search engine, as de-\nscribed here, is limited to standard-practice Western music,\nthoughmodiﬁcationscouldbemadetosupportothermusical\ntraditions.\n4.1 Extensibility\nOther types of musical searching beyond these core capa-\nbilities require additional layers of musical knowledge to be\nbuilt on top of the search engine. The general design of the\nsearch engine encourages such extensibility. Any analytical\ndata that can be derived from the score data can be gener-\nated ofﬂine (ahead of time) and later used as search criteria.This data can be generated by new custom tools or existing\nanalysis tools such as theHumdrum toolkit (Huron 1999).\nForexample,thesearchenginecouldbeextendedtosupport\nharmonic searches with respect to harmonic function. West-\nern tonal harmonic theory is ambiguous, making it difﬁcult\ntoobjectivelyinterpretandlabelharmonies. Thisisalargely\nunsolved problem that is not the subject of our present re-\nsearch. However, assuming an acceptable solution to these\nissues could be found, labeling of harmonic function could\nbe implemented as an inputﬁlter.\nAlso, the core search engine does not include any notion\nof melodic similarity. This is an open problem strongly\ntied to subjective matters of human perception (Hewlett and\nSelfridge-Field 1998). It is possible for a specialized front-\nend to include notions of melodic similarity by generating\nspecializedsearchqueries. Thesearchquerylanguageofthe\ncore search engine is expressive enough that these advanced\nfeatures could be added withoutmodifying the core itself.\n4.2 Meeting diverse user requirements\nWe deﬁne the users of our musical search engine as any-\none who wants to access the collection in a musical way. Of\ncourse,theneedsofdifferentusersaregreatlyvaried. Anon-\nmusician may want to hum into a microphone to retrieve a\nparticularmelody. Acopyrightlawyermaywanttotrackthe\noriginsofaparticularmelody,evenmelodiesthataremerely\nsimilar. Amusicologistmaywanttodeterminethefrequency\nof particular melodic or rhythmic events. To meet these di-\nverseneeds,itisnecessarytoprovidedifferentinterfacesfor\ndifferent users. The set of interfaces is arbitrary and can be\nextendedasnewtypesofusersareidentiﬁed. Itmayinclude\ngraphicalapplications,web-basedformsandapplets,ortext-\nbased query languages. Audio interfaces, with pitch- and\nrhythm-trackingmayalsobeincluded. Thepurposeofthese\ninterfaces is to translate a set of user-friendly commands or\ninteractions into a query string accepted by the search en-\ngine. The details of that query can be hidden from the end-\nuser and therefore can be arbitrarilycomplex.\nAt present, we have focused our attention on the core search\nengine itself. In the second phase of the search engine\nproject, the user interfaces will be developed in collabora-\ntion with a usability specialist.\n5. THE CORE SEARCH ENGINE\nThe core search engine in our system was originally devel-\noped for text-based retrieval of scores based on their meta-\ndata and full-text lyrics. Its overall design was inspired by\nrecent developments in the ﬁeld of natural-language search-\ning (DiLauro et al. 2001). These features allow the user to\nperform search queries using the embedded context in natu-\nral languages, such as parts of speech, rhyming scheme, and\nscansion. While not originally intended for musical search-\ning,itwassoondiscoveredthatthecorewasverywellsuited\nfor searching across symbolicmusical data.The core itself did not need to be modiﬁed to support music\nsearching. Instead, specialized ﬁlters and front-ends were\nadded to adapt it to the music domain. In the ingestion\nstage, the data is ﬁltered to store it in the appropriate in-\ndicesandpartitions(seeSection6). Whensearching,special\nuser interfaces handle the details of generating search query\nstringsandﬁlteringanddisplayingtheresultingdata. Figure\n2showshowtheindividualpartsofthesystemﬁttogetherto\ningest and query the data.\nImages\nSymbolic Music\n(GUIDO)OMR\nFiltering /\nConversion\nPrimary IndexSecondary\nIndices\n...\nCORE SEARCH ENGINEINGESTION QUERY\nUser Interfaces\nQuery\ngenerationDisplay\nFilter\nPartitions\nFigure2: Workﬂowdiagramofthemusicalsearchengine.\n5.1 Inverted lists\nManysearchengines,includingours,arebuiltontheconcept\nof an inverted list. For a complete discussion of inverted list\nsearch engines, see Wittenet al. (1999).\nSequential data, such as English prose or melodic data, is\nstored on disk as a sequence of atoms. In the case of En-\nglish, the atom is the word and the sequence is simply the\nordered words as they appear in sentences and paragraphs.\nTake for example the following sentence:\nTo be , or not to be , that is\nthe question .\nNote that both words and punctuation are treated as indivis-\nible atoms. To search for a particular atom in this string, a\ncomputerprogramwouldneedtoexamineallthirteenatoms\nand compare it with a query atom. To increase searching ef-\nﬁciency,aninvertedlistsearchenginewouldstorethisstring\ninternally as:,\u0000!f3, 8g\n.\u0000!f13g\nbe\u0000!f2, 7g\nis\u0000!f10g\nnot\u0000!f5g\nor\u0000!f4g\nquestion\u0000!f12g\nthat\u0000!f9g\nthe\u0000!f11g\nto\u0000!f1, 6g\nHere, each atom in the string is stored with a list of num-\nbers indicating the atom’s ordinal location within the string.\nThe set of words in the index is called the vocabulary of the\nindex. To search for a particular atom using the index, the\nprogram needs only to ﬁnd that word in the vocabulary and\nitcaneasilyobtainalistofindices(orpointers)towherethat\natom is located within the string. Since the vocabulary can\nbe sorted, the lookup can be made faster using hashing or a\nbinary search.\nInverted lists perform extremely well when the size of the\nvocabulary is small relative to the size of the corpus. In the\ncase of English, of course, the vocabulary is much smaller\nrelative to the size of all the works written in that language.\nThispropertyalsoallowsustoimprovetheefﬁciencyofmu-\nsical searching as we willsee below.\n6. THE MUSICAL SEARCH ENGINE\nThe musical search capabilities are supported by three main\nfeatures of the core searchengine:\n1.Secondary indices allow the amount of speciﬁcity to\nvary with each token.\n2.Partitions allow search queries to be performed upon\nspeciﬁc discontiguous partsof the corpus.\n3.Regular expressions allow advanced pattern match-\ning.\nFigure 3: A measure of music, from the Levy collection,\nused as an example throughout this section. (Guion, D.\nW., arr. 1930. “Home on the range.” New York: G.\nSchirmer.)\n6.1 Secondary indices\nInthecaseofmusic,thesearchableatomisnottheword,but\nthemusicalevent. Eventsincludeanythingthatoccursinthescore, such as notes, rests, clefs, and barlines. Each of these\nevents, of course, can have many properties associated with\nit. For instance, the note b[at the beginning of the fragment\nin Figure 3 has the followingproperties:\n\u000fPitch name: b\n\u000fAccidental: [\n\u000fOctave:-1 (ﬁrst octave below middle- c)\n\u000fTwelve-tone pitch: 10 (10thsemitone above c)\n\u000fBase-40 pitch: -3 (see Hewlett 1992)\n\u000fDuration: eighth note\n\u000fInterval to next note: perfect 4th\n\u000fContour (direction) to nextnote: up\n\u000fScale degree: so(5thscale degree in E[major)\n\u000fLyric syllable: “sel–”\n\u000fMetric position: Beat 1 in a6\n8measure\nAll of these properties are self-evident, with the exception\nof base-40 pitch, which is a numeric pitch representation\nwhere the intervals are invariant under transposition while\nmaintaining enharmonic spelling (Hewlett 1992). Note also,\nweuseGUIDO-styleoctavenumbers,wheretheoctavecon-\ntaining middle- cis zero, as opposed to ISO-standard octave\nnumbers.\nTheconceptofsecondaryindicesallowstheindividualprop-\nertiesofeachatomtobeindexedindependentlyofanyother\nproperties. Thisallowssearchqueriestohavearbitrarylevels\nof speciﬁcity in each event. The set of properties can be ex-\ntendedtoincludeanykindsofdatathatcanbeextractedfrom\nthe musical source. For example, if the harmonic function\nof chords could be determined unambiguously, a secondary\nindex containing chord names in Roman numeral notation\ncould be added. In our design, we use secondary indices to\nhandlethepropertiesofeventsthatchangefromnotetonote.\nContinuous properties of events, that are carried from one\nevent to the next, such as clefs, time signatures, and key sig-\nnatures, are handled using partitions, explained below (see\nSection 6.2).\n6.1.1 Ingestion of secondaryindices\nDuring the ingestion phase, the source GUIDO data is ﬁrst\nconverted to an interim format where all of each event’s\nproperties are fully speciﬁed. For example, the GUIDO rep-\nresentation of Figure 3 isas follows:\n[ \\clef<\"treble\"> \\key<-3> \\time<6/8>\n\\lyric<\"sel-\"> b&-1*/8.\n\\lyric<\"dom\"> e&*0\n\\lyric<\"is\"> f\n\\lyric<\"heard\"> g/4\n\\lyric<\"A\"> e&/16\n\\lyric<\"dis-\"> d\n]Each event is then extended so it is fully speciﬁed. In this\nformat, each note event isa tuple of properties:\npitch-name, accidental, octave, twelve-tone-\npitch,base-40-pitch,duration,interval,contour,\nscale-degree, lyric-syllable, metric-position\nFigure 4 shows the example in fully-speciﬁed symbolic rep-\nresentation.\nEach one of these ﬁelds is used to index the database in a\nparticular secondary corpus. For example, if the notes in the\nexample were labeled 1 through 6, the data in the secondary\nindices may look somethinglike:\n\u000fPitch name\na\u0000!;\nb\u0000!f1g\nc\u0000!;\nd\u0000!f6g\ne\u0000!f2, 5g\nf\u0000!f3g\ng\u0000!f4g\n\u000fAccidentals\nn(\\)\u0000!f3, 4, 6g\n&([)\u0000!f1, 2, 5g\n\u000fOctave\n-1(octave below middle- c)\u0000!f1g\n0(octave above middle- c)\u0000!f2, 3, 4, 5, 6g\n\u000fDuration\n1/4(quarter note)\u0000!f4g\n1/8(eighth note)\u0000!f1, 2, 3g\n1/16(sixteenth note)\u0000!f5, 6g\n6.1.2 Searching using secondaryindices\nThe search query itself is simply a series of events. Each\nevent can be indicated as speciﬁcally or as generally as the\nend user (as represented by a user interface) desires. For ex-\nample, the following query would match any melodic frag-\nment that begins on a b[eighth note, has a sequence of 3\nascending notes, ending ona g:\nb,&,1/8 / / / g\nTo execute a search query using secondary indices, the\nsearchenginelooksupeach“parameter”intheircorrespond-\ning secondary indices, and retrieves tokens in the secondary\nindex. Thesetokensarethenlookedupintheprimaryindex,\nreturningalistofpositions. Theselistsareintersectedtoﬁnd\nthe common elements. This list of locations is then ﬁltered\nto include only those events that are sequenced according to\nthe search query.\n6.1.3 Supported user interfaces\nThisdesignsupportsabroadrangeofuserinterfaces. Atext-\nbased user interface may allow a user to be very speciﬁc in[ \\clef<\"treble\"> \\key<-3> \\time<6/8>\nb, &, -1, 10, -3, 1/8, P4, /, so, \"sel-\", 0\ne, &, 0, 3, 14, 1/8, M2, /, do, \"dom\", 1/8\nf, n, 0, 5, 20, 1/8, M2, /, re, \"is\", 1/4\ng, n, 0, 7, 25, 1/4, M3, \\, mi, \"heard\", 3/8\ne, &, 0, 3, 15, 1/16, m2, \\, do, \"A\", 5/8\nd, n, 0, 2, 9, 1/16, M2, \\, ti, \"dis-\", 11/16\n]\nFigure 4: Fully speciﬁed symbolicrepresentation of the example in Figure 3 .\nthe query, and then incrementally remove layers of speci-\nﬁcity until the desired match is retrieved. An audio-based\nuser interface could be more or less speciﬁc depending on\nthe pitch tracker’s conﬁdencein each event.\n6.1.4 Efﬁciency of secondaryindices\nOneoftheefﬁciencyproblemswiththisapproachisthatthe\nvocabularies of the individual secondary indices tend to be\nquite small, and thus the index lists for each atom are very\nlarge. For instance, the “pitch name” secondary index has\nonly seven atoms in its vocabulary ( a-g). “Accidentals”\nis even smaller:f[[,[,\\,],\u0002g. Therefore, a search for a\nb[must intersect two very large lists: the list of all b’s and\nthe list of all ﬂats. However, the search engine can combine\nthese secondary indices in any desired combination off-line.\nFor example, given the “pitch name” and “accidental” in-\ndices, the search engine can automatically generate a hybrid\nindex in which the vocabulary is all possible combinations\nof pitch names and accidentals. The secondary indices can\nbe automatically combined in all possible combinations, to\nan arbitrary order.\n6.2 Partitions\nPartitioningcanbeusedtorestrictasearchquerytoapartic-\nularpartofthecorpus. Eachpartitionisadescriptionofhow\nto divide the corpus into discontiguous, non-overlapping re-\ngions. More speciﬁcally, each partition is a ﬁle containing a\nlist of regions. Each region within a partition is named and\nhas a list of its start andstop positions.\nIn our music search engine, the metadata is used to parti-\ntion the corpus into regions. For example, all works by a\ngivencomposerwouldmakeupadiscontiguousregioninthe\n“composer” partition. Partitions exist for all types of meta-\ndatainthecollection,includingdate,publisher,geographical\nlocation, etc.\nInaddition,wehaveextendedpartitioningtoincludemusical\nelementsderiveddirectlyfromtheGUIDOdata. Regionsare\ngenerated from key signatures, clefs, time signatures, mea-\nsures,movements,repeats,etc. Thisallowsforsearchingfor\naparticularmelodyinaparticularkeyandclef,forexample.\n6.2.1 Ingestion of partitiondata\nWhen a new work is added to the corpus, the data is par-\ntitioned automatically. First, the metadata regions, such astitle, composer, and date, are set to include the entire piece.\nAs the piece is scanned, continuous musical elements, such\nasclef,keysignature,andtimesignature,areregionedonthe\nﬂy. Therefore, when the ingestion ﬁlter sees a “treble clef”\ntoken, all further events are added to the “treble clef” region\nuntil another clef token is encountered. Lastly, events are\nadded to the moment regionson an event-by-event basis.\nFor the example in Figure 3, again assuming the notes are\nnumbered 1 through 6, the partitions may look something\nlike:\n\u000fTitle partition\n“Home on the range” \u0000![1, 6]\n\u000fClef partition\nTreble clef\u0000![1, 6]\n\u000fTime signature partition\n6\n8\u0000![1, 6]\n6.2.2 Searching using partitions\nExtending the example in Section 6.1.2, the user may wish\nto limit the search to thekey signature of E[-major:\n( b,&,1/8 / / / g ) @ key:\"E& major\"\nHere the non-partitioned search query is performed as de-\nscribed above, and then the results are intersected with the\nresults of the partition lookup. Since in our case, the entire\nrange of notes [1, 6] is in the key signature of E[-major, the\nquery will retrieve the examplein Figure 3.\n6.2.3 Searching with simultanaeityusing partitions\nScores are also partitioned at the most atomic level by “mo-\nments.” A moment is deﬁned as a point in time when any\nevent begins or ends in any part. Moments almost always\ncontain multiple events, and events can belong to multiple\nmoments (e.g. when a half note is carried over two quarter\nnotes in another part). Each moment within a score is given\na unique numeric identiﬁer, and all events active at a given\npoint are included in a moment region. In this way, one can\nsearch for simultaneous polyphonicevents very efﬁciently.\nTo explain this further, Figure 5 shows the example measure\nwithitsassignedmomentnumbers. Eacheventisassignedto\none or more moments so that it can be determined which, if\nany,oftheeventsareactiveatthesametime. Thesemomentnumbers are used to create regions. For example, the dotted\nhalfnoteinthelefthandofthepianopartwouldbeassigned\nto all seven moment regions.\nTo perform searches involving simultanaeity, the query for\neachpartisperformedseparately,andthentheresultsarein-\ntersectedbasedontheirmoments. Onlythequeryresultsthat\noccuratthesametime(existinginthesamemomentregions)\nwill be presented to the user.\nFigure 5: The example measure of music showing mo-\nment numbers.\n6.3 Regular expressions\nThe core search engine supports a full complement of\nPOSIX-compliant regular expressions. Regular expressions,\na large topic beyond the scope of this paper, are primar-\nily used for pattern-matching within a search string (Friedl\n1997).\nMany users ﬁnd regular expressions difﬁcult and cumber-\nsome ways to express searches. However, it is our intent\nthatmostofthesedetailswillbehiddenfromtheuserbyap-\npropriateinterfaces. Forexample,regularexpressionswould\nbe very useful for an interface that allowed searching by\nmelodicsimilarity. Whatisimportanttoourpresentresearch\nis that regular expressions are supported in the core search\nengine, leaving such possibilitiesopen.\n7. CONCLUSION\nBased on existing advanced natural-language search tech-\nniques, we have developed an expressive and efﬁcient mu-\nsical search engine. Its special capabilities include: sec-\nondary indices for gradiated speciﬁcity, partitions for selec-tive scope and simultanaeity, and regular expressions for ex-\npressive pattern matching. This allows users with different\nsearchneedstoaccessthedatabaseinpowerfulandefﬁcient\nways.\n8. ACKNOWLEDGMENTS\nThe second phase of the Levy Project is funded through\nthe NSF’s DLI-2 initiative (Award #9817430),an IMLSNa-\ntional Leadership Grant, and support from the Levy Family.\nWe would like to thank David Yarowsky of the Department\nof Computer Science for motivation and discussion.\n9. REFERENCES\nChoudhury, S., T. DiLauro, M. Droettboom, I. Fujinaga, B.\nHarrington, and K. MacMillan. 2000. Optical music recog-\nnition within a large-scale digitization project. ISMIR 2000\nConference .\nChoudhury, G. S., T. DiLauro, M. Droettboom, I. Fujinaga,\nand K. MacMillan. 2001. Strike up the score: Deriving\nsearchableandplayabledigitalformatsfromsheetmusic. D-\nLib Magazine : 7(2).\nDiLauro, T., G. S. Choudhury, M. Patton, J. W. Warner, and\nE. W. Brown. 2001. Automated name authority control and\nenhancedsearchingintheLevycollection. D-LibMagazine :\n7(4).\nFriedl, J. E. F. 1997. Mastering regular expressions. Se-\nbastopol, CA: O’Reilly.\nHewlett, W. B. 1992. A base-40 number-line representation\nof musical pitch notation. Musikometrika 4: 1–14.\nHewlett, W. B., and E. Selfridge-Field. 1998. Melodic sim-\nilarity: Concepts, procedures and applications. Cambridge,\nMA: MIT Press.\nHoos, H. H., and K. Hamel. 1997. GUIDO music nota-\ntion: Speciﬁcation Part I, Basic GUIDO. Technical Report\nTI 20/97, Technische Universit ¨at Darmstadt.\nHuron, D., W. Hewlett, E. Selfridge-Field, et al. 2001. How\nThemeﬁnder works. http://www.themefinder.org\nHuron, D. 1999. Music research using Humdrum: A user’s\nguide.Menlo Park, CA: Center for Computer Assisted Re-\nsearch in the Humanities.\nMcNab, R. J., L. A. Smith, D. Bainbridge, and I. H. Witten.\n1997. The New Zealand Digital Library MELody inDEX.\nD-Lib Magazine : 3(5).\nMcNab, R. J., L. A. Smith, I. H. Witten, and C. L. Hender-\nson. 2000. Tune retrieval in the multimedia library. Multi-\nmedia Tools and Applications 10(2/3): 113–32.\nWitten, I., A. Moffat, and T. Bell. 1999. Managing giga-\nbytes. 2nd Ed. San Francisco: MorganKaufmann."
    },
    {
        "title": "Melody Spotting Using Hidden Markov Models.",
        "author": [
            "Adriane Durey"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415680",
        "url": "https://doi.org/10.5281/zenodo.1415680",
        "ee": "https://zenodo.org/records/1415680/files/Durey01.pdf",
        "abstract": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007 \u000e\r\u0010\u000f\u0012\u0011\u0013\u0005\u0015\u0014\u0017\u0016 \u0011\u0013\u0005\u0006\u0018\u0010\u000f\u0012\u0019 \u000f\u0012\u001a\u001b\u000f\u0012\u001c\f\u0005\u001d\u0018\u001e\t\u001f\u0016 \u0010\u000f\u0012\u0005\u001d\u0001\u0004\u0016 !\u0010\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007 $ \u0011\u0013\u0005\u001d\t\u001f\u0016 \u0011%\u0018'& \u000f\u0012()\u0019*\u0001\u001f+,\u000f\u0012\u001a\u0002-.\u0005/\t#\u00160\u0014\u0017\u0005\u001d\u00011\u000f\"()\t\u001f\u0011\u0013\u0005\u001d\u00072\u0001\u001b\u000f\"()\u0019*$435()\u0005\u001d\t\u001f\u0005\u001d\u0001\u001b\u00016\u0007/\u0011%37\u001a8\u0016:9)\u0007\u0007?\t#\u00160\u0014\u0017 @\r)\u001a8\u0005\u001d\u0011A\u000f\"(B\u00016\u00162\u0011%\u001a6\u000f\"()\u0019:\u001a69)\u0011\u0013\u0016*\r)\u0019*9C\u001a\u001b9)\u0005D\u000f\u0012('& !E\u0016 \u0011#\u0014\u0017\u0007/\u001a\u001b\u000f\u0012\u0016*(F\u001a\u001b9)\u00072\u001aG\u000f\u0012\u001aG\u00016\u001a6\u0016 \u0011\u0013\u0005\u001d\u0001\u001fHAIJ(F\u001a69\u0010\u000f\"\u0001G )\u00072 .\u0005<\u0011<+K\u0003\u0015\u0005L \u0010\u0011\u0013\u0016 .\u0016*\u00016\u0005 \u0007M()\u0005<\u0003N\u000163)\u00016\u001a6\u0005/\u0014O!P\u0016 \u0011=\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)30&Q-\u0010\u0007 \u00016\u0005\u001d\u0018R\u0011\u0013\u0005<\u001a6\u0011%\u000f\u0012\u0005<;/\u00072$\u0006\u00162!S\u0007M\u00016\u0016 ()\u0019 !E\u0011\u0013\u00160\u0014T\u0007U\u0014L\r\u0010\u0001\u001b\u000f\u0012\t\u001f\u0007 $\u001e\u0018)\u0007/\u001a6\u00072-\u0010\u00072\u00016\u0005V\u0003W9)\u000f\"\t#9X\u0007 \u0018'\u00072 \u0010\u001a\u001b\u0001C\u0003\u0006\u0016 \u0011%\u0018\u0010\u00016 .\u0016 \u001a8& \u001a6\u000f\"()\u0019F\u001a6\u0005\u001d\t#9\u0010()\u000f\"\u000b'\r'\u0005/\u0001\u001e!E\u0011%\u0016*\u0014Y\u00072\r)\u001a6\u00160\u0014L\u00072\u001a\u001b\u000f\"\tZ\u00016 .\u0005<\u0005/\t%9R\u0011\u0013\u0005\u001d\t\u001f\u0016 \u0019*(\u0010\u000f4\u001a\u001b\u000f\u0012\u0016*( \u001a8\u0016B\t#\u0011%\u0005<\u0007/\u001a6\u0005?\u00077\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3[\u00016 .\u0016 \u001a6\u001a\u001b\u000f\u0012()\u00195\u000163'\u00016\u001a6\u0005/\u0014\\\u000f\"([\u001a\u001b9'\u0005M\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t\u001f\u00072$ \u0018'\u00160\u0014\u0017\u0007 \u000f\u0012(]H_^\u00069\u0010\u000f\"\u0001_\u000163'\u00016\u001a6\u0005/\u0014`\u000f\"\u0001\b\u001a8\u0005/\u00016\u001a8\u0005/\u0018\u0017\r\u0010\u0001\u001b\u000f\"()\u0019a\u0007\u0002;\u001d\u00072\u0011%\u000f\u0012\u0005<\u001aJ3L\u0016 !b!P\u0005<\u0007\u001d& \u001a6\r)\u0011\u0013\u00055\u00016\u0005<\u001a\u001b\u0001D\u0018)\u0005<\u0011%\u000f\u0012;*\u0005\u001d\u0018V!E\u0011\u0013\u00160\u0014c\u0011\u0013\u0007\u001d\u0003d\u0007 \u0010\u0018\u0010\u000f4\u0016e\u0018)\u00072\u001a6\u0007\u000eHfIg\u001aD\u0011\u0013\u0005/\u0001\u001b\r)$\u0012\u001a\u001b\u0001 \u000f\u0012(h\u0007i\u0001\u001b\r\u0010\t\f\t\u001f\u0005\u001d\u0001\u001b\u00016!P\r\u0010$j \u0010\u0011\u0013\u0016\u000e\u00162!L\u00162!L\u001a69)\u0005[\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3U\u00016 .\u0016 \u001a6\u001a6\u000f\"()\u0019k\t\u001f\u0016 ('& #\u0005\u001d )\u001a:\u0003W9\u0010\u000f\u0012\t#9U\u00162lK\u0005<\u0011%\u0001?\u00192\u0011%\u0005<\u0007/\u001a: .\u0016 \u001a6\u0005\u001d(0\u001a\u001b\u000f\u0012\u0007 $G!P\u0016 \u0011C\u0018'\u0005\u001d;*\u0005\u001d$\u0012\u00162 .\u0014\u0017\u0005\u001d(\u000e\u001a \u000f\u0012(\u000e\u001a6\u0016\u0017\u0007=\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007 $b\u0018)\u00072\u001a6\u0007/-\u0010\u0007 \u00016\u0005\u001e\t#\u00072 \u0010\u0007/-@$\u0012\u0005a\u0016 !m-.\u0005\u001d\u000f\"()\u0019n\u000b'\r)\u0005<\u0011%\u000f\u0012\u0005\u001d\u0018?-\u000e3 \u0014L\u0005/$4\u0016'\u0018'3 H",
        "zenodo_id": 1415680,
        "dblp_key": "conf/ismir/Durey01",
        "keywords": [
            "abstract",
            "article",
            "key aspects",
            "captures",
            "Qwen",
            "helpful assistant",
            "captures",
            "key aspects",
            "abstract",
            "article"
        ],
        "content": "MelodySpotting UsingHiddenMarkovModels\nAdrianeSwalmDurey\nCenterforSignalandImageProcessing\nSchoolofElectricalandComputer Engineering\nGeorgiaInstituteofTechnology\nAtlanta,Georgia30332\n404-894-8361\ngte401k@ece .gatech.eduMarkA.Clements\nCenterforSignalandImageProcessing\nSchoolofElectricalandComputer Engineering\nGeorgiaInstituteofTechnology\nAtlanta,Georgia30332\n404-894-4584\nclements@ece .gatech.edu\nABSTRA CT\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0013\u0005\u0015\u0014\u0017\u0016\n\u0011\u0013\u0005\u0006\u0018\u0010\u000f\u0012\u0019\n\u000f\u0012\u001a\u001b\u000f\u0012\u001c\f\u0005\u001d\u0018\u001e\t\u001f\u0016\n \u0010\u000f\u0012\u0005\u001d\u0001\u0004\u0016\n!\u0010\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007\n$\n\u0011\u0013\u0005\u001d\t\u001f\u0016\n\u0011%\u0018'&\u000f\u0012()\u0019*\u0001\u001f+,\u000f\u0012\u001a\u0002-.\u0005/\t#\u00160\u0014\u0017\u0005\u001d\u00011\u000f\"()\t\u001f\u0011\u0013\u0005\u001d\u00072\u0001\u001b\u000f\"()\u0019*$435()\u0005\u001d\t\u001f\u0005\u001d\u0001\u001b\u00016\u0007/\u0011%37\u001a8\u0016:9)\u0007<;*\u0005\u0017\u001a\u001b9)\u0005\u00072\u0001\u001b\u0001\u001b\u000f\"\u00016\u001a6\u00072(\u0010\t\u001f\u0005=\u0016\n!>\u0007?\t#\u00160\u0014\u0017 @\r)\u001a8\u0005\u001d\u0011A\u000f\"(B\u00016\u00162\u0011%\u001a6\u000f\"()\u0019:\u001a69)\u0011\u0013\u0016*\r)\u0019*9C\u001a\u001b9)\u0005D\u000f\u0012('&!E\u0016\n\u0011#\u0014\u0017\u0007/\u001a\u001b\u000f\u0012\u0016*(F\u001a\u001b9)\u00072\u001aG\u000f\u0012\u001aG\u00016\u001a6\u0016\n\u0011\u0013\u0005\u001d\u0001\u001fHAIJ(F\u001a69\u0010\u000f\"\u0001G )\u00072 .\u0005<\u0011<+K\u0003\u0015\u0005L \u0010\u0011\u0013\u0016\n .\u0016*\u00016\u0005\u0007M()\u0005<\u0003N\u000163)\u00016\u001a6\u0005/\u0014O!P\u0016\n\u0011=\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)30&Q-\u0010\u0007\n\u00016\u0005\u001d\u0018R\u0011\u0013\u0005<\u001a6\u0011%\u000f\u0012\u0005<;/\u00072$\u0006\u00162!S\u0007M\u00016\u0016\n()\u0019!E\u0011\u0013\u00160\u0014T\u0007U\u0014L\r\u0010\u0001\u001b\u000f\u0012\t\u001f\u0007\n$\u001e\u0018)\u0007/\u001a6\u00072-\u0010\u00072\u00016\u0005V\u0003W9)\u000f\"\t#9X\u0007\n\u0018'\u00072 \u0010\u001a\u001b\u0001C\u0003\u0006\u0016\n\u0011%\u0018\u0010\u00016 .\u0016\n\u001a8&\u001a6\u000f\"()\u0019F\u001a6\u0005\u001d\t#9\u0010()\u000f\"\u000b'\r'\u0005/\u0001\u001e!E\u0011%\u0016*\u0014Y\u00072\r)\u001a6\u00160\u0014L\u00072\u001a\u001b\u000f\"\tZ\u00016 .\u0005<\u0005/\t%9R\u0011\u0013\u0005\u001d\t\u001f\u0016\n\u0019*(\u0010\u000f4\u001a\u001b\u000f\u0012\u0016*(\u001a8\u0016B\t#\u0011%\u0005<\u0007/\u001a6\u0005?\u00077\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3[\u00016 .\u0016\n\u001a6\u001a\u001b\u000f\u0012()\u00195\u000163'\u00016\u001a6\u0005/\u0014\\\u000f\"([\u001a\u001b9'\u0005M\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t\u001f\u00072$\u0018'\u00160\u0014\u0017\u0007\n\u000f\u0012(]H_^\u00069\u0010\u000f\"\u0001_\u000163'\u00016\u001a6\u0005/\u0014`\u000f\"\u0001\b\u001a8\u0005/\u00016\u001a8\u0005/\u0018\u0017\r\u0010\u0001\u001b\u000f\"()\u0019a\u0007\u0002;\u001d\u00072\u0011%\u000f\u0012\u0005<\u001aJ3L\u0016\n!b!P\u0005<\u0007\u001d&\u001a6\r)\u0011\u0013\u00055\u00016\u0005<\u001a\u001b\u0001D\u0018)\u0005<\u0011%\u000f\u0012;*\u0005\u001d\u0018V!E\u0011\u0013\u00160\u0014c\u0011\u0013\u0007\u001d\u0003d\u0007\n\r\u0010\u0018\u0010\u000f4\u0016e\u0018)\u00072\u001a6\u0007\u000eHfIg\u001aD\u0011\u0013\u0005/\u0001\u001b\r)$\u0012\u001a\u001b\u0001\u000f\u0012(h\u0007i\u0001\u001b\r\u0010\t\f\t\u001f\u0005\u001d\u0001\u001b\u00016!P\r\u0010$j \u0010\u0011\u0013\u0016\u000e\u00162!L\u00162!L\u001a69)\u0005[\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3U\u00016 .\u0016\n\u001a6\u001a6\u000f\"()\u0019k\t\u001f\u0016\n('&\t#\u0005\u001d )\u001a:\u0003W9\u0010\u000f\u0012\t#9U\u00162lK\u0005<\u0011%\u0001?\u00192\u0011%\u0005<\u0007/\u001a: .\u0016\n\u001a6\u0005\u001d(0\u001a\u001b\u000f\u0012\u0007\n$G!P\u0016\n\u0011C\u0018'\u0005\u001d;*\u0005\u001d$\u0012\u00162 .\u0014\u0017\u0005\u001d(\u000e\u001a\u000f\u0012(\u000e\u001a6\u0016\u0017\u0007=\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007\n$b\u0018)\u00072\u001a6\u0007/-\u0010\u0007\n\u00016\u0005\u001e\t#\u00072 \u0010\u0007/-@$\u0012\u0005a\u0016\n!m-.\u0005\u001d\u000f\"()\u0019n\u000b'\r)\u0005<\u0011%\u000f\u0012\u0005\u001d\u0018?-\u000e3\u0014L\u0005/$4\u0016'\u0018'3\nH\n1.INTRODUCTIONo,pKqsr2tvu/wyx#wyqgz%{0z#x#t>|/}s~#x#}st6pQpQt8p6/Kt>r\nz\u001b\ftvz%q\u0004~\u001f/}\u0004u/wypQ|*~\u001fpsz%{\u000ez#jwy/\n}st\u001bz#pQwy2x\u0015z#W~#2qb~#)u/wyx\u001fwqswy6t\u001buWwy/\u0012~#}sjz\u0013qsw~#W}gz#2x#wy2x_\u0012}s~#|/{4z%wyqstJ\u001dq]qs~\u0006z#\nu\u001dwy~/<<wu/t6~\u001d\u001dz#\nuW\u00152{qswyWt\u001bu/wz\n~#qst6q\u001b@W~%}gu/t8}\u0004\u0012~#}\u0004z|*tJ}spQ~\u001fqs~_pQw\u0012q]qsr/}s~\u001f/x\u001frSqsr/wyp]#z#pq]\nz%<qswqPS~%0w\u001d\"~%}sjz%qswy~\u001f\u000ewqKwyp/t\nt6pQpsz\u0013}Qaqs~jt62{ywypq>qsr/t\u0015z%wu\u001e~%]qsr/t\n~\u001fW|//qst8},r2w\nrLpqs~%}st6p>wq\u001b\n~#}\u0006qst8<q\u001b\u0010}gz#|/w4u=jz\u0013q\nr/wy2x1WtJqsr2~<u/pr2z\u0013\u001ftW*t\n~\u001fWtqsr/tWz#Wwy{wz\u0013}_pQt\u001bz%}\nr\u001et62x#wy2t6p\b~%.qsr2t=~#}s{u17wu\u001dt\u0006nt6',=~%}s1~\u001f\u001e<wu/t6~wyjz%x\u001ft\u0013}st\n~#x\u001f2wqswy~\u001fBz#2uMz#2u/wy~\npQ|*t6t\nr*}st\n~#x\u001f2wqswy~\u001fCr2z#pGz{y~#2xjr2wypqs~#}Qv>/{1}st\nt8<qs{1r\nz\u001b\ft_\u0004t_}st\n~#x\u001f/w8t\u001bu\u001eqsr/t2t6t\u001bu1qs~u\u001dt6\ft6{y~#|}st\n~\u001fx#2wqswy~\u001fjp\u001dpqst6Wp.\"~#}K~#qsr/t8}]qP\u001d|*t6p.~#'z#2u/wy~/<|2z%}Qqsw\n/{z%}s{1\u00152pQw\n\nr/tS|\u001d}s~\u001f2{yt6h\u0004t\u0015wy{y{bz\u001fu/u\u001d}st6pQp_wyaqsr2wyp\b|2z#|*t8}_wyp\bqsr2z%q\b~%\u0004\u00152pQw\nz#{\u0004u/z%qgz%\nz#pQtj/t8}QL<\u001eWt8{~<u<j\u0017pQ\nr=z1p\u001dpqst6A\u000eqsr2tW/pQt8}|\u001d}s~\u0013\u001dwu\u001dt6pGqsr/t\u0017/t8}Q:Wt8{~<u<\n?r\u001d/WWwy2x/Kr2wypqs{yw/x/>\ft8<*~\u001fz%}gu0\u0004t8q\n z%\nuDqsr2t1\u00152pQw\nz%{vu/z%qgz%\nz#pQt1p/pqst8`}st8qs\u001d}s2pSqsr/tW~#pqS{ywy\ft6{njz%q\nr2t6pqs~awq\u001b\nr</p6@\u0004tGz%}stjwy<qstJ}st6pqst\u001buDwy=}st\n~#x\u001f/w8w/xGqsr/t\u0006Wt8{~<u\u001dwS\n~#<qst8<qv~#KzW|2wyt\nt\u0015~#]S/pQw\n\nqsr\nz\u0013q\b|\nz\u0013}Qqr/w\nrwyp.W~\u001fpq@}st\n~\u001fx#2wy\u001bz#/{yt,qs~>qsr/t\nz%pQ\nz#{/{ywpqst82t8}\u001b<z%\nu0#qsr/t8}st8\u0012~#}st\u001f2W~\u001fpqt\u001bz%pG\u0012~#}vr2wy qs~\u0015}st\nz#{y{\n\nr2tJ}st>wypmzx#}s~\u001bwy2xS*~<u<W~#\u000e}st6pQt\u001bz\u0013}\nrGz\u001fu/u\u001d}st6pQpQwy/x\u0006qsr/tv|\u001d}s~\u001f2{yt6~#2S/pQw\nwy/\"~%}sjz%qswy~\u001f\u0006}st8qQ}swyt6#z#{\nE¡¢>'~\u001f\u0006qsr2t]Wt6{y~<u\u001d_{yt6\ft8{£\u0010qsr2t\u0006\"~#{{y~\u001bwy2xApQt\nqswy~\u001f'\u000e\u0004t\u0006wy{{bz#u2u\u001d}st8pQp_pQ~\u001fWt\u0006~#Kqsr/t\u0015WtJqsr2~<u/p2pQt6uDqs~\u0017z#|/|/}s~\u001fz\nr¤qsr/wyp|/}s~\nt8pQpwy?|\u001d}st6<w~#2p\u0006}st6pQt\u001bz%}\nr'\nr/t6wyF¥\u001dt\nqsw~#[¦\u001f §<v\u0004t\u001ew{y{_t8\u001d|/{~%}st\u001e~\u001f2tL~#qsr/tL/t8¨u\u001dw}st\nqswy~\u001f/pwy=r2w\nr=qsr2wyp\u0006}st8pQt\u001bz%}\nr¤u/w}st\nqsp\u00152p6.z#u2z%|/qswy2xAqsr/t\u0002/pQtj~#r2wu<u/t8\n¡z\u0013}s\f~\u0013W~<u/t6{ypK4}s~\u001f pQ|*t8t\nrj}st\n~#x\u001f/wyqswy~#Gqs~\u0015qsr/t6w}m2pQt,\"~%}Wt6{y~<u\u001dj}st\n~\u001fx\u001f/wqsw~#']A¥\u001dt\nqswy~#a§\u0006Kt\bw{y{'{~~#Az\u0013qmqsr2tu/tJqgz#wy{yp~#@qsr/t_p\u001dpqst6 \u0004t_|\u001d}s~\u001f|*~\u001fpQt#¥\u001dt\nqswy~\u001fa©\u0015wy{y{@u\u001dwp\n2pQpmqsr2t\b}st6pQ/{qsp~#]|/}st8{wyWwy\nz\u0013}QAqst6pqswy2x\u0002~#bqsr2wyp\bp\u001dpqst6A\n/qs\u001d}st\u0006u/w}st\nqswy~\u001f/p\b\"~%}}st6pQt\u001bz\u0013}\nrnz#2u\n~#\n{y/pQw~#2p_wy{y{]*t\u0015|/}st6pQt6qst\u001bu\u001ewyLqsr/t\u0015ª2\nz#{@pQt\nqswy~\u001f\u000e\n1.1Melody-Le velMIRResearch\nr2tKpQt6Wwy\nz#{K~#}s<p.w\u0015Wt6{y~<u/w\n¡P¢\u0006\u0013qsr2~#pQt\u0004\u0015«>r2wz#p6\ft8qbz#{/¬ ­\u0013®z#2u\n¡6¯\nz#\u000e\u0015tJqnz%{S¬¦\u001f¦J®£\u0015\u0012~#}s°qsr2t=2z#pQwypa\"~%}n\u0015\nrB~%qsr2tWt6{y~<u\u001d<E{yt6\ft6{\n¡¢B}st6pQt\u001bz%}\nrAr2w\nrA\"~#{{y~\u001b\u0004t6u\u001eqsr/t6A\n~#{y{~\u001bwy2xqsr2t8wy}K\"~#}sjz\u0013q\u001br2w\nrGwyp\u0004x#t62t8}gz%{yw8t\u001buGwy\nwyx\u001f/}st\u0015¦\u001f\u001dz\u0006u2z%qgz%\nz%pQt\b~#S/pQw\nz#{\u0010}st\n~#}gu/wy/x\u001fp\bwypu/t8\ft6{y~\u001f|*t\u001buaAr2z#2u\u001ewy\n¡±\b\n~%}_pQ~\u001fWtpQwyWwy{4z\u0013}ju/wyp\n}stJqstG\"~%}sjz%qJSz%\nuDpqs~#}st6uZwy¤W~\u001f/~\u001f|2r/~\u001f/w\nqQ}gz\n<p6\nr2t8pQtbqQ}gz\n<p\u0010z\u0013}stbqsr/t6\b\"\u001d}Qqsr2t8}\u0010z#/pqQ}gz\nqst\u001bu_wyqs~mWt6{y~<u/w\nz#\nu2²%~%}}sr<qsr2Wwj\n~\u001fqs~\u001f\u001d}spSr2~#pQt\n~#qst6q#z%}swyt6pS4}s~\u001f³x#t62tJ}gz#{\n2|\u000eu/~\u001b\u000e\n~#},qsr2t_psz%Wt\u0013Kqs~Wt8/z\nq\n|/{/p~#}vWw</pzpQ|*t\nwª\n<2*t8}\u0015~#,pQt6Ww£qs~\u001f/t6p6 C7r/t6nqsr2tj/pQt8}/t8}swyt6p\u0006qsr/tjp/pqst8A@r2tpQwy2x#pK~#}]r<2Wp\u0010qsr/tu/t6pQw}st\u001buWt6{y~<u<\nr2tp\u001dpqst6Rqsr/t6t8<qQ}gz\nqspqsr2tS|2wq\nr/t6p\nz#2uL}sr<qsr2\u0002vwy\u0017qsr2z%q\u00152t8}QLz%\nunt8\n~<u\u001dt6p_qsr2t6z#p_qsr/tWu2z\u0013qgz#\nz%pQtWr\nz%p_*t6t6nt8\n~<u\u001dt\u001bu\u000e\nr/tWu2z\u0013qgz#2z#pQt\n~#<qst8<qspz#2u\u0006qsr/tm2t8}Q\u0006z\u0013}stKqsr2t6\n~\u001fW|2z%}st\u001bu0\u001fx#t62tJ}gz#{y{S/pQw/x>pQ~#Wt\u0004\u001fz\u0013}swz%qswy~#A~#bu</2z#Ww\n|\u001d}s~\u001fx%}gz#WWwy2x\n±_´KJ\u001dt6pQ|*t\nwz#{y{1qsr\nz\u0013q,u/t6\u001ft6{~\u001f|*t6uL\n¡~#2x\u001ft6z#=z%\nu\u0017¥/z%2\f~%µM¬¦\u0013§\u001b®E\nr2wyp\b|\u001d}s~\nt6pQp_|/}s~<u\u001d\nt8pz1}gz#/\ft\u001bu={ywypq~%mqsr2tjpQ~\u001f/x\u001fpSwy=qsr/tGu/z%qgz%\nz#pQtWr/w\nrZz%}stjW~\u001fpq{ywy\ft6{aqs~\n~\u001fqgz#wy\u001eqsr2t\u0006Wt6{y~<u\u001d\b7r/wy{t\u0006qsr2wyp_u\u001d~t6p\b2~%qz#|/|2{Aqs~z#{y{.Wt6{y~<u/w\n¡¢ep\u001dpqst6Wpvr/w\nrLr\nz\u001b\ft\u0006*t6t6L|/}s~\u001f|*~#pQt\u001bu\u000e\u000ejz%<~#@qsr/t6kz#u/~\u001f|\u001dq,z%q,{yt\u001bz#pqvpQ~\u001fWt>|*~#}Qqswy~\u001fA~%@qsr2wypm\u0012}gz%Wt8\u0004~%}s0\nr2tJ}st\u0015z\u0013}st\u0006z\u001d/S*t8}~%.|/}s~#2{yt6Wpwu/t6qswª*z%2{ytwAqsr/wyp,x#t62tJ}gz#{pqQ}s\nqs/}st\u001f\nr2tª2}spq>~#.qsr2t8pQt\u0015wypvqsr/t2pQt\u0006~#]r2wyx\u001fr/{y1pqQ}s\nqs\u001d}st\u001buz#2u/wy~\b\"~%}sjz%qsp6pQ\nrz#p\n¡P±\bJ%qs~\b}st6|\u001d}st6pQt6q.qsr2tu2z\u0013qgz#2z#pQt\n~\u001f\u001dqst6qsp6\u0015ovLz8\n/}gz\u0013qstWz#\u001dqs~\u001fjz%qsw\n|/}s~\nt6pQp\b\u0012~#}x#t62t8}gz\u0013qswy2x\n¡P±\b\u0012}s~#N|*~\u001f{\u001d|2r2~#2w\n}gz6hz#\nu\u001dwy~=r\nz%pj\ft8qjqs~=*t\u001eu/t6\u001ft6{y~\u001f|*t\u001bu0¶,qsr/wypWt\u001bz%2pGqsr2z%q\n¡±\bWt6\n~<u/wy/x?\u00152pqG~#\u0012qst8B*t\u0017u/~#2tL<:r\nz%\nuz#2u=wyx\u001f2~%}st6pzA#z#pqKt\u001bz#{qsr¤~#vu2z\u0013qgzaz#{}st\u001bz#u\u001dDu\u001dwyx\u001fwqswy6t\u001buDwyD{t8pQppqQ}s\nqs/}st\u001bu1\u0012~#}sjz%qsp\n{yw\u001ft\n¡´b·K«i¸)z6t8}\b©\fJ\nr2wyp>\u0012~#}sjz\u0013q\bz%{pQ~}st\u001b2w}st6p]qsr\nz%qKqsr2t\n~\u001fqst6qspK~%)qsr2t>u2z\u0013qgz#\nz%pQt>*t,ª2}sW{\nt6\ft8\u0002wt8}Q}s~#2t6~#2pQ{2\u0006u\u001dt8ª\n/t\u001bu\u0017*t8\u0012~#}st\u0002|/}s~\nt8pQpQw/xa*t6x\u001fwy/p6\u001e¸'wy\ft8wypQt\u001f.wqsr2tSu2z\u0013qgz#\nz%pQtW/qswy{ywy6t6p~\u001f/{y\u001eWt8{~<u\u001dwyt6pwL|/}s~\nt6pQpQwy2x/)qsr2t8aqs~~S/pq*tA|\u001d}st8Et8<qQ}gz\nqst6uZZz#?t8\u001d|*t8}Qq\u0012~#}jx#}st6z%qst6pqGz8\n/}gz\nDSP Tools\ne.g., FFTDatabase of\nMusical\nRecordings QueryUser's\nSung\nContour\nGeneratorContour\nGenerator\nPitch and/or\nRhythm\nContoursPitch and/or\nRhythm\nContours\nProgramminge.g., DynamicRecognitionPattern\nRanked List\nof Matching\nSongsDatabase of\nMIDI\nMelodiesMIDI\nMelodyQueryHand\nTranscription\nof Melody¹\n\u000f\u0012\u0019\n\r)\u0011\u0013\u0005\u001eº2»¼\u0002\u0005\u001d()\u0005<\u0011\u0013\u0007\n$\u0012\u000f\u0012\u001c\f\u0005\u001d\u0018=\u00072\u0011%\t%9\u0010\u000f\u0012\u001a6\u0005\u001d\t\u001f\u001a\u001b\r'\u0011%\u0005G\r\u0010\u00016\u0005\u001d\u0018\u0017!E\u0016\n\u0011\u0015\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)30&-)\u0007\n\u00016\u0005\u001d\u0018M\u0014\u001e\r\u0010\u0001\u001b\u000f\"\tA\u000f\u0012()!P\u00162\u0011\u001f\u0014L\u00072\u001a\u001b\u000f\u0012\u0016\n(C\u0011\u0013\u0005<\u001a6\u0011%\u000f\u0012\u0005<;\u001d\u0007\n$qsr/~\u001f/x\u001frA}st\u001bz%pQ~\u001f\nz%2{ax#~~<u\u001eWt8{~<u<<E4}s~\u001f\n¡±>Kt8<qQ}gz\nqs~#}sp\bz\u0013}st*t8w/xu\u001dt6\ft6{y~\u001f|*t6uL¬¦\u0013½\u001b®\u0012z%\nu\u0002z\u0006r2z%}guGu/t\nwypQwy~\u001fGjz\u001fu\u001dt>z%*~\u001f/qmqsr/t6w}\n~\u001fqst6q\u001bb´\u0010~\u001f{\u001d|2r/~\u001f2w\nz#\nu\u001dwy~Swypm~#4qst61/~#q,z\u001fu/u\u001d}st6pQpQt6u\u0002wy1Wt8{~<u<{yt6\u001ft6{\n¡¢¾p\u001dpqst6Wp6bqsr2~\u001f/x\u001fr¤pQ~\u001fWtJqswWt8pSwqWwyp}st6|/}st8pQt6qst\u001bu¤z#ppQt8\ft8}gz#{\u000eW~\u001f/~\u001f|/r2~\u001f/w\nWt6{y~<u/wyt6pmt6z\nrG}st6|\u001d}st6pQt6qswy2xS~#2t\b\u001f~\u001fw\nt\b~%qsr/t1{z%}sx#t8}j|*~\u001f{\u001d|2r/~\u001f/w\n|2wyt\nt#\nr2t1t8\n~<u\u001dw/xn~%\bWt6{y~<u/wyt6pSw|/wq\nrnz#\nua}sr<qsr2\n~\u001fqs~\u001f\u001d}sp_wpz\u0002x\u001f~~<u\u0017\u0004z6Lqs~\u0002}st6pQ~\u001f{y\ftWtJ}Q}s~#}spwyLqsr2tWu/z%qgz#2z#pQtjz%\nu\u0017jz#u/taqsr/t2pQt8}\u001b¿>~\u001b\u0004t6\u001ft8}\u001b)KtS/pq*t\nz%}st8\u00122{\u000e2~#qqs~Wu\u001dwyp\nz%}guGqs~~SS\nr\u00022t8}Q\u0002wy/\u0012~#}sjz%qswy~#1r2t6Kt\nz#1r\nz\u001b\ft_z\u0015}st6z#pQ~\u001f2z#/{t{yt6\ft8{'~%\n~\u001f\u001dª*u/t8\nt\bwyAwq\u001b\nr/~\u001f/x\u001frF~#qsr2tJ}aWt8qsr2~<u\u001dp1z%|2|/{wyt\u001bu7qs~ZWt6{y~<u/w\n¡P¢¨r\nz\u001b\ftLwy/\n{2u/t\u001bu>qst\nr/2w2t6p0pQ\nrz#p)u/wypqgz#\nt.Wt\u001bz#pQ\u001d}st\u0013²\u0013qst8W|2{z%qst.jz%q\nr/wy/xM¬ ½\u0013®£À*Px%}gz#Ájz\u0013q\nr/wy2x:¬¦6Â%®£mqQ}st6tAwy\nu\u001dt8\u001dwy2x:¬ ©\u0013®E,z%\nuDqst8<q}stJqQ}swt8\u001fz%{>¬ Ã#®v¬ Â%®£@qsr2t|\u001d}swyjz%}Q\u001eWt8qsr/~<u/~\u001f{y~#x#\u0017~#W~\u001fpqWt8{~<u\u001dw¡P¢ p\u001dpqst6WpwypSz%pSu\u001dt6p\n}swy*t\u001buDz#*~\u0013\ft#\u001e¥\u001d\nrnt8\n{y2pQwy\ftW2pQtj~%u<\u001d\nz#Ww\n|\u001d}s~\u001fx%}gz#WWwy2xS\u0012~#}vWt6{y~<u/w\n¡¢Bwyx\u001f2~%}st6p,~#qsr2tJ}\n~#|2z%}swypQ~\u001fLqst\nr22w2t8pvr2w\nr\u001er\nz\u001b\ft\u0006*t6t6LpQ6\nt8pQp\"2{y{\u0017z#|/|2{ywyt\u001bu\u001eqs~~%qsr2t8}'\"~%}sWp\u0010~#2z#2u/wy~pQwx#\nz%{2\nu\u001dt8}spqgz#2u/wy2x\u001d%|\nz%}Qqsw\n/{z%}s{\bqsr2~#pQtqs~~#{yp2pQt\u001bun\u0012~#}WpQ|*t8t\nr=}st\n~\u001fx#2wqswy~\u001f'\u0017=qsr/tj2t8<q\u0015pQt\nqswy~\u001f\u000ebKtwy{y{\bt8\u001d|/{~%}stAqsr2ta/t8¾}st6pQt\u001bz%}\nrCu/w}st\nqsw~#2pjpQ2x#x\u001ft6pqst6uM<¤qsr/t|\u001d}s~\u001f/{t8Wpmu/t6p\n}sw*t6u\u0002z%*~\u0013\ft\bz%\nu\u0002z\u0006|2z%}Qqsw\n2{z\u0013}\u0004qs~~\u001f{02pQt6uj\u0012~#}z%/qs~#jz%qsw\npQ|*t6t\nrA}st\n~#x\u001f2wqswy~\u001f\u000e*qsr2tr2wu/u/t6\n¡z\u0013}s\f~\u00131W~<u\u001dt6{\u0010z%p>wqwypvz#|/|2{ywyt\u001bu\u0002qs~\u001ft8<\u0004~#}gu\u0002pQ|*~#qQqswy2x\u001d\n1.2HiddenMarkovModelResearch\nr/t1z%\nz%{y\u001dpQwypW~#,|\u001d}st6<w~#2p\u0006\u0004~#}sD~#¤Wt6{y~<u\u001d<E\nz#pQt6u\n¡¢kpQ/x#x#t6pqsp\bz</S*t8}m~%.|*~#qst6qswz#{'2t8<q,pqst6|/p6\nr2tx#~\fz#{)~#@qsr2tz\u001btJ}gz#x\u001ft\u00062pQtJ}v~%bz\n¡¢Bp/pqst8kpQ\nraz#p\u0004t|/}s~#|*~\u001fpQtwyp,qs~S{~\nz%qstz1}st\n~#}gu\u001dw/xa~#vzA|2wyt\ntW~#,S/pQw\nr/w\nr=wyp\u0006S\nr\u0017W~#}stWt\u001bz%pQw{\u0012~\u001f/\nuawy\u0017z%\u001e22pqQ}s\nqs/}st6u1\"~#}sjz\u0013q\n\u0012~#}_t8/z%W|2{yt\u001f0~\u001fLz\u0002ÄK±i~#}wyjqsr2t,2/w4/wyqs~#2p.Å<Æ<Ç\u0006\u0012~#}sjz%q\u001b \nr</p6\u0004tvK~\u001f/{4uj{ywy\ft>~#/}mp\u001dp\nqst65qs~>~\u001f|*t8}gz\u0013qstm~\u001fS}gz6?z#2u/wy~>\"~%}sjz%qsp6\u001fpQ\nrz#p@È\u001dÉ%Ê2<ÉË\u0013Ì\fÌ2<z%\nuÅ\u001dÆ<Ç\u001d\n~Wz\u001b\f~#wuGqsr2tz#u2u/wqswy~\u001f2z#{'|/}s~\u001f/{yt6¾~#@jz%\u001dwy/xWzSr\nz\u0013}guAu/tJ\nwypQwy~\u001fGz#*~#/q\n~\u001fqst6qbr2t6Wx#t62t8}gz\u0013qswy2x\u0015z\bpqQ}s\nqs\u001d}st\u001but6\n~<u/wy2x\u0012}s~#`}gz6kz%\nu\u001dw~\u001d.\u0004tj\u0004~\u001f/{u¤|\u001d}st8\"tJ}Wzap\u001dpqst6¨qsr\nz%qSjz#wyqgz#wy2pzA<2\u0015*t8}_~#,x#2t6pQpQt6p\u0015z#*~#/q\u0006qsr2t\n~#qst6q\u0015~%,z1}st\n~%}gu/wy2x\u001ez%\nu\nz%{wª2t6pSt\u001bz\nr¤x#2t6pQpSwqsrZza{yw\u001ft6{ywr/~~<uZ~%\n~#}Q}st\nqs/t6pQp6\nr/wypwyp\bz%\nz#{y~#x\u001f~\u001f/pvqs~qsr/tqQ}st6z%qsWt6q,~#bu/z%qgzWwyApQ|*t6t\nrA}st\n~#x\u001f2wqswy~\u001fu2z\u0013qgz#2z#pQt6p6Í\n}st\u001bz%<w/xaz6mz6\u001e\u0012}s~#Xu/wyp\n}st8qswy6t\u001bu\u0017u2z\u0013qgzG\"~%}sjz%qspz#{ypQ~ApQ/x\u001fx#t6pqsp/}st6z#<wy2xLz6mz6=qs~\u001et8\u001d|2{y~#}stG2t8\"~%}sWpS~#>u2z\u0013qgz\n~\u001fW|2z%}gz%qs~%}sp6¡z#n~#,qsr2tjwu/t\u001bz%pSt8\u001d|*t8}swyWt6qst\u001bu\u0017wqsr=qsr<2pz\u0013}wy\n¡¢V}stJpQt\u001bz\u0013}\nr\u0002pQr\nz\u0013}st\bz\n~\u001fWW~#W\nz\n<x#}s~#22uSwyqsr\u0002pQ|*t6t\nrW}st\n~#x\u001f2wqswy~\u001f|/}s~\nt6pQpQwy2x/\n~~\u001f{yp]pQ\nr\u0015z#p@qsr/t]z%pq\n~\u001f\u001d}swtJ}bqQ}gz%2p\u0012~#}sk¬¦\u001b©\u0013®\nz%\nuu\u001d\u001d2z#Ww\n|\u001d}s~\u001fx#}gz%WWwy2x\n±\b´KK¬¦\u001f¦J®\nr2z\u0013\u001ft\u0004*t6t6\u00152pQt\u001bu\"~#}bWt6{y~<u\u001dwjz%q\nr2wy/x\bwyqsrj#z%}Q\u001dwy2x_{yt6\ft8{pK~#\u000epQ6\nt6pQp6Kvqsr2t8}bqs~~\u001f{yp\u0004pQ\nrjz%pWt6{£\u0012}st\u001b/t6\n\nt6|2pqQ}gz%{\n~tJÎ\nwyt6qsp\nE¡\nÄmÄKpg\b¬ ½\u0013®@z%\nujr2wu2u\u001dt6¡z%}s\u001f~%aW~<u\u001dt6{yp\n¿\n¡a¡pg\u0015¬ §<¦8®br2z\u0013\u001ft\u0015*t6t8\u0017/pQt\u001buA\"~#}r/wyx\u001fr/E{yt6\ft8{\n~#<qst8<qGjz%q\nr2wy/x/\n{z#pQpQw\u0012\u001dwy2x:z#2u/wy~D:q\u001d|*t\u001f,//q1z%}stL{t8pQp\u0012}st6</t6qs{1z%|2|/{wyt\u001bu\u0002qs~Wt6{y~<u\u001dW}st\n~\u001fx\u001f/wqsw~#'\nr2tJ}st\u0004z%}st.\"tJ=}st8\"tJ}st6\nt8p@w_qsr/tb{ywqst8}gz\u0013qs/}st]qs~mqsr/t]/pQt]~%2¿\n¡a¡p\"~%}Lz#2z#{\u001dpQwpa~%S\u00152pQw\nz%{z#\nu\u001dwy~/U¸'~#x\fz%Rz#2uFÄKr\u001di¬¦\u001bÏ\u0013®\u0015/pQt\u001bu¿\n¡a¡p@qs~_z%\nz%{y\u001d6tmqsr/t\u0004pqQ}s\nqs\u001d}st\u0004~#*}s~\n\u0015z#\nu\u0006|*~\u001f|pQ~#2x#pbr/w\nrr\nz#un*t6t6\u0017}st6|/}st6pQt8<qst6u=z#p\n¡\nÄmÄKp6\nr/wyp\u0015pqQ}s\nqs/}gz%{Kz%\nz#{\u001dpQwypmz%pKqsr2t8W2pQt\u001buWz%pmz_\nz#pQwypb\"~#}\u0004t8<qQ}gz\nqswy2x\u001eÐP\ft8|/r/}gz%pQt6pQÑ\nqsr2~\u001fpQt~#\u0010wqst8}st8pqvpqQ}s\nqs/}gz%{y{y\u001dt\u001f x\u001d\nz\nr2~#}s/pg]\u0012}s~# qsr2t_z%\nu/wy~\u001d\nr/wypp\u001dpqst6\n~#2{u*tm2pQt\u001bu\u0006qs~_pQ2|2|/{\u0015\u00152pQw\nz%{\u001dqsr<2\u001522z#wy{yp@~#0{z%}sx#t8}}st\n~%}gu/wy2x#p\u0004qs~Sz\u0015u2z\u0013qgz#\nz%pQt_z8\nt6pQpmp\u001dpqst6A<2\u001dqKqsr/t\bz#\u001dqsr2~%}spmu/wu2~%q\bpQ|*t\nwyª\nz#{y{\u001ez#u2u\u001d}st8pQp\bwy\n~%}s|*~#}gz%qswy~#\u0017~%bqsr/t6w}\bpQ~%\u0012q\u0004z%}stwyqs~pQ\nr\u001ezWu2z\u0013qgz#2z#pQt\u001fÍ\nz\u0013qs{{yt\u0015z#2u\u001eÄ\u0004z#2~\u001e¬¦8®\u0010{wy\ftJwpQt\u00152pQt\u001buA\u00152pQw\nz%{z#2u/wy~\n~#\u001d\u001ft8}Qqst\u001bu?qs~\n¡\nÄmÄKpAz#2u:t6/t8}sx#?qs~=qQ}gz#wy\n~#W|*t8qswqswy\ft,¿\n¡a¡p]qs~|*t8}Q\u0012~#}sVz/{yw2u\n22pQ/|*t8}s<wypQt\u001bu\n@pQt6x\u001fWt8<qgz\u0013qswy~\u001fqgz#pQ*¿\n¡a¡p@r2z\u001b\ft]/~#q\u0010\ft8q\u0010*t6t8Sz%|2|2{ywyt\u001bupQ|*t\nwª\nz#{y{\u0006qs~,Wt6{y~<u/w]\n~\u001f\u001dqst6q,}st\n~\u001fx#2wqswy~\u001f\u000e\nr/t\u0015z%|2|/{w\nz\u0013qswy~\u001fL~#b¿\n¡a¡p\n{y~\u001fpQt6pqvqs~\n¡P¢z#|/|*t\u001bz%}sp)w¢vz#|/r\nz#t8{\n¬¦\u0013Ò\u0013®r/t8}stK¿\n¡a¡p\u0010z\u0013}stK2pQt6u_z#p@zm|/}st\n/}spQ~%}qs~vz#/qs~#jz%qsw\nz6\n~#W|\nz#/wyWt6q)_|*t8}Q\u0012~#}sWwy2x,pQt6x#Wt6qgz%qswy~\u001f\u0006~#}gz6Fz#2u/wy~\u0015r/t6\u0002qsr/t>p\n~%}st\bwypm<2~\u001b'.¿\n¡a¡pz%}st\n~#2pqQ}s\nqst\u001bu\nz%pQt\u001bu\u0015~#\u0015qsr/t\u0004|2wq\nr2t6p.z%\nuSu\u001d/}gz\u0013qsw~#S/~#qst\u001buwyz>p\n~%}stm2pQwy2x,\"t\u001bz\u0013qs/}st8p]2z#pQt\u001buS~#jz#W|/{wqs2u/t\u001f\fz\nqswy<wqz%\nu\u00154}st\u001b2t6\n\n~\u001fqst6qb~#qsr2t\u0002pQwyx\u001f2z#{?o,\u0012qstJ}qQ}gz#wy2wy/x/bqsr2t6pQtG¿\n¡a¡pWz\u0013}st12pQt6u¤qs~\u0017z#{ywyx\u001fz#2u/wy~Au/z%qgzjwyqsrLqsr2t\u0015p\n~#}stSqsr2z%q_}st6|\u001d}st6pQt6qsp\bwq\u001b\nr/tS|\u001d}s~\u001f2{yt6r2w\nrA\u0004t_wy{y{@z\u001fu/u\u001d}st6pQp>z#{y{y~\u0013p>u2z\u0013qgzWqs~W*t_{yt6pQp,\u0004t6{y{@u\u001dt8ª\n/t\u001bu1wyz\u001fu\u001d\u001fz%\nt>z#2uGz%|2|2{ywyt6p,z{t8pQp\n~\u001f2pqQ}gz%wy2t\u001buGz#|2|\u001d}s~\fz\nr\u0002qs~Sz%\nz#{\u001dpQwyp~#@qsr/t\n~#<qst8<qspm~%bz\u0015}st\n~%}gu/wy2x\u001d¿\n¡a¡p\u0004z#\nuS}st8{4z\u0013qst\u001buWqst\nr/2w2t6pbr\nz\u001b\ft,z%{pQ~\u0006*t6t6\u0002z%|2|2{ywyt\u001buqs~_qsr2t|2wq\nrnt8<qQ}gz\nqswy~\u001f\u0017qgz%pQ*\u0002«>~#qs~=¬ Ó\u0013®\u00042pQt6uLt6pqswyjz\u0013qsw~#=jz%\u001dwyWwy\u001bz%qswy~\u001f\n·\n¡@qst\nr22w2t8p\n~#WW~\u001fSqs~qsr2tqQ}gz#wy2wy/x\u0015~%\n~#qsw</~\u001f2pu/wypqQ}swy2\u001dqswy~\u001fA¿\n¡a¡pqs~St6\u001fz%{y\nz%qst\bqsr/t\n~#<qQ}swy//qswy~\u001f/p,~%@#z%}swy~\u001f/pqs~\u001f/t1W~<u\u001dt6{yp\u0015qs~L|*~\u001f{\u001d|2r2~#2w\nz#\nu\u001dwy~\u0017psz%W|2{yt\u001bu=\u0012}s~#Áz\u0017ÄK±S.¿>tqsr2t8\u0017/pQt\u001buLqsr/t6pQt\u0015Kt6wyx\u001frqsp_qs~1jz#\u001ftz1u\u001dt8qst8}sWwy\nz\u0013qswy~\u001fL~#\u0004r/w\nr\u0012}st6</t6\nwt8p\u0015\u0004tJ}stjqsr/tjW~\u001fpq\u0006|/}s~#\nz#/{ytGWt6{y~<u<=z%\nun2z#pQpS{ywy2t8pwqsrn­#­\fÔÕz6\n\u001d}gz\nLz%\nuL­\u001fÏ\fÔhz6\n\u001d}gz\na}st8pQ|*t\nqswy\ft6{\u0015¥\u001d\nrLzp\u001dpqst6\n~\u001f/{4uDt6\u001ft6qs\nz#{y{=|\u001d}s~\u0013\u001dwu\u001dtGza#z#{y\nz%2{ytW\u0012}s~#<qSt6\nunqs~LzS/pQw\nz#{'u2z\u0013qgz#\nz%pQt_p\u001dpqst6A¡z#a~%Kqsr/t\u0006qgz%pQ\u001dpv|*t8}Q\"~%}sWt\u001buawy\u0017pQ|*t6t\nr\u001e|/}s~\nt6pQpQwy2x1r2z\u001b\ftSu\u001dw}st\nq@z#\nz%{y~\u001fx\u001f/t6p.wy\u0006\u00152pQw\n|/}s~\nt6pQpQwy/x/bov\u001dqs~\u001fjz\u0013qsw\npQ|*t6t\nr}st\n~\u001fx%2wqswy~\u001f\no\b¥\u001d¢>\nz%*tm{yw\u001ft62t\u001bu\u0006qs~\bqQ}gz#2p\n}sw|\u001dqswy~\u001fS4}s~\u001f[}gz6Mz%\nu/wy~qs~\n~\u001fWW~#Z\u00152pQw\n2~#qgz\u0013qswy~\u001f'ZÖ_t8<K~#}guD\u0004~#}gu\u001dpQ|*~#qQqswy2x\nF¥2\nz#A*t\b{ywy\ft6/t\u001buWqs~pQ|*~#qQqswy2xSz2t8}QjWt8{~<u<j~6\n\u001d}Q}sw/xwyG}gz6z%\nu\u001dw~\u001d\nr/tapQ6\nt6pQp\"/{>2pQta~%¿\n¡a¡pjwy7z#\u001dqs~\u001fjz\u0013qsw\npQ|*t8t\nr}st\n~\u001fx#2wqswy~\u001f\u0015z#\nu>K~#}gu\u001dpQ|*~#qQqswy2x,z#|/|2{yw\nz%qswy~#2p)pQ/x\u001fx#t6pqsp)qsr\nz\u0013q\u001b\u0013{yw\u001ft±\b´@#pQ\nrSqs~~#{p]Wwyx\u001frq.jz#\ftzvpQ6\nt8pQp\"2{/qQ}gz#2pQwqswy~\u001fqs~_Wt8{~<u<}st\n~\u001fx#2wqswy~\u001f'\nr2t6pQt\n~#}Q}st6{z\u0013qsw~#2pu<}sw\u001ftqsr/t\u0015p\u001dpqst6hu/t8\ft6{y~\u001f|*t\u001buwy1qsr/wyp,|2z#|*t8}\u001bP\u001epQ|*t6t\nr\u0017|\u001d}s~\nt6pQpQwy2x\u001d'K~#}gu/pQ|*~%qQqswy2x1wyp>qsr/t\u0006qgz%pQa~#]pQt\u001bz%}\nr2wy/x\u0012~#}\u0015\ft8<\u0004~%}gu/p_~6\n/}Q}swy2xApQ~#Wt8r2tJ}stWw=z1{z%}sx\u001ftJ}\u0015*~<u<L~#{yt6pQp\n~\u001f2pqQ}gz%wy2t\u001bu=z%\nu\u001dw~1u2z\u0013qgz\n/~\u001fwypQt6p6\u0010/~\u001f\u001dP\u001ft8<\u0004~#}gu\u001dp6\u000et8q\n S¬¦\u001b­%®]\u00152pQw\n'\u0004tSK~\u001f2{un{ywy\ftqs~ApQt6z%}\nrn\"~%}\u0015pQ|*t\nwª\nWt6{y~<u/wyt6p~6\n/}Q}swy/xpQ~\u001fWt8r/t8}st_wyAzS{z\u0013}sx\u001ft8}v*~<u\u001d\nqsr/t_u2z\u0013qgz#\nz%pQt\u0013\u0004~#.{yt6pQp\n~#/pqQ}gz%wy2t\u001bu\u0017z#\nu\u001dwy~1u/z%qgz\n~%qsr2t8}\bS2pQw\n\u000er2z%}sW~\u001f*/~\u001fwypQt6p6'pQ|*t6t\nr'tJq\n U¥<~\u001fWt\u0002~#vqsr2t1W~#pqWpQ8\nt6pQp\"/{vWtJqsr2~<u/p\u0006\"~%}W\u0004~#}gu\u001dpQ|*~#qQqswy/x\u0017r2z\u0013\u001ft1wy<\f~\u001f{y\ft\u001buD¿\n¡a¡p6×1~#2t1pQtJqqQ}gz#wy2t6u¤qs~\u001e}st\n~#x\u001f2wy6tAz\u001ft8nK~#}gu=~#}\u0015\u0004~%}gu/pz%\nu=~#2tjpQt8qSqQ}gz%wy2t\u001bunqs~Lz8\n~\u001f/<q\u0006\"~%}Sqsr/tÐPx\fz\u0013}s\nz#x#t6Ñ1jz%<w/xW2|Aqsr/t}st8pq>~%]qsr/t\u0015z%\nu\u001dw~\u001d=t|/}s~#|*~\u001fpQt\u0015qs~u\u001dt8ª\n/t,zpQtJq\u0004~#'¿\n¡a¡pbqs~_}st\n~\u001fx#2wy6t>z\u0006/t8}QSWt8{~<u<Wz%\nujz_pQt8q~%)¿\n¡a¡pKqQ}gz#wy/t\u001buqs~\u0015z6\n~#2qK\u0012~#}Kqsr/t>z%\nu\u001dwyqs~%}QDÐx\u001fz%}s2z#x\u001ft8ÑS2~%qz%pQpQ~\nwz%qst\u001bunwqsrLqsr\nz%q_</t8}Q\nr<2p6\u000e\u0004tr/~\u001f|*t\u0006qs~1|*t8}Q\u0012~#}sÕqsr/t\u00152pQw\nz%{\u0004t\u001b2wy#z#{yt6qS~%\u0004~#}gu\u001dpQ|*~#qQqswy2x\u001d\nr/tG/t8<qSpQt\nqswy~\u001fDwy{{u\u001dt6\ft6{y~#|qsr2tmpQ|*t\nwªv\n~#/ª2x\u001f/}gz\u0013qswy~\u001f2pb~#\u000e¿\n¡a¡pK2pQt6uSwyW~\u001f/}]wy|/{yt6Wt6qgz%qswy~\u001fG~#.Wt6{y~<u<GpQ|*~%qQqswy2x/\n2.EXPERIMENT ALSET-UP\nr/t\u0006\u0012~\u001f{y{y~\u001bw/xGqP\u0004~WpQt\nqswy~#2p>wy{y{@u/t8p\n}swy*t\u0006qsr/t\n~\u001fW|*~\u001fpQwqswy~\u001f\u001e~%qsr/tAWt6{y~<u\u001d¤pQ|*~%qQqswy2x=p\u001dpqst6³KtAqst6pqjwy?qsr/wpj|2z#|*t8}\u001b\nw}spq\u001bqsr/tWt8qsr2~<u\u001dp,~%]u2z%qgz\n~#{y{t\nqswy~\u001f\u001e2pQt\u001buAwya~#/}vt8\u001d|*t8}swyWt6qspvz\u0013}stu\u001dt6p\n}swy*t\u001bu0GntWt6{z#*~#}gz\u0013qstj~\u001fnqsr/t\n~\u001f/pqQ}s\nqswy~\u001fD~%m~\u001f\u001d}\u0006¿\n¡a¡Wt8{~<u<\u0002pQ|*~#qQqswy2xp\u001dpqst6kwy\u001e¥<t\nqswy~\u001fA§< §<\n2.1Database andTrainingData\nr/tu/z%qgz\"~%}_qsr2wyp>pQt8}swyt6p>~#]t8\u001d|*t8}swyWt6qsp\n~#2pQwypqsp\b~#Kz\n~\u001f{y{yt\nqswy~#~#*pQwyW|/{tKW~\u001f2~#|2r/~\u001f2w\nWt6{y~<u/wyt6p6\nr2wypbz#{y{y~\u001bp]/p\u0010qs~>*t6x#wywqsr\u0002z\b}st6{z%qswy\ft6{Wt6z#pqgz%pQ0\fpQw\ntqsr2tWt6{y~<u\u001dSwpbqsr2t~\u001f/{WS\u001dpQw\nz%{@u2z\u0013qgzwyAt\u001bz\nr1}st\n~#}gu/wy/x/\nr/tWt6{y~<u/wyt6pm\u0004t8}st_qQ}gz%2pQ|*~\u001fpQt6upQ\nr\u0006qsr\nz\u0013q.2~\bz6\nwu/t8<qgz%{yp]~6\n/}Q}st\u001bu\u0006w\u0015|2{z6qs~\bt62pQ\u001d}stmz\u001fu\u001dt\u001b\nz\u0013qst\n~%\u001ft8}gz#x#t\b~#)t\u001bz\nrG/~#qst\u001f]o,{{\nqsr/t>/~#qst6pK\u0012t6{y{0wqs~\u0006qsr2t}gz#2x#t>\u0012}s~#ÄmØmqs~\u0015«_Ù#.·]z\nrjWt6{y~<u<Smz%pK|/{4z6t6uSqst6WqswWt8p]~#jz\u001ft8\u001d*~\fz%}guZz%Mz#jz\u0013qst6/}S2pQw\nwz%'\nr2tAWt6{y~<u\u001dwt8p\u0004t8}sta|/{4z6t6uZt\u001bz\nrqswyWt\u0002wyZqsr/tGpsz%WtG\ftJ¤z#2uD}st8x\u001fwypqst8}\nqsr2t\u0002psz#Wt\u0002t8/z\nqW2~%qst6pg//q>2pQwy2xWª\n\u001ft\u0015u\u001dwµ0t8}st6q\bwy2pqQ}s/Wt6qgz#{'\f~\u001fw\nt6p\nqK~\u0002}st\n~#}gu/wy/x\u001fp|*tJ}v\u001f~\u001fw\nt\u001f \nr2tu\u001dwµ0t8}st6qm}st\n~%}gu/wy2x#p,Kt8}st_|/}s~<u\u001d\nt\u001buj\"~%}>t\u001bz\nrWt8{~<u<jqs~z#{y{y~\u001bB\u0012~#},\nz\u0013qs/}gz%{'#z%}swz%qswy~#2pwy1|2{z6\nr2t\nr2z#2x#t6pwyjwy2pqQ}s/Wt6qsp]z%\nuWu\u001d/}gz%qswy~#Gz#u2upQ~#Wt>u\u001dwyÎ\n2{q\u0006qs~_qsr2tmqgz#pQ*¶/~Dq\u0004~=}st8\nu/wqswy~\u001f/p1~%SzDWt6{y~<u<Mz\u0013}st\u0017t8/z\nqs{7z#{ywy\ft\u001fo,p\u0002qsr/tz\n~\u001f/pqsw\nu2z\u0013qgz\u0004z#p\n~#{{yt\nqst\u001bu\u000e/pQ~\u0006\u0004z#p\u0004qsr/t\n~%}Q}st6pQ|*~\u001f2u/wy2x\n¡±\bu/z%qgz<\nr/t\b2pQtv~#@t6z\nr\u0002qP/|*tv~#.u/z%qgzw{y{\u000e*t>wy{y{y2pqQ}gz%qst6u1wy\u0002qsr/t/t8<q\u0004pQt\nqsw~#'\nr/t>pQt6{yt\nqswy~#1~#\u000eqs2/t6p]}st\n~#}gu\u001dt\u001buWwpKx\u001fwy\ft6jwy\nz%/{yt\u0006¦vz#2uSqsr2twy2pqQ}s/Wt6q]\u001f~\u001fw\nt6p]w\nz#2{yt>§<\no:qQ}gz%2p\n}swy|/qswy~#~%@qsr2t\bpQ~#2x#p>z%p}st\n~#}gu\u001dt\u001bu1wyp|/}s~\u0013<w4u\u001dt\u001buGwyao,|2|*t62u/w\u0002o\u0006 \nr/tSu/z%qgzSKt8}st\n~\u001f{y{yt\nqst\u001buA\u0012}s~#ÕzSÚKz#jz%r\nz\u0015C½W\u001ft8\u001d*~\fz%}gu1z%q>zpsz%W|2{ywy2x1}gz\u0013qstW~#v§#§#Ï\fÒ%ÏA¿>t8}QqsW/pQwy2xAW~#2~Az%\nu\u001dw~1w/|2\u001dq\u0006z#\nupsz\u001b\ft6ujwyjÈ\u001dÉ#Ê\u0006\"~#}sjz\u0013q\u001b\nr2wyp\u0004wyqQ}s~<u/\nt\u001bujz\u001fu/u/wqswy~\u001f\nz%{\n~\u001fW|/{yt8\u001dwyqPqs~\u0017qsr2t1qgz%pQ?u\u001d2t1qs~Lqsr2tA#z%}Q\u001dwy2x\u0017{yt6\ft6{ypW~#\b2~\u001fwypQt\npQ~#Wt8qswyWt6p/t6x\u001f{ywyx\u001fwy/{t#]pQ~#Wt8qswyWt6p_2t6z%}s{n~\u0013\ftJ}s|*~\u0013Kt8}swy2x\fwynqsr/tWu2z\u0013qgzApQt8qx#t62tJ}gz%qst\u001buS\bqsr/tK}st\n~%}gu/wy2xv|/}s~\nt8pQp6\nr2wyp\u0010}st6pQ2{qst\u001bu\u0006wyz#|/|/}s~\u001b<wyjz\u0013qst6{\u0017r2z#{,z#\u0017r2~#/}\u0006~#\u0004}st\n~#}gu\u001dt\u001bu=u2z\u0013qgz\u0002}st\u001b/wy}swy/xaz#*~#/qj¦\u001bÏ#Ï¡<qst6p~#\n~\u001fW|//qst8}pqs~%}gz#x\u001ft#\n^\u0004\u0007/-@$\u0012\u0005=º\n»Û]\u000f\"\u00016\u001aW\u0016\n!>Ü\u000e\u0016*('\u0019*\u0001WÝ1\u00016\u0005\u001d\u0018:\u000f\u0012(M^\u0004\u0005\u001d\u00016\u001a\u001b\u000f\"()\u0019Ü\u000e\u0016\n()\u0019D^\u0006\u000f\u0012\u001a\u001b$\u0012\u0005¦ Þ,ß\u001dàyá\u0015â)ã#ä<åWæ2ç%ä0è§ évã#ê8ësã#êQãÞvà\"àèJä© ì0êsèJêsè\u0006í<ã\fîsï8ß\nèJðÃ ñ_ãJò\fò\nçéó\u0012êJô\u0012õ/á\u001fã#çWôEö=÷\nö#ßÒ ø/ù úUã\u0006â.ó\"ôô£àèjû0èQãJò\u001dö#ôÂ ü\u0017ã%êJçSñ_ã\fá\u0002ãSâ@ó\"ôô£àè>â)ã%úWë½ æ*îsã#ê8ësö#êQö#ß\u001få\u0013õAì)ã%ó\"ê­ û'õó\"ðýmàyá\u0015ü\u0017ã#äÓ û'õêsèsè_é,à ó\"ä\ná\u0015ü\u001eó£îgè¦\u001bÏ û\u0010þ.ó\"ä<ÿ#àè\u0001\u0000,û\u0010þbó\u0012ä\u001dÿ#àè\u0001\u0000)â.ó\"ôô£àè_æ2ôEã#ê^\u0004\u0007/-@$\u0012\u0005\u0003\u0002\u000e»_ÛK\u000f\"\u00016\u001a\u00162!>IJ(\u0010\u00016\u001a6\u0011%\r@\u0014\u0017\u0005\u001d(0\u001a\u0005\u0004_\u0016*\u000f\"\t\u001f\u0005\u001d\u0001ÝG\u00016\u0005\u001d\u0018:\u000f\"(M^\u0004\u0005\u001d\u00016\u001a\u001b\u000f\u0012()\u0019I8(\u0010\u00016\u001a6\u0011%\r@\u0014L\u0005/(0\u001a\u0007\u00061\u0007\n\u0014\u0017\u0005ÄK{z%}swy2tJq\n{\u001dqst´.w4z%2~¥\u001d~\u001f|\u001d}gz#/~j¥/z\u0013/~#|2r/~\u001f2t\b\nwy~#{wy\n2.2HMMMelodyRecognition System\nr2t\n~\u001f2pqQ}s\nqswy~\u001fW~%0~\u001f/}bWt6{y~<u<\u0015pQ|*~%qQqswy2x_p\u001dpqst6[wyp.\ft8}Q\u0006pQwywy{z%}Kqs~_qsr2z%q]~#'z\b\u0004~#}gu\u001dpQ|*~#qQqswy2xp\u001dpqst6[\"~%}mpQ|*t6t\nr}st\n~#x\u001f/wyqswy~#'=tWwy{y{u/t6p\n}sw*tjqsr2tjp\u001dpqst6X\u0012}s~# qsr/t\u0002{y~\u001b\u0004t8pqW{yt6\ft6{yp6.r2t8}stqsr2t\n~#{y{t\nqst\u001bu\u001eu2z\u0013qgzwqst8}spQt\nq\bwq\u001b\nqs~qsr/t\u0006r/wyx\u001fr2t8pq\u001b\nr2t8}st_Kt\u0015pQt6tqsr2tWt6{y~<u<ApQ|*~#qQqswy/xj}st8pQ2{qsp6*qsr\nz\u0013q\bwyp6*qsr2t~6\n\u001d}Q}st6\nt6p>wyaqsr2tu2z\u0013qgz#2z#pQt~#\u0010qsr/t_u/t8pQwy}st6uA2t8}Qo¤r2wu2u\u001dt6\n¡z\u0013}s\f~\u0013_W~<u/t6{\n¿\n¡a¡\u0010u/t6p\n}sw*t8p.z>u/~#2/{ypqs~\nr\nz%pqsw\n|\u001d}s~\nt6pQp\u0004wyWr2w\nrG~#2{qsr2t\n\u001d}Q}st6qKpqgz\u0013qst\bz%µ0t\nqspKqsr2t\nr2~\u001fw\nt~#\u0015qsr2t\u00172t8<qApqgz\u0013qst\u001f\u0006¬¦6Ã#®\u0015¿\n¡a¡paW~<u\u001dt6{>qP\u0004~:{yt6\u001ft6{ypa~#Wz\nqswywqz\u0017\u001dwypQwy2{yta{z6t8}j}st6|\u001d}st6pQt6qst\u001bu:¤qsr2tA~#2pQt8}s#z%qswy~#7u2z\u0013qgznwq|/}s~<u\u001d\nt8p6@z#2unz%n22u/t8}s{\u001dwy2x\u0002r2wu2u\u001dt6L{4z6tJ}\u0006}st8|/}st6pQt6qswy2x\u0002qsr2tpqgz%qst8pqsr\u001d}s~\u001f/x\u001fr1r/w\nrGqsr/t\bW~<u/t8{\u000e|2z#pQpQt6p6\nr2t\bqQ}gz%2pQwqswy~\u001f/p,*tJqKt6t6Cpqgz%qst8paz#2u7qsr/t\u0017~#2pQt8}s#z%qswy~#2paz\u0013}st\u0017x#~%\u001ft8}s2t\u001bu7M|\u001d}s~\u001f\u001dz#/wy{wqswyt6p\u0002{yt\u001bz%}s/t\u001bu?\u0012}s~#Áqsr2t1qQ}gz%wy2wy2xDu/z%qgzn/pQwy2x=z\n~\u001f{y{yt\nqswy~\u001f~#b\u0004t8{{E<2~\u001baqst\nr22w2t8p6_>\u001d}\bwyW|2{yt6Wt6qgz\u0013qsw~#awyp>\nz%pQt\u001bu\u001e~\u001fqsr2tj¿\n¡a¡\n~~#{mÖ_wyq\n¿\nÖ\u0006J\u0004¬ §%Ï%®\nr/wp\u0015p\u001dpqst6¨mz#pSpQt6{yt\nqst\u001bu*t\nz%2pQt\u0002wqr\nz%pS*t6t8Z/pQt\u001bu=pQ8\nt6pQp\"/{y{y¤wyZz#\u001dqs~\u001fjz\u0013qsw\npQ|*t6t\nr}st\n~#x\u001f/wyqswy~#}st6pQt\u001bz%}\nr\nz#pbwy\u0015n~~<u/{z#2u\u000et8q]z%{£2¬¦\u001bÓ%®£ b//q]z#{y{y~\u001bpqsr2tK2pQt8}bt6/~\u001f2x#rWu/t8pQwx#S\u0012}st8t\u001bu/~#Rqs~_z\u001fu/z#|/q.wq.qs~_z>\u00152pQw\nz%{\u001dqgz%pQpQ\nrAz%pv~#/}~\u001b'\nr2t]¿\n¡a¡wp'qsr2tb\nz#pQw\n22wy{u/wy/x,/{y~\n>\u0012~#}@~#/}@Wt8{~<u<\bpQ|*~%qQqst8}\u001bo[pQwyW|2{yt\u001f\u000e{yt8\u0012qQ£qs~#£}swyx\u001frq\u001b0ª\n\u001ft\u0006pqgz\u0013qstS¿\n¡a¡wyp\bqQ}gz#wy2t\u001buAqs~j}st6|/}stJpQt6q\u0006t\u001bz\nr=/~#qstS\u0012~#}\u0006r2w\nr=u2z%qgzGwp\u0006z\u001b#z#wy{z#2{yt\nÄmØ\n\t\u001d«Ù6J\u0010|2{y2pz}st6pq\npQwy{yt6\nt\u0013²%/~\u001fwypQt\u0013J:¥\u001d\nr¤z%?¿\n¡a¡wyppQr2~\u001b¤wy\nwyx#/}sta§\nr2t\u0006ª2}spqz%\nuaª\n2z#{@pqgz%qst8pz%}st/~\u001f/Et6WwqQqswy2x\u001d0pQt8}s<wy2x\u0002~\u001f2{aqs~u/t8pQwx#\nz\u0013qst>qsr2tv*t6x\u001fwy2/wy2x\u0015z#\nujt62uj~%)qsr/t>¿\n¡a¡/´@}s~\u001f2z#2wy{ywqswyt6p\u000b\r\f \u000e\nx\u001f~\u0013\ft8}s1qsr/t>qQ}gz#/pQwqsw~#2p,\u0012}s~\u001fkpqgz\u0013qst\u0010\u000fbqs~Wpqgz\u0013qst\u0012\u0011/m´@}s~\u001f\nz%2wy{wq¤u\u001dwypqQ}sw//qswy~\u001f/p\u0014\u0013\f\n\u0016\u0015\u0018\u0017}st8|/}st6pQt6q\u0015qsr2tj|/}s~#\nz#/wy{wqP¤~#vpQt6t6wy2xzax\u001fwy\ft6¤~#2pQt8}s#z%qswy~\u001f\n\u0015z%qSqswyWt\u0007\u0019_r2wy{yt1wyDpqgz\u0013qst\u001a\u000fQ\nr2t\u000b\n\f \u000ez#2u\u001b\u0013\f\n\u0016\u0015\u0018\u0017Kz\u0013}st>qsr2tv|\nz\u0013}gz#Wt8qst8}sp\u0004qsr2z%q\u0004Kt\bS/pqKqQ}gz%wG2pQwy2x\u0006qsr2tz\u001b\u001fz%wy{4z%2{yt\u0006u2z\u0013qgz\u001db\u0002~\u001f\u001d}vwyW|/{t8Wt6qgz%qswy~\u001f\u000e\u001dqsr/t\b|/}s~#\nz%2wy{ywyqPAu\u001dt6/pQwq\u00122\nqswy~\u001f2p\u001c\u0013\f\n\u0016\u0015\u001d\u0017.\u0012~#}mt\u001bz\nrjpqgz\u0013qst>z\u0013}st>W~<u/t8{t6uWz#pm«\bz%2pQpQwz#u/wypqQ}swy2\u001dqswy~\u001f2pmwqsrazS/~\u001f\u001dP/2w\"~%}sku/wz%x\u001f~\u001f2z#{\u0010#z%}swz%\nt\u001fb2(   ) b3(   ) b4(   )a13a12a22\na23a33\na34a44\na45\na35 a24s1 s2 s3 s4 s5¹\n\u000f\u0012\u0019\n\r)\u0011\u0013\u0005\u001e\u0002\u000e» \u001fA\u000f\"\u0018\u0010\u0018)\u0005\u001d(\"!5\u0007/\u0011$#\n\u0016/;?\u0014\u0017\u0016\u000e\u0018)\u0005\u001d$K\u0011\u0013\u0005< \u0010\u0011\u0013\u0005\u001d\u00016\u0005/(0\u001a\u001b\u000f\"('\u0019=\u0007n-\u0010\u0007\u001d&\u0001\u001b\u000f\u0012\t\u001e\u0014L\r\u0010\u0001\u001b\u000f\u0012\t\u001f\u0007\n$\u0004\r)(\u0010\u000f\u0012\u001a\u001f»\u0007\u0017()\u0016\n\u001a6\u0005&%!E\u0016\n\u0011j\u0005(''\u0007\n\u0014\u0017 @$\u0012\u0005*+\u001c)\nØ$*\u0016\n\u0011j\u0011\u0013\u0005\u001d\u00016\u001a\nr/t1}gz6Uz#2u/wy~nu/z%qgz\u0017u/t6p\n}swy*t\u001bu¤wyZqsr/t1|\u001d}st6<w~#2ppQt\nqswy~\u001fMz\u0013}stpqs~%}st\u001bu\u0002\u0012~#}vu2z\u0013qgz#2z#pQt\b|//}s|*~\u001fpQt8p62//qmz\u0013}stz#{ypQ~|\u001d}s~\nt6pQpQt\u001buj\"~#},/pQtwy\u0002qQ}gz#wy2wy2xSz#2uGt6#z#{y\nz\u0013qswy2x\u0015qsr/t>¿\n¡a¡p6\n~#}\n~#W|\nz%}swypQ~#1|2\u001d}Q|*~#pQt6p6)Ktt8<qQ}gz\nq\bqK~Au/wµ0t8}st8<q\bpQt8qsp~#K\u0012t\u001bz%qs\u001d}st6p6\nr/t6pQtS\u0012t\u001bz%qs\u001d}st6p)Kt8}st\nr2~#pQt6\u0015z#p\u0010pQwyW|2{ytbpqgz%}Qqswy2x,|*~\u001fwyqsp\u0010z%\nu\bwy{y{\u001d*t.qst6pqst\u001buz%x\fz%w/pq\bz\u0006pQt6{yt\nqswy~\u001fa~%@~#qsr/t8}m}st6|\u001d}st6pQt6qgz%qswy~\u001f/pwG{z%qst8},\ft8}spQwy~#2p~%\u0010qsr2wyp\u0004p\u001dpqst6A\u0010\u0002*~#qsr\nz#pQt6p6/t\u001bz\nrG}gz6Fz%\nu/wy~\u0006ª\n{yt>wypm2~%}sjz#{wy6t6u\u000e\nr/t\u0002ª/}spqS\"t6z%qs/}st8pjz%}st\u0002|/}s~<u/\nt\u001buD2pQwy/xnzAz%pq\n~#/}swyt8}qQ}gz%2p\u0012~#}s\n@@\n\bwqsrZzA¿\bz#WWwy/x1wy\nu/~\u001bV{t82x#qsrD~%>§#Ï#Ã\u001f­z%\nu\u001ezWpQ<w|\u001e~%,¦6Ï\f§\u0013Ã\nt\u001bz\nrawy\nu\u001d~\u0013\n~\u0013\ft8}sp\bz%|2|\u001d}s~\u0013\u001dwyjz%qst8{y\u0017¦\u001bÏ#ÏWpQt,+\ny¦KpQt\nJ\nr/tK\"/{y{\f}st6pQ2{qsp@~#\u001dqsr2t@@\nz%}st]qsr2t6_qQ}swyWWt\u001bu2z#2u\u001dE{ywWwqst\u001bu20qs~,2pQt\u0004~#2{_qsr2~#pQtK/w/p\n~%}Q}st6pQ|*~\u001f2u/wy2x>qs~vz,}st6z%pQ~#\nz%2{yt\u0002}gz%2x#tG~%,4}st\u001b2t6\nwyt6p\"~#}SS/pQw\nz%\nun\u0012~#}Sqsr/tWqgz#pQ=z\u0013qr2z#2u\n\u0012}s~\u001fÕÄ\nØqs~GÄ.-\u001f*~%}_z%|2|/}s~\u001b\u001dwyjz%qst6{A§#Â<¦\u0001\t\n¦6Ï#Ã#ÃA¿>\u001f \n~%}qsr/tvpQt\n~\u001f2u\u0002pQt8q\u0004~#\u000e\"t\u001bz\u0013qs/}st6p6/zpQw/x\u001f{yt>|/wq\nrjt6pqswyjz%qst,wyp\u0004pQt6{yt\nqst6u/pQwy2xvz#\u0006z#\u001dqs~\n~#}Q}st6{z\u0013qsw~#SWt8qsr/~<u\u000e\u001f¬ Ò\u001b®\nr2tb}st6pQ2{q\u0010wy*~#qsr\nz%pQt6pwyp\u0015z1pQt8}swyt6p\u0006~#~#2pQt8}s#z%qswy~\u001fD\u001ft\nqs~#}sp_}st\u001bz#u\u001dLqs~A*tW|/}s~\nt6pQpQt\u001bu=qsr/tS¿\n¡a¡p6'~#2t\u0015~#]{t82x#qsrn½\u001fÒ0qsr2t\u0006~#qsr2tJ}_~#]{yt62x%qsr\u0017~#2t\u001f\nr/t\n~#}Q}st6pQ|*~#\nu/wy/x\n¡P±\b_u/z%qgz1wyp\u0015z#{ywyx\u001f2t6uDwqsrnqsr/tW}st\n~#}gu\u001dwy2xajz\u0013q\nr/wy2x\u0017qsr/tA~\u001f/pQt8q\u0002~#\bqsr/t1ª2}spq\u0002/~#qstAwy:qsr/t1È\u001dÉ#Ê=}st\n~#}gu/wy/xwqsr\u001eqsr2t_ª2}spq>2~%qst\u0015wyaqsr2t\n¡±\b\u0004ª2{yt\u001f\bEq\bwyp>qsr2t8AqQ}gz#2p\u0012~#}sWt\u001buwyqs~az\u0002{4z%*t6{.ª\n{ytWwy\u0017qsr2tSpq\u001d{yt2pQt\u001buL\u001e¿\nÖ0r2w\nr\u0017wy{y{K|2{z6zS}s~#{yt\u0015wy1qQ}gz%w/wy2x\u0002z#2uGqst6pqswy2xW~%@qsr2t¿\n¡a¡Wt6{y~<u\u001dGpQ|*~#qQqswy/xp\u001dpqst6A\nwx#/}st>©~#/qs{ywy2t6pbqsr2wypK|\u001d}s~\nt6pQp6bW~%}gu/t8}]qs~psz\u0013\u001ftv|\u001d}s~#\nt6pQpQwy2x\u0015qswyWt\u001f\u001fqsr2wypK|\u001d}st6|/}s~\nt8pQpQw/x\nz#j*tv|*tJ}Q\"~#}sWt6uj~#\nt#\u001dz#\nuqsr/t\b}st6pQ2{qswy2xu2z%qgz\u0015pqs~#}st\u001buG\u0012~#},{z%qst8}v2pQt#\nFilesAudioRaw Raw\nAudio\nFiles\nFast Fourier\nTransformAuto-\nCorrelationMIDI/Audio\nAlignmentRaw\nAudio\nFiles\nSpectraWindowed Windowed\nAC\nPassBandFrequency Peak\nPickerMIDI to Label\nConvertorAligned\nMIDI\nVectorsFFT\nObservationHTK\nLabel\nFilesObservationPitch\n(Scalar)MIDI¹\n\u000f\u0012\u0019\n\r)\u0011\u0013\u0005\u0003/\u000e»\u00100G\u0007/\u001a6\u0007\u001e \u0010\u0011\u0013\u0016\u000e\t\u001f\u0005\u001d\u0001\u001b\u0001\u001b\u000f\"()\u0019D!E\u0011%\u0016*\u0014N\u0011\u0013\u0007<\u0003Õ\u00072\r\u0010\u0018\u0010\u000f\u0012\u0016\u0017\u001a6\u0016\u001e\u001f1!2!\u00162-@\u00016\u0005<\u0011%;\u001d\u00072\u001a6\u000f\u0012\u0016*( ;*\u0005\u001d\t\u001f\u001a6\u0016\n\u0011%\u0001n\u00072(\u0010\u0018!E\u0011\u0013\u00160\u00143!5I401I\u001e\u001a6\u00165\u001f1^76 $\u0012\u00072-.\u0005\u001d$\u0018'\u00072\u001a6\u0007Mwyqsrqsr/tm¿\n¡a¡|/}s~%qs~#q\u001d|*tz%\nu\u0015qsr/t,|\u001d}st6|/}s~\nt6pQpQt\u001buWu2z\u0013qgz\u001d\fqQ}gz%wy/wy/x\nz#=/~\u001bV|\u001d}s~\nt6t\u001bu0ALqsr2wyp|/}st6{ywyWwy\nz\u0013}Qnp\u001dpqst6A)*t\nz#/pQt~%)qsr/t>pQjz#{y{0pQwy6t\b~%)qsr2tvWt6{y~<u/w\nu/z%qgz%\nz#pQt#/*~%qsr\u0002qQ}gz%wy2wy2xSz#\nuqst8pqsw/x1\u001dqsw{ywy6t\u001bu\u0017z#{y{.~#]qsr2tz\u001b#z#wy{z#/{tju/z%qgz<\u0015ntSr2~#|*t\u0015qsr2z%q\b\u0012/\nqs/}st_K~#}sGwy{y{\u0010pQr/~\u0013Bqsr\nz\u0013qvwq>wp,|*~\u001fpQpQwy/{tqs~jz\u001fu\u001dt\u001b\nz\u0013qst6{AW~<u/t8{qsr2t\n~\u001fqst6qsp\u0010~%\nqsr2tmu/z%qgz%\nz#pQt]wyqsr/~\u001f\u001dqb}st6</w}sw/xvqsr/tKp\u001dpqst65qs~*tqQ}gz#wy2t\u001buj2pQwy/x\u0015z%{y{\u000e~%'qsr/tv\u00152pQw,\n~\u001fqgz#wy/t\u001buW<\u0015qsr2tvu2z\u0013qgz#\nz%pQt\u001f\nw}spq\u001b\u0010qsr2tS{4z%*t6{]u2z%qgzGwp/pQt\u001buLqs~ApQt8x\u001fWt6q_qsr2tSw/|2\u001dq\u0006u2z%qgzG<2~%qst1z%pSpQr/~\u0013¤wy\nwyx\u001f/}st\u0002Ã/n·bz\nrZpQt8x\u001fWt6q\u0015~#>u2z%qgzawypS/pQt\u001buqs~\u0002w/wqsw4z%{yw8tSqsr2t\u0006¿\n¡a¡}st8|/}st6pQt6qswy2xWqsr2z%q_/~#qst\u00062pQwy2x\b\nwqst8}s/wz#{ywyx\u001f/Wt6q\u001b\nr/tpsz#Wt\b\"t6z%qs/}stpQt6x#Wt6qsp,z%}st_qsr2t612pQt\u001buGqs~}stJª\n/tAt\u001bz\nr?¿\n¡a¡2pQwy/xÍ\nz#/£=t6{\nrZ}st6t8pqswjz\u0013qswy~\u001f'\nw2z#{y{\n~#\nz\u0013qst6\nz\u0013qsw~#2p,~#\u0010qsr/t\b2~%qst\b{yt6\ft6{0¿\n¡a¡pvz%|2|\u001d}s~\u001f|/}swz\u0013qst\bqs~St\u001bz\nrqQ}gz#wy/w/x7Wt6{y~<u\u001d5z%}st\n}st\u001bz\u0013qst\u001bu[z#2u5t6S*t6u2u/t6uÍ\nz%2£=t6{\nr}st6t6pqswyjz\u0013qsw~#¤wp\u0015|*t8}Q\"~%}sWt\u001buD~\u001fDt\u001bz\nr¤~#2t\u00022pQwy2xaqsr2t\n~#W|2{yt8qstpQt\u001b2t8\nt\u0006~#]\"t\u001bz\u0013qs/}stS\u001ft\nqs~#}spv\"~#}>qsr\nz%qv}st\n~%}gu/wy2x\u001d]¬ §%Ï%®\n¥\u001d\nr\u001ez\n~#\nz\u0013qst6\nz\u0013qsw~#¤wp\u0015wy{{y/pqQ}gz%qst\u001bu¤z%q\u0006qsr2tWqs~#|D~#\nwyx\u001f\u001d}stAÒ 5>\ntqsr2wypSqQ}gz%wy2wy2xLwyp\n~\u001fW|/{tJqst\u001f.qsr2t1p\u001dpqst6`wypS}st\u001bz\u001fu<=qs~\u0017z6\nt8|/qjz2t8}Q\u0015\u001089\u0015;:<\u0015\u0018=>\u0015Ø\n\u0015Ù\n\u0015-\n\u0015;?@\u0015\u0018A BCBDBE\u0015\u001dF\n¹\n\u000f\u0012\u0019*\r)\u0011\u0013\u0005HG)»\u0002\u0000 \u00016\u0005<\u0011%\u000f\u0012\u0005\u001d\u0001G\u00162!_!P\u0005<\u00072\u001a6\r)\u0011\u0013\u0005=;*\u0005\u001d\t\u001f\u001a6\u0016\n\u0011%\u0001\u001f+\n\u0015\n\u0017+\u0004\u0005('\u000e\u001a6\u0011\u0013\u0007\n\t\u001f\u001a8\u0005/\u0018!E\u0011%\u0016*\u0014 \u0011\u0013\u0007\u001d\u0003`\u0007\n\r\u0010\u0018)\u000f\u0012\u0016M\u0007\n()\u00185\u00016\u0005<\u00190\u0014\u0017\u0005\u001d(0\u001a6\u0005\u001d\u0018B\r\u0010\u0001\u001b\u000f\u0012()\u0019M\u001a\u001b9)\u0005I!5I401IG$\u0012\u0007/&-.\u0005\u001d$S\u0018)\u0007/\u001a6\u0007'H^\u00069\u0010\u000f\"\u0001=\u000f\"\u0001\u0017\u001a\u001b9)\u0005KJ@\u0011%\u00016\u001a=\u0005\u001d\u000f\u0012\u0019\n9\u000e\u001a\u0017()\u0016\n\u001a6\u0005\u001d\u0001n\u00162!\u0002\u001a69)\u00057\u00016\u0016*()\u0019LNMPO\rQSRPT\rQPT\rUWV(XHYZO\r[\\Q(]7r/t6\u0006z,pQt\u001b/t6\nt]~#/2~%qst6p@u/tJª\n2wy/xvz,2t8}Q\bwp)|/}s~\u0013<w4u\u001dt\u001bu_\bqsr2t2pQtJ}\u001b\u001f\u0004t\u0004z%}st\u0004}st\u001bz#u\u001d_qs~\n}st6z%qstKqsr/tK}st6jz#wy2u/t8}@~%*~\u001f\u001d}@}st\n~#x\u001f2wqswy~\u001fp\u001dpqst6A\n~#}_qsr/wyp\b*t6x\u001fwy/2wy2xGp/pqst8A\u000eqsr/t2tJ}Qawyp_pQwW|/{Lz#t8/z\nq.pQt\u001b2t6\nt\u0004~#\n2~#qst8p@t6qst8}st\u001buSz%p\u0010qst8<q._qsr2t\u0004/pQt8}\u001b\u001ft# x\u001d@Ð±\bØ#±\nØ_^C^C^\u000e*·\nØ Ñ\nwy}spq\u001b\n~\u001f|/wt8pv~%bqsr/t_2~%qst8E{t8\ft6{\u000e¿\n¡a¡p,jz#<wy2x2|=qsr/tj2t8}Qnz%}st\n~\u001f\nz%qst8\nz%qst6uDqs~#x\u001ft8qsr/t8}Sqs~A\u0012~#}s`z#D¿\n¡a¡}st6|\u001d}st6pQt6qswy2xSqsr/t</t8}QGz#pvzSr2~#{t#\n¥<t6t\nwyx\u001f/}st\u0015Ò Í\nt\nz#/pQt~#*qsr/t\n/}Q}st8<q.pQt8{t\nqswy~\u001fW~#0W~<u\u001dt6{ypbz#\nu\u0006\"t\u001bz\u0013qs/}st6p6\fqsr/wyp.wy{y{2pQt8t6z#\u0002t8/z\nqjz%q\nrjqs~qsr2tv|2wq\nr2t6p\u0004wyjqsr2t></t8}Q\nr/t6'/z#{y{0~#'qsr2tz\u001b\u001fz%wy{4z%2{yt\b/~#qst6pKz#\nujz_}st6pqJ²%/~\u001fwypQt\b¿\n¡a¡z%}stv|2{z\nt\u001bu\u0002wyj|\nz\u0013}gz#{y{yt6{wqsrnqsr2tSu/t6pQw}st\u001bu\u0017Wt6{y~<u\u001daqs~Az\nq\u0015z#p_ª\n{y{yt8}sp_~%}\u0015x\u001fz%}s2z#x\u001ft\n~#{y{t\nqs~#}sp6\u000e¬¦\u001b­\u0013®\n~\u0006|/}st6\ft8<q]qsr2tª\n{y{yt8}spK4}s~\u001f ~\u0013\ft8}Qr/t6{yWwy2x_qsr2t,W~#}st\n~#W|2{yt8\nz%\nu1qsr/t8}st8\"~%}st\u0015{yt6pQp>|/}s~\u001f2z#/{t\u001b,Wt6{y~<u\u001dG¿\n¡a¡0t8<qQ}Qwyqs~qsr2~\u001fpQt\u0006pqgz%qst8p>wypv|*t8\nz#{ywy6t\u001bu00t6pQ|*t\nwz%{{A\u0012~#}vqsr2~\u001fpQt\u00062~%qst6p,~\n\n\u001d}Q}sw/xw\bqsr/t.Wt6{y~<u\u001d)¸'~~\u001f|/wy2xm\u0012}s~#Fqsr/t.ª2\nz%{\u001fqs~mwy2wqswz#{pqgz%qst8pz#{y{y~\u001bp,qsr/t\b¿\n¡a¡p\u001dpqst6A</~\u001b5z\u0006S/pQw\nz#{0{z#2x#\nz%x\u001ft_W~<u/t8{£<qs~|/}s~\nt6pQp,z\u0015S\nr\u0002{z%}sx#t8},|2wyt\nt\b~%.u2z\u0013qgz\u001d2z\n~#W|2{yt8qst\bpQ~#2x\u0015pqs~#}st\u001buwy1qsr2t\bu2z%qgz%\nz%pQt\u001f\nr2t]\"t\u001bz\u0013qs/}st6p\u0010}st6|\u001d}st6pQt6qswy2xvt\u001bz\nr\u0015pQ~#2x\bwyqsr2tKu2z%qgz%\nz%pQtmz%}stKqsr/t6}s2\bqsr\u001d}s~\u001f/x\u001fr\bqsr2t\n~#2pqQ}s\nqst\u001bu_¿\n¡a¡2pQwy2x\b\nwqst8}s/w\u001fp\n~%}swy2x/¶#qsr2tpQ~\u001f/x_wyp]pQt\nqsw~#2t\u001buWz%\nut\u001bz\nrSpQt\nqswy~\u001fWwypK{z%*t6{y{t6uWz#p.qs~_wyqspb|/}s~#\nz\u00132wy{ywq\u0015~#0*t6{y~#2x\u001fwy/x>qs~\bqsr2t\u0004Wt8{~<u<\u0006~#}.qs~\b~\u001f/tm~#*qsr/t\u0004ª\n{y{yt8}sp6\nr2t{y~\nz%qswy~#2p\u0006{4z%*t6{y{yt\u001buDz#p\bqsr/tWt6{y~<u\u001d\u0017z%}st\u0015qsr2t6\u0017pQ~#}Qqst\u001bu\u0017z6\n~%}gu/wy2xqs~\bqsr2t8wy}]|/}s~#\nz%2wy{ywyqPS~%\u000e~6\n/}Q}st8\nt\u001f\f|/}s~<u/\nwy2xz>}gz%2\ft\u001bu\u0006{ywpqK~#W~\u001fpq.{ywy\ft6{\u0015~6\n\u001d}Q}st6\nt8p]~%*qsr2t\u0004Wt6{y~<u<\u0015wySqsr/tu2z\u0013qgz#\nz%pQtmpQ~\u001f/x\u001fp6qst8pqsw/x/<qsr2t8pQt,}st6pQ/{qsp\nz%\u0002*t\n~#W|\nz%}st6uSqs~qsr/t\n¡P±\b\u0010}st8\u0012t8}Qt6\ntSu/z%qgzqs~Gu/t8qst8}sWwy/t\bqsr2t6w}_z6\n\u001d}gz\n\n>\ntwy2wqswz#{ywy\u001bz%qswy~#wyp\n~\u001fW|2{yt8qst#)qsr2t\n¡P±\b\bu/z%qgz1wyp\u00062~%q\u0015/pQt\u001bunt8\nt8|/q_\"~%}\u0018`\nu\u001dx\u001fwy2xqsr2t1z6\n/}gz\n¤~#>qsr/t\u0002}st8pQ2{qsp}st8qs/}s/t\u001buD=qsr2t\u0002¿\n¡a¡p\u001dpqst6Musical\nUnit\nHMM\nP0\nP1\nP2E S\nD4C4\nRestn1=D4 n2=D4 nx=E4\nP13Query\nFillers¹\n\u000f\u0012\u0019\n\r)\u0011\u0013\u0005ba\u000e»\u0010)\u0006\u0016*(\u0010\u00016\u001a6\u0011%\r\u0010\t#\u001a\u001b\u000f\u0012\u0016*(Z\u0016\n!\u0004\u0007\u001e\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t\u001f\u00072$.$\u0012\u0007\n()\u0019\n\r)\u00072\u0019\n\u0005A\u0014\u0017\u0016\u000e\u0018)\u0005\u001d$\u0011\u0013\u0005< \u0010\u0011\u0013\u0005\u001d\u00016\u0005\u001d(\u000e\u001a\u001b\u000f\u0012()\u0019=\u0007n\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018\u0010\u000f\"\ta\u000b\u000e\r)\u0005<\u0011%3¤!E\u0011\u0013\u00160\u0014 \u001a\u001b9'\u0005n\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007\n$\u0004\r\u0010()\u000f\u0012\u001a\u001f\u001a!c!R\u0001\u0001\u001b9)\u0016\u001d\u0003W(D\u000f\"(¹\n\u000f\u0012\u0019*\r)\u0011\u0013\u0005b\u00020H_^\u00069)\u0005A\u0014\u001e\r\u0010\u0001\u001b\u000f\"\t#\u0007\n$.\r\u0010(\u0010\u000f\u0012\u001ad\u001f1!2!R\u0001\u0007/\u0011\u0013\u0005?\r)\u00016\u0005/\u0018B-b\u00162\u001a\u001b9[\u001a6\u0016C\u0014\u0017\u0016\u000e\u0018)\u0005\u001d$\b\u001a\u001b9)\u0005Z\u000b\u000e\r)\u0005<\u0011\u00133R\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3F\u0007\n()\u0018R\u001a\u001b9)\u0005J@$\"$\u0012\u0005<\u00111\t#\u00160\u0014\u0017 .\u0016*\u0001\u001b\u000f\"()\u0019D\u001a69)\u0005\u001e\u0011%\u0005\u001d\u00016\u001a\u00162!,\u0007n\u0018)\u00072\u001a8\u00072-\u0010\u00072\u00016\u0005a\u0005/(0\u001a6\u0011\u00133\nHwy\u001eqst6pqswy2x\u001d \u001eP]wq\bwyp_z#\u0017z\nqs\nz#{./t8}Q\nqsr2t}st6pQ/{yqsp\nz#L*t\u0015|\u001d}st8pQt8<qst6u1qs~qsr2t2pQt8},\"~%}_z%\nu/wqswy~\u001f\u000e\nr2tpQt\nqswy~#2wy2x\u0002z#2uAp\n~#}swy/xz%{ypQ~Wz#{y{y~\u001bpv/p\u0004qs~Sz%\nu/wqswy~\u001f\u0002qsr2t\b|/wyt\nt>r/t8}st>qsr2tvWt6{y~<u\u001dj{ywy\ft6{~8\n/}sp6/}gz%qsr2tJ},qsr2z#\u0002\u0012}s~\u001f qsr/t_*t6x\u001fwy/2wy2x\u001dEqmwypmwyW|*~#}Qqgz#q\u0004qs~\u00062~%qst>qsr\nz\u0013q\u0004qsr2t\n¡±\b]u2z\u0013qgz\u0006wypm/~#q,z_}st\u001b2w}st8Wt8<q0\u0012~#})qsr/wyp)p\u001dpqst6A\u001b//q\u000e}gz%qsr2tJ}\u0010z\n~#\u001d\u001ft62wyt6\nt\u001f'2wqswz#{ywy\u001bz\u0013qsw~#\n~\u001f2{uL*tWz6\n~\u001fW|2{ywypQr2t6uLwqsr2~#/q\bqsr/tt8/z\nq\n¡±\b>u2z%qgzj2pQwy/x~#2{Zz\u001e2~#qst\u0002{ywypq\n2~\u001e~\u001f/pQt8qjz#2uZ~%µ0pQt8qju/z%qgz\fwy:z\u001e*~~#qspqQ}gz%|WtJqsr2~<u\u000e'¬ §#Ï\u0013®@Pq,pQr2~#2{uAz%{pQ~j*t_|*~#pQpQw/{yt\u0006qs~Wwy2wqswz#{ywy6tqsr2t_p\u001dpqst8hwqsrnzjpQ2/pQt8q_~%\n¡P±\bPE{z#*t6{y{yt\u001bua}st\n~%}gu/wy2x#p_pQ/Î\nwyt6q\bqs~qQ}gz%wyLqsr/t\u0015\u00152pQw\nz#{@W~<u\u001dt6{yp>wy\u001eqsr2t\u0006p/pqst8A\nr2t\u0015t6\ft6qs2z#{\u0010x#~\fz#{K~\u001f/{4uZ*tGqs~\u0017z\u001fu/u¤qs~\u001eqsr2tGu2z\u0013qgz#\nz%pQtA~#qsr/t8}W\u00152pQw\nz#{m|/wyt\nt6p\u0012~#}r/w\nrApQ\nr1{4z%*t6{'u2z%qgz\u0015wp,22z\u001b\u001fz%w{z%2{yt\u001f\n/}Qqsr/t8}mqst6pqswy2x\u0015\u0004~\u001f/{u*t/t6t\u001bu/t6uSqs~u/t8qst8}sWwy/t\u0004qsr2tmt8Î\nz\nS~#0qsr2t6pQt|/{4z%2p\n~\u001fW|2z%}st\u001buqs~S~\u001f/}\n\u001d}Q}st6qmqQ}gz#wy2wy/x|/}s~\nt8pQp6\n3.EXPERIMENT ALRESULTS¡t8{~<u<_}st\n~#x\u001f2wy6tJ}sp@\nz#pQt6u\u0006~#\u0006*~%qsrpQt8qsp\u0010~#/~\u001f/pQt8}s#z%qswy~\u001f\u0006\ft\nqs~%}spKt8}st_qQ}gz#wy2t6uG2pQwy/xSqsr2t>qst\nr/2w2t6pu\u001dt6p\n}swy*t\u001bu1wy1qsr/t\b|/}st6<wy~\u001f/ppQt\nqswy~\u001f'\nr/t6pQtap\u001dpqst6Wp\u0004tJ}stAqsr2t6?t6\u001fz%{y\nz%qst6u?¤pQt\u001bz%}\nr2wy/x\u0012~#}\bzjpQt8{t\nqswy~\u001f\u001e~#bWt6{y~<u/wyt6p,~#b#z%}swy~\u001f/p>{yt6/x#qsr2p62\u0012}s~\u001f¾qsr/}st8tqs~qst8:2~%qst6pWt\u001bz\nr\u000e7ovpWWt8<qswy~#2t\u001buD|/}st8\u001dwy~#2pQ{Kqsr/tA2t8}swyt6pjz\u0013}st|*~#pQt\u001buDz#p\u0006z1pqQ}swy/xa~#mqst8<qs\nz%{.2~#qstW2z#Wt6p6)t\u001f x/_Ð«\nØ\nØ.·\nØ±\bØ\u001f<ÄmØ# Ñ\n~#}.qsr2t6pQtwy2wqswz#{/qst6pqsp]~%*qsr2tmp\u001dpqst6A\u001f2~_t8}Q}s~%}spbKt8}stwyqQ}s~<u/\nt\u001buawyqs~\u0002qsr/tS2tJ}swt8p6\nr2t\u0006qst6pq2tJ}swt8pv\u0004tJ}stSpQt6{yt\nqst6u2z#pQt\u001bu\u0006~\u001f\u0006qsr2t6w}@\u0012}st6</t6\n~#*~6\n/}Q}st6\ntmwy\u0015qsr2tKu2z\u0013qgz#\nz%pQtmpQ~\u001f/x\u001fppQ~qsr2z%q,pQt6\ftJ}gz#{\n~#|2wyt6p\u0004~#2{uAt8\u001dwypq,qs~W*t_pQt6z%}\nr2t\u001bu1\u0012~#}v<jqsr/tp\u001dpqst6A\nr2tmpQt6{yt\nqst6uW</t8}Q\u0015Wt6{y~<u/wyt6p]z#\nu\u0006qsr2tmpQ~\u001f/x\u001fpbwyr2w\nrqsr/t8\u0002~6\n\u001d}>z\u0013}st_x\u001fwy\ft6Gwy\nz%2{yt©\u001d\n~%}pQ~\u001fWt_{yt62x%qsr2p6/S/{yqswy|/{t/t8}swyt6pbKt8}st,2pQt\u001buSqs~z%{{y~\u001b:qsr/tpQt\u001bz%}\nrqs~~\u001f|*t8}gz\u0013qst,~\u0013\ftJ}KpQt6\u001ft8}gz#{u\u001dwµ0t8}st6q.pQ~\u001f/x\u001fp6\nr/~\u001f2x#r\u0015qsr/wp@wyp.z%z\u001fu\u001dWwqQqst\u001bu/{_pQjz%{{\u001dpQt6{yt\nqswy~#~%02t8}swyt6p\u00104}s~\u001fez>pQjz%{y{\n~\u001f{y{yt\nqswy~\u001fW~%\nWt6{y~<u/wyt6p6#\"\u001d}Qqsr2t8}@qst6pqswy/x~#az#1t8\u001d|\nz#2u/t\u001bu\u0002u2z\u0013qgz#2z#pQtwyp\n/}Q}st6qs{\u00022\nu\u001dt8}Qmz6>\ntt\u001bz\nrL2t8}QGwyp>pQt6{yt\nqst6u\u000e0zju/tJqst\nqswy~\u001f\u001e2tJq\u0004~%}s1pQwyWwy{z%}>qs~qsr2z%qKw\nwyx#/}st>Ò_wyp\n~\u001f/pqQ}s\nqst\u001buqs~_}st6|\u001d}st6pQt6qKwq\u001b.P2wqswz#{\n|*t62z#{\n^\u0004\u00072-\u0010$\u0012\u0005\u0005/0»,ÛK\u000f\"\u00016\u001a,\u0016\n!fe\r)\u0005<\u0011\u00133b!5\u0005\u001d$\u0012\u0016\u000e\u0018\u0010\u000f\u0012\u0005\u001d\u0001\u0007\n(\u0010\u0018\u001e^\u00069)\u0005\u001d\u000f\u0012\u0011\bÛb\u0016\u000e\t\u001f\u00072\u001a\u001b\u000f4\u0016*(\u0010\u0001g\nß2èJêJç hö#ôPè_æ2ôêgó\"ä<å ì)ö#ß<ä*áó\"äaæ*ö%ä<å#ð©#z\u001d ·\nØ±\nØÄ\nØ§<*Ò<\n½<2Ó©%' ·KÙ>±Ù_ÄmÙ ©\u001d\n­\u001d\u000e¦\u001bÏÃ/\nØ·\nØ±\nØÄ\nØ§<*Ò<\n½<2ÓÒ< «\bØ\nØ_·KØ\b±\bØ\u0006Ä\u0004Ø §<*½<2ÓÂ\u001d ±_Ù_ÄÙ_ÄÙÍ\nØÍ\nØ_ovØ ¦\u001bÏ½%z\u001d Ä\nÙÄ\nÙÍ\nØo\nØÍ\nØÄ\nÙ«\nØÓ½\u0013' «\bØ\b±Ù>±Ù\b·]Ù\b·KÙ\b±_Ù_ÄÙ ¦\u001bÏ­#z\u001d Ä\nÙÄ\nÙÍ\nØo\nØÍ\nØÄ\nÙ«\nØ«\nØÓ­%' Ä\u0004Ø\b±_Ø_·]Ø\nØ«\bØÄÙ\bo,Ø\u0015ÄmÙ Ò­\n ·\nÙ·\nÙ±\nÙÄ\nÙÄ\nÙÍ\nØÍ\nØo\nØ¦\u001bÏÓ#z\u001d ÄmÙ_Ä,ÙÍ\nØ_o>ØÍ\nØÄmÙ_«_Ø«\bØ«_Ø ÓÓ%' Ä\u0004Ø\b±_Ø_·]Ø\nØ«\bØÄÙ\bo,Ø\u0015ÄmÙ_«_Ø ÒÓ\n «\bØ«\bØ\b±Ù>±Ù\b·]Ù·]Ù>±Ù_ÄmÙÄmÙ ¦\u001bÏ¦\u001bÏ#z\u001d «\bØÄmÙ\u0006ÄmÙÍ\nØ_o,ØÍ\nØÄÙ_«\bØ«_Ø«\bØ Ó¦\u001bÏ%' «\nØ«\nØ±\nÙ±\nÙ·\nÙ·\nÙ±\nÙÄ\nÙÄ\nÙÍ\nØ¦\u001bÏqswyt6pm\"~%}vqsr/t\bWt6{y~<u\u001dGz#2u\u0002ª2{{yt8}vz\u0013}\np>z\u0013}stz#pQpQwyx\u001f/t\u001buaz#p6×iZj+\nkl m\n§on4prq z%}\nwyqs~Wt6{y~<u\u001d\u0002/{~\n¦Pn4prq z%}\nwyqs~Sª\n{y{yt8},2~%qst_2~%qw1Wt6{y~<u\u001d¦Pn¦\u001bÏ#Ï\u001fÏ\rqfz%}\nwyqs~Sª\n{y{yt8},2~%qst_wy1Wt6{y~<u\u001dr2tJ}st\u0014p wpqsr2t_</S*t8}\u0004qsr/t\bª\n{y{yt8}v2{y~\n<pvz#2u1Wt6{y~<u\u001d\u00022{y~\n<pwyWqsr\nz%q]2tJq\u0004~%}s\n\u0012~#}mt8/z%W|2{yt\u001f(p+\n¦8Ã_\"~#}Ä\u0004Ø\u0001\t\u001d«Ù|/{y2p]z_}st8pqz#2unzG</t8}Q\u001eWt6{y~<u\u001d\u001e/{y~\n* \nr/tpQt6{yt\nqswy~\u001fn~#\u0004qsr/t|*t6\nz%{qswt8p\"~%}\u0004ª\n{y{yt8}sp]~6\n/}Q}swy/x\u0015wyqsr2tWt6{y~<u<Swy{y{**t>z#u2u\u001d}st8pQpQt\u001bujz#x\fz%wyGwyqsr2t\u0006\"~#{{y~\u001bwy2xapQt\nqsw~#2p6W>\nt\u0006qsr2wyp_wypu/~\u001f/t\u001f't\u001bz\nr\u0017pQ~#2xAwy\u001eqsr2tu2z\u0013qgz#2z#pQtjwypSp\n~%}st\u001bu=2pQwy2xAqsr/tj2t8}QL2tJq\u0004~%}s*\u0010}st6pQ2{qswy2x\u001ewyDzpQt6x#Wt6qgz%qswy~\u001fW~%'t6z\nrWpQ~\u001f2x_wyqs~_Wt6{y~<u\u001dSz#2uSª\n{y{yt8}sp6t\u001bz\nrwqsrzApqgz%}Qq\u0015z%\nunpqs~#|=qswWtz#\nunz#¤z#pQpQ~\nw4z\u0013qst\u001buD|/}s~#\nz#/wy{wqPnp\n~%}st\u001f¯\n~\u001b_.qsr2t\u0002pQt6x#Wt6qsp\u0015{z#*t8{{yt\u001buZz#pSWt6{y~<u/wyt6p\nz#¤*tW}gz#2\u001ft\u001buD<p\n~%}stSz%\nuA|/}s~\u0013<wu/t\u001bu\u0002qs~GzWu/z%qgz%\nz#pQt\u00062pQtJ}\bz%pv}st6pQ/{yqsp6>¥<w\nt_qsr2tu2z\u0013qgz\bwyp]|\u001d}st6|/}s~\nt8pQpQt\u001bu\u000e\fqsr/tmp\n~#}swy/x\b}st\u001b2w}st6pb{t8pQpbqsr2z#S}st\u001bz%{EqswyWtqs~\n~#W|2/qst#1¢,t\u001bz#{£qswyWtWr2t8}stWwypSu/t8ª22t\u001buL<Z¦\u001bÏ#ÏApQ~\u001f/x\u001fp\u0015qswyWt6pz#Az\u0013\u001ft8}gz#x#t_~#b§%ÏSpQt\n~#\nu/p,zSpQ~#2x~%}\u001dsF§%Ï\u001fÏ#ÏpQt\u0018+\n©#©\r^ ©\u001f©Wwy0>DzaÂ\u001fÒ#Ï\n¡¿>\u0002´\u0010t6qswy2XQQ>wqsrZ§\u001fÒ%ÂaÖ_=~#,¢vo\n¡)}st\nt6wywy2x}st6pQ2{qspK4}s~\u001fiqsr2t,p\n~#}swy2xSz#{yx\u001f~%}swqsr2V}st\u001b/wy}st8pK~\u001f/{\u0002z#*~\u001f\u001dqmzWwy</qst\u001f/zSpQ|*t6t6uG2|G~#bz%|2|/}s~\u001b\u001dwyjz%qst6{\u0002©#©\u001fÏ\u001fÏ\u001fÔ\u0002\n~%}Gqst6pqswy/x/qsr2ta}st6pQ/{yqsp\u0002~#qsr/tap\n~#}swy2xZz\u0013}st\n~\u001fW|\nz\u0013}st\u001bu?qs~¤z{z#*t6{bª\n{ytr2w\nrDjz%}s<p\bqsr2tj/t8}QL{~\nz%qswy~\u001f/pSwynqsr2tju/z%qgz%\nz#pQtqsr2t\u0006pQt\n~#\nu\u001e2pQt\u0015~#bqsr2t\n¡±\bm}st8\u0012t8}st6\ntWu2z\u0013qgz\u001d \nr/wpvqst6pq_t6\u001dpQ/}st8pjqsr2z%qjqsr2~#pQt\u001eu2z\u0013qgz#\nz%pQt\u001et6{yt6Wt6qspW{z#*t6{y{yt\u001bu7z#p\n~#<qgz%wy2wy2xqsr2t\u001e/t8}QZu/~=pQ~¤z%\nuZw?qsr2tA{y~\nz\u0013qsw~#2pGx\u001fwy\ft6MDqsr/tA}st\n~\u001fx%2wy6tJ}\u001b\n\u0004~\u001e}st6pQ/{yqspz%}st1|\u001d}s~\u0013\u001dwu\u001dt\u001bu=nqsr2wyp\n~\u001fW|2z%}swypQ~\u001f\u000e\nr2tª2}spqbwypbz\b<2Wt8}sw\nz%{<ª2x\u001f\u001d}st8E~#\u0012EWt8}swq\n\n\n¡\n~\u001fWW~\u001f/{\u0015/pQt\u001buSwyt6#z#{y\nz\u0013qswy2xWpQ|*t6t\nr/E2z#pQt\u001bu1K~#}gu/pQ|*~%qQqst8}sp6\nr2t\n\n¡|/{4z\nt6p>z#2|/|*t8}QE*~\u001f/\nu1t6pqswyjz\u0013qst\u0015~#\u001eK~#}guapQ|*~#qQqswy/xGz6\n\u001d}gz\n\nqsr/t\u0006|*tJ}Q\nt6qgz%x\u001ft\u0006~#@qQ}s2tr/wyqspgz\u001b\ft8}gz%x\u001ft\u001bua~\u0013\ft8}¦qs~A¦\u001bÏz%{ypQtSz%{4z\u0013}sWp>|*t8}r2~#/}\u001b\u001d¬ §#Ï\u0013®\n\nr/t\n\n¡z%p.u/t8ª22t\u001bu¿\nÖMwyp@\"\u001d}Qqsr2t8}.u\u001dt6p\n}swy*t\u001buwyWov|/|*t6\nu\u001dwÍ\n \u0006P\u0015qsr2t\nz#pQt~#*z>Wt6{y~<u\u001dpQ|*~%qQqst8}\u001br2~\u001bKt6\ft8}\u001b#wqwypv|*~#pQpQwy2{yt\u0006\u0012~#},qsr2t\b\"z#{ypQtSz%{z%}sWpqs~W*t_wqst8}s|\u001d}st8qst\u001buAu\u001dwµ0t8}st6qs{\nr2tJAS/pq*tt8/z#Wwy2t\u001bu\u0002qs~ju/t8qst8}sWwy/t_wy@qsr/t8GpQ~\u001f22uApQwyWw{z\u0013}qs~Sqsr/t_2t8}QutMz#1wy/t8/z\nq\u001b/2\u001dqmpqswy{{'\u001fz%{yw4u02jz\u0013q\nr\u000e\nr2t\u0015\"~\u001f{y{y~\u001bwy2x1qP\u0004~1pQt\nqswy~\u001f2pz#u2u<}st6pQp\bqsr2t\u0015p/pqst8Wp>2/wy{yq\b2pQwy2xt\u001bz\nr¤~#qsr/tG~#2pQt8}s#z%qswy~\u001f¤\ft\nqs~#}spSu/t8ª22t\u001bu=wyZ¥\u001dt\nqswy~\u001f?§<=¥\u001dt\nqswy~#W©\u001d ©_u/t8p\n}swy*t6pbzvqsr2w}gu\u0015p\u001dpqst6Rqst8pqst\u001buS~\u001fSz>S/pQw\nz#{y{S2z#pQt\u001bupQ/2pQt8q]~#*qsr/t@@\nu/z%qgz<.±\bwyp\n2pQpQwy~\u001fW~%0}st6pQ2{qsp\n~\u001fWW~#\u0015qs~z#{y{~%]qsr/t6pQtSp\u001dpqst6Wp,wyp\b|/}st8pQt6qst\u001buAwyn¥<t\nqswy~\u001f\u0017©\u001d Ã\u001d´.}s~#2{yt6Wp>z#\nu\u0012/qs\u001d}st\u0004~#}sLpQ2x#x\u001ft6pqst6unLqsr/wypqst8pqsw/xaz%}stjz#u2u<}st6pQpQt\u001bunwynqsr/tª2\nz%{'pQt\nqsw~#A~#@qsr/t\b|\nz%|*t8}\u001b\n3.1Pitch-Based Recognizer>\u001d}aª2}spq\u001eWt6{y~<u<F}st\n~\u001fx#2wqswy~\u001f[p\u001dpqst6°wyp\n~\u001f/pqQ}s\nqst\u001buF~\u001f5z{yt6/x#qsrW~#2tm~\u001f/pQt8}s#z%qswy~\u001fW\ft\nqs~#}\u001b\fqsr/t\u0004\"/\nu2z%Wt6qgz#{\u0012}st6</t6\nS~%qsr/tmz#2u/wy~>pQwyx#\nz#{/z#p.t6pqswyjz\u0013qst\u001bu\u0006/pQwy2x\bz%/qs~\n~%}Q}st6{z%qswy~\u001f\no,ÄmJ\u001d¬ Ò\u0013®\nr/wypmt6pqswyjz%qs~#}Kmz#pmpQt8{t\nqst\u001buG*t\nz#/pQt\u001f\u001d~#)qsr/~\u001fpQt,qst6pqst\u001bu\u000e\u001dwqm|\u001d}s~#<wu/t6uGqsr2t\b*t8pqv|/wq\nrAt8pqswjz\u0013qst6p2pQwy2xS~\u001f/}vu/z%qgzSpQt8q\u001b\nr2t_|\u001d}st8|\u001d}s~\nt6pQpQwy2xjqswyWt_2t\nt6pQpsz%}QGqs~\nz#{\n/{4z\u0013qst\u0006qsr/t6pQt_\"t\u001bz\u0013qs/}st6p,\"~%}_z#{y{~%0qsr2tmS/pQw\nw\u0015qsr2t,u2z\u0013qgz#\nz%pQtwp]z#|/|/}s~\u001b\u001dwyjz%qst6{\u0015Ã\fÏ\bWwy</qst6p.~\u001fzSÂ\fÒ#Ï\n¡¿v´\u0010t6qswy2 QQbr2t6\n~\u001fW|2\u001dqst\u001buj2pQwy2x\n¡z\u0013qs{z#'\n~#}Gt\u001bz\nrC2t8}Q?Wt6{y~<u\u001d\u0004Kt\u0017*t8x\fz#Mqst8pqsw/x=qsr2tA}st\n~\u001fx\u001f/wqsw~#wqsr\u001ezS|*t8\nz#{qPG~%m¦Pn¦\u001bÏ\u001fÏ#Ï\u0002~#1ª\n{y{yt8}sp~6\n\u001d}Q}swy2xWwy1qsr2t_Wt8{~<u<qsr/t61qst6pqst6u1pQt6\ft8}gz%{){y~\u001b\u0004t8}v|*t62z#{qswyt6p>z%p,Kt6{y{\n8«>t6/t8}gz#{y{\n/t8|*t8\nz#{qswyt6pAKt8}stnqQ}swyt\u001buFz#pA{y~\u001f/xMz%pAqsr2t8\n~\u001fqswy\u001d/t\u001buMqs~Z}st6u/\nt\"z#{ypQt\u0017z#{z%}sWpjwqsr2~\u001f\u001dq\nz#2pQwy/x?z%Mu\u001dt\n}st\u001bz%pQtLwyMqsr/ta\u001d/S*t8}~%.z8\n/}gz\u0013qst_r2wqsp6\nr2t\b|*t62z#{qswyt6pm\u0004t8}st_/t6\ft8}\u0004{t8pQp,qsr2z#\u001e¦Pn\u001f§#Ò<\nz{ywyWwq@pQ{ywyx\u001frqs{x#}st\u001bz\u0013qst8}@qsr\nz%qsr/t]|*t8\nz#{qswyt6p\n¦Dn<¦6Ã\f\u0010z#|/|2{ywyt\u001bu\bqs~,qsr/t~%qsr2t8}ª2{{yt8}sp6\nr/t\br2wqsp6\u001dz#{ypQt\u0006z#{z\u0013}sWp6\nz\nqs\nz%{'~6\n/}Q}st6\nt6p62z#\nuª2x\u001f\u001d}st6pS~%>Wt8}swqSz\nr2wyt6\ft6uD\u0012~#}St\u001bz\nrZ</t8}QDz%\nunqsr/tG|*t62z#{qswyt6pz%|2|/{wyt\u001bu\u0002qs~Sqsr2t>ª2{y{tJ}sp,\u0012~#}qsr\nz\u0013qv#z#{y/t_z%}st_x#w\u001ft6Awy\nz%2{ytÃ/^\u0004\u0007/-@$\u0012\u0005rG)»5e\r)\u0005<\u0011%3wvW\u0005/\u0001\u001b\r)$\u0012\u001a\u001b\u0001ZÝ1\u0001\u001b\u000f\"()\u0019yx_\u000f4\u001a\u001b\t#9\u000e&{z\u0015\u00072\u00016\u0005/\u0018|\u001f1!2!vW\u0005\u001d\t\u001f\u0016\n\u0019\n(\u0010\u000f\u0012\u001c\f\u0005<\u0011g\nß2èJêgç }eñ>ó\"ôð<}[ì2Þ>ð<}iÞ,î8ôß/ã\u001fà ìmý]üij©\u001fz< ½%Ï § ½%Ï Ó\u001fÒ< Â\u001f© ¦Dn\u001fÒ%Ï\u001fÏ©#' ¦6Ó Â\u001fÓ Ò%Ï Ï\u001d Ï\u001fÏÃ\u001d Ã\u001fÏ Ï Ã\u001fÏ ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Dn\u001fÒ%Ï\u001fÏÒ ©#Ï ¦\u001bÏ ©#Ï ½%­\u001d ­\u001fÏ ¦Dn\u001fÒ%Ï\u001fÏÂ< Ã\u001fÏ ©\f§ Ã\u001fÏ Ï\u001d Ï\u001fÏ ¦Pn\u001fÒ%Ï½#z< ©#Ï ¦\u001bÓ ©#Ï Ã\f§< ­\u001fÏ ¦Dn\u001f§#Ò#Ï½%' ­ Ò §%Ï §%­\u001d Ï\u001fÏ­\u001fz< ©#Ï ¦\u001bÏ ©#Ï ½%­\u001d Â\u001fÏ ¦Dn\u001fÒ%Ï\u001fÏ­#' §%Ï Ï §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ­\n ¦6­ §%Ã §%Ï §#½< Ò#ÏÓ\u001fz< ©#Ï ½ ©#Ï Ó#©\u001d Ã\fÏ ¦Dn\u001fÒ%Ï\u001fÏÓ#' §%Ï Ï §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏÓ\n ­ ½ §%Ï §%©\u001d §#Ï¦6Ï\u001fz< §%Ï ©\u001fÏ §%Ï §%©\u001d ­\u001fÏ ¦Dn\u001fÒ%Ï\u001fÏ¦6Ï#\u000e ­ ­ §%Ï §%Ï\u001d ­\u001fÏ>\u001ft8}gz#{y{0qsr2t|*t8}Q\"~%}sjz#\ntS~%bqsr/t\u0015|/wq\nr\u001dP2z#pQt\u001bu1}st\n~\u001fx#2wy6t8}_\u0004z#p|*~~%}st8}1qsr2z#Mqsr2z%qG~#qsr2t@@\nE\nz%pQt\u001buM}st\n~\u001fx#2wy6t8}\u001bvt6pQ|*t\nw4z%{y{y\u0012~#}>2t8}swyt6pmwy1qsr/t>}gz#/x\u001ft_~#.Ä,Ù~\t\u001d«Ù#.Pqm\u0004z#p,~\u001f2pQtJ}s\ft\u001bu\u0002qsr2z%qmqsr/t|/wq\nrGt6pqswyjz%qs~#}\u0004z#p,{t8pQpvz6\n\u001d}gz%qst\b\"~%}vqsr/t6pQt>4}st\u001b2t6\nwyt6pmqsr\nz#\u0012~#}qsr2~#pQtwyLqsr/tS{y~\u001b\u0004t8}}st6x#wypqst8}\u001b\u0010pQ~Awq\b\u0004z#pt8\u001d|*t\nqst6u\u001eqsr2z%q\bqsr/t}st\n~\u001fx#2wy6t8}\u0006qQ}gz#wy2t6uL/pQw/x1qsr2~#pQt|2wq\nr\u0017t8pqswjz\u0013qst6p\b\u0004~#2{u\u0017*tz#pKt6{y{vEqmz#p>z#{ypQ~jW~#}stu\u001dwyÎ\n2{q,qs~}st\u001bu/\nt_z#{ypQt\u0015z#{z%}sWp,2pQwy/xqsr/tv|*t8\nz#{qswyt6pKwqsr/~\u001f/qK{y~\u0013Kt8}swy2xqsr2t,r2wqsp\u0004z%p]Kt6{y{£]o,pKWwyx\u001frq]*ttJ/|*t\nqst\u001bu\u000eqsr/t>2z#{ywqW~%)qsr2tz%{ypQt_z#{z%}sWp]mz#pK2~#q\u0004x\u001f~~<ujwGz#{y{8´\u0010t6\nz%{yqPnwy{y{mr2t6\nt8\u0012~#}Qqsr=}st8\u0012t8}Sqs~aqsr2tW|*t8\nz#{qP=~\u001fDª2{{yt8}sp\u0015~\n\n/}Q}swy2xFwyqsr/wyVqsr2t:</t8}QeWt6{y~<u\u001d\n~%}¤~\u001f/}¤|//}s|*~#pQt6p6Gz{y~\u001b\u0004tJ}W|*t6\nz%{q=wyW|/{wyt6p\u0015zA\u0012}gz\nqswy~\u001f¤wyqsr¤zapQjz%{{yt8}u/t6/~\u001fWwy\nz\u0013qs~%}\u001b¶\u001dr2t6\nt\u001f<{y~\u001b\u0004t8}swy2xSz\u0006|*t8\nz#{qPWwy\n}st\u001bz%pQt6p\u0004qsr2t,|/}s~#\nz%2wy{ywyqPqsr\nz\u0013qz\u0006ª\n{y{yt8}w{y{'*t\b\"~\u001f/\nu1wy\u0002qsr2t_u/z%qgzSpQt8q\u001b\n\nz%pQt6p62t8pQ|*t\nwz#{y{jr2t6\u0002{z%}sx\u001ft>\u001d/S*t8}sp]~#\u000eqsr2t6i\u0004t8}st,}st8qs/}s/t\u001bujqsr2t\bp\u001dpqst6A\n3.2FFT-BasedRecognizer\nr2tbpQt\n~#\nu_Wt6{y~<u\u001d,}st\n~#x\u001f/wyqswy~#\u0015p\u001dpqst6Fwyp)\nz%pQt\u001bu_~\u001f\bqsr/t]{yt6/x#qsr½\u001fÒ\u001d\nz#2u\u001dE{ywyWwyqst6u\u000e\f\"z#pq\n~#/}swyt8}\u0004qQ}gz%2p\"~%}s\n@@\n]~%)qsr/t>z%\nu/wy~u2z\u0013qgz\u001d\nr2tb|/}st6|\u001d}s~\nt6pQpQwy2xqswyWt.}st6</w}st\u001bu>qs~\n~\u001fW|//qst@wy\n¡z%qs{z#qsr2t@@\np_~#Kz#{y{@qsr2t\u0006S/pQw\nwyaqsr2tSu2z%qgzWpQt8q\bwp\bz#|2|\u001d}s~\u001b/wyjz\u0013qst6{ª\n\u001ft\bWwy\u001d\u001dqst6p6ovx\u001fz#wy'\bt\u001bz\nrF</t8}Q7Wt6{y~<u<Mmz%p1qst6pqst\u001buC22u/t8}AzZ#z%}swyt8qP7~#|*t62z#{qswyt6p)}gz#/x\u001fwy2x\u0012}s~#¦Dn<¦\u001bÏ#Ï\u001fÏqs~¦Dn\u001f§#Ò<\nr2t.}st6pQ/{yqsp'~#\u001dqst6pqswy2xqsr2wyp}st\n~#x\u001f2wy6tJ}Wz%}stjpQ/Wjz%}swy6t\u001buLw\nz#/{tGÒ\u001e«>t6/t8}gz#{y{@qsr2t@@\nE\nz%pQt\u001bun}st\n~#x\u001f2wy6tJ}Smz%pz%2{ytWqs~a~\u001f|*t8}gz\u0013qst\u0002}st\u001bz%pQ~\u001f2z#2{\u0017\u0004t6{y{22u/t8}qsr/t_psz#Wt_|*t62z#{q\u000e¦Pn¦\u001bÏ\u001fÏ#Ï\u001d0{y~\nz\u0013qsw/x\u0002z#{y{\u00102\u001dq~\u001f/t\u0006~%@qsr2tz\nqs\nz#{'~6\n\u001d}Q}st6\nt8p60z#2u\u000e/x\u001ft6/t8}gz#{y{\n~#2{G\"z#{ypQt\u0015z#{z%}sWppQ~\u001f/\nu<wy2x\u001epQwyWwy{4z\u0013}qs~aqsr2t\u00022t8}Q\nr/wypS}st\n~#x\u001f2wy6tJ}ju/wu=2~%qpQt6t8 qs~pQr\nz\u0013}st,qsr/t2|2|*tJ}b}st8x\u001fwypqst8}mu\u001dwÎ\n2{qswyt6p]~#0qsr2t|2wq\nr/E\nz%pQt\u001buSWt\nr/z#/wypQA@W~\u001fpq\u0015|/}s~#\nz#/{n*t\nz#2pQt\u00022~ar2z%}gu=u\u001dt\nwypQwy~\u001fZz#*~#/q\u0006qsr2t\n~#<qst8<q\u0006~%,qsr/tju2z\u0013qgz\nw£ t\u001fy@qsr/t\u0002|/wq\nr=~%mqsr2tjWt6{y~<u\u001d2v\u0004z#p\u0015}stJ2w}st\u001bu=*t8\u0012~#}st\u0002|/}s~\nt8pQpQw/xaqsr2tj~\u001f/pQt8}s\u001fz\u0013qswy~\u001f2p6LPnqsr2wyp\u0006p/pqst8A\u0004tSz#{y{y~\u001b\u0004t\u001buaqsr2t\u0006¿\n¡a¡qs~\u0002{yt\u001bz\u0013}s\u0017pqgz\u0013qswypqsw\nz#{@}st6|/}st8pQt6qgz%qswy~\u001f/p\b~#qsr2t\n~\u001fqst6q~%Kqsr2t\u0015pQwyx\u001f\nz%{\u0010}gz%qsr/t8}_qsr\nz%nu\u001dt8ª\n/w/x\u0002qsr/tSWt6{y~<u\u001dw\n~#<qst8<q\nqsr2t,|2wq\nr*b*tJ\"~#}st>z#|/|2{\u001dw/xpqgz%qswypqsw\nz#{0qs~~\u001f{yp\u0004qs~wq\u001b.W~\u001fpq\nz%pQt6p60wqv\u0004z#pv|*~\u001fpQpQwy2{ytqs~jz\nr2wyt6\ft\u0006z#p,S\nr\u0002}st\u001bu/\nqswy~\u001fawyz%{ypQt\u0015z%{z%}sWp,z#pvu/t6pQw}st\u001bu\u0002wyqsr/~\u001f\u001dqpsz\n}swª\nw/xWpQ6\nt6pQp\"/{)r2wqsp6^\u0004\u00072-\u0010$\u0012\u0005a\u000e»eW\r'\u0005\u001d\u0011\u00133vj\u0005\u001d\u0001\u001b\r\u0010$4\u001a\u001b\u0001CÝ1\u0001\u001b\u000f\"()\u0019¹¹\n^_&{z\u0015\u00072\u00016\u0005/\u0018\u001f1!c!vW\u0005\u001d\t\u001f\u0016\n\u0019*(\u0010\u000f4\u001c\f\u0005<\u0011g\nß2èJêJç }eñ\bó\u0012ôð@}eì/Þ\bð<}eÞî8ôß2ã#à ìýbüiZj©#z\u001d ½#Ï ¦ ½#Ï ¦6Ï\u001fÏ< Ï#Ï ¦Pn¦\u001bÏ\u001fÏ#Ï©%' Ã\fÓ ½ Ò#Ï ­\u001f©< §\u0013ÃÃ/ Ã\fÏ © Ã\fÏ Ó\u001fÂ<¦6Ï ¦Pn¦\u001bÏ\u001fÏ#ÏÒ< ©\u001fÏ Ï ©\u001fÏ ¦6Ï\u001fÏ< Ï#Ï ¦Dn\u001fÒ#ÏÂ\u001d Ã\fÏ ¦\u001bÏ Ã\fÏ Ó\u001fÓ< §#Ò ¦Pn¦\u001bÏ\u001fÏ#Ï½%z\u001d ©\u001fÏ Ï ©\u001fÏ ¦6Ï\u001fÏ< Ï#Ï ¦Pn¦\u001bÏ\u001fÏ#Ï½\u0013' §#Ï ¦\u001b© §#Ï ¦6Ï\u001fÏ< Ï#Ï­#z\u001d ©\u001fÏ Ï ©\u001fÏ ¦6Ï\u001fÏ< Ï#Ï ¦Pn¦\u001bÏ\u001fÏ#Ï­%' §#Ï Ï §#Ï ¦6Ï\u001fÏ< Ï#Ï­\n §#Ï §#Ï §#Ï ¦6Ï\u001fÏ< Ï#ÏÓ#z\u001d ©\u001fÏ Ï ©\u001fÏ ¦6Ï\u001fÏ< Ï#Ï ¦Pn¦\u001bÏ\u001fÏ#ÏÓ%' §#Ï Ï §#Ï ¦6Ï\u001fÏ< Ï#ÏÓ\n §#Ï ¦\u001bÏ §#Ï ¦6Ï\u001fÏ< Ï#Ï¦\u001bÏ#z\u001d §#Ï ¦\u001bÏ §#Ï ½#Ó< Â#Ï ¦Pn¦\u001bÏ\u001fÏ#Ï¦\u001bÏ%' §#Ï ¦\u0013§ §#Ï ¦6Ï\u001fÏ< Ï#Ï\n3.3Scale-Based Recognizeroqsr/wy}gu\u0017p/pqst8 2z#pQt\u001bu=~#DzApQ2/pQt8q\u0006~#qsr2t@@\n~\u001f/pQt8}s#z%qswy~\u001f\ft\nqs~#}.u/z%qgzmmz#p@wyW|/{t8Wt6qst\u001bu_z#2u_qst8pqst\u001bu\u0015z%p\u0010\u0004t6{y{\nw}spq\u001b%qsr2~\u001fpQt@@\n/w/p\n~#<qgz%wy2wy2xjqsr2t\"/\nu2z%Wt6qgz#{)4}st\u001b2t6\nwyt6p_z%\nuar\nz\u0013}QW~\u001f/w\npK~%\u000eqsr/t2~#qst8pKwyWqsr2tm}gz#/x\u001ft_Ä\nØ\t<«\nÙ\n~%\u000eqsr/tt\u001b\nz#{y{qst6|*t8}st\u001bu\u0015p\nz#{yt\u0013\u0010\u0004t8}stwu/t8<qswª2t\u001bu\u000e\nr2t8\u0006\u0004t8}stmqsr/t6S/pQt\u001bu\u0015qs~\bpQt6{yt\nq\u0004zpQ2/pQt8q,~#K§\u001fÒS~%@qsr2t\u0006½#ÒW/wy2p\n~\u001fW|*~#pQwy2xqsr2t_|\u001d}st6<wy~\u001f2pQ{\nz#{\n\u001d{z%qst\u001bu@@\n\"t6z%qs/}st>\ft\nqs~%}\u001b\nr2t>p\u001dpqst6mz#pu\u001dt6pQwyx\u001f/t\u001bujqs~\u0006~#µ0tJ}z\n~\u001fW|\u001d}s~\u001fWwypQt\u0015*tJq\u0004t8t6a\"~#}\nw/xAzWr\nz\u0013}gua|2wq\nrLu\u001dt\nwypQwy~\u001fn*tJ\"~#}st|/}s~\nt6pQpQwy2x\nz#pbwySqsr2to,ÄD|2wq\nr\u0006\"t\u001bz\u0013qs/}st\u0013.z#2ut8<qQ}gz\nqswy/x_z>\"t\u001bz\u0013qs/}stWpQtJq\u0015/pQwy2xA/~ApQ|*t\nwª\nz#{y{=S/pQw\nz#{.<2~\u001b{yt\u001bu/x#t\nz#pwynqsr2t@@\n\u0012t\u001bz%qs\u001d}stG\u001ft\nqs~#}JJ\nr2tj|/}st6|\u001d}s~\nt6pQpQwy2x\u001eqswyWt\"~%}Sqsr2wyp\"t\u001bz\u0013qs/}stbpQt8q@wyp\u0010/t6x\u001f{ywyx\u001fwy/{t#\fpQwy\nt]wq@pQwyW|2{\bjz%pQ\u001dp\u000eqsr2t]|/}st8\n~#W|2/qst6u@@\nu2z\u0013qgz\u001d\nr/t,</t8}Q\u0015u2z\u0013qgz\bz%}stmqst6pqst\u001buSz#p.*t8\"~%}st,z#\nu\u0006qsr2t\u0004}st6pQ/{yqsp]z%}stmpQ/jz\u0013}swy6t\u001bu¤w\nz%2{ytaÂ<\nr2t6pQt\u0002}st6pQ2{qsppQ/x\u001fx\u001ft8pqWqsr\nz%qSqsr2t\u0002p\nz#{yt82z#pQt\u001bu1}st\n~\u001fx#2wy6t8}>pqQ}sw\u001ft6p\bzx#~~<u\u001e2z#{z#\nt*t8qKt6t6\u001eu\u001dwp\nz%}gu/wy/xz%p]\u0015\nrS/22t\nt6pQpsz%}Q\u0015wy/\u0012~#}sjz%qswy~#jz#p.|*~\u001fpQpQwy2{ytr2wy{ytm}st8qgz#wy2wy/xz%pv\u0015\nr1wy/\u0012~#}sjz\u0013qsw~#az#p,wq\nz#Aqsr2z%q,wyp}st8{t8\u001fz%<qqs~qsr/wp\u00152pQw\nz#{\u0010qgz#pQ*vPq>pQ6\nt8pQp\"2{y{aª\n2u/pvz#{y{@~#bqsr2t~6\n/}Q}st6\nt6p\b~%Kt\u001bz\nr/t8}Q\u0013x\u001ft6/t8}gz#{y{wqsr2~\u001f\u001dq@|/}s~<u\u001d\nwy/x,\"z#{ypQtmz#{z\u0013}sWp)r2w\nr\u0015pQ~#2\nuqs~~\u001eu\u001dwypQpQwWwy{z%}\u0006\u0012}s~#¨qsr2tj2t8}QjEq\u0006pQt6t6Wp_qs~A~\u001f|*tJ}gz%qst\u0002z#p\u0004t6{y{z%p\b~#}\b*t8qQqst8},qsr\nz#aqsr/t@@\nE\nz%pQt\u001buA}st\n~#x\u001f2wy6tJ}_r2wy{yt\u0015}st\u001bu\u001d\nwy2xqsr/t>pQwy6t\b~%'qsr/tv\u0012t\u001bz%qs\u001d}st>\ft\nqs~#}sp\u0004jzz\nqs~#}~#\u000eqsr/}st8t\nz#2uqsr\u001d/pqsr/t_pqs~#}gz%x\u001ft}st\u001b2w}st6Wt6qsp6<qsr2t_<2\u0015*t8}m~%@\u0012}st6t|\nz\u0013}gz#Wt8qstJ}spqs~qQ}gz%wy'*z%\nujqsr2t\b~\u0013\ft8}gz%{y{)|/}s~\nt6pQpQwy2xqswyWt\u001f ^\u0004\u0007/-@$\u0012\u0005c\u000e»e\r)\u0005<\u0011\u00133vW\u0005/\u0001\u001b\r)$\u0012\u001a\u001b\u0001:ÝG\u0001\u001b\u000f\"()\u0019 Ü'\t\u001f\u00072$\u0012\u0005&{z\u0015\u00072\u00016\u0005/\u0018|\u001f1!2!vW\u0005\u001d\t\u001f\u0016\n\u0019\n(\u0010\u000f\u0012\u001c\f\u0005<\u0011g\nß2èJêgç }eñ>ó\"ôð<}[ì2Þ>ð<}iÞ,î8ôß/ã\u001fà ìmý]üij©\u001fz< ½%Ï Ï ½%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Dn<¦6Ï\u001fÏ©#' Ò%Ï Ï Ò%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏÃ\u001d Ã\u001fÏ Ï Ã\u001fÏ ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Pn\u001f§#ÒÒ ©#Ï Ï ©#Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Pn\u001f§#ÒÂ< Ã\u001fÏ ¦\u001bÏ Ã\u001fÏ ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Dn<¦6Ï\u001fÏ½#z< ©#Ï © ©#Ï ­\u001f§< Ï\u001fÏ ¦Dn<¦6Ï\u001fÏ½%' §%Ï ¦\u001f¦ §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ­\u001fz< ©#Ï Ï ©#Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Dn<¦6Ï\u001fÏ­#' §%Ï Ï §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ­\n §%Ï ¦\u001f¦ §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏÓ\u001fz< ©#Ï Ï ©#Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ ¦Dn<¦6Ï\u001fÏÓ#' §%Ï Ï §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏÓ\n §%Ï ¦\u001bÏ §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ¦6Ï\u001fz< §%Ï ¦\u001bÏ §%Ï ½%Â\u001d Ó\u001fÏ ¦Dn<¦6Ï\u001fÏ¦6Ï#\u000e §%Ï ¦\u001bÏ §%Ï ¦\u001bÏ#Ï\u001d Ï\u001fÏ\n3.4Common Results\nr/t8}st\u0015z%}stpQt6\ft8}gz%{\n~#WW~\u001fGqsr\u001d}st\u001bz\u001fu\u001dp,}s/2/w/xqsr/}s~#2x\u001frGqsr2t_}st8pQ/{qspv|\u001d}st6pQt6qst\u001bu1z%*~\u0013\ft\u001f\nr2t\bª2}spq,~%@qsr2t6pQt_wypqsr\nz%q\u001b/wyAx\u001ft82t8}gz%{£qsr/t\bx#}st6z%qst8}qsr2tv|*t6\nz%{qswt8p62qsr2tvW~#}st>\"z#{ypQtz#{z\u0013}sWpvz%\nujr2wqsp,z#{{y~\u001b\u0004t6uA<\u0002qsr2t_p\u001dpqst6A/z%\nuGqsr/t_{~\u001bKt8}>qsr/t_|*t6\nz%{yqswyt6p6/qsr2t_W~%}st\"z#{ypQtz%{z%}sWpvz%}st_}st\u001bu/\nt\u001bu\nqsr/~\u001f/x\u001frApQ~#Wt8qswyWt6p,z%q,qsr2t_t8\u001d|*t6/pQt~%qsr2tjr2wqsp6 FP=qsr2t\nz%pQtG~%,qsr/tjpQr2~#}QqstJ}2tJ}swt8p6.wyqSwyp\u0015W~%}st}st6z#pQ~\u001f2z#/{tqs~\b}st\u001bu\u001d\nt\u0004qsr/tm|*t6\nz%{yqswyt6pb*t\nz%2pQtmqsr2tJ\u0015qst62u\u0015qs~\b|\u001d}s~#u\u001d\ntzG{z%}sx#t\u0002</S*t8}>~#mz6\n\u001d}gz%qstWr2wqsp6W¿v~\u001b\u0004t6\ftJ}\u001b\u0010z%p\u0015/t8}swyt6p*t\n~\u001fWtb{y~\u001f2x#t8}\u001b#wyq\u0010*t\n~\u001fWt8p)W~#}st]u/t8pQwy}gz%2{ytKqs~,z#{y{y~\u001bZW~#}stb2~\u001f\u001dtJ2z\nq\nÐPwy\nz8\n/}gz\u0013qst6Ñjz%q\nr2t6pqs~W*t\u0015|\u001d}st6pQt6qst\u001buaz%p,}st6pQ/{qsp60pQ~wy\n}st\u001bz#pQwy/xnqsr2tG|*t6\nz%{yqswyt6pqs~\u0017z#{y{y~\u001bUW~#}stjz#{ypQt\u001ez%{4z\u0013}sWpWjz#\ft8ppQt82pQt\u001fLPq\u0006wyp\u0015K~#}QqsrD/~#qswy2xAqsr2z%q\u00152\nu\u001dt8}\u0006jz#nu/wµ0t8}st6q\u0006{yt6\ft6{yp~%)|*t8\nz#{qswyt6p6\u001d2t6z%}s{jz#{y{*~%\u000eqsr/t>u\u001dt6pQw}st\u001bu}st6pQ2{qspKKt8}stvz6\n\u001d}gz%qst6{}stJqs/}s2t6u\u0017\u001eqsr2tWp\u001dpqst6A\nr2wyp\u0006wyW|2{ywyt6p_qsr\nz%q\u0006|*t62z#{qswyt6p\n~\u001f/{4u*tm/pQt\u001bu\u0006qs~>u\u001d\u001d\nz%Ww\nz#{y{\n~\u001fqQ}s~\u001f{<qsr2tK{t8\ft6{\u001d~#*wy2tJ2z\nq.}st6pQ|*~\u001f/pQt6p|\u001d}s~\u0013\u001dwu\u001dt\u001buGjqsr2t\bp\u001dpqst6APaz#{y{)qst8pq\nz%pQt6p6\nqsr/t8}st_wyp\bz#{ypQ~jpQ~\u001fWtJqsr2wy2xSqsr/t\n\n¡u/~t6p,2~%qqst8{{\u000e2p6×@qsr/t\b\nz%{ywyqPj~#\u0010qsr/t>pQt6x#Wt6qsp\u0004wq{z#*t6{yp,z#pmz%{ypQtz#{z%}sWp6P1qsr2t\nz%pQt~#.\u0004~%}gu/pQ|*~#qQqswy/x/0u\u001dwpqswy/x\u001f2wypQr/w/x\n{y~\u001fpQt\u00062\u001dq,wy{u/{u\u001dwµ0t8}st6q>K~#}gu/pv{yw\u001ftnÐx#~\fz\u0013qsÑ\u00024}s~\u001fdÐPx\u001f~\u001fz%qst6t6ÑGjz#\ft8pvpQt6/pQt\u001f\u000e2\u001dqwy\u0002S/pQw\npQ\nr1z\n{y~\u001fpQt>jz%q\nrW~#\u0012qst8GpQ~#2\nu\u001dp\u0004pQwyWwy{4z\u0013}mt6/~\u001f2x#r\u0002qs~\n~\u001f2q\u0006z%p\u0015zGx\u001f~~<unjz\u0013q\nr\u000ej>2tSt8/z#W|2{yt\u0015~#\u0004qsr/wp_|/r2t6/~\u001fWt6/~\u001f4}s~\u001fXqsr2tWqst6pqSu2z\u0013qgz\u001ewyp\u0006pQr2~\u001b¤wy\nwx#/}stGÂ\u001d\u0017«>t6/t8}gz#{y{@qsr2~#pQtwy/t8/z\nq\u0006}st6pQ2{qsp\"~#2\nu=\u001eqsr2tjp\u001dpqst6f}st6pQ/{qst\u001bun\u0012}s~#\n~#2pQ~#{wu/z%qswy~\u001fn~#K}st8|*t\u001bz%qst\u001bu\u00172~#qst8p_wLqsr2tW/t8}Q\u001ewy<qs~G~\u001f/t{y~\u001f2x#t8}\u00062~%qstwynqsr/tS}st6pQ|*~#2pQtj~#}\u0012}s~#Xu/t6{yt8qswy~#D~#z1/~#qst4}s~\u001f¨qsr2tW/t8}Q\n\nr<2pqsr2t\n\n¡wyp>*~%qsra2pQt8\u00122{@z%\nuau/t\nt6|/qswy\ft#\npQwy\nt\u0006jz#G~#qsr2tjz%{ypQtaz#{z%}sWp\u0006\u0004~#2{uZ*tj\u0004~%}Qqsr?|\u001d}st6pQt6qswy2xaqs~Lz\u0017u/z%qgz%\nz#pQt2pQtJ}>z%p|*~\u001fpQpQwy2{ytjz%q\nr2t6p\u0004qs~zSWt6{y~<u/w\n2t8}Q«\bØ «\bØ ±_Ù ±\bÙ ·KÙ ·KÙ ±_Ù ÄmÙ ÄÙÍ\nØS( {3~ S( ~~      (\\  S~\ft8}spQ/p«\nØ±\nÙ·\nÙ±\nÙÄ\nÙÍ\nØ ~ \u0001 ¡\r E¢{E£\n\u0016   ¤  { ¤¹\n\u000f\u0012\u0019*\r)\u0011\u0013\u0005r0»:\u0000d\u000b'\r)\u0005<\u0011\u00133e\u0014\u0017\u0005\u001d$\u0012\u0016\u000e\u0018)3[!E\u0011\u0013\u00160\u0014¦¥¨§\u0018[\\©Zª\r«¬o­®¥¨§\u0018[\\©Zª\r«¬o­¯\n[~°°¤«±¬²LZ°O\rQ\u0006\u00072(\u0010\u0018Z\u0007\u001e\u0001\u001b9'\u0016\n\u0011\u0013\u001a6\u0005<\u0011j\u000f\"()\u0005(''\u00072\t\u001f\u001a\u001f+)-@\r)\u001a\u0015\u0001\u001b\u000f\u0014n\u000f\"$\u0012\u0007/\u0011\u0002\u00016\u0016*\r\u0010(\u0010\u0018'&\u000f\"()\u0019'+]\u0014\u0017\u00072\u001a\u001b\t%9:!E\u0011\u0013\u00160\u0014³Y´Q$¬$QP¬1µ¶O·MP¸¹UZ¬oº\n4.CONCLUSIONS ANDFUTURE WORKPq\u0006wyp\u0015z#|2|2z%}st6q\u0012}s~#¨qsr2wyp\u0006K~#}s\u001eqsr\nz\u0013q\u0015qsr/tWWt6{y~<u\u001d\u001e}st\n~#x\u001f2wqswy~\u001fp\u001dpqst6Wp'u/t6p\n}swy*t\u001bu\br/t8}st.jz6>pqgz%\nu\b~\u001f/{z#p)z\u0004|/}s~~#4E~#\u0012\n~#\nt8|/q\u001b7r/w{ytW\u0004tW\u0012t6t6{\u0004wq\u0015wp\u0015zApQ8\nt6pQp\"/{Kª2}spq\u0012~#}gz6@qsr2t8}stjwyp\u0015zAx#}st\u001bz\u0013qu/t6z#{b~#m\"\u001dqs/}stSK~#}s\u001eqsr\nz\u0013q\u0015pQr/~\u001f/{4uL*tju/~#2tSqs~AwyW|\u001d}s~%\u001ftSqsr2t6A\nr2wypK~#}s\nz%:*t1x%}s~\u001f/|*t\u001buZwyqs~\u0017qsr\u001d}st6tAz\u0013}st\u001bz#p6×Gqsr\nz\u0013q}st6{z%qswy2xqs~jqsr2t\u0015u2z%qgz%\nz%pQtz#2uawyqsp\n~\u001fqst6qsp6*qs~jqsr2t\u0006¿\n¡a¡}st\n~\u001fx\u001f/wy6t8}sp6z#2u¤qs~\u001eqst6pqswy2xLqsr2t\u0002p\u001dpqst6AZov{y{m~#vqsr2t6pQtGpQ2x#x\u001ft6pqswy~\u001f/pjpQr\nz%}stqsr2t\u001et6\u001ft6qs\nz#{>z#wy ~#\u0006|/}s~\u0013<wu/wy2x¤zn}gz6¨z%\nu\u001dw~%E\nz#pQt6uC\u00152pQw\nz%{u2z\u0013qgz#2z#pQt_r2w\nr1wypm\u0012}swyt6\nu\u001d{Gqs~/~\u0013\u001dw\ntz%\nu\u0002t8\u001d|*t8}Qq2pQt8}vz%{wy\ft#\nr2tJ}st\u0004z%}st\u0004z,<2\u0015*t8}'~#\nwyW|\u001d}s~%\u001ft6Wt6qsp*qsr\nz%q\u0010pQr2~\u001f/{u\u0006*tbjz\u001fu\u001dtbqs~qsr2t>u2z\u0013qgz#\nz%pQt\bz%\nuWqsr2t\bu/z%qgz_r2w\nrGwq\n~#qgz#wy2p6.=t\u0004~#2{uj{wy\ftqs~qst6pq)qsr/wp'p\u001dpqst65~#\u0015z\n~#{{yt\nqsw~#S~%/}st\n~#}gu/wy/x\u001fp)4}s~\u001fRz\n~\u001f/pqswwy2pqQ}s/Wt6qsp6]}gz\u0013qsr2t8}qsr2z#¤\u0012}s~#Áp\u001d<qsr/t6pQwy6t\u001bu¤~\u001f/t6p6\u0004pQwy\nt1qsr2tx\u001f~\u001fz#{\u0010~%@qsr2t_p\u001dpqst6kwypqs~~\u001f|*t8}gz\u0013qst~\u001fApQ\nrau/z%qgz<\u0004¸\u000ew\u001ft8wypQt\u001f\n\u0004t\u0004~#2{u\u0015{ywy\ftKqs~,qst6pq@qsr2tKp/pqst85wqsrS|*~\u001f{\u001d|2r/~\u001f/w\nu2z\u0013qgz\bz#p\u0010Kt6{y{2z%pW~\u001f/~\u001f|/r2~\u001f/w\n\u0013qsr\u001d/p)}st6|\u001d}st6pQt6qswy2xvqsr/tK#z#pq@jzD`~%}swq~#/}st\n~%}gu/t\u001buS/pQw\n\u001e=t\u0002z#{ypQ~\u001emz#q\u0006qs~\u001ew\n}st\u001bz%pQtGqsr2tjpQwy6t\u0002~#>z#Du/z%qgz%\nz#pQt~\u001fLr2w\nr\u0017\u0004tW~#|*t8}gz%qstqs~G*tW~#}st\u0015}st\u001bz#{ywypqsw\n@|/}stJ\"t8}gz%2{\u0017r/wy{t}st8qgz%w/wy2x?z#CwyW|2{yt6Wt6qgz%qswy~#Mqsr\nz%q1u/~t8pA2~%q1}st\u001b2w}stLr\nz%\nu\u001djz%}s<wy2x\u0006qsr2t\n~\u001fW|2{yt8qst\n~\u001fqst6qspm~#)qsr2t_u/z%qgz#2z#pQt\u001f\nwy\nz%{y{y\u001d\u0004t\u0004~#2{uW{yw\u001ftqs~_wyW|\u001d}s~%\u001ft\u0004qsr2twy<qstJ}Qz\nw/x~#0qsr2t,u2z\u0013qgz#2z#pQt\"~#}]qsr2t*t6/t8ª2q~%@*~#qsr1qsr/t>}st6pQt\u001bz\u0013}\nr/t8}>z#\nujqsr2t\bt8\ft6qs\nz%{\u000et8\nu\u001dE/pQt8}\u001b\nr2tJ}stvz\u0013}stvjz%Sz%pQ|*t\nqsp]~#*}st6pQt\u001bz\u0013}\nrj~#W¿\n¡a¡p.\"~#}\u0004z#\u001dqs~\u001fjz%qswpQ|*t6t\nr\u001e}st\n~\u001fx#2wqswy~\u001fnz#\nuAK~#}gu/pQ|*~%qQqswy2xGqsr2z%qv\u0004t\u0015K~\u001f/{4u\u001e{ywy\ft\u0015qs~t8\u001d|2{y~%}st\bwy\n~#¹`2\nqsw~#Gwqsr1~#/}Wt6{y~<u<}st\n~#x\u001f2wqswy~\u001f1p/pqst8APqK~\u001f2{uA*t_#z#{y\nz%2{yt_qs~Wt8\u001d|*t8}swyWt6qmwqsrAqQ}gz#wy2wy2xqsr2z%qvu/~t8p2~%qm}st\u001b2w}st>\u00122{y{'{z%*t6{y{wy/xW~#)qsr/t_u2z\u0013qgz\u0006\u0012~#}vz#{y{'u2z%qgz%\nz%pQt_t6qQ}swt8pz#2u>wqsrqQ}gz%wy2wy2x~\u001f~#2{_zmpQ2/pQt8q'~#<qsr2tbu2z\u0013qgz\u001d\u0010nt@\u0004~\u001f/{u\b{wy\ftqs~Sr2z#2u/{yt_u/wµ0t8}st8<q,z%2pqQ}gz\nqswy~\u001f2p,~#@</t8}Q\u0002z#2uGu/z%qgz%\nz#pQtu/z%qgzz#p,~#qsr/t8}vp/pqst8Wpvr2z\u0013\u001ftu/~\u001f/t_2pQwy2xjWt6{y~<u\u001dw_\n~\u001fqs~\u001f\u001d}spg2z#pQt\u001bu~\u001f\u00022pQtJ}\n~\u001f\u001dª*u/t8\ntvwjqsr2t8wy}m</t8}Q\nr/wp\u0004wy{y{0{yw\u001ft6{\u0002}st6</w}st>2pqs~\b}st6pQt\u001bz\u0013}\nrjz>u/wµ0t8}st6qbpQt6{yt\nqswy~\u001fj~%*\"t\u001bz\u0013qs/}st\ft\nqs~%}sp@qsr\nz#\u0015qsr2~\u001fpQt|/}st8pQt6qst\u001buGr/t8}st\u001f.=t>wy{y{\u0010z%{ypQ~mz#qqs~t8/z%Ww/t\br2~\u001bBu/wµ0t8}st8<q\"t6z%qs/}st,\ft\nqs~#}sp\u0004z\u0013µ0t\nq]qsr2t,\nz#{ywqS~#\u000e~\u001f\u001d}]}st8pQ2{qsp6¶<\"~#}Kt8/z#W|/{t#zG*tJqQqst8}|2wq\nrnt8<qQ}gz\nqs~#}\nz#p_wy=«>~#qs~=¬ Ó%®\u0012\bWwyx\u001frqz%{{y~\u001b2p>qs~2pQtvpQjz#{y{yt8}m~#2pQt8}s#z%qswy~\u001f\u0002\ft\nqs~#}spKwqsr2~#/qmpsz\n}swª\nwy/xz8\n/}gz\n~#}@z\n~#2pqgz#qQ¤»?qQ}gz%2p\"~%}s ¬ §\u0013®\u001dWwyx\u001frq@z%{y{~\u001bZ2p'qs~v/pQtK~\u001f/{_qsr\nz%q\u0012}st6</t6\nwy/\u0012~#}sjz%qswy~#G|*t8}Qqgz%wy2wy2xqs~_qsr2t,S2pQw\nz%{\np\nz%{t#\u0004¥<\nr\nr\nz#/x\u001ft6p1Wwx#rqAz%{{y~\u001b¨/pGqs~¤jz#wyqgz#wyC~#}1w\n}st\u001bz%pQt=z8\n/}gz\nr2wy{yt_}st\u001bu/\nwy2xSqsr/t_<2S*tJ}m~#.\u0012}st8t\u0006|2z%}gz%Wt8qst8}spt6pqswyjz%qst\u001buG<qsr2tbp\u001dpqst6Rz%\nu_qsr2tb~\u0013\ft8}gz#{y{<|/}s~\nt6pQpQwy2xvqswyWt.}st\u001b2w}st\u001bu0\nt\u001bz\u0013qs/}stpQt6{yt\nqswy~#\u0002wy{y{0z#{ypQ~\u0006wW|2z\nqbqsr2tmqP/|*t8p]~%'wy/|2/qKz8\nt6|/qst6u<\u0015qsr2tp\u001dpqst6A¶<z\u0006u/wµ0t8}st6q]pQt6{yt\nqswy~\u001f\u0002~#\u000e\"t6z%qs/}st8p\u0004Wwyx\u001frqbjz#\ftwq\u0004t\u001bz%pQwyt8}\"~%}vqsr/t\bp\u001dpqst6 qs~z6\nt6|\u001dq,pQ22x</t8}swyt6p6\u001d\"~#}vt8/z#W|/{yt\u001f\nt\nr/2w2t6p>{wy\ftSw\n~#}s|*~%}gz%qswy2x12w,z#2u\u001eqQ}swEx#}gz%Wp6\u0010z%p_wpu\u001d~\u001f/twyB{4z\u0013}sx\u001ft=\u001f~\nz#/2{z%}Q\n~#<qswy<2~#2pApQ|*t8t\nrC}st\n~#x\u001f/w8t8}sp6\u0006Wwx#rqz%{ypQ~_wW|\u001d}s~\u0013\ftKz6\n\u001d}gz\n\fz#p@Wwyx\u001frq@t8\u001d|/{~%}swy2x>ª2{{yt8}.W~<u\u001dt6{yp@~#qsr/t8}qsr2z#nz\nqs2z#{b2~#qst8p6W=tpQr/~\u001f2{u=z%{ypQ~1\"\u001d}Qqsr2t8}\u0006t8\u001d|2{y~%}st|*t6\nz%{qpQt8{t\nqswy~\u001f'\u001dr/~\u001f|*t8\u00122{y{Sª\n2u/wy2xz\n~\u001f2pQwypqst6qKWt8qsr2~<uS\u0006r2w\nrjqs~z%|2|/{yLqsr2t8AW=tW2t8t\u001buLqs~A{y~~#=z%q\u0004z\u001b\u001dp\u0004t\nz%=t6/pQ/}stqsr\nz\u0013qqsr/t\u0004p\u001dpqst6[z6\n~\u001f/qsp\u0010\"~#}]z%{{y~\u001bmz%2{yt,Wt6{y~<u/w\nu/wµ0t8}st8\nt6p\ntJ}Q}s~#}sp/pQt8}sp\bWwyx\u001frqvjz#\ftwyLzj</t8}Q2{ywy\ft\u0006wy2pQt8}Qqswy~\u001f/p6\u000epQ22pqswqs\u001dqsw~#2p6u\u001dt6{yt8qswy~\u001f/p6t8q\n'=t\u0004\u00152pq.z%{pQ~>t8\u001d|2{y~#}st\u0004\u0004z\u001b\u001dp\u0010qs~\bwy\n~#}s|*~%}gz%qstu/\u001d}gz\u0013qswy~\u001fLw\u001d\"~%}sjz%qswy~\u001f\u001ewyaqsr2t\u00062t8}Q\n|*t8}sr\nz%|2p>pQwyWwy{z%}s{Aqs~\u001e¬¦\u0013Ò\u001b®E ÄK\u001d}Q}st6qs{mqsr2wypGwy\u001d\"~#}sjz\u0013qswy~\u001f7wypG/~#}sjz%{wy6t6uM~\u001f/q\u0002~%\u0006qsr/ta2~%qst{yt6\u001ft6{0¿\n¡a¡p,u/\u001d}sw/x\u0006qQ}gz%wy2wy2x\u001dbnt\b2t8t\u001buGz%{ypQ~Sqs~z#u2u\u001d}st8pQpv/t6pqswy~#2pj~#\n~\u001fW|2{yt8\u001dwqZz%\nuZp\nz#{z#/wy{wqPm|*t8}sr\nz%|2pjt8\u001d|2{y~%}sw/x\u0017qsr/t\n~\u001fWW~\u001fDx%}s~\u001f/\nu¤*t8qKt6t6¤~\u001f\u001d}Wp\u001dpqst6Áz%\nu=qsr2~#pQtAu/t8\ft6{y~\u001f|*t\u001bu\u0012~#}_{z\u0013}sx\u001ftp\nz%{ytSpQ|*t6t\nra}st\n~#x\u001f2wqswy~\u001fLqgz#pQ<p6_¸'z#pqs{*\u0004t\u00152t6t6uaqs~\n~\u001f{y{yt\nq\u0006qQ}gz#wy2wy2xAu2z%qgz\u0002~\u0013\ft8}\u0006z1\u001d}s~\fz\u001fu\u001dt8}_}gz#/x\u001ftW~%m2~%qst6p_pQ~1qsr\nz\u0013q/~#qst_{yt6\ft8{)¿\n¡a¡p\nz%A*t\bqQ}gz#wy2t6uG\"~%}vqsr/t6¾z#pm\u0004t8{{¶\nKt\n~\u001f/{4uz%{ypQ~1t8\u001d|2{y~%}st\u0006\u0004z\u001b\u001dpv~#bª\n{y{yw/xGwyaqsr2t\u00062~%qst6pv\u0012~#}\br/w\nrLz\u001fu\u001dt\u001b\nz\u0013qstu/z%qgzWwyp>WwypQpQwy2x\u001d0|*t8}sr\nz%|2pvwqsr\u001e|\nz\u0013}gz#Wt8qstJ}vqP/wy/x/\nt\nr2/w4/t6p{ywy\ftW|2z%}gz#WtJqst8}\bq\u001dwy2xGWwyx\u001frqz#{ypQ~\u001ez%{{y~\u001bz1#z#{y\nz%2{ytS}st\u001bu\u001d\nqswy~\u001f~%@qsr2t\b</S*t8}m~%\u0010\u0012}st8t\u0006|2z%}gz%Wt8qst8}sp\u0004qs~*t\bqQ}gz%w/t\u001bu\u000e\nwy2z#{y{\u001dqsr/tvp\u001dpqst62t6t6u/p\u0004#z#pqs{Wt8\u001d|2z#\nu\u001dt\u001buqst6pqswy2x\u001d/pQwy\ntvqsr/t}st8pQ2{qsp\u0015~%mpQ\nr\u0017qst8pqsw/x1\u0004~#2{un*tWwy<\u001fz%{y\nz#/{ytWwLqsr2tWu\u001dt6\ft6{y~#|/Wt8<q~#_{z%qstJ}jx\u001ft6/t8}gz%qswy~\u001f/pG~%\bqsr2wypWWt6{y~<u<D}st\n~\u001fx#2wqswy~\u001fMpQ~%\u0012qQ\u0004z%}st\u001f1Pq/t6t\u001bu/p_qs~A*tqst8pqst\u001bun~\u001f=z1{z%}sx\u001ftJ}\n~#{y{t\nqswy~\u001f¤~#t8/z\nqWt8{~<u\u001dwyt6p6\nz%p\u0004\u0004t6{y{\u000ez#pm~#GWt8{~<u\u001dwyt6p\u0004r2w\nr1t6\n~\u001fW|2z#pQpm2tJ}QWt8}Q}s~%}sp\n~#WW~\u001f2{\u0002jz\u001fu\u001dt<Az2pQt8}\u001b,=t/t6t\u001bu1qs~\u0002u/t6\u001ft6{y~\u001f|\u001ezmz6qs~\n~\u001f<\ftJ}Qq,z\u00152pQtJ},</t8}Qjwy1z%Az%\nu/wy~\u0006\"~#}sjz\u0013q\npQ2/x//r\u001d/WWt\u001bu\u000er/wypqs{t6u\u000e\u000et8q\n vwyqs~\u0002zj\u0012~#}sfz%|2|\u001d}s~\u001f|/}swz\u0013qst\u0015qs~\u00022t8}Q\u001dwy2xWqsr2t\u00062\u001du\u001dt8}s{\u001dw/xS¿\n¡a¡p/pqst8A\nÄK/}Q}st6qs{qsr2t>W~\u001fpqm<wz#/{t>Wt8qsr2~<u~%mu\u001d~\u001fwy2x\u0002pQ~\u0002K~\u001f/{4u\u001e*t\u0015qs~\u0002|*t8}Q\u0012~#}sÕ|/wq\nrLu/tJqst\nqswy~\u001fL~\u001f\u001eqsr2t\u0006wy/|//q\u001b.qsr2t8D2pQtGqsr/t1u\u001dt8qst\nqst6u=|2wq\nr/t6pwy¤|2{z\nt1~#v~\u001f\u001d}Sqst8<qs2z#{/t8}QDpqQ}swy2x\u001d iPqK~\u001f2{uZz#{ypQ~n*tGu/t8pQwy}gz%2{yt1qs~\n~\u001fW|2z%}st\u0002qsr2wypp\u001dpqst6iqs~~#qsr2tJ}\n~#W|\nz%}gz%2{yt,p/pqst8Wp6@¿>~\u001b\u0004t8\ft8}\u001bqsr2tJ}st>z\u0013}st>2~qQ}s/{\n~\u001fW|2z%}gz%2{yt>p\u001dpqst6Wp]wWqsr/tv{ywqst8}gz%qs\u001d}st\bz\u0013q\u0004qsr2wypKqswyWt\nw t#qsr/~\u001fpQtmr2w\nr|*t8}Q\u0012~#}seWt6{y~<u\u001d}st\n~#x\u001f/wyqswy~#W~\u001fju2z\u0013qgz#2z#pQtmt6qQ}swyt6pqsr2z%q\u0015r\nz\u001b\ftW/~#q\u0006*t6t6¤u\u001dwp\n}st8qst6{=t6\n~<u/t6u\n\b2~%}u\u001d~t6p\u0006qsr/t8}stjt8<wypqjzapqgz%\nu2z\u0013}guD*~<u\u001dD~%v\u00152pQw\n~\u001fDr2w\nr¤qs~\u001e|*t8}Q\u0012~#}s`pQ\nr¤z\n~\u001fW|\nz\u0013}swypQ~\u001f'P\bqsr2wyp\u000e|\nz#|*tJ}\u001b%Kt.r\nz\u001b\ft.|/}st6pQt8<qst6u_z|\u001d}s~#qs~%q\u001d|*t@p\u001dpqst6F\u0012~#})pQ|*~#qQqswy/x1zj2t8}QAWt6{y~<u\u001dAwyLzj{4z\u0013}sx\u001ft8}\b*~<u\u001da~%]}gz6[\u00152pQw\nz#{\u0010z#\nu\u001dwy~u/z%qgzA\u0017z\u001fu/z#|/qswy/xA¿\n¡a¡\u0004~%}gu/pQ|*~%qQqsw/xAqst\nr22w2t8p\b\u0012}s~\u001f¨qsr/tpQ|*t8t\nr¤}st\n~#x\u001f/wyqswy~#7u/~#jz#wyDqs~\u0017qsr2t\u0002S/pQw\nz#{u\u001d~\u001fjz#wy\u000eD7r2wy{ytqsr/tp\u001dpqst6¨z%p\u0006|\u001d}st6pQt6qst\u001bu\u001er2t8}st\u0015}st6|/}st6pQt8<qspzG|/}s~~#4E~#\u0012\n~#\nt8|/q\u0012~#}\u0002qsr2wyp\u0004~%}s*wyq\u0002~%µ0t8}spjx#}st\u001bz\u0013qG|*~%qst6qswz#{v\u0012~#}j\"\u001dqs/}stau\u001dt6\ft6{y~#|/Wt8<q\u001bvPq\bz#{ypQ~1~%µ0t8}sp\b<E|/}s~<u/\nqspqs~\u0002~%qsr2t8}\bS2pQw\n}st6{z%qst6u\u0017z%|/|/{yw\nz%qswy~\u001f/p6>pQ\nr7z%pWqQ}gz#2p\n}sw|\u001dqswy~\u001fM\u0012}s~# }gz\u001bÕz#2u/wy~=qs~=\u00152pQw\nz#{\b/~#qgz%qswy~#Bz%\nu7z%/qs~\u001fjz\u0013qsw\nz6\n~#W|\nz#/wyWt6q\u001bR=t\u001e|2{z%Cqs~\n~\u001fqswy\u001d/t1u\u001dt6\ft6{y~#|2wy2x\u001eqsr2wypSp\u001dpqst6`wy<qs~\u0017zaW~#}stj\"/{{\nz%|\nz%2{ytWt8{~<u<<P2z#pQt\u001bujS/pQw\nz#{0u/z%qgz#2z#pQt2tJ}QWp\u001dpqst6A\"~#{{y~\u001bwy2xqsr/tpQ/x\u001fx#t6pqswy~\u001f2p>pQt8q>\u0012~#}Qqsr\u001ez#*~\u0013\ft#\n~\u0002z\u001fu2u<}st6pQpvqsr/wyp>u\u001dt6pQw}st\u001f\nqsr2t\u0006z%/qsr/~#}sp]z%}st\n/}Q}st8<qs{\u0006\u0004~%}s\u001dwy/x\u0006~#qst6pqswy2xz%Wt8\u001d|\nz#2u/t\u001bu\u0015Wt6{y~<u\u001d<~#2{\u0017z#2u\u0017|*~\u001f{\u001d|2r/~\u001f/w\nu2z%qgz%\nz%pQtW2pQwy2x1z\n~#{{yt\nqsw~#D~#\u0004\u0012t\u001bz%qs\u001d}st\u001ft\nqs~#}spz#\nuDwy\n~#}s|*~#}gz\u0013qswy2xL2t8kt8{t8Wt6qsp6]pQ\nrZz%pt8}Q}s~#}spSw/t8}Qj\"~#}s\u00152{z%qswy~#'/wqs~Sqsr/t>qst6pqswy2x\u001d\nAcknowledgments\nr/t\u0004z%/qsr2~%}sp\u0010\u0004~#2{u\u0006{ywy\ftbqs~>z\n<2~\u001b{yt\u001bu\u001dx\u001ftm¸\u000ew\n~\u001f{y\u0006±\b/}stJ\b\"~%}.r2wypr/t6{y|/\u00122{bpQ2x\u001fx#t6pqswy~\u001f/p6.·bW|*tJ}s~#}¸\u000ew<\u001da\"~#}\u0006x#t62t8}s~#2p\n~#W|2/qstJ}u\u001d~\u001f2z%qswy~\u001f/p6\u001fqsr2tKqst8}gz\nqsw\u001ft\n¡t\u001bu\u001dw4z\nt\nr22~#{y~\u001fx#SÄKt6qst8}@\u0012~#}b{y~\fz%~%'qsr/t,ÚKz#jz%r\nz_\ft8\u001d*~\u001fz%}gu\u000ez#\nuqsr/t\bÄK{z%}stÍ\n~~#qsrG¸'\nt\nt6{y{~\u001bm\npQr2wy|A|\u001d}s~\u001fx%}gz# \"~%}vqsr/t6w}mª\n2z#\nw4z%{)pQ/|2|*~#}Qq~%\u0010qsr2wyp|/}s~P`t\nq\u001b\n5.REFERENCES¬¦J®\u0006·Í\nz%qs{y{yt\u0006z#2uA´\u0010\nÄ\u0004z%2~\u001d*ov\u001dqs~\u001fjz\u0013qsw\npQt6x\u001fWt6qgz\u0013qsw~#1\"~%}S2pQw\b\n{4z%pQpQwª\nz\u0013qsw~#\u001e2pQwy2x\n~\u001fW|*t8qswqswy\ft\br/w4u/u/t6\n¡z%}s\u001f~%W~<u/t6{yp6\u001d\u001a¼mêö\u001bîP½bö¾,øgæ\u001düAø~¿v\u001d´.{y\u001dW~#/qsr'\n¡o\n\nq\u001b\n§%Ï\u001fÏ#Ï\u001dov#z#wy{z#2{yt_~#1qsr2t\bPqst8}s2t8q\u001b×\u0010r<qQqs|\u000e× ²\u001f²\nww}\u001b\np6 2jz%pQp6 t\u001bu/\n²S2pQw\n§%Ï\u001fÏ#Ï\f²<¬ §\u001b®dÀ/*ÄvÍ\n}s~\u001b'\nÄ\u0004z#{\n/{4z\u0013qswy~\u001f\u001e~#.z\n~\u001f/pqgz#q\u0018»epQ|*t\nqQ}gz#{qQ}gz#2p\u0012~#}sA'í\r½@ö¾\bô\u0012õ2è_Þ,îsö#ß<ðsôó£îsã\u001fà)æ\nö\u001bî8óEèJôç\u0002ö¾vÞ,újèJêgó£îsã#­\u001fÓ\n¦\u001bJ× Ã§\u001fÒC\tÃ\u001f©#Ã/_À\fz%'\u000e¦\u001bÓ#Ó\u001d¦\u001f¬ ©\u0013®dÀ/*Äv*Äv*ÄKr/t6az%\nu1o/¸b2´@\nÄKr2t8'·Á>2t8}Q\u0002W}sr\u001dqsr/A×ovAz%|2|\u001d}s~\fz\nr1\u0012~#},pQ~\u001f/xS}st8qQ}swyt6#z#{)wyGS2pQw\nu/z%qgz%\nz#pQt8p6\n¼mêö\u001bîP½bö¾ Â\u001fô\u0012õ\u0002øgä2ôPèJêgä*ã%ôó£ö#ä*ã#à\u001dÃAö%êsÿ%ðõ/öJò=ö#ä1¿,èJð8èQã#êîgõøgðgðsß2èJð\bó\"ä1Äã#ôEã\u0014Å\u0004ä<å#ó\u0012ä0èsèJêJó\u0012ä<å#\n|2z#x#t6p_¦\u001b©\u001fÓD\t\n¦8Ã\fÂ\u001d'v}s{z%\nu/~\u001d\n¸b\nt6'0¦6Ó\u001fÓ\u001f­<¬ Ã%®\n¡*ÄK{z#/pQt6'2¢\u00062·.2x\u001ft8{\u001d}st\nr<q\u001b/±S\n¡t8tJ}\u001b\nz%\nuÀ/*¥\nr/Wwqs\u001f/´b¢,\n¡¥*×2oC\u0004t6\u001dE\nz#pQt6uGqs~~\u001f{\u000e\"~#},pQt\u001bz\u0013}\nr/wy2xw1|*~\u001f{\u001d|2r2~#2w\n\u00152pQw\n\u001d1¼mêö\u001bîP½bö¾vøgæ<üAø~¿v2´.{\u001dW~\u001f/qsr\u000e¡o\u0006\n\nq\u001b2§#Ï\u001fÏ#Ï\u001d0o,\u001fz%wy{4z%2{yt_~\u001fGqsr2t\bP<qstJ}s2t8q\u001b×r<qQqs|\u000e× ²\u001f²\nww}\u001b\np6 2jz%pQp6 t\u001bu/\n²%S/pQw\n§#Ï#Ï\u001fÏ\f²¬ Ò\u001b®dÀ/2¢/±\bt6{y{yt8}\u001b·À\u001d0«\u00152´.}s~\u001fz#<wyp6*z#2u\u001aÀ/2¿\u0015/¸b/¿\bz%2pQt6\u000eÄ_ó\"ðJî8êsèJôPè4Ægû\u0010ó\"úWè;¼mêö\u001bîgèJðgðsó\"äå1ö¾\bæ\u001fò/èsèQîgõ\u001eæ/óå#ä*ã\u001fà ðs2´.}st8<qsw\nt¿\bz#{y{ÈÇ>|2|*tJ}v¥/z\u001fu/u/{yt_¢,w\u001ft8}\u001b¯\nÀ/\u000e¦\u001bÓ#­\f½<¬ Â\u0013®dÀ/*¥*2±>~\u0013/wyt\u001f\n¡/pQw\n}stJqQ}swt8\u001fz%{)z%pqstJ\u001dqm}stJqQ}swt8\u001fz%{£×K¥<wW|/{ytt8qt8µ0t\nqsw\u001ft\u001f2Pb¼\u0004êQö\u001bîP½.ö¾\bæ\u001dø$É]ø~¿v\u001d|\nz%x\u001ft6p>§#Ó\u001f½D\t<§#Ó\u001f­<Í\nt8}s\ft6{yt8\nÄ\u0004o\u00062ov/x/0¦\u001bÓ#Ó\u001fÓ<¬ ½\u001b®dÀ/\n~~#qst\u001f\nov1~\u0013\ftJ}s\u001dwyt8F~%.z#2u/wy~wy/\u0012~#}sjz%qswy~#1}st8qQ}swyt6#z#{ÞdÊ)ü üaß/à ôó\u0012újèQá#ó£ãæ2ç%ðsôPèJúSðí\r½\n½\n¦\u001bJ× §D\t\n¦6Ï\u001dËÀ\u001fz#''¦\u001bÓ#Ó\u001fÓ\u001d¬ ­\u0013®\u0006o\u00062«>r2wz#p6·À/*¸\u000e~\u001fx\u001fz#'/±S\nÄKr\nz%S*t8}s{ywy'2z#2uÍ\n\nÄv0¥<Wwyqsr\u000eÁ>2t8}Q\u0002jr<2WWwy2x\u001d×\n¡2pQw\nz%{)wy/\u0012~#}sjz\u0013qsw~#A}st8qQ}swyt6#z#{\u000ewz#az%\nu/wy~Wu/z%qgz%\nz#pQt#2Pb¼\u0004êQö\u001bîP½.ö¾\bÞdÊ)üXüAü\\ù ÌoÍ#2|2z#x#t6p§#©\u001d¦\n\t\u001d§%©\u001fÂ\u001d'¥\u001dz#\n}gz#\nwyp\n~/*Ä\u0004o\u0006¯\n~\u0013*\u000e¦6Ó\u001fÓ\u001fÒ<¬ Ó\u0013®\n¡*«>~%qs~/*oF|/}st\u001bu\u001d~\u001fWwy\nz%qQ\nÏ\u0006t8<qswyjz%qswy~\u001fGWt8qsr2~<uW\"~%}ÄK±R}st\n~#}gu\u001dwy2x\u001fp6×\n¡ov´?t6pqswyjz%qswy~#A2pQwy2x·\n¡z#{yx#~#}swqsr2\"~#}vz#u2z#|\u001dqswy\ft\bqs~\u001f/t_W~<u/t6{yp6\u001d1¼mêö\u001bîP½bö¾>ø$Ê)Þ>æ2æ(¼\u00042¥/z%{yq¸)z#\u001ftÄKwqÈÇ\n\n¡z\u001bA§#Ï#Ï\u001d¦#*·.{t\nqQ}s~\u001f/w\n´@}s~\n¬¦\u001bÏ\u0013®Í\n\n¸\u000e~\u001fx\u001fz#\u001ez#2uA¥**ÄKr<'\n¡/pQw\npQ/Wjz%}swy\u001bz%qswy~#12pQwy/x\ft8|2r/}gz%pQt6p6/b¼\u0004êQö\u001bîP½.ö¾vø$Ê)Þ>æ2æ(¼\u0004/\f~\u001f{y2Wt_§</|\nz#x#t6p½%Ã\fÓD\t\u001d½#Ò\u001f§<0Ppqgz#<2/{\n/}s\ft8\rÀ#22t\u0006§%Ï\u001fÏ#Ï\u001d¬¦\u001f¦J®¢\u0006ÈÀ/\n¡6¯\nz%'2¸b2o\u00062¥\u001dWwqsr'\u001d±SÍ\nz%wy\u001d\u001d}swu/x\u001ft#*z#2uGg2¿\u00067wqQqst6'\nr2t¯\nt8ÏÎ0t6z#{z#2u1±\bwyx\u001fwqgz#{)¸'wy/}gz\u0013}Q\n¡·b¸'~<u<w/±_·¨Ð\u0015·Ä\u0010Æ\u0012â.ó£ë\bü\u0017ã\u001bå\fãDÑ8ó\"ä*èJ\n¡z6L¦\u001bÓ\u001fÓ\u001f½<\nov#z#wy{z#2{yt_~#1qsr2tqst8}s2tJq\u001b×@rqQqs|'× ²\u001f²\u001bmm_ u\u001d{yw\u000e ~#}sx²%u/{ywy*²\u0013jz\u001b/Ó\u001f½\u001f²%Wt8{4u\u001dt82²Ï\fÒ\u0013wqQqst6\u000e r<qsW{¬¦\u0013§\u001b®\n¡\n¡~#2x\u001ft6z#\u001ez#2uG±S\n¥/z%2\f~%µ@2ÄK~\u001fW|\nz\u0013}swypQ~\u001fA~%.S2pQw\nz%{pQt\u001b2t6\nt6p6fÊbö#ú>ò\nß<ôPèJêgð\u0006ã#ä*áWô\u0012õ2è>ñ\bß<úWã%ä2ó\"ôóEèJðs§%Ã\n©\u001fJ×y¦\u001bÂ\u001d¦\n\t\n¦\u001b½\u001fÒ<_À\u001f/2tS¦6Ó\u001fÓ#Ï\u001d¬¦\u001b©\u0013®\u0015¥02´.\u0012t6wµ0t8}\u001b0¥*\nwyp\nr2t8}\u001b\nz#2u\u0002R2·.µ0t8{pQ*tJ}sx/*o,/qs~#jz%qswz#\nu\u001dwy~\n~\u001fqst6q,z%\nz#{\u001dpQwyp6\nPb¼\u0004êQö\u001bîP½.ö¾vÞdÊ)ü`üAü ù ÌuÒ%|\nz#x#t6pv§<¦\n\t<©\u001fÏ<Í\n~#pqs~\u001f'\n¡o\u0006¯\n~\u0013*0¦\u001bÓ\u001fÓ#Â\u001d¬¦6Ã%®\u0006¸b/¢\u0006\u001d¢vz#2wy/t8}vz#\nuÍ\nÓÀ/WÀ\u001f2z#/x/2ov1wyqQ}s~<u/\nqswy~\u001f1qs~r2wu2u\u001dt6\n¡z\u0013}s\f~\u0013\u0002W~<u\u001dt6{yp60øÅfÅfÅ7Þ,ß/á#ó£ö¹\u0000.æ\u001fò/èsèQîgõ(\u0000\u0004ã#ä*áæ2óå#ä\nã\u001fà·¼\u0004êQö\u001bîgèJðsðsó\"ä<åjüLã\u001bå\fãDÑ8ó\"ä0èJ/|\nz%x\u001ft6p,Ã$\t2¦\u001bÂ<WÀ\fz%')¦6Ó\u001f­#Â\u001d¬¦\u001bÒ\u0013®\u0015Äv\n¢vz#|/r\nz%t6{\no,/qs~\u001fjz\u0013qsw\npQt6x#Wt6qgz%qswy~\u001fA~%bz\n~#2pqswS/pQw\nz#{\u000epQwyx\u001f\nz%{ypv/pQw/xr2wu/u/t6\n¡z%}s\f~\u0013jW~<u/t6{yp6*øÅ\u001cÅfÅû2êã#ä2ð4½.ö#ä\u001a¼'Þ,üAøg2§<¦\nÃJ× ©\u001fÂ#ÏP\t©\f½#Ï<'ov|/}swy{.¦\u001bÓ#Ó\u001fÓ\u001d¬¦6Â%®\u0006o\u0006ÈÇ>wqgu\u001dt6<*~\u001fx\u001ftJ}gu1z#2u\u001aÀ/WÎ0~#*t6{\n¡t8{~<u\u001dw\njz\u0013q\nr/wy2xqst\nr2/w4/t6p\u0004\"~%}v{z\u0013}sx\u001ftS/pQw\nu2z%qgz%\nz%pQt6p6\nPb¼\u0004êQö\u001bîP½.ö¾vÞdÊ)üüAü\\ù ÌSÌ%2|2z#x\u001ft8p>Ò#½D\tÂ\u001fÂ\u001d0v}s{4z%\nu\u001d~/\n¸.*\nq\u001b \t¯\n~\u0013*0¦\u001bÓ\u001fÓ#Ó\u001d¬¦\u001b½\u0013®\u0006o\u00062¸bÈÇ>wqgu\u001dt6<*~\u001fx\u001ftJ}gu1z#2u\u001aÀ/·Î*~\u001f*t6{\n¡z%2wy|2/{z%qswy~\u001fa~%S/pQw\n\"~%},Wt6{y~<u\u001d\u0002jz%q\nr2wy2x\u001d<1¼mêö\u001bîP½.ö¾>ÞdÊ)ü¨ü1ü ù ÌoÂ\u001f|\nz%x\u001ft6p>§#©\u001fÒD\t<§%Ã\fÏ<Í\n}swypqs~\u001f{WÇvÖ2¥\u001dt6|\u001dq\u001b\u000e¦6Ó\u001fÓ#­\u001d¬¦6­%®dÀ/*«\u0015/7wy{y|*~\u001f\u000e\n¸.2¢\u001d¢>z%2wy2tJ}\u001b*Äv E¿\u00152¸'t6t#\nz%\nuA·m2¢«>~\u001f{u\u001djz#'2ov\u001dqs~\u001fjz\u0013qsw\n}st\n~\u001fx#2wqswy~\u001fa~%@\ft8<\u0004~%}gu/pmwy2\n~\u001f/pqQ}gz#wy2t6u1pQ|*t6t\nr1/pQw/xr2wu/u/t6\n¡z%}s\f~\u0013\u0002W~<u/t6{yp6øÅfÅ\u001cÅû/êQã#ä/ðC½.ö#äAÞ>æ2æ(¼\u00042©\u001f­\n¦#¦\u0013J×y¦\u001b­\u001f½#ÏD\t\n¦\u001b­\u001f½#­<¯\n~%*\u000e¦\u001bÓ#Ó\u001fÏ\u001d¬¦6Ó%®\u0006´\u0010*Äv\u001d=~~<u\u001d{z#\nu0WÀ/ÓÀ/\n\bu/t6{y{\b\n\b\nz%{yq\nr2t80/z%\nuA¥*·À\u001dÚ]~\u001f2/x//¸)z\u0013}sx\u001ft\f~\nz%2/{4z\u0013}Q\n~\u001fqswy\u001d/~\u001f/pmpQ|*t6t\nr}st\n~#x\u001f/wyqswy~#G/pQw/x\u0015¿\nÖ\u001b¼\u0004êQö\u001bîP½\u0010ö¾mø$Ê)Þ>æ2æ(¼\u0004\f~#{y2Wtv§<|\nz%x\u001ft6p,Q\\\t2¦\u0013§#ÒD\t\n¦\u001b§#­<0o>u\u001dt6{z#wu/t\u001f\nov/pqQ}gz#{ywz\u001d2ov|/}swy{.¦\u001bÓ#Ó#Ã/¬ §%Ï%®\u0015¥0\u001dÚ]~\u001f2/x/\u001d±S/Ö_t8}spQr\nz6_ÓÀ/\n\bu/t6{y{\n±S2>{y{z#pQ~#'\b\n\b\nz#{q\nr/t6*/z#\nuG´\u0010/n~~<u/{z#2u\u000e]û'õ2è\bñû_Ôeévö\u001bö\u0013ÿbÕ ¾6ö%êñû_Ô×Ö2èJêgðsó£ö#ä®Ø(½±Ù\u0001Ú\u001b\nr2t\n¡w\n}s~\u001fpQ~%\u0012q_ÄK~#}s|*~#}gz\u0013qswy~\u001f'0§#Ï\u001fÏ#Ï\u001dov#z#wy{z#/{yt_~\u001f1qsr/t\bwy<qstJ}s2t8q\u001b×.rqQqs|'× ²#²%rqs* t82x/\nz%A z\n 2/²¬ §¦8®\nËÎ*r\nz%2xSz#\nuAÄv ÄvÓÀ\fz61Ö_/~//¿>wyt8}gz\u0013}\nr/w\nz#{)p\u001dpqst6 \u0012~#}\n~#<qst8<qQE2z#pQt\u001buGz#\nu\u001dwy~\n{z#pQpQwª\nz%qswy~\u001fLz#2u\u0002}st8qQ}swyt6#z#{2¼\u0004êQö\u001bîP½.ö¾\bô\u0012õ2è\u0006æ(¼]øÅÛÊbö#äC¾\u001bèJêsèJä\nîgèSö#äAü\u001eß\u001dà ôó\"újèQá#óãWæ/ôEö#êQã6åèã#ä\náÞ,êîgõ<ó±Ü\u001bó\"äåjæ2ç%ðsôPèJúSð>øQøQøg\u001d|\nz%x\u001ft6p,©#Ó\u001f­P\t\fÃ\fÏ#Ó\u001dÍ\n~#pqs~\u001f'¡o\u0006¯\n~\u0013*0¦\u001bÓ#Ó\u001f­\u001d\nAPPENDIX\nA.SONGTRANSCRIPTIONS¯\n~#qst\u0006\nz%WtqQ}gz%2p\n}swy|\u001dqsw~#2p_z\u0013}st\u0015|\u001d}s~\u0013\u001dwu\u001dt\u001buAr2t8}st\"~%}_t\u001bz\nr\u001e~#.qsr/t|/wyt\nt6p~#)S/pQw\n2pQt\u001bujwyGqst6pqswy2x\u0015~\u001f\u001d}Wt6{y~<u\u001djpQ|*~#qQqswy/xSp\u001dpqst6A±>/}gz%qswy~#2p.z%}stKt8\n{y\nu/t6u\u0012~#}bpQ|\nz\nt\n~#2pQwu/t8}gz\u0013qswy~\u001f2pbz%\nu*t\nz#/pQtqsr/t\n\u001d}Q}st6qwW|/{yt6Wt6qgz%qswy~\u001f\u0002t8µ0t\nqsw\u001ft6{1/~#}sjz#{ywy6t6pmqsr/t6 ~\u001f\u001dq~%\n~\u001f/pQwu/t8}gz\u0013qsw~#'K·./z%W|2{yt6pm~#\u0010t\u001bz\nrA|2wyt\nt_jz\u001b\u0002*t\br/t\u001bz%}gu1z%q\u001b×ÝSÞSÞ\nÆfßàoà%È\fÈ\fÈfá¤â(ã¹âNá¤äÉÞ\nâ(ãÝ\náâoåoæ\rà(çDäÞ\nâoèué·ê\nëÈà6Å·âuìSí$åSîÈï\u0013Æ\ríÞuÞ\nËPðuä(à\nr/~\u001f/x\u001frLz#{y{\u0010|/wyq\nr2t8p>z\u0013}st\u0015|/{4z6t6uaz#p>/~#qgz\u0013qst\u001bu\u000e*/~#q>z#{y{.u/\u001d}gz%qswy~\u001f/pz\u0013}st\u0017|/{z\u001b\ft\u001bu7t\u001b\nz#{y{7u\u001d2taqs~Dqsr2t\u001e\nz\u0013qs/}gz%{\b\u001fz\u0013}swz%qswy~\u001f7qsr\nz\u0013q1\u0004z#pu\u001dt6pQw}st\u001bu1wy1qsr/t>qQ}gz#wy2wy/xWu2z\u0013qgz\u001d\nAuldLangSyneÄmØ\nØ_·]Ø\nØ_o>Ø«\bØ\nØ\u0006«\bØ_o>Ø«\bØ\nØ\nØ_o,Ø\u0015ÄmÙ>±_Ù±_Ù_Ä,Ù\bo,ØovØ\nØ\u0006«\bØ\nØ«_Ø_o,Ø\u0015«\bØ\nØ\b±\bØ\b±_ØÄmØ\nØ\nBarbara AllenÄ\nØ·\nØ\nØ«\nØ\nØ·\nØ±\nØÄ\nØ±\nØ·\nØ«\nØÄ\nÙÄ\nÙÍ\nØ«\nØÄÙ_ÄmÙ_o,Ø«Ø\nØ_ovØ«\bØ_·KØ\b±>Ø\u0015Ä\u0004Ø\b±\bØ_·]Ø\nØ«\bØ\nØ_·]Ø\b±\bØ\nFrereJacques«_Ø_o,ØÍ\nØ«\bØ\u0006«\bØ_o>ØÍ\nØ«_ØÍ\nØÄÙ>±ÙÍ\nØÄÙ>±Ù±\nÙ·\nÙ±\nÙÄ\nÙÍ\nØ«\nØ±\nÙ·\nÙ±\nÙÄ\nÙÍ\nØ«\nØ«_Ø\b±>Ø\u0015«\bØ«\bØ_±>Ø«ØHappyBirthday«\nØ«\nØo\nØ«\nØÄ\nÙÍ\nØ«\nØ«\nØo\nØ«\nØ±\nÙÄ\nÙ«\bØ\u0006«\bØ«_Ù\b·\u0004Ù_ÄmÙÍ\nØ_ovØ\nÙ\nÙ·]Ù_ÄmÙ\b±_Ù_ÄÙ\nI’maLittleTeapotÄmØ\b±\bØ·bØ\nØ«ØÄmÙ\bovØÄÙ_«\bØ\nØ\nØ«\nØ·\nØ·\nØ±\nØ±\nØ·\nØÄ\nØÄmØ\b±\bØ·bØ\nØ«ØÄmÙ\bovØÄÙ_«\bØÄÙ_Ä\u0004Ø\b±_Ø_·KØ\nØ_·bØ\b±_ØÄmØ\nMaryHadaLittleLambÍ\nØo\nØ«\nØo\nØÍ\nØÍ\nØÍ\nØo\nØo\nØo\nØÍ\nØ±\nÙ±\nÙÍ\nØo,Ø«Ø_o,ØÍ\nØÍ\nØÍ\nØÍ\nØ_ovØ_ovØÍ\nØ_ovØ«\bØ\nScarboroughFair±_Ø\b±_Ø_o,ØovØ_ovØ_·]Ø\nØ_·bØ\b±_Øo>ØÄmÙ\b±\bÙ_Ä,Ù\bovØÍ\nØ«_Ø_ovØ±Ù>±Ù>±\bÙÄmÙ\bo>Ø_o>Ø«\bØ\nØ·bØÄmØ±_Ø_ovØ«\bØ\nØ_·KØ\b±_ØÄ\u0004Ø\b±_Ø\nThisOldMan±ÙÍ\nØ_±\bÙ>±ÙÍ\nØ\b±_Ù_·]Ù>±Ù_ÄmÙÍ\nØ_o>ØÍ\nØ\u0006ÄmÙÍ\nØ\u0006ÄmÙ>±Ù_«\bØ\u0006«\bØ«\bØ\u0006«\bØ«\bØ_o>ØÍ\nØÄmÙ>±Ù±Ù\bovØ_ovØÄÙÍ\nØ_o,Ø\u0015«\bØ\nThreeBlindMice·bØ±>ØÄmØ_·]Ø\b±_ØÄmØ«\bØ\nØ\nØ_·bØ\u0015«\bØ\nØ\nØ·bØ«\nØÄ\nÙÄ\nÙÍ\nØo\nØÍ\nØÄ\nÙ«\nØ«\nØ«\bØ\u0006ÄmÙ_ÄÙ_ÄmÙÍ\nØ_o>ØÍ\nØÄÙ_«\bØ\u0006«\bØ«\nØ«\nØÄ\nÙÄ\nÙÍ\nØo\nØÍ\nØÄ\nÙ«\nØ«\nØ«\nØ\nØ·\nØ±\nØÄ\nØ\nTwinkle,Twinkle,LittleStar«\nØ«\nØ±\nÙ±\nÙ·\nÙ·\nÙ±\nÙÄ\nÙÄ\nÙÍ\nØÍ\nØo\nØo\nØ«\nØ±Ù>±Ù_ÄmÙ_ÄÙÍ\nØÍ\nØ_ovØ\b±Ù>±Ù_ÄmÙ_ÄÙÍ\nØÍ\nØ_ovØ«\nØ«\nØ±\nÙ±\nÙ·\nÙ·\nÙ±\nÙÄ\nÙÄ\nÙÍ\nØÍ\nØo\nØo\nØ«\nØ\nB.FIGURE-OF-MERIT¿\nÖ5¬ §%Ï%®/2pQt6p'qsr2t¯\nz%qswy~\u001f2z#{\u001d/pqswyqs\u001dqst]~%0¥qgz#\nu/z%}gu\u001dp@z#2u\nt\nr\u001d2~#{y~\u001fx#\n¯\nQ¥\n.\u0004~#}gu\u001dpQ|*~#qQqswy2x\nwyx\u001f/}stJP~%\u0012\n¡t8}swq>u/t8ª22t\u001bujz#pWÐz#2|/|*t8}QE*~\u001f/\nuAt6pqswyjz%qst\u0015~\u001f\u001e\u0004~%}guLpQ|*~%qQqsw/xAz6\n/}gz\nLz\u001b\ft8}gz%x\u001ft\u001bu~\u0013\ft8}j¦qs~n¦6ÏGz%{ypQtGz%{z%}sWp\u0006|*t8}r2~#/}\u001b ÑVEqwp\nz%{\n2{z%qst\u001bu\u00172pQwy2xqsr2tv\"~\u001f{y{y~\u001bwy2x\u0002t\u001b\nz%qswy~#'×\n\n¡+\n¦¦6Ï$ñ\nóò¶8¶ôbòË:¨ô^õ^õ^\nôbòËö\u001eô\u000b\nòWöø÷Z8r2tJ}st\u000b\n+\n¦\u001bÏPñúùHû¨wyp\bzS\"z\nqs~%}>qsr2z%q,wy<qstJ}s|*~\u001f{z%qst6p,qs~A¦\u001bÏz%{ypQtz#{z\u0013}sWpA|*t8}Ar/~\u001f\u001d}Az%\nuúñ¨wypGqsr/tLqs~%qgz#{_r2~#/}spG~#Su//}gz\u0013qswy~\u001fC~#qsr2tWqst6pqu2z\u0013qgz\u001d\n~\u001e|*t8}Q\"~%}s qsr/wyp\nz%{\n2{z%qswy~\u001f\u000e]ª/}spqz%{y{m~#qsr2tpQ|*~#qQqst6u\u001eK~#}gu/p\nWt8{~<u\u001dwyt6pg>z%}st\u0006}gz#/\ft\u001bu\u001ewLp\n~%}st~#}gu\u001dt8}\u001b\nr2t6\u000eqsr2t\u001e|*t8}\nt6qgz#x\u001ft\u001e~%\u0006qQ}s/t\u001er2wqsp\nò\f\n\"~#22uM*t8\"~%}stLqsr/t\u0003\u000f~ü qsrMz%{ypQtz#{z\u0013}sÕwyp\nz%{\n2{z%qst\u001bua\"~%};\u000f+\n¦oqC^õ^õ^õq\nû\nô¦r/t8}st\u0014û wypvqsr2tª2}spqwyqst6x\u001ft8}x#}st\u001bz\u0013qst8}qsr\nz%D~#}Wt6<2z#{Kqs~D¦6Ï$ñ2ù:Ï\r^ Òu^]P=qsr2t\nz#pQtG~#qsr2t\u0015t8\u001d|*t8}swyWt6qsp\bu\u001dt6p\n}swy*t\u001bu\u001er2t8}st\u001f´ñ+\nÏ\r^ Òjr2~\u001f\u001d}sp\b~#]}st\n~%}gu/t\u001buS/pQw\nz#{'u2z\u0013qgz\u001d/pQ~/×\n\n¡+\n¦Ò\nóò\n8ôbò\n:ôýò\n=ôbòØ\nôýòÙ"
    },
    {
        "title": "Towards a Cognitive Model of Melodic Similarity.",
        "author": [
            "Ludger Hofmann-Engl"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417359",
        "url": "https://doi.org/10.5281/zenodo.1417359",
        "ee": "https://zenodo.org/records/1417359/files/Hofmann-Engl01.pdf",
        "abstract": "In recent years the interest in melodic similarity has mushroomed mainly due to the increased importance of music information retrieval (MIR). A great number of similarity models and algorithms have been developed, but little or no attention has been paid to cognitive or perceptual aspects to the issue at hand. Questions, about the relevant parameters and the appropriate implementation are under-researched as are experimental data. This paper focuses on the pitch aspect of melodic similarity, scrutinising the term pitch replacing it by a less ambivalent and overused term, which we will call meloton. Based on the term meloton the paper suggests to approach the issue of ‘melotonic’ similarity from a transformational angle, where transformations are executed as reflections and translations. ‘Melotonic’ similarity then is seen as related to the transformation process in form of a transpositional and interval vector. Finally, melotonic similarity as portrait in a psychological context emerges as a multi-facet phenomenon requiring the development of flexible models.",
        "zenodo_id": 1417359,
        "dblp_key": "conf/ismir/Hofmann-Engl01",
        "keywords": [
            "melodic similarity",
            "music information retrieval",
            "pitch aspect",
            "meloton",
            "transformational angle",
            "translational",
            "transpositional",
            "interval vector",
            "psychological context",
            "flexible models"
        ],
        "content": "Permission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit orcommercial advantage and that copies bear this notice and thefull citation on the first page.Towards a cognitive model of melodic similarity\nLudger Hofmann-Engl\nKeele University UK\n+44 (0) 20 87710639\nludger.hofmann-engl@virgin.net\nABSTRACT\nIn recent years the interest in melodic similarity has\nmushroomed mainly due to the increased importance of musicinformation retrieval (MIR). A great number of similaritymodels and algorithms have been developed, but little or noattention has been paid to cognitive or perceptual aspects to theissue at hand. Questions, about the relevant parameters and theappropriate implementation are under-researched as areexperimental data. This paper focuses on the pitch aspect ofmelodic similarity, scrutinising the term pitch replacing it by aless ambivalent and overused term, which we will call meloton.Based on the term meloton the paper suggests to approach theissue of ‘melotonic’ similarity from a transformational angle,\nwhere transformations are executed as reflections andtranslations. ‘Melotonic’ similarity then is seen as related to thetransformation process in form of a transpositional and intervalvector. Finally, melotonic similarity as portrait in apsychological context emerges as a multi-facet phenomenonrequiring the development of flexible models.\n1. INTRODUCTION\nUnarguably, melodic similarity has been of great interest to\ncomposers (e.g., Schoenberg, 1967), et hnomusicologists (e.g.,\nAdams, 1976) and music analysts (e.g., Reti, 1951). However,the issue has received new interest due to the development of theinternet and the need to administrate and retrieve musicalinformation. Early works on MIR date back to the 60’s withKassler (1966) as one of the pioneers. Not much research wasdone on the topic for some time, but by now the growing interestis reflected for instance in the fact that in 2000 the firstinternational symposium on MIR was organised and attended byresearchers from a great variety of fields. The interest of MIR inthe issue of similarity does not necessarily add importance to theissue but certainly urgency to develop reliable and relevantsimilarity models. In fact, by now several models and algorithmshave been proposed (Anagnostopoulou, Hörnel & Höthker, 1999;Cambouropoulos, 2000; Crawford, Iliopoulos & Raman, 1998;Dovey &Crawford, 1999; Downie, 1999; Kluge, 1996; Maidin\n& Fernström, 2000;  Smith & McNab & Witten, 1998), but noneof these models take cognitive or perceptual issues sufficientlyinto account, nor do they pay a closer look at as to what\nparameters to select and how to implement them. \nAdmittedly, the psychological and more specific the\nmusic-psychological research in this field leaves possibly morequestions unanswered than answered (compare Goldstone,1998), but this seems not to justify the dismissal of existingresearch. Notably, none of the researchers takes dynamic aspectsinto account and rhythmic aspects play no or little role byregarding melodic similarity exclusively as a pitch phenomenon,without considering the limitations of their models.Interestingly, the question of what the term pitch, a term which\nfrom a psychological angle is more than problematic, is notbeing asked, although there exists some awareness to the relatedissue of musical representation; that is whether we are dealingwith the representation of music in form of a score, withrecorded music or digital sources such as MIDI files (e.g.Wiggins & Harris & Smaill, 1989), but the central issue of pitchperception is hardly ever touched. \nMost strikingly, the works of Egmond & Povel & Maris (1996)\nhave not been referred to in a single instance to the knowledgeof the author, although their findings are in agreement withprevious research (Franc ès, 1988) and also in agreement with\nmore recent research by Hofmann-Engl & Parncutt (1998). Theirexperiments indicate that transposition is a significant factor formelodic similarity judgements whereby melodic similaritydecreases with increasing size of the transpositional interval.However, all the models known to the author are transpositionalinvariant. This demonstrates the strong tendency of researchersto borrow their tools from music theoretical teachings wheretranspositions of a motive are regarded as being equivalent. Thisis not to say that search tools within MIR should under nocircumstances consider transpositions as equivalent (for instancewhen analysing the structure of a specific composition), butwhere perceptual issues are of importance such attempt will haveto be seen as a shortcoming. \nWhile some models are seemingly unaware of the  evidence as\nproduced by researchers (e.g.  Dowling & Harwood, 1986;Dowling 1994; Edworthy, 1982, 1985; White, 1960) that contouris somewhat a factor determining cognitive similarity (forinstance models based on dynamic programming) there aremodels which take contour into account (for instance Maidin ’s\nmodel), but still reference to psychological research is not given.\nExperiments by Hofmann-Engl and Parncutt (1998) indicate that\ncontour is in fact an imbedded factor of what they called intervaldifference. This is, a reference-melody raised by an interval I\nbetween two consecutive tones (let us say tone 1 and tone 2)produces the interval difference D = I - I’, with I’ being the\ninterval between the two corresponding tones (tone 1 and tone 2)of the comparison-melody. Melodies which show contourdifferences also produce interval differences and hence contourappears to be a factor. However, multiple regression shows thatinterval difference is the sole factor. Up to this date no modelhas accounted for these findings.\nFinally, none of the models takes emotional aspects into account.\nTrue, that at this point it seems an almost unattainable task, butresearch by Tekman (1998) shows, that emotional aspects can beat least partly sensibly measured. Clearly, there exists a level ofunawareness amongst melodic similarity researchers ofpsychological issues which seems hardly acceptable.\nIt is the intention of the present paper to contribute to the\nbridging of exactly this gap.  Although as mentioned beforedynamic, rhythmic and emotional aspects will have to be seen asfactors alongside pitch, we will focus on pitch exclusively. Thisis, pitch is the most discussed aspect of melodic similarity, andtreating all parameters would exceed the framework of thispaper. However, the author is in process of developing asimilarity model which takes dynamics and rhythmic featuresinto account alongside with pitch. In the first instance we willscrutinise the term pitch arguing for its substitution by the newterm, which will be called melot on, we then will consider the\ntransformation of melodies and finally we will develop asimilarity model based on the composition of two specifictransformations.\n2. PITCH VERSUS MELOTON\nThe term pitch is intriguing and perplexing at the same time,intriguing, because it is probably the most discussed musical andmusic psychological term, and perplexing, because it has beenemployed  in so many different contexts that it frequentlyrequires specification to what actually is meant by it. A situationwhich led Rohwer (1970) to question the usage of the term pitchaltogether. However, pitch is commonly understood to be thecorrelate to the fundamental frequency as Rasch & Plomp (1982)explain: “Pitch is the most characteristic property of tones ... .\nPitch is related to the frequency of a simple tone and to thefundamental frequency of a complex tone. ” Pitch is seen here\nexclusively as being related to a physical quan tity. This is as\nwidespread an approach as is insufficient because subtleties ofpitch perception are not captured by referring to physicaldimensions only. It seems this is the issue Sundberg (1991) isaddressing when he suggests to see pitch as locating musicalsound in terms of musical intervals and to call other aspects ofpitch perception tone height (the high or lowness of sound).However, the categorical differentiation between pitch and toneheight seems frail as even sounds with low pitch salience (e.g.,musical chimes) can produce a distinctive pitch sensation (Askill1997). Confronted with the phenomenon that sounds withoutfundamental frequency can produce pitch sensations, Schouten(1938) introduced the term residual pitch. However, from aphenomenological angle there is no difference between pitch andresidual pitch — both appear to a listener in the same way.\nConcepts of virtual pitch (Houtsma & Goldstein 1972, Terhardt1982, Hofmann-Engl 1999) added further complexity to the issueby demonstrating that sounds do not have one single pitch but amultiplicity of pitches with varying degrees of probabilities.\nEmploying this concept the term pitch has to be replaced by theterm most probable pitch. It seems the introduction of a newterm which will have some specific psychological meaning ismore than appropriate. We argue to use the term meloton assuggested by Hofmann-Engl (1989).\nWithout going into detail, we will consider some features of\nwhat this term is to deliver. We wish for this term to be purelyof psychological meaning. This is we endeavour to understandmelodic similarity from a cognitive angle. Thus, concepts offundamental frequency and other physicalistic approaches are\ninadequate. Hereby, the term meloton will represent thepsychological concept whereby a listener listens to a sounddirecting her/his attention to the sound with the intention todecide whether the sound is high or low. True, this does notdeliver a quantity we could input into a similarity model, andhence we will have to define the value of a meloton somehowwithout using a physicalistic concept. In this context it seemsmost appropriate to consider an experimental setting asemployed by  Schouten (1938). A selected group of listeners isasked to tune in a (sinusoidal) comparison tone with variablefrequency to match according to the listener ’s  perception a test\ntone (for which we want to obtain a melotonic value).  Thelogarithm of the comparison tone then will be called m-responseof this listener. Assuming that the group of listeners consist ofn listeners, we will obtain n m-responses. We will call the mean\nof this distribution the m-center. The mode of the distributionwill be called m-peak. The relative density of the m-peak will becalled melograde, and can be defined as:\nMDp\nDlgm\nmi\nin=\n=/Ge5()\n()\n1\nwhere Mg is the melograde, D(pm) the density of the peak of\nthe m-distribution,  D(lm)i the density of the location lm at the\nplace i and n the number of locations. The range of Mg is ]0,\n1]. Note, that models of the pitch salience (Terhardt, Stoll &Seewann 1982) are predictors for the peak of the responsedistribution.\nWe finally define the value M of a meloton as given by the value\nof the m-centre. However, if the melograde of a m-distributionis larger than 0.75 and the peak and the centre coincide withmaximum deviation of 25 cents, the value of the meloton isgiven by the peak of the m-distribution. In this case we speak ofstrong meloton M\ns. In all other cases we speak of weak meloton\nMw.\nThere are several advantages to this approach as there are\nlimitations.  Firstly, the classification of melota into weak andstrong melota guaranties that tones such as produced by a druminstrument will also fetch a melotonic value. This allows for theinclusion of ‘drum-melodies ’ into a melodic similarity model.\nSecondly, we replaced the dogmatic attitude towards pitchperception by an understanding which is sensitive towardsindividual, cultural, educational and social differences; whatmight appear to one group as a sound with a certain melotonicvalue might appear to another group as a sound with a different12345-1000100200\nnumber of tonerelative value of melotonp\nm1m'1\nreflection lineM(ch)\nM(ch')melotonic value. This is certainly of importance when\nconsidering melodic similarity. Right at the basis melodicsimilarity judgement might differ due to lower level perceptualdifferences. Objections might be raised that this approach isimpracticable in many ways, as the measurement of melotonicvalues is time consuming and expensive. Still, we might expectthat data bases containing melotonic measurements might beestablished and made available in future. Another objectionmight be that even if measurements of single tones are available,there is no guaranty that meloton will retain their values whenput into a melodic context. Although this might be true, this willhave to be an issue to be investigated. However, considering thesuccess of aural training (where listeners are required to identifytones in any context), we expect that this approach is more thanpromising.\nBefore we will base a transformation theory on melota, we will\nabandon the term melody due its many ambivalent connotationsand replace it by the term chain. We will as mentioned onlyconsider the melotonic component of a chain (excludingproperties such as timbre, duration and loudness). A melotonicchain will be written as m-chain and as M(ch). Now, we will\nconsider transformations of melotonic chains.\n2. MELOTONIC TRANSFORMATIONS\nThe motivation for developing a transformation theory is drivenby the proposal as put forward by Palmer (1983), wheresimilarity is understood to be related to the transformationprocess involved in mapping two objects onto each other. Therehave been several attempts within the camp of pitch classtheorists to introduce similarity measures and transformations ofpitch class sets (e.g., Isaacson (1996), Lewin (1977), Morris(1979)). However, none of these approaches are of interest in\nthis context, as a pitch class set is fundamentally a differententity than is a melotonic chain. One attempt to describe melodictransformations has been put forward by Mazzola (1987) and hasbeen further developed by Hammel (1999) in form of matrices.Without going into detail, the main deficiency of theirtransformation matrices is the combination of time and pitch inone matrix leading to time and pitch appearing as mixed terms.Maybe even more important, the matrices as they stand, do notallow for general transformations. Thus, a theory of similaritybased on their concept would not allow for the comparison ofany melody with any melody.\nInstead we will take inversions and transpositions as a starting\npoint and generalise these two transformations. As we aredealing with melotonic transformations, we will require that twochains will have the same rhythm and the same dynamicstructure. This is a restriction to the model, but this deficiencycan only be overcome at a later stage including rhythm anddynamics and possibly emotional aspects in a final model.However, as mentioned, this would exceed the framework of thispaper.\n2.1. Inversion\nIt is a well known geometrical fact, that inversion can beillustrated as a reflection along a straight line. Taking amelotonic chain (melody) M(ch) to consist of an initial tone with\nmeloton m\n1 and then the subsequent intervals 100, 100, -100,\n-100 (where 100 might be taken to mean 100 cents), we write:M(ch) = (m\n1)[100, 100, -100, -100] to be reflected onto the chain\nM(ch’) = (m1’)[-100, -100, 100, 100] will require a straight line\nthrough the point p = 50 (see Figure 1)\nFigure 1: The m-chain M(ch) = ( m1)[100, 100, -100, -100] is mapped onto the chain M(ch’) = ( m1’)[-100,-100, 100, 100] via\nthe reflection line through p, with m1 = 0 and  p = 50\n2.2. Transposition\nExecuting two reflections along two different reflection lines\nresults in transposition. Taking the example from above, whereM(ch) = ( m\n1)[100, 100, -100, -100], we obtain the inversion\nM(ch’) = ( m1’)[-100, -100, 100, 100], when reflecting M(ch)\nthrough p1 = 50 and the transposition M(ch’’) = (m1’’)[100, 100,\n-100, -100] when reflecting M(ch’) through p2 = 150. Thedifference between p2 and p1 is p2 - p1 = 100, and the\ntransposition interval between M(ch) and M(ch’’) is 2( p2 - p1) =\n200 (Figure 2). The transposition interval is generally 2( p2 - p1)\nregardless where p2 and p1 are located. Allowing for the\nreflecting of single melota rather than the reflection of an entirem-chain, we can illustrate transposition as a reflection along areflection chain, which we will call M( \nχ), we obtain figure 3.123450100200300400\nnumber of tonerelative value of meloton M(ch)M(ch')\npp\npp\np\n123\n4\n5\n5 4 3 2 1400\n300\n200\n100\n0\n-100\nnumber of tonerelative value of melotonM(ch)\nM(ch')M(ch'')\np1p2\nreflection line through p1reflection line throughp2\nFigure 2: The m-chain M(ch) = (m1)[100, 100, - 100, -100] is\nmapped onto the chain M(ch’’) = ( m1’)[-100,-100, 100, 100]\nvia the reflection line through p1 = 50 and onto M(ch’’) =\n(m1’’)[100, 100, -100, -100] via the reflection line through p2\n= 150. The graph illustrates, that reflection along two lines\nresults in transposition.Figure 3: The m-chain M(ch) = (m1)[100, 100, -100, -100] is\nmapped onto the chain M(ch’) = (m1’)[-100,-100, 100, 100] via\nthe sequence of reflection points p1, p2, ... p5, thus effecting\nthe transposition of M(ch).\n2.3. General melotonic transformations\nGiven a m-chain M(ch) of the length n and a reflection chain\nM(χ) of the length n, we find that M(ch) will be mapped onto\nM(ch’), where for all mi /G30 M(ch) and all pi /G30 M(χ), that m’i = 2pi -\nmi, for all m’i  /G30 M(ch’). This is important, when defining\nreflections and translations (we will use the mathematical term\ninstead for transposition) within the vector-space Rn+1. As we will\nsee later, the composition of two specific reflections will enableus to produce similarity measures. However, we definereflections and translations on a more formal level first. For this\npurpose we define the m-vector :\n/G72m\nDefinition:\nGiven a m-chain of length n, with M(ch) = [m1, m2, ..., mn], we\ndefine the m-vector of length n+1 as:/G72m\n/G76mm\nm\nmn=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Gf7/Gf71\n2\n1.\n.\nThis enables us to define the reflection matrix R as:Definition\nRp\np\npn=−\n−\n−/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf710 02\n01 0 2\n00 1 2\n00 0 11\n2..\n.. .\n.. .\nMultiplying the reflection matrix R  by a m-vector , we/G72m\nobtain:\nRmp\np\npm\nm\nmmp\nmp\nmpnn n n⋅=−\n−\n−/Ge6\n/Ge8/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7/Ge6\n/Ge8/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7=−\n−\n−/Ge6\n/Ge8/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7\n/G7210 02\n01 0 2\n00 1 2\n00 0 1 12\n2\n2\n11\n21\n211\n22..\n.. .\n.. ..\n..\n.\nClearly, multiplying the reflection matrix by a m-vector, results\nin the reflection of this m-vector. As two reflections result intranslation, we will define the translation matrix, where each\ncomponent m\ni /G30 will be translated by the translation interval/G72mIi = 2(p2i - p1i). We define:\nDefinition:\nTI\nI\nIn=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Gf7/Gf710 0\n01 0\n00 1\n00 0 11\n2\n.. .\n.. .\n..\nMultiplying the translation matrix  T  with a m-vector , we get:/G72m\nTmI\nI\nIm\nm\nmmI\nmI\nmInn n n⋅=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7=+\n+\n+/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7\n/G7210 0\n01 0\n00 1\n00 0 1 1 11\n21\n211\n22..\n.. .\n.. .\n...\n..\n.\nThere exists a  complex algebraic structure between reflections\nand translations. However, the framework of the paper exceedsa discussion of this issue. Still, it might be worth mentioning thatthe composition of two reflection matrices results in a translationmatrix. This is the reason, why we had to introduce translations,although translations are of no significance in context ofmelotonic similarity. We are now equipped to consider melotonicsimilarity.\n3. MELOTONIC SIMILARITY\nIt seems the best way of approaching melotonic similarity is,\nwhen we consider two m-vectors and , such as:/G72m1/G72m2\n/G72/G72mm\nm\nmmm\nm\nmma\nma\nmann n111\n12\n1221\n22\n211\n12\n1\n11 1=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7=+\n+\n+/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7\n.\n..\n..\n. and  \nwith a as a constant\nWe will now reflect the m-vector through the 0-point Rn+1 via/G72m1the reflection matrix R0. We obtain the m-vector , with:/G72m'1\n/G72mm\nm\nmm\nm\nmnn'..\n.. .\n.. .\n...\n..\n.11\n21\n210 00\n01 0 0\n00 1 0\n00 0 1 1 1=−\n−\n−/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7=−\n−\n−/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7\nReflecting the m-vector onto the m-vector requires the/G72m'1/G72m2\nreflection matrix Rs given as:\nRmm\nmm\nmms\nnn=−−+/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7\n−−+/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7\n−−+/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7\n/Gf7\n10 0 22\n01 0 22\n00 1 22\n00 0 111 21\n12 22\n12..\n.. .\n.. .\n..\nAs we find:\nRm ms⋅=/G72 /G72'12\nIsolating the last column in the subspace Rn of  Rn+1, we can\ndefine the similarity vector :/G72\nVs\nDefinition:\n/G72\nVmm\nmm\nmmmm\nmm\nmms\nnn nn=−+\n−+\n−+/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7=−+\n−+\n−+/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7\n/Gf7\n/Gf7\n22\n2\n211 21\n12 22\n1211 22\n12 22\n12.\n..\n.\nWith m2i = m1i + a, we obtain the similarity vector: /G72Va\na\nas=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7\n/Gf7.\n.\nGeometrically, this means that  the similarity vector comes to\ncoincide with the diagonal of the space Rn. We further find for\nthe length of this vector:\n/G72\nVa ns=\n Clearly, the larger the transposition interval a is, the larger willbe the length of the similarity vector. According the vanEgmond, Povel & Maris (1996) melodic similarity decreaseswith increasing transposition interval. Thus, we will expect thatthe length of the similarity vector will be correlated to thetranspositional component of melotonic similarity. \nThe intervalic component of melotonic similarity,\naccording to Hofmann-Engl & Parncutt (1998) is a significant\npredictor. This is, when two m-vectors are not\n/G72 /G72mm12 and \nsimply transpositions of each other but deviate in shape. The\nsimilarity vector will then deviate from the dia gonal in Rn./G72\nVs\nWithout going into a lengthily discussion, the angle between the\nsimilarity vector and the diagonal of Rn is not a suitable measure\nof the intervalic similarity component, as small intervalicchanges can lead to a sudden increase of the angle. However, thedifferences between the components of the similarity vector are.\nGiven the similarity vector as: \n/G72\nVs\n/G72\nVs\ns\nss\nn=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf71\n2\n.\n.\nWe define the interval vector with as a vector/G72\nVi Is sii i=−+1\nof the dimension Rn-1\nDefinition:\n/G72\nVI\nI\nIi\nn=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7\n/Gf7\n−1\n2\n1.\n.We will give an example referring to the four m-chains M(ch1)\n= (m1)[1, -1], M(ch2) =  ( m1)[3, -3], M(ch3) = ( m1)[-1, 1]  and\nM(ch4) = ( m1)[1, 1] (where 1 unit might be one semitone).\nSetting m1 to be m1 = 0, we obtain the four m-vectors:\n/G72/G72 /G72 /G72mmm m123 40\n1010\n3\n010\n1\n010\n1\n2\n1\n=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7=−/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7/Gf7\n ,   ,   and  \nIn musical notation we obtain (setting the first tone to be c):\nM(ch1) =\nM(ch2) =\nM(ch3) =\nM(ch4) =\n                                         \n     \nWe find:\n/G72/G72/G72 /G72/G72/G72\n/G72/G72/G72 /G72/G72/G72\n/G72/G72/G72 /G72/G72/G72Vmm Vmm\nVm m Vmm\nVmm Vmmsi\nsi\nsi(, ) (, )\n() ( , )\n(, ) (, ),12 12\n13 1 3\n14 140\n2\n02\n2\n0\n2\n02\n2\n0\n0\n20\n2=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7=−/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7\n=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7=−/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7\n=/Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7=/Ge6\n/Ge8/Ge7/Gf6\n/Gf8/Gf7  and  \n  and  \n  and  \nThe lengths of the similarity vectors ,/G72/G72/G72Vmms(, )12\nand are identical (=2). Moreover, the/G72/G72/G72Vmms(, )13/G72/G72/G72Vmms(, )14\nlengths  of the interval  vectors  and /G72/G72/G72Vmmi(, )12/G72/G72/G72Vmmi(, )13\nare identical (= ), although  M(ch1) and M(ch2) have the 8/G72/G72\nFe\nn\ne\nn\ne\nne\nnFe\nn\ne\nn\ne\nnk\nns\nk\nns\nk\nnsk\nns\nink\nnI\nk\nnI\nk\nnIc\nc\ncncic\nc\nc n12\n1\n21\n1\n11\n112\n1\n122\n1\n121\n122\n212\n2\n222\n2\n221\n1\n1=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7\n/Gf7=/Ge6\n/Ge8/Ge7\n/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n=−\n−\n−\n−\n−\n−−\n=−\n−\n−\n−\n−\n−/Ge5\n.\n..\n.()\n()\n()  and  /Ge6\n/Ge8/Ge7\n/Ge7\n/Ge7\n/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7/Ge7\n/Ge7\n/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7\n/Gf7\n/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7/Gf7\n/Gf7\n/Gf7/Gf7=/Ge6\n/Ge8/Ge7\n/Ge7/Ge7/Gf6\n/Gf8/Gf7\n/Gf7/Gf7\n−\n−\n−\n=−\n/Ge5e\nnk\nnI\nin\nc i2\n22\n12\n11\n1()same contour, while M(ch1) and M(ch3) have different  contour.\nHowever, both chains  show the same interval difference. Asmentioned above contour differences  are imbedded withininterval differences. Thus, similarity and interval vector are inagreement with experimental findings.  Further, the comparisonof M(ch\n1) and M(ch4) demonstrates, that the length of the\nsimilarity vector (=2) does not necessarily produce a length of\n . Thus, similarity and interval vector are independent8\nsimilarity predictors. Because of the smaller length of the\ninterval vector, we expect M(ch1) to be more similar when\ncompared with M(ch4) than when compared with M(ch2).\nWe will expect that  melotonic similarity will be correlated to the\nlength or a derivative of the similarity vector (the longer thevector the smaller the similarity) and the deviation of thesimilarity vector from the diagonal measured by the length or aderivative of the interval vector (the longer the interval vector thesmaller the similarity).\nWhile the similarity vector takes the differences of two\ncorresponding melota  m\ni and m’i into account without considering\nany higher order (it does not matter where a pair of melota isplaced within a chain), the interval vector considers higher orderrelationships in as much as pair-wise groupings are covered (thedifference  between m\ni - mi+1 and m’i - m’i+1). We could take even\nhigher order relationships into account by forming thedifferences of the components of the interval vector in thefashion we formed the differences of the similarity vectorobtaining the interval vector. We then could from the differencesof these differences and so on. We then would obtain a series ofvectors with decreasing dimensions starting with the similarityvector of dimension n, followed by the interval vector with thedimension n-1, followed by the differences of the interval vectorproducing a vector of dimension n-2 and so on till we obtain a\nvector of dimension 1. Thus, higher order relationships wouldbecovered and a predictor of melotonic similarity could bemodelled around something comparable to a Taylor series.However, we expect that the similarity vector and intervalvector will be sufficient to produce useful approximations.\nBasing a similarity model exclusively on the lengths of the\nsimilarity and interval vector will still pr oduce several\ncomplications. Without going into much detail, we willconsider some experimental findings. Hofmann-Engl &Parncutt (1998) showed that keeping the transposition intervalconstant and varying the length of two melodic fragments (oneto five tones), that similarity judgements increase withincreasing length of the fragments. This seems to call forenveloping the components of the similarity vector by aexponential function giving more weight to earlier tones thanlater tones. Further, comparing two given m-chains of thelength n, which are identical except one interval, Hofmann-Engl\n& Parncutt (1998) found that by varying the length n, thatsimilarity judgements increase with increasing length n. Thus,a model will also have to be length sens itive. According to\nthese researchers, tempo is not a factor in melotonic similarity,but appears as a rhythmic factor. We also might expect thataspects concerning the shape of two m-chains as covered by theinterval vector will affected by the primacy/recency effect,where earlier and later tones are weighted more than are tonesin the middle. This might call for enveloping the components ofthe interval vector by a Gauss distribution. Finally, a suitablemodel will require some empirical constants which will have tobe determined through experimentation. However, at this pointwe might suggest a simple melotonic model of the form:\n:\nwhere is the transpositional similarity predictor and is the interval similarity predictor, k\n1 and k2 are empirical/G72\nF1/G72\nF2\nconstants determining the strength of each interval component, c1 and c2 are empirical constants determining ho much the length\nof a chain affects similarity, s1, s2, ... sn are the components of the similarity vector, I1, I2, ..., In-1 are the components of the\ninterval vector and n is the length of the compared  m-chains.An overall similarity could then be defined as:\nSFF=⋅/G72 /G72\n12\nIn fact, setting c1 = 1 and c2 = -2, we obtain a correlation of\n87% with the data as produced by the two experiments asconducted by Hofmann-Engl & Parncutt (1998).\nAn overall similarity model taking rhythmic, dynamic and\npitch features into account, might be of the form:\nSS S Smdr =+ +αβ γ\nwhere α,α,α,α,    β ,β,β,β,    γγγγ are empirical constants,  Sm as the pitch\nsimilarity, Sd as the dynamic similarity and  Sr as the\nrhythmic similarity\n This is not to say that this  the most adequate model, but it is\nfashioned based on some available data, some theoreticalconcepts and is similar to Shepard ’s (1987) model. However,\nthe model as it stands does not take into account any rhythmicor dynamic aspects nor does it pay tribute to harmonic featuresor emotional aspects. It also allows for the comparison ofm-chains only which have equal length. Further, we might findthat tones which are longer will bear more weight than shortertones. Thus, as the model stands it might be only suitable as apredictor for short isochronous m-chains.\n4. CONCLUSION\nThis paper set out to investigate an aspect of melodic\nsimilarity from a c ognitive angle.  We found that the term\npitch is little satisfying and we argued for replacing it by theterm meloton which was defined as a cognitive quality ofsound. We further proposed that melotonic similarity is bestapproached by defining a set of transformations (reflectionsand translations). Based on the composition of two specificreflections we were able to define a similarity and intervalvector which we propose to be somewhat sufficient to form thebasis for a melotonic predictor. Specifically, we presented asimple similarity model which admittedly shows limitationsbut might demonstrate that more complex and comprehensivemodels can be developed. However, before a morecomprehensive model will become available, many moreexperiments on melodic similarity will have to be conducted.Considering that we covered melotonic similarity only, we canby now conclude that the construction of sufficient models isa far more complex task than generally acknowledged, but atthe same time it appears to be an achievable task. 5. REFERENCES\nAdams, C. Melodic Contour Typology. Ethnomusicology, vol20.2 (1976)\nAnagnostopoulou, C., H örnel, D. & H öthker, K. Investigating\nthe Influence of Representations and Algorithms in MusicClassification. Proceedings of the AISB ‘99 Symposium on\nMusical Creativity. (1999, Edinburgh)\nAskill, J. The Subjective Pitch of Musical Chimes. (1997,\nOnline publication).\nhttp://faculty. millikin.edu/~jaskill.nsm.faculty.mu/chimes.ht\nml\nCambour opoulos, E. Melodic cue abstracti on, similiarty and\ncategory formation:A computational appraoch. In  Proceedingsof ICMPC 2000. (2000,  Keele University)\nCrawford, T., Iliopoulos, C. & Raman, R. String-Matching\nTechniques for Musical Similarity and Melodic Recognition.In:  Melodic Similarity - Concepts, Procedures, andApplications (ed. Hewlett, W. & Selfridge-Field, E.),Computing in Musicology 11. MIT Press, CA, 1998\nEdworthy, J. Pitch and contour in music processing.\nPsychomusicology, 2,  (1982),  44-46\nEdworthy, J. Interval and contour in melody processing. Music\nPerception, 2, (1985), 375-388\nDovey, M. & Crawford, T. Heuristic Models of Relevance\nRanking in Searching Polyphonic Music. In: ProceedingsDiderot Forum on Mathematics and Music, (1999, Vienna,Austria), pp 111-123.\nDowling, W. J. Recognition of inversion of melodies and\nmelodic contour. Perception & psychophysics, 12.5, (1971),417-421\nDowling, W. J. & Harwood, D., (1986). Music Cognition,\nNew York Academic Press, 1986\nDownie, S., (1999). Evaluating a Simple Approach to Musical\nInformation Retrieval: Conceiving Melodic N-Grams as Text.Ph.D. dissertation, University of Western Ontario\nEgmond van, R. & Povel, D-J.. & Maris, E. The influence of\nheight and key on the perceptual similarity of transposedmelodies. Perception & Psychophysics, vol. 58, 1996,1252-1259Franc és, R. The perception of music. (trans. Dowling),\nHillsdale, New York, 1988\nGoldstone, R. L. The Role of Similarity in Categorization:\nProviding a Groundwork. (1998, Online publication)http://cognitrn.psych.indiana.edu/rgoldsto/s im&cat.html,\nIndiana University\nHammel, B. Motivic Transformations & Lie Algebras. (1999,\nOnline publication) http://graham.main.nc.us/~bhammel/Music/Liemotiv.html\nHofmann-Engl, L. J. Beitr äge zur theoretischen\nMusikwissenschaft. M 65+1, vol. 2, 1989, TechnicalUniversity Berlin\nHofmann-Engl, L. J. Virtual Pitch and pitch salience in\ncontemporary composing.. In Preceedings the VI BrazilianSymposium on Computer Music at (1999, PUC Rio deJaneiro)\nHofmann-Engl, L. & Parncutt, R.. Computational modeling of\nmelodic similarity judgments - two experimetns onisochronous melodic fragments. (1998, Online publication)http://www.chameleongroup.org.uk/research/sim.html\nHofmann-Engl, L. Review : Review of W.B. Hewlett & E.\nSelfridge-Field, eds., Melodic Similarity: Concepts,Procedures, and Applications.(Cambridge, Massachusetts:MIT Press, 1999).  MTO 5.4, 1999, MIT CA \nHoutsma, A J. M. & Goldstein, J. L. The central origin of the\npitch of complex tones: Evidence from musical intervalrecognition. Journal of the Acoustical Society of America, vol.51, 1972, 520-529\nIsaacson, E. Issues in the Study of Similarity in Atonal Music.\nMTO 2.7, 1996, MIT CA\nKassler, M.. Toward Musical Information Retrieval.\nPerspectives of Music 4.2, 1966,  59-67\nKluge, R. Ähnlichkeitskriterien f ür Melodieanalyse.\nSystematische Musikwissenschaft, 4.1-2, 1996, 91-99\nLewin, D. Forte ’s Interval Vector, My Interval Function, and\nRegner ’s Common-note Function. Journal of Music Theory 21,\n1977, 194-237\nMaidin O, D. & Fernstr öm, M. The Best of two Worlds:\nRetrieving and Browsing. Proceedings of COST G-6 on DigitalAudio Effects, (2000, Verona)Mazzola, G. Geometrie der T öne. Birkh äuser, Basel, 1990\nMorris, R. A Similarity Index of Pitch-class Sets. Prespectives\nof New Music 18, 1979\nPalmer, S. The psychology of perceptual organization. A\ntransformational approach. Human and Machine Vision, NewYork Academic Press, 1983, 269-339.\nRasch, R. A. & Plomp, R.The Perception of Musical Tones.\nIn: The Psychology of Music (e.g.. Deutsch),  Academic Press,New York, 1982\nReti, R. The Thematic Process in Music. New York,\nMacmillan Company, 1951 \nRohwer, J. Die harmonischen Grundlagen der Musik.\nBärenreiter, Kassel, 1970\nSchönberg, A. Fundamentals of Musical Composition. St.\nMartin ’s Press, New York, 1967\nSchouten, J. F. The perception of subjective tones.\nProceedings of the Koninklijke Nederlandse Akademie vanWetenschappen, 41, 1938, 1418-1424\nSmith, L., McNab, J. & Witten, I. Sequence-Based Melodic\nComparison: A Dynamic-Programming Approach. In:Melodic Similarity - Concepts, Procedures, and Applications(ed. Hewlett, W. & Selfridge-Field, E.), Computing inMusicology 11, 1998, MIT Press, CA\nSundberg, J.The Science of Musical S ound. Academic Press,\nNew York, 1991\nTekman, H. G. A Multidimensional Study of Preference\nJudgments for Pieces of Music. Psychological Report, 82,1998, 851-860\nTerhardt, E Die Psychoakustischen Grundlagen der\nmusikalischen Akkordgrundt öne und deren algorithmische\nBestimmung. In: Tiefenstruktur der Musik, TechnischeUniversit ät Berlin, Berlin, 1979\nTerhardt, E. & Stoll, G. & Seewann, M. Algorithm for\nextraction of pitch and pitch salience from complex tonalsignals. Journal of the Acoustical Society of America, vol. 71,1982, 679-688\nWhite, B.  Recognition of distorted melodies. American\nJournal of Psychology, 73, 1960, 100-107"
    },
    {
        "title": "GUIDO/MIR an Experimental Musical Information Retrieval System based on GUIDO Music Notation.",
        "author": [
            "Holger H. Hoos",
            "Kai Renz",
            "Marko Görg"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417517",
        "url": "https://doi.org/10.5281/zenodo.1417517",
        "ee": "https://zenodo.org/records/1417517/files/Holger01.pdf",
        "abstract": "Musical databases are growing in number, size, and complexity, and they are becoming increasingly relevant for a broad range of academic as well as commercial applications. The features and performance of musical database systems critically depend on two factors: The nature and representation of the information stored in the database, and the search and retrieval mechanisms available to the user. In this paper, we present an experimental database and retrieval system for score-level musical information based on GUIDO Music Notation as the underlying music representation. We motivate and describe the database design as well as the flexible and efficient query and retrieval mechanism, a query-by-example technique based on probabilistic matching over a clustered dataset. This approach has numerous advantages, and based on experience with a first, experimental implementation, we believe it provides a solid foundation for powerful, efficient, and usable database and retrieval systems for structured musical information. 1",
        "zenodo_id": 1417517,
        "dblp_key": "conf/ismir/Holger01",
        "keywords": [
            "Musical databases",
            "Growing in number",
            "Size",
            "Complexity",
            "Relevance",
            "Academic",
            "Commercial applications",
            "Features",
            "Performance",
            "Factors"
        ],
        "content": "GUIDO/MIR —anExperimental\nMusicalInformation RetrievalSystem\nbasedonGUIDOMusicNotation\nHolgerH.Hoos\n\u0000\u0002\u0001andKaiRenz\n\u0001andMarkoG¨org\n\u0001\nAbstract\nMusicaldatabases aregrowinginnumber,\nsize,andcomplexity,andtheyarebecom-\ningincreasingly relevantforabroadrange\nofacademic aswellascommercial applica-\ntions.Thefeaturesandperformance ofmu-\nsicaldatabase systemscritically dependon\ntwofactors:Thenatureandrepresentation of\ntheinformation storedinthedatabase, and\nthesearchandretrievalmechanisms avail-\nabletotheuser.Inthispaper,wepresentan\nexperimental database andretrievalsystem\nforscore-levelmusicalinformation basedon\nGUIDOMusicNotation astheunderlying\nmusicrepresentation. Wemotivateandde-\nscribethedatabasedesignaswellastheﬂex-\nibleandefﬁcientqueryandretrievalmech-\nanism,aquery-by-e xampletechnique based\nonprobabilistic matching overaclustered\ndataset.Thisapproach hasnumerous advan-\ntages,andbasedonexperience withaﬁrst,\nexperimental implementation, webelieveit\nprovidesasolidfoundation forpowerful,ef-\nﬁcient,andusabledatabaseandretrievalsys-\ntemsforstructured musicalinformation.\n1Introduction\nMultimedia databases playanimportant role,espe-\nciallyinthecontextofonlinesystemsavailableonthe\nWorldWideWeb.Asthesedatabases growinnumber,\nsize,andcomplexity,itbecomes increasingly impor-\ntanttoprovideﬂexibleandefﬁcientsearchandretrieval\ntechniques. Whendealingwithmusicaldata,twomain\ndifﬁcultiesareencountered: Firstly,themultidimen-\nsional,oftencomplexstructure ofthedatamakesboth\ntheformulation ofqueriesandthematching ofstored\u0003Corresponding author.UniversityofBritishColumbia,\nDepartment ofComputer Science,2366MainMall,Vancou-\nver,BC,V6T1Z4,Canada, hoos@cs.ubc.ca\u0004\nTechnische Universit¨atDarmstadt, Fachbereich Infor-\nmatik,Wilhelminenstr .7,D-64283 Darmstadt, Germany,\nrenz@iti.informatik.tu-darmstadt.dedatawithagivenquerydifﬁcult.Secondly,thereis\noftenaconsiderable amountofuncertainty orinaccu-\nracyinthequeryand/orthedata,stemming fromlimi-\ntationsofthemethodsusedforobtaining queries,such\nas“Query-By-Humming” [12],orforacquiring musi-\ncaldata,suchasautomated performance transcription,\naswellasfromsimplehumanerrorwhenenteringdata.\nWhilethereisastrongandincreasing interestinda-\ntabaseandretrievalsystemsforsoundandsound-level\ndescriptions ofmusic,manyapplication contexts(par-\nticularlyinmusicalanalysis, composition, andperfor-\nmance)beneﬁtfromorrequirehigher-level,structured\nmusicrepresentations. Consequently ,thereisagrow-\ningbodyofresearch onmusicaldatabases andmu-\nsicinformation retrievalbasedonstructured, score-\nlevelmusicrepresentations (see,e.g.,[3;21;8]).In\nthiswork,wefocusoncontent-based musicinforma-\ntionretrievalfromadatabase ofscore-levelmusical\ndatabasedonthequery-by-e xampleapproach [2].The\nmaincontributionsofourwork,canbesummarised as\nfollows:\n1.WeuseGUIDOMusicNotation [16]asthemu-\nsicrepresentation underlying thedatabaseaswell\nasforformulating queries.Compared totheuse\nofMIDIandvariousothermusicrepresentation\nformats,thisapproach hasanumberofconcep-\ntualandpractical advantageswhichwillbedis-\ncussedindetailinthefollowingsections. Weﬁnd\nthatGUIDOisparticularly suitableforformulat-\ningqueriesinaquery-by-e xampleapproach, and\nweoutlinehowasmallandnaturalextensionof\nGUIDOallowstheexplicitandlocalised repre-\nsentation ofuncertainty associated withagiven\nquery.\n2.Weintroduce anovelmusicretrievalmechanism\nbasedonprobabilistic modelsandahierarchically\nclustered musicaldatabase. Usingprobabilistic\nmodelsformusicalinformation retrievalhasthe\nadvantageofofferingnatural,elegant,andﬂexible\nwaysofscoringexactandapproximate matches\nbetweenpiecesinthedatabaseandagivenquery.\nWhileinthiswork,weintroduce andillustratethis\ngeneralconceptusingrathersimpleprobabilistic\nmodels,theapproach canbeeasilygeneralised tomorecomplexprobabilistic models.\n3.Wepresentanexperimental databaseandretrieval\nsystemwhichimplements thedesignandtech-\nniquesproposed inthispaper.Thisprototypical\nsystem,whichisavailableontheWWW,supports\nvariouscombinations ofmelodicandrhythmic\nquerytypesforretrievinginformation fromadata-\nbaseofpiecesofvaryingcomplexity.Thesystem\nisimplemented inPerl[1]andhighlyportable; the\nunderlying, object-oriented andmodular design\nfacilitates theimplementation ofdifferentsearch\nandretrievaltechniques andtheinvestigation of\ntheirbehaviour.\nInthefollowing,wepresentanddiscussouroverall\napproach inmoredetail.Westartwithabriefintroduc-\ntionofGUIDOMusicNotation anddiscussitsusein\nthecontextofmusicaldatabase andretrievalsystems.\nInSection3,weoutlineourexperimental musicaldata-\nbasedesignandimplementation. Section4isthecore\nofourwork;itmotivatesanddescribes ourapproach\ntomusicinformation retrieval.Relatedapproaches are\nbrieﬂydiscussed inSection5,andSection6presents\nsomeconclusions andoutlinesanumberofdirections\nforfutureresearch.\n2WhyGUIDO?\nGUIDOMusicNotation1isageneralpurposefor-\nmallanguage forrepresenting scorelevelmusicina\nplatform independent, plain-textandhuman-readable\nway[16].TheGUIDOdesignconcentrates ongen-\neralmusicalconcepts (asopposedtoonlynotational,\ni.e.,graphical features). Itskeyfeatureisrepresenta-\ntionaladequacy ,meaningthatsimplemusicalconcepts\nshouldberepresented inasimplewayandonlycom-\nplexnotionsshouldrequirecomplexrepresentations.\nFigure1containsthreesimpleexamplesofGUIDO\nMusicNotation andthematching conventionalmusic\nnotation.\nTheGUIDOdesignisorganisedinthreelayers:Ba-\nsic,Advanced,andExtended GUIDOMusicNotation.\nBasicGUIDOintroduces thebasicGUIDOsyntactical\nstructures andcoversbasicmusicalnotions;Advanced\nGUIDOextendsthislayertosupportexactscorefor-\nmattingandmoresophisticated musicalconcepts; and\nExtended GUIDOintroduces featureswhicharebe-\nyondconventionalmusicnotation. GUIDOMusicNo-\ntationisdesigned asaﬂexibleandeasilyextensible\nopenstandard. Thus,itcanbeeasilyadaptedandcus-\ntomisedtocoverspecialised musicalconceptsasmight\nberequiredinthecontextofresearchprojectsincom-\nputational musicology .GUIDOhasnotbeendevel-\nopedwithaparticular application inmindbuttopro-\nvideanadequate representation formalism forscore-\nlevelmusicoverabroadrangeofapplications. The\n1GUIDOMusicNotation isnamedafterGuidod’Arezzo\n(ca.992-1050), arenownedmusictheoristofhistimeandim-\nportantcontributortotoday’sconventionalmusicalnotation.intended application areasincludenotationsoftware,\ncompositional andanalytical systemsandtools,musi-\ncaldatabases, performance systems,andmusiconthe\nWWW.Currently ,agrowingnumberofapplications is\nusingGUIDOastheirmusicrepresentation format.\nGUIDOvs.MIDI\nCurrently ,virtuallyevery(content-based) MIRsystem\nworksonMIDIﬁles.Thetwomainreasonsforthat\nare:\u0005theenormous amountofmusicavailableasMIDI\nﬁlesontheWWW\u0005thelackofacommonly usedandaccepted repre-\nsentation formatforstructured music\nAlthough Standard MIDIFile(SMF)formatisthe\nmostcommonly usedmusicinterchange format,itdoes\nnotadequately supportactivitiesotherthanplayback.\nMIDIwasneverintendedtobethenotation(andmu-\nsic)interchange formatthatithasbecometoday.\nThereareseveralreasons,whyMIDIisnotverywell\nsuitedforMIR.AMIDIﬁlecontains alowlevel-\ndescription ofmusicwhichdescribes onlythetiming\nandintensity ofnotes.Sincestructural information\nsuchaschords,slursortiescannotbestoredinaStan-\ndardMIDIﬁle2,ahigh-ormultileveldescription is\nnotpossible. Someofthebasiclimitations ofaMIDI\nﬁlearethelackofdifferentiation betweenenharmonic\nequivalents(e.g..C-sharpandD-ﬂat),andlackofpre-\ncisioninthedurations betweenevents(whichareex-\npressedinMIDI-ticks).\nOurMIRsystemhasbeenimplemented usingGUIDO\nasitsunderlying musicrepresentation language. To\nstillbeabletousethehugebodyofMIDIﬁlesonthe\nWWW,ourgrouphasdevelopedconvertersbetween\nGUIDOandMIDI3.\nGUIDOvs.XML\nXMLisasimpliﬁed subsetofSGML,ageneralmarkup\nlanguage thathasbeenofﬁciallyregisteredasastan-\ndard(ISO8879). Becauseofitsincreasing popularity ,\ntherehavebeenquiteanumberofattemptstouseitfor\nstoringmusicaldata[14;6]andaswellasforMusicIn-\nformation Retrieval[26].XMLhasobviousandunde-\nniablestrengths asageneralrepresentation language:\nitisplatformindependent, text-based, human-readable\nandextensible. Additionally ,byusingXMLtorepre-\nsentmusic,onegainstheadvantageofusingastan-\ndardisedmetalanguage forwhichagrowingnumberof\ntoolsarebecoming available.Toourknowledgenone\noftheapproaches tomusicrepresentation usingXML\npublished sofarhasyetgainedwideacceptance. One\n2Usingnon-standard techniques, itispossibletostoread-\nditionalinformation inMIDIﬁles;however,thesemecha-\nnismsarenotpartofthestandard\n3GMN2MIDI andMIDI2GMN areavailableatourweb\nsitehttp://www.salieri.org/guido[\\key<\"A\"> a1/4 hc#2d/8e/16 f#16_/8\ng#*1/4. {a1/4,c#,e2,a2} ]\n[\\key<\"C\"> \\meter<\"4/4\"> g1/4 ee/2\nf/4dd/2c/4defggg/2g/4ee/2\nf/4dd/2c/4eggc/1]\n{[\\tempo<\"Vivace\">\n\\meter<\"5/8\"> \\intens<\"p\"> \\sl(\\bm(g1*1/8 ab)\n\\bm(b& c2)\\bm(c# b1ab&a&))],\n[\\meter<\"5/8\"> \\sl(g1*3/8 d/4c#*3/8 d/4)]}Vivace\n4\n7\n10\n131\nFigure1:SimpleexamplesofGUIDOMusicNotation; morecomplexexamplescanbefoundin[17,18].\nofthereasonsforthisseemstolieinthecomplexity\nofmusicalstructure; justusinga“new”formatdoes\nnotautomatically leadtoasimpleandeasytousedata\nstructure.\nToallowtheuseofXML-tools whereneeded,wehave\ndevelopedGUIDO/XML, aXMLcompliant format\nthatcompletely encapsulates GUIDOwithinaXML\nstructure. UsingGUIDO/XML issimple:wepro-\nvidetoolsthatconvertGUIDOMusicNotation ﬁles\nintoGUIDO/XML ﬁlesandviceversa.Usingthisap-\nproach,wecancontinuetouseGUIDOMusicNotation\nanditsassociated tools(SALIERI, NoteAbility ,Note-\nServer,ParserKit,etc.)butarealsofreetouseanycur-\nrentoremergingXMLtool.\nOneadvantageofXMLisitsabilitytostoresocalled\nmetadata.Apieceofmusiccanbeassociated with\nacomposer ,atitle,apublisher,apublishing dateand\nevenversioninformation. Onecaneasilyaddnew\nmetadataﬁeldsencoding additional musicalinforma-\ntion(likeforexampleperformance-related datafora\npiece).UsingGUIDO/XML inconjunction withaset\nofmetadatainformation canleadtocomplete XML-\ncompatible descriptions ofstructured music.\nUsingGUIDOMusicNotation forMusical\nDatabases andMIR\nAswehaveshown,GUIDOMusicNotation offersan\nintuitiveyetcomplete approach forrepresenting musi-\ncaldata.UsingGUIDOinmusicaldatabases isthere-\nforeastraightforwardtask:becauseitisaplain-text\nformat,noadditional toolsarenecessary tocreate,ma-\nnipulateortostoreGMNﬁles.Itisalsopossibletouse\nstandard text-compression toolstominimise storage\nspace(thesizeofcompressed GMNﬁlescompares to\nthesizeofMIDIﬁles).Byusingexistingtoolslikethe\nGUIDONoteServ er[25],onecancreateconventionalmusicnotationfromGUIDOdescriptions quickly.\nBecause ofitsrepresentationally adequate design,\nGMNisalsoverywellsuitedforMIR:Queriescan\nbewrittenas(enhanced) GUIDOstrings.Userswith\nabackground inGUIDOcanspecifyevencomplex\nqueriesinaneasyway.Byusingadditional toolslike\navirtualpiano-keyboard,evennoviceusersareableto\nbuildqueriesquickly.InSection4itwillbeshown,\nhowusingGUIDOastheunderlying musicrepresen-\ntationlanguage simpliﬁes thetaskofbuildingquery-\nenginesandwealsodemonstrate, howaslightexten-\nsiontoGUIDOleadstoanintuitiveapproach toap-\nproximate matching.\nOtherrepresentation formats(likeXML)donotpro-\nvidethisfeature:anewquerylanguage hastobecre-\natedinordertoaccessthestoredinformation. Asthere\nisnostandardformusicalqueries(likeSQLisforre-\nlationaldatabases systems) awholerangeofdifferent\nmusicalquerylanguages willbeproposed inthefuture.\n3TheExperimental GUIDODatabase\nAswasshownintheprevioussection,GUIDOMusic\nNotation iswellsuitedasageneralmusicrepresenta-\ntionlanguage. Ourprototypical MIRsystemisbuild\nonthebasisofanexperimental GUIDODatabase that\nwillbedescribed inthissection.\nTheGUIDODatabase containsmusicalpiecesstored\nasGMNﬁlesalongwithsomeadditional information\nwhichisusedforefﬁcientretrieval(thiswillbedis-\ncussedinmoredetailinthenextsection). Insteadof\nbuildingourmusicaldatabasebasedonaconventional\ndatabase system,wedecidedtoimplement itinPerl\n[1],usingtheregularﬁlesystemforinformation stor-\nage.Thisdesignoffersanumbersofadvantages:\u0005ThePerllanguage hasgoodsupportformanipula-ClusterSearch\nRhythmusSEQ\nIntervallSEGM\nM.Content\nMatrixMusical\nContentMusical\nObjectsSearch\nEngine\nroot\ncontent\ncontent\ncontent\n1obj\n#Voicesobj{max(#obj) = 32}\nFigure2:Overviewoftheobject-oriented designofour\nexperimental database andinformation retrievalsys-\ntem.\ntingtextualdata(suchasGUIDOorHTMLdata)\nandiswellsuitedforrapidprototyping.\u0005UsingPERLallowsforveryeasyintegrationin\nonlinesystems.\u0005Diskstorageischeap,andtextualdatacanbe\ncompressed efﬁcientlyusinggeneralﬁlecompres-\nsiontechniques; furthermore, modernoperating\nsystemsallowtime-efﬁcientﬁleaccessthrough\ncaching.\u0005Itiseasyandreasonably efﬁcienttobuildindex\nstructures onaﬁlesystem.\u0005Maintenance andupdating ofthedatabase isrel-\nativelyeasy,sincefunctionality oftheoperating\nsystemandunderlying ﬁlesystemcanbeused.\nOnedrawbackofthisapproach isthefactthatstandard\ndatabasefunctionality ,suchasconcurrent writeaccess,\ntheimplementation ofaccesscontrol,andtransaction\ncontrolwouldhavetobeimplemented separately and\narecurrently notsupported. However,itshouldbe\nnotedthatinthecontextofmusicinformation retrieval,\nwriteoperations (i.e.,modiﬁcations oforadditions to\nthedatabase) arerelativelyrarecompared toreadac-\ncess(suchasretrieval)andusuallyrestricted toselected\nusers.Interestingly ,thesameholdsformanylargeand\nheavilyusedonlinebiomedical andliterature database\nsystems. Ourmodelisbasedonanoff-lineupdate\nmechanism, wherepiecesareaddedtothedatabaseby\ntakingthedatabase off-lineandgenerating /updating\ntheindexstructurewhilenootheraccessispermitted.\nOurimplementation followsanobjectorienteddesign\nwhichisgraphically summarised inFigure2.Details\noftheimplementation canbeseenfromthesourcesof\nourPerlmodules, whicharepubliclyavailablefrom\nhttp://www.salieri.org/guido/mir .Currently ,ourdatabase systemcontains about150\nﬁles,mostofwhichhavebeenconvertedtoGUIDO\nfromotherformatslikeabcandMIDI.Because the\nconversionfromMIDItoGUIDOisacomplextask\nthatsometimes needsmanualinteraction, extending\nthiscorpusistime-consuming. However,weexpect\nthatbasedonrecentimprovementsofourconversion\ntools,wewillbeabletoextendourdatabasetoamuch\nlargerbodyofﬁles.Ourexperimental systemisnot\noptimised forspeed,andwearequitecertainthatwe\nwillneedtoincreaseitsefﬁciencywhenoperating ona\nmuchlargerdatabase.\n4TheExperimental MIREngine\nOurmusicinformation retrievalapproach isbasedon\nthe“QuerybyExample” (QBE)paradigm [2].QBE\nhastheadvantagethatqueriescanbeformulated inan\neasyandintuitiveway.Inmanysearchandretrieval\nsituations, usersappeartoprefertheQBEapproach\novertheuseofquerylanguages, whichsupportmore\ncomplexquerieslikebooleanexpressions, wildcards,\norregularexpressions.\nQueryTypes\nManymusicinformation retrievalsystemsareprimar-\nilybasedonmelodic, i.e.,pitch-related information4.\nTypesofmelodicinformation thatcanbeusedfor\nqueriesareabsolutepitches,pitch-classes, intervals,in-\ntervalclasses(e.g.,large/smallintervals)andmelodic\ntrends(e.g.,up/down/equal). Alternately oraddition-\nally,rhythmic information canbeusedasabasisfor\nretrieval.Again,varioustypesofrhythmic informa-\ntioncanbedistinguished: absolutedurations, relative\ndurations (ordurationratios),ortrends(e.g.,shorter,\nlonger,equal).\nOurprototypical MIREnginesupportsqueriesthatar-\nbitrarilycombine oneoutofﬁvetypesofmelodicin-\nformation withoneoutofthreetypesofrhythmic infor-\nmation.Themelodicqueryfeaturesarethefollowing:\nabsolutepitch(suchasc1,d#2,etc.),intervals(such\nasminorthird,majorsixth,etc.),intervaltypes(such\nassecond,fourth,etc.),intervalclasses(equal,small,\nmedium, large),melodictrend(upwards,downwards,\nstatic).Thethreecurrently supported rhythmic features\nareabsolutedurations (suchas1/4,1/8.,etc.),relative\ndurations (suchas1:2,4:3,etc.),andrhythmic trends\n(shorter,longer,equal).Allthesefeaturesaredeter-\nminedforindividualnotesorpairsofnotes,respec-\ntively,suchthataqueryeffectivelyspeciﬁessequences\nofthesefeatures.\nSincewearefollowingaQBEapproach, thesevari-\nousquerytypes(andtheircombinations) correspond\nmerelytodifferentinterpretations ofthesamemusical\nquery.Forinstance, theGMNfragment[g1/4e1/4\ne1/4]canbeusedasapurelymelodicquery,usingab-\n4see[21]foranoverviewofMIRsystemsandtheirpitch\nrepresentationssolutepitch.Inthiscase,onlythemelodicsequence\n[g1e1e1]wouldbematched, regardlessofrhythm.\nThesamefragment, usedasapurelyrhythmical query\nwouldalsomatch[e1/4e1/4e1/4],andeven[/4/4\n/4].Forinformation retrievalbasedontheQBEap-\nproach,thisparadigm of“query=data+featureselec-\ntion”isverynatural;thisappliesparticularly tomulti-\ndimensional, complexdatasuchasmusicalorgraphi-\ncalobjects.\nWecanalsodistinguish exactretrieval,wherethetask\nistoﬁndexactoccurrences oftheinformation speci-\nﬁedinthequery,orapproximate (orerror-tolerant) re-\ntrieval,whereacertainamountofdeviationbetween\nthequeryinformation andthedatatoberetrievedis\npermitted. Here,weﬁrstconsiderexactretrieval,and\nlaterdiscussbrieﬂyanextensionofourapproach toap-\nproximate retrieval.\nProbabilistic Models\nThemusicinformation retrievalapproach takenhereis\nbasedonthegeneralideaofcharacterising andsum-\nmarising musicalstructure usingprobabilistic models.\nSearching forafragment withaspeciﬁcmusicalstruc-\nture(speciﬁed inaquery)canthenbedonebyproba-\nbilisticmatching usingthesemodels.Here,wepropose\narathersimpleapproach, whichisbasedonﬁrst-order\nMarkovchainsformodeling themelodicandrhyth-\nmiccontours ofamonophonic pieceofmusic[15;\n9].Currently ,wefocusonhorizontal queriesonly,\ni.e.querieswhichonlyinvolvemonophonic music,and\ntreatpieceswithmultiplevoices(orchords)ascollec-\ntionsofmonophonic pieces.\nIntuitively,a(discrete time)ﬁrst-order Markovchain\nisaprobabilistic modelforaprocesswhichateach\ntimeisinastate,andateachtimestepprobabilisti-\ncallychangesintoasuccessor state(whichcanbethe\nsameasthecurrentstate)withaprobability thatonly\ndependsonthepresentstate.Hence,ﬁrst-order Markov\nchainsarecharacterised bythetransition probabilities\u0006\b\u0007\n\tforenteringstate \u000basthenextstate,whenthecur-\nrentstateis \f.5Thetransition probabilities characteris-\ningaﬁrst-order Markovchaincanbewritteninformof\nasquarematrix \r\u000f\u000e\u0011\u0010\n\u0006\u0007\n\t\u0013\u0012whoserowsandcolumin-\ndicescorrespond tothestatesofthechain.Itshouldbe\nnotedthatﬁrst-order Markovchainswithaﬁnitesetof\nstatescorrespond tonon-deterministic ﬁnitestatema-\nchines(FSMs),andcanalsobeseenasaspecialcase\nofHiddenMarkovModels(HMMs) whereemissions\nforallstatesaredeterministic [24].\nIntheapplication considered here,weconceptually\nuseoneﬁrst-order Markovchainforeachmelodicand\nrhythmic querytypeandeachgivenmonophonic piece.\nThestatesofthesechainscorrespond topitchesforab-\n5Inthiswork,weonlyusehomogenous Markovchains,\ni.e.chains,forwhichthetransition probabilities donot\nchangeovertime.InSection6webrieﬂydiscusshowand\nwhyamoregeneralapproach equivalenttousinginhomoge-\nneouschainsmightbeadvantageous.solutepitchqueries,tointervalsforintervalqueries,\ntorelativedurations forrelativerhythmic queries,\netc.Thecorresponding transition probabilities are\ndetermined fromfrequencycountsoverneighbouring\npitches,intervals,notedurations, etc.whicharenor-\nmalisedtoobtainproperprobabilities.\nFigure3showsthetransition probability matricesfor\ntheMarkovchainscharacterising thesequences ofab-\nsolutepitchesanddurations forthe“H¨anschenKlein”6\nexamplefromthesecondrowofFigure1aswellas\nthecorresponding representations asnon-deterministic\nﬁnitestatemachines; thelatterrepresentation isoften\nmoreintuitiveandconcise.\nThetransition probabilities oftheseﬁrst-order Markov\nchainssummarise statistical properties ofthepieces\ninthemusicaldatabase. Whentryingtoﬁndexact\nmatchesbetweenagivenqueryandpiecesintheda-\ntabase,wecanmakeuseofthefollowingsimpleobser-\nvation:Ifforagivenpiece \u0014,atransition thatispresent\ninthequery(e.g.,anupwardﬁfthfollowedbyadown-\nwardthird)hasprobability zero,thereisnoexactmatch\nofthequeryin \u0014.Unfortunately ,theconverseisnot\ntrue:Therearecaseswherethereisnoexactmatchof\nthegivenqueryin \u0014,yetforanyneighbouring features\ninthequery,thecorresponding transition probabilities\nin\u0014aregreaterthanzero.\nGenerally ,thekeyideaofinformation retrievalbased\nonprobabilistic modelsisthefollowing:Givenapiece\u0014andaprobabilistic model\u0015\u0016\u0010\u0017\u0014\n\u0012forthispiece,this\nmodelcanbeusedtogeneratepieces\u0014\u0019\u0018withproper-\ntiessimilarto\u0014.Here,theseproperties arethetransi-\ntionprobabilities oftheﬁrst-order Markovchainswe\nuseforcharacterising sequences offeatures. Toas-\nsessthepotential ofamatchgivenaquerysequence\u001a\u000e\n\u001a\u001c\u001b\u001e\u001d\u001f\u001a! \"\u001d!#$#%#$\u001d\u001f\u001a'&(wherethe\n\u001a!(areindividualfeatures\nsuchaspitches)andacandidate piece \u0014fromtheda-\ntabase,wedetermine theprobability )*\u0010\n\u001a,+\u0015\u0016\u0010\u0017\u0014\n\u0012\u001f\u0012that\ntheprobabilistic modelof \u0014,denoted \u0015\u0016\u0010-\u0014\n\u0012,generates\nthefeaturesequence corresponding tothegivenquery\u001a.Foroursimpleprobabilistic model, \u0015\u0016\u0010-\u0014\n\u0012ischarac-\nterisedbyamatrixoftransition probabilities\n\u0006\u0007\n\t,and\ntheprobability ofgenerating thequerysequence given\nthatmodelisgivenbytheproductofthetransition\nprobabilities\n\u0006\b\u0007\n\twhichcorrespond toallneighbouring\nfeaturesinthequerysequence:)*\u0010\n\u001a,+\u0015\u0016\u0010\u0017\u0014\n\u0012\u001f\u0012\u000e/.10\n\u0006\b\u0007\n\t+\f2\u000e\n\u001a'(3\u001d\u000b4\u000e\n\u001a!($56\u001b7\u001d'8:9<;>=<?A@\nIntuitively,thisprobability scorewillbehigherfor\npieceswhichcontainmanyexactmatches thanfor\npieceswhichcontainfewexactmatches, andasex-\nplainedabove,itwillbezeroforpieceswhichdonot\ncontainanyexactmatchesatall.Sincetheprobabilis-\nticmodelweuseisverysimplistic andiscertainly far\nfromcapturing allrelevant(statistical) featuresofthe\npiecesinthedatabase, wecannotexpectthisintuition\ntobefullymet.However,ourpracticalexperience with\n6“H¨anschenKlein”isapopuparGermanchildrens songB\npitch C\nD E F G HD I IKJ L IKJ L I IE IKJ M IKJ M IKJ N I IF I I IKJ OPOQIKJ LQIKJ RTSG I IKJ SPS I I IKJ OPOH IKJ RTM I IKJ NVU I IKJ LPW\ncd\nef\ng14%50%50%40%40%\n66%\n33%20%\n16%50%\n29%33%\n57%\nB\nrhythm C\nXY\nXZ\nXXXY\nIKJ S3WQIKJ NV[\\IKJ IPSXZ\nR I IXX\nI I I1/4\n1/228%100%67%\n6%\n1/1\nFigure3:Transitionprobability matricesandFiniteStateMachines forabsolutepitchandabsoluterhythm\ntheexperimental systemdescribed hereindicates that\nevenwhenusingthissimplistic probabilistic model,the\ncorrelation betweenprobability scoresandpiecescon-\ntainingexactmatchesissufﬁcienttobeusedasabasis\nforamusicinformation retrievalmechanism.\nObviously,thetransition probability matrices corre-\nsponding torelatedfeatures, suchasabsolutepitches\nandintervals,arenotindependent, andinfacttwo\nmatrices (oneforabsolute pitches,oneforabsolute\ndurations) aresufﬁcientasabasisforhandling any\ntypeofquery.However,inpractice, thereisatrade-\noffbetween theamountofpre-computed statistical\ndata(transition probabilities), andthetimerequiredfor\nmatching agivenqueryagainstaprobabilistic model\nthatmightnotbeexplicitlyavailable.\nNote:Thetechniques presented heredonotdirectly\nsupporttheefﬁcientsearchofmatcheswithinagiven\npiece(whichmighthavebeenselectedbasedona\nhighprobability scoreforagivenquery). Toefﬁ-\ncientlysearchmatcheswithinapiece,conventional\ntechniques, suchassufﬁxtrees(see,e.g.,[20])canbe\nused.Alternatively,piecescanbesegmented (manu-\nally,orautomatically ,usinganysuitablesegmentation\nalgorithm; see,e.g.,[22]),andprobabilistic modelling\nandmatching canbeappliedtothesegmentsindividu-\nally.\nHierarchicalClustering\nTheprobabilistic matching technique described before\ncanhelptoreducesearcheffortbyeliminating some\nofthepiecesthatdonotmatchagivenquery,and\nmoreimportantly ,byidentifying promising candidate\npiecesbasedontheirtransition probability matrices\nonly.However,anaivesearchforgoodcandidates\nbasedonprobability scoreswouldstillrequiretoeval-\nuatethequeryagainsttheprobabilistic modelsforall\npiecesinthedatabase. Forverylargedatabases, or\nwhenshortresponse timesarerequired, thismightbe\ntootime-consuming.\nOnewayofaddressing thisproblemistoorganisethe\ndatabaseinformofatree,whereeachleafcorresponds\ntooneelement(i.e.,piece)ofthemusicaldatabase. For\nagivenquery,wecouldnowstartattherootandfol-\nlowthepathsleadingtotheleaveswhichcontainpieces\nwhichmatchthequery.Thiswouldallowustoretrievematchesintimeproportional totheheightofthetree,\ni.e.,logarithmic inthenumberofleavesforabalanced\ntree.Inordertodothis,weneedamechanism thatat\neachnodeofthetreeallowsustoidentifythesubtree\nthatismostlikelytocontainamatch.\nAsaﬁrstapproximation tosuchamechanism, weuse\ncombined probabilistic modelswhichsummarise the\nproperties ofallsequences inagivensubtree. Note\nthatourﬁrst-order Markovchainmodelcanbeeasily\ngeneralised tosetsofpiecesinsteadofsinglepieces:\nGiventwopieces \u0014\n\u001b\n\u001d\u0014\n ,wecombine thetwotran-\nsitionprobability matrices\r]\u0010\u0017\u0014\n\u001b\n\u0012\u001d\r]\u0010\u0017\u0014\n \n\u0012derivedfrom\ntheirrespectiveintervalsequences intoonejointmatrix\r]\u001030\u001f\u0014\n\u001b\n\u001d\u0014\n \n@\n\u0012bycomputing aweighted sumsuchthat\ntheresulting transition probabilities areequivalentto\nthosethatwouldhavebeenobtainedbyderivingatran-\nsitionprobability matrixfromtheconcatenation \u0014\n\u001b_^\u0014\n \nofthetwosequences:\u0006\u001030\u001f\u0014\n\u001b7\u001d\u0014\n `@\n\u0012\n\u0007\n\t\u000e\n\u0006\u0010-\u0014\n\u001ba^\u0014\n \n\u0012\n\u0007!\t\u000e\n+\u0014\n\u001b\u001e+\n\u0006\u0010-\u0014\n\u001b\n\u0012\n\u0007\n\t6b+\u0014\n c+\n\u0006\u0010-\u0014\n \n\u0012\n\u0007\n\t+\u0014\n\u001b\u001e+\nb+\u0014\n c+\nThismethodgeneralises tothecaseofcombining the\nmodelsofmorethantwosequences inastraight-\nforwardway.Matching forthesecombined probabilis-\nticmodelsworksexactlyasforsinglepieces,andthe\nprobability scoresthusobtained canbeusedtoguide\nthesearchformatchesinatreestructured indexofthe\ndatabase. Figure4showshowthetreestructureoftran-\nsitionmatricesisbuild;thesearchforapatternbegins\nattherootmatrixandthencontinues atthedescendant\nmatricesaslongasthesematchthetransition probabil-\nitiesofthequery.\nObviously,thetopology ofthetreeaswellasthede-\ncisionhowpiecesandsetsofpiecesaregroupedto-\ngethercanhavealargeimpactontheefﬁciencyof\ntheproposed searchmechanism. Onepotentially very\nfruitfulapproach forderivingtreestructures istheuse\nofhierarchical clustering techniques [10].However,\nitispresently notclearwhethersimilarpiecesshould\nbeclustered togetherorwhetherclustering dissimi-\nlarpiecestogetherwouldbemorebeneﬁcial; thefor-\nmerapproach mightmakeiteasiertoidentifylarger\nsetsofpromising candidates formatchesearlyinthe...Root\n... .........\nFigure4:Treestructured indexofthedatabase\nsearch,whilethelattershouldfacilitateselecting the\nmostprobablematchfromasetofpieces.\nTheissuesarisinginthiscontextarerathercomplex\nandrequirethorough empirical analyses; weplanto\nfurtherinvestigateanddiscusstheseelsewhere.For\nourpresentprototype, weuseasimpleandratherar-\nbitraryhierarchical clustering resulting inabalanced\ntreewhereeachnodehasupto32children.7Further-\nmore,tospeedupthesearchwithinthistree,foreach\nnodewestorethreebitmatriceswhoseentriesindicate\nwhetherthetransition probabilities intheprobabilis-\nticmodelfortheclustercorresponding tothatnode\nexceedsthresholds of0,0.15,and0.3,respectively.\nThosethreshold matricesareusedforrapidlyselecting\nthemostpromising subcluster ateachinternalnodeof\ntheclustertreethatisvisitedduringthesearch.(For\ndetailsofthismechanism, see[13].)\nOnceagain,itshouldbenotedthatthemechanisms in-\ntroduced hereserveadoublepurpose: Theycanpo-\ntentiallyprunelargepartsofthesearch,forwhichex-\nactmatchescannotbeencountered (basedontheoc-\ncurrence oftransition probabilities withvaluezero),\nandtheyalsoheuristically guidethesearchsuchthat\npromising candidate piecesareidentiﬁed earlyinthe\nsearch.\nApproximateMatching andError-tolerant\nSearch\nOften,queriesareinaccurate ormaycontainerrors,\nandrelevantmatchescannotexpectedtobeperfect\nmatches. Inothercases,auserquerying amusicalda-\ntabasesystemmightbeinterested in“almostmatches”,\nwhichmightindicateinteresting musicalsimilarities.\nOnewayofaddressing thissituation istouseexact\n7Thenumber32ischoseninordertoallowbit-parallel\noperations tobeusedonthisdata,seealso[21].matching incombination with“fuzzy”queriesthatsup-\nportfeaturessuchasmelodicorrhythmic trendsorin-\ntervalclasses.Butthisisnotalwaysthemostappro-\npriateapproach, andmanymusicapproximate retrieval\nmechanisms insteadoradditionally supporttrueerror-\ntolerantsearch,whichallows(penalised) mismatches\nwhenmatching queriesagainstpiecesfromthegiven\nmusicaldatabase8.\nOurretrievalmechanism basedonprobabilistic mod-\nels,although primarily developedforexactmatch-\ning,quitenaturally extendstoacertaintypeoferror-\ntolerantsearch.Tothatend,boththesearchforgood\ncandidate sequences, aswellasthesearchwithinthe\nsequences needtobemodiﬁed. Whilewecannotdis-\ncussthetechnical detailsoftheseextensions here,we\nwilloutlinethegeneralideasandprovideadetailedde-\nscriptionelsewhere.\nTolocalisecandidate sequences inanerror-tolerant\nway,wecouldmodifytheprobabilistic modelsasso-\nciatedwiththeindividualpiecesinthedatabase with\npriorinformation byfactoring pseudo-observ ations\nintoalltransition probabilities (thisisastandard\nmethodinmachine learning, whichisappliedfre-\nquentlywhenprobabilistically modelling sequence\ndata,see,e.g.,[4]).Intuitively,thiswouldreﬂectacer-\ntaindegreeofuncertainty aboutthepiecesintheda-\ntabase.Thehierarchical clustering oftheprobabilis-\nticmodelsandthesearchprocessbasedonscoring\nthequerysequence usingtheseprobabilistic models\nremainsunchanged, butthewholeprocessnowsup-\nportsimperfect matches, whicharestillpenalised, but\nnolongerruledout.\nThesameeffectcanachievedbyfactoringthepriorin-\nformation intothequery;thiscorresponds toallowing\nforerrorsorinaccuracies inthequery.Themechanism\nisexactlythesame,onlynowthepriorisassociated\nwiththequeryandgetsdynamically factoredintothe\nprobabilistic scoringprocessratherthanfoldedstati-\ncallyintothetransition matricesstoredinthedatabase.\nUnderthisview,itispossibletoallowtheuncertainty\nassociated withparticular aspectsofthequerytobe\nexplicitlyspeciﬁed. Forexample,ausermightbeab-\nsolutelycertainabouttheﬁrstandthesecondpitchof\namelodicfragment usedasaquery,butlesscertain\naboutthethirdpitch,andveryuncertain aboutafourth\none.9WedevisedanextensionofGUIDOMusicNo-\ntationthatallowstoexpresssuchlocaluncertainties in\nasimpleandintuitivewaybyusingathesymbols“?”\nand“!”.Aninstanceoftheexamplegivenabovecould\nthusbespeciﬁed as[g1!e1!e1?f1??].Wearecur-\nrentlyworkingonextendingthisconcepttoallmelodic\nandrhythmic featuressupported byourMIREngine.\n8See[21]foranoverviewofMIRsystemandtheirap-\nproximate matching types.\n9Notethatthisinformation neednotnecessarily beex-\nplicitlyenteredbytheuser—itcouldtheoretically beadded\nautomatically basedonalearnedmodeloftypicalerrors\nmadeby(particular) users,e.g.,inthecontextofaQuery-\nby-Humming approach.Thesecondstageoferror-tolerant retrieval,locating\napproximate matcheswithincandidate pieces,canbe\nhandledinmanydifferentways,including standard\nmethodsbasedonedit-distances aswellastechniques\ncloselyrelatedtotheonewediscussed forﬁndingcan-\ndidatepiecesinanerror-tolerantway.Thelatterap-\nproachappearstobeconceptually moreelegant;weare\ncurrently developingauniﬁedapproximate retrieval\nmechanism basedonthisidea,whichwillbediscussed\nindetailelsewhere.(Thecurrentimplementation ofour\nexperimental RetrievalEnginecontainsamoreadhoc\nmethodforerror-tolerantsearch,whichweintendto\nreplacewiththetheoretically moresolidapproach out-\nlinedabove.)\n5RelatedWork\nOverthelastfewyears,asubstantial amountofwork\nonmusicdatabaseandretrievalsystemshasbeenpub-\nlished.Whilewecannotnearlycoverallrelevantap-\nproaches, wewilloutlineanddiscusssimilarities and\ndifferencesbetweenthekeyideasofourapproach and\nsomerecentandearlierworkintheﬁeld.\nLemstr¨omandLainerecognised earlythatmusicrep-\nresentations whicharemoreexpressivethanMIDIpro-\nvideabetterbasisforcertainretrievaltasks(see[20];\nthispaperalsocontainsaniceoverviewofearlierwork\ninmusicinformation retrieval).Recently,anumber\nofmusicaldatabase andretrievalsystemshavebeen\ndevelopedinwhichmusicrepresentations other(and\nmoreexpressive)thanMIDIareused(see,e.g.,[5]);\nhowever,webelievethatouruseofGUIDOgoesone\nstepfurtherthanmostoftheseinusingauniformfor-\nmalismforrepresenting piecesinthedatabaseandfor\nformulating querieswhichispowerfulenoughtocap-\nturebasically anyaspectofamusicalscore.Although\nourpresentsystemonlysupportsqueriesbasedonpri-\nmarymelodicandrhythmic features, wefeelthatthe\nabilitytoextendthisinanaturalwaytoothermusical\nconcepts, suchaskey,metre,orbarlineinformation, is\nanimportant advantageofourapproach.\nRecently,anumberofXML-based musicrepresenta-\ntionshavebeenproposed (see,e.g.,[14;26;6].While\ntheseoffersomeadvantagesbyallowingtheuseof\nstandardXMLtoolsandcertainly havethepotentialto\nrepresent arbitraryaspectsofscore-levelmusic,weare\nnotawareofanyexistingmusicaldatabaseandretrieval\nsystembasedonanXML-representation. Asdiscussed\ninSection2ofthispaper,XML-based representations\nsharemanydesirable featureswithGUIDO. Aside\nfromtoolsupport,wecannotseeanyfeatureswhich\nwouldmakeXMLintrinsically suitableforcontent-\nbasedmusicinformation retrieval.WhileXML-based\nrepresentations aretypically muchtooverboseand\nsyntactically complextobeuseddirectlyformusical\nqueries,manyaspectsofourwork(particularly our\nretrievaltechnique) areindependent fromtheuseof\nGUIDOastheunderlying musicrepresentation, and\ncanbeeasilyappliedtoabroadrangeofotherformats.Sonodaetal.havebeendevelopingaWWW-based\nsystemforretrievingmusicalinformation froman\nonlinemusicaldatabase basedonthe“Query-by-\nHumming” approach [19;28].Theirsystemisbased\nonMIDIastheunderlying musicrepresentation, and\ntheirindexingandretrievalmethod,whichusesdy-\nnamicprogramming formatching, hasrecentlybeen\noptimised forefﬁcientretrievalfromlargemusical\ndatabases [29].Similartotheirapproach, wefollow\nthe“Query-by-Example” paradigm (usingGUIDOin-\nsteadofMIDI)andacknowledgethatmatching against\nlargedatabases, usingdynamic programming orsimi-\nlartechniques, canbeprohibitivelyinefﬁcient,partic-\nularlyinthecontextofanon-linesystem.Ourprob-\nabilisticmatching technique isfundamentally different\nfromtheir“ShortDynamic Programming”. Theirtech-\nniquerequiresverylargeindeces(compared tothesize\nofthedatabase), whileourprobabilistic modelsarerel-\nativelycompact. Theirretrievaltechnique isarather\nefﬁcientstand-alone methodforﬁndingmatchesinthe\ngivendatabase.10Incontrast, wemainlyfocusona\ntechnique foridentifying promising candidate pieces\ninthedatabase, whichcanbecombined withvarious\nmethodsforidentifying matcheswithinagivenpiece\n(e.g.,dynamic programming). Anotherdifferencebe-\ntweentheirapproach andoursisthefactthattheyfo-\ncusonmelodicinformation alone,whilewesupport\nqueriesthatcancombinevariousmelodicandrhythmic\nfeatures. Evidence fortheimportance ofsupporting\nsuchcombined queriesisgivenin[8],whouseaﬁxed\ntime-grid forrhythmical structure (incontrasttoour\nmoreﬂexiblerhythmical querytypes)andaretrieval\nmethodbasedoninvertedﬁleindexing.\nAninteresting approach tomusicinformation retrieval\nwhichhasrecentlygainedsomepopularity istheuseof\ntext-retrievalmethodsonsuitablyencodedmusicrep-\nresentations [23].Although oursystemusesatext-\nbasedmusicrepresentation, ourapproach tomusicin-\nformation retrievalisradically different,andactually\nmorerelatedtotechniques forbiomolecular sequence\nanalysisandgenomic information retrieval(see,e.g.,\n[11;4]).Itisourbeliefthatmusicalinformation is\ninmanywaysinherently differentfromtext,andthat\nspeciﬁcproperties ofmusicaldatashouldbeexploited\nformusicinformation retrieval.Tothatend,sequence\nretrievalmethods developedfortextdatacanpoten-\ntiallyprovideavaluablestartingpoint(ashasbeen\nthecaseforbiomolecular sequence analysis), butul-\ntimatelywillhavetobecomplemented andaugmented\nbytechniques speciﬁcally developedformusicaldata.\nTheprobabilistic matching approach weproposepro-\nvidesabasisforsuchtechniques, andtheoveralldesign\nofoursystemfacilitates suchextensions. Furthermore,\ntext-basedmethodscanbeusedinthecontextofour\napproach forlocatingmatcheswithincandidate pieces\nidentiﬁed byourprobabilistic matching technique.\n10Sincetheonlyevaluationoftheirapproach weareaware\nofisbasedonadatabase ofmainlyrandompieces,wefeel\nthattheaccuracyofthemethodinpracticeishardtoassess.Generally ,ourprobabilistic modelling approach is\nbasedoncharacterisations oftheunderlying musical\ndatawhichcanbepotentially usefulforpurposes other\nthaninformation retrieval,suchasanalysisorcompo-\nsition(see,e.g.,[9]).11Inthissense,ourapproach\nisrelatedtoworkbyThomandDannenber g[31;\n30],whouseprobabilistic modelsandmachinelearn-\ningtechniques forcharacterising melodies.\nFinally,letuspointoutageneralproblemwithalmost\nanyworkoncontent-based musicinformation retrieval\nweareawareof(including ourownworkpresented\nhere):thelackofacorpusofmusicfortestingtheef-\nﬁciencyandaccuracyofmusicretrievalsystems. Part\nofthereasonforthisisthelackofacommonly used\nandwidelysupported musicinterchange format.We\nbelievethatGUIDOMusicNotation hasthepotential\ntoremedythissituation, andwearecurrently working\nontranslating variouscollections ofmusicalmaterial\nintoGUIDO,inordertointegratetheseintoourexper-\nimentalmusicaldatabase.\n6Conclusions andFutureWork\nInthispaperwehavepresented theconceptofada-\ntabasesystemforstructured, score-levelmusicalin-\nformation andintroduced aquery-by-e xamplemech-\nanismforretrievinginformation basedonavarietyof\nmelodicandrhythmic searchcriteria.Theunderlying\nmusicretrievalmethodusesprobabilistic modelsanda\nhierarchical clustering ofthedatabaseforpruningand\nheuristically guidingthesearch.Wealsopresented an\nextensionofGUIDOMusicNotation, themusicrepre-\nsentation language weuseforthepiecesinthedata-\nbaseaswellasforqueries,whichallowsexpressing lo-\ncaliseduncertainty inmusicalqueries;andwebrieﬂy\ndescribed anextensionofourretrievalmechanism that\nusessuchextendedqueriesforapproximate probabilis-\nticmatching.\nAﬁrstprototype ofthedatabase systemandretrieval\nenginehasbeenimplemented andtestedonasetof\nabout150relativelysimplemusicalpiecesinGUIDO\nNotation Format.Thisexperimental systemhasbeen\nequipped withaWWWinterfaceandisavailable\nonlineathttp://www.salieri.org/guido/mir/ .\nOurexperience withthissmallprototype suggeststhat\ntheapproach presented herecanprovideasolidfoun-\ndationforlargerandmorecomplexdatabaseandinfor-\nmationretrievalsystemsforstructured musicaldata.\nConceptually aswellaswithrespecttotheimplemen-\ntation,thisworkisstillinarelativelyearlystage,and\nmanyaspectsofitwillbefurtherexploredandreﬁned\ninthefuture.Onthepracticalside,anobviousexten-\nsionofourworkistotestoursystemandmethodson\nlargermusicaldatabases. Tothatend,wehavebegun\ntoincludeabroadrangeofstructured musicaldata,in-\ncludingthe“EssenFolksongCollection” [27]intoour\n11Thesimpleprobabilistic modelsusedhere,aswellas\nmorecomplexmodels,canbeusedforgenerating statistically\nsimilarfragments ofmusic.dataset.Finally,wehopetogetaccesstothedatacom-\ningoutofFujinaga etal.’s“Optical MusicRecogni-\ntionSystem”[7],wherealargecollection ofAmerican\nsheetmusicisautomatically convertedintoGUIDOde-\nscriptions. Withthisadditional data,wehopetobeable\ntoconductsometestswiththousands toten-thousands\nofpiecesinGUIDOMusicNotation inthenearfu-\nture.Wealsointendtoimprovetheintegrationwiththe\nexperimental database/MIR systemwithotherGUIDO\ntoolsandapplications, inparticular withthelatestver-\nsionoftheGUIDONoteServ er[25](forvisualising\nthemusicaldata),converters(inparticular GUIDO-to-\nMIDIforplayback), andanalysistoolswhicharecur-\nrentlyunderdevelopment.\nAnotherdirection wewouldliketoexploreinthenear\nfutureistosupportquerieswhichallowtheuseof\nGUIDOtagsinadditiontomelodicandrhythmic in-\nformation. Clearly,theinformation represented bytags\nintheGUIDOdatacomprising theelements oftheda-\ntabasecanbemusically verymeaningful, andinmany\ncontextsweconsideritdesirable toincludesuchinfor-\nmationinmusicalqueries.Thiscouldbeveryuseful,\ne.g.,inordertosupportthespeciﬁcation oftonality,\nmetre,orinstrument information inaquery;similarly,\nconstraints onthemetricpositionwithinbarscouldbe\nexpressedinqueriesbyincluding barlines,andinclud-\ningexpressivemarkings ordynamicinformation could\nhelptomakeapproximate queriesmorespeciﬁc. The\nprobabilistic matching mechanism presented herecan\nbeextendedinvariouswaystoaccommodate queries\nincluding taginformation, anddetermining atheoret-\nicallyelegantandpractically effectivesolutiontothis\nproblemisachallenging problemforfutureresearch.\nEvenwhenjustconsidering themelodicandrhythmic\nquerytypessupported inourpresentsystem,itmight\nbeinteresting toinvestigatemorepowerfulprobabilis-\nticmodelsasabasisforthecharacterisation ofthemu-\nsicaldatawhichisatthecoreofourretrievalmecha-\nnism.Obviously,higher-orderMarkovmodelscould\nbeusedtocapturemoreofthelocalstructure, andad-\nditionalstatistical information whichbetterresembles\naspectsoftheglobalstructure oflargerpiecescould\nbeusedinadditiontosimpleMarkovchains.Fur-\nthermore, largerpiecescanbemoreappropriately han-\ndledbysegmenting themintosmallerfragments (using\nstandardsegmentation approaches), forwhichproba-\nbilisticmodelsarethenconstructed individually.This\nway,localstructure canbecaptured moreadequately\nandprobabilistic matching basedonthefragment mod-\nelswillbemoreaccurate.\nFinally,weareinterested inextendingourapproach be-\nyondpurelyhorizontal queriesbyallowingpolyphonic\nfeaturestobeincluded inqueries. Weseetwofun-\ndamental approaches forsuchanextension: Allowing\nchordsandpossibly tagsreferring toharmonic con-\ntexttobeincludedinmonophonic queries,orsupport-\ningfullpolyphonic queriesthatspecifysimultaneous\nmonophonic voices.Webelievethatourgeneralap-\nproachshouldinprinciple beapplicable toeithertypeofpolyphonic query,butclearly,substantial furtherin-\nvestigation willberequiredtodeviseandimplement\nthecorresponding retrievalalgorithms. Overall,weare\nconvincedthattheworkpresented herewillprovide\nagoodbasisfortheseandothergeneralised retrieval\ntasks.\nReferences\n[1]http://www.perl.com\n[2]Foradeﬁnition of“QuerybyExample” lookat\nhttp://www.whatis.com/definition/\n0,289893,sid9_gci214554,00.html\n[3]Bainbridge D.TheRoleofMusicIRintheNew\nZealandDigitalLibraryproject.Proceedings IS-\nMIR00.\n[4]BaldiP.;BrunakS.Bioinformatics: themachine\nlearningapproach. MITPress,1998.\n[5]BoehmC.;MacLellan D.;HallC.MuTaDeD’II\n–ASystemforMusicInformation Retrievalof\nEncodedMusic.Proceedings ICMC00.174–177.\n[6]CastanG.NIFFML: AnXMLImplementation of\ntheNotation Interchange FileFormat.inCom-\nputinginMusicology (12).MITPress,2001.\n[7]Choudhury G.;DiLauroT.;Droettboom M.;Fu-\njinagaI.;Harrington B.;MacMillan K.Optical\nMusicRecognition SystemwithinaLarge-Scale\nDigitization Project.Proceedings ISMIR00.\n[8]ClausenM.;Engelbrecht R.;MeyerD.;Schmitz\nJ.PROMS:AWeb-based ToolforSearching in\nPolyphonic Music.Proceedings ISMIR00.\n[9]Dodge,C.;JerseT.Computer Music:Synthesis,\nComposition, andPerformance, 2nded.,Shirmer\nBooks:NewYork,1997.361–368.\n[10]DudaR.O.;HartP.E.PatternClassiﬁcation and\nSceneAnalysis. NewYork:Wiley,1973.\n[11]Durbin;Eddy;Krogh;Mitchison Biological se-\nquenceanalysis: Probabilistic modelsofproteins\nandnucleicacids.Cambridge UniversityPress,\n1998.\n[12]GhiasA.;LoganJ.;Chamberlin D.;SmithB.C.\nQuerybyHumming: MusicalInformation Re-\ntrievalinanAudioDatabase. Proceedings of\nACMMultimedia 95,231–236.\n[13]G¨orgM.GUIDO/MIR –EinGUIDObasiertes\nMusicInformation RetrievalSystem. Masters\nThesis,TUDarmstadt, Fachbereich Informatik,\n1999.\n[14]GoodM.Representing MusicUsingXML.Pro-\nceedingsISMIR00.\n[15]Grimmett, G.R.;Stirzaker,D.R.Probability and\nRandomProcesses, 2nded.Clarendon Press:Ox-\nford,1994.Chapter6.[16]HoosH.;HamelK.;RenzK.;KilianJ.The\nGUIDONotation Format–ANovelApproach\nforAdequately Representing Score-LevelMusic\nProceedings ICMC98.451–454.\n[17]HoosH.;HamelK.;RenzK.;UsingAdvanced\nGUIDOasaNotation Interchange Format.Pro-\nceedingsICMC99.395–398.\n[18]HoosH.;HamelK.;RenzK.;KilianJ.Rep-\nresenting Score-LevelMusicUsingtheGUIDO\nMusic-Notation Format.Computing inMusicol-\nogy,Vol12,Editors:W.B.Hewlett,E.Selfridge-\nField;MITPress,2001.\n[19]KageyamaT.;Mochizuki K.;Takashima Y.\nMelodyRetrievalwithHumming. Proceedings\nICMC93.349–351.\n[20]Lemstr¨omK.;Laine,P.MusicalInformation Re-\ntrievalusingMusicalParameters. Proceedings\nICMC98,341–348.\n[21]Lemstr¨omK.;PerttuS.SEMEX–AnEfﬁcient\nMusicRetrievalPrototype. Proceedings ISMIR\n00.\n[22]MelucciM.;OrioN.Theuseofmelodicsegmen-\ntationforcontent-based retrievalofmusicaldata.\nProceedings ICMC99.120–123.\n[23]PickensJ.AComparison ofLanguage Model-\ningandProbabilistic TextInformation Retrieval\n–Approaches toMonophonic MusicRetrieval.\nProceedings ISMIR00\n[24]RabinerL.R.;JuangB.H.Anintroduction tohid-\ndenMarkovmodels.IEEEASSPMagazine, Jan-\nuary1986,4–15.\n[25]RenzK.;HoosH.AWEB-based Approach\ntoMusicNotation usingGUIDO. Proceedings\nICMC98.455–458.\n[26]RolandP.XML4MIR: Extensible MarkupLan-\nguageforMusicInformation Retrieval.Proceed-\ningsISMIR00.\n[27]Schaffrath,H.TheEssenFolksongCollection\nintheHumdrum KernFormat.D.Huron(ed.).\nMenloPark,CA:CenterforComputer Assisted\nResearch intheHumanities, 1995.\n[28]SonodaT.;GotoM.;Muraoka Y.AWWW-based\nMelodyRetrievalSystem.Proceedings ICMC98.\n349–352.\n[29]SonodaT.;Muraoka Y.AWWW-basedMelody-\nRetrievalSystem–AnIndexingMethodfora\nLargeMelodyDatabase. Proceedings ICMC00.\n170–173.\n[30]ThomB.Learning ModelsforInteractive\nMelodicImprovisation. Proceedings ICMC99.\n190–193.\n[31]ThomB.;Dannenber gR.Predicting Chordsin\nJazz.Proceedings ICMC95.237–238."
    },
    {
        "title": "The JRing System for Computer-Assisted Musicological Analysis.",
        "author": [
            "Andreas Kornstädt"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1416100",
        "url": "https://doi.org/10.5281/zenodo.1416100",
        "ee": "https://zenodo.org/records/1416100/files/Kornstadt01.pdf",
        "abstract": "Among other factors, high complexity and mandatory expert computer knowledge make many music IR and music analysis systems unsuitable for the majority of largely computer-illiterate musicologists. The JRing system offers highly flexible yet intuitively usable search and comparison operations.  to aid musicologists during score analysis. This paper discusses the requirement analysis that led to JRing’s inception, its IR tools and graphical user interface plus the kind of musical material it works on and the Humdrum-based technical realization of IR operations. 1 USER NEEDS JRing was conceived as set of tools to assist musicologists during that kind of score analysis which aims at: • a score in which all musicologically relevant elements are completely marked up • a catalogue which contains all occurrences of all of these elements in complete form plus an arbitrary set of annotations Depending on the type of work and / or analysis, the elements could be themes, leitmotivs or sets. At the beginning of the JRing development process, musicologists at the University of Hamburg and Stanford University were asked to specify the kind of computer assistance they would like to have during analysis. The five results that directly or indirectly pertain to IR were: (1) Print-like rendition of all musical materials (score, excerpts, search results) as the basis for identifying and comparing elements optically. (2) Search capabilities for finding elements by arbitrary combinations of musical features (pitch, harmony, and rhythm in various forms). (3) Tools that help to create catalogue entries, comprising (a) making excepts and (b) filling in information about the elements that can be automatically derived from the excerpt such as set class or position within the score. (4) Catalogue management capabilities to sort and filter catalogue entries according to certain criteria. (5) Customization of the structure of catalogue entries and consequently the search and comparison operations based on them. Other – non IR-related – requirements included the ability to switch back and forth between different kinds of analyses plus a maximum degree of platform independence. It becomes evident from the composition of the set of requirements what at least the musicologists that took part in the development of JRing do aim for. It is not a “big automaton” that can be fed with a score and some kind of theory description and that churns out a results that has to be interpreted by the musicologist [1, 7]. Instead, what is asked for is a set of tools that leaves the analyst permanently in charge of analysis decisions and that merely assist him in making choices faster and with less effort. The basic ways of traditional, manual analyses should not be changed. 2 SOLUTION COMPONENTS A comprehensive solution that meets all the above-mentioned requirements can hardly be furnished single-handedly. Although there is no adequate reusable and platform independent graphical user interface, many results in the area of data storage and retrieval can be incorporated and made accessible through a new user interface.",
        "zenodo_id": 1416100,
        "dblp_key": "conf/ismir/Kornstadt01",
        "keywords": [
            "high complexity",
            "mandatory expert computer knowledge",
            "music IR",
            "music analysis systems",
            "musicologists",
            "computer-illiterate",
            "score analysis",
            "JRing system",
            "flexible yet intuitively usable",
            "search and comparison operations"
        ],
        "content": "The JRing System for\nComputer-Assisted Musicological Analysis\nAndreas Kornstädt\nArbeitsbereich Softwaretechnik (SWT), Fachbereich Informatik, Universität Hamburg\nVogt-Kölln-Straße 30, 22527 Hamburg, Germany\n++49 40 69424-200\nkornstae@informatik.uni-hamburg.de\nABSTRACT\nAmong other factors, high complexity and mandatory expert\ncomputer knowledge make many music IR and music analysis\nsystems unsuitable for the majority of largely computer-illiterate\nmusicologists. The JRing system offers highly flexible yet intui-\ntively usable search and comparison operations.  to aid musicolo-\ngists during score analysis. This paper discusses the requirement\nanalysis that led to JRing’s inception, its IR tools and graphical\nuser interface plus the kind of musical material it works on and\nthe Humdrum-based technical realization of IR operations.\n1 USER NEEDS\nJRing was conceived as set of tools to assist musicologists during\nthat kind of score analysis which aims at:\n• a score in which all musicologically relevant elements are\ncompletely marked up\n• a catalogue which contains all occurrences of all of these\nelements in complete form plus an arbitrary set of annota-\ntions\nDepending on the type of work and / or analysis, the elements\ncould be themes, leitmotivs or sets. At the beginning of the JRing\ndevelopment process, musicologists at the University of Hamburg\nand Stanford University were asked to specify the kind of com-\nputer assistance they would like to have during analysis. The five\nresults that directly or indirectly pertain to IR were:\n(1) Print-like rendition of all musical materials (score, excerpts,\nsearch results) as the basis for identifying and comparing\nelements optically.\n(2) Search capabilities for finding elements by arbitrary combi-\nnations of musical features (pitch, harmony, and rhythm in\nvarious forms).\n(3) Tools that help to create catalogue entries, comprising (a)\nmaking excepts and (b) filling in information about the ele-\nments that can be automatically derived from the excerpt\nsuch as set class or position within the score.\n(4) Catalogue management capabilities to sort and filter cata-\nlogue entries according to certain criteria.\n(5) Customization of the structure of catalogue entries and con-\nsequently the search and comparison operations based on\nthem.\nOther – non IR-related – requirements included the ability to\nswitch back and forth between different kinds of analyses plus a\nmaximum degree of platform independence.It becomes evident from the composition of the set of require-\nments what at least the musicologists that took part in the devel-\nopment of JRing do aim for. It is not a “big automaton” that can\nbe fed with a score and some kind of theory description and that\nchurns out a results that has to be interpreted by the musicologist\n[1, 7]. Instead, what is asked for is a set of tools that leaves the\nanalyst permanently in charge of analysis decisions and that\nmerely assist him in making choices faster and with less effort.\nThe basic ways of traditional, manual analyses should not be\nchanged.\n2 SOLUTION COMPONENTS\nA comprehensive solution that meets all the above-mentioned\nrequirements can hardly be furnished single-handedly. Although\nthere is no adequate reusable and platform independent graphical\nuser interface, many results in the area of data storage and re-\ntrieval can be incorporated and made accessible through a new\nuser interface.\n2.1 Data\nThe foremost problem is a lack of data to be analyzed that is\navailable in an appropriate format. Although MIDI and score\nnotation data is widely available, these formats are ill-suited for\nanalysis and musical IR.\n• MIDI data focuses on pitch while durations are often not\nquantized. As MIDI is mainly intended for controlling elec-\ntronic instruments, enharmonic spelling, articulation, orna-\nmentation, and lyrics cannot be represented  among other\nthings. Although there are several extensions of the MIDI\nfile format that try to overcome some of these limitations,\nnone has captured a sizable market share and is thus a good\nsource for widely usable analytical data [9].\n• Data from notation programs such as SCORE, Finale or NP-\nDARMS tends to be layout-oriented and often subordinates\nlogical musical information (C##) to purely spatial descrip-\ntions (“black circle between two horizontal lines”). The true\npitch can only be determined by scanning backwards for a\nclef, key signatures, ottava marks, etc. Although, these for-\nmats are mostly page-oriented so that musical features that\ncross page boundaries are very difficult to recognize.\nTherefore, analytic applications that work on this kind of\ndata often restrict themselves to dealing with works that fit\non a single page [8]. Also, they need to implement complex\nalgorithms to extract the logical musical information from\nthe spatial information.\nIn contrast to highly specialized MIDI and notation formats,\nanalytical formats like Humdrum [4] and MuseData [3] are better\nsuited for analytic applications. Both have been specifically de-\nsigned with music analysis and IR in mind. They do not exclu-\nsively focus on one single aspect of the score (audible pitch orPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the first page.visual rendition) but offer flexible, extensible encoding schemes\nthat can be used to represent arbitrary musical abstractions in\ndifferent (parts of) files. Common abstractions besides pitch and\nduration are melodic contour, rhythmic weight, scale degree, pitch\nclass, frequency, MIDI events or lyrics (full text, syllables and\nphonetic equivalents). All in all, over predefined 40 Humdrum\nformats exist. As Humdrum makes only minimal structural re-\nquirements, new formats can be added to encode almost any kind\nof new musical abstraction that musicologist can conceive.\nThe separation of different musical aspects within the data repre-\nsentation forms the basis for that kind of search and comparison\nfeatures that users require according to the results of section 1.\nTherefore, developers of music analysis and IR programs can\nmake use of these existing analytical formats instead of coming up\nwith completely new encoding schemes.\nBecause MuseData has been less widely publicized than Hum-\ndrum and does not split different musical aspects into different\nfiles, Humdrum is becoming the main target format for conversion\nprograms. If copyright problems can be solved, the vast majority\nof high quality scores can be converted into Humdrum using\nFinalSCORE (by Leland Smith), scr2hmd [6] (by this author) and\nmuse2kern (by Bret Aarden and this author).\n2.2 Information Retrieval\nAs on the data side, Humdrum offers an ideal platform for infor-\nmation retrieval programs. It comes with over 40 specialized\nUNIX-programs and tools, many of which can work with all file\nformats. As all formats have the same structure, they can be ma-\nnipulated and analyzed with a few common tools that – among\nother things – can assemble individual specialized files into one\ncombined file or extract certain section either by data type (e.g.\npitch) or by position (e.g. measures 60 to 70). Analytic questions\nthat pertain to a certain representation can be answered by running\nchains (“pipes” in UNIX-lingo) of small programs, each of which\nperforms a very limited task. Plugged together in a useful way,\nthese pipes can find answers to quite complex questions such as\n“Do melodic passages in folk songs tend to exhibit an arch\nshape?”. Because Humdrum (1) runs in the UNIX environment,\n(2) stores its data in ASCII format, and (3) provides a wide range\nof reference records for encoding non-musical information, the\nstandard UNIX tools for data management (find, grep, sort,\nsed, etc.) can be used.\nFor example, in order to mark instances of a certain pattern in a\nscore by its semitone contour, the following UNIX pipe of com-\nmands is necessary:\nextract –i’**kern’ score.krn | semits -x |\nxdelta –s = | patt –t MotivX –s = -f MotivX.dat |\nextract –i’**patt’ | assemble score.krn\nThe complexity of the patterns to be matched depends on the\nprogram used:\n(1) When the patt command is used, search patterns are limited\nto quite simple sequences of tokens. To search for a melodic\ncontour of “same, up 2, down 2”, that pattern looks like this:\n0\n+2\n-2\n(2) The pattern command allows for highly complex search\npatterns. The following sequence of tokens matches one or\nmore G naturals followed optionally by a single G-sharp\nfollowed by one or more records containing one or morepitches from an A major triad the last of which must end a\nphrase.\n[Gg]+[^#-] +\n[Gg]+#[^#-] ?\n([Aa]+|([Cc]+#)|[Ee]+)[^#-] *\n(}.*([Aa]+|([Cc]+#)|[Ee]+)[^#-]))|(\n([Aa]+|([Cc]+#)|[Ee]+)[^#-].*})\n(3) A different level of flexibility can be achieved by using the\nsimil command. It does not only match precise instances of\na given pattern but measures the editing distance between the\ngiven pattern and the material in the score [5]. The result is a\nlist of numbers between 1 (perfect match) and 0 (no similar-\nity at all) that quantifies the degree of similarity for every po-\nsition in the score.\nHumdrum data and tools form the ideal technical infrastructure for\nanalytic and IR applications. They permit a wide range of analyti-\ncal and IR operations while being open for custom-made exten-\nsions. Especially, Humdrum is suitable to realize all those search\nand comparison features described in the requirements section.\nThe basic modus operandi can be described as follows:\n1. Any element (theme / leitmotiv / set) as well as the score is\nconverted into those specific Humdrum formats that can be\nused for conducting searches and comparisons. E.g. semitone\nintervals, pitch classes, durations and lyrics in syllables.\n2. According to the specifications of the user, one or more of\nthese representations are separately processed with the ap-\npropriate program (patt, pattern, simil). E.g. the\nsemitone representation is searched for the sequence “same,\nup 2, down 2” while the duration representation is searched\nfor “Hap-py birth-day”\n3. The results are merged to form the combined result. For\nexample, only those positions in the score are returned that\nfeature the semitone pattern AND the lyrics.\nDespite its benefits, for the vast majority of musicologists,\nHumdrum can only serve as technical infrastructure and not as IR\nor analytic system itself because it does not provide a graphical\nuser interface that allows analysts to manipulate the score, its\nelements and catalogues in a way that they are accustomed to.\nAlthough there are sometimes GUIs that help constructing the\ncommand pipes [2, 10], working with musical materials the tradi-\ntional way is still not possible.\n3 JRING\nJRing aims at providing musicologists with an electronic equiva-\nlent of their physical desktop complete with fully graphical scores,\nthematic / leitmotivic / set notes and catalogues of these elements.\nScores, notes and catalogues can be perused in the same way as\ntheir physical equivalents, i.e. always with a view of the print-like\nview of the material. It also offers IR-related functionality that\ngoes beyond what can be done with physical scores and cata-\nlogues.\nIn order to better understand the way in which IR functionality is\nembedded into JRing, its non-IR related features are described\nfirst.\n3.1 Non-IR-Related Features\nJRing works on scores, notes and catalogues.3.1.1 Scores\nScores are displayed in a high quality, print-like rendition. Voices\nare basically arranged the same way as in the printed score but\nvertical positions are fixed for every voice. Therefore, the topmost\nrow is always reserved for e.g. the piccolo while the lowest voice\nalways holds the display for e.g. the double base. Therefore, the\nvertical size of the score is always the same even if some voices\nare pausing. This grid-like organization of the score makes it easy\n(1) to browse the score without the need to search for a specific\ninstrument and (2) to mark an occurrence of a specific element in\none piece even if it covers one or more system / page breaks in the\nprinted score.\nThe tool that works on scores is the score analyzer. It displays the\nscore in the above-mentioned way. It has several features:\n• The position within the score can be changed by either mak-\ning use of the scroll bars or by jumping to any logical loca-\ntion such as measure 23 of the 2nd scene of the 3rd act.\n• Already marked up elements can be shown or hidden. The\nability to see intermediate results gives valuable information\non where to look for new elements. In combination with\nzooming out to, say, a 10% magnification this feature to get\nan overview over large-scale patterns of elements.\n• The score can be searched for arbitrary combinations of\nmusical features (see section 3.2.2).• Newly found elements can be graphically marked up with a\nmarker tool (see figure 2). Although marks can consist of a\nsingle contiguous block, they can be made up of any number\nof blocks. Marks form the basis for notes.\n3.1.2 Notes\nNotes describe one occurrence of a musically relevant element of\na work. They consist of a graphical rendition of the marked ele-\nment (see above) plus an arbitrary number of additional fields. As\nnotes used in manual analysis, they contain fields for non- or\nmeta-musical information such as the position within the score,\nthe formal relation of the element to other elements, the reason for\nFigure 2. The marker tool.\nFigure 1. The main components of the JRing system: desktop, score analyzer (partly covered), and catalogue browser.the element’s occurrence or – for leitmotivic analysis – the name\nof the leitmotiv. Note fields form the basis for comparisons with\nother elements which are penned down on other notes.\nTwo tools directly work on notes:\n• Note editors pop up after an element has been marked up\nusing the marker tool. Its graphical excerpt and its position\nwithin the score are automatically filled in. Information that\ndepends on human judgement (formal relation, reason for\noccurrence, etc.) needs to be filled in manually.\n• Note viewers (see rightmost sub tool in catalogue browser in\nfigure 1) display notes.\nA third kind of note-related tool are note selectors. They do not\nwork on individual notes but on catalogues.\n3.1.3 Catalogues\nCatalogues contain all the notes an analyst furnishes. As a musi-\ncologist can find a huge number of elements in a sizable score,\ntool support for managing a catalogue is need. This support is\nprovided by catalogue browsers.\nCatalogue browsers consist of three sub tools: A catalogue lister, a\nnote selector and a note viewer. Basically, the user can select a\nnote in the note lister which is then shown by the note viewer. The\nnote selector can be used to make the lister show only a specific\nsubset of notes from the catalogue.\nNote selectors (see central sub tool in catalogue browser in fig-\nure 1) are structurally similar to note viewers and note editors:\nLike these, they list every field of the note and like note editors\nevery filed is editable. Different from these, the values entered\ninto the fields of a note selector do not describe a specific note but\na pattern that is matched against all notes contained in a cata-\nlogue. If for example “1” is entered into the field named “Act”,\nonly notes that pertain to elements from the first act are listed in\nthe note lister.\nAlthough valuable, matching a single criterion does not meet the\nuser requirements stated in section 1 (cf.). Therefore, note selec-\ntors offer two additional features for describing note selections\nmore precisely:\n1. A combo box to select the matching condition. For textual\nfields the options are “equals”, “contains”, “starts with” and\n“ends with”. For numerical fields these are “equals”, “less\nthan”, “less than or equals”, etc.\n2. A checkbox which users can employ to indicate whether a\nfield should be taken into account when finding matches or\nnot. In figure 1 this feature is used to show only those notes\nin the lister that matches certain work names AND occur in\nthe first act.\n3.2 IR-Related Features\nThe features described so far replicate the traditional way of\ndealing with scores, notes and catalogues. Although they facilitate\nthe analytic process considerably by relieving musicologists of\nstrenuous tasks by means of automatically doing excerpts, filling\nout large parts of notes, and dealing with catalogues but they do\nnot provide any IR capabilities.\nJRing provides these capabilities not through completely new\ntools but integrates them into the tools already described in sec-\ntion 3.1.3.2.1 Catalogue browser\nIn manual analysis, the musicologist just needs to write down the\nexcerpt and can then make any kind of comparison based solely\non that excerpt. Because he can make arbitrary abstractions of that\nexcerpt, he can simply choose to concentrate on certain abstract\nfeatures and then browse the other excerpts in the catalogue to\nfind exact, partial or fuzzy matches. As a computer IR system\ncannot know in which respect two notes should be compared, this\nchoice of the analyst has to made explicit in a computer assisted\nsystem. Therefore, in addition to the text fields discussed in sec-\ntion 3.1.2, each note in JRing carries a list of musical abstractions\nthat are automatically derived from the excerpt. For example, the\npitch information of the marked up element can be used to derive\nabsolute pitch, pitch class, and any melodic contour in semitone\nsteps (see table 1).\nTable 1. Some melodic abstractions of a Beethoven theme\npitcha1b1-d2c2b1-a1g1c1f1g1a1b1-a1g1\npitch\nclass9 A20A970579A97\nsemitone\ninterval* +1+4-2-2-1-2-7+5+2+2+1-1-2\nrefined\ncontour*^/vvvv\\/^^^vv\ngross\ncontour*//\\\\\\\\\\////\\\\\nIn the note selector, these fields with musical abstractions can be\nused in the same way as text fields in order to determine the notes\nthat are shown in the note lister: The checkbox next to them indi-\ncates whether or not a certain field is to be included in the com-\nparison, and the combo box can be used to determine the match-\ning condition (from a 100% perfect match down to 5% similarity).\nTo make filling in the fields of the note selector easier, the con-\ntents of the note displayed in the note viewer can be copied into\nthe note selector and then modified.\n3.2.2 Score Searcher\nThe score searcher is a sub tool of the score analyzer described in\nsection 3.1.1. It basically has the same layout as the note selector,\nbut has only fields for musical abstractions and no text fields\nexcept for specifying a search range within the score. When the\nuser chooses to search the score for a certain combination of\nfeatures, the results are temporarily marked up in the score and a\nresult list browser pops up. Clicking on a result entry in the lister\ntakes the user to the score position of the match. He can decide to\ndiscard the result list or can invoke the score marker to make\nindividual results the basis for new notes in the catalogue.\nJRing’s tools are adequate to fulfill user requirements (1) to (4)\nstated in section 1. The remaining fifth requirement - customiza-\ntion of the structure of catalogue entries – is the topic of section\n4.2.\n4 TECHNICAL REALIZATION\n4.1 IR-Related Features\nAs motivated in section 2, JRing uses Humdrum as its technical\ninfrastructure and does not implement music IR functionality\nitself. While presenting Humdrum data in a way that is tailored tothe needs of the majority of musicologists, JRing can be seen as\nan – albeit very extensive – GUI front-end for Humdrum. It\ntransforms user input into Humdrum commands and parses the\nresults in such a way that it can be displayed in a form that is\nunderstandable for musicologists that are used to carrying out\nanalyses in a traditional form.\nThe core of the transformation process are the structurally identi-\ncal files the contain the analytical score information and the\ngraphical layout information. Because they are synchronized, user\ninput that pertains to the graphic representation on the screen can\nbe mapped to a precise portion of analytical information and\nsearch results can be mapped back to the graphical score rendition\n(see figure 3). A search in the score is thus carried out the fol-\nlowing way:\n1. For every field that the user marked by ticking the checkbox\nnext to it, its contents are interpreted and converted into the\nappropriate Humdrum representation.\n2. If the user has not included one of the selected representa-\ntions, the required abstraction file is generated with Hum-\ndrum. E.g. **semits for semitone intervals.\n3. For every representation, a separate Humdrum pattern\nmatching tool is invoked. If perfect matches are desired,\npatt is used, simil otherwise.\n4. The individual results are merged. Only those hits become\npart of the comprehensive result that occur in every individ-\nual result.\n5. JRing compiles the result list from the comprehensive result.\nBy going from analytical spines to synchronized layout\nspines, the its precise position within the graphically ren-\ndered score can be determined.\nBecause melodic matching often requires finding roving or hidden\nthemes [9], JRing generates an extra file that contains the highest\nconsonant pitches of the whole score. This “melody” file is auto-\nmatically included into any melodic search.\nThe technical realization of note comparisons using the note\nselector is similar to using the score searcher. The only difference\nis that notes are matched against each other and that results are\nlisted in the note lister instead and not in a separate result lister.\n4.2 Customization\nAlthough the list of musical abstractions discussed so far does\ncover some of the more frequently used features, it is not exhaus-\ntive by far. Taking into account that Humdrum comes with over\n40 data formats and that new formats can easily be added, no toolthat comes with a fixed and thus limited set of abstractions / note\nelements can be very useful. If all possible abstractions are auto-\nmatically generated, notes become huge and unwieldy. On the\nother hand, if the “right” abstractions are not offered, the tool is in\nrisk of becoming useless.\nSimilar cases can be made for the capability to display scores in\ndifferent formats (mensural, CMN, chromatic) or to work with\ndifferent Humdrum implementations or without any Humdrum-\nsubsystem.\nTo deal with the demand for flexibility, JRing can be customized\nin three ways:\n(1) The structure of notes. Depending on the type of analysis,\nnotes can be made up of different fields. In a leitmotivic\nanalysis, there might be several name fields for leitmotiv\nnames according to different sources, and musical abstrac-\ntions from the melodic, harmonic and rhythmic domain. For\nan analysis based on Forte’s set theory, the name of the ele-\nment can be automatically derived from the excerpt (by de-\ntermining the prime form and its set class) while there is no\nneed for melodic, harmonic and rhythmic abstractions.\n(2) The type of score rendition. To represent a score on screen,\nthere needs to be a component that takes a graphical score\npresentation and displays it. Depending on the type of score\nnotation (mensural, CMN, chromatic, or something com-\npletely new), different display components can be selected.\n(3) Type of musical subsystem. As Humdrum consists of UNIX\ncommands, it requires a UNIX shell for operation. This\nmight not be available on non-UNIX systems because the\nUNIX shell emulators available on these platforms cannot be\nused free of charge. Therefore, the component that connects\nto the musical subsystem can be totally absent in some cases\nor a substitute might be available. If totally absent, JRing\nfunctions normally except that searches and comparisons\nbased on musical features are disabled.\nThe source of JRing’s flexibility is the slide-in approach. Compo-\nnents implementing (1) a single musical abstraction, (2) score\nrenderer, or (3) musical subsystem can be put into matching\n“holes” of the JRing core system at startup time. In contrast to\nplug-ins or other kinds of dynamic libraries, slid-ins have the\nfollowing advantages:\n1. Slide-ins implement exactly one flexible feature of the sys-\ntem whose functionality actually means something to its us-\ners.\n2. Slide-ins only fit into the matching slide-in frame (“hole”) of\nthe core system, thus making misconfigurations impossible.Horn (F)dolcissimo\n3\n4\n**kern **layout\n*Icor *SCORE\n*Itrd4c7*\n=1 =1\n*clefG2 *\n*M4/4 *\n4G 1 14 13.3602 2 10 0 1|16 14 13.6785 17 1 1 0 0 0 0 0 13 13.2094 dolcissimo|5 14 14.4385 10.5 ...\n4c 1 14 19.5083 5 10 0 1\n4c 1 14 25.654 5 10 0 1|5 14 27.0518 8 10 32.0118 1.348 -1\n4e 1 14 31.7996 7 10 0 1|14 14 37.9542 1\n=2 =2\n... ...3. Individual slide-in frames have specific capacities. While the\nslide-in frame for musical abstractions can hold any number\nof slide-ins, the slide-in frame for musical subsystems can\nhave at most one slide-in (either Humdrum, a substitute or\nnothing at all), and the slide-in frame for score renders must\nhave exactly one slide-in.\nBecause slide-ins are easy to visualize, a configuration desktop is\nprovided that can even be used by those musicologists that would\nnot normally be able to configure technical systems.\nAs does Humdrum, JRing just offers a core system that can be\neasily extended in compliance with its interfaces. In the case of\nHumdrum, these interfaces are the Humdrum file format structure\nand the POSIX standard. In the case of JRing, these interfaces are\nthe three slide-in frames (for (1) new abstractions, (2) score ren-\nderers, and (2) musical subsystems) plus the Java programming\nlanguage. As with Humdrum, users of JRing can profit from its\nhigh reuse potential, i.e. when starting a new project, chances are\nthat the required slide-ins already have been written by some one\nelse. If not, just the missing slide-ins need to be implemented and\ncan later be passed on into the pool of available slide-ins so that\nthey can be used in future projects by others.\n5 DISCUSSION\nJRing meets all requirements listed in section 1 by providing a\ngraphical user interface that can easily be used by the vast major-\nity of musicologists. While this GUI is merely a Humdrum GUI\nfrom a technical point of view, it adds the user-oriented features\nof notes and catalogues that are not present in the Humdrum\nworld. The system allows for complex searches and comparisons\nby combining arbitrary musical abstractions in precise or fuzzy\nsearches using combinations of Humdrum’s patt and simil\ncommands. Due to its compliance with the slide-in approach, it\ncan easily be extended in a  fashion similar to Humdrum.\nThe limitation of JRing stem from the way in which searches and\ncomparisons can be formulated. While Humdrum allows ex-\ntremely complex pattern definitions (see section 2.2), JRing pares\ndown searches to fixed length patterns that can merely be com-\nbined with Boolean conjunctions (ANDs). Although patterns can\nbe matched using similarity, not even Boolean subjunctions (ORs)\nare possible.Still, this limitation appears to be acceptable as most musicologi-\ncal queries done by traditional musicologists do not require the\nmaximum degree of flexibility offered by Humdrum. To make this\nlimitation acceptable to more demanding musicologists, JRing\nmaintains all notes as separate Humdrum files (with text informa-\ntion as reference records). These files can be used independently\nfrom JRing in Humdrum pipes to make full use of Humdrum’s\npowerful yet difficult pattern matching syntax.\n6 REFERENCES\n[1]Bo Alphonce, The Invariance Matrix,\nDissertation, New Haven, CT, Yale University,\n1974.\n[2]Peter Castine, Set Theory Objects: Abstractions\nfor Computer-Aided Analysis and Composition\nof Serial and Atonal Music, Frankfurt: Peter\nLang, 1994.\n[3]Walter Hewlett, „MuseData: Multipurpose Re-\npresentation“, Eleanor Selfridge-Field (ed.),\nBeyond MIDI: The Handbook of Musical Codes,\nCambridge, MA: The MIT Press, 1997, pp. 402-\n447.\n[4]David Huron, The Humdrum Toolkit Reference\nManual, Menlo Park, CA, CCARH, 1994.\n[5]David Huron, Keith Orpen, „Measurement of\nSimilarity in Music: A Quantitative Approach for\nNon-parametric Re presentations“, Computers in\nMusic Research, Vol. 4, 1992, pp. 1-44.\n[6]Andreas Kornstädt, „SCORE-to-Humdrum: A Gra-\nphical Environment for Musicological Analysis“,\nComputing in Musicology, Vol. 10, 1996,\npp. 105-122.\n[7]Guerino Mazzola, Thomas Noll, and Oliver\nZahorka, „The RUBATO Platform“, Computing\nin Musicology, Vol. 10, 1996, pp. 143-149.\n[8]Nigel Nettheim, „Melodic Pattern-Detection Using\nMuSearch in Schubert’s Die schöne Müllerin“,\nComputing in Musicology, Vol. 11, 1998,\npp. 159-168.\n[9]Eleanor Selfridge-Field, „Introduction“, Beyond\nMIDI: The Handbook of Musical Codes,\nCambridge, MA, The MIT Press, 1997, pp. 3-38\n[10]Michael Taylor, Humdrum Graphical User\nInterface, MA Thesis, Belfast, Queen’s\nUniversity, 1996."
    },
    {
        "title": "Adventures in Standardization, or How We Learned to Stop Worrying and Love MPEG-7.",
        "author": [
            "Adam Lindsay",
            "Youngmoo E. Kim"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1418071",
        "url": "https://doi.org/10.5281/zenodo.1418071",
        "ee": "https://zenodo.org/records/1418071/files/LindsayK01.pdf",
        "abstract": "The authors give a brief account of their combined 7+ years in multimedia standardization, namely in the MPEG arena. They discuss specifics on musical content description in MPEG-7 Audio and other items relevant to Music Information Retrieval among the MPEG-7 Multimedia Description Schemes. In the presentation, they will give a historical overview of the MPEG-7 standard, its motivations, and what led to its current state.",
        "zenodo_id": 1418071,
        "dblp_key": "conf/ismir/LindsayK01",
        "keywords": [
            "multimedia standardization",
            "MPEG arena",
            "musical content description",
            "MPEG-7 Audio",
            "Music Information Retrieval",
            "MPEG-7 Multimedia Description Schemes",
            "historical overview",
            "motivations",
            "current state",
            "MPEG-7 standard"
        ],
        "content": "ISMIR2001 Invited Address\nAdventures in Standardization, or,\n how we learned to stop worrying and love MPEG-7\nAdam Lindsay : Computing Department\nLancaster University\nLancaster, LA1 4YR, UK\n+44 1524 594 537\natl@comp.lancs.ac.ukYoungmoo Kim\nMIT Media Lab,\nE15-401, 15 Ames St.\nCambridge, MA, USA 02138\n+1-617-253-0619\nmoo@media.mit.edu\nABSTRACT\nThe authors give a brief account of their combined 7+ years in\nmultimedia standardization, namely in the MPEG arena. They\ndiscuss specifics on musical content description in MPEG-7\nAudio and other items relevant to Music Information Retrieval\namong the MPEG-7 Multimedia Description Schemes. In the\npresentation, they will give a historical overview of the MPEG -7\nstandard, its motivations, and what led to its current state.\n1. INTRODUCTION\nMPEG-7, officially known as the Multimedia Content Description\nInterface, is an ISO/IEC standard whose first version will be\nfinalized at the end of 2001. Its goal is to provide a unified\ninterface for describing multimedia content in all forms. Although\nan obvious application for this is in multimedia information\nretrieval, it by no means limits itself to that domain, also\nencompassing broadcast-style scenarios, real-time monitoring, and\npotentially semi-automated editing. The Audio part of the\nstandard includes descriptors for musical timbre and for melodic\nsimilarity.\n2. MPEG-7 AUDIO\n2.1 Melody in MPEG-7\nOf chief interest to the authors in MPEG-7 is the Melody\ndescription scheme. The MPEG-7 Audio standard will include a\nunified Description Scheme (DS) for melodic information, which\ncontains two variants at different levels of detail. Within the\nDescription Scheme, there are a series of features common, but\nauxiliary, to either variant. These features include meter, key, and\nscale used, and their use is optional. Of the two options for\nrepresenting the melody itself, the first, called MelodyContour , was\ndesigned to facilitate the type of imprecise musical matching\nrequired by the query-by-humming application with as little\noverhead as possible. The second representation,\nMelodySequence , is considerably more verbose as a precise\ndescription of melody to facilitate search and retrieval using a\nwide variety of queries [1].\nDefining exactly what is or is not a melody can be somewhat\narbitrary. Melodies can be monophonic, homophonic, or\ncontrapuntal. Sometimes what one person perceives to be themelody is not what another perceives. A melody can be pitched or\npurely rhythmic, such as a percussion riff. The MPEG-7 Melody\nDS does not attempt to address all of these cases and is limited in\nscope to pitched, monophonic melodies.\nImportantly for recognition purposes, people can still uniquely\nidentify melodies after they have undergone transposition (we still\nrecognize a familiar tune in a different key as being the same\ntune). For this reason, absolute pitch is not the best descriptor for\nmelodic pitch information. What is important are the relative\nintervals between successive notes in a melody, since interval\nrelations are also invariant to key transposition. This is a key fact\nexploited by both MelodyContour  and MelodySequence .\nWith MelodyContour , however, we do not assume that a query will\ncontain precise and accurate intervals. A more robust feature is\nthe melody contour , which is derived from interval information\nand is also invariant to transposition [4]. Based on research and\nexperimental evidence, a five-level contour was chosen for\nMelodyContour  in MPEG-7, dividing the contour into small and\nlarge ascending and descending intervals with one level indicating\nno pitch change. Separation of the five contour levels is defined as\nin table 1.\nTable 1: The five levels of contour information in\nMelodyContour\nContour value Change in interval\n-2 Descent of a minor-third or greater\n-1 Descent of a half-step or whole-step\n0 No change\n1 Ascent of a half-step or whole-step\n2 Ascent of a minor-third or greater\nIn contrast, the MelodySequence  m a k e s  n o  a priori  assumptions\nabout the errors made in the query, and does not try to eliminate\nthem through quantization. Rather, it takes the approach that the\ndata set itself provides the best recovery for user error [5]. For a\nquery on a melody with n notes, the representation transforms the\nquery into n-1 dimensional interval space , to enable a comparison\nbetween two melodies using an L2 norm. This approach has the\nadvantages of not eliminating any pitch or rhythm information,\nand therefore is able to reconstruct melodies and queries after the Permission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.fact. MelodyContour  and MelodySequence  are compared in Tables\n2 and 3.\nFigure 1: The first measures of \"Moon River\"\nTable 2: MelodyContour  code for “Moon River”\n<Contour>\n<!—- MelodyContour description: “Moon River”-->\n<!-- (7 intervals = 8 notes total) -->\n  <ContourData>2 –1 –1 –1 –1 –1 1</ ContourData>\n</Contour>\n<Meter>\n  <Numerator>3</Numerator>\n  <Denominator>4</Denominator>\n</Meter>\n<Beat>\n  <BeatData>1 4 5 7 8 9 9 10</BeatData>\n</Beat>\nTable 3: MelodySequence  code for “Moon River”\n<MelodySequence>\n  <!-- [+7 -2 -1 -2 -2 -2 +2]                -->\n  <!-- [2.3219 -1.5850 1 -0.4150 -1.5650 0 0]-->\n    <Note>\n      <Interval>7</Interval>\n      <NoteRelDuration>2.3219</ NoteRelDuration>\n      <Lyric>Moon</Lyric>\n      <PhoneNGram>m u: n</ PhoneNGram>\n    </Note>\n    <Note>\n      <Interval>-2</Interval>\n      <NoteRelDuration>-1.5850</ NoteRelDuration>\n      <Lyric> Ri-</Lyric>\n    </Note>\n    <Note>\n      <Interval>-1</Interval>\n      <NoteRelDuration>1</ NoteRelDuration>\n      <Lyric> ver</Lyric>\n    </Note>\n    <!-- Other notes elided                  -->\n</MelodySequence>\n2.2 Other features in MPEG-7 Audio\nThe other Description Scheme relevant to music is the musical\ninstrument timbre DS. This description scheme groups up to five\ndifferent features to estimate the perceptual similarity between\nsegmented musical tones. There are two different “timbre spaces”\npossible, with harmonic, sustained, coherent sounds, and with\nnon-sustained, percussive sounds [6]. The different spaces use the\nfollowing features as shown in table 4.\nTable 4: Timbre features\nHarmonic Percussive\nHarmonic Spectral Centroid Log Attack Time\nHarmonic Spectral Deviation Temporal Centroid\nHarmonic Spectral Spread Spectral Centroid\nHarmonic Spectral Variation\nLog Attack Time\nThe application-oriented Description Schemes within MPEG -7\naudio also include a representation of spoken content (e.g. as an\noutput of a speech recognition engine) [3], robust audio\nidentification, and generalized sound recognition tools that use\nspectral basis functions [2].To complement the application-oriented Description Schemes,\nthere are general audio features that may apply to any signal.\nThese low-level audio descriptors form a basic compatibility\nframework so that different applications have a baseline\nagreement on such aspects as standard definitions of features,\nstandard sampling rates for regularly sampled descriptors, or how\nto segment sounds hierarchically [7].\n3. GENERAL MPEG-7 FEATURES\nTypical descriptors for traditional information retrieval, such as\ntitle, composer, and year of recording, are covered in detail in the\nMultimedia Description Schemes (MDS) part of the standard,\nparticularly in the section on creation and production information.\nSimilarly, one may try to describe musical genre as a hierarchical\nontology, or describe musical instrument from a list of controlled\nterms. The mechanisms by which one can create these ontologies\nand dictionaries are in the MDS as the Controlled Term datatype\nand the Classification Scheme DS. One may also describe aspects\nof the medium itself, such as the encoding format or sound\nquality, with the media description tools in the MDS [7].\nOther important technologies in MPEG-7, such as the Description\nDefinition Language (DDL)—the XML Schema-based language\nfor giving the syntax of Description Schemes—and various\nsystems technologies for compressing MPEG-7 data, are sadly out\nof the scope of this paper.\n4. ACKNOWLEDGMENTS\nThe authors gratefully acknowledge the work of the hundreds of\npeople actively developing MPEG-7 (especially the dozen or so\ncool people in the Audio subgroup), and the organizers of\nISMIR2001 for giving us the chance to present a small cross-\nsection of that work.\n5. REFERENCES\n[1] Information Technology—Multimedia Content Description\nInterface—Part 4: Audio , ISO/IEC FDIS 15938-4, 2001.\n[2] Casey, M., “MPEG-7 Sound-Recognition Tools,” IEEE\nTrans. on Circuits and Systems for Video Technology, Vol.\n11, No. 6 (June 2001), pp. 737–747.\n[3] Charlesworth, J.P.A., and P.N. Garner, “Spoken Content\nRepresentation in MPEG-7,” IEEE Trans. on Circuits and\nSystems for Video Technology, Vol. 11, No. 6 (June 2001),\npp. 730–736.\n[4] Ghias, A., J. Logan, D. Chamberlin, and B. C. Smith. “Query\nby Humming: musical information retrieval in an audio\ndatabase.” Proc. ACM Multimedia , San Francisco, 1995.\n[5] Lindsay, A.T . , “Using Contour as a Mid-level Represenation\nof Melody,” September 1996, unpublished S.M. Thesis at\nMIT Media Laboratory. Also at\nhttp://sound.media.mit.edu/~alindsay/thesis/\n[6] Peeters, G., S. McAdams, and P. Herrera, “Instrument sound\ndescription in the context of MPEG-7,” Proc. ICMC 2000 ,\nInternational Computer Music Conference, Berlin, 2000.\n[7] Quackenbush, S., and A. Lindsay. “Overview of MPEG-7\nAudio,” IEEE Trans. on Circuits and Systems for Video\nTechnology, Vol. 11, No. 6 (June 2001), pp. 725–729."
    },
    {
        "title": "Score Processing For MIR.",
        "author": [
            "Donncha Ó Maidín"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1416442",
        "url": "https://doi.org/10.5281/zenodo.1416442",
        "ee": "https://zenodo.org/records/1416442/files/Maidin01.pdf",
        "abstract": "The focus of this paper is on the design and use of a music score representation. The structure of the representation is discussed and illustrated with sample algorithms, including some from music information retrieval. The score representation was designed for the development of general algorithms and applications. The common container-iterator paradigm is used, in which the score is modelled as a container of objects, such as clefs, key signatures, time signatures, notes, rests and barlines. Access to objects within the score is achieved through iterators. These iterators provide the developer with a mechanism for accessing the information content of the score. The iterators are designed to achieve a high level of data hiding, so that the user is shielded from the substantial underlying complexity of score representation, while at the same time, having access to the score’s full information content.",
        "zenodo_id": 1416442,
        "dblp_key": "conf/ismir/Maidin01",
        "keywords": [
            "music score representation",
            "container-iterator paradigm",
            "clefs",
            "key signatures",
            "time signatures",
            "notes",
            "rests",
            "barlines",
            "algorithm development",
            "information content"
        ],
        "content": "SCORE PROCESSING FOR MIR\nDonncha S. Ó Maidín \nCentre for Computational Musicology and Computer \nMusic \nDepartment of Computer Science and Information \nSystems \nUniversity of Limerick \n353(0)61202705 \ndonncha.omaidin@ul.ie  Margaret Cahill \nCentre for Computational Musicology and Computer \nMusic \nDepartment of Computer Science and Information \nSystems \nUniversity of Limerick \n353(0)61202759 \nmargaret.cahill@ul.ie\n  \nABSTRACT \nThe focus of this paper is on the design and use of a music score \nrepresentation. The structure of the representation is discussed and illustrated with sample algorithms, including some from music information retrieval. The score representation was designed for the development of general algorithms and applications. The common container-iterator paradigm is used, in which the score is modelled as a container of objects, such as clefs, key signatures, time signatures, notes, rests and barlines. Access to objects within the score is achieved through iterators. These iterators provide the developer with a mechanism for accessing the information content of the score. The iterators are designed to achieve a high level of data hiding, so that the user is shielded from the substantial underlying complexity of score representation, while at the same time, having access to the score’s full information content. \n1. INTRODUCTION \nThe focus of this paper is on representing music scores. The \nmusic score is the primary document for practically all music of the past. It holds a primary place in literacy, in education, in composition and performance and in music theory.  \nIn the computer era, two main sources for digital versions of \nscores arise. The first of these is from digitising initiatives, such as those at Center for Computer Assisted Research in the Humanities at Stanford University. The second comes as a by-product of music publishing.  These activities have resulted in the production quantities of machine-readable scores.  Unfortunately, not all of these efforts are readily usable in IR research. Lack of agreed open standards and lack of openness on the part of notation software developers form some of the main barriers to more general use. \nIn practice MIDI has become the representation used in \nmuch of music information retrieval research.  The MIDI standard was invented to capture the gestures of a keyboard player.  Its ability to provide the pitch and duration content of music has resulted in its acceptability for music research.  However basic MIDI representation is radically different from score representation. Rests, slurs, barlines, staccatos, trills, ornaments,  \n_______________________________________________________________ \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n triplets and chromaticisms, are examples of concepts that are not explicit in MIDI.  \nAn alternate prospect to using MIDI is that of having an \navailability of music scores, encoded to professional editorial standards, together with appropriate tools.  One consequence of \n this approach is that the music IR researcher may work with \nthe full information content of a score, rather than a simplified view of pitch and duration. Additionally this approach will serve to facilitate cooperation between the music IR researcher and music theorists, who deal primarily with the score. \n2. CONTAINER-ITERATOR \nSome endeavours in Computer Science have been concerned with discovering useful ways for organising collections of data. One of the most basic ways of organising data is to conceive of a complex \nobject as a collection of objects that are included in a container . \nObjects within the container are accessed by means of iterators. Iterators are often characterised as ‘safe’ pointers – that is that they can point to objects within a container, and can be safely manipulated, or moved about so as to make all of the internal objects accessible.  The success of this approach has led to the development of many libraries, the most widely used one being the C++ Standard Template Library that was developed at Hewlett-Packard Labs by Alexander Stepanov and Meng Lee[1][2] and was adapted as part of ANS and ISO standards in the 90’s.  The containers in the STL allow the user to structure the data as vectors, lists, deques, sets, maps, stack and queues. Most of these data structures are one-dimensional. Their associated iterators are built so that all objects in the more general containers may be accessed in a left-to-right fashion, and possibly in a reverse order. Additionally some iterator/container combinations allow random access to the contained objects. \nIt is appropriate to consider how the music score may be modelled \nas a container.  Items in a score may be represented as objects within the container. Objects are used to represent notes, barlines, key and time signature. Iterators allow the software developer access to the information content of a score. \nA strict adherence to the S.T.L. model has proved inappropriate \nfor score representation and manipulation. In S.T.L., one of the main functions of iterators is to allow access to container members. In C.P.N.View, the iterator is used to carry out the substantial scoping resolution. Automatic scorping resolution is essential in order to achieve an appropriate level of abstraction or complexity hiding. Iterators must be able to randomly access objects (e.g. the uppermost object half ways through bar no 22 in the second violin line). Iterators may be required to move vertically, to access harmony or horizontally to follow a melody. \nThe iterator must, at the same time keep track of scoping information about aspects of the current context such as clef, key signature, time signature, metronome settings, tempo indications, accidental alterations and location within a bar.  This is necessary in order to free the user from the considerable scoping complexities, that would otherwise tend to get in the way of the development. \n3. COMMON PRACTICE NOTATION \nVIEW \nC.P.N.View[3][4] is a score representation written in C++, that \nwas developed in the mid-1990s.  It implements a representation of scores as containers and provides iterators for use with algorithms.  A score object is created either algorithmically or by using one of the components for converting from various file-based representations (ALMA, *kern, NIFF, EsAC). \nCreating a score object: \nScore s(filename); \nOne or more score iterators may be created in order to gain access \nto the internal objects in the score as \nScoreIterator si(s); \nFor the initial examples we will assume that the score is \nmonophonic. \nA number of functions exist to move the score iterator about the \nscore.  Random access is achieved using the locate  member \nfunction. This  moves the iterator to an arbitrary place in the score  \nsi.locate(NOTE, 23); \nwill move the iterator si to the 23\nrd note of the score. This function \nreturns a TRUE/FALSE value to signal the success or otherwise of the operation. For example if we call this function on a score that has only 22 notes, we get a FALSE result. Almost all of the functions in C.P.N.View return a TRUE/FALSE result, where appropriate. \nsi.locate(BAR, 20); \nmoves the iterator si to the start of the 20\nth bar of the score, if it \nexists. \nRelative movement of the iterator is achieved by the step member \nfunction.  This function may take a parameter, indicating the kind of object that it is required to move to. The following code fragment may be used to traverse all of the notes of the score \ncontained in filename . \nScore s(filename); \nScoreIterator si(s); \nwhile (si.step(NOTE)) \n{ \n doSomething(si); \n} \n \nIterators in C.P.N.View are used to directly extract information about the objects in the score. C.P.N.View iterators carry out \ndomain level processing. Much of this processing is involved with resolving contextual information. For example, where a score \niterator points to a note, the member function pitch12()  returns \nthe chromatic note number (effectively the MIDI note number).  C.P.N.View performs the following operations automatically. (1) key signature in use; (2) checks if any accidental alterations are present since the start of the bar in which the note in question resides, and (3) calculates relevant adjustments to the final pitch of the current note.  Using all of this information the correct pitch12 value is returned. \nThe following fragment illustrates how the constructs discussed so \nfar can be used to identify and print out the highest and lowest note of a piece and to calculate the pitch range of the piece. \n \nScore s(filename); \nScoreIterator si(s); \nsi.step(NOTE); \nScoreIterator highest = si, lowest = si; \nwhile (si.step(NOTE)) \n{ \nif ( si.getPitch12() < lowest.getPitch12()) lowest = si; \nif (si.getPitch12() > highest.getPitch12()) highest = si; \n} \nstd::cout << “\\nHighest note is “ << highest; \nstd::cout << “\\nLowest note is “ << lowest; \nstd::cout << “\\nRange is “ << highest.getPitch12() – lowest.getPitch12() \n<< “ semitones.\\n”; \n Similarly, a program to locate the longest and shortest note in a \npiece, excluding grace notes, \nScore s(filename); \nScoreIterator si(s); \nsi.step(NOTE); \nScoreIterator longest = si, shortest = si; \nwhile (si.step(NOTE)) \n{ \n    if (!si.hasAttribute(GRACE_NOTE)) \n   { \nif ( si.getRDuration() < shortest.getRDuration()) shortest = si; \nif (si..getRDuration( ) > highest.getRDuration()) longest = si; \n    } \n} \nstd::cout << “\\nLongest note is “ << longest; \nstd::cout << “\\nShortest note is “ << shortest;  \n \nA question arises of what happens if we run these programs on a \npolyphonic score? The answer is that these will work and produce meaningful results. \nTo explain what happens it is necessary to consider two cases. \nThe first one is where a score consist of a single stave, but has simultaneous notes.  Some examples of this will be found in string music in which multiple stopping occurs. More complicated examples happen in scores where two lines occupy the same stave.   \nAn iterator in C.P.N.View has an internal mode setting of MONO \nor POLY.  In MONO mode the iterator traverses the uppermost notes on each stave only, and skips others.  In POLY mode the iterator traverses all of the notes, moving vertically, from top to  bottom, where possible, and moving to the next highest rightmost \nobject otherwise.  \nFunctions exist for setting and querying the scanning mode of an \niterator. \nsi.putScanMode(MONO); \nsi.putScanMode(POLY); getScanMode();  \nFigures 1 and 2 show iterators in MONO and POLY mode \noperating on a single stave piece. \n \nFigure.1 Single stave traversal in MONO mode \n \nFigure 2 Single stave traversal in POLY mode \n \nBy default, the iterators created above will have MONO scan \nmodes if the score contained in filename  has one stave. \nOn the other hand, if the score is a multistave score, the scan \nmode will default to POLY, and the iterator will scan all of the objects in the score. To understand how the iterators scan across multiple staves, it is necessary to regard the score as being divided vertically into windows.  Each window has an associated width, corresponding to a time span. The left and right borders of each \nwindow correspond to onsets or offsets of notes or rests. A window may not contain internal onsets or offsets. \nSuccessive calls to the step()  function moves the iterator \nvertically, wherever possible, from the uppermost object on the top stave in a window to objects on the lowermost stave in the same window.  Where the score iterator points to the lowermost \nallowable object, a call made to step()  moves the iterator to the \nuppermost object in the next adj acent wi ndow to the right. Figure \n3 shows an such an iterator. Figure 4 demonstrates the concept of dividing the score into vertical windows. \n \nFigure 3 Multi-stave traversal in POLY mode \n \n \nFigure 4 Multi-stave score divided into vertical windows \nThe program fragments above will work correctly with a \npolyphonic score and will search through all of the notes present.  The occurrence of different clefs, changes in key and accidental changes in the score are all dealt automatically. \nIn cases where we want to scan individual staves of a polyphonic \nscore, the constructor for the ScoreIterator takes an additional parameter corresponding to the stave number.  The uppermost stave is numbered 0. An iterator created in this way will have a default scanning mode of MONO.  To access objects on the last stave of a polyphonic score that has 10 staves, the following iterator could be used. \nScoreIterator si(s, 9); \nIn the previous examples, we have seen use of the member \nfunctions locate , step, getPitch12 , getRDuration , getScanMode  \nand putScanMode .  A design strategy arises in creating such \ninformation retrieving functions. One could aim to design a minimal set of functions to retrieve the basic information content from the score. Such a minimal set will compromise convenience. The opposite approach of providing functions to retrieve every conceivable form will make things more difficult for the user, who will have a larger set to sift through and remember. The current set of 131 functions and operators might appear to lean towards the second approach. However many of these operators and functions cluster into families and thereby reduce the cognitive load in familiarization.  Also many of these group into meaningful pairs. For example most member functions that start with ‘get’ have a counterpart that starts with ‘put’.  Additionally some of \nthese are more frequently used than others.  For example the step \nand locate  functions are the main navigation mechanisms. There \nis very little additional to learn. The getPitch12 , getRDuration  \ndeliver information similar to that available in a basic MIDI file. A short review of some of the main functions is given below. \nThe getTag  member function give the type of object that is \ncurrent. \nIf the current object is a note or rest, duration values may be \nretrieved \ngetHead()  returns the head value, \ngetDots()  returns the dot count, \ngetRDuration()  retrieves the rational time value of the note or \nrest, including the resolution of groupette scoping.  Pitch information can be retrieved in many forms, including  \ngetAlpha()  returns the alphabetic note name, \ngetOctave()  returns the octave number, \ngetAccid()  returns details of any accidental placed directly on the \nnote, \ngetPitch12() returns the MIDI pitch number of the note, with all \nnecessary scoping resolved,  \nMany function return scoping information. These include the self \nobvious getKeySig(), getTimeSig(), getClef() , getBarNo() . \ngetBarDist()  returns the distance of the current position from the \nstart of the bar. \nA facility exists for annotating any score object, and for querying \nthese annotations.  \n4. IR EXAMPLES \nThe following illustrates the use of C.P.N.View in an IR context. \nIt contains a complete implementation of the dynamic programming algorithm as documented in Sequence-Based Melodic Comparison: A Dynamic-Programming Approach[5]. The two encoded score fragments used in the algorithm have been encoded in two files that appear in lines 1 and 2.  These are “Innsbruck ich muss dich lassen” and “Nun ruhen alle Waelder”. \nThis algorithm is based on the concept of a minimal edit cost of \ntransforming a source melody into a target melody, using operations of insert, delete and replace. The cost matrix d, represents the minimal cost of transforming the notes of the source tune, represented in rows, into notes of the target tune represented across the columns. The recurrence equations for generating the matrix are\n \n000=d       a 1  \n1 ),,(0,1 0 ≥ + =− i aw d di i i φ    a 2  \n1 ),,(1,0 0 ≥ + =− jb w d dj j j φ    a 3  \n)},( ),,( ),,( min{1, 1,1 ,1 j ji j i ji i jiji\nbw dbaw d aw dd\nφ φ + + +=\n− −− −\n      \n      a4 \nWhere    \n  ),(φiaw , is the cost of inserting note a i, \n  ),(jb wφ ,  is the cost of deleting b j,,  \nboth of these have value 4 in this example.   \n  ),(j ibaw , is the cost of substituting a i with b j,  \nThis cost is calculated by taking the absolute value of the \ndifference in MIDI note number, then adding half the absolute value of the difference in duration measured in 16\nth note units. \n \n    \nScore s1(\"C:\\\\Mdb\\\\Others\\\\inns.alm\");   // 1 \nScore s2(\"C:\\\\Mdb\\\\Others\\\\nur.alm\");   // 2 \nScoreIterator si1(s1);     // 3 \nScoreIterator si2(s2);     // 4 \nScoreIterator siAr1[100] = {ScoreIterator()}, \n     siAr2[100]= {ScoreIterator()};   // 5 \nint length1 = 0;     // 6 \nwhile ( si1.step(NOTE)) siAr1[++length1] = si1;  // 7 \nint length2 = 0;     // 8 \nwhile ( si2.step(NOTE)) siAr2[++length2] = si2;  // 9 \ndouble diffMatrix[100][100];    // 10 \nint i, j;      // 11 \ndiffMatrix[0][0] = 0.0;    \n       \n  \nfor ( i = 1; i <= length1; i++)    //12  \ndiffMatrix[i][0] = diffMatrix[i-1][0] + 4.0;  //13 \nfor ( j = 1; j <= length2; j++)     //14 \ndiffMatrix[0][j] = diffMatrix[0][j-1] + 4.0;  // 15 \n \nfor ( i = 1; i <= length1; i++)    // 16 \n{ \n  for ( j = 1; j <= length2; j++)    // 17 \n  { \n    diffMatrix[i][j] =     // 18 \n      min3(   \n        diffMatrix[i-1][j] + 4.0,    // 19 \n        diffMatrix[i][j-1] + 4.0,    // 20 \n        diffMatrix[i-1][j-1] +    // 21 \n          fabs(siAr1[i].getPitch12() - siAr2[j].getPitch12()) + \n          0.5*16*fabs(siAr1[i].getRDuration()- siAr2[j].getRDuration())); \n  } \n} \n \nLines 1 to 9 two arrays siAr1 and siAr2 are created, and contain \niterators that point to each note of the score \nLines 12 to 15 correspond to equations a1, a2 and a3. Lines 16 to 21 implement a4. The function min3 is not documented here. \n \n     Table 1.  The difference matrix diffMatrix.   \n   A  F G A B C B A \n   0   4   8 12 16 20 24 28 32 \nF   4   6   6 10 14 18 22 26 30 \nF   8   8   6   8 12 16 20 24 28 \nG 12 10 10   6 10 14 18 22 26 \nA 16 14 14 10   9 13 17 19 22 \nC 20 18 18 14 13 14 15 19 22 \nB 24 22 22 18 17 16 18 15 19 \nA 28 26 26 22 21 20 21 19 15 Each cell in the matrix represents the difference at a particular \npoint between the two fragments of “Innsbruck ich muss dich lassen” and “Nun ruhen alle Waelder”. The total distance between the two melodies is represented by the value in the bottom right corner, 15. The combination of edit operators that yield this result may also be determined by tracing the best path from the bottom, rightmost cell to the top left corner. \nIn this example, it will appear that little is to be gained from  \nusing the score representation instead of MIDI.  However it is worth emphasising that the with score representation this algorithm may be refined since all of the information content of the score is available. Some examples of using score information, instead of MIDI may be gleaned from the following possibilities.  Stressed notes may be distinguished by their position in the bar, \nusing information for getBarDist  and getTimeSig  functions.  \nUsing these, one could allow greater weights for inserting and deleting stressed notes. Pitch information can be dealt with at a finer level by distinguishing between enharmonic versions of the same note. Hence a C sharp may be treated differently than a D flat. A policy on handling rests will be necessary, if this algorithm is to have general applicability.  The algorithm is not explicit on how grace notes are handled.  Are they to be treated as part of the pitch contour? Perhaps a researcher may wish to treat them as providing extra emphasis on the following note. \nA final example will illustrate a partial implementation of this \nsame basic algorithm, as it was designed by Mongeau and Sankoff[6].  They used a more sophisticated model for the calculation of replacement costs than in the previous example.  The previous example was encoded above as part of line 21, by calculating the absolute value of the two MIDI note number. \nfabs(siAr1[i].getPitch12() - siAr2[j].getPitch12()) \nMongeau and Sankoff used the pitch differences between pairs of notes to calculate difference weights, based on consonances of their intervals. This involves a simple table look-up for diatonic notes.  However for comparing pairs of notes in cases where one or other note involved is chromatic, an alternate table is used, as is \nreflected in the following algorithm. The array deg holds the \ndifference weights for the diatonic table and ton holds the \ncorresponding weights for chromatic intervals. \nThe algorithm checks if the notes involved are diatonic, and \napplies the appropriate transformation.  The function pitchD  is a \nsimplified version of the production algorithm, that is applicable to music is in a major key only. \nThe function noteInKey  was developed specifically for this \napplication. Its coding is given below. \nint ScoreIterator::noteInKey() \n{ \n int key = currentKs; \n int actual12 = getPitch12(); \n int actual7 = getPitch7(); \n int nrOctaves7 = actual7/7; \n int octaveDisp12 = nrOctaves7*12; \n int scaleStep7 = actual7%7; \n int diatonicSteps[] = {0,2,4,5,7,9,11}; \n int unalteredPitch12 = octaveDisp12 + diatonicSteps[scaleStep7] +  \n                                                              getKeySigAdjust(); \n if ( unalteredPitch12 == actual12 ) return TRUE; \n return FALSE; \n} double pitchD(ScoreIterator &si1, int major1, ScoreIterator & si2) \n{ \n  double pitchDist = 0; \n  if ( inKey(si1, major1 )) \n  { \n   double deg[] = {0.0, 0.9, 0.2, 0.5, 0.1, 0.35, 0.8}; \n   double ton[] = {0.6, 2.6, 2.3, 1.0, 1.0, 1.6, 1.8, 0.8,  \n                              1.3, 1.3, 2.2, 2.5};   \n   if ( si1.noteInKey() && si2.noteInKey() ) \n   { \n    int diatonicSteps = fabs(si1.getPitch7() –  \n                                          si2.getPitch7()); \n     diatonicSteps = diatonicSteps % 7; \n     pitchDist = deg[diatonicSteps]; \n    } \n    else \n    { \n     int chromaticSteps = fabs(si1.getPitch12() –  \n                                             si2.getPitch12()); \n     chromaticSteps = chromaticSteps % 12; \n     pitchDist = ton[chromaticSteps]; \n   } \n  } \n  return pitchDist; \n} \n5. CONCLUSION \nProcessing of music scores gives the prospect for accessing ever increasing corpora that have been created to high editorial standards. The Container/Iterator model gives an appropriate tool for algorithmic construction.  Experience with C.P.N.View raises some interesting issues. An illustration of one such, that has been mentioned earlier in this paper is on devising an optimal set of operations to include in C.P.N.View.  A minimal set, makes it easier for anyone to learn to use C.P.N.View.  A more extensive set of operations, make it easier to write algorithms.  A case in \npoint is the noteInKey  function above. This was not developed \ninitially as part of C.P.N.View, but instead formed part of the implementation of the Mongeau and Sankoff algorithm. It was added to C.P.N.View, on the basis that it provided potential for reuse in other algorithms.  C.P.N.View provides a sufficiently abstract model of a score that it is potentially useable with a wide range of score representations, including some representations from notation packages.  Currently C.P.N.View can accept i nput from score codings in ALMA, \nNIFF, *kern and EsAC.  Some incomplete work has been done with SCORE and Enigma files. \n6. REFERENCES \n[1] Stepanov, A.A., and Lee, M. The Standard  Template Library, Technical Report HPL-95911, Hewlett-Packard Laboratories, Palo Alto VA, February 1995. \n[2] Plauger, P.J., Stepanov, A.A., Lee, M., and Musser, D.R. The C++ Stamdard Template Library, Prentice-Hall, NJ, 2001. \n[3] Ó Maidín, D. Common Practice Notation View: a Score Representation for the Construction of Algorithms, Proceeding of the 1999 ICMC (Beijing,1999), ICMA, San \nFrancisco, 248-251. [4] Ó Maidín, D. “Common Practice Notation View User’ \nManual” Technical Report UL-CSIS-98-02, University of Limerick, 1998. \n[5] Smith, L., McNab, R., Witten, I. , Sequence-Based Melodic Comparison: A Dynamic-Programming Approach,  in Melodic Similarity, Concepts, Procedures and Applications,Computing in Musicology 11. Hewlett W., \nSelfridge-Field, E. (eds).  MIT Press 1998, 101-117. \n[6] Mongeau, M.,Sankoff,D., Comparison of Musical Sequences, Computers and the Humanities 24,Kluwer Academic Publishers 1990, 161-175."
    },
    {
        "title": "Thematic Extractor.",
        "author": [
            "Colin Meek"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1414828",
        "url": "https://doi.org/10.5281/zenodo.1414828",
        "ee": "https://zenodo.org/records/1414828/files/Meek01.pdf",
        "abstract": "We have created a system that identifies musical keywords or themes. The system searches for all patterns composed of melodic (intervallic for our purposes) repetition in a piece. This process generally uncovers a large number of patterns, many of which are either uninteresting or only superficially important. Filters reduce the number or prevalence, or both, of such patterns. Patterns are then rated according to perceptually significant characteristics. The top-ranked patterns correspond to important thematic or motivic musical content, as has been verified by comparisons with published musical thematic catalogs. The system operates robustly across a broad range of styles, and relies on no meta-data on its input, allowing it to independently and efficiently catalog multimedia data.",
        "zenodo_id": 1414828,
        "dblp_key": "conf/ismir/Meek01",
        "keywords": [
            "musical keywords",
            "melodic repetition",
            "patterns",
            "filters",
            "perceptually significant",
            "multimedia data",
            "published thematic catalogs",
            "robustly across styles",
            "meta-data",
            "independent and efficient catalog"
        ],
        "content": "Thematic Extractor \nColin Meek \nUniversity of Michigan \n1101 Beal Avenue \nAnn Arbor MI 48104 \n1-734-763-1561 \nmeek@umich.edu William P. Birmingham \nUniversity of Michigan \n1101 Beal Avenue \nAnn Arbor MI 48104 \n1-734-936-1590 \nwpb@umich.edu\n \nABSTRACT  \nWe have created a system that identifies musical keywords  or \nthemes. The system searches for all patterns composed of melodic (intervallic for our purposes) repetition in a piece. This process generally uncovers a large number of patterns, many of which are either uninteresting or only superficially important. Filters reduce the number or prevalence, or both, of such patterns. Patterns are then rated according to perceptually significant characteristics. The top-ranked patterns correspond to important thematic or motivic musical content, as has been verified by comparisons with published musical thematic catalogs. The system operates robustly across a broad range of styles, and relies on no meta-data on its input, allowing it to independently and efficiently catalog multimedia data.  \n1. INTRODUCTION \nWe are interested in extracting the major themes from a musical \npiece: recognizing patterns and motives in the music that a human listener would most likely retain. Thematic extraction , as we term \nit, has interested musician and AI researchers for years. Music librarians and music theorists create thematic indices (e.g., Köchel catalog [1]) to catalog the works of a composer or performer. Moreover, musicians often use thematic indices (e.g., Barlow's A \nDictionary of Musical Themes [2]) when searching for pieces \n(e.g., a musician may remember the major theme, and then use the index to find the name or composer of that work). These indices are constructed from themes that are manually extracted by trained music theorists. Construction of these indices is time consuming and requires specialized expertise. Figure 1 shows a simple example. \nBackground Material\n1stTheme from Barlow\n \nFigure 1: Sample Thematic Extraction from opening of \nDvorak's American Quartet  \nTheme extraction using computers has proven very difficult. The \nbest known methods require some ‘hand tweaking’ [3] to at least provide clues about what a theme may be, or generate thematic \nlistings based solely on repetition and string length [4]. Yet, automatically extracting major themes is an extremely important problem to solve. In addition to aiding music librarians and archivists, exploiting musical themes is key to developing efficient music-retrieval systems. The reasons for this are twofold. First, it appears that themes are a highly attractive way to query a music-retrieval system. Second, b ecause themes are much smaller \nand less redundant than full pi eces, by searching a database of \nthemes, we simultaneously get faster retrieval (by searching a smaller space) and get increased relevancy. Relevancy is increased as only crucial elements, variously named motives, themes, melodies or hooks, are searched, thus reducing the chance that less important, but commonly occurring, elements will fool the system. \nThere are many aspects to music, such as melody, structure and \nharmony, each of which may affect what we perceive as major thematic material. Extracting themes is a difficult problem for many reasons. Among these are the following: \n• The major themes may occur anywhere in a piece. Thus, \none cannot simply scan a specific section of pi ece (e.g., \nthe beginning). \n• The major themes may be carried by any voice. For \nexample, in Figure 2, the viola, the third lowest voice, carries the principal theme. Thus, one cannot simply “listen” to the upper voices. \n• There are highly redundant elements that may appear as \nthemes, but should be filtered out. For example, scales are ubiquitous, but rarely constitute a theme. Thus, the relative frequency of a series of notes is not sufficient to make it a theme. \nIn this paper, we introduce an algorithm, Melodic Motive \nExtractor (MME), that automatically extracts themes from a piece of music, where music is in a note representation. Pitch and duration information are given; metrical and key information is not required. \nMME exploits redundancy that is found in music: composers will \nrepeat important thematic material. Thus, by breaking a piece into note sequences and seeing how often sequences repeat, we identify the themes. Breaking up involves examining all note sequence lengths of two to some constant. Moreover, because of the problems listed earlier, we must examine the entire piece and all voices. This leads to very large numbers of sequences (roughly 7000 sequences on average, after filtering), thus we must use a very efficient algorithm to compare these sequences.  \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n \n TimePitch\n \nFigure 2: Opening Phrase of Dvorak's “ American ” Quartet  \nOnce repeating sequences have been identified, we must further \ncharacterize them with respect to various perceptually important features in order to evaluate if the sequence is a theme. Learning how best to weight these features for the thematic value function is an important part of our work. For example, we have found that the frequency of a pattern is a stronger indication of thematic importance than is the register in which the pattern occurs (a counterintuitive finding). We implement hill-climbing techniques to learn weights across features. The resulting evaluation function then rates the sequences. \nAcross a corpus of 60 works, drawn from the Baroque, classical, \nromantic and contemporary periods, MME extracts sections identified by Barlow as “1\nst themes” over 98% of the time. \n1.1 Problem Formulation \nInput to MME is a set of note events making up a musical composition N = {n\n1,n2...n3}. A note event is a triple consisting of \nan onset time, an offset time and a pitch (in MIDI note numbers, where 60 = ‘Middle C’ and the resolution is the semi-tone): n\ni = \n<onset , offset , pitch >. We note that several other valid \nrepresentations of a musical composition exist, taking into account amplitude, timbre, meter and expression markings among others [6]. We limit the domain because pitch is reliably and consistently stored in MIDI files--the most easily accessible electronic representation for music--and because we are interested primarily in voice contour as a measure of redundancy.  \nThe goal of MME is to identify patterns and rank them according \nto their perceptual importance as a theme. We readily acknowledge that there may, in some cases, be disagreement among listener about what constitutes a theme in a pi ece of music; \nhowever, , we note that t published thematic catalogs represent common convention. These catalogs thereby provide a concrete measure by which the system can be evaluated.. \n2. Algorithm \nIn this section, we describe the operation of MME. This includes identifying patterns and computing pattern characteristics, such that “interesting” patterns can be identified. MME’s main processing steps are the following: \n1. Input  \n2. Register 3. Stream segregation \n4. Filter top voice 5. Calculate event transitions 6. Generate event keys 7. Identify and filter patterns 8. Frequency 9. Compute other pattern features 10. Rate patterns 11. Return results \n2.1 Input \nMME generally takes as input MIDI files, which are translated into lists of note events in the described format. Information is also maintained about the channel and track of each event, which \nis used to separate events into streams. \n2.2 Register \nRegister is an important indicator of perceptual prominence [10]: we listen for higher pitched material. For the purposes of MME, we define register in terms of the voicing, so that for a set of n \nconcurrent note events, the event with the highest pitch is assigned a register of 1, and the event with the lowest pitch is assigned a register value of n. For consistency across a piece, we \nmap register values to the range [0,1] for any set of concurrent events, such that 0 indicates the highest pitch, 1 the lowest. \nGiven the input set of events N[]:\n1.Sort(N,onset[N])\n2.ActiveList ÅNULL\n3.indexÅ0\n4.whileindex<n\n4.onsetÅOnset[N[index]]\n5. • remove all inactive events\n6.Remove(ActiveList, Offset [N]Onset)\n7.  add all events with the same onset\n8.whileindex<n–1andOnset[N[index]] =onset\n9. Register[N[index]]Å0\n10. addN[index]toActiveList\n11. increment index\n12.  updateRegistervalue of active events\n13. Sort(ActiveList ,Pitch[ActiveList ])\n14. nÅSize[ActiveList ]–1\n15. forjÅ0ton\n16. register Ån-j/n\n17. ifregister>Register[ActiveList [j]]\n18. Register[ActiveList [j]]Åregister\nAlgorithm 1: Calculating Register \nTable 1: Register values at each iteration of register algorithm \nAdding e0 e1 e2 e3 e4 e5 e6 e7 ActiveList \ne0 0         {e0} \ne1 1 0       {e0,  e1} \ne2 1 0 1/2       {e0, e1, e2} \ne3 1 0 1 0     {e2, e3} \ne4, e5 1 0 1 2/3 1/3 0   {e2, e3, e4, e5} \ne6, e7 1 0 1 2/3 1/3 0 1/2  1 {e4, e6, e7} \nWe need to define the notion of concurrency more precisely. Two \nevents with intervals I1 = [s1,e1] and I2 = [s2,e2] are considered \nconcurrent if there exists an common interval Ic = [sc,ec] such that \nsc < ec and Ic ⊆ I1 ∧ Ic ⊆ I2. The simplest way of computing \nthese values is to walk through the event set ordered on onset \ntime, maintaining a list of (notes that are on) events, or events sharing a common interval (see Algorithm 1). Consider the example piece in Figure 3. The register value \nassigned to each event { e0…e7} at each iteration is shown in \nTable 1. \nTimePitch\ne0e1\ne2e3e4e5\ne6\ne7\n \nFigure 3: Register, Example Piece \n2.3 Stream Segregation and Filtering Top \nVoice \nGenerally, the individual channels of a MIDI file correspond to \nthe different instruments or voices of a piece. Figure 2 shows a relatively straightforward example of segmentation, from the opening of Dvorak's “ American ” Quartet , where four voices are \npresent. In cases where several concurrent voices are present in one instrument, for example in piano music, we deal with only the top sounding voice. This is clearly a restriction, albeit a reasonable one, as certain events are disregarded. This restriction is necessary . Although existing analysis tools, such as MELISMA [7], perform stream segregation on abstracted music, i.e., note-event representation, they have trouble with overlapping voices [8], as seen between the middle voices in Figure 2. \nIdentifying the top sounding voice is not as straightforward as it \nmay appear. Some MIDI scores contain overlapping consecutive events within a single voice. To avoid filtering out such notes, we employ an algorithm similar to the register algorithm (see Algorithm 1), wherein events are removed from the active list for their particular channel some ratio (0.5) of their duration from their onset, and as such avoid being falsely labeled as “lower-sounding” notes. For instance, an event in the time interval [30, 50] will be removed from the active list when the sweep reaches time 40. \nAdditionally, when long pauses (greater than some time constant) \nare found in a stream, the stream is broken at that point. In this manner, we exclude sequences enclosing large stretches of silence from gaining arbitrary advantage from the duration feature. \nFor the purposes of this paper, we will indicate events using the \nnotation e\nstream , index, such that e0,1 indicates the second note of the \nfirst stream. \n2.4 Calculating Transitions \nWe are primarily concerned with melodic contour as an indicator of redundancy. For our purposes, contour is defined as the sequence of pitch intervals across a sequence of note events in a stream. For instance, the stream consisting of the following event sequence: e\ns = {<0, 1, 60>, <1, 2, 62>, <2, 3, 64>, <3, 4, 62>, <4, \n5, 60>} has contour cs  = {+2, +2, -2, -2}. \nMME considers contour in terms of simple interval, which means \nthat although the sign of an interval (+/-) is considered, octave is not. As such, an interval of +2 is equivalent to an interval of +14 = (+2 + octave = +2 + 12). We normalize each interval corresponding to an event, i.e., the interval between that event and its successor, to the range [-12, 12]: \n\n−< − −+≤ ≤−\n=− =+\no.w. _ mod12 _  if _ mod  12 _ 12 if , _][ ] [ _\n, 12, ,, 12, ,\n,, 1, ,\nisis isis is\nisis is is\ninterval realinterval real interval realinterval real interval real\nce Pitch e Pitch interval real \nAnother transition measure we employ is known as the Inter-\nOnset Interval (IOI), used to describe the rhythmic content of a sequence, and the rhythmic consistency of a pattern. This measure ignores the rhythmic articulation of events, but maintains the basic rhythmic information. In the above example, the IOI values are simply {1, 1, 1, 1}: \n][ ] [ ][, 1, , is is is e Onset e Onset eIOI − =+ \n2.5 Calculating Keys \nTo efficiently uncover patterns, or repeating sequences, we assign \na key k to each event in the piece that uniquely identifies a \nsequence of m intervals, where m is the maximum pattern length \nunder consideration. Length refers to the number of intervals in a pattern, one less than the number of events. The keys must exhibit the following property: \n} ,, , {} ,, , {)( )(1 2,1 12,2 2,2 1 1,1 11,1 1,1 2,2 1,1 −+ + −+ + = ↔ =mis is is mis is is is is c c c c c c m k m k K K  \nSince only 25 distinct simple intervals exist, we can refer to \nsequences of intervals in radix-26 notation, reserving a digit (0) for the ends of streams. An m-digit radix-26 number, where each \ndigit corresponds to an interval in sequence, thus uniquely identifies that sequence of intervals, and our key values can then be calculated as follows, re-mapping intervals to the range [1, 25]: \n∑−\n=−−\n++ =1\n01\n, , 26*)13 ( )(m\njjm\njis is c mk \nThe following derivations allow us to more efficiently calculate \nthe value of ks,i: \nEquation 1 \n13 )1(, , + =is is c k  \nEquation 2 \n\n−− ≤ +−=+ −−+\n.. 26*) ( if)1( )1( *26)(\n,1 , ,\n,wo i cki cn k nknkicn\ns iss nip ip\niss \nThe second case of this last equation deals with the situation \nwhere no additional information is gained by increasing n, since \nthere are no additional intervals to consider beyond the end of the stream. It is derived from the observation that when \nsci≥ , \n0)1(,=isk , the end of stream zero padding. \nBy removing the most significant digit of a key )(,nkis, we get the \nkey for the subsequent event )1(1, −+n kis: Equation 3 \n1\n, , 1, 26*)1( )( )1(−\n+ − =−n\nis is is knk n k  \nThis in turn allows us to calculate the subsequent key value in \nconstant time, using Equation 2. \nUsing Equation 1 and Equation 2, we can calculate the key if the \nfirst event in a stream in linear time with respect to the maximum pattern length, or the stream length, whichever is smaller (this is essentially an application of Horner’s Rule [9]). Equation 3 \nallows us to calculate the key of each subsequent event in constant time (as with the Rabin-Karp algorithm [9]). As such, the overall \ncomplexity for calculating keys is \n)(nΘ  with respect to the \nnumber of events. \nConsider the following simple example for m = 4, a single phrase \nfrom Mozart’s Symphony no. 40 : c0 = {-1, 0, +1, -1, 0, +1, -1, 0 \n+8}. \nFirst we calculate the key value for the first event ( k0,0(4)), using \nEquation 1 and Equation 2 recursively: \n22007612)14)1312*26(*26(*2612)14))1( )1( *26(*26(*2612))1( )2( *26(*26)1( )3( *26)4(\n1,0 0,02,0 0,03,0 0,0 0,0\n=+ + + =+ + + =+ + =+ =\nk kk kk k k\n \nThen we calculate the remaining key values: \n9164 26*)1( )4( )3(3\n0,0 0,0 1,0 = − = k k k  (Equation 3) \n238277)1( )3( *26)4(4,0 1,0 1,0 = + = k k k  (Equation 2) \nUsing the same procedure, we generate the remaining key values: \n0)4( 369096)4( 242684)4( 220246)4(254535)4( 238277)4( 220076)4( 254528)4(\n9,0 8,0 7,0 6,05,0 4,0 3,0 2,0\n= = = == = = =\nk k k kk k k k  \n2.6 Identifying and Filtering Patterns \nWe employ one final derivation on k for the pattern identification: \nEquation 4 \n\n= ≤< ∀−nmis\nismknkmn n\n26)()(: 0,,\n, \nEvents are then sorted on key so that pattern occurrences are \nadjacent in the ordering. We make a pass through the list for pattern lengths from \n]2 [Km n= , resulting in a set of patterns, \nordered from longest to shortest. This procedure is \nstraightforward: during each pass through the list, we group together keys for which the value of \n)(nk - calculated using \nEquation 4 – is the same. Such groups are consecutive in the \nsorted list. Occurrences of a given pattern are then ordered according to their onset time, a property necessary for later operations. \nContinuing with the Mozart example, sorting the keys we get: \n{k\n0,9, k0,0, k0,3, k0,6, k0, 1, k0,4, k0,7, k0,2, k0,5, k0,8}. \nOn our first pass through the list, for n = 4, we identify patterns \n{k0,0, k0,3} and { k0,1, k0,4}, since there keys are identical. During \nthe second pass, for n = 3, we identify patterns { k0,0, k0,3}, {k0,1, \nk0,4} and { k0,2, k0,5}, noting that k0,2/264-3 = k0,5/264-3 (which by \nEquation 4 indicates that a pattern of length three exists.) Similarly, we identify the following patterns for n = 2: { k0,0, k0,3, \nk0,6}, {k0,1, k0,4} and { k0,2, k0,5}. The patterns are shown in Table 2. \nTable 2: Patterns in opening phrase of Mozart's \n Symphony no. 40  \nPattern Occurrences at Characteristic interval \nsequence \nP0 e0, 0, e0, 3 {-1, 0, +1, -1} \nP1 e0, 1, e0, 4 {0, +1, -1, 0} \nP2 e0, 0, e0, 3 {-1, 0, +1} \nP3 e0, 1, e0, 4 {0, +1, -1} \nP4 e0, 2, e0, 5 {+1, -1, 0} \nP5 e0, 0, e0, 3, e0, 6 {-1, 0} \nP6 e0, 1, e0, 4 {0, +1} \nP7 e0, 2, e0, 5 {+1, -1} \nWe associate a vector of parameter values \n> =<n i v vv V ,,,2 1K and a set of occurrences to each pattern. \nLength, lengthv , is one such parameter. The assumption was made \nthat longer patterns are more significant, simply because they are \nless likely to occur by chance. \nAs patterns are identified, they are filtered according to several \ncriteria. Since zero padding is used at the ends of streams, it must be verified that a sequence does not overrun the end of a stream, which frequently happens since all streams end with the same zero-padding. Two other filtering criteria are considered as well: intervallic variety, and doublings. \n2.6.1 Intervallic Variety \nEarly experiments with this system indicated that sequences of repetitive, simple pitch-interval patterns dominate given the parameters outlined thus far. For instance, in the Dvorak example (see Figure 2) the melody is contained in the second voice from the bottom, but highly consistent, redundant figurations exist in the upper two voices. Intervallic variety provides a means of distinguishing these two types of line, and tends to favor important thematic material since that material is often more varied in terms of contour. \nGiven that intervallic variety is a useful indicator of how \ninteresting a particular passage appears, we count the number of distinct intervals observed within a pattern, not including 0. We calculate two interval counts: one in which intervals of + n or - n \nare considered equivalent, the other taking into account interval direction. Considering the entire Mozart example, which is indeed a pattern within the context of the whole piece, there are three distinct directed intervals, -1, +1 and 8, and two distinct undirected intervals, 1 and 8. \nAt this stage, we filter out all patterns whose characteristic \ninterval sequence has below certain minimum values for these interval counts. In addition, interval counts are maintained for each pattern. \n2.6.2 Doublings \nDoublings are a special case in MME. A doubled passage occurs where two or more voices simultaneously play the same line. In such instances, only one of the simultaneous occurrences is retained for a particular pattern, the highest sounding to maintain \nthe accuracy of the register measure.  \nWe must provide a definition of simultaneity to clearly describe \nthis parameter. To provide for inexact performance, we allow for a looser definition: two occurrences o\na and ob, with initial events \nes1,i1 and es2,i2 respectively, and length n, are considered \nsimultaneous if and only if jisenj j+ ≤≤ ∀1,1: 0,  overlaps es2,i2+j. \nTwo events are in turn considered overlapping if they strictly \nintersect. It is easier to check for the non-intersecting relations -- using the conventions and notations of Beek [11] -- e\ns1,i1 before \n(b) es2,i2 or the inverse ( bi) (see Algorithm 2): \nWe check each occurrence of a pattern against every other \noccurrence. Note that since occurrences are sorted on onset, we know that if o\ni and oj are not doublings, where j > i, oi cannot \ndouble ok for all k > j. This provides a way of curtailing searches \nfor doublings in our algorithm, and provides significant performance gains (experimentally, a tenfold improvement). This is because partial doublings rarely occur, where only some subset \nof corresponding intervals is simultaneous. \nGiven a pattern Pwithnoccurrences in O[] and length l\n1.foriÅ0ton–2\n2.forjÅi+1ton–1\n3. if~Remove[O[i]]and~Remove[O[j]]\n4. Simultaneous =true\n5. forkÅ0tol\n6. if~Intersects(eStream[O[i]],Index[O[i]],eStream[O[j]],Index[O[j]])then\n7. Simultaneous Åfalse\n8. kÅl+1\n9. ifSimultaneous then\n10. ifPitch(eStream[O[i]],Index[O[i]])>Pitch(eStream[O[j]],Index[O[j]])\n11. Remove[j]Åtrue\n12. Doubled[i]Åtrue\n13. else\n14. Remove[i]Åtrue\n15. Doubled[j]Åtrue\n16. else\n17. jÅn\n18.Remove(O,Remove[O])\nAlgorithm 2: Filter Doublings \nThis doubling filtering occurs before other computations, and thus \ninfluences frequency. We, however, retain the doubling information (Lines 12 and 15, Algorithm 2), as it is a musical emphasis technique. \nIf after filtering doublings less than two occurrences remain, the \npattern is no longer considered a pattern, and removed from consideration. Doublings serve to reinforce a voice, and as such do not constitute repetition. \n2.7 Frequency \nFrequency of occurrence is one of the principal parameters considered by MME in establishing pattern importance. All other things being equal, higher occurrence frequency is considered an indicator of higher importance. Our definition of frequency is complicated by the inclusion of partial pattern occurrences. For a particular pattern, characterized by the interval sequence \n} ,...,,{1 1 0 −lengthvC CC , the frequency of occurrences is defined as \nfollows: \nlengthvlv\nj lj j j\nvl C CC\nlengthlength\n∑∑\n=−\n= −+ +2 1\n0 1 1 *} ,, ,{of s occurrence filtered-un and redundant -non\nK An occurrence is considered non-redundant if it has not already \nbeen counted, or partially counted (i.e., it contains part of another sub-sequence that is longer or precedes it.) Consider the piece consisting of the following interval sequence, in the stream e\n0: \n}2,2,2,2,5,5,2,2,2,2,5,5,2,2,2,2{0 +−+−+−+−+−+−+−+−=c , and \nthe pattern 2,-5} 2,-2, {-2, + + . Clearly, there are two complete \noccurrences at e0,0 and e0,6, but also a partial occurrence of length \nfour at e0,12. The frequency is then 2.8 for this pattern. \nTo efficiently calculate frequency, we first construct a set of \npattern occurrence lattices, on the following binary occurrence \nrelation p: \nGiven occurrences o1 and o2 characterized by event sequences E1 \nand E2, 2 1 2 1 E E oo ⊂ ⇔p . In other words, each occurrence in the \nlattice covers all patterns occurrences containing a subsequence of \nthat occurrence. \nAs such, in establishing frequency, we need consider only those \npatterns covered by occurrences of P in the lattices. Two \nproperties of our data facilitate this construction: \n1. The pattern identification procedure adds patterns in \nreverse order of pattern length. \n2. For any pattern occurrence of length n > 2, there are at \nmost two occurrences of length n – 1, one sharing the \nsame initial event, one sharing the same final event. If one of these two child occurrences does not exist, it is due to the filtering described above. Because of the nature of the filtering, no patterns of length less than n – \n1 will be covered by the occurrence in these instances, so we need only generate links to occurrences of length n – 1 in the lattices. The branching factor is thus limited \nto two. \nThe lattice is described as follows: given a node representing an \noccurrence of a pattern o with length l, the left child is an \noccurrence of length l – 1 beginning at the same event. The right \nchild is an occurrence of length l – 1 beginning at the following \nevent. The left parent is an occurrence of length l + 1 beginning at \nthe previous event, and the right parent is an occurrence of length l + 1 beginning at the same event. Consider the patterns the \nMozart excerpt (see Table 2): P\n0's first occurrence, with length 4 \nand at e0,0, directly covers two other occurrences of length 3: P2's \nfirst occurrence at e0,0 (left child) and P3's first occurrence at e0,1 \n(right child). The full lattice is shown in Figure 4, where each occurrence in the lattice is labeled with its respective pattern. \nLattices are constructed from the top down, since patterns are \nadded in reverse order of length. Each note event in the piece contains a pointer to an occurrence, such that as occurrences are added, lattice links can be built in constant time (see Algorithm 3). \nConsider the patterns identified in the Mozart example (Table 2), \nfrom which we build the lattice in Figure 1. When the first occurrence of pattern P\n4 is inserted, o_left  = the first occurrence \nof P3, and o_right = null. Since P3 has the same length as P4, we \ncheck the right parent of the o_right , and update the link between \nthose occurrences of P1 and P4. Other links are updated in a more \nstraightforward manner. Given a series of npatternsP[]\n1.foriÅ0ton–1\n2.OÅOccurrences [P[i]]\n3.forjÅ0toSize[O]\n4.  occurrence pointed to by the first event of O\n5. o_right ÅOccurrence [eStream[O[j]],eIndex[O[j]]]\n6.  occurrence pointed to by the preceding event\n7. ifIndex[O[j]] =0\n8. o_leftÅnull\n9. else\n10. o_leftÅOccurrence [eStream[O[j]],eIndex[O[j]]-1]\n11.  we consider three cases for the value of o_left\n12. ifo_left=null\nwe learn nothing about the lattice\n13. else ifLength[o_left]>Length[O[j]]\n14. Right_Child [o_left]ÅO[j]\n15. else\n16. Right_Child [Right_Parent [o_left]]ÅO[j]\n17.  we consider two cases for the value of o_right\n18. ifo_right=null\nwe learn nothing about the lattice\n19. else\n20. Right_Parent [O[j]]Åo_rightused in line 16\n21. Left_Child [o_right]ÅO[j]\n22. Occurrence [eStream[O[j]],eIndex[O[j]]]ÅO[j]\nAlgorithm 3: Lattice Construction \nLength = 4\nLength = 3\nLength = 2e0,0e0,1e0,2e0,3e0,4e0,5e0,6\nP0 P0 P1 P1\nP2P3 P4P2 P3P4\nP5P6 P7P5P6P7P5\nright parent\nleft childleft parent\nright child\n \nFigure 4: Lattice for the First Phrase of Mozart's Symphony \nno. 40 \nFrom this lattice, we easily identify non-redundant partial \noccurrences of patterns. For each pattern, we perform a breadth-first traversal from its occurrences in the lattice, marking patterns and events as they are counted so that none are included twice. Simultaneously, the number of doubled occurrences is counted. In this manner, we calculate the value of the v\ndoublings  and vfrequency  \nfeatures for each pattern (see Algorithm 4). \n \nTake for instance pattern P2 in the Mozart example. By breadth-\nfirst traversal, starting from either occurrence of P2, the following \nelements are added to Q: P2, P5 and P6. First, we add the two \noccurrences of P2, tagging events e0,0, e0,1, … , e0,5, and setting  \nvfrequency  Å 6. The first two occurrences of P5 contain tagged \nevents, so we reject them, but the third occurrence at e0,6 is un-\ntagged, so we tag e0,6, e0,7, e0,8 and set vfrequency Å 6 + 2. All \noccurrences of P6 are tagged, so the frequency of P2 is equal to 8 / \n3. Given a pattern P:\n1.idÅunique identifier for pattern\n2.Tag[P]Åid\n3.push(Q,P)\n4.while~empty(Q)\n5.  add chbildren to Queue (DFS)\n6.pop(Q,p)\n7.o_leftÅLeft_Child [Occurrences [p][0]]\n8.o_right ÅRight_Child [Occurrences [p][0]]\n9.ifo_left~=null andTag[Pattern][o_left]~ =id\n10. Tag[Pattern[o_left]]Åid\n11. push(Q,Pattern[o_left]])\n12.ifo_right~=null andTag[Pattern][o_right]~ =id\n13. Tag[Pattern[o_right]]Åid\n14. push(Q,Pattern[o_left]])\n15.  count non-redundant occurrences of p\n16.foriÅ0toSize[Occurrences [p] ]–1\n17. ifevents in Occurrences [p][i]haveTag~=id1\n18. setTagÅ\nidfor all events in Occurrences [p][i]\n19. vfrequency[P]Åvfrequency[P]+Length[Occurrences [p][i]]\n20. vdoublings[P]Åvdoublings[P]+Length[Occurrences [p][i]]\n21.vfrequency[P]Åvfrequency[P]/vlength[P]\n22.vdoublings[P]Åvdoublings[P]/vlength[P]\nAlgorithm 4: Calculating Frequency \n2.8 Other Pattern Features \nSeveral pattern features have been described thus far: vinterval_count , \nvabsolute_interval_count , vlength, vfrequency  and vdoublings . In addition, we \nconsider pattern duration ( vduration ), rhythmic consistency ( vrhythm), \nposition in the piece ( vposition ), and register (calculated from event \nregister, vregister ). \n2.8.1 Duration \nThe duration parameter is an indicator of the temporal interval \nover which occurrences of a pattern exist. For a given occurrence o, with initial event e\ns1,i1 and final event es2,i2, the duration D(o) = \nOffset [es2,i2] – Onset [es1,i1]. For a pattern P, with occurrences o0,  \no1, ... , on-1, the distance parameter is calculated to be the average \nduration of all occurrences: \n \nnoD\nvn\nii\nduration∑−\n==1\n0)(\n \n2.8.2 Rhythmic Consistency \nWe calculate the rhythmic distance between a pair of occurrences \nas the angle difference between the vectors built from the IOI values of each occurrence. For occurrence o, with events E\n0, E1, \n… , Elength – 1 ,2 the IOI vector is \n> =<−] [ ],...,[ ],[ )(1 1 0 lengthEIOI EIOIEIOI oV . The rhythmic \ndistance between a pair of occurrences oa and ob is then the angle \ndistance between the vectors V(oa) and V(ob): \n \n\n\n\n ⋅=−\n)()()()(cos),(1\nb ab a\nb aoVoVoVoVooD \n                                                                 \n1 If the first and last events of Occurrences [p][i] are un-tagged, \nthen we can assume the occurrence has not been counted even in part, since previously considered occurrences are necessarily of greater or equal length. As such, only the first and last events are examined here. \n2 We use the notation Ej to refer to an arbitrary event es,i. Note that \nEj and Ej+1 refer to consecutive events es,i and es,i+1.  Figure 5: Rhythmic Distance Measure \nA 3-dimensional example of the rhythmic distance calculation \nbetween two occurrences oa and ob is shown in \nFigure 5. \nWe take the average of the distances between all occurrence ( o0,  \no1, ... , on-1) pairs for a pattern P to calculate its rhythmic \nconsistency: \n2)1(2\n01\n1))(),((\n−−\n=−\n+=∑∑\n=nnn\nin\nijj i\nrhythmoVoVD\nv \nThis value is a measure of how similar different occurrences are \nwith respect to rhythm. Notice that two occurrences with the same notated rhythm presented at different tempi have a distance of 0. Consider the case where o\na has k times the tempo of ob. In this \ncase, )( )(a b okV oV = , and \n0))( ),(( ))(),(( = =a a b a okVoVD oVoVD . \nOccurrences with similar rhythmic profiles have low distance, so \nthis approach is robust with respect to performance and compositional variation. For instance, in the Well-Tempered \nClavier  Bach often repeats fugue subjects at half speed. The \nrhythm vectors for the main subject statement and the subsequent stretched statement will thus have the same angle, and a distance of zero. Similarly, if two presentations of a theme have slightly different rhythmic inflections, their IOI vectors will nonetheless be quite similar. \n2.8.3 Position \nNoting that significant themes are sometimes introduced near the start of a piece, we also characterize patterns according to the onset time of their first occurrence ( o). Note that occurrences are \nsorted according to Onset as patterns are identified, so the first \noccurrence is also the earliest occurrence: \n] [][ ],[ o Indexo Stream position e Onset v =  2.8.4 Register \nGiven the register values calculated for note events, the register \nvalue for a pattern P with occurrences o0,  o1, ... , on-1, is equal to \nthe average register of all events contained in those occurrences: \n \n)1 (*] [][ ],[1\n00\n+=+−\n==∑∑\nlengthj o Indexo Phrasen\niv\nj\nregistervne Register\nvi ilength\n \n2.9 Rating Patterns \nFor each pattern P, we have calculated several feature values. We \nare interested in comparing the importance of these patterns, and a convenient means of doing this is to calculate percentile values for each parameter in each pattern, corres ponding to the \npercentage of patterns over which a given pattern is considered stronger for a particular feature. These percentile values are stored in a feature vector: \n \n> =<register count interval Length p p p PF ,..., , ][_ \nWe define stronger as either less than or greater than depending \non the feature. Higher values are considered desirable for length, duration, interval counts, doublings and frequency; lower values are desirable for rhythmic consistency, pattern position and register. \nThe rating of a pattern P, given some weighting of features W, is: \n ][ ][ PFW P Rating ⋅ ←  \n2.10 Returning Results \nPatterns are then sorted according to their Rating field. This \nsorted list is scanned from the highest to the lowest rated pattern until some pre-specified number ( k) of note events has been \nreturned. Often, MME will rate a sub-sequence of an important theme highly, but not the actual theme, owing to the fact that parts IOI:             11             5             12\nIOI:             12             4             12oa\nob\n024681012\n012345024681012V(oa)V(ob)\n40)()()()(cos),(1\nπ≅\n\n\n ⋅=−\nb ab a\nb aoVoVoVoVooD\nD(oa ,oa)θ> =< 12,5,11)(aoV\n> =< 12,4,12)(boVof a theme are more faithfully repeated than are others. As such, \nMME will return an occurrence of a pattern with an added margin on either end, corresponding to some ratio g of the occurrences \nduration, and some ratio of the number of note events h, \nwhichever ratio yields the tightest bound. \nIn order to return a high number of patterns within k events, we \nuse a greedy algorithm to choose occurrences of patterns when they are added: whichever occurrence adds the least number of events is used. \nOutput from MME is a MIDI file consisting of a single channel of \nmonophonic (single voice) note events, corresponding to important thematic material in the input pi ece. \n3. Results \nA set of 60 pieces from the Bar oque, Classical, Romantic, \nImpressionistic and 20th Century were used to train and test the \nsoftware. Bach, Mozart, Beethoven, Brahms, Schubert, Mendelssohn, Dvorak, Smetana, Debussy, Bartok and Stravinsky are represented, in chamber, orchestral and solo piano works. \nA few details of MME’s configuration should be mentioned: the \nintervallic variety filter required a minimum of at least zero distinct intervals, and two distinct absolute intervals. Maximum pattern length is set to 12 transitions, and streams are broken with silences longer than one and a half seconds. For the sake of result output and training, there is a margin of 0.5 on both ends for both events and duration. Up to 240 note events are returned for each \npiece, as compared with an average of over 8500 notes per pi ece \noriginally. We employ a hill-climbing algorithm to discover good values for W. \n3.1 Preliminary Results \nGiven even feature weighting, the primary theme was returned in \n51 of the 60 pieces. Learning weights W across  this entire set, and \ntesting across the same set, the primary theme was returned on 60 of the 60 pieces. These results are presented only to provide context for later results, and to provide some indication of the importance of learning appropriate weights. \n3.2 Training Trials \nWe performed 30 trials, randomly selecting a 30-pi ece training set \nfor each trial. During each trial, the hill-climbing algorithm was permitted 50 random restarts. These weights were then evaluated against the test set, consisting of the remaining 30 pieces. In two trials, MME identified 28 of the 30 primary themes, in seven trials 29 out of 30, and in 21 trials 30 out of 30, or on average roughly 29.6 out of 30, as compared with an expected average of 25.5 out of 30 using even weights (see Figure 6.) \n051015202530\n1\n3\n5\n79\n11\n1315\n17\n1921\n23\n2527\n29 Trial NumberNumber of Primary \nThemes Returned \nCorrectly out of 30\n \nFigure 6: Trial Results 3.2.1 Weights \nExamining the weights learned during the trials, we get some idea \nof the relative importance of the different pattern features examined. The average and median weights across the 30 trials are listed in Table 3. \nOf particular interest is the negative weight for absolute interval \ncount. Although our early experiments indicated that filtering patterns with low intervallic variety improves algorithm performance, it appears this parameter does not usefully distinguish the remaining patterns. The weight given to the register feature is perhaps most surprising., as we normally associate important melodies with the highest-sounding voice in a passage. Position is clearly the dominant feature, perhaps owing to our focus on primary themes, which tend to occur near the opening of pieces. \nTable 3: Feature Weights \nFeature Average \nWeight Median \nWeight \nabsolute interval count -0.016249988 -0.021642894 \nregister 0.051727027 0.041694308 \ndoublings 0.085212347 0.055842776 \ninterval count 0.121993193 0.110731687 \nfrequency 0.119746216 0.125918866 \nrhythmic consistency 0.176786867 0.181440092 \nduration 0.233749767 0.237064805 \nlength 0.344768215 0.274449283 \nposition 0.819313306 0.872008477 \n3.2.2 Errors \nThree pieces were res ponsible for all errors in MME’s output: the \nfirst movement of Mozart’s Symphony no. 40 , the second \nmovement of Brahms’ Cello Sonata in E minor , and Brahms’ \nAcademic Festival Overture . In the first two cases, the proper \ntheme was only partly returned in some trials, and in the last case, another theme sometimes dominated, albeit one that might be considered subjectively more prevalent than that listed first in Barlow. \nExamining the Mozart example (see Figure 7), the opening few \nnotes exhibit a low absolute interval count (only minor seconds, +/- 1), which explains why MME returned only the subsequent portion of the theme in some trials. This piece was included in 20 of the 30 test sets, and in three of those cases, the output was offset as described. In the remaining 17 cases, the proper theme was returned in full.  \nBarlow, 1sttheme\nMME output includes \nFigure 7: Mozart Symphony no. 40 1st Theme \nIn the case of the cello sonata, MME again selected only a portion \nof the 1st theme, in four of the 14 trials in which it appeared in the \ntest set.  This movement contains a great deal of repetition and variation, on the one hand offering a wealth of potentially important targets, and on the other, confusing the system due to \nits reliance on exact repetition. \nThe Academic Festival Overture contains a large number of \nthemes, and in every trial, MME returned a fair number of them. The first theme listed in Barlow, however, was returned only six of the 10 times the piece appeared in the test set. In all cases, MME returned another theme (see Figure 8). \n \nBarlow theme\nMME theme \nFigure 8: Themes from Brahms' Academic Festival Orchestra \n \n3.2.3 Sample of Output \nMME’s output from Smetana’s The Moldau  (a movement of My \nCountry ) is shown in Figure 10. The first section A contains the \n1st theme as indicated by Barlow. Section F contains a slight \nrhythmic variation on the same material, and section H presents \nthe subsequent phrase. In addition, section B and D contain tonal \nvariations of the same material (presented here in the major, whereas the main presentation is in the minor.) To many listeners, these sections sound similar. This highlights a potential weakness of the algorithm: although the correct material is returned, there is redundancy in the output. \n3.2.4 Popular Music \nMME has been tested on several pieces of popular music, though \nwe present no formal results in the absence of an accepted benchmark for system performance in this genre. Across 20 songs, ranging from the Beatles to Nirvana, an untrained version of MME returned the chorus where applicable, and what we considered to be significant “hooks” in all cases. \n050010001500200025003000\n0 5000 10000 15000 20000 25000 30000\nNumber of Note Events in InputTime (msec)\n \nFigure 9 \n4. Summary \nIdentifying the major themes in a sophisticated musical work is a \ndifficult task. The results show that MME correctly identifies the major themes in 100% of the test cases (when learning is employed), and identifies 85% of the major when learning is not used. It is interesting to note that MME contains no deep musical \nknowledge, such as theory of melody, harmony, or rhythm. Rather, it works entirely from surface features, such as pitch contour, register, and relatively duration. We found, surprisingly, that register is not a good indicator of the thematic importance. \nMME is computationally efficient. The system’s overall \ncomplexity is dominated by the frequency calculation, which in \nthe worst-case operates in ) (\n23nmΘ  time, where m is the \nmaximum pattern length under consideration, and n is the number \nof note events in the input pi ece. In practice, however, we observe \nsub-linear performance (see Figure 9), and reasonable running times on even the largest input pi eces.  \n5. Acknowledgements \nWe gratefully acknowledge the support of the National Science \nFoundation under grant IIS-0085945, and The University of Michigan College of Engineering seed grant to the MusEn project. The opinions in this paper are solely those of the authors and do not necessarily reflect the opinions of the funding agencies. \nWe also thank member of the MusEn research group for their \ncomments. This group includes Greg Wakefield, Roger Dannenberg, Mary Simoni, Mark Bartsch and Bryan Pardo. \n/g9/g6/g27/g25\n/g44/g143/g143/g44/g143/g143/g44/g143/g143/g44/g143 /g17/g143 /g17/g143 /g143\n/g44/g143/g17/g143 /g143/g44/g143 /g17/g143 /g143/g44/g143/g143/g44/g143/g143/g44/g143\n/g9/g6\n/g44/g143/g143/g44/g143/g143/g6/g44/g143/g143/g44/g143/g17/g143 /g143/g44/g143/g17/g143 /g17/g143 /g143/g44/g143 /g17/g143 /g143/g44/g143/g143/g44/g143/g17/g143 /g17/g143 /g143/g44/g143\n/g9/g6\n/g143/g143/g143/g143/g143/g143/g17/g143/g143 /g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g44/g143/g143/g44/g143/g143/g6/g44/g143/g143/g44/g143/g17/g143 /g17/g143 /g17/g143 /g17/g143/g143/g44/g143\n/g9/g6/g17/g143 /g143/g44/g143/g143/g44/g143/g17/g143 /g17/g143 /g17/g143\n/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143\n/g9/g6/g143/g143/g143/g136/g164/g136/g143/g143/g143/g79/g143/g143/g143/g143/g143/g143/g143/g143/g79/g143/g143/g143/g143/g75/g143/g143/g143/g143/g143/g79/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g143/g164/g136\n/g143/g143/g75\n/g143\n/g9/g6\n/g44/g143/g143/g44/g143/g143/g44/g143/g143/g44/g143/g17/g143 /g17/g143 /g17/g143 /g17/g143/g143/g44/g143 /g17/g143/g143/g44/g143/g143/g44/g143/g143/g44/g143/g17/g143 /g17/g143 /g143/g142/g44/g143/g143/g44/g143/g17/g143\n/g9/g6\n/g143/g6/g143/g142\n/g143/g143/g6/g143/g6/g143/g143/g143/g75/g143/g142\n/g143/g143/g6/g143/g6/g143/g143/g143/g83/g143/g136/g142\n/g143/g143/g6/g143/g6/g143/g143/g143/g75/g143/g44/g143/g143/g44/g143\n/g164/g75/g143\n/g9/g6/g143/g44/g143/g143/g44/g143/g143/g44/g143/g143/g44/g143 /g17/g143/g143/g44/g143 /g17/g143 /g143/g44/g143 /g17/g143 /g143/g44/g143 /g17/g143/g142/g44/g143 /g17/g143 /g143/g44/g143A\nB\nC D\nE\nF\nGH\n \nFigure 10: Output from Smetana's Moldau \n6. References \n[1] Ludwig Ritter von Köchel. Chronologisch-thematisches \nVerzeichnis sämtilicher Tonwerke Wolfgang Amadé Mozarts; nebst Angabe der verlorengegangenen, ange l’angene, von fremder Hand bearbelteten, zwelfelhaften und unterschobe-nen Kompositionen . Wiesbaden, \nBreitkopf & Härtel, 6th edition, 1964. [2] H. Barlow. A dictionary of Musical Theme s. Crown \nPublish-ers, New York, 1975. \n[3] David Cope. Experiments in Musical Intelligenc e. A-R \nEditions, 1996. \n[4] Alexandra and Uitdenbogerd. Manipulation of music for \nmelody matching. ACM Multimedia, Electronic \nProceedings , 1998. \n[5] Y.-H. Tseng. Content-based retrieval for music \ncollections. SIGIR , 1999. \n[6] M. Simoni, C. Rozell, C. Meek, and G. Wakefield. A \ntheoretical framework for electro-acoustic music. ICMC , 2000. \n[7] David Temperley. Modeling meter and harmony: A \npreference-rule approach. Computer Music Journal , 23. [8] David Temperley. A model for contrapontal analysis. \nunpublished . \n[9] R. L. Rivest T. H. Cormen, C. E. Leiserson. \nIntroduction to Algorithms . The MIT Press, Cambridge, \nMass., 1999. \n[10] A.S. Bregman. Auditory Scene Analysis: The Perceptual \nOrganization of Sound . MIT Press. \n[11] Peter van Beek. The design and experimental analysis of \nalgorithms for temporal reasoning. Journal of Artificial \nIntelligence Research , 1996. \n[12] Joseph G. D’Ambrosio, William O, Birmingham. \nPreference-Directed Design. AI EDAM , 1994. R. L. \nRivest T. H. Cormen, C. E. Leiserson. Introduction to \nAlgorithms . The MIT Press, Cambridge, Mass., 1999."
    },
    {
        "title": "Music Signal Spotting Retrieval by a Humming Query Using Start Frame Feature Dependent Continuous Dynamic Programming.",
        "author": [
            "Takuichi Nishimura"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1417191",
        "url": "https://doi.org/10.5281/zenodo.1417191",
        "ee": "https://zenodo.org/records/1417191/files/Nishimura01.pdf",
        "abstract": "We have developed a music retrieval method that takes a humming query and finds similar audio intervals (segments) in a music audio database. This method can also address a personally recorded video database containing melodies in its audio track. Our previous retrieving method took too much time to retrieve a segment: for example, a 60-minute database required about 10-minute computation on a personal computer. In this paper, we propose a new high-speed retrieving method, called start frame feature dependent continuous Dynamic Programming, which assumes that the pitch of the interval start point is accurate. Test results show that the proposed method reduces retrieval time to about 1/40 of present",
        "zenodo_id": 1417191,
        "dblp_key": "conf/ismir/Nishimura01",
        "keywords": [
            "music retrieval",
            "humming query",
            "audio intervals",
            "personal computer",
            "high-speed retrieving",
            "start frame feature",
            "Dynamic Programming",
            "pitch accuracy",
            "test results",
            "retrieval time"
        ],
        "content": "Music Signal Spotting Retrieval by a Humming Query \nUsing Start Frame Feature Dependent Continuous \nDynamic Programming \n \n \nTakuichi Nishimura \nReal World Computing Partnership \n/ National Institute of Advanced \nIndustrial Science and Technology \nTsukuba Mitsui Building 13F, 1-6-1 \nTakezono Tsukuba-shi, Ibaraki  \n305-0032, Japan \n+81-298-53-1686 \nnishi@rwcp.or.jp \n \nJ. Xin Zhang \nMedia Drive Co.  \n3-195 Tsukuba, Kumagaya-shi, \nSaitama, 360-0037, Japan \n+81-48-524-0501 \nchou@mediadrive.co.jp Hiroki Hashiguchi \nReal World Computing Partnership  \nTsukuba Mitsui Building 13F, 1-6-1 \nTakezono Tsukuba-shi, Ibaraki  \n305-0032, Japan \n+81-298-53-1668 \nhiro@rwcp.or.jp \n \n \n \nMasataka Goto \nNational Institute of Advanced \nIndustrial Science and Technology \n/\"Information and Human Activity''  \nPRESTO, JST \n1-1-1 Umezono, Tsukuba-shi, Ibaraki \n305-8568, Japan \n+81-298-61-5898 \nm.goto@aist.go.jp \n  Junko Takita \nMathematical Systems Inc. \n2-4-3 Shinjuku, Shinjuku-ku, Tokyo \n160-0022 J apan \n+81-3-3358-1701 \ntakita@msi.co.jp \n \n \n \n \nRyuichi Oka \nReal World Computing Partnership \nTsukuba Mitsui Building 13F, 1-6-1 \nTakezono Tsukuba-shi, Ibaraki  \n305-0032, Japan \n+81-298-53-1686 \noka@rwcp.or.jp \n \n \nABSTRACT  \nWe have developed a music retrieval method that \ntakes a humming query and finds similar audio intervals (segments) in a music audio database. This method can also address a personally recorded video database containing melodies in its audio track. Our previous retrieving method took too much time to retrieve a segment: for example, a 60-minute database required about 10-minute computation on a personal computer. In this paper, we propose a new high-speed retrieving method, called start frame feature \ndependent continuous Dynamic Programming , which \nassumes that the pitch of the interval start point is accurate. Test results show that the proposed method reduces retrieval time to about 1/40 of present methods.  \n1. INTRODUCTION \nAlthough a large amount and variety of musical audio \nsignals have been available on the internet and stored in personal hard disks at home, information retrieval \nmethods for those signals are still in infancy. A typical method is a simple text-based search which seeks property tags attached to those signals, such as the song title, artist name, and genre. The purpose of this research is to enable a user to retrieve a segment of a musical audio signal desired just by singing its melody. Such a music retrieval method is also useful for retrieving video clips if those clips contain music in audio tracks. For practical application, we think it important that the method can be applied to an audio database that is not segmented into musical pieces (as is often the case with broadcast recordings). \nIn this paper, we focus on a music retrieval system \nthat takes a humming query and finds similar audio intervals (segments) in a music audio database. If the database is composed of melody scores such as MIDI, symbols (e.g., relative pitch change or span change) can be extracted robustly. In this case, symbol-based retrieval [1-4] is very efficient. On the other hand, \nextracted melody from an audio signal usually suffers a lot of error and such symbol-based methods are not applicable. Therefore, we developed the pattern-based retrieval method [5]. Fundamentally, we find a similar pitch sequence of a query (query pattern) in the melody-likeness pattern on the pitch-temporal plane obtained from the database shifting the query pattern along pitch axis and warping temporally as shown in Figure 1. The previous matching method is called model-driven path\n Continuous Dynamic \nProgramming (mpCDP)  which compares the \nreference and all the partial intervals in the database and outputs similar intervals by considering multiple possibilities of transpositions. Because the mpCDP achieves pattern-based matching, the method can also take whistle sound or tempo-varying humming. (In the following, we use two terms, \"reference pattern'' and \"input pattern'': we extract a \"reference pattern'' from a query and an \"input pattern'' from a database in order to feed those patterns into matching methods.) This previous mpCDP, however, is too slow to be used in practical applications: it needs much computational cost because it accumulates local similarities in 3-D space, which is composed from model-axis, input-axis, and pitch-axis. In this paper, we propose a new quick retrieval method, called start frame feature dependent \ncontinuous DP (s-CDP), which searches only in 2-D \nspace (reference-axis and input-axis) assuming that the feature (pitch in this paper) of the start point of the extracted optimal interval is accurate. Our new s-CDP is different from conventional continuous DP in the respect that we do not calculate local similarities beforehand because the start point of the optimal interval is obtained from bottom left to top right successively in the 2-D space.   \n \n \nFigure 1. Pattern matching for humming retrieval.  \nPitch shift and temporal-warp are considered. \n \n We evaluate our new method using 20 popular music \nselections comparing the conventional method to show that the proposed method can reduce search time to about 1/40 with retrieval rates in excess of 70%. \nThe following section describes the conventional \nretrieval method. The s-CDP and the retrieval method using s-CDP is proposed in Section 3. The method is evaluated in Section 4 and newly developed retrieval system is introduced in Section 5. \n2. OUR CONVENTIONAL RETRIEVAL \nMETHOD \nContinuous DP [7] is improved from temporally \nmonotonous Dynamic Programming (DP) for speech or gesture recognition. Continuous DP achieves inconsistent recognition because it can segment the input pattern automatically but it cannot consider multiple possibilities of transpositions. Therefore, we proposed mpCDP [5] for music retrieval method adding one dimension (pitch) for input pattern and finding similar pitch sequence to the query.  \n \n2.1 Continuous DP \nAs shown in Figure 2 (a), continuous DP calculates \naccumulated similarities ),(τtS  between reference \nτR ) 1( T≤≤τ  and input tI ) 0(∞≤≤t  by adding \nlocal similarities ),(τts . Denoting the temporal axis \nof reference and input as tand τ, respectively, \ncontinuous DP calculates ),(τtS  using the following \nrecursive equations. Boundary conditions )0, 1( t T≤≤≤τ : \n0)1,( )0,( =−=tS tS  \n0),0(),1( ==− ττS S  \nRecursive equations  ( t≤1 ): \n)1,(3)1,( ts tS⋅=  \n=),(τtS  \n\n+−+−−+−−+−+−−\n),(3)1,(3)2,1(),(3)1,1(),(),1(2)1,2(\nmax\nτ τ ττ τττ τ\nts ts tSts tSts ts tS\n   (1)  \n) 2( T≤≤τ  \nIn this equation, local path with maximum similarity \nis chosen among the three paths as shown in Figure 2(c). Numbers besides each point are weights for local similarities. Notice that the summation of weights is always three for one frame up along the reference axis. Therefore, dividing accumulated similarities by three can normalize them. \nNext, we find a similar interval. Figure 2(b) shows \naccumulated similarities \n),(TtS ; and the continuous \nDP decide the maximum point with higher similarity \nthan a certain threshold α as the end point of the \nsimilar interval.  \nIn brief, continuous DP finds the optimal path and \nmaximum accumulated similarities by choosing the maximum local path successively from bottom left to top right in the reference-input plane. The \n),(TtS  is \nthe similarity between the whole reference and the \ninput considering temporal warps from 1/2 to 2 times. \n  \n \n \nFigure 2. Continuous Dynamic Programming. \n \n \n2.2 Humming Retrieval by mpCDP \nThis section explains the conventional humming \nretrieval method using Figure 3. The method has three steps. First, database audio signals are analyzed every frame, 64ms in this paper, and sequence of melody-likeness of each pitch (SMLP) is obtained. When a query is input to the method, highest melody-likeness pitch is chosen from SMLP every frame and query model is created by the relative pitch change. Third, model-driven path Continuous Dynamic Programming (mpCDP) compares the model and all partial intervals in the database and output similar intervals by considering multiple possibilities of transpositions.  \nThe local path of mpCDP is different from that of \ncontinuous DP in that they are shifted along the pitch axis according to the model, which is the relative pitch change of the query. By executing such processes successively, accumulated similarity \n),,( xTtS (Here x denotes pitch axis) takes a \nmaximum similarity along the pitch axis at the end of \nthe model as shown in top-right figure of Figure 3. Then, \n),,( max xTtS\nx changes similar to Figure 2(b). \nFinally the mpCDP outputs the maximum point with \nhigher similarity than a certain threshold α as the end \npoint of the similar interval.   \n \n    \n   \n  \n \n \n \nFigure 3. Conventional retrieval method. \n \n \n3. PROPOSAL OF s-CDP \n3.1 s-CDP \nThe definition of s-CDP is that local similarities \n),(τts  are dependent on the feature of the start point \n)1),,((τtp of optimal paths. Therefore local \nsimilarities of s-CDP are described as \n) ,,,( ),(),( 1 τ ττtp t IRIRf ts=  using a certain similarity \nfunction ()f . (For continuous DP: \n),( ),(tIRf tsττ= ). \n \n \n \n \nFigure 4. Comparison of matching methods. \n \n The difference between conventional continuous DP \nand s-CDP is shown in Figure 4. All local similarities can be calculated beforehand for continuous DP (Figure 4(a)), whereas local similarities are obtained incrementally as the optimal paths are fixed for s-CDP as shown in Figure 4(b). There are three local similarities for each point ),(\nτt  because the start \npoints are different among three local paths as shown \nin Figure 2 (c). Equation (1) is rewritten for s-CDP as:  Recursive equations  ( t≤1 ): \n)1,(3)1,(2ts tS⋅=  \n=),(τtS  \n \n \n(2) \n \n \n) 2( T≤≤τ  \n \nHere we call the local paths in Figure 2 (c) path1, \npath2, path3 from top-left and define \n),(1τts , ),(2τts , ),(3τts  as local similarities for path1, \npath2, and path3, respectively. Those are defined as:  \n) ,,,( ),(),( 1 2 τ ττtp t IRIRf ts= )1(=τ   \n) ,,,( ),()1,2( 1 1 −− =τ ττtp t IRIRf ts  ) 2(τ≤ \n) ,,,( ),()1,1( 1 2 −− =τ ττtp t IRIRf ts  ) 2(τ≤ \n) ,,,( ),()2,1( 1 3 −− =τ ττtp t IRIRf ts ) 2(τ≤ \nThe ),(2τts  alone takes 1=τ  and requires exception \nbecause the start point is the same as the point )1,(t. \nSecond terms of path1 and path3 in equation (2) are \n),(2⋅⋅s  because both points come from points )1,1(−−  \nrelatively in the plane. \nOutputs of s-CDP are obtained as points with \nmaximum accumulated similarities in the same way as continuous DP, but they have positions of start points and features of start points. \nTo calculate the start position ),(\nτtp  on the input \naxis, first initialize with time t when 1,0=τ . \nt tp tp == )1,( )0,(  \nNext, copy the start point memorized at the local start \npoint to ),(τtp  as follows: \n\n−−−−−−\n=\n)3 ()2,1()2 ()1,1()1 ()1,2(\n),(\npathif tppathif tppathif tp\ntp\nτττ\nτ  \nStart positions )1),,((τtp  are obtained incrementally \nby the recursive equations above. In case of error in start points, features of start points \n),( 1,τtpIR  are incorrect - leading to miscalculation of \naccumulated similarities. Therefore, one must choose \na high melody-likeness point as the start point in order to segment the reference. As for input, segmenting a similar interval is the method’s purpose, so such a measure is impossible. Still, those errors are recovered if one feature near the start point is correct because s-CDP matches, warping the reference temporarily. \n \n3.2 Apply for Humming Retrieval \nIn this section, s-CDP is modified for the humming retrieval method. Because melody is expressed as the pitch sequence, the feature is pitch and each pitch is subtracted from the start pitch to cope with transpositions, assuming that start pitch is correct. Query is transposed to make the start pitch 0 as shown in Figure 5. On the other hand, database transposition is possible just after the start pitch is fixed by tracing back the optimal path derived from s-CDP. Then the segmented database is similarly transposed to make the start pitch 0 as shown in Figure 5. Finally, s-CDP compares the transposed query and database without any influence of transpositions. \n  \n \n \nFigure 5. Overview of melody matching \nconsidering transpositions. \n \n+−+−−+−−+−+−−\n),(3)1,(3)2,1(),(3)1,1(),( ),1(2)1,2(\nmax\n3 221 2\nτ τ ττ τττ τ\nts ts tSts tSts ts tSFigure 6 helps to explain the detail method. First, \nmake relative pitch sequence of query as the reference \n1'R R R−=τ τ . Second, obtain N high melody-likeness \ncandidates from database musical signal as input \n),,1 )(( N kkIt L= . In this study, local similarities \n),(τts  are defined: \n\n≠==)0 (0)0 (1),(\n,,\nτττ\ntt\nDDts  \nHere )}]1( )({ [ min),('\n, τ τ τ tp tkt IkI Rabs D −− =  and local \nsimilarities are 1 if one of the relative pitches of N candidates is equal to relative pitch of the query. We \nset 5=N  in experiments in this paper. \n \n \n  \n \n \nFigure 6. Proposed retrieval method using s-CDP. \n \n \n4. EXPERIMENTS \n4.1 Experimental Method \nWe prepared 20 WAV files (about 80 minutes in \ntotal) in 16 kHz sampling and monaural recording format as a music database to compare s-CDP with mpCDP. This database includes 10 Japanese pop songs, 8 children's songs, an animation song, and a Japanese enka . There are 4 male and 16 female vocal \nartists in the database. Also, 3 males and 2 females sang a portion of each song in the database for about 20 seconds. Hence, the total number of queries was 100. \nLet f\nb[Hz] (in this experiment: 55[Hz]) denote the \nlowest frequency of melody. We compute the melody likeness for the frequency ),,1( 212/X xfbxL=  [Hz] by \nFFT analysis in consideration of the harmonic \nstructure of acoustic signals. This method is a simple version of [5]. The step of the pitch axis of mpCDP is set at 60 (5 octaves: \n60=X ) considering the male and \nfemale voice pitch range. \nTwo thresholds segmented a reference from a query. \nTo decide the start point, the threshold is set at P5.0 , \nwhere P is average power (summation of square of \nsignal). For the end point, the threshold is set at P1.0 . \nThe start point threshold is set larger because the start \npoint pitch should be correct. \nThe search rate, which depends on a threshold α(0 ≤ α \n≤ 1), is defined as the average of precision rate \nNC/ND and recall rate NC/NT , where NC, ND, and NT \nrepresent the number of similar terms to the humming query in detected terms, the number of detected terms by mp-CDP, and the number of similar terms to the humming, respectively. The detected term by mp-CDP is correct if the following overlapping rate \n  term detected  rm similar te term detected  rm similar te  rate g overlappin∪∩=  \nis greater than 0.5. This means the overlapping terms \nbetween similar and detected ones has 70% intersection when lengths of both terms are mutually identical. Since the search rate depends on threshold α, \n\"the search rate for a humming query\" is defined as the maximum value running over all α. The computer \nfor this experiment is OS: Windows2000, CPU: Pentium IV 1.5GHz. \n \n4.2 Results \nTable 1 shows average search rates and search times \nfor 5 persons × 20 songs = 100 queries for query \nduration of 20 seconds. Those results show that s-CDP reduced search time to about 1/40. The mpCDP \nhas 60 times the local path calculation because it has 60 steps in pitch axis. The reduction effect is only about 1/40 because s-CDP calculates the start point and has three times the number of local similarities. \nThe s-CDP search rate was lower by 16%, though it is \nmore than 70%, showing practical usefulness. \n \n \nTable 1: Comparison of the two search methods. \nSearch method mpCDP s-CDP \nAverage search \nrate 89.0% 73.3% \nSearch time 532(s) 14.2(s) \n \n \n5. RETRIEVAL SYSTEM \nWe made a humming retrieval system as shown in \nFigure 7. From the top window, the query wave, the query pitch sequence, similarities in the database, and hit results are shown. On clicking the peak on the similarities or the hit results, similar intervals of video or music are played in the right-bottom window. Using a notebook PC with a Pentium III 750MHz \nCPU, retrieving a 10[s] query from 60[min] database took about 10 seconds. There are several similarity peaks in Figure 7 because the same song was recorded from a TV program and a radio program and also the song had several repeated melodies. Retrieving from an audio video database that captured a scene from karaoke has been successful with this system, which will be demonstrated in presentation. \n \n  \n \nFigure 7. Monitor of the developed retrieval system. \n \n \n6. SUMMARY \nWe proposed a start frame feature dependent \ncontinuous DP assuming that the start point pitch of the extracted optimal interval is correct. Test results showed that the proposed method reduces computational costs to about 1/40. \nOne method for further reducing retrieval time is to \ncompress similar intervals in the database because a song usually has a repeated melody. \n \n7. REFERENCES \n[1] Kageyama T., Mochizuki K., and Takashima Y.: \nMelody Retrieval with Humming, ICMC Proc., 1993, 349-351. \n[2] Asif Ghias and Logan J.: Query By Humming – Musical Information Retrieval in an Audio Database, ACM Multimedia '95, Electronic Proc., 1995. [3] Sonoda T., Goto M., and Muraoka Y.: A WWW-based Melody Retrieval System, ICMC’98 Proc., 1998, 349-352. \n[4] Kosugi N., Nishihara Y., Sakata T., Yamamuro M., and Kushima K.: A practical Query-by-Humming system for a large music database, ACM Multimedia 2000, 333--342. \n[5] Hashiguchi H., Nishimura T., Takita J., Zhang J. X., and Oka R.: Music Signal Spotting Retrieval by Humming Query Using Model Driven Path Continuous Dynamic Programming, SCI2001,  \n[6] Goto M.: A Predominat-F0 Estimation Method for CD Recordings: MAP Estimation using EM Algorithm for Adaptive Tone Models, Proc. of ICASSP 2001. \n[7] Oka R.: Continuous word recognition with Continuous DP (in Japanese), Report of the Acoustic Society of Japan, S78-20, 1978, 145-152."
    },
    {
        "title": "A Naturalist Approach to Music File Name Analysis.",
        "author": [
            "François Pachet"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415856",
        "url": "https://doi.org/10.5281/zenodo.1415856",
        "ee": "https://zenodo.org/records/1415856/files/Pachet01.pdf",
        "abstract": "Music title identification is a key ingredient of contentbased electronic music distribution. Because of the lack of standards in music identification – or the lack of enforcement of existing standards – there is a huge amount of unidentified music files in the world. We propose here an identification mechanism that exploits the information possibly contained in the file name itself. We study large corpora of files whose names are decided by humans without particular constraints other than readability, and draw various hypotheses concerning the natural syntaxes that emerge from these corpora. A central hypothesis is the local syntactic consistency, which claims that file name syntaxes, whatever they are, are locally consistent within clusters of related music files. These heuristics allow to parse successfully file names without knowing their syntax a priori, using statistical measures on clusters of files, rather than on parsing files on a strict individual basis. Based on these validated hypothesis we propose a heuristics-based parsing system and illustrate it in the context of an Electronic Music Distribution project. 1",
        "zenodo_id": 1415856,
        "dblp_key": "conf/ismir/Pachet01",
        "keywords": [
            "Music title identification",
            "content-based electronic music distribution",
            "lack of standards",
            "huge amount of unidentified music files",
            "exploits file name information",
            "large corpora of files",
            "natural syntaxes",
            "local syntactic consistency",
            "heuristics-based parsing system",
            "Electronic Music Distribution project"
        ],
        "content": "A Naturalist Approach to Music File Name Analysis \n \nFrançois Pachet \nSony CSL-Paris \n6, rue Amyot \n75005 PARIS \nFrance \npachet@csl.sony.fr \n Damien Laigre \nSony CSL-Paris \n6, rue Amyot \n75005 PARIS \nFrance \ndlaigre@csl.sony.fr\n \n \nAbstract: \nMusic title identification is a key ingredient of content-\nbased electronic music distribution. Because of the lack \nof standards in music identification – or the lack of \nenforcement of existing standards – there is a huge \namount of unidentified music files in the world. We \npropose here an identification mechanism that exploits \nthe information possibly contained in the file name itself. \nWe study large corpora of files whose names are decided \nby humans without particular constraints other than \nreadability, and draw various hypotheses concerning the \nnatural syntaxes that emerge from these corpora. A \ncentral hypothesis is the local syntactic consistency, \nwhich claims that file name syntaxes, whatever they are, \nare locally consistent within clusters of related music \nfiles. These heuristics allow to parse successfully file \nnames without knowing their syntax a priori, using \nstatistical measures on clusters of files, rather than on \nparsing files on a strict individual basis. Based on these \nvalidated hypothesis we propose a heuristics-based \nparsing system and illustrate it in the context of an \nElectronic Music Distribution project. \n1 Introduction \nThe recent progress of digital audio technologies and the \navailability of easy and cheap Internet access have led to \nthe proliferation of music files on the planet. \nEfficient digital audio compression format such as mp3 \nhave made possible the distribution of music on a large \nscale, using all sorts of broadcasting techniques and \nsupports, such as peer-to-peer communication systems. \nThis proliferation of music data around the globe is not \nincidental, and may be seen as a sign of the huge pressure \nfor Electronic Music Distribution (EMD) from the \ncommunity of music listeners. \nEMD, however, is more than just representing music as \naudio files. Confronted to large databases, users can only \naccess what they know, and content-based management \ntechniques are acknowledged to be a necessary ingredient \nto fulfil the target of true, personalized music distribution.  \n \n \n \n \n \n \n \n \n Content-based music access requires, between other \nthings, the ability of extracting features from the signal, \nof gathering descriptions of various source of textual \ninformation, of modelling user profiles and matching \nthese profiles to music descriptors, etc. (see Pachet, \n2001a for a survey). Among these requirements, one key \nissue is music identification: how to identify in a non-\nambiguous way music files. This identification is crucial \nto allow the management of metadata, copyrights, \nprofiles, recommendation systems, etc. Without a solid \nidentification mechanism, EMD may well turn into a \ngigantic and serendipitous adventure for users, content \nproviders and distributors. \nVarious standardization efforts have been conducted to \ndefine universal codes for music titles. The most famous \nis probably the ISRC (International Standard Recording \nCode), developed by ISO (ISO 3901) to identify sound \nand audio-visual recordings. ISRC is a unique identifier \nof each recording that makes up an album. Unfortunately \nit is not followed by all music production companies, and \nhardly used in unofficial music sources such as peer-to-\npeer communication systems. \nAnother problem is that, even when a code could be used, \nit is not: for instance, digital music encoded in the audio \nCD format usually does not contain information on the \nmusic identification. Strangely enough, it is not possible \nto get the track listing information from a CD. External \ndatabases of track listings for commercial CDs have been \ndeveloped, such as CDDB. CDDB works by associating \ntrack-listing information to audio signatures of CDs. To \nallow scaling up, CDDB is a collective effort: the \ndatabase is made up by the users themselves. While this \ncollaborative aspect does allow scaling up (there are more \nthan 4 millions CDs registered on CDDB), there is an \nobvious drawback to this enterprise: the track listing \ninformation is not guaranteed, which leads to many \nerrors, duplications and to the difficulty of identifying \ncorrectly music titles. \nThere are other sectors of the music production chain that \nare concerned with music title identification, such as \nradios (which display their track listing on Internet for \ninstance) or copyright associations (which have to keep \ntrack of broadcasted titles to compute the payment of \nroyalties). In each case, ad hoc and proprietary schemes \nhave been devised, but there is no convergence of music \nidentification methods. \nThere are several approaches to the identification of \naudio music sources. The most straightforward one \nconsists in analysing the signal, typically a portion of the \nwhole music title, to extract an audio signature. This \nsignature is then matched against a database of pre-\nrecorded music signals. This task is, for instance, \naddressed by technologies such as Broadcast Data \nSystems (US) or MediaControl (Germany), and is used   by copyrights management companies to infer radio play \nlists. The techniques used to perform the identification \nrange from usual pattern matching to more elaborate \nstatistical methods based on characterization of the \nevolution of spectral behaviours. In all cases, the \nidentification requires a database of all music files \ncreated beforehand. Such a global database is far from \nrealistic in the near future sot the approach can work only \nwithin limited contexts. \nAnother approach consists in exploiting external \ninformation about the music source. For instance, the \nEmarker system (Emarker, 2001), exploits the \ngeographical and temporal location of a radio listener \nrequesting a song, and then queries a large database \ncontaining all radio stations programs by time and \nlocation. The approach is of course much lighter than the \nsignal based approach since no signal processing is \nrequired, and can scale-up to recognize virtually any \nnumber of titles. It works of course only for titles played \non official radio stations. \nIn this paper we describe another approach, more suited \nto personal music file management systems, for which no \nradio track listing is available for identification, and \nwhich does not require the management of a global, \nuniversal database of music titles. This approach is based \non the analysis of actual music file names. \nMore precisely, we consider the context of popular music \ntitles, and therefore seek to identify two main information \nfor a music source: the artist (or performer) identification, \nand the actual name of the music title. We consider music \nfile names coming from natural sources, such as personal \nhard disk drives (usually filled with audio files coming \nfrom peer-to-peer communication systems), track listing \ndatabases (such as CDDB), or radio track listings. In all \nthese cases, the file names are input by users who do not \nfollow any constraint, other than human readability. \nWe consider here information contained in music file \nnames, and not identification from the signal, or from \nother external sources of information (such as ID tags in \nmp3 files, see Hacker, 2000). These other methods are \northogonal to the method proposed here. In an ideal case, \nmusic identification could exploit all these methods \ncollaboratively. \nWe will first introduce the context of our study, and the \ncorpora analysed (Section 2.1). We then propose several \nassumptions for guiding the analysis process, the main \nassumption being a local consistency assumption (Section \n2.2). We perform a statistical analysis of these corpora to \nvalidate the assumptions and draw corresponding \nheuristics. Finally, we describe FNI, a system that \nimplements our heuristics, and illustrate how it performs \nin the context of a real world Electronic Music \nDistribution system developed at Sony CSL, within the \nEuropean CUIDADO IST-funded project. \n2 Popular Music file names \nMusic file names may contain various types of \ninformation about a music title. In our context we focus \non popular music, for which two information are of \ninterest: the artist or interpreter identifier, and the actual \ntitle name. In some cases, file names can also contain \nother information such as the album or track number. In the case of Classical music, the notion of artist is more \ncomplex, and identification may contain both composer \nand performer identifier. Additionally, various identifiers \nmay also be present, such as the version (instrumental, \nremix, etc). Several statistical approaches have been \nproposed to parse text automatically into coherent \nsegments, corresponding for instance to different topics \nin news transcripts (see e.g Beeferman et al., 1999). In \nour case, the textual data considered is much shorter and \nthe domain (music works) is narrower, so we show that it \nis possible to derive heuristics to implement an efficient \nparsing system without a learning component, at least as \na first approximation. \n2.1 Corpora studied \nMusic files names are typically found in the following \nlocations: 1) personal storage systems such as hard disks, \n2) radio program track listings and 3) repositories of \nmusical metadata. For the purpose of our study, we \nidentified three such databases: a subset of 22, 302 album \ntrack listings from the CDDB database containing track \nlistings for about 4 millions CD albums, 2) a 1-year \nlisting of a radio station broadcasting music in a large \nvariety of styles (Fip/RadioFrance) and 3) a set of file \nlistings (about 3000 files) of personal hard-disks of \nintensive users of peer-to-peer music communication \nsystems. \nThese three cases share a common characteristic: the file \nnames they contain have been specified by individuals on \nwhom no particular syntactic constraint was enforced, \nother than human readability, i.e. the fact that these \nnames should be understood easily by other individuals \nof the same community. The individuals name their files \nas they wish, and these personal conventions are simply \nspread through the community without modification. In \nCDDB, the principle of the database is collaboration: \nalbums track-listings are given by the users themselves. \nAlthough the editors for entering track-listing information \nmay in some case force some structure (e.g. differentiate \ntitle and artists), there is no unique syntax valid for all \ntrack listings, as illustrated below. In the case of radio \nstations broadcasting their programs, there may more be \ncohesion since these programs are entered by a smaller \nnumber of individuals, but, similarly, the syntax will not \nbe constant, and will differ from a radio station to another \none. However, the case of radios is simplified by the fact \nthat the syntax of file names is usually constant for a \ngiven radio. \nTo illustrate our study, we give below some typical \nexamples of file names coming from the sources at hand. \n \nThe Rolling Stones - Angie \nThe Beatles - Oh! Darling \nEagles - Hotel California \nSimon & Garfunkel - The Sound Of Silence \nKansas - Dust In The Wind \nAmerica - The Last Unicorn \nCreedence Clearwater Revival - I Put A Spell On \nYou \nThe Beatles - Let It Be \nThe Tremeloes - Silence Is Golden \nHollies - He Ain’t Heavy He’s My Brother \nZz Top - Blue Jeans Blues \nSimon & Garfunken - El Condor Pasa (It I Could) \nBee Gees - Massachusetts   Omega - The Girl With The Pearl’s Hair - \nFeaturing Gabor Presser, Ann \nFigure 1. File names found on the CDDB database, for an \nalbum entitled “Golden Rock Ballads V.1” \n \nd:\\mp3\\CSL2-1\\Various - Animals - The house of \nthe rising sun.mp3 \nd:\\mp3\\CSL2-1\\Various - The Mindbenders - A \ngroovy kind of love.mp3 \nd:\\mp3\\CSL2-1\\Various - Hollies - The Air That I \nBreathe.mp3 \nd:\\mp3\\CSL2-1\\Various - The Beatles - Ain’t she \nsweet.mp3 \nd:\\mp3\\CSL2-1\\Various - Bee Gees - \nMassachusetts.mp3 \nd:\\mp3\\CSL2-1\\Various - The Moody Blues - Nights \nin white satin.mp3 \nd:\\mp3\\CSL2-1\\Simon and Garfunkel – El Condor \nPasa (If I Could).mp3 \nd:\\mp3\\CSL2-1\\Simon and Garfunkel – The Sound of \nSilence.mp3 \nd:\\mp3\\CSL2-1\\Bee Gees - Saturday Night \nFever.mp3 \nd:\\mp3\\CSL2-1\\Beastie Boys - Song for Junior.mp3 \nd:\\mp3\\CSL2-1\\Beach Boys - Good Vibrations.mp3 \nd:\\mp3\\CSL2-1\\01 - The Beatles - Doctor \nRobert.mp3 \nd:\\mp3\\CSL2-1\\05 - The Beatles - Sgt Pepper’s \nLonely Hearts.mp3 \nd:\\mp3\\CSL2-9\\Various - Rock F.M\\Original Rock \nN°5 - Crack The World Ltd - Fine Young Cannibals \n- She Drives Me Crazy.mp3 \nd:\\mp3\\CSL2-9\\Various - Rock F.M\\Original Rock \nN°5 - Crack The World Ltd - The Beach Boys - I \nGet Around.mp3 \nd:\\mp3\\Jazz\\STAN_GETZ\\MENINA_MOCA.mp3 \nd:\\mp3\\Jazz\\STAN_GETZ\\SAMBA_DE_UMA_NOTA_SO.mp3 \nFigure 2.  File names found on a personal hard disk. \n \n17:54 OH DARLING, THE BEATLES \nABBEY ROAD (1969 EMI) \n17:57 I BELONG TO YOU, LENNY KRAVITZ \n5 (1998 VIRGIN) \n18:01 FATIGUE D ETRE FATIGUE, LES RITA MITSOUKO \nCOOL FRENESIE (2000 DELABEL) \n18:09 IT AIN T NECESSARILY SO, MILES DAVIS \nBESS (1958 CBS) \n18:14 ENTRE VOUS NOUVIAUX MARIES, ALLA FRANCESCA \nBEAUTE PARFAITE / ALLA FRANCESCA (1997 \nOPUS 111) \n18:16 FOR EMILY WHENEVER I MAY FIND HER, SIMON \nAND GARFUNKEL \nCOLLECTED WORKS (1966 CBS) \nFigure 3. A typical radio program on Fip/Radio France. \n \n9:28 Bach: Concerto #4 in A, BWV 1055 (Glenn \nGould, piano, Columbia SO/Golschmann) CBS 38524 \n9:50 Bach/Manze: Toccata & fugue in d, BWV 565 \n(Andrew Manze, solo violin) Harmonia Mundi \n907250.51 \n10:04 Jaromír Weinberger: Polka & fugue from \nSchwanda the Bagpiper (Philadelphia O/Ormandy) \nSony 63053 \n10:21 Shostakovich: Piano concerto #2 in F, \nOp.101 (Mikhail Rudy, St. Petersburg PO/Jansons) \nEMI Classics 56591  \n10:49 Dvorák: Bagatelles, Op.47 (Takács Qrt.) \nLondon 430 077  \n11:14 Falla: El sombrero de tres picos (Three-\nCornered Hat), part 1 (Jennifer Larmore, Chicago \nSO/Barenboim) Teldec 0630-17145  \nFigure 4. Another typical radio program on \nWFCR/Western New England. 2.2 Clusters \nAn important remark to be made is that the music files \nconsidered are usually organized in different levels. In \nCDDB, there is only one level which is the album, itself \ncontaining tracks. On personal hard disks, there may be \nany number of levels, represented by the directory \nstructure of file systems. For the sake of generality, we \nconsider that the database of file names is structured by \nclusters – possibly – recursively. Clusters may contain \neither other clusters of file names. \nAs we can see, there is no universally valid syntax, either \nat the lexeme level (morphology of informative elements) \nor the music file level (actual syntax). However, these file \nnames are not totally random, and some regularities can \nbe identified, in particular at the cluster level. In the next \nsection, we examine more closely the regularities found \nin these various sources, from which we will draw a set \nof heuristics for an automatic file name recogniser.  \n2.3 An Empirical Analysis \nA manual analysis of a subset of our databases was \nperformed, to identify the most salient characteristics of \nfile names. This manual analysis of some examples yields \na number of regularities: \n \n1) Regularities at the file name level. There is a small \nnumber of delimiters that are used for separating \nartist and title information. Based on these \ndelimiters, there are some syntaxes with a higher \ndegree of probability than others. For instance: \n“artist – title “ such as “The Beatles - Oh! Darling”, \n“title – artist” such as “Oh Darling, The Beatles”,  or \n“constant term – artist – title” such as “Various - The \nBeatles - Ain't she sweet”, etc. \n2) Regularities at the word level. Artist names are \nusually found under a restricted number of syntactic \nforms, such as: “Paul McCartney”, “McCartney, \nPaul”, “Mc Cartney”, or “The Beatles”, “Beatles, \nthe”, “Beatles”. \n3) Most importantly, regularities at the cluster level. It \nappears that syntaxes, as cumbersome as they may \nsometimes be, are not distributed uniformly: within a \ncluster, it is often the case than all titles follow the \nsame syntax, or, at least, a small number of syntaxes. \nThis remark is at the core of our proposal, as we will \nsee below.  \n \nBased on these remarks, we propose the following four \nhypotheses relative to music file name analysis: \n \nDelimiter Hypothesis: \nThis hypothesis states that the artist and title name \ninformation are indeed separated by delimiters, which are \nspecial characters within a given, small set of characters.  \nAs a special case, we consider that a file name using no \nseparators is a title name without reference to its artist. \n  \nConstant Term Hypothesis: \nSeveral syntaxes may contain constant terms, which are \nnot directly relevant. A constant term can be for instance \nthe album name, a date, or key words such as “Various \nArtists” (see Figure 2). The notion of constant term here \nis augmented by integrating possibly varying numerals, to   handle cases such as track numbers (“Track 1,” Track 2”, \netc. see Figure 2). \n \nWord Morphology Hypothesis: \nArtist names and title names have statistically different \nmorphologies. For instance, the number of words for \nartist names is less important than the number of words \nused for title names. Additionally, artist names often \nmake use of a limited number of specific heuristics \nrelated to first name (McCartney, Paul” is the same than \n“Paul McCartney” or “McCartney, P.”). These heuristics \nmay be used to determine whether a piece of information \ndenotes an artist or a title name. \n \nLocal Syntactic Consistency Hypothesis \nThis hypothesis asserts that syntaxes of file names are \nconsistent within a given cluster (what we call a syntax \nwill be defined more precisely below). In reality, the \nhypothesis is weakened by the fact that this consistency \nmay not actually occur entirely within a cluster. For \ninstance, Figure 2 shows a directory listing containing \nfour main syntaxes (for a total 13 titles, which is indeed \nan extreme case). We weaken this hypothesis by \nconsidering sub-clusters sharing the same syntax, and \nshowing that only a small number of sub-clusters is \nneeded – in general – to perform the analysis correctly. \n \nIn the next Section, we show the results of an automatic \nanalysis performed on our databases to assess the validity \nor our hypothesis. \n3 Statistical Analysis of File Name \nCorpora \n3.1 Delimiter Hypothesis \nWe call here a delimiter a character used to separate \ndifferent type of information in a given segment. The \nhypothesis states that there are indeed delimiters: these \nspecial characters are - most often - used as separators, \nrather than significant characters for artists or title names.  \nThe most encountered delimiters in the corpora are the \nfollowing: ‘-‘, ‘/’, ‘(‘, ‘)’, ‘[‘, ’]’, ‘{‘, ‘}’, ‘;’, ‘:’. \nTo validate the Delimiter Hypothesis, we have to show \nthat the file names use delimiters to separate artist and \ntitle information. To do this systematically would require \na thorough check of over 300.000 titles, which is too hard \na task to be done manually. Instead, we show here that \ndelimiters are used in a consistent manner within each \ncluster. Although this check does not guarantee that \ndelimiters are indeed used to separate, e.g. artist and title \ninformation, it does a give strong indication that there is a \nconsistent use of these characters as syntactical elements \nrather than significant characters.  \nMore precisely we call “common delimiter” a character \ndelimiter found in all the segments of a given cluster. \nThis delimiter indicates in most of the case a separation \nbetween different information types. As the following \ntable shows, many (64.4%) though not all clusters have \none common delimiter. Some clusters have no delimiters \n(7.2%), which corresponds to cases where the file name \nonly contains the title information (the artist name is then \nmost often contained in the album name for CDDB, or in the super directory for personal files). In the remaining \ncases, several delimiters are found in given clusters. We \nthen look for the minimum number of delimiters that \n“cover” the whole cluster. What the table shows is that \nthere is, in most of the cases, a small number of such \ncovering delimiters, which is once again a strong \nindication that these delimiters are used for syntactical \npurposes. \n \n Nb clusters: Percentage: \nno delimiter: 1615  7.2 % \n1 common delimiter : 14354 64.4 % \n2 delimiters cover the \ncluster : 4763 21.3 % \n3 delimiters cover the \ncluster : 1338  6.0 % \n4 delimiters cover the \ncluster : 215  1.0 % \n5 delimiters cover the \ncluster : 17  0.1 % \nTotal : 22302 100.0 % \nFigure 5 Analysis of delimiters in  our CDDB play lists. \n3.2 Word Morphology Hypothesis \nThe word morphology hypothesis asserts that artist \nnames are shorter on average than title names. Although \nthis hypothesis is certainly not always true (e.g. the group \nnamed “Everything but the girl” has recorded a song \nnamed “Angel”), it is true in average, and in particular \nwithin clusters. \nAn analysis of about 17,000 titles from CDDB yields an \naverage of 1.6 words per artist names against an average \nof 3.2 words for title names, i.e. a ratio of 2 times more \nwords in artist names. Similarly, an analysis of 19,648 \ntitles from the FIP radio program yields 2.1 words for \nartist names against 3.1 words for the title names, i.e. a \nratio of 1.5. \nThis shows clearly that titles names are, on average, \nlonger than artist names. As we will show below, this \nheuristic may be used when no other clue allows to infer \nwhether a string is an artist or title name. \n3.3 Constant Term Hypothesis \nThe constant term hypothesis asserts that clusters may \nhappen to contain constant terms in all their segments. \nThese constant terms can refer for instance to the artist \nname, but also to information which is useless in our \ncontext.  \nThe analysis of our CDDB database yields 800 constant \nterms, of which about 20% are not artist names. As an \nindication, here are the 10 most frequent useless constant \nterms retrieved from this list: \n   Sampler  \nVarious Artists  \nVarious \nPassion \nUnknown \nFabulous \nSuccess \nDreams \nMemories \nMixery \nFigure 6 Most Frequent constant terms in CDDB play lists \nThese constant terms are used in our system to \ndifferentiate between useless information that can be \ndiscarded from useful information such as artist names. \n3.4 Local Syntactic Consistency Hypothesis \nThis hypothesis is the most important in our study, since \nit will allow us to determine the syntax according to the \nanalysis of a group of titles, rather than individual titles \nonly. To validate this hypothesis, we need to estimate the \naverage number of different syntaxes a cluster contains.  \nTo do so we introduce the notion of syntax as follows. \nFor a given file name string, we replace all the token \nstrings encountered by an alphabetic letter incremented \nautomatically (a, then b, then c, etc.) and we replace all \nnumbers by a digit (0, 1, 2; etc.). We let the delimiters \nunchanged. The resulting string may be seen as a \ncanonical representation of the syntax of the file name. \n \nHere are some examples of file names and their \nassociated syntax as extracted by our analysis: \n \nFile name  Syntax \nVarious - Bee Gees - \nMassachusetts.mp3 a-b-c \nSimon and Garfunkel – El Condor \nPasa (If I Could).mp3 a-b(c) \nThe Beatles - 05 - Sgt Pepper’s \nLonely Hearts Cl.mp3 a-0-b \nOriginal Rock N°5 - Crack The \nWorld Ltd - The Beach Boys (I \nGet Around).mp3 a-b-c(d) \nAll you need is love.mp3 a \nFigure 7 Canonical syntaxes for various music file names. \nAs an illustration of the process, here are the most \nfrequent syntaxes retrieved (in number of lines): a 69277 a-b 66637 \na/b 64584 a(b)c 30569 \na-b(c)d 15351 a/b(c)d 19191 \na:b 5561 a(b)-c 4198 \na-b-c 3050 a/b-c 2508 \na(b)/c 1959 a,b 1918 \na-b/c 1708 a(b-c)d 1319 \na[b]c 1317 a--b 1128 \na/b,c 954 a,b/c 930 \na:b(c)d 906 a-b,c 727 \na-b[c]d 703 a:b-c 673 \na,b-c 589 a/b[c]d 556 \n(a)b 524 a/b(c-d)e 517 \na-b-c(d)e 506 a(b)(c)d 500 \nFigure 8 Main syntaxes found in our CDDB play lists \nOnce syntaxes are extracted, we compute, for each \ncluster, the number of different syntaxes it contains. This \ncomputation simply consists in comparing syntaxes using \nstring comparison operators. The following table shows \nthe result of this computation. \n \n Nb clusters: Percentage : \n1 common syntax : 4871 21.8 % \n2 syntaxes in the cluster : 7702 34.6 % \n3 syntaxes in the cluster : 5160 23.1 % \n4 syntaxes in the cluster : 2691 12.1 % \n5 syntaxes in the cluster : 1162 5.2 % \n6 syntaxes in the cluster : 409 1.8 % \n7 syntaxes in the cluster : 180 0.8 % \n8 syntaxes in the cluster : 51 0.2 % \n9 syntaxes in the cluster : 27 0.1 % \nOver  9 syntaxes: 49 0.2 % \nTotal: 22302 100.0 % \nFigure 9 Analysis of syntaxes in our CDDB play lists \nThese results clearly show that there is indeed a syntactic \nconsistency in most of the clusters encountered. This \nconsistency, in turn, will be used to parse file names \naccording to the most prominent syntax within clusters, \nas shown in the next section. \n4 The FileNameInterpreter (FNI) System \nThe hypotheses we made and validated have been \nexploited to design and implement a file name interpreter, \nin the context of an EMD application. This application is \npart of Cuidado, a large European project for content-\nbased music access (see Pachet, 2001b). In this section, \nwe describe the overall design of this system, and show \nits performance on real world examples. \n4.1 Overview \nThe input of our system is a file containing a structured \nlist of file names. The output is a file containing the \nanalysed artist and title name information. This analysis \nis performed by applying heuristics as described below. \nTo allow flexibility, the user always has the possibility to \ncorrect manually the analysis proposed, and this \ncorrection is then substituted to the analysis in the output \nfile, and retrieved in later analysis to avoid repeating \ncorrections.   4.2 Initialization \nA pre-processing phase is applied systematically to the \ninput list of music file names. This pre-processing \nconsists in: \n \n1) Grouping together file names having the same syntax \ninto sub-clusters, \n2) Chunking related file names into segments according \nto delimiters. \n \nFor instance, if we consider the input file as given in \nFigure 2, considering only the first cluster we obtain the \nfollowing: \n \n1) The syntaxes found in this corpus are: “a-b-c”, “0-\na-b”, “a-b”, “a-b(c)”. \n \n2) The lists relative to the syntaxes are then the following \n(“|” indicates separation between recognized segments): \n \nSyntax: a-b-c \nVarious | Animals | The house of the rising \nsun \nVarious | The Mindbenders | A groovy kind \nof love \nVarious | Hollies | The Air That I Breathe \nVarious | The Beatles | Ain’t she sweet \nVarious | Bee Gees | Massachusetts \nVarious | The Moody Blues | Nights in white \nsatin \n \nSyntax: 0-a-b \n01 | The Beatles | Doctor Robert \n05 | The Beatles | Sgt Pepper’s Lonely \nHearts \n \nSyntax: a-b \nSimon and Garfunkel | The Sound of Silence \nBee Gees | Saturday Night Fever \nBeastie Boys | Song for Junior \nBeach Boys | Good Vibrations \n \nSyntax: a-b(c)  \nSimon and Garfunkel | El Condor Pasa (If I \nCould) \n \nEach of these three sub clusters is now treated using the \nimplementation of the heuristics as described below. \nIn the next sections, we consider each sub cluster as an \narray. The lines of the array match the lines of the sub \ncluster, and the columns of the array match the segments \nin each line of the sub cluster. \n4.3 Management of Identifiers \nIn order to take into account differences in the spelling of \nProper names (artists) and title names, we implement \nretrieval mechanisms based on a canonical representation \nof identifiers. This representation is computed so that \ndifferent spellings of a given identifier yield the same \nrepresentation. \nThe principle is to build a unique String composed only \nof the significant characters of a given identifier, \nremoving blanks, spaces, and non-standard characters. \n Additionally, there is a specific provision for managing \nartist names: artist names may have several attributes \nsuch as “firstName”, or “group prefix” (e.g. “The” or \n“Les” in French). These attributes are specified in a lazy \nmode, that is as they are encountered. \nFor instance, the first time we encounter the artist spelled \nas “McCartney, Paul”, we create an entry in the artist \ndirectory, with a canonical representation being \n“mccartney”, a first name being “paul”. \nWhen we encounter another occurrence of McCartney, \nbut with a different ordering or spelling, such as \n“McCartney” or “Paul McCartney”, we are able to \nretrieve the previously entered occurrence by trying \nseveral all the possible combinations. \n \nLastly, this specific procedure is augmented with a fuzzy \nmatching algorithm to take into account possible \nmisspelling and errors. This procedure is not discussed \nhere for reasons of space. \n4.4 Implementation of the heuristics \nWe describe here how we implement and prioritise the \ndifferent heuristics to infer the artist and title information \nfrom a given sub cluster in which all titles share the same \nsyntax. We do not describe the whole analyser here, but \nonly highlight its main structure. \n4.4.1 Case 1, implicit information \nIf the sub cluster contains only one segment, the only \nhypothesis we can make is that 1) the segment denotes \nthe title name, and 2) the artist information is contained \nin the super cluster (super directory usually). For \ninstance, if the corpus is a directory from a personal \ndatabase of music file names, the artist name can be the \nname of the directory containing the music files. This is \nthe case with the 2 “Stan Getz” files in Figure 2 for \ninstance. \n4.4.2 General Case \nAs illustrated in Section 2.2, about 93% of the play lists \nanalysed from CDDB have at least two segments. We \ntherefore assume that these segments contain at least both \nthe title name and the artist name. The problem is now to \ndetermine which segment is the artist name, which one \nthe title name, and which ones are useless groups of \nwords such as constant terms, dates, etc. \n \nHere is the ordering of the heuristics to identify properly \nthe artist and title information. \n \n1) Constant term heuristics, \n2) Artist names heuristics, \n3) Title name heuristics. \n4.4.2.1 Constant Terms Heuristics \nThis heuristics is applied only if the syntax of the sub \ncluster considered contains at least two segments. \nWe first check if the array contains any constant terms in \na whole column. If a column contains the same constant \nterm, there are two possible interpretations:   - The array contains two columns: the column containing \nthe constant terms is assumed to be the artist name \ncolumn. \n- The array contains more than two columns: we must \ncheck if the constant term belongs to a list of known \nconstant terms as illustrated in section 3.3. The list of \nwell-known constant terms we use has been retrieved \nfrom our CDDB database. We cannot determine whether \nor not a constant term is an artist name if it does not \nbelong to our list. If the constant term belongs to our list \nof constant terms, we will not take into account the \ncolumn relative to this constant term anymore in the title \nidentification and consider that the artist and title names \nare the remaining columns of the array. \n4.4.2.2 Artist Names Heuristics \nTo determine if a column is an artist name, we consider \nthe following information in the following order: \n \n1) Number of comas. \nOne heuristic is to consider that the column containing \nthe most commas in its strings is the artist names column. \nIndeed, even if the percentage of cases where the artist \nname is written with a comma (ex: “Beatles, the”, “Mc \nCartney, paul”) is not very high, this is a first way to \nretrieve the artist name. \n \n2) Known artists \nThen, if the artist column has not been found, we propose \nto take into account the artists already known by the \nsystem. If a known artist is found in a column, this is the \nartist column, following our local consistency hypothesis \n(all the artist names are in the same column). \n \n3) Number of different words \nOnce the elimination of columns containing useless terms \nhas been performed and if there are only two columns left \nin the array considered, we count the number of different \nwords in all the valid columns of the array. If the number \nis smaller in one of the columns, we assume this column \nrepresents the artist names. \n4.4.2.3 Title Name Heuristics \nAt this step of the identification, the column of the array \ncontaining the title names may be inferred in most of the \ncases by elimination since there is most often only one \ncolumn remaining. \nHowever, if we have more than one column, we apply \nagain the heuristics about the number of words: if the \nnumber of different words is greater in one of the \ncolumns, we considered it as the title names column. \n5 Experimentations \nOur system has been tested and validated on our three \ndatabases. To validate the system, we made about 1500 \nrandom experiments, by drawing a random title, and \nchecking manually whether the parse was correct or not. \n95 % of the cases where correctly analysed. We assume \nthe most frequent cases have been encountered. \n \n \nFigure 10. The interface of FNI. \n \nThe incorrect cases are most often non interpretable file \nnames. For instance, as illustrated in Figure 10,  “Various \n- Toots Thielemans - Jane's Theme - 05” has too many \nsegments. The “05” is duly recognized as a constant term, \nbut the system cannot determine which segment refers to \nthe title name and which one refers to the artist name.  In \nthis case, even a human could not infer the right syntax, \nunless he/she would know the track listings and albums \nnames of  Toots Thielmans. \nA few cases were not correctly analysed because the \nsyntax exceptionally did not match our heuristics. \nExample: “Johnny Lee Hooker - Boom, Boom.mp3”. The \nartist name has more words than the title name, and the \ntitle name contains a comma. However our system allows \nto correct manually the wrong file names (see Figure 10). \nAdditionally, the list of “known artists” is updated \nautomatically, so mistakes are only done once. \nFNI is integrated in Personal Radio, a working EMD \nsystem that has already been tested on over 100 users. \nMore tests are being conducted within the Cuidado \nEuropean project (Pachet, 2001b). \n6 Conclusion \nWe described a method for parsing music file names \nwithout knowing their syntax a priori. The method is \nbased on a set of justified heuristics which are validated \nby a prior analysis of a large corpora of “natural” file \nnames, and by a working system integrated in a large \nscale EMD project. The success of the approach lies \nmainly in the local consistency hypothesis, which states \nthat syntaxes are usually consistent within related groups \nof music files. This hypothesis allows to solve a number \nof ambiguity by making choices based on statistical \nproperties of file clusters rather than on individual files. \nExtensions of the approach for handling other types of   music (e.g. Classical) or non-Western filenames are \nunder study, and may require different sets of heuristics, \nbut we believe the approach in general is still valid. \nLastly, we plan to integrate a learning module to FNI that \nis able to learn automatically new syntaxes from errors, in \nthe spirit of (Petasis et al., 2001). \n7 References \nBeeferman, D. Berger, A. Lafferty, J. (1999) Statistical \nModels for Text Segmentation, Machine Learning, 34, \n1-3, Feb. 1999. \nHacker, Scott (2000) MP3, the definitive guide, O’Reilly. \nMaurel, D. Piton, O. Eggert, E. (2001) Automatic \nProcessing of Proper Nouns Vol. 41 N.3, February \n2001. \nPachet, F. (2001a) Content management for Electronic \nMusic Distribution: The Real Issues, submitted to \nCommunications of the ACM, 2001. \nPachet, F. (2001b) Metadata for music and sounds: The \nCuidado Project, Content-Based Multimedia Indexing \nWorkshop, Brescia (It). \nPetasis, G. Vichot, F. Wolinski, F. Paliouras, G. \nKarkaletsis, V. Spyropoulos, C. (2001) Using Machine \nLearning to Maintain Rule-based Named-Entity \nRecognition and Classification Systems, Association \nfor Computational Linguistics, ACL."
    },
    {
        "title": "An Audio Front End for Query-by-Humming Systems.",
        "author": [
            "Emanuele Pollastri"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415056",
        "url": "https://doi.org/10.5281/zenodo.1415056",
        "ee": "https://zenodo.org/records/1415056/files/Pollastri01.pdf",
        "abstract": "In this paper, the problem of processing audio signals is addressed in the context of query-by-humming systems. Since singing is naturally used as input, we aim to develop a front end dedicated to the symbolic translation of voice into a sequence of pitch and duration pairs. This operation is crucial for the effectiveness of searching for music by melodic similarity. In order to identify and segment a tune, well-known signal processing techniques are applied to the singing voice. After detecting pitch, a novel postprocessing stage is proposed to adjust the intonation of the user. A global refinement is based on a relative scale estimated out of the most frequent errors made by singers. Four rules are then employed to eliminate local errors. This front end has been tested with five subjects and four short tunes, detecting some 90% of right notes. Results have been compared to other approximation",
        "zenodo_id": 1415056,
        "dblp_key": "conf/ismir/Pollastri01",
        "keywords": [
            "audio signals",
            "query-by-humming systems",
            "symbolic translation",
            "pitch and duration pairs",
            "melodic similarity",
            "signal processing techniques",
            "tune identification",
            "postprocessing stage",
            "intonation adjustment",
            "relative scale"
        ],
        "content": "An Audio Front End for Query -by-Humming Systems  \nGoffredo Haus  Emanuele Pollastri\nL.I.M. -Laboratorio di Informatica Musicale, Dipartimento di Scienze dell’Informazione, Università Statale di Milano  \nvia Comelico, 39; I -20135 Milan (Italy)  \n+39-02-58356222  \nhaus@dsi.unimi.it  +39-02-58356297  \npollastri@dsi.unimi.it  \n \nABSTRACT  \nIn this paper, the problem of processing audio signals is addressed \nin the context of query -by-humming systems. Since singing is \nnaturally used as input, we aim to develop a front end d edicated to \nthe symbolic translation of voice into a sequence of pitch and \nduration pairs. This operation is crucial for the effectiveness of \nsearching for music by melodic similarity. In order to identify and \nsegment a tune, well -known signal processing t echniques are \napplied to the singing voice. After detecting pitch, a novel post -\nprocessing stage is proposed to adjust the intonation of the user. A \nglobal refinement is based on a relative scale estimated out of the \nmost frequent errors made by singers. F our rules are then em -\nployed to eliminate local errors. This front end has been tested \nwith five subjects and four short tunes, detecting some 90% of \nright notes. Results have been compared to other approximation \nmethods like rounding to the nearest absolut e tone/interval and an \nexample of adaptive moving tuning, achieving respectively 74%, \n80% and 44% of right estimations. A special session of tests has \nbeen conducted to verify the capability of the system in detecting \nvibrato/legato notes. Finally, issues about the best representation \nfor the translated symbols are briefly discussed.  \n1. INTRODUCTION  \nIn the last few years, the amount of bandwidth for multimedia \napplications and the dimension of digital archives have been \ncontinuously growing, so that accessibil ity and retrieval of \ninfor mation are becoming the new emergency. In the case of \ndigital music archive, querying by melodic content received a lot \nof attention. The preferred strategy has been the introduction of \nquery -by-humming interfaces that enable even  non -professional \nusers to query by musical content. A number of different imple -\nmentations has been presented since the first work by Ghias et al. \n[4] and a brief overview is introduced in the next section. In spite \nof this fact, the digital audio processi ng of an hummed tune has \nbeen tackled with naive algorithms or with software tools avail -\nable on the market. This fact results in a poor performance of the \ntranslation from audio signals to symbols. Furthermore, previ ous \nquery -by-humming systems can be hard ly extended to handle \nsung queries (i.e. with lyrics) instead of hummed queries.  The quality of a query -by-humming system is strictly connected to \nthe accuracy of the audio translation. It is well known that the \namount of musica l pieces retrieved through a melody grows when \nthe length of the query decreases [8, 12, 13, 22]. Employing \nrepresentations like the 3 -level contour will further lengthen the \nlist of matched pieces. In the same time, we can not expect users \nto search throu gh very long queries (more than twenty notes long) \nor to sing perfectly, without errors and approximations. Interval \nrepresentations show another source of errors, since a misplaced \nnote propagates to the contiguous one. Thus, an accurate transla -\ntion of th e input is surely a basic requirement for every query -by-\nhumming system.  \nIn this paper, we propose an audio front end for the translation of \nacoustic events into note -like attributes and dedicated to the \nsinging voice. We will focus on the post -processing of the voice \nin order to minimize the characteristic errors of a singer. In other \nwords, the audio processing will be conducted in a user -oriented \nway, that is, trying to understand the intention of the singer. This \nwork follows the one presented in [5] wh ere some preliminary \nwork and experiments have been briefly illustrated.  \n2. RELATED WORK  \nThere are many techniques to extract pitch information from audio \nsignals, primarily developed for speech and then extended to the \nmusic domain. The detection of pitch f rom monophonic sources is \nwell understood and can be easily accomplished through the \nanalysis of the sampled waveform, the estimation of the spectrum, \nthe autocorrelation function or the cepstrum method.  \nPrevious query -by-humming systems employed some bas ic pitch \ntracking algorithms with only little pre - and post - processing, if \nany. For example, Ghias et al. performed pitch extraction by \nfinding the peak of the autocorrelation of the signal [4], McNab et \nal. employed the Gold -Rabiner algorithm [12], while  Prechelt and \nTypke looked for prominent peaks in the signal spectrum [16]. \nRolland et al. [19] applied an autocorrelation algorithm with \nheuristic rules for post -processing. Some works focused mainly \non the matching and indexing stages of the query -by-humming, \nusing software tools available on the market for the audio transla -\ntion [3,7].  \n \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed fo r profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n  \n  Frame  Frame/Block  Event  MIC InputPre-Processing  Pitch -Tracking  Post-Processing  \nMIDI output  \nFigure 1. Architecture of the system developed.  \nOutside of the Music Information Retrieval community, the analy -\nsis of the singing voice constitutes an established research field, \nespecially in the framework of voice an alysis/re -synthesis. Typical \nexamples are the voice morphing system by Loscos et al. [10], the \nstructured audio approach to singing analysis score driven by Kim \n[6] and the synthesis of voice based on sinusoidal modeling by \nMacon et al. [11].  \n3. BACKGROUND  \nDespite its monophonic nature, singing has proved to be difficult \nto analyze [21]. The time -varying spectral characteristics of voice \nare similar during speech and singing. In both cases, we can \ndivide the generated sounds in voiced and unvoiced1. In order t o \nhave an approximate idea of this property, we can think of the \nformer kind of sounds as consonants2 and the latter as vowels. \nSince voiced sounds are constituted by periodic waveform, they \nare easier to analyze, while unvoiced sounds have a state similar  \nto noise. Luckily, during singing the voiced properties are pre -\ndominant and contain what we call musical pitches. However, the \ninformation held by unvoiced regions are important as well, since \nthey often contain the rhythmic aspect of the performance. Unl ike \nspeech, the singing voice shows a slowly -changing temporal \nmodulation both in the pitch and in the amplitude (vibrato). In \naddition to these acoustic properties, singing voice analysis \nshould deal with human performance that is typically affected by \nerrors and unstable. Previous researches revealed that errors \nremain constant regardless of the note distance in time and in \nfrequency [9]. We will follow these findings in the post -\nprocess ing step of the proposed front end.  \n4. VOICE PROCESSING  \nAn audio front e nd for a query -by-humming/singing system \nshould contain all the elements needed to perform the transforma -\ntion from audio to symbols, where audio is the singing voice and \nsymbols are the most likely sequences of notes and durations. It \nshould be able to ada pt to the user automatically, i.e. without any \nuser-defined parameter settings. Further, it should not require a \nparticular way of singing, like inserting some little pause between \nnotes or following some reference musical scale or metronome. In \na query -by-singing application, the last requirements are impor -\ntant to avoid limiting the number of potential users, who are \nexpected to be most non -professional users [5].  \n                                                                 \n1 A more rigorous definition is the following: “speech sounds can \nbe voiced, fricative (or unvoiced) and plosive, according to their \nmode of excitati on” [18]. In the present paper, plosive and \nfricative sounds will be grouped into the unvoiced category.  \n2 with the exception of [m][n][l] which are voiced.  We suggest to elaborate the audio signal at three different levels \nof abstraction, each one w ith a particular set of operations and \nsuitable approximations:  \n1- event  \n2- block  \n3- frame  \nAt the event level, we estimate starting/ending points of musically \nmeaningful signal, signal gain and, as a last step of computation, \npitches and durations. At the block level, a background noise \nthreshold is determined, voiced -unvoiced segments are isolated \nand pitches are approximated; eventually, effects of vibrato or \nbending are eliminated. At a frame level, we estimate spectrum, \nzero crossing rate, RMS power and octav e errors. From the above \nobservations, we derived an architecture (Figure 1) in which every \nuncertainty about the audio signal is resolved with subsequent \napproximations. The elaboration path is divided into three stages; \ndetails of each stage are presente d in the following sections. The \nsystem developed is designed for offline voice processing and is \nnot currently developed for real -time operations. Thus, audio is \ncaptured from a microphone, stored as wave file with sampling \nfrequency of 44100 samples/sec and 16 bit of quantization, and \nthen analyzed.  \n4.1 Pre-Processing  \nThe first purpose of the audio front end is to estimate the \nback ground noise. We evaluate the RMS power of the first 60 \nmsec. of the signal; a threshold for the Signal/Noise discrimi na-\ntion is set to a value of 15% above this level (S/N threshold). If \nthis value is above –30dB, the user is asked to repeat the \nrecording in a less noisy room. Otherwise, two iterative processes \nbegin to analyze the waveform, one from the beginning and \nanother from the end. Both the processes perform the same \nalgorithm: the RMS power of the signal is calculated for frame \n440 samples long (about 10 msec.) and compared with the S/N \nthreshold. To avoid the presence of ghost onsets caused by \nimpul sive noise, the value of  the n -th frame is compared to the \n(n+4) -th. The value of 40 msec. is too long for such noise and it is \nnot enough to skip a true note. The forward and backward analy -\nsis are then composed giving respectively a first estimate of the \nonset and offset points.  The fragments of signal between each \nonset and offset represent the musically meaningful events.  \nBefore localizing voiced and unvoiced regions, we calculate the \nderivative of the signal normalized to the maximum value, so that \nthe difference in amplitude  is emphasized. This way, it will be \neasier detecting the voiced consonants since their energy is most \nlikely to be lower than the energy of vowels. A well -known tech -\nnique for performing the voice/unvoiced discrimination is derived \nfrom speech recognition studies and relies on the estima tion of the \nRMS power and the Zero Crossing Rate [1, 18]. Plo sive sounds \nshow high values of zero crossing rate because the spectral energy \nis almost distributed at higher frequencies. Mean experimental Figure 3.  The proposed pitch -tracking stage; pitch detection is followed by a quantization step in \nwhich median approximation, vibrato suppression and legato detection are appli ed. The output is \na sequence of pitches and durations.  \n Hamming \n(46 msec)  FFT Peak Detection  Median  \n(~120 msec)  Voiced \nregions  \nEvent \nBoundaries  Shift (23 msec)  Octave  \nError Check  Vibrato \nDetection  \nLegato & split \nevents  Boundaries \nDetermination  \nNotes  \n(pitch -duration)  Pitch \nDecision  Figure 2.  The pre -processing stage of the system developed. An audio signal given in input is \nsegmented into musically meaningful events. Each event is characterized by its location in time \n(even t boundaries) and by its voiced region.   Noise level \nestimation  \nS/N \ndiscrimination  Δ signal  On/offset \ndetection  Voiced/ \nUnvoiced  Voiced regions  Event \nBoundaries  \nvalues of average num ber of zero crossings are 49 for unvoiced \nsounds and 14 for voiced sounds in a 10 msec win dow. The task \nis not trivial for other speech utterance like weak fricatives. A \nbetter technique employs mean and standard deviation of the \nRMS power and zero -crossin g rate of both back ground noise and \nsignal as thresholds. Moreover, heuristic rules about the maxi -\nmum duration admitted for each utterance are used. For ex ample, \nevents longer than 260 msec can not be un voiced. These methods \nare applied to the derivative o f the sig nal, detecting voiced conso -\nnants, unvoiced sounds and vowels. Thanks to this proce dure, we \ncan refine the on/offset estimation. In Figure 2 the process \nexplained so far is illustrated.  \n4.2 Pitch -Tracking  \nAs we said, the pitch of a sung note is captu red by its voiced \nregion and in particular by vowels. Thus, we will estimate pitch \nonly on those fragments. Compared to unvoiced sounds, voiced \nsounds exhibit a relatively slowly -changing pitch. Thus, the frame \nsize can be widen. For each voiced fragment i dentified in the \nsegmentation step discussed above, the signal is divided into half -\noverlapping Hamming windows of 46 msec (2048 samples) (see \nFigure 3). A FFT algorithm is performed for each frame and the \nmost prominent peaks of the estimated spectrum are  passed to the \nnext step. Here, it is taken the decision of pitch at the frame level. \nThe algorithm is a simplified version of the one presented in [15]. \nThe basic rule is quite simple: the candidate peak centered at a \nfrequency in the range 87 Hz – 800 Hz  that clearly shows at least \ntwo overtones is the fundamental frequency. Then, fundamental \nfrequencies within an event are mediated along each three subse -\nquent frames (median approximation) and are checked for octave \nerrors. A group of four contiguous fram es with similar funda -\nmental frequencies constitutes a block. This further level of abstraction is needed to look for vibrato and legato (with \nglissando), which are slowly changing modulations in pitch and in \namplitude. In the case of singing, vibrato is a regular modulation \nwith rate of 4/7 Hz (i.e. with a 150/240 msec period or about 1/2 \nblocks) and depth between 4% and 15% [14, 20]. Legato is \ndetected when adjacent blocks have pitches more than 0.8 semi -\ntones apart. This former case is resolved generating two dif ferent \nevents; otherwise, the adjacent blocks are joint to form an event. \nFor each event, pitch values are set to the average of the pitches of \nthe constituting blocks. These information are gathered with the \nrelative positions of consonants and the  exact bounds of each note \nare estimated.  \n4.3 Post-Processing  \nThe most critical stage is the post processing where the informa -\ntion captured by earlier stages are interpreted as pitch and dura -\ntion. The intonation of the user is rarely absolute3 and the \ntranscri ption process has to take into account a relative musical \nscale. Pitches are measured in fraction of semitones to improve \nthe importance of the relative distance between tones in the frame \nof the tempered musical scale. We use the definition of MIDI \nnote; the number resulting from the following equation is rounded \noff to three decimal places:  \n                                                                 \n3 Only about 1 in 10,000 people claim to have tone -Absolute \nPitch [17]  012log\n2 log1\nffNote MIDI= Eq. 1  Figure 4.  The post -processing stage of the system; the sequence of notes estimated in the previous stages is adjusted \nby means of a relative scale and  four local rules. The definitive tune is given in output.   Notes  \n(pitch -duration)  Reference \nDeviation \nEstimation  Scale \nadjustment  Local \nRules  TUNE   \nBin Range Notes within a binAverage \nDeviation\nBin 1 0.0-0.199 0\nBin 2 0.1-0.299 61.255; 58.286 0.27\nBin 3 0.2-0.399 61.255;58.286;58.328; 63.352 0.305\nBin 4 0.3-0.499 58.328;63.352;56.423;56.435 0.384\nBin 5 0.4-0.599 56.423;56.435; 61.537 0.465\nBin 6 0.5-0.699 61.537; 56.693; 56.623; 56.628; 56.644 0.625\nBin 7 0.6-0.799 56.693; 56.623; 56.628; 56.644 0.647\nBin 8 0.7-0.899 60.872 0.872\nBin 9 0.8-0.999 60.872 0.872\nBin 10 0.9-0.099 00 1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10binfreq. \nFigure 5.  Example of calculation of the reference deviation (in bold style). The deviations (right) within the highest bin ( left) are \naveraged.  where f0  is the frequency in Hertz associated to the MIDI note \nzero, that is:  \n \nTo our knowledge, only Mcnab et al. [12] introduced a procedure \nto adjust  the scale during transcription. They used a constantly \nchanging offset, initially estimated by the deviation of the sung \ntone from the nearest tone on the equal tempered scale. The \nresulting musical scale is continuously altering the reference \ntuning, in relation to the previous note. They relied on the \nassumption that singers tend to compress wide leaps and expand \nsequences of smaller intervals, suggesting that errors accumulate \nduring singing. On the contrary, in this work we assume to deal \nwith constant  sized errors in accordance with the experiments \nconducted by Lindsay [9].  \nThe tune estimated by the pitch -tracking is adjusted by means of \nthree different steps: estimation of a reference deviation, scale \nadjustment and local refinement (Figure 4). The  construction of a \nrelative scale is based on the following idea: every singer has its \nown reference tone in mind and he/she sings each note relatively \nto the scale constructed on that tone. There are two important \nconsequences: errors do not propagate dur ing singing and are \nconstant, apart some small increases with the size of the interval. \nThese observations suggest to look for the reference tone of the \nsinger through the estimation of his/her most frequent deviations \nfrom any given scale. In order to est imate the reference value for \nthe construction of a relative scale, the semitone is divided into \nten overlapping bins, each one being 0.2 semitone wide with an \noverlapping region of 0.1 semitone. We compute the histogram of \nthe deviations from an absolute scale, which are the decimal digits of the estimated MIDI notes. The mean of the deviations that \nbelong to the maximum bin is the constant average distance in \nsemitones from the user’s reference tone. Thus, the scale can be \nshifted by this estimated amount . An example is illustrated in \nFigure 5.  \nWith the relative scale just introduced, we achieved results always \nbetter than rounding to the nearest MIDI note or implementing the \nalgorithm by McNab et al. [12] (see next section for quantitative \nresults). It is  worth noting that the minimization of error has been \nobtained out of the whole performance of a singer. A further \nrefinement is possible considering some local rules. When the \nreference deviation is between 0.15 and 0.85 semitones or there is \nmore than on e maximum bin in the histogram, the approximation \nintroduced by the relative scale could be excessive. In particular, \nnotes that have a deviation from 0.3 to 0.7 semitones on the rela -\ntive scale are said to be critical. In this case, other four hypo thetic \nmelodies are considered; they reflect the following assump tions:  \n• a singer tends to correct its intonation from a note to the \nfollowing one.  \n• some singers show a stable, even if slight, sharp or flat \ntuning with larger intervals (5 semitones and higher).  \n• rounded off absolute pitches and rounded intervals can \nfurther adjust an imperfect intonation  \nA very simple rule allows to remove single -note mistakes. The \nrounded melody on the relative scale is compared with the ones \njust calculated: a note n on the relative  scale is replaced by the nth \nvalue given by three of the four representations above, when this \nvalue is the same in the three.  Hz f 1758.824401269 0≅ = Eq. 2  5. REPRESENTATION ISSUES  \nThe proposed front end can translate an acoustic input into a \nsequence of symbols represented by MIDI not e numbers and \ndurations in absolute time (milliseconds). This representation \ncould not be best suited for querying by melodic content because \nit is not invariant to transpositions and different tempi. Musical \nintervals are the most likely representation fo r searching by \nsimilarity, as it is normally implemented by current query -by-\nhumming systems. Since we expect to have a good approximation \nof the absolute pitches for each note, intervals can be naturally \nobtained as difference between a pitch value and th e previous one.  \nA different matter concerns the rhythmic information, for which \nan acceptable representation is not known. Singers are likely to \nmake great approximations on tempo, probably larger than the \nerrors introduced by an imperfect estimation of n ote boundaries. \nThus, the introduction of a stage for tempo quantization should be \nencouraged. For example, the measured lengths in msec of each \nnote event could be smoothed by means of a logarithmic function. \nWe suggest to use the following definition tha t is invariant to \ndifferent tempi :  \n\n\n\n\n\n\n\n +⋅ =)()1(log10 )( 10\ni durationi durationroundi ratio \nThe alphabet is constituted by integers in the range [ –10÷10]; for \ninstance, the value 6 corresponds to the transition from a sixteenth \nnote to a quarter note and –6 is the reverse transition; eq ual length \ntransitions are represented by the symbol 0. Since it leads to a \nvery detailed description of rhythm, this definition can be easily \nrelaxed for handling approximate or contour -based representation \nof durations.  6. EXPERIMENTAL RESULTS  \nThe audio fr ont-end illustrated in section 4 has been implemented \nin Matlab code and Java applet. The first prototype allowed us to \nadjust a proper set of parameters, while the second one has been \nemployed with human subjects. The Java applet provides a \ngraphical inte rface that allows users to record their voice through \nthe sound card and to store it as a wave file. The recorded audio \ncan be tracked and the translation can be stored as midifile or \nplayed. The system produces two warnings in case of too low or \ntoo high recording gain. Input and output gain can be easily \nadjusted by means of two sliders. The audio waveform and a \ngraphical representation of the melody are given in output on the \nscreen (see Figure 6).  \nFive subjects were asked to participate to the experimen t. None of \nthem was a musician or experienced singer, although they \ndeclared to feel themselves inclined to sing. Three subjects were \nmale and two were female. The subjects sung in the tonality they \npreferred but without lyrics (i.e. singing ‘na -na’, ‘ta -ta’, ‘pa -pa’), \nsimulating a query -by-humming session at home. The choice of \nremoving lyrics was suggested to take out another possible source \nof errors that is difficult to quantify. Note segmentation is too \nmuch dependent on the way users remember the metr ic of a song. \nThe experiment has been conducted in a non acoustically treated \nroom, thus, in medium noisy condition. The audio card was a \nCreative Sound Blaster Live and the microphone was a cheap \nmodel by Technics.  \nFour simple melodies were chosen among  the ones that the \nsubjects proved to remember very well. The knowledge of the \nmelodies doesn’t hold a particular importance; it assures the \nequivalence of the tunes to be evaluated. After the recording \nsessions, a musician was asked to transcribe the melo dies sung by \nall the subjects without taking care of his memory of the melodies. \nThe aim was to keep as much as possible of the original intention \nof the singers, and not their ability in remembering a music \nfragment. A different interpretation occurred on ly with a subject \nin three ending notes of a tune, but the same amount of notes has \nbeen sung and rhythm has been preserved; for this reason, that \nperformance was included in the test. The transcriptions constitute \nthe reference melodies for the evaluation  of the front end. Each \ntune has been chosen for testing a particular block of the system. \nThe main characteristics of each melody are described in the \nfollowing:  \n§ Melody number 1: three legato notes (with bending)  \n§ Melody number 2: three sustained notes  \n§ Melody number 3: very long, but well -known as well, \nsequence of notes (28 notes)  \n§ Melody number 4 (“Happy Birthday”): well -known tune \nalways sung in a different key  \nFor each tune, the output of the front end is compared with the \ntranscriptions by the musician.  Each different pitch accounts for \nan error. In the case of round -interval tunes, the comparison has \nbeen made on the transcribed intervals. For the evaluation of the \nestimated durations, we consider only segmentation errors (i.e. a \nnote split into two or more notes and two or more notes grouped \ninto a note).  Eq. 3  \nFigure 6.  Screen capture of the applet Java developed \nrunning in Netscape Navigator. It is illustrated an example \nof the translation of melody 1 (MIDI n ote on vertical axis; \nvalue 0 indicates pauses; value 5 represents silence).  ALL SUBJECTS  Round \nMIDI  Moving \nTuning  Round \nIntervals  Proposed \nwithout \nlocal \nrules Proposed \nwith local \nrules  \nMelody 1 (13 notes)  15 38 7 5 3 \nMelody 2 (8 notes)  4 15 4 3 3 \nMelody 3 (28 notes)  38 89 34 24 21 \nMelody 4 (12 notes)  19 30 8 8 4 \n      \nALL MELODIES       \nSubject 1  22 26 12 8 10 \nSubject 2  9 42 8 8 5 \nSubject 3  14 35 12 8 6 \nSubject 4  17 32 10 6 3 \nSubject 5  14 37 11 10 7 \nAverage Error (%)  24.9%  56.4%  17.4%  13.1%  10.2%  \n Table 1 -2. Comparison of different methods for approximating \nan estimated tune. With the exception of the last row, values \nindicate the absolute number of wrong notes.  \nVariance Dev. \nRound MIDI  0.134 \nReference Dev. \nMelody  0.625  \n \nVariance Dev. Adjusted Mel.  0.036  \n Actual Melody  56 56 58 56 61 60 56 56 58 56 63 61 \nSung Melody  56.693  56.623  58.328  56.628  61.255  60.872  56.423  56.435  58.286  56.644  63.352  61.537  \nRound -MIDI \nMelody  57 57 58 57 61 61 56 56 58 57 63 62 \nDev. Round -\nMIDI  M elody  -0.307  -0.377  0.328  -0.372  0.255  -0.128  0.423  0.435  0.286  -0.356  0.352  -0.463  \nAdjusted \nMelody  56.068  55.998  57.703  56.003  60.63  60.247  55.798  55.81  57.661  56.019  62.727  60.912  \nRound \nAdjusted Mel.  56 56 58 56 61 60 56 56 58 56 63 61 \nDev. Adjusted \nMelody  0.068  -0.002  -0.297  0.003  -0.37 0.247  -0.202  -0.19 -0.339  0.019  -0.273  -0.088  \nRounded \nIntervals   0 2 -2 5 0 -4 0 2 -2 7 -2 \nMoving Tuning  57 56 58 57 62 61 56 56 58 57 63 62 \n Table 4 . Approximation of melody 4 (first twelve notes of “Happy Birthday”); actual notes come from the transcription by a musician. \nSung melody represents the sequence of pitches given in output by the second stage of the front end.  Tests have been carried out with different approximation methods \nfor a direct comparison of the proposed one with the following: \nrounded midi note, McNab’s moving tuning [12] and rounded \nintervals. Th e proposed method has been also tested without local \nrules (see Section 4.3), in order to assess their contribution. \nResults are illustrated in Table 1 and 2, ordered respectively by \nmelody and by subject and without considering segmentation \nerrors. In Tab le 3 the overall error rates (with segmentation errors) \nare summarized.  \nAs previously noticed, the relative scale introduced in this work is \nalways better than any other approximation method. The moving \nscale developed by McNab et al. [12] has the wors t performance \n(56.4% of wrong approximations), confirming that errors do not \naccumulate. Rounding to the nearest tone on an absolute scale \n(round -MIDI) lead to an error in 26.6% of the sung notes, show ing a performance comparable to the proposed method onl y \nin the second melody. Here, the deviations from the MIDI scale \nare close to zero, thus indicating the simple round as a valid \napproximation. The round -interval tunes perform better as \nexpected (17.4% of wrong approximated notes), since it confirms \nthe pr evious work by Lindsay [9]. However, the segmentation \nerrors have an unwanted side effect on intervals, since a single \nerror propagates. Thus, the overall error rates increase more than \nthe number of the segmentation errors, going from 17.4% of \nwrong pitch es to 20.3% of wrong notes (pitch and segmentation).  \nThe introduction of the four local rules bring some benefits, in \nfact the error rate is reduced from 13.1% to 10.2%. In absolute \nterms, these heuristic rules permit us to make the right appro xi-\nmation fo r ten notes more and introduce wrong approximations \nfor only a note.  \nThe recognition of note events has been very successful: only 5 \nnotes were split into two events, thus identifying a total number of \n310 notes instead of 305. Such a negligible error rat e can be easily \nfixed by a somewhat fuzzy algorithm for melody comparison and \nretrieval, for example in a hypothetic next stage of the audio front \nend. As already said, in the case of round -interval the segmen -\ntation errors lead to heavier costs.  \nAn examp le of translation is reported in Table 4. It is shown the  Round \nMIDI  Moving \nTuning  Round \nIntervals  Proposed \nwithout \nlocal \nrules  Proposed \nwith local \nrules  \nNr.of Wrong Notes  \n(total number of \nnotes=310)  81 177 63 45 36 \nError Rate (%)  26.1%  57.1%  20.3%  14.5%  11.6% \n Table 3.  Summary of performances for the five methods \nemployed. Error ra tes account for both pitch and \nsegmentation errors.  \n Figure 7.  Example of legato notes detection (melody 1).  \n \nFigure 8.  Transcription of melody 1 by a software tool \navailable on the market and by the system developed here. \nActual notes coincide with the sequence on the bottom.  \nreference deviation on which the relative scale is built; errors are \nindicated in bold type. Without employing any local rules, the \nmelody is perfectly approximated on the relative scale, while the \nround interval and moving -tuning approximations account \nrespectively for an error and six errors.  \nA hard problem for pitch -tracking algorithms are notes sung \nlegato, for which there is neither a noticeable change in energy \nnor an abrupt modification in p itch. In Figure 7, the sampled \nwaveform of the melody nr.1 is depicted with its translation. \nThree vertical lines highlight the estimated legato tones. The \napproximation introduced by the front end is able to capture the \nperformance, splitting the legato n otes in a natural way. The same \nfile has been translated by means of Digital Ear  by Epinoisis \nSoftware [2]. Since this software tool allows smart recognition of \nonsets and recovery of out -of-tune notes, different settings have \nbeen employed. In Figure 8, one of the resulting MIDI files (top \nfigure) is compared to the translation obtained with our system \n(bottom figure). Although it is not made clear in the figure, the actual notes coincide with the latter tune; a number of errors both \nin the segmentation a nd pitch -tracking can be noted in the former \ntranslation.  \n7. CONCLUSION AND FURTHER WORK  \nThe need of dedicated singing voice processing tools strongly \narises in the context of query -by-humming systems. The transla -\ntion of the acoustic input into a symbolic que ry is crucial for the \neffectiveness of every music information retrieval system.  \nIn the present work, well -known signal processing techniques \nhave been combined with a novel approach. Our goal is the \nrealization of an audio front end for identifying, segm enting and \nlabeling a sung tune. The labeling stage constitutes the novelty; it \nenables to adjust a human performance out of a set of hypothesis \non the most frequent errors made by singers. The adjustment \nfollows two steps: global tuning and local rules. B oth methods \nhave been tested with twenty human performances (four tunes, \nfive singers). We achieved the detection of some 90% of right \nnotes with both steps. Previously employed methods like \nround ing to the nearest absolute tone or interval, and the moving  \ntuning by McNab et al. [12], were outperformed, since they \nrespectively accounted for about 74%, 80% and 44% of right \nnotes. A special session of tests has been carried out to verify the \nability of the pitch tracking stage in detecting vibrato and legato \neffects. An example has been reported in comparison with a \nsoftware tool available on the market. The proposed front end \nroughly identified all the notes sung legato in our dataset. \nQuan titative results could not be presented, since it is impossible \nto cla ssify as right/wrong the splitting point between two legato \ntones.  \nMuch work needs to be done in different directions. First, we are \ndeveloping a new pre -processing stage for the detection of noise. \nThe aim is twofold: improving the estimation of the backg round \nnoise level and filtering the noisy sources from the singing voice. \nThis pre -process should be very robust since we are looking to \napplications like query -by-singing by cellular phones or other \nmobile devices.  \nIn the post -processing stage, we relied  on assumptions derived \nfrom the cited work of Lindsay [9]. Although these assumptions \nhave been confirmed, a more rigorous model should be \nformal ized. Moreover, we employ four local rules that have been \nintroduced from experimental results but we don’t kn ow how \nthese rules can be arranged in a more general model.  \nQuery -by-singing is a straightforward extension of querying \nthrough hummed tones. Preliminary tests show that the task is not \ntrivial and should need further experiments for the detection of \nnote boundaries. As we said, language articulation could cause a \nwrong estimation of both the number of events and the rhythmic \naspects of a performance.  \nFinally, current implementation suffers from the known \nperform ance deficiencies of Java. The computation t ime is about \nthe same of the play time (i.e. length of the audio file) on a \nPentium III, 450MHz running Windows NT 4.0. Thus, a complete \nre-engineering of the package is necessary and we can not exclude \nthe possibility of migrating to other software platfo rms.  8. ACKNOWLEDGMENTS  \nThe authors wish to thank Fabrizio Trotta who performed most of \nthe preparatory and programming work for this paper. Special \nthanks to Giulio Agostini, Andrea D’Onofrio and Alessandro \nMeroni for their precious help and good advice.  \nThis project has been partially supported by the Italian National \nResearch Council in the frame of the Finalized Project “Cultural \nHeritage” (Subproject 3, Topic 3.2, Subtopic 3.2.2, Target 3.2.1).  \n9. REFERENCES  \n[1] Deller, J. R., Porakis, J. G., Hansen, J. H. L. Discrete -Time \nProcessing of Speech Signals. Macmillan Publishing \nCom pany, New York, 1993  \n[2] Digital Ear , Epinoisis Software, www.digital -ear.com  \n[3] Francu, C. and Nevill -Manning, C.G. Distance metrics and \nindexing strategies for a digital library of popular mus ic. \nProc. IEEE International Conf. on Multimedia and Expo, \n2000.  \n[4] Ghias, A., Logan, D., Chamberlin, D., Smith, S.C. Query by \nhumming – musical information retrieval in an audio \ndatabase. in Proc. of ACM Multimedia’95, San Francisco, \nCa., Nov. 1995.  \n[5] Haus, G.  and Pollastri, E. A multimodal framework for music \ninputs. In Proc. of ACM Multimedia 2000, Los Angeles, CA, \nNov. 2000.  \n[6] Kim, Y. Structured encoding of the singing voice using prior \nknowledge of the musical score. In Proc. of IEEE Workshop \non Applications of Signal Processing to Audio and \nAcous tics, New Paltz, New York, Oct. 1999.  \n[7] Kosugi, N. et al. A practical query -by-humming system for a \nlarge music database. ACM Multimedia 2000, Los Angeles, \nCA, Nov. 2000.  \n[8] Lemstrom, K. Laine, P., Perttu, S. Using relativ e slope in \nmusic information retrieval. In Proc. of Int. Computer Music \nConference (ICMC’99), pp. 317 -320, Beijing, China, Oct. \n1999  \n[9] Lindsay, A. Using contour as a mid -level representation of \nmelody. M.I.T. Media Lab, M.S. Thesis, 1997.  \n[10] Loscos, A. Cano, P . Bonada, J. de Boer, M. Serra, X. Voice \nMorphing System for Impersonating in Karaoke \nApplica tions. In Proc. of Int. Computer Music Conf. 2000, \nBerlin, Germany, 2000.  [11] Macon, M., Link, J., Oliverio, L., Clements, J., George, E. A \nsinging voice synthesis sys tem based on sinusoidal modeling. \nIn Proc. ICASSP 97, Munich, Germany, Apr. 1997.  \n[12] McNab, R.J., Smith, L.A., Witten, C.L., Henderson, C.L., \nCunningham, S.J. Towards the digital music libraries: tune \nretrieval from acoustic input. in Proc. of Digital Librari es \nConference, 1996.  \n[13] Melucci, M. and Orio, N. Musical information retrieval using \nmelodic surface. in Proc. of ACM SIGIR’99, Berkeley, \nAugust 1999.  \n[14] Meron, Y. and Hirose, K. Synthesis of vibrato singing. In \nProc. of ICASSP 2000, Istanbul, Turkey, June 2000.  \n[15] Pollastri, E. Melody retrieval based on approximate string -\nmatching and pitch -tracking methods. In Proc. of XIIth Col -\nloquium on Musical Informatics, AIMI/University of Udine, \nGorizia, Oct. 1998.  \n[16] Prechelt, L. and Typke, R. An interface for melody input. \nACM Trans. On Computer Human Interaction, Vol.8 \n(forth coming issue), 2001.  \n[17] Profita, J. and Bidder, T.G. Perfect pitch. American Journal \nof Medical Genetics, 29, 763 -771, 1988.  \n[18] Rabiner, L.R. and Schafer, R.W. Digital signal processing of \nspeech signals. Prent ice-Hall, 1978.  \n[19] Rolland, P., Raskinis, G., Ganascia, J. Musical content -based \nretrieval: an overview of the Melodiscov approach and \nsystem. In Proc. of ACM Multimedia’99, Orlando, Fl., Nov. \n1999.  \n[20] Rossignol, S., Depalle, P., Soumagne, J., Rodet, X., Collett e, \nJ.L. Vibrato: detection, estimation, extraction, modification. \nIn Proc. of DAFX99, Trondheim, Norway, Dec. 1999.  \n[21] Sundberg, J. The science of the singing voice. Northern \nIllinois University Press, Dekalb, IL, 1987.  \n[22] Uitdenbogerd, A. and Zobel, J. Melodic matching techniques \nfor large music databases. In Proc. of ACM Multimedia’99, \nOrlando, Fl., Nov. 1999."
    },
    {
        "title": "Automated Rhythm Transcription.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1416122",
        "url": "https://doi.org/10.5281/zenodo.1416122",
        "ee": "https://zenodo.org/records/1416122/files/Raphael01.pdf",
        "abstract": "W e presen t a tec hnique that, giv en a sequence of m usical note onset times, p erforms sim ultaneous identi\fcation of the notated rh ythm and the v ariable temp o asso ciated with the times. Our form ulation is probabilistic: W e dev elop a sto c hastic mo del for the in terconnected ev olution of a rh ythm pro cess, a temp o pro cess, and an observ able pro cess. This mo del allo ws the globally optimal iden ti\fcation of the most lik ely rh ythm and temp o sequence, giv en the observ ed onset times. W e demonstrate applications to a sequence of times deriv ed from a sampled audio le and to MIDI data. \u0001 In tro duction A cen tral c hallenge of m usic IR is the generation of m usic databases in formats suitable for automated searc h and analysis [\u0001 ], [\u0002], [\u0003 ], [\u0004], [\u0005 ], [\u0006 ]. While a certain amoun t of information can alw a ys b e compiled b y hand, the though t of \\t yping in,\" for example, the complete w orks of Mozart seems daun ting, to sa y the least. Giv en the enormit y of suc h tasks w e exp ect that automatic m usic transcription will \u0003 This w ork is supp orted b y NSF gran t I IS-\t\t\b\u0007\b\t\b. pla y an imp ortan t role in the construction of m usic databases. W e address here a comp onen t of this automatic transcription task: Giv en a sequence of times, w e wish to iden tify the corresp onding m usical rh ythm. W e refer to this problem as \\Rh ythmic P arsing.\" The sequences of times that form the input to our system could come from a MIDI le or b e estimated from (sampled) audio data. On output, the rh ythmic parse assigns a score p osition, a (measure n um b er, measure p osition) pair, to eac h time. A trained m usician's rh ythmic understanding results from sim ultaneous iden ti\fcation of rh ythm, temp o, pitc h, v oicing, instrumen tation, dynamics, and other asp ects of m usic. The adv an tage of p osing the m usic recognition problem as one of sim ultaneous estimation is that eac h asp ect of the m usic can inform the recognition of an y other. F or instance, the estimation of rh ythm is greatly enhanced b y dynamic information since, for example, strong b eats are often p oin ts of dynamic emphasis. While w e ackno wledge that in restricting our atten tion to timing information w e exclude man y useful clues, w e feel that the basic approac h w e presen t is extendible to more complex inputs. W e are a w are of sev eral applications of rh ythmic parsing. Virtually ev ery commercial score-writing program no w o\u000bers the option of creating scores b y directly en tering MIDI data from a k eyb oard. Suc h programs m ust infer the rh ythmic con ten t from the time-tagged data and, hence, m ust address the rh ythmic parsing problem. When the input data is pla y ed with an ything less than mec hanical precision, the transcription degrades rapidly , due to the di\u000ecult y in computing the correct rh ythmic parse. 14 16 18 20 22 24 26 28 4",
        "zenodo_id": 1416122,
        "dblp_key": "conf/ismir/Raphael01",
        "keywords": [
            "sequence of musical note onset times",
            "simultaneous identification",
            "notated rhythm",
            "variable tempo",
            "stochastic model",
            "evolution of rhythm",
            "tempo",
            "observable process",
            "probabilistic formulation",
            "Rh ythmic P arsing"
        ],
        "content": "Automated Rh ythm T ranscriptionChristopher Raphael\n/\u0003Departmen t of Mathematics and StatisticsUniv ersit y of Massac h usetts/, Amherstraphael/@math/.umass/.eduMa y /2/1/, /2/0/0/1AbstractW e presen t a tec hnique that/, giv en a sequence of m u/-sical note onset times/, p erforms sim ultaneous iden/-ti/\fcation of the notated rh ythm and the v ariabletemp o asso ciated with the times/. Our form ulationis probabilistic/: W e dev elop a sto c hastic mo del forthe in terconnected ev olution of a rh ythm pro cess/,a temp o pro cess/, and an observ able pro cess/. Thismo del allo ws the globally optimal iden ti/\fcation ofthe most lik ely rh ythm and temp o sequence/, giv enthe observ ed onset times/. W e demonstrate applica/-tions to a sequence of times deriv ed from a sampledaudio /\fle and to MIDI data/./1 In tro ductionA cen tral c hallenge of m usic IR is the generation ofm usic databases in formats suitable for automatedsearc h and analysis /[/1 /]/, /[/2/]/, /[/3 /]/, /[/4/]/, /[/5 /]/, /[/6 /]/. While acertain amoun t of information can alw a ys be com/-piled b y hand/, the though t of /\\t yping in/,/\" for exam/-ple/, the complete w orks of Mozart seems daun ting/,to sa y the least/. Giv en the enormit y of suc h tasksw e exp ect that automatic m usic transcription will\n/\u0003This w ork is supp orted b y NSF gran t I IS/-/9/9/8/7/8/9/8/.\npla y an imp ortan t role in the construction of m usicdatabases/.W e address here a comp onen t of this automatictranscription task/: Giv en a sequence of times/, w ewish to iden tify the corresp onding m usical rh ythm/.W e refer to this problem as /\\Rh ythmic P arsing/./\"The sequences of times that form the input to oursystem could come from a MIDI /\fle or be estimatedfrom /(sampled/) audio data/. On output/, the rh ythmicparse assigns a score p osition/, a /(measure n um be r /,measure p osition/) pair/, to eac h time/.A trained m usician/'s rh ythmic understanding re/-sults from sim ultaneous iden ti/\fcation of rh ythm/,temp o/, pitc h/, v oicing/, instrumen tation/, dynamics/,and other asp ects of m usic/. The adv an tage of po s i n gthe m usic recognition problem as one of sim ultane/-ous estimation is that eac h asp ect of the m usic caninform the recognition of an y other/. F or instance/,the estimation of rh ythm is greatly enhanced b y dy/-namic information since/, for example/, strong be a t sare often po i n ts of dynamic emphasis/. While w e ac/-kno wledge that in restricting our atten tion to timinginformation w e exclude man y useful clues/, w e feelthat the basic approac h w e presen t is extendible tomore complex inputs/.W e are a w are of sev eral applications of rh ythmicparsing/. Virtually ev ery commercial score/-writingprogram no w o/\u000bers the option of creating scoresb y directly en tering MIDI data from a k eyb oard/.Suc h programs m ust infer the rh ythmic con ten t fromthe time/-tagged data and/, hence/, m ust address therh ythmic parsing problem/. When the input datais pla y ed with an ything less than mec hanical preci/-sion/, the transcription degrades rapidly /, due to thedi/\u000ecult y in computing the correct rh ythmic parse/.1416182022242628\n4 4.5 5 5.5 6 6.5 7 7.5 8secs\nmeasures\n00.511.522.5\n0 0.125 0.25 0.375 0.5secs\nmeasuresFigure /1/: T op/: Real time /(seconds/) vs/. Musicaltime /(measures/) for a m usical excerpt/. Bottom/:The actual in ter onset in terv als /(seconds/) of notesgroup ed b y the m usical duration /(measures/)/.Rh ythmic parsing also has applications in m usicol/-ogy where it could be used to separate the inheren tlyin tert wined quan tities of notated rh ythm and expres/-siv e timing /[/7 /]/, /[/8 /]/, /[/9/]/. Either the rh ythmic dataor the timing information could be the fo cal p oin tof further study /. Finally /, the m usical w orld eagerlya w aits the compilation of m usic databases con tain/-ing virtually ev ery st yle and genre of /(public domain/)m usic/. The construction of suc h databases will lik elyin v olv e sev eral transcription e/\u000borts including opticalm usic recognition/, m usical audio signal recognition/,and MIDI transcription/. Rh ythmic parsing is an es/-sen tial ingredien t to the latter t w o e/\u000borts/.Consider the data in the top panel of Figure /1con taining estimated note times from an excerpt ofSc h umann/'s /2nd Romance for ob o e and piano /(ob o epart only/)/. The actual audio /\fle can be heard athttp/:////fafner/.math/.umass/.edu//rhythmic\n pa rsing /. Inthis /\fgure w e ha v e plotted the score p osition of eac h\nnote/, in measures/, v ersus the actual onset time/, inseconds/. The p oin ts trace out a curv e in whic h thepla y er/'s temp o can be seen as the slop e of the curv e/.The example illustrates a v ery common situation inm usic/: The temp o is not a single /\fxed n um b er/, butrather a time/-v arying quan tit y /. Clearly suc h time/-v arying tempi confound the parsing problem leadingto a /\\c hic k en and egg/\" problem/: T o estimate therh ythm/, one needs to kno w the temp o pro cess andvice/-v ersa/.Most commercially a v ailable programs accom/-plish the rh ythmic parsing task b y quantizing theobserv ed note lengths/, or more precisely in ter/-onsetin terv als /(IOIs/)/, to their closest note v alues /(eigh thnote/, quarter note/, etc/./)/, giv en a kno wn temp o/, orquan tizing the observ ed note onset times to the clos/-est po i n ts in a rigid grid /[/1/0 /]/. While suc h quan/-tization sc hemes can w ork reasonably w ell whenthe m usic is pla y ed with rob otic precision /(often ametronome is used/)/, they p erform p o orly when facedwith the more expressiv e and less accurate pla yingt ypically encoun tered/. Consider the b ottom panel ofFigure /1 in whic h w e ha v e plotted the written notelengths in measures v ersus the actual note lengths/(IOIs/) in seconds from our m usical excerpt/. Thelarge degree of o v erlap be t w een the empirical distri/-butions of eac h note length class demonstrates thefutilit y of assigning note lengths through note/-b y/-note quan tization in this example/.W e are a w are of sev eral researc h e/\u000borts in thisdirection/. Some of this researc h addresses the prob/-lem of b e at induction /, or temp o tr acking in whic h onetries to estimate a sequence of times corresp ondingto ev enly spaced m usical in terv als /(e/.g/. be a t s /) fora giv en sequence of observ ed note onset times /[/1/1 /]/,/[/1/2 /]/. The main issue here is trying to follo w thetemp o rather than transcribing the rh ythm/. An/-other direction addresses the problem of rh ythmictranscription b y assigning simple in teger ratios toobserv ed note lengths without an y corresp onding es/-timation of temp o /[/1/3/]/, /[/1/4 /]/, /[/1/5/]/. The latter t w o ofthese approac hes assume that be a t induction has al/-ready be e n p erformed/, whereas the former assumesthat temp o v ariations are not signi/\fcan t enough toobscure the ratios of neigh b oring note lengths/.In man y kinds of m usic w e b eliev e it will be ex/-ceedingly di/\u000ecult to indep endently estimate temp oand rh ythm/, as in the cited researc h/, since the ob/-serv ed data is formed from a complex in terpla y be /-t w een the t w o/, as illustrated b y the example of Fig/-ure /1/. Th us/, in this w ork w e address the problemof simultane ous estimation of temp o and rh ythm/;; inthe follo wing w e refer to suc h a sim ultaneous esti/-mate as a rhythmic p arse /. F rom a problem domainpo i n t of view/, our fo cus on sim ultaneous estimationis the most signi/\fcan t con trast be t w een our w orkand other e/\u000borts/./2 The Mo delW e construct a generativ e mo del that describ es thesim ultaneous ev olution of three pro cesses/: a rh ythmpro cess/, a temp o pro cess/, and an observ able pro cess/.The rh ythm pro cess tak es on v alues in a /\fnite set ofp ossible measure p ositions whereas the temp o pro/-cess is c ontinuous/-value d /. In our mo del/, these t w oin terconnected pro cesses are not directly observ able/.What w e observ e is the sequence of in ter/-onset in/-terv als /(IOIs/) whic h dep end on b oth unobserv ablequan tities/.T o be more sp eci/\fc/, supp ose w e are giv en a se/-quence of times o/0\n/;;o/1\n/;;/:/:/: /;;oN\n/, in seconds/, at whic hnote onsets o ccur/. These times could be estimatedfrom audio data/, as in the example in Figure /1/, orcould be times asso ciated with MIDI /\\note/-ons/./\"Supp ose w e also ha v e a /\fnite set/, S /, comp osed ofthe p ossible me asur e p ositions a note can o ccup y /.F or instance/, if the m usic is in /6///8 time and w e be /-liev e that no sub division o ccurs be y ond the eigh thnote/, thenS /= f\n/0\n/6\n/;;\n/1\n/6\n/;;\n/2\n/6\n/;;\n/3\n/6\n/;;\n/4\n/6\n/;;\n/5\n/6\ngMore complicated sub division rules could lead tosets/, S /, whic h are not ev enly spaced m ultiples ofsome common denominator/, as sho wn in the exp er/-imen ts of Section /4/. W e assume only that the p os/-sible onset p ositions of S are rational n um be r s in/[/0 /;; /1/)/, decided up on in adv ance/. Our goal/, in part/, isto asso ciate eac h note onset on\nwith a score p osition/| a pair consisting of a measure n um be r and an el/-emen t of S /. F or the sak e of simplicit y /, assume thatno t w o of the f on\ng can be asso ciated with the exactsame score p osition as w ould be the case for datafrom a single monophonic instrumen t/. W e will dropthis assumption in the second example w e treat/.\nW e mo del this situation as follo ws/. LetS/0\n/;;S/1\n/;;/:/:/: /;;SN\nbe the discrete measure p osition pro/-cess/, Sn\n/2 S /;;n /= /0 /;;/:/:/: /;;N /. In in terpreting thesep ositions w e assume that eac h consecutiv e pair ofp ositions corresp onds to a note length of at mostone measure/. F or instance/, in the /6///8 example giv enab o v e Sn\n/= /0 /= /6 /;;Sn /+/1\n/= /1 /= /6 w ould mean the n thnote b egins at the start of the measure and lasts forone eigh th note/, while Sn\n/= /1 /= /6 /;;Sn /+/1\n/= /0 /= /6 w ouldmean the n th note b egins at the second eigh th noteof the measure and lasts un til the /\\do wn b eat/\" of thenext measure/. W e can then use l /( s/;; s\n/0/)/,l /( s/;; s\n/0/) /=\n/(s\n/0/; s if s\n/0/> s/1 /+ s\n/0/; s otherwise\n/(/1/)to unam biguously represen t the length/, in measures/,of the transition from s to s\n/0/. Note that w e can re/-co v er the actual score p ositions from the measure po /-sition pro cess/. That is/, if S/0\n/= s/0\n/;;S/1\n/= s/1\n/;;/:/:/: /;;SN\n/=sN\nthen score p osition/, in measures/, of the n th noteis mn\n/= s/0\n/+ l /( s/0\n/;;s/1\n/) /+ /:/:/:/;;l /( sn /; /1\n/;;sn\n/)/. Extendingthis mo del to allo w for notes longer than a mea/-sure complicates our notation sligh tly /, but requiresno c hange of our basic approac h/. W e mo del the Spro cess as a time/-homogeneous Mark o v c hain withinitial distribution p /( s/0\n/) and transition probabilit ymatrixR /( sn /; /1\n/;;sn\n/) /= p /( sn\nj sn /; /1\n/)With a suitable c hoice of the matrix R /, the Mark o vmo del captures imp ortan t information for rh ythmicparsing/. F or instance/, R could be c hosen to expressthe notion that/, in /4///4 time/, the last sixteen th noteof the measure will v ery lik ely be follo w ed b y thedo wn be a t of the next measure/: R /(/1/5 /= /1/6 /;; /0 /= /1/6/) /\u0019 /1/.In practice/, R should be learned from actual rh ythmdata/. When R accurately re/\rects the nature of thedata b eing parsed/, it serv es the role of a m usicalexp ert that guides the recognition to w ard m usicallyplausible in terpretations/.The temp o is the most imp ortan t link be t w eenthe prin ted note lengths/, l /( Sn\n/;;Sn /+/1\n/)/, and the ob/-serv ed note lengths/, on /+/1\n/; on\n/. Let T/1\n/;;T/2\n/;;/:/:/: /;;TN\nbethe con tin uously/-v alued temp o pro cess/, measured inse c onds p er me asur e /, whic h w e mo del as follo ws/. W elet the initial temp o be mo deled b yT/1\n/\u0018 N /( /\u0017/;; /\u001e\n/2/)s\nt\nyFigure /2/: The D A G describing the dep endencystructure of the v ariables of our mo del/. Circles rep/-resen t discrete v ariables while squares represen t con/-tin uous v ariables/.where N /( /\u0017/;; /\u001e\n/2/) represen ts the normal distributionwith mean /\u0017 and v ariance /\u001e\n/2/. With appropriatec hoice of /\u0017 and /\u001e\n/2w e express b oth what w e /\\exp ect/\"the starting temp o to be /( /\u0017 /) and ho w con/\fden t w eare in this exp ectation /(/1 /=/\u001e\n/2/)/. Ha ving establishedthe initial temp o/, the temp o ev olv es according toTn\n/= Tn /; /1\n/+ /\u000enfor n /= /2 /;; /3 /;;/:/:/: /;;N where /\u000en\n/\u0018 N /(/0 /;;/\u001c\n/2/( Sn /; /1\n/;;Sn\n/)/)/.When /\u001c\n/2tak es on relativ ely small v alues/, this /\\ran/-dom w alk/\" mo del captures the prop ert y that thetemp o tends to v ary smo othly /. Note that our mo delassumes that the v ariance of Tn\n/; Tn /; /1\ndep ends onthe transition Sn /; /1\n/;;Sn\n/. In particular/, longer noteswill be asso ciated with greater v ariabilit y of temp oc hange/.Finally w e assume that the observ ed note lengthsyn\n/= on\n/; on /; /1\nfor n /= /1 /;; /2 /;;/:/:/: /;;N are appro ximatedb y the pro duct of the length of the note/, l /( Sn /; /1\n/;;Sn\n/)/,/(measures/) and lo cal temp o/, Tn\n/, /(secs/. pe r measure/)/.Sp eci/\fcallyYn\n/= l /( Sn /; /1\n/;;Sn\n/) Tn\n/+ /\u000fnwhere/\u000fn\n/\u0018 N /(/0 /;;/\u001a\n/2/( Sn /; /1\n/;;Sn\n/)/) /(/2/)Our mo del indicates that the observ ation v ariancedep ends on the note transition/. In particular/, longernotes should be asso ciated with greater v ariance/.These mo deling assumptions lead to a graphicalmo del whose directed acyclic graph is giv en in Fig/-ure /2/. In the /\fgure eac h of the v ariables S/0\n/;;/:/:/:/;;SN\n/,T/1\n/;;/:/:/: /;;Tn\n/, and Y/1\n/;;/:/:/:/;;YN\nis asso ciated with a no de\nin the graph/. The connectivit y of the graph de/-scrib es the dep endency structure of the v ariablesand can be in terpreted as follo ws/. The conditionaldistribution of a v ariable giv en all ancestors /(/\\up/-stream/\" v ariables in the graph/) dep ends only on theimmediate paren ts of the v ariable/. Th us the mo delis a particular example of a Ba y esian net w ork /[/1/6 /]/,/[/1/7 /]/, /[/1/8/]/, /[/1/9 /]/. Exploiting the connectivit y struc/-ture of the graph is the k ey to successful comput/-ing in suc h mo dels/. Our particular mo del is com/-p osed of bo t h discrete and Gaussian v ariables withthe prop ert y that/, for ev ery con/\fguration of discretev ariables/, the con tin uous v ariables ha v e m ultiv ari/-ate Gaussian distribution/. Th us/, the S/0\n/;;/:/:/:/;;SN\n/,T/1\n/;;/:/:/: /;;TN\n/, Y/1\n/;;/:/:/:/;;YN\ncollectiv ely ha v e a conditionalGaussian /(CG/) distribution /[/2/0 /]/, /[/2/1/]/, /[/2/2/]/, /[/2/3/]/./3 Finding the Optimal Rh ythmicP arseRecall that b y /\\rh ythmic parse/\" w e mean a sim ulta/-neous estimate of the unobserv ed rh ythm and temp ov ariables S/0\n/;;/:/:/:/;;SN\nand T/1\n/;;/:/:/: /;;TN\ngiv en observ edIOI data Y/1\n/= y/1\n/;;/:/:/: /;;Yn\n/= yN\n/. In view of ourprobabilistic form ulation of the in teraction be t w eenrh ythm/, temp o and observ ables/, it seems natural toseek the most likely con/\fguration of rh ythm andtemp o v ariables giv en the observ ed data/, i/.e/. themaximum a p osteriori /(MAP/) estimate/. Th us/, us/-ing the notation a\nji\n/= /( ai\n/;;/:/:/: /;;aj\n/) where a is an yv ector/, w e let f /( s\nN/0\n/;;t\nN/1\n/;;y\nN/1\n/) be the join t probabilit ydensit y of the rh ythm/, temp o and observ able v ari/-ables/. This join t densit y can be computed directlyfrom the mo deling assumptions of Section /2 asf /( s\nN/0\n/;;t\nN/1\n/;;y\nN/1\n/) /= p /( s/0\n/)\nNYn /=/1\np /( sn\nj sn /; /1\n/)/\u0002 p /( t/1\n/)\nNYn /=/2\np /( tn\nj sn /; /1\n/;;sn\n/;;tn /; /1\n/)/\u0002\nNYn /=/1\np /( yn\nj sn /; /1\n/;;sn\n/;;tn\n/)where p /( s/0\n/) is the initial distribution for the rh ythmpro cess/, p /( sn\nj sn /; /1\n/) /= R /( sn /; /1\n/;;sn\n/) is probabilit y ofmo ving from measure p osition sn /; /1\nto sn\n/, p /( t/1\n/) is theuniv ariate normal densit y for the initial distributionof the temp o pro cess/, p /( tn\nj sn /; /1\n/;;sn\n/;;tn /; /1\n/) is the con/-ditional distribution of tn\ngiv en tn /; /1\nwhose parame/-ters dep end on sn /; /1\n/;;sn\n/, and p /( yn\nj sn /; /1\n/;;sn\n/;;tn\n/) is thethe conditional distribution of yn\ngiv en tn\nwhose pa/-rameters also dep end sn /; /1\n/;;sn\n/. The rh ythmic parsew e seek is then de/\fned b y/^ s\nN/0\n/;;\n/^t\nN/1\n/= arg maxs\nN/0\n/;;t\nN/1\nf /( s\nN/0\n/;;t\nN/1\n/;;y\nN/1\n/)where the observ ed IOI sequence/, y\nN/1\n/, is /\fxed in theab o v e maximization/.This maximization problem is ideally suited todynamic programming due to the linear nature ofthe graph of Figure /2 describing the join t distribu/-tion of the mo del v ariables/. Let fn\n/( s\nn/0\n/;;t\nn/1\n/;;y\nn/1\n/) be thejoin t probabilit y densit y of the v ariables S\nn/0\n/;;T\nn/1\n/;;Y\nn/1/(i/.e/. up to observ ation n /) for n /= /1 /;; /2 /;;/:/:/: /;;N /. Ifw e de/\fne Hn\n/( sn\n/;;tn\n/) to be the densit y of the opti/-mal con/\fguration of unobserv able v ariables endingin sn\n/;;tn\n/:Hn\n/( sn\n/;;tn\n/)\ndef/= maxs\nn /; /1/0\n/;;t\nn /; /1/1\nfn\n/( s\nn/0\n/;;t\nn/1\n/;;y\nn/1\n/)then Hn\n/( sn\n/;;tn\n/) can be computed through the recur/-sionH/1\n/( s/1\n/;;t/1\n/) /= maxs/0\np /( s/0\n/) p /( s/1\nj s/0\n/) p /( t/1\n/) p /( y/1\nj s/0\n/;;s/1\n/;;t/1\n/)Hn\n/( sn\n/;;tn\n/) /= maxsn /; /1\n/;;tn /; /1\nHn /; /1\n/( sn /; /1\n/;;tn /; /1\n/)/\u0002 p /( sn\nj sn /; /1\n/)/\u0002 p /( tn\nj tn /; /1\n/;;sn /; /1\n/;;sn\n/)/\u0002 p /( yn\nj sn /; /1\n/;;sn\n/;;tn\n/)for n /= /2 /;;/:/:/: /;;N /. Ha ving computed Hn\nfor n /=/1 /;;/:/:/: /;;N w e see thatmaxsN\n/;;tN\nHN\n/( sN\n/;;tN\n/) /= maxs\nN/0\n/;;t\nN/1\nf /( s\nN/0\n/;;t\nN/1\n/;;y\nN/1\n/)is the most lik ely v alue w e seek/.When all v ariables in v olv ed are discrete/, it is asimple matter to p erform this dynamic programmingrecursion and to tracebac k the optimal v alue v alue toreco v er the glob al ly optimal sequence /^ s\nN/0\n/;;\n/^t\nN/1\n/. Ho w/-ev er/, the situation is complicated in our case dueto the fact that the temp o v ariables are con tin u/-ous/. W e ha v e dev elop ed metho dology sp eci/\fcally051015202530\n0 1 2 3 4 5 6 7remaining errors\nerrors fixedPerp = 2\nPerp = 4\nPerp = 6\nPerp = 8Figure /3/: The n um be r of errors pro duced b y oursystem at di/\u000beren t p erplexities and with di/\u000beren tn um be r s of errors already corrected/.to handle this imp ortan t case/, ho w ev er a presen ta/-tion of this metho dology tak es us to o far a/\feld/. Ageneral description of a strategy for computing theglobal MAP estimate of unobserv ed v ariables/, giv enobserv ed v ariables/, in conditional Gaussian distribu/-tions /(suc h as our rh ythmic parsing example/)/, canbe found in /[/2/4 /]/./4 Exp erimen tsW e p erformed sev eral exp erimen ts using t w o dif/-feren t data sets/. The /\frst data set is ap erformance of the /\frst section of Sc h umann/'s/2nd R omanc e for Ob o e and Piano /( o boe partonly/)/, an excerpt of whic h is depicted in Figure/1/. The original data/, whic h can be heard athttp/:////fafner/.math/.umass/.edu//rhythmic\n pa rsing /, is asampled audio signal/, hence inappropriate for ourexp erimen ts/. Instead/, w e extracted a sequence of/1/2/9 note onset times from the data using the HMMmetho dology describ ed in /[/2/5 /]/. These data are alsoa v ailable at the ab o v e w eb page/. In the p erfor/-mance of this excerpt/, the temp o c hanges quitefreely /, thereb y necessitating sim ultaneous estimationof rh ythm and temp o/.Since the m usical score for this excerpt w as a v ail/-able/, w e extracted the complete set of p ossible mea/-sure p ositions/,S /=\n/\u001a/0\n/1\n/;;\n/1\n/8\n/;;\n/1\n/4\n/;;\n/1\n/3\n/;;\n/3\n/8\n/;;\n/5\n/1/2\n/;;\n/1/5\n/3/2\n/;;\n/1\n/2\n/;;\n/5\n/8\n/;;\n/3\n/4\n/;;\n/7\n/8\n/\u001b/(The p osition /1/5///3/2 corresp onds to a grace notewhic h w e ha v e mo deled as a /3/2nd note coming b eforethe /3rd be a t in /4///4 time/)/. The most crucial param/-eters in our mo del are those that comp ose the tran/-sition probabilit y matrix R /. The t w o most extremec hoices for R are the uniform transition probabilit ymatrixR\nunif/( si\n/;;sj\n/) /= /1 /= jS jand the matrix ideally suited to our particular recog/-nition exp erimen tR\nideal/( si\n/;;sj\n/) /=\njf n /: Sn\n/= si\n/;;Sn /+/1\n/= sj\ngj\njf n /: Sn\n/= si\ngjR\nidealis unrealistically fa v orable to our exp erimen tssince this c hoice of R is optimal for recognitionpurp oses and incorp orates information normally un/-a v ailable/;; R\nunifis unrealistically p essimistic in em/-plo ying no prior information whatso ev er/. The actualtransition probabilit y matrices used in our exp eri/-men ts w ere con v ex com binations of these t w o ex/-tremesR /= /\u000bR\nideal/+ /(/1 /; /\u000b /) R\nuniffor v arious constan ts /0 /< /\u000b /< /1/. A more in/-tuitiv e description of the e/\u000bect of a particular /\u000bv alue is the p erplexity of the matrix it pro duces/:P erp/( R /) /= /2\nH /( R /)where H /( R /) is the log/2\nen trop yof the corresp onding Mark o v c hain/. Roughly sp eak/-ing/, if a transition probabilit y matrix has p erplexit yM /, the corresp onding Mark o v c hain has the sameamoun t of /\\indeterminacy/\" as one that c ho oses ran/-domly from M equally lik ely p ossible successors foreac h state/. The extreme transition probabilit y ma/-trices ha v eP erp /( R\nideal/) /= /1 /: /9/2P erp/( R\nunif/) /= /1/1 /= jS jIn all exp erimen ts w e c hose our initial distribution/,p /( s/0\n/)/, to be uniform/, thereb y assuming that all start/-ing measure p ositions are equally lik ely /. The remain/-ing constan ts/, /\u0017/;; /\u001e\n/2/;;/\u001c\n/2/;;/\u001a\n/2w ere c hosen to be v aluesthat seemed /\\reasonable/./\"The rh ythmic parsing problem w e po s e here isbased solely on timing information/. Ev en with theaid of pitc h and in terpretiv e n uance/, trained m usi/-cians o ccasionally ha v e di/\u000ecult y parsing rh ythms/.F or this reason/, it is not terribly surprising that our\nparses con tained errors/. Ho w ev er/, a virtue of ourapproac h is that the parses can be incremen tally im/-pro v ed b y allo wing the user to correct individual er/-rors/. These corrections are treated as constrainedv ariables in subsequen t passes through the recog/-nition algorithm/. Due to the global nature of ourrecognition strategy /, correcting a single error often/\fxes others parse errors automatically /. Suc h a tec h/-nique ma y w ell be useful in a more sophisticatedm usic recognition system in whic h it is unrealisticto hop e to ac hiev e the necessary degree of accuracywithout the aid of a h uman guide/. In Figure /3 w esho w the n um be r of errors pro duced under v ariousexp erimen tal conditions/. The four traces in the plotcorresp ond to p erplexities /2 /;; /4 /;; /6 /;; /8/, while eac h in/-dividual trace giv es the n um be r of errors pro ducedb y the recognition after correcting /0 /;;/:/:/: /;; /7 errors/. Ineac h pass the /\frst error found from the previous passw as corrected/. In eac h case w e w ere able to ac hiev e ap erfect parse after correcting /7 or few er errors/. Fig/-ure /3 also demonstrates that recognition accuracyimpro v es with decreasing p erplexit y /, th us sho wingthat signi/\fcan t b ene/\ft results from using a transi/-tion probabilit y matrix w ell/-suited to the actual testdata/.In our next/, and considerably more am bitious/,example w e parsed a MIDI p erformance of theChopin Mazurk a Op/. /6/, no/. /3/. for solo piano/. Un/-lik e the monophonic instrumen t of the previous ex/-ample/, the piano can pla y sev eral notes at a singlescore p osition/. This situation can be handled witha v ery simple mo di/\fcation of the approac h w e ha v edescrib ed ab o v e/. Recall from Section /2 that l /( s/;; s\n/0/)describ es the note length asso ciated with the transi/-tion from state s to state s\n/0/. W e mo dify the de/\fni/-tion of Eqn/. /1 to bel /( s/;; s\n/0/) /=\n/(s\n/0/; s if s\n/0/\u0015 s/1 /+ s\n/0/; s otherwisewhere w e ha v e simply replaced the /> in Eqn/. /1 b y/\u0015 /. The e/\u000bect is that a /\\self/-transition/\" /(from states to state s /) is in terpreted ha ving /0 length/, i/.e/. cor/-resp onding to t w o notes ha ving the same score p osi/-tion/.F or this example/, in /3///4 time/, w e to ok the p os/-sible measure p ositions from the actual score/, giving02468 1 00 50 100 150 200 250Chopin Mazurka op. 6 no. 3\nerrors fixedremaining errors1334 notesFigure /4/: Results of rh ythmic parses of ChopinMazurk a Op/. /6/, No/. /3/.the setS /=\n/\u001a/0\n/1\n/;;\n/1\n/3\n/;;\n/2\n/3\n/;;\n/1\n/6\n/;;\n/1/1\n/1/2\n/;;\n/2/3\n/2/4\n/;;\n/1\n/4\n/;;\n/1\n/9\n/;;\n/2\n/9\n/;;\n/1\n/2\n/;;\n/5\n/6\n/;;\n/1\n/1/2\n/;;\n/1/3\n/2/4\n/;;\n/7\n/1/2\n/;;\n/1\n/2/4\n/\u001bAgain/, sev eral of the measure p ositions corresp ondto grace notes/. Rather than /\fxing the parametersof our mo del b y hand/, w e instead estimated themfrom actual data/. The transition probabilit y ma/-trix/, R /, w as estimated from scores of sev eral dif/-feren t Chopin Mazurk a extracted from MIDI /\fles/.The result w as a transition probabilit y matrix ha vingP erp/( R /) /= /2 /: /0/2/, thereb y pro viding a mo del that hasenormously impro v ed predictiv e po w er o v er the uni/-form transition mo del ha ving p erplexit y P erp /( R /) /=jS j /= /1/5/. W e also learned the v ariances of our mo del/,/\u001c\n/2/( Sn /; /1\n/;;Sn\n/) and /\u001a\n/2/( Sn /; /1\n/;;Sn\n/) b y applying the EMalgorithm to a MIDI Mazurk a using a kno wn score/.W e then iterated the pro cedure of parsingthe data and then /\fxing the error b eginningthe longest run of consecutiv e errors/. The re/-sults of our exp erimen ts with this data set aresho wn in Figure /4/. The example con tained/1/3/3/4 notes/. The MIDI /\fle can be heard athttp/:////fafner/.math/.umass/.edu//rhythmic\n pa rsing /.\n/5 DiscussionW e ha v e presen ted a metho d for sim ultaneous esti/-mation of rh ythm and temp o/, giv en a sequence ofnote onset times/. Our metho d assumes that the col/-lection of p ossible measure p ositions is giv en in ad/-v ance/. W e b eliev e this assumption is a relativ ely sim/-ple w a y of limiting the complexit y of the recognizedrh ythm pro duced b y the algorithm/. When arbitraryrh ythmic complexit y is allo w ed without pe n a l t y /, onecan alw a ys /\fnd a rh ythm with an arbitrarily accu/-rate matc h to the observ ed time sequence/. Th us/,w e exp ect that an y approac h to rh ythm recognitionwill need some form of information that limits orp enalizes this complexit y /. Other than this assump/-tion/, all parameters of our mo del can/, and should/,be learned from actual data/, as in our second ex/-ample/. Suc h estimation requires a set of trainingdata that /\\matc hes/\" the test data to be recognizedin terms of rh ythmic con ten t and rh ythmic in terpre/-tation/. F or example/, w e w ould not exp ect success/-ful results if w e trained our mo del on Igor Stra vin/-sky/'s L e Sacr e du Printemps and recognized on HankWilliams/' Y our Che atin /' He art /. In our exp erimen tswith the Chopin Mazurk a in Section /4/, w e used dif/-feren t Chopin Mazurk as for training/;; ho w ev er/, it islik ely that a less precise matc h be t w een training andtest w ould still pro v e w ork able/.W e b eliev e that the basic ideas w e ha v e pre/-sen ted can be extended signi/\fcan tly be y ond whatw e ha v e describ ed/. W e are curren tly exp erimen tingwith a mo del that represen ts sim ultaneous ev olutionof rh ythm and pitch /. Since these quan tities are in ti/-mately in tert wined/, one w ould exp ect b etter recog/-nition of rh ythm when pitc h is giv en/, as in MIDIdata/. F or instance/, consider the commonly encoun/-tered situation in whic h do wn b eats are often mark edb y lo w notes as in the Chopin example/.The exp erimen ts presen ted here deal with esti/-mating the c omp osite rh ythm obtained b y sup erim/-p osing the v arious parts on one another/. A disad/-v an tage of this approac h is that comp osite rh ythmscan be quite complicated ev en when the individualv oices ha v e simple rep etitiv e rh ythmic structure/. F orinstance/, consider a case in whic h one v oice usestriple sub divisions while another use duple sub di/-visions/. A more sophisticated pro ject w e are explor/-ing is the sim ultaneous estimation of rh ythm/, temp oand v oicing/. Our hop e is that rh ythmic structureb ecomes simpler and easier to recognize when onemo dels and recognizes rh ythm as the sup erp ositionof sev eral rh ythmic sources/. Rh ythm and v oicingcollectiv e constitute the /\\lion/'s share/\" of what oneneeds for for automatic transcription of MIDI data/.While the Sc h umann example w as m uc h simplerthan the Chopin example/, it illustrates another di/-rection w e will pursue/. Rh ythmic parsing can pla yan imp ortan t roll in in terpreting the results of apreliminary analysis of audio data that con v erts asampled acoustic signal in to a /\\piano roll/\" t yp e ofrepresen tation/. As discussed/, w e fa v or sim ultaneousestimation o v er /\\staged/\" estimation whenev er p os/-sible/, but w e feel that an e/\u000bort to sim ultaneouslyreco v er all parameters of in terest from an acousticsignal is extremely am bitious/, to sa y the least/. W efeel that the t w o problems of /\\signal/-to/-piano/-roll/\"and rh ythmic parsing together constitute a reason/-able partition of the problem in to manageable pieces/.W e in tend to consider the transcription of audio datafor considerably more complex data than those dis/-cussed here/.References/[/1/] Hewlett W/./, /(/1/9/9/2/)/, /\\A Base/-/4/0 Num b er/-LineRepresen tation of Musical Pitc h Notation/,/\"Musikometrika V ol/. /4/, /1/{/1/4/, /1/9/9/2/./[/2/] Hewlett W/./, /(/1/9/8/7/)/, /\\The Represen tation ofMusical Information in Mac hine/-Readable F or/-mat/,/\" Dir e ctory of Computer Assiste d R ese ar chin Music olo gy /, V ol/. /3/, /1/{/2/2 /1/9/8/7/./[/3/] Selfridge/-Field E/./, /(/1/9/9/4/)/, /\\The MuseData Uni/-v erse/: A System of Musical Information/,/\" Com/-puting in Music olo gy /, V ol/. /9/, /9/{/3/0/, /1/9/9/4/./[/4/] McNab R/./, Smith L/./, Bain bridge D/./, Wit/-ten I/./, /(/1/9/9/7/) /\\The New Zealand DigitalLibrary MELo dy inDEX/,/\" D/-Lib Magazine/,http/:////www/.dlib/.o rg//dlib//ma y/9/7//meldex///0/5witten/.htmlMa y /1/9/9/7/./[/5/] Bain bridge D/. /(/1/9/9/8/)/, /\\MELDEX/: A W eb/-based Melo dic Index Searc h Service/,/\" Comput/-ing in Music olo gy V ol/. /1/1 /2/2/3/{/2/3/0/, /1/9/9/8/.\n/[/6/] Sc ha/\u000brath/, H/./, /(/1/9/9/2/)/, /\\The EsA C Databasesand MAPPET Soft w are/,/\" Computing and Mu/-sic olo gy v ol/. /8/, /1/9/9/2/, /6/6/./[/7/] Desain P /, Honing H/./, /(/1/9/9/1/) /\\T o w ards a calcu/-lus for expressiv e timing in m usic/,/\" Computersin Music R ese ar ch /, V ol/. /3/,/4/3/{/1/2/0/, /1/9/9/1/./[/8/] Repp B/./, /(/1/9/9/0/)/, /\\P atterns of Expressiv e Tim/-ing In P erformances of a Beetho v en Min uetb y Nineteen F amous Pianists/,/\" Journal of theA c oustic al So ciety of A meric a V ol/. /8/8/, /6/2/2/{/6/4/1/,/1/9/9/0/./[/9/] Bilmes J/./, /(/1/9/9/3/)/, /\\Timing is of the essence/:P erceptual and computational tec hniques forrepresen ting/, learning/, and repro ducing expres/-siv e timing in p ercussiv e m usic/,/\" S/.M/. thesis/,Massac h usetts Institute of T ec hnology MediaLab/, Cam bridge/, /1/9/9/3/./[/1/0/] T rilsb eek P /./, v an Thienen H/./, /(/1/9/9/9/)/, /\\Quan ti/-zation for Notation/: Metho ds used in Commer/-cial Music Soft w are/,/\" handout at /1/0/6th A udioEngine ering So ciety c onfer enc e /, Ma y /1/9/9/9/, Mu/-nic h/./[/1/1/] Cemgil A/. T/./, Kapp en B/./, Desain P /./, Honing/,H/. /(/2/0/0/0/)/, /\\On T emp o T rac king/: T emp ogramRepresen tation and Kalman Filtering/\" Pr o c e e d/-ings of the International Computer Music Con/-fer enc e /, Berlin/, /2/0/0/0/./[/1/2/] Desain P /./, Honing H/. /(/1/9/9/4/)/, /\\A Brief In tro/-duction to Beat Induction/,/\" Pr o c e e dings of theInternational Computer Music Confer enc e /, SanF rancisco/, /1/9/9/4/./[/1/3/] Desain P /./, Honing H/. /(/1/9/8/9/)/, /\\The Quan tiza/-tion of Musical Time/: A Connectionist Ap/-proac h/,/\" Computer Music Journal /, V ol /1/3/, no/./3/./[/1/4/] Desain P /./, Aarts R/./, Cemgil A/. T/./, Kapp en B/./,v an Thienen H/, T rilsb eek P /. /(/1/9/9/9/)/, /\\RobustTime/-Quan tization for Music from P erformanceto Score/,/\" Pr o c e e dings of /1/0/6th A udio Engine er/-ing So ciety c onfer enc e /, Ma y /1/9/9/9/, Munic h/./[/1/5/] Cemgil A/. T/./, Desain P /./, Kapp en B/. /(/1/9/9/9/)/,/\\Rh ythm Quan tization for T ranscription/,/\"Computer Music Journal /, /6/0/-/7/6/./[/1/6/] Lauritzen S/. L/./, /(/1/9/9/6/)/, /\\Graphical Mo dels/,/\"Oxford Univ ersit y Press/, New Y ork/./[/1/7/] Spiegelhalter D/./, Da wid A/. P /./, Lauritzen S/./,Co w ell R/. /(/1/9/9/3/)/, /\\Ba y esian Analysis in Exp ertSystems/,/\" Statistic al Scienc e/, V ol/. /8/, No/. /3/, pp/./2/1/9/{/2/8/3/./[/1/8/] Jensen F/./, /(/1/9/9/6/)/, /\\An In tro duction to Ba y esianNet w orks/,/\" Springer/-V erlag/, New Y ork/./[/1/9/] Co w ell R/./, Da wid A/. P /./, Lauritzen S/./, Spiegel/-halter D/. /(/1/9/9/9/)/, /\\Probabilistic Net w orks andExp ert Systems/,/\" Springer/, New Y ork/./[/2/0/] Lauritzen S/. L/. and W erm uth N /(/1/9/8/4/)/, /\\MixedIn teraction Mo dels/,/\" T e chnic al R ep ort R/-/8/4/-/8 /,Institute for Electronic Systems/, Aalb org Uni/-v ersit y /./[/2/1/] Lauritzen S/. L/. and W erm uth N /(/1/9/8/9/)/,/\\Graphical Mo dels for Asso ciations Bet w eenV ariables/, some of whic h are Qualitativ e andsome Quan titativ e/,/\" A nnals of Statistics /, /1/7/,/3/1/-/5/7/./[/2/2/] Lauritzen S/. /(/1/9/9/2/)/, /\\Propagation of Probabili/-ties/, Means/, and V ariances in Mixed GraphicalAsso ciation Mo dels/,/\" Journal of the A meric anStatistic al Asso ciation /, V ol/. /8/7/, No/. /4/2/0/, /(The/-ory and Metho ds/)/, pp/. /1/0/9/8/{/1/1/0/8/./[/2/3/] Lauritzen S/. L/./, Jensen F/. /(/1/9/9/9/)/, /\\Stable Lo calComputation with Conditional Gaussian Distri/-butions/,/\" T e chnic al R ep ort R/-/9/9/-/2/0/1/4 /, Depart/-men t of Mathematic Sciences/, Aalb org Univ er/-sit y /./[/2/4/] Raphael C/./, /(/2/0/0/1/)/, /\\A Mixed Graphical Mo delfor Rh ythmic P arsing/,/\" Pr o c e e dings of /1/7thConfer enc e on Unc ertainty in A rti/\fcial Intel/-ligenc e /, Seattle/, /2/0/0/1/[/2/5/] Raphael C/./, /(/1/9/9/9/)/, /\\Automatic Segmen ta/-tion of Acoustic Musical Signals Using HiddenMark o v Mo dels/,/\" IEEE T r ansactions on Pat/-tern A nalysis and Machine Intel ligenc e /, v ol/. /2/1/,no /4/, /3/6/0 /{ /3/7/0/, /1/9/9/9/."
    },
    {
        "title": "Efficient Multidimensional Searching Routines.",
        "author": [
            "Josh Reiss"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415546",
        "url": "https://doi.org/10.5281/zenodo.1415546",
        "ee": "https://zenodo.org/records/1415546/files/Reiss01.pdf",
        "abstract": "The problem of Music Information Retrieval can often be formalized as 'searching for multidimensional trajectories'. It is well known that string-matching techniques provide robust and effective theoretic solutions to this problem. However, for low dimensional searches, especially queries concerning a single vector as opposed to a series of vectors, there are a wide variety of other methods available. In this work we examine and benchmark those methods and attempt to determine if they may be useful in the field of information retrieval. Notably, we propose the use of KD-Trees for multidimensional nearneighbor searching. We show that a KD-Tree is optimized for multidimensional data, and is preferred over other methods that have been suggested, such as the K-Tree, the box-assisted sort and the multidimensional quick-sort.",
        "zenodo_id": 1415546,
        "dblp_key": "conf/ismir/Reiss01",
        "keywords": [
            "Music Information Retrieval",
            "searching for multidimensional trajectories",
            "string-matching techniques",
            "low dimensional searches",
            "queries concerning a single vector",
            "series of vectors",
            "methods available",
            "field of information retrieval",
            "KD-Trees",
            "multidimensional nearneighbor searching"
        ],
        "content": "Efficient Multidimensional Searching Routines for Music \nInformation Retrieval \n \nJosh Reiss \nDepartment of Electronic Engineering,  \nQueen Mary, University of London \nMile End Road, London E14NS  \nUK \n+44 207-882-7986 \njosh.reiss@elec.qmw.ac.ukJean-Julien Aucouturier \nSony Computer Science Laboratory \n6, rue Amyot,  \nParis 75005,  \nFrance \n+33 1-44-08-05-01 \njjaucouturier@caramail.com Mark Sandler \nDepartment of Electronic Engineering,  \nQueen Mary, University of London \nMile End Road, London E14NS \nUK \n+44 207-882-7680 \nmark.sandler@elec.qmw.ac.uk \n \n \nABSTRACT \n \nThe problem of Music Information Retrieval can often be \nformalized as “searching for multidimensional trajectories”. It is \nwell known that string-matching techniques provide robust and \neffective theoretic solutions to this problem. However, for low \ndimensional searches, especially queries concerning a single \nvector as opposed to a series of vectors, there are a wide variety \nof other methods available. In this work we examine and \nbenchmark those methods and attempt to determine if they may \nbe useful in the field of information retrieval. Notably, we \npropose the use of KD-Trees for multidimensional near-\nneighbor searching. We show that a KD-Tree is optimized for \nmultidimensional data, and is preferred over other methods that \nhave been suggested, such as the K-Tree, the box-assisted sort \nand the multidimensional quick-sort.  \n  \n1. MULTIDIMENSIONAL SEARCHING \nIN MUSIC IR \n \nThe generic task in Music IR is to search for a query \npattern, either a few seconds of raw acoustic data, or some type \nof symbolic file (such as MIDI), in a database of the same \nformat.  \nTo perform this task, we have to encode the files in a \nconvenient way. If the files are raw acoustic data, we often \nresort to a feature extraction (fig. 1). The files are cut into M \ntime frames and for each frame, we apply a signal-processing \ntransform that outputs a vector of n features (e.g. \npsychoacoustics parameters such as pitch, loudness, brightness, \netc…). If the data is symbolic, we similarly encode each symbol \n(e.g. each note, suppose there are M of them) with an n-\ndimensional vector (e.g. pitch, duration). In both cases, the files \nin the database are turned into a trajectory of M vectors of \ndimension n.  \n \n \nFigure 1- Feature extraction \n \n \nWithin this framework, two search strategies can be considered: \n  \n- String-matching techniques try to align two vector sequences \nof length Mm\u0013 , ((1),(2),...()) xxxm and ((1),(2),...()) yyym \nusing a set of elementary operations (substitutions, \ninsertions…). They have received much coverage in the Music \nIR community (see for example [1]) since they allow a context-\ndependent measure of similarity and thus can account for many \nof the high-level specificities of a musical query (i.e., replacing a \nnote by its octave shouldn’t be a mismatch). They are robust and \nrelatively fast.  \n \n- Another approach would be to “fold” the trajectories of m \nvectors of dimension n into embedded vectors of higher \ndimension Nmn=⋅. For example, with m=3 and n=2: \n( )( )1 2 1 2 1 2 (1),(2),..()(1),(1),(2),(2),(3),(3) xxxmxxxxxx = \nThe search problem now consists of identifying the nearest \nvector in a multidimensional data set (i.e., the database) to some \nspecified vector (i.e., the query). This approach may seem \nawkward, because \n- We lose structure in the data that could be used to help the \nsearch routines (e.g., knowledge that 1(1)x  and 1(2)x  are \ncoordinates of the same “kind”). \n- We increase the dimensionality of the search. \n Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the \nfull citation on the first page.  \n However, there has been a considerable amount of work in \ndevising very efficient searching and sorting routines for such \nmultidimensional data. A complete review of the \nmultidimensional data structures that might be required is \ndescribed by Samet, et al. [2,3]. Non-hierarchical methods, such \nas the use of grid files [4] and extendable hashing [5], have been \napplied to multidimensional searching and analyzed extensively. \nIn many areas of research, the KD-Tree has become accepted as \none of the most efficient and versatile methods of searching. \nThis and other techniques have been studied in great detail \nthroughout the field of computational geometry [6,7]. \n \nTherefore, we feel that Music IR should capitalize on \nthese well-established techniques. It is our hope that we can \nshed some light on the beneficial uses of KD-Trees in this field, \nand how the multi-dimensional framework can be adapted to the \npeculiarities of music data.  \n \nThe paper is organized as follows. In the next four \nsections, we review four multidimensional searching routines: \nThe KD-Tree, the K-Tree, the Multidimensional Quick-sort, \nwhich is an original algorithm proposed by the authors, and the \nBox-Assisted Method. Discussion of each of these methods \nassumes that the data consists of M N-dimensional vectors, \nregardless of what each dimension represents or how the vectors \nwere created or extracted. We then benchmark and compare \nthese routines, with an emphasis on the very efficient KD-Tree \nalgorithm. Finally, we examine some properties of these \nalgorithms as regards a multidimensional approach to Music IR. \n \n2. THE KD-TREE \n \n2.1 Description \n \nThe K-dimensional binary search tree (or KD-Tree) is a \nhighly adaptable, well-researched method for searching \nmultidimensional data. This tree was first introduced and \nimplemented in Bentley, et al. [8], studied extensively in [9] and \na highly efficient and versatile implementation was described in \n[10]. It is this second implementation, and variations upon it, \nthat we will be dealing with here. \nThere are two types of nodes in a KD-Tree, the terminal \nnodes and the internal nodes. The internal nodes have two \nchildren, a left and a right son. These children represent a  \npartition along a given dimension of the  N-dimensional \nhyperplane. Records on one side of the partition are stored in the \nleft sub-tree, and on the other side are records stored in the right \nsub-tree. The terminal nodes are buckets which contain up to a \nset amount of points. A one- dimensional KD-Tree would in \neffect be a simple quick-sort.  \n \n2.2 Method \n \nThe building of the KD-Tree works by first determining \nwhich dimension of the data has the largest spread, i.e. \ndifference between the maximum and the minimum. The sorting \nat the first node is then performed along that dimension. A \nquickselect algorithm, which runs in order M time for M vectors, finds the midpoint of this data. The data is then sorted \nalong a branch depending on whether it is larger or smaller than \nthe midpoint. This succeeds in dividing the data set into two \nsmaller data sets of equal size. The same procedure is used at \neach node to determine the branching of the smaller data sets \nresiding at each node.  When the number of data points \ncontained at a node is smaller than or equal to a specified size, \nthen that node becomes a bucket and the data contained within \nis no longer sorted. \n \nConsider the following data:  \n \nA (7,-3); B (4,2); C (-6,7); D (2,-1); E (8,0); \nF (1,-8); G (5,-6); H (-8,9); I (9,8); J (-3,-4); \n  \nFig. 2 depicts the partition in 2 dimensions for this data set. At \neach node the cut dimension (X or Y) and the cut value (the \nmedian of the corresponding data) are stored. The bucket size \nhas been chosen to be one. \n \n \n \nFigure 2- The KD-Tree created using the sample data. \n \nThe corresponding partitioning of the plane is given in \nFig. 3. We note that this example comes from a larger data set \nand thus does not appear properly balanced. This data set will be \nused as an example in the discussion of other methods. \n \nFigure 3- The sample data partitioned using the KD-Tree. A nearest neighbor search may then be performed as a top-\ndown recursive traversal of a portion of the tree. At each node, \nthe query point is compared with the cut value along the \nspecified cut dimension. If along the cut dimension the query \npoint is less than the cut value, then the left branch is descended. \nOtherwise, the right branch is descended. When a bucket is \nreached, all points in the bucket are compared to see if any of \nthem is closer than the distance to the nearest neighbor found so \nfar. After the descent is completed, at any node encountered, if \nthe distance to the closest neighbor found is greater than the \ndistance to the cut value, then the other branch at that node \nneeds to be descended as well. Searching stops when no more \nbranches need to be descended. \nBentley recommends the use of parent nodes for each node \nin a tree structure. A search may then be performed using a \nbottom-up approach, starting with the bucket containing the \nsearch point and searching through a small number of buckets \nuntil the appropriate neighbors have been found. For nearest \nneighbor searches this reduces computational time from O(log \nM) to O(1). This however, does not immediately improve on \nsearch time for finding near neighbors of points not in the \ndatabase. Timed trials indicated that the increased speed due to \nbottom-up (as opposed to top-down) searches was negligible. \nThis is because most of the computational time is spent in \ndistance calculations, and the reduced number of comparisons is \nnegligible.  \n \n3. THE K-TREE \n \n3.1 Description \n \nK-trees are a generalization of the single-dimensional M-\nary search tree. As a data comparative search tree, a K-tree \nstores data objects in both internal and leaf nodes. A hierarchical \nrecursive subdivision of the N-dimensional search space is \ninduced with the space partitions following the locality of the \ndata. Each node in a K-tree contains K=2N child pointers. The \nroot node of the tree represents the entire search space and each \nchild of the root represents a K-ant of the parent space.  \nOne of the disadvantages of the K-tree is its storage space \nrequirements. In a standard implementation, as described here, a \ntree of M N-dimensional vectors requires a minimum of \nN(2+N)M⋅ fields. Only M-1 of the 2N branches actually point \nto a node. The rest point to NULL data. For large N, this waste \nbecomes prohibitive. \n \n3.2 Method \n \nConsider the case of two-dimensional data (N=2, K=4). \nThis K-tree is known as a quad-tree, and is a 4-ary tree with \neach node possessing 4 child pointers. The search space is a \nplane and the partitioning induced by the structure is a \nhierarchical subdivision of the plane into disjoint quadrants. If \nthe data consists of the 10 vectors described in Section 2.2, then \nthe corresponding tree is depicted in Fig. 4 and the partitioning \nof the plane in Fig. 5.  \n \nFigure 4- The KDTree created using the sample data. \n \nNote that much of the tree is consumed by null pointers. \n \n \nFigure 5- Sample data partitioned using the KTree method. \n \nSearching the tree is a recursive two-step process. A cube \nthat corresponds to the bounding extent of the search sphere is \nintersected with the tree at each node encountered. The bounds \nof this cube are maintained in an N-dimensional range array. \nThis array is initialized based on the search vector. At each \nnode, the direction of search is determined based on this \nintersection. A search on a child is discontinued if the region \nrepresented by the child does not intersect the search cube. This \nsame general method may be applied to weighted, radial, and \nnearest neighbor searches. For radial searches, the radius of the \nsearch sphere is fixed. For nearest neighbor searches it is \ndoubled if the nearest neighbor has not been found, and for \nweighted searches it is doubled if enough neighbors have not \nbeen found. \n4. MULTIDIMENSIONAL QUICKSORT \n \n4.1 Description \n \nFor many analyses, one wishes to search only select \ndimensions of the data. A problem frequently encountered is \nthat a different sort would need to be performed for each search \nbased on a different dimension or subset of all the dimensions. \nWe propose here a multidimensional generalization of the \nquick-sort routine. \n 4.2 Method   \n \nA quick-sort is performed on each of the N dimensions. \nThe original array is not modified. Instead, two new arrays are \ncreated for each quick-sort. The first is the quick-sort array, an \ninteger array where the value at position k in this array is the \nposition in the data array of the kth smallest value in this \ndimension. The second array is the inverted quick-sort. It is an \ninteger array where the value at position k in the array is the \nposition in the quick-sort array of the value k.  Keeping both \narrays allows one to identify both the location of a sorted value \nin the original array, and the location of a value in the sorted \narray. Thus, if (1)x has the second smallest value in the third \ndimension, then it may be represented as 3(2)x. The value \nstored at the second index in the quick-sort array for the third \ndimension will be 1, and the value stored at the first index in the \ninverted quick-sort array for the third dimension will be 2. Note \nthat the additional memory overhead need not be large. For each \nfloating-point value in the original data, two additional integer \nvalues are stored-, one from the quick-sort array and one from \nthe inverted quick-sort array. \nWe begin by looking at a simple case and showing how \nthe method can easily be generalized. We consider the case of \ntwo-dimensional data, with coordinates x and y, where we make \nno assumptions about delay coordinate embeddings or \nuniformity of data.  \nSuppose we wish to find the nearest neighbor of the 2-\ndimensional vector 12(,) xxx= . If this vector’s position on the \nfirst axis quick-sort is i and its position on the second axis \nquick-sort is j (i and j are found using the inverted quick-sorts), \nthen it may also be represented as  \n1 11 2 2 2\n1 2 1 2 ()((),())()((),()) xxixixixjxjxj == == .  \nUsing the quick-sorts, we search outward from the search \nvector, eliminating search directions as we go. Reasonable \ncandidates for nearest neighbor are the nearest neighbors on \neither side on the first axis quick-sort, and the nearest neighbors \non either side on the second axis quick-sort. The vector \n1 1 1\n1 2 (1)((1),(1)) xixixi −=−− corresponding to position i-1 on the \nfirst axis quick-sort is the vector with the closest coordinate on \nthe first dimension such that 1\n1 1(1)xix−<. Similarly, the vector \n1(1)xi+ corresponding to  i+1 on the first axis quick-sort is the \nvector with the closest coordinate on the first dimension such \nthat 1\n1 1(1)xix+<. And from the y-axis quick-sort, we have the \nvectors 2(1)xj− and 2(1)xj+.  These are the four vectors \nadjacent to the search vector in the two quick-sorts. Each \nvector's distance to the search vector is calculated and we store \nthe minimal distance and the corresponding minimal vector. If \n1|(1)| xix−− is greater than the minimal distance, then we know \nthat all vectors 1(1)xi−,1(2)xi−,...1(1)x must also be further \naway than the minimal vector. In that case, we will no longer \nsearch in decreasing values on the first axis quick-sort. We \nwould also no longer search in decreasing values on the first \naxis quick-sort if 1(1)x has been reached. Likewise, if \n1|(1)| xix+− is greater than the minimal distance, then we know \nthat all vectors 1(1)xi+,1(2)xi+,...1()xM must also be further \naway than the minimal vector. If either that is the case or 1()xM \nhas been reached then we would no longer search in increasing values on the x-axis quick-sort.  The same rule applies to \n2|(1)| xjx−− and 2|(1)| xjx+−. \nWe then look at the four vectors, \n1(2)xi−,1(2)xi+,2(2)xj−and 2(2)xj+. If any of these is closer \nthan the minimal vector, then we replace the minimal vector \nwith this one, and the minimal distance with this distance. If \n1|(2)| xix−− is greater than the minimal distance, then we no \nlonger need to continue searching in this direction.  A similar \ncomparison is made for 1|(2)| xix+−,2|(2)| xjx−− and \n2|(2)| xjx+−. \nThis procedure is repeated for 1(3)xi−, 1(3)xi+, \n2(3)xj− and 2(3)xj+, and so on, until all search directions have \nbeen eliminated.  We find the distance of the four points from \nour point of interest and, if possible, replace the minimal \ndistance. We then proceed to the next four points and proceed \nthis way until all directions of search have been eliminated. \nThe minimal vector must be the nearest neighbor, since all \nother neighbor distances have either been calculated and found \nto be greater than the minimal distance, or have been shown that \nthey must be greater than the minimal distance. \nExtension of this algorithm to higher dimensions is \nstraightforward. In N dimensions there are 2N possible \ndirections. Thus 2N immediate neighbors are checked. A \nminimal distance is found, and then the next 2N neighbors are \nchecked. This is continued until it can be shown that none of the \n2N directions can contain a nearer neighbor. \nIt is easy to construct data sets for which this is a very \ninefficient search. For instance, if one is looking for the closest \npoint to (0,0) and one were to find a large quantity of points \nresiding outside the circle of radius 1 but inside the square of \nside length 1 then all these points would need to be measured \nbefore the closer point at (1,0) is considered. However, similar \nsituations can be constructed for most multidimensional sort and \nsearch methods, and preventative measures can be taken. \n \n5. THE BOX-ASSISTED METHOD \n \nThe box-assisted search method was described by \nSchreiber, et al.[11] as a simple multidimensional search method \nfor nonlinear time series analysis. A grid is created and all the \nvectors are sorted into boxes in the grid. Fig. 6 demonstrates a \ntwo-dimensional grid that would be created for the sample data. \nSearching then involves finding the box that a point is in, then \nsearching that box and all adjacent boxes. If the nearest \nneighbor has not been found, then the search is expanded to the \nnext adjacent boxes. The search is continued until all required \nneighbors have been found. \nOne of the difficulties with this method is the \ndetermination of the appropriate box size. The sort is frequently \ntailored to the type of search that is required, since a box size is \nrequired and the preferred box size is dependent on the type of \nsearch to be done. However, one usually has only limited a \npriori knowledge of the searches that may be performed. Thus \nthe appropriate box size for one search may not be appropriate \nfor another. If the box size is too small, then many boxes are left \nunfilled and many boxes will need to be searched. This results in \nboth excessive use of memory and excessive computation. \n  \nFigure 6- The sample data set as gridded into 16 boxes in two \ndimensions, using the box-assisted method. \n \nThe choice of box dimensionality may also be \nproblematic. Schreiber, et al.[11] suggest 2 dimensional boxes. \nHowever, this may lead to inefficient searches for high \ndimensional data. Higher dimensional data may still be searched \nalthough many more boxes are often needed in order to find a \nnearest neighbor. On the other hand, using higher dimensional \nboxes will exacerbate the memory inefficiency. In the \nbenchmarking section, we will consider both two and three-\ndimensional boxes.  \n \n6. BENCHMARKING AND \nCOMPARISON OF METHODS \n \nIn this section we compare the suggested sorting and \nsearching methods, namely the box assisted method, the KD-\nTree, the K-tree, and the multidimensional quick-sort. All of \nthese methods are preferable to a brute force search (where no \nsorting is done, and all data vectors are examined each time we \ndo the searching). However, computational speed is not the only \nrelevant factor. Complexity, memory use, and versatility of each \nmethod will also be discussed. The versatility of the method \ncomes in two flavors- how well the method works on unusual \ndata and how adaptable the method is to unusual searches. The \nmultidimensional binary representation and the uniform K-Tree, \ndescribed in the previous two sections, are not compared with \nthe others because they are specialized sorts used only for \nexceptional circumstances. \n \n6.1 Benchmarking of the KDTree \n \nOne benefit of the KD-Tree is its rough independence of \nsearch time on data set size. Figure 7 compares the average \nsearch time to find a nearest neighbor with the data set size. For \nlarge data set size, the search time has a roughly logarithmic \ndependence on the number of data points. This is due to the time \nit takes to determine the search point’s location in the tree. If the \nsearch point were already in the tree, then the nearest neighbor \nsearch time is reduced from O(log n) to O(1). This can be \naccomplished with the implementation of Bentley's suggested use of parent pointers for each node in the tree structure.[10] \nThis is true even for higher dimensional data, although the \nconvergence is much slower.  \n \n \nFigure 7- The dependence of average search time on data set \nsize. \n \nIn Figure 8, the KD-Tree is shown to have an exponential \ndependence on the dimensionality of the data. This is an \nimportant result, not mentioned in other work providing \ndiagnostic tests of the KD-Tree.[10, 12] It implies that KD-\nTrees become inefficient for high dimensional data. It is not yet \nknown what search method is most preferable for neighbor \nsearching in a high dimension (greater than 8), although Liu, et. \nal. have proposed a method similar to the multidimensional \nquicksort for use in multimedia data retrieval.[13] \n \n \nFigure 8- A log plot of search time vs dimension. \n \nFigure 9 shows the relationship between the average search \ntime to find n neighbors of a data point and the value n. In this \nplot, 10 data sets were generated with different seed values and \nsearch times were computed for each data set. The figure shows \nthat the average search time is almost nearly linearly dependent \non the number of neighbors n. Thus a variety of searches \n(weighted, radial, with or without exclusion) may be performed \nwith only a linear loss in speed. \nThe drawbacks of the KD-Tree, while few, are transparent. \nFirst, if searching is to be done in many different dimensions, \neither a highly inefficient search is used or additional search \ntrees must be built. Also the method is somewhat memory intensive. In even the simplest KD-Tree, a number indicating the \ncutting value is required at each node, as well as an ordered \narray of data (similar to the quick-sort). If pointers to the parent \nnode or principal cuts are used then the tree must contain even \nmore information at each node. Although this increase may at \nfirst seem unimportant, one should note that a music information \nretrieval system may consist of a vast number of files, or \nalternately, a vast number of samples within each file. Thus \nmemory may prove unmanageable for many workstation \ncomputers. \n \n \nFigure 9- A plot of the average search time to find n \nneighbors of a data point, as a function of n. \n \nWe have implemented the KD-Tree as a multiplatform \ndynamic linked library consisting of a set of fully functional \nobject oriented routines. The advantage of such an \nimplementation is that the existing code can be easily ported \ninto existing MIR systems.  In short, the core code consists of \nthe following functions \n \nCreate(*phTree, nCoords, nDims, nBucketSize,*aPoints); \nFindNearestNeighbor(Tree,*pSearchPoint, *pFoundPoint); \nFindMultipleNeighbors(Tree, *pSearchPoint, \n*pnNeighbors, *aPoints); \nFindRadialNeighbors(Tree, *pSearchPoint, radius, \n**paPoints, *pnNeighbors); \nReleaseRadialNeighborList(*aPoints); \nRelease(Tree); \n \n6.2 Comparison of methods \n \nThe KD-Tree implementation was tested in timed trials \nagainst the multidimensional quick-sort and the box-assisted \nmethod. In Figure 10 through Figure 13, we depict the \ndependence of search time on data set size for one through four \ndimensional data, respectively.  \n \n \nFigure 10- Comparison of search times for different \nmethods using 1 dimensional random data. \n \n \n \nFigure 11- Comparison of search times for different \nmethods using 2 dimensional random data. \n \nIn Figure 10, the multidimensional quick-sort reduces to a \none-dimensional sort and the box assisted method as described \nby [11] is not feasible since it requires that the data be at least \ntwo-dimensional. We note from the slopes of these plots that the \nbox-assisted method, the KDTree and the KTree all have an O(n \nlog n) dependence on data set size, whereas the quick-sort based \nmethods have approximately O(n1.5) dependence on data set size \nfor 2 dimensional data and O(n1.8) dependence on data set size \nfor 3 or 4 dimensional data. As expected, the brute force method \nhas O(n2) dependence.  \nDespite its theoretical O(n log n) performance, the KTree \nstill performs far worse than the box-assisted and KDTree \nmethods. This is because of a large constant factor worse \nperformance that is still significant for large data sets (64,000 \npoints). This constant worse performance relates to the poor \nbalancing of the KTree. Whereas for the KDTree, the data may \nbe permuted so that cut values are always chosen at medians in \nthe data, the KTree does not offer this option because there is no \nclear multidimensional median. In addition, many more \nbranches in the tree may need to be searched in the KTree \nbecause at each cut, there are 2k instead of 2 branches. \n  \nFigure 12- Comparison of search times for different \nmethods using 3 dimensional random data. \n \nFigure13- Comparison of search times for different \nmethods using 4 dimensional random data. \n \nHowever, all of the above trials were performed using \nuniform random noise. They say nothing of how these methods \nperform with other types of data. In order to compare the sorting \nand searching methods performance on other types of data, we \ncompared their times for nearest neighbor searches on a variety \nof data sets. Table 1 depicts the estimated time in milliseconds \nto find all nearest neighbors in different 10,000 point data sets \nfor each of the benchmarked search methods. The uniform noise \ndata was similar to that discussed in the previous section. \n Each Gaussian noise data set had a mean of 0 and standard \ndeviation of 1 in each dimension. The identical dimensions and \none valid dimension data sets were designed to test performance \nunder unusual circumstances. \n For the identical dimensions data, uniform random data \nwas used and each coordinate of a vector was set equal, e.g., \n123 111 ()((),(),())((),(),()) xixixixixixixi = = \nFor the data with only one valid dimension, uniform random \ndata was used in only the first dimension, e.g., \n123 1 ()((),(),())((),0,0) xixixixixi = = \nIn all cases the KD-Tree proved an effective method of sorting \nand searching the data. Only for the last two data sets did the \nmultidimensional quick-sort method prove faster, and these data \nsets were constructed so that they were, in effect, one- \ndimensional. In addition, the box method proved particularly ineffective for high dimensional Gaussian data where the \ndimensionality guaranteed that an excessive number of boxes \nneeded to be searched, and for the Lorenz data, where the highly \nnon-uniform distribution ensured that many boxes went unfilled. \nThe K-tree also performed poorly for high dimensional data \n(four and five dimensional), due to the exponential increase in \nthe number of searched boxes with respect to dimension. \n \nA summary of the comparison of the four routines can be found \nin Table 2. The “adaptive” and “flexible” criteria refer to the \nnext section.  \n \nTable 2- Comparison of some features of the four routines. \nRating from 1=best to 4=worst.  \n \nAlgorithm Memory Build Search Adapt. Flexible \nKDTree 2 3/4 1 yes yes \nKTree 3 3/4 3 no no \nQuick-sort 1 2 4 yes yes \nBoxAssisted 4 1 2 no yes \n \n7. INTERESTING PROPERTIES FOR \nMUSIC IR \n \nThe multi-dimensional search approach to Music IR, and \nthe corresponding algorithms presented above have a number of \ninteresting properties and conceptual advantages. \n \n7.1 Adaptive to the distribution \n \nA truly multi-dimensional approach enables an adaptation to the \ndistribution of the data set. For example, the KD-Tree algorithm  \nfocuses its discriminating power in a non-uniform way. The \nsearch tree it creates represents a best fit to the density of the \ndata. This could be efficient for, say, search tasks in a database \nwhere part of the features remain quasi constant, e.g. a database \nof samples which are all pure tones of a given instrument, with \nquasi constant pitch, and a varying brightness. It is interesting to \ncompare this adaptive behavior with a string-matching algorithm \nthat would have to compare sequences that all begin with \n“aaa…”. The latter can’t adapt and systematically tests the first \nthree digits, which is an obvious waste of time. \n \n \n7.2 Independent of the metric and of the \nalphabet \n \nAll the methods presented here are blind to the metric that \nis used. This is especially useful if the set of features is \ncomposite, and requires a different metric for each coordinate, \ne.g. pitches can be measured modulo 12. The routines are also \nindependent of the alphabet, and work for integers as well as for  \n \n \n Table 1- Nearest neighbor search times for data sets consisting of 10000 points. The brute force method, multidim. quick-sort, the \nbox assisted method in 2 and 3 dimensions, the KDTree and the KTree were compared. An X indicates that it wasn’t possible to use \nthis search method on this type of data. The fastest method is given in bold and the second fastest method is given in italics. \n \n \n \n \n \n \n \n \n \nfloating-points. This makes them very general, as they can deal \nwith a variety of queries on mixed low-level features and high-\nlevel meta-data such as: \nNearest neighbor )\"\",3,2,1( BACH pitch pitch pitch  \n \n7.3 Flexibility \n \nThere are a variety of searches that are often performed on \nmultidimensional data.[14] Perhaps the most common type of \nsearch, and one of the simplest, is the nearest neighbor search. \nThis search involves the identification of the nearest vector in \nthe data set to some specified vector, known as the search \nvector. The search vector may or may not also be in the data set. \nExpansions on this type of search include the radial search, \nwhere one wishes to find all vectors within a given distance of \nthe search vector, and the weighted search, where one wishes to \nfind the nearest A vectors to the search vector, for some positive \ninteger A. \nEach of these searches (weighted, radial and nearest \nneighbor) may come with further restrictions. For instance, \npoints or collections of points may be excluded from the search. \nAdditional functionality may also be required. The returned data \nmay be ordered from closest to furthest from the search vector, \nand the sorting and searching may be required to handle the \ninsertion and deletion of points. That is, if points are deleted \nfrom or added to the data, these additional points should be \nadded or deleted to the sort so that they can be removed or \nincluded in the search. Such a feature is essential if searching is \nto performed with real-time analysis. \nMost sorting and searching routine presented above are \nable to perform all the common types of searches, and are \nadaptable enough so that they may be made to perform any \nsearch.  \n \n7.4 A note on dimensionality \n \nOne of the restrictions shared by the multidimensional \nsearch routines presented on this paper is their dependence on \nthe dimensionality of the data-set (not its size). This is \ndetrimental  to the  sheer  “folding”  of  the  trajectory  search  as  \n  \n \n \npresented in the introduction, especially when it involves long \nm-sequences of high-n-dimension features (dimension Nmn=⋅ \nmay be too high). However, as we mentioned in the course of \nthis paper, there are still a variety of searches that can fit into the \nmultidimensional framework.  We notably wish to suggest: \n \n- Searches for combinations of high-level metadata (m=1) \n- It is possible to reduce N with classic dimensionality \nreduction techniques, such as Principal Component Analysis or \nVector Quantization. \n- It is possible to reduce M by computing only 1 vector of \nfeatures per audio piece. It is the approach taken in the Muscle \nFish™ technology [15], where the mean, variance and \ncorrelation of the features are included in the feature vector. \n- It is possible to reduce M by computing the features not \non a frame-to-frame basis, but only when a significant change \noccurs (“event-based feature extraction”, see for example [16]). \n- For finite alphabets, it is always possible to reduce the \ndimension of a search by increasing the size of the alphabet. For \nexample, searching for a set of 9 notes out of a 12 semi-tone \nalphabet can be reduced to a 3D search over an alphabet of \n312symbols.  \n \n8. CONCLUSION \n \nWe’ve presented and discussed four algorithms for a \nmultidimensional approach to Music IR. The KD search tree is a \nhighly adaptable, well-researched method for searching \nmultidimensional data. As such it is very fast, but also can be \nmemory intensive, and requires care in building the binary \nsearch tree. The k tree is a similar method, less versatile, more \nmemory intensive, but easier to implement. The box-assisted \nmethod on the other hand, is used in a form designed for \nnonlinear time series analysis. It falls into many of the same \ntraps that the other methods do. Finally the multidimensional \nquick-sort is an original method designed so that only one \nsearch tree is used regardless of how many dimensions are used.  \nThese routines share a number of conceptual advantages \nover the approaches taken so far in the Music IR community, \nwhich -we believe- can be useful for a variety of musical \nsearches. The aim of the paper is to be only a review, and the \nstarting point of a reflection about search algorithms for music. Data set Dimension Brute Quicksort Box (2) \nmethod Box (3) \nmethod KDTree KTree \nUniform noise 3 32567 2128 344 210 129 845 \nGaussian 2 16795 280 623 X 56 581 \nGaussian 4 44388 8114 54626 195401 408 3047 \nIdentical dimensions 3 33010 19  1080  5405 42 405 \nOne valid dimension 3 30261 31  1201  7033 37 453 In particular, we still have to implement specific music retrieval \nsystems that use the results presented here. \n \n9. REFERENCES \n \n[1] K. Lemstrom, String Matching Techniques for Music \nRetrieval. Report A-2000-4, University of Helsinki \nPress. \n[2] H. Samet, Applications of Spatial Data Structures: \nAddison-Wesley, 1989. \n[3] H. Samet, The design and analysis of spatial data \nstructures: Addison-Wesley, 1989. \n[4] H. Hinterberger, K. C. Sevcik, and J. Nievergelt, ACM \nTrans. On Database Systems, vol. 9, pp. 38, 1984. \n[5] N. Pippenger, R. Fagin, J. Nievergelt, and H. R. \nStrong, ACM Trans. On Database Systems, vol. 4, pp. \n315, 1979. \n[6] K. Mehlhorn, Data Structures and Algorithms 3: \nMultidimensional Searching and Computational \nGeometry: Springer-Verlag, 1984. \n[7] F. P. Preparata and M. I. Shamos, Computational \ngeometry: An introduction. New York: Springer-\nVerlag, 1985. \n[8] J. H. Bentley, “Multidimensional Binary Search Trees \nUsed for Associative Searching,” Communications of \nthe ACM, vol. 18, pp. 509-517, 1975. [9] J. H. Friedman, J. L. Bentley, and R. A. Finkel, “An \nalgorithm for finding best matches in logarithmic \nexpected time,” ACM Trans. Math. Software, vol. 3, \npp. 209, 1977. \n[10] J. L. Bentley, “K-d trees for semidynamic point sets,” \nin Sixth Annual ACM Symposium on Computational \nGeometry, vol. 91. San Francisco, 1990. \n[11] T. Schreiber, “Efficient neighbor searching in \nnonlinear time series analysis,” Int. J. of Bifurcation \nand Chaos, vol. 5, pp. 349-358, 1995. \n[12] R. F. Sproull, “Refinement to nearest-neighbour \nsearching in k-d trees,” Algorithmica, vol. 6, p. 579-\n589, 1991. \n[13] C.-C. Liu,  J.-L. Hsu, A. L. P. Chen, “Efficient Near \nNeighbor Searching Using Multi-Indexes for Content-\nBased Multimedia Data Retrieval,” Multimedia Tools \nand Applications, Vol 13, No. 3, 2001, p.235-254. \n[14] J. Orenstein, Information Processing Letters, vol. 14, \npp. 150, 1982. \n[15] E. Wold, T. Blum et al., “Content Based \nClassification, Search and Retrieval of Audio”, in \nIEEE Multimedia, Vol.3, No. 3, Fall 1996, p.27-36. \n[16] F. Kurth, M. Clausen, “Full Text Indexing of Very \nLarge Audio Databases”, in Proc. 110th AES \nConvention, Amsterdam, May 2001."
    },
    {
        "title": "Musical Works as Information Retrieval Entities: Epistemological Perspectives.",
        "author": [
            "Richard P. Smiraglia"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1416512",
        "url": "https://doi.org/10.5281/zenodo.1416512",
        "ee": "https://zenodo.org/records/1416512/files/Smiraglia01.pdf",
        "abstract": "Musical works form a key entity for music information retrieval. Explicit linkage of relationships among entities is critical for document-based information retrieval. Works contain representations of recorded knowledge. Core bodies of work— canons—function to preserve and disseminate the parameters of a culture. A musical work is an intellectual sonic conception. Musical works take documentary form in a variety of instantiations. Epistemology for documentary analysis provides key perceptual information about the objects of knowledge organization. Works are carriers of knowledge, representing deliberately-constructed packages of both rational and empirical evidence of human knowledge. Smiraglia (2001) suggests the parameters of a theory of the work, incorporating the tools of epistemology to comprehend works by expressing theoretical parameters in the context of a taxonomic definition. A work is a signifying, concrete set of ideational conceptions that finds realization through semantic or symbolic expression. Semiotic analysis suggests a variety of cultural and social roles for works. Musical works, defined as entities for information retrieval, are creations. Variability over time, demonstrated empirically, is an innate aspect of the set of all instantiations of a musical work, leading to complexity in the information retrieval domain.",
        "zenodo_id": 1416512,
        "dblp_key": "conf/ismir/Smiraglia01",
        "keywords": [
            "Musical works",
            "Documentary form",
            "Knowledge representation",
            "Epistemology",
            "Taxonomic definition",
            "Signifying entities",
            "Semantic expression",
            "Cultural roles",
            "Variability over time",
            "Information retrieval"
        ],
        "content": "Musical Works as Information Retrieval Entities: \nEpistemological Perspectives \n \nRichard P. Smiraglia \nProfessor, \nPalmer School of Library and Information Science \nLong Island University \n720 Northern Blvd. \nBrookville, NY  11548 \n(516) 299-2174 \nRichard.Smiraglia@liu.edu \n \n \nABSTRACT \nMusical works form a key entity for music information retrieval. \nExplicit linkage of relationships among entities is critical for \ndocument-based information retrieval. Works contain \nrepresentations of recorded knowledge. Core bodies of work—\ncanons—function to preserve and disseminate the parameters of \na culture. A musical work is an intellectual sonic conception. \nMusical works take documentary form in a variety of \ninstantiations. Epistemology for documentary analysis provides \nkey perceptual information about the objects of knowledge \norganization. Works are carriers of knowledge, representing \ndeliberately-constructed packages of both rational and empirical \nevidence of human knowledge. Smiraglia (2001) suggests the \nparameters of a theory of the work, incorporating the tools of \nepistemology to comprehend works by expressing theoretical \nparameters in the context of a taxonomic definition. A work is a \nsignifying, concrete set of ideational conceptions that finds \nrealization through semantic or symbolic expression. Semiotic \nanalysis suggests a variety of cultural and social roles for works. \nMusical works, defined as entities for information retrieval, are \nseen to constitute sets of varying instantiations of abstract \ncreations. Variability over time, demonstrated empirically, is an \ninnate aspect of the set of all instantiations of a musical work, \nleading to complexity in the information retrieval domain. \n1. INTRODUCTION \nMusical works (as opposed to musical documents, such as scores \nor recordings of musical works) form a key entity for music  \ninformation retrieval. Ultimately, searches for a given musical \nwork rely on the hope of subsequent selection of instantiation in \none of several documentary formats. Musical works have been \nvariously and industriously described by musicologists and music \nbibliographers. However, in the information retrieval domain, the \nwork as opposed to the document, has only recently received \nfocused attention (Smiraglia 2001). Efforts to define works as \ninformation retrieval entities and to document their occurrence \nempirically are quite recent. In fact, systems for bibliographic \ninformation retrieval, and more recently for information storage and retrieval, have been designed with the document as the key \nentity, and works have been dismissed as too abstract or difficult \nto define empirically to take a role in information retrieval. \nRecent work, summarized in Smiraglia (2001), points to the \nprimacy of works for bibliographic information retrieval, and to \nthe importance of works as concepts for all text-based \ninformation storage and retrieval systems. In this paper, \ndefinitions of works as entities (from the information retrieval \nperspective) and of musical works (from the musicological \nperspective) are examined. A taxonomic definition is presented. \nAn epistemological perspective, including empirical evidence, \naids in understanding the components of the taxonomic \ndefinition. Musical works, thus defined as entities for \ninformation retrieval, are seen to constitute sets of varying \ninstantiations of abstract creations. \n2. Documentary Entities \nA documentary entity is a unique instance of knowledge (e.g., a \nthesis, a sculpture, a research report, etc.). Each documentary \nentity has physical and intellectual properties. A containing \nrelationship exists between these two properties. That is, the \nphysical property is the package for the intellectual. The explicit \nlinkage of relationships among documentary entities is critical \nfor document-based information retrieval. Empirical research \ntechniques have illuminated the technical problems of bringing \nthe objective of collocating works, as opposed to documents, into \nprimary position. Tillett (1987) sought to classify and quantify \nthe entire range of bibliographic relationships--relationships that \nexist among documentary entities. Smiraglia (1992) investigated \nthe derivative relationship, which holds among all versions of a \nwork, refining its definition to include several different \ncategories of derivation. These categories are: \n·simultaneous derivations \n·successive derivations \n·translations \n·amplifications \n·extractions \n·adaptations, and \n·performances. \nLeazer (1993 and 1994) described a conceptual schema for the \nexplicit control of works in catalogs, taking into account both copyright box \n Tillet and Smiraglia’s taxonomies of relationship types. Leazer \nand Smiraglia studied the presence of derivative relationships in \nthe OCLC WorldCat (Smiraglia and Leazer 1995 and 1999, \nLeazer and Smiraglia 1996 and 1999) affirming the taxonomy of \nderivative relationship types. Yee examined problems of \nrelationships among moving image materials, including the \nsubstantial problems of associating bibliographic records for \nvarying instantiations of films. Vellucci (1997) examined \nmusical works and found that the categories Tillett and Smiraglia \nhad suggested were present, and in large numbers; 85.4% of the \nworks in her sample drawn from the catalog of the Sibley Music \nLibrary demonstrated derivative relationships. Vellucci also \npostulated two new categories of derivation applicable only to \nmusical works: musical presentation, and notational \ntranscription. \nA 1998 report by a study group of The International Federation of \nLibrary Associations (IFLA) was devoted to outlining functional \nrequirements for bibliographic records. Representing the \nproducts of intellectual or artistic endeavor, the report suggested \na group of documentary entities works, expressions, \nmanifestations, and items. A work was described as a distinct \nintellectual or artistic creation, an expression as the intellectual \nor artistic realization of a work. The entities work and expression \nreflected intellectual or artistic content. A manifestation \nembodied an expression of a work, which was in turn embodied \nby an item. The entities manifestation and item, then, reflected \nphysical form. The report noted that a work might be realized \nthrough one or more expressions, which might be embodied in \none or more manifestations, which in turn might be exemplified \nin one or more items (IFLA 1998, 12-13). \n3. WORKS AS VEHICLES FOR \nCOMMUNICATION \nWorks contain representations of recorded knowledge. Works are \ncreated deliberately to represent the thoughts, data, syntheses, \nknowledge, art and artifice of their creators. Works, then, serve \nas vehicles to communicate one or more of these aspects of new \nknowledge to potential consumers (readers, scholars, etc.). \nConsumers of works may and often do use them to inform their \nown new works, which likewise serve as vehicles to \ncommunicate knowledge across time and space to new \nconsumers. In this manner, we can observe the social role of \nworks. Therein we see works as vehicles that transport ideas \nalong a human continuum, contributing to the advancement of \nhuman knowledge in specific ways and to the advancement of the \nhuman social condition in more general ways. \nSaussure described a system for the study of the life of signs in a \nsociety, which he named semiology (1959, 16). Smiraglia (2001) \nhas used Saussure's system to demonstrate the cultural role of \nworks. Works function in a manner analogous to signs, uniting \nthe conceptual with the semantic, and demonstrating the two \nproperties immutability and mutability. Peirce and his school of \nsemiotics also shed light on the mutability of signs and the \nprobability of their varying perception across chronological and \ncultural barriers. Peirce ([1894] 1998, 5) asserted a triad of types \nof signs: a) likenesses, which convey ideas of the things they \nrepresent through imitation; b) indications, which show \nsomething about things by being physically connected with them; \nand c) symbols, or general signs, which have become associated \nwith their meanings by usage. The meaning of a symbol is not \nfixed, but rather is a function of its perception. Barthes also described reception mutability, suggesting that consumers of \nworks were not concerned so much with the integrity of a text as \nwith their own experience of it (1975, 11). For example, an \nindividual work might be consulted for information, it might be \nused for recreation, or it might form the basis of a scholar's \ndiscourse. Barthes suggests that in essence a text is as though it \nwere tissue (1975, 64). Poster (1990) suggested that cultural \nhistory was demarcated by variations in the structure of symbolic \nexchange. In literate society, works are the vehicles that facilitate \nthe propagation of culture through formal symbolic exchange. \nWorks can be seen as analogous to signs that are mutable over \ntime. The texts of works act as signifiers, seemingly immutable \nwhen first fixed, but with other properties (such as cultural \nidentity) that are themselves very mutable indeed. Works are \nvehicles of culture, entities that arise from a particular cultural \nperspective. As such they are vehicles with certain cultural \nobligations--among them dissemination and propagation of the \nculture from which they spring. This analogy has been \ndemonstrated graphically by Smiraglia (2001) and is reproduced \nin Figure 1. \n  \nFigure 1. Works are Analogous to Signs \n4. WORKS AS ELEMENTS OF CANON \nEach work is in some way a part of a larger body of related work. \nThese bodies of work derive meaning from their function in \nculture as well as from their relations with other works and other \nbodies of work. Individual works derive meaning from their \nrelations to their human receptors. These core bodies of work, \nsometimes referred to as canons, function to preserve and \ndisseminate the parameters of a culture by inculcating cultural values through the information conveyed as a whole and in each \nof the works that comprise them. Smiraglia and Leazer (1999) \nreported that the size of a family of instantiations of a work \nseems to be related to its popularity, or ... its canonicity. Most \nfamilies are formed and reach full size soon after publication of \nthe progenitor. On the other hand, older progenitors are the locus \nfor larger families. \nRelations that are observed among works in a canon are thought \nto be conventional rather than natural. That is, they are functions \nof their roles in the culture from which they spring rather than \ndetermined by any inherent characteristics. Eggert (1994) \ndescribed a phenomenological view of works of art, seeing works \nas ongoing entities that incorporate across their chronological \nexistence all of the reactions of those who encounter them. \nThrough the vehicle of works, culture is continually \ncommunicated. Works have no unchanging existential anchor, no \nsingle perfect exemplar. Rather they derive much of their \nmeaning from their reception and continuous reinterpretation in \nevolving cultures. Works follow the same pattern as Saussure’s \nlinguistic signs, mutating across time through the collaboration of \nthe cultures that embrace them. Works are shaped by their \naudiences, and they reflect the functional requirements of those \nwho will use them. Therefore, works are artifacts of the cultures \nfrom which they arise. \n5. MUSICAL WORKS \nA musical work is an intellectual sonic conception. Musical \nworks take documentary form in a variety of instantiations (i.e., \na sounding of it as in performance, or its representation in \nprinting as in score). The primary purpose of any physical \ninstantiation of a work is to convey the intellectual conception \nfrom one person to others. Because musical works fundamentally \nare meant to be heard, physical instantiations are not of primary \nimportance in the exchange between creator and consumer. \nRather, they are media through which musical ideas captured at \none end of the continuum may be reproduced so that they may be \nabsorbed at the other. Defining a musical work as a sonic \nconception allows us to bridge the difficulty that arises between \nworks that are composed (such as those in the supposed canon of \nWestern Art Music) and those that are improvised or otherwise \nrealized primarily through performance. In information retrieval, \nit is critical to make a distinction between the physical artifactual \ndocument, on the one hand, and its musical content, on the other. \nBecause a musical work must first exist in time to be \napprehended by an audience, the more accurate instantiation of a \nmusical work truly is likely its performance. Krummel (1988) \nargues that music is an entity that occurs in time, not on paper. \nEach performance is a \"re-creation\" of the work. A performance \nof a musical work, and by extension a recording thereof, \ndelineates the time factor of a musical work for the receiving \naudience. For Dahlhaus (1983), the musical work actually \ninheres in the receiving audience. \nKrummel (1970) summarized the historical use of musical \ndocuments, which serve as evidence of musical works that have \nexisted and perhaps been performed in the past. He wrote (16): \n“Behind both [score and performance], apart from but governing \nboth, as something of a Platonic ideal, is the abstract concept of \nthe work of music itself.” That is, a musical work (like any \ncreation) is existentially viewed as an abstract concept in time \nrather than a particular physical entity in space. Scores, \nperformances (and recordings) represent instances of the work, none of which can be equated fully with the work itself. Nattiez \n(1990) described a semiology of music that comprehends musical \nworks as multi-dimensional because their realization is in sound. \nGoehr (1992) pointed to the human’s natural tendency to take \nmusical works for granted, enjoying their reception but without \nany clear understanding of the complexity of their origin or \nexistence. Goehr posited an imaginary museum of works--\nimaginary to those who cannot see beyond the objectification of \nworks of sonic art. With Nattiez and Goehr we approach the \nconcept of mutability of works one step further. That is, we can \nclearly comprehend works that might have no concrete tokens--as \nliterary works have words on paper--but which find their \nrealization in sonic performances, each of which is uniquely \ncreated and uniquely perceived. Ingarden (1986) approached the \ncentral problem of the nature of a musical work by considering \nthat the work represents a congruence between the composer and \nthe listener. Talbot (2000) includes eleven papers on the musical \nwork-concept, demonstrating little consensus on the historical \nmeaning of the concept or its time of origin. There is however, \nconvergence that a musical work must be discrete, reproducible \nand attributable (Talbot 2000, 3). The volume is filled with \ncriticisms of the concepts of the musical work and the attendant \ncanons. Curiously, just as scholars of information storage and \nretrieval and of knowledge organization have turned their \nattention to the concept of the work as an entity for information \nretrieval, musical scholars seem to be less sanguine about the \nconcept. \nThomas and Smiraglia (1998) reflect on more than a century of \nformal rules for the cataloging of musical documents, speaking to \nthe cataloging community at the point at which video-recordings \nof musical performances have become entities for documentary \nretrieval. They described the nature of the musical work as an \nentity for information retrieval, suggesting the concept functions \nin the manner of a surname for a family, around which cluster all \ninstantiations known by that concept-name in horizontal, but \nexplicitly described, relations. \nLigabue (1998) attempts to discover a real process of semiosis in \nmusic, beginning with the understanding that every sign is \nessentially inherently empty--a signifier without signification. \nThus a sign finds its meaning revealed only within a relational \ncontext (p. 35). In music, single, isolated sounds can offer only \npure information about themselves; only when contextualized \ndoes a sound acquire specificity. Therefore, sounds \"become \nmeaningful only and exclusively in relation to a context (p. 37)\" \nIn other words, sounds alone are no more musical signs than are \nletters or words linguistic signs. Rather, the semiosis is context-\ndependent. Signs are cultural constructs, and musical signs, like \nlinguistic signs, depend on specific cultural contexts for their \nmeaning. Liguabue demonstrates that \"within organized sound \nsystems, the perceptive act undergoes a mental rationalizing \nprocess [which is] culturally determined.\" Therefore, he writes, \nmusic as organized sonic events demonstrates a signification \nprocess analogous to other semiotic systems (p. 43): \nMeaning is not to be found among notes, but in them, even if it \nmanifests itself only among them. Therefore if a sign only exists \nin virtue of another sign, which, though different, shares its nature \nbut not its essence, the same thing occurs in music where each \nnote has its precise meaning, which expresses itself in its \nspecificity but can manifest itself only in the wholeness of the \nsystem. This manifestation takes place in a musical context \naccording to existing modes which cannot be the same as those of \nthe verbal context. He concludes, that what is heard or listened to (in other words, \nwhat is signified) is in essence different from the acoustic \nphysical phenomenon, and is interpreted within conventional \ncultural behaviors symbolically interpreted. \nHamman (1999) wrote about the role of computers in music \ncomposition, asserting that computers generate semiotic rather \nthan symbolic frameworks. Hamman suggests that a composer is \nnot only producer of musical artifacts, which he defines as \n“pieces,” “sounds,” etc. (in other words, works). Rather, the \ncomposer (102): “makes traces of processes by which abstract \nideas are concretized according to particular performances and \ninteractions vis a task environment.” Turino (1999), like \nLigabue, asserts a Peircian semiotic theory of music in which \ncomponents of musical units (that is, works) such as pitch, scale, \ntempo, etc. function as components of signs. The present paper \nrelies on applied semiotics to demonstrate the effect of the social \nrole of works on their complexity as entities for information \nretrieval. Turino’s semiotic analysis demonstrates the complex \nfunctioning of music and its components as signs at a meta-level. \nEchoing the comments of Eggert and Poster, van Leeuwen (1998) \nsuggests a systemic-functional semiotics of music in which music \nis seen as an abstract representation of social organization, \nconcerned with meta-level cultural interactions that find their \nexpression in music functioning as signs. \n6. EPISTEMOLOGY, KNOWLEDGE \nORGANIZATION, INFORMATION \nRETRIEVAL \nEpistemology is the division of philosophy that investigates the \nnature and origin of knowledge. Poli (1996) contrasted the tools \nof ontology and epistemology for knowledge organization, \nsuggesting that where ontology represents the \"objective\" side of \nreality, epistemology represents the \"subjective\" side. Ontology \n(\"being\") provides a general objective framework within which \nknowledge may be organized, but epistemology (\"knowing\") \nallows for the perception of the knowledge and its subjective \nrole. Olson (1996) used an epistemic approach to comprehend \nDewey’s classification, asserting a single knowable reality \nreflected in the topography of recorded knowledge. Dick (1999) \ndescribed epistemological positions in library and information \nscience. He suggested that experience (empiricism) provides the \nmaterial of knowledge, and reason (rationalism) adds the \nprinciples for its ordering. Rationalism and empiricism supply \nthe basic platform for epistemological positions. \nHjørland (1998) asserts a basic epistemological approach to base \nproblems of information retrieval, particularly to the analysis of \nthe contents of documentary entities. He begins from a basic \nmetaphysical stance, stating that ontology and metaphysics \ndescribe what exists (basic kinds, properties, etc.), whereas \nepistemology is about knowledge and ways in which we come to \nknow. Hjørland lists four basic epistemological stances:  \n·Empiricism: derived from observation and experience; \n·Rationalism: derived from the employment of reason; \n·Historicism: derived from cultural hermeneutics; and, \n·Pragmatism: derived from the consideration of goals and their \nconsequences. \nHjørland describes a domain-analytic approach to subject \nanalysis, recognizing that any given document may have different \nmeanings and potential uses to different groups of users. Hjørland and Albrechtsen (1999) delineate recent trends in \nclassification research, demonstrating the utility of Hjørland’s \nepistemological framework for deriving categories. \nMarco and Navarro (1993) described contributions of the \ncognitive sciences and epistemology to a theory of classification. \nThey suggest that (p. 128): \nThe study of epistemology is, therefore, essential for the design and \nimplementation of better cognitive strategies for guiding the process of \ndocumentary analysis, particularly for indexing and abstracting \nscientific documents. The ordering and classifying of information \ncontained in documents will be improved, thus allowing their effective \nretrieval only, if it is possible to discover the conceptual framework \n(terms, concepts, categories, propositions, hypotheses, theories, \npatterns, and paradigms) or their authors from the discursive elements \nof texts (words, sentences and paragraphs). \nEpistemology, then, is concerned with the theory of the nature of \nknowledge. The potential uses of epistemology for documentary \nanalysis are many; a few have been attempted. Whereas ontology \nmay be relied upon to frame the organization of knowledge, \nepistemology provides us with key perceptual information about \nthe objects of knowledge organization. Empiricism can lead us to \ntaxonomies of knowledge entities. Rationalism can demonstrate \nthe cultural role of, and impact on, knowledge entities. \nWorks are key carriers of knowledge, representing not simply \nraw data or facts, but deliberately-constructed packages of both \nrational and empirical evidence of human knowledge. The \norganization of works for information retrieval along topical and \ndisciplinary lines has been the key task of knowledge \norganization, specifically of classification. But works, too--\nespecially those with canonical importance, have been organized \nusing inadequate alphabetico-classified orders. \nFor instance, we can take the example of a well-known musical \nwork, Beethoven’s Moonlight sonata. An important part of \nBeethoven’s oeuvre, this popular work has become a cultural \nicon. Quite aside from its formal performance, the lilting \narpeggios are associated in the public imagination with concepts \nof nighttime and sleep. The work has demonstrated Eggert’s \nconcept of canonical mutation by becoming part of our cultural \nconsciousness. As Ligabue and Turino suggest, the signifying \nrole of the Moonlight sonata is grounded in the personal \nexperience of listeners over time and across cultures. In the \nsummer of 2000, it was used as background for a television \ncommercial for a new sleep-inducing medication. \nIn Figure 2 we see an array of descriptions of physical \ninstantiations of this work in a typical online bibliographic \nretrieval system. As is often the case, this array consists of \ntraditional name-title citations, qualified by publisher and date. \nNote there is no differentiation among the citations that can \nindicate any sort of variation among the sonic instantiations they \nrepresent. \n \nBeethoven, Ludwig v Moonlight.                      E.F. Kalmus,   1970 \nBeethoven, Ludwig v Moonlight Sonata.            presso Gio. Ca 1802 \nBeethoven, Ludwig v Moonlight sonata.            G.D. Russell & 1863 \nBeethoven, Ludwig v Moonlight sonata.            F. A. North &  1872 \nBeethoven, Ludwig v Moonlight sonata.            Schirmer,      1894 \nBeethoven, Ludwig v Moonlight sonata.            T. Presser,    1900 \nBeethoven, Ludwig v Moonlight sonata              Carl Fischer,  1906 \nBeethoven, Ludwig v Moonlight sonata.            Century Music, 1906 Beethoven, Ludwig v Moonlight sonata.            Fischer,       1906 \nBeethoven, Ludwig v Moonlight sonata.            Carl Fischer,  1916 \nBeethoven, Ludwig v Moonlight sonata              H.W. Gray,     1918 \nBeethoven, Ludwig v Moonlight sonata.            Angel Publicat 1961 \nBeethoven, Ludwig v Moonlight sonata.            Shattinger-Int 1971 \nBeethoven, Ludwig v Moonlight sonata.            Lyra Music Co. 1975 \nBeethoven, Ludwig v Moonlight sonata              The Hornists’  1978 \nBeethoven, Ludwig v Moonlight sonata.            G. Schirmer ;  1980 \nBeethoven, Ludwig v Moonlight sonata.            Alfred Pub. Co 1986 \nBeethoven, Ludwig v Moonlight sonata              Alfred Pub. Co 1991 \nBeethoven, Ludwig v Moonlight sonata              Beam Me Up Mus 1992 \nFigure 2. Moonlight sonata \n \nTo solve this problem, music librarians have traditionally \nsuperimposed an ordering device called a uniform title. Inserted \nin square brackets between the composer’s name and the \ntranscription of the title from the physical instantiation, the \nuniform title consists of a bibliographically significant title for \nthe work, based on its original as given by the composer. To this \nare added musical identifiers (such as opus number and key), to \nassist with both differentiation and order in a file consisting of \nall of the composer’s works. Excerpts are identified by \nmovement or section title, and to all of this might be added terms \nthat indicate variation in the sonic instantiation of the work. \nTaken altogether the name-uniform title citation provides the \nmeans for an alphabetico-classified ordering of a composer’s \nworks in an information retrieval venue. \nIn Figure 2 the last citation carries the curious publisher name \n“Beam Me Up Music.” This citation actually identifies an \narrangement of the adagio movement of Moonlight arranged for \nguitar. The uniform title for this work is as follows: \n \nBeethoven, Ludwig van, 1770-1827. \n[Sonatas, piano, no. 14, op. 27, no. 2, C# minor. Adagio sostenuto; \narr.] \n \nThe purpose of this example is to demonstrate the centrality of \nthe identity of musical works for music information retrieval. \nThe uniform title not only identifies the present physical \ninstantiation, but it also places it well amidst other physical \ninstantiations, themselves representative of a variety of sonic \ninstantiations. From the uniform title we learn the form, medium, \nnumber and key of the original work, the title of the specific \nmovement, and the fact that this edition represents an \narrangement. Seen in array, as in Figure 3, the alphabetical \nidentifiers serve a classificatory role, arranging and displaying \nfor differentiation the total available instantiations (physical and \nsonic) of the work. \n \nBeethoven, Ludwig van, 1770-1827. \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR.] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR; \nARR.] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nADAGIO SOSTENUTO] [SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nADAGIO SOSTENUTO; ARR.] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nALLEGRETTO] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nALLEGRETTO; ARR.] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nPRESTO AGITATO] \n[SONATAS, PIANO, NO. 14, OP. 27, NO. 2, C# MINOR. \nPRESTO AGITATO; ARR.] \nFigure 3. Instantiations Arranged by Uniform Title \nWe also see in this example a simple representation of the need \nfor a complex definition of the musical work as an entity for \ninformation retrieval. Musical works constitute complex sets of \nvarying sonic and physical instantiations, all derived from a \ncommon progenitor. Information retrieval systems need to go \nwell beyond the simple identification of the progenitor work. As \nwe see demonstrated in this example, a useful information \nretrieval system needs to have the capability to differentiate \namong the varying instantiations, in order to allow searches to \nmake the best possible choice among alternatives. \n7. A TAXONOMIC DEFINITION OF THE \nWORK \nSmiraglia (2001) suggests the parameters of a theory of the work. \nSmiraglia (2000) incorporated the tools of epistemology to \ncomprehend works by incorporating those theoretical parameters \nin the context of a taxonomic definition, which is repeated here. \nA work is a signifying, concrete set of ideational conceptions that \nfinds realization through semantic or symbolic expression. That \nis, a work embraces a set of ideas that constitute both the \nconceptual (signified) and image (signifier) components of a \nsign. A work functions in society in the same manner that a sign \nfunctions in language. Works, like signs, demonstrate the \ncharacteristics of arbitrariness (the absence of a natural link \nbetween the signified and the signifier) and linearity (signifiers \nunfold sequentially over time). Therefore, works are subject to \nthe natural ambiguity of signs, having both the properties of \nimmutability (the fixed nature of a signifier in a given \ncommunity) and mutability (change over time in their perception \nand use). \nFurther, a work has the characteristics of a Peircean symbol, \nreflecting both the physical connections of indications and the \nimitative ideational likenesses. Like works, Peircean symbols \nincorporate words or phrases that have become associated with \ntheir meanings by usage. \nIf a work enters a canon then its signifying texts may derive and \nmutate. Derivations may take one or more forms: 1) simultaneous \neditions; 2) successive editions; 3) amplifications; or, 4) \nextractions. Musical works, according to Vellucci (1997), may \nalso derive in two additional ways through musical presentation \nor notational transcription. In these categories the work derives \nculturally over time, but ideational and semantic content do not \nchange. \nMutations may take one or more forms as well: 1) translations; 2) \nadaptations; or 3) performances. In these categories the \nideational and semantic content have mutated to some degree. \nThe relations among the exemplars of a work constitute a network of related entities that has been described variously as a \nbibliographic family (Smiraglia 1992) or a textual identity \nnetwork (Leazer and Furner 1999). \nUsing Hjørland’s epistemological framework we can \ncomprehend the origins of the components of this taxonomic \ndefinition. Empirically derived components are those that have \nbeen demonstrated quantitatively in the research by Smiraglia, \nSmiraglia and Leazer, and Vellucci. Through these studies we \nhave quantitative evidence that works are signifying sets of \nideational conceptions that take realization through semantic or \nsymbolic expression. The characteristics of arbitrariness and \nlinearity are clearly demonstrated by the quantification of \nderivations and mutations of works. Evidence of canonicity is \ndemonstrated by the increased rate of derivation and mutation \nobserved among works that have become part of the academic \ncanon. \nRationalism allows us to perceive the cultural function of works, \nwhich function in society in the same manner that signs function \nin language. We also see through the application of rationalism \nthat works have the characteristics of Peircean symbols, \nreflecting both the physical connections of indications and the \nimitative ideational likenesses. Pragmatism gives us the \nperspective that the array of instantiations of works for \ninformation retrieval must incorporate mechanisms to \ndifferentiate among the demonstrated derivations and mutations \nof a given work. Works, particularly musical works, that gain \npopularity take on the perspective of cultural icons, and from that \npoint the rate of derivation and mutation and thus of the creation \nof varying physical and sonic instantiations increases. Finally, \nhistoricism provides the nominal anchor for a set of instantiations \nof a work. That is, the citation for the original work (such as the \nvery useful uniform title), derived through bibliographical \nresearch, stands as the central point for linkage of instantiations \nin an information retrieval system. \nThus our epistemological perspective yields a logic for the \nconstruction of music information retrieval mechanisms. The \nnominal anchor for the accumulated artifacts or their \nrepresentations is the historically-derived citation for the original \nideational set, occasionally altered as a result of the natural \nevolutionary action over time. Rationalism provides the \nprinciples for apprehending and ordering the entire construct. \nEntities are derived empirically; their cultural role is described \npragmatically. Derivation, mutation, and the rate thereof are \nempirically verifiable, pragmatic, collaborative socio-cultural \nconstructs. \n8. CONCLUSION \nMusical works form a key entity for music information retrieval. \nSemiotic analysis suggests a variety of cultural and social roles \nfor works, and for music in particular. Musical works, defined as \nentities for information retrieval, are seen to constitute sets of \nvarying instantiations of abstract creations. Variability over time, \ndemonstrated empirically, is an innate aspect of the set of all \ninstantiations of a musical work, leading to complexity in the \ninformation retrieval domain. \nMusical works have been well comprehended as documentary \nentities. Understanding the social roles of musical works expands \nthe boundaries of their definition. Epistemological frameworks \ncan help us understand the socio-cultural origins of concepts of \nthe musical works. Taxonomic definition contributes to the epistemological perception of works as specific entities of \nrecorded knowledge. An historically-generated nominal anchor \nfor a musical work can be used to collect the entire array of \ninstantiations.  \nMore importantly, for music information retrieval, it is critical to \ncomprehend the cultural role of musical works because it is at \nthe heart of their dissemination and reception. In a digital era of \nmusic information retrieval, the question of the degree to which \ndiffering sonic instantiations represent the same work have \nepistemological bases. In the nineteenth century one bought a \nmusical work by buying its score, and creating one's own sonic \nconception. In the twentieth century one bought a musical work \nby buying a recording of a performance of it--LP or CD. In both \ncases all copies were identical. But in the digital age, the \nopportunities for mutation are rampant. This must raise \nconstantly then, the question of just what constitutes a given \nmusical work. The answer is to be found in the epistemological \nunderstanding of the reception of musical works, and in the \nsemiotic explanation of the role of musical works as cultural \nicons. \nIn any event, an expanded perception of musical works helps us \nunderstand the variety of ways in which mechanisms for their \ncontrol and retrieval might better be shaped in future. \n9. REFERENCES \nBarthes, Roland. 1975. The pleasure of the text; trans. by Richard \nMiller with a note on the text by Richard Howard. New \nYork: Noonday Press. \nEggert, Paul. 1994. Editing paintings/conserving literature: The \nnature of the 'work.' In Studies in Bibliography v.47 ed. by \nDavid L. Vander Meulen, pp. 65-78. Charlottesville, Pub. \nfor The Bibliographical Society of the University of Virginia \nby The University Press of Virginia. \nDahlhaus, Carl. 1983. Foundations of music history, trans. J.B. \nRobinson. Cambridge; New York: Cambridge University \nPress. \nDick, Archie L.. 1999. Epistemological positions and library and \ninformation science. Library quarterly 69: 305-23. \nGoehr, Lydia. The Imaginary museum of musical works: an essay \nin the philosophy of music. Oxford: Clarendon, 1992. \nHamman, Michael. 1999. From symbol to semiotic: \nRepresentation, signification, and the composition of music \ninteraction. Journal of new music research 28: 90-104. \nHjørland, Birger. 1998. Theory and metatheory of information \nscience: a new interpretation. Journal of documentation 54: \n606-21. \nHjørland, Birger and Hanne Albrechtsen. 1999. An analysis of \nsome trends in classification research. Knowledge \norganization 26: 131-9.] \nIngarden, Roman. 1986. The work of music and the problem of \nits identity, trans. Adam Czerniawski. Berkeley, Calif.: \nUniv. of Calif. Pr. \nInternational Federation of Library Associations, Study Group on \nthe Functional Requirements for Bibliographic Records. \n1998. Functional requirements for bibliographic records. \nMünchen: K.G. Saur.. Krummel, D.W. 1988. The memory of sound: Observations on \nthe history of music on paper. Washington, D.C.: Library of \nCongress. \nKrummel, D.W. 1970. \"Music as Vibrations and as Flyspecks,\" \nWisconsin Academy of Sciences, Arts & Letters 58. \nLeazer, Gregory Hart. 1993. A conceptual plan for the \ndescription and control of bibliographic works. DLS diss., \nColumbia Univ. \nLeazer, Gregory H. 1994. A conceptual schema for the control of \nbibliographic works. In Navigating the networks: \nProceedings of the ASIS mid-year meeting 1994, ed. by D. \nL. Andersen, T. J. Galvin and M. D. Giguere. Medford, NJ: \nLearned Information, Inc., pp. 115-35. \nLeazer, Gregory H. and Jonathan Furner. 1999. Topological \nindices of textual identity networks. Proceedings of the \n62nd annual meeting of the American Society for \nInformation Science, ed. Larry Woods. Medford, NJ: \nInformation Today, pp. 345-58. \nLeazer, Gregory H. and Richard P. Smiraglia. 1996. Toward the \nbibliographic control of works: Derivative bibliographic \nrelationships in an online union catalog. In Digital libraries \n'96: 1st ACM International Conference on Digital \nLibraries, March 20-23, 1996, Bethesda, Maryland. Assn. \nfor Computing Machinery. \nLeazer, Gregory H. and Richard P. Smiraglia. 1999. \nBibliographic families in the library catalog: A qualitative \nanalysis and grounded theory. Library resources & \ntechnical services 43:191-212. \nLigabue, Marco. 1998. Sound and sign: some notes and \nreflections on the modalities of signification in music. \nContemporary music review 17n2: 35-46. \nMarco, Francisco Javier Garcia and Miguel Angel Esteban \nNavarro. 1993. On some contributions of the cognitive \nsciences and epistemology to a theory of classification. \nKnowledge organization 20: 126-32. \nNattiez, Jean-Jacques. 1990. Music and discourse: Toward a \nsemiology of music, trans. by Carolyn Abbate. Princeton, \nN.J.: Princeton Univ. Pr. \nOlson, Hope A.. (1996). Dewey thinks therefore he is: The \nepistemic stance of Dewey and DDC. In Green, Rebecca \n(Ed.), Knowledge organization and change: proceedings of \nthe Fourth International ISKO Conference, 15-18 July \n1996, Washington, DC, USA (pp. 302-3). Advances in \nknowledge organization, 5. Frankfurt/Main: Indeks Verlag. \nPeirce, Charles Sanders. [1894] 1998. The essential Peirce: \nselected philosophical writings. Vol. 2 (1893-1913), ed. by \nthe Peirce Edition Project, Nathan Houser [et al.], p. 4-10, \n\"What is a sign?\" Bloomington: Indiana Univ. Pr. \nPoli, Roberto. 1996. Ontology for knowledge organization In \nGreen, Rebecca (Ed.), Knowledge organization and \nchange: proceedings of the Fourth International ISKO Conference, 15-18 July 1996, Washington, DC, USA (pp. \n313-19). Advances in knowledge organization, 5. \nFrankfurt/Main: Indeks Verlag. \nPoster, Mark. 1990. The mode of information: Poststructuralism \nand social context. Univ. of Chicago Pr. \nSaussure, Ferdinand de. 1959. Course in general linguistics. Ed. \nby Charles Bally and Albert Sechehaye; in collaboration \nwith Albert Riedlinger; trans., with an introd. and notes by \nWade Baskin. New York: McGraw-Hill. \nSmiraglia, Richard P. 1992. Authority control and the extent of \nderivative bibliographic relationships. Ph.D. diss., \nUniversity of Chicago. \nSmiraglia, Richard P. 2000. \"Words and works; Signs, symbols \nand canons: The epistemology of the work.\"  In Dynamisn \nand stability in knowledge organization: Proceedings of the \nSixth International ISKO Conference, 10-13 July 2000, \nToronto, Canada, ed. Clare Beghtol, Lynn C. Howarth, \nNancy J. Williamson. Advances in knowledge organization \nv. 7. Würzburg: Ergon Verlag, pp. 295-300. \nSmiraglia, Richard P. 2001. The nature of \"The work.\" Lanham, \nMd.: Scarecrow Press. \nSmiraglia, Richard P. and Gregory H. Leazer. 1995. Toward the \nbibliographic control of works: Derivative bibliographic \nrelationships in the online union catalog. OCLC annual \nreview of research 1995  Dublin, OH: OCLC Online \nComputer Library Center, Inc.. \nSmiraglia, Richard P. and Gregory H. Leazer. 1999. Derivative \nbibliographic relationships: The work relationship in a \nglobal bibliographic database. Journal of the American \nSociety for Information Science 50:493-504. \nTalbot, Michael, ed. 2000. The musical work: Reality or \ninvention? Liverpool Music Symposium 1. Liverpool: \nLiverpool University Press. \nThomas, David H. and Richard P. Smiraglia. 1998. Beyond the \nscore. Notes: The quarterly journal of the Music Library \nAssociation 54:649-66. \nTillett, Barbara Ann Barnett. 1987. Bibliographic relationships: \nToward a  conceptual structure of bibliographic information \nused in cataloging. Ph.D. dissertation, University of \nCalifornia, Los Angeles. \nTurino, Thomas. 1999. Signs of imagination, identity, and \nexperience: A Peircian semiotic theory for music. \nEthnomusicology 43: 221-55. \nvan Leeuwen, Theo. 1998. Music and ideology: Notes toward a \nsociosemiotics of mass media music. Popular music and \nsociety 22n4: 25-54. \nVellucci, Sherry L. 1997. Bibliographic relationships in music \ncatalogs. Lanham, Md.: Scarecrow Press."
    },
    {
        "title": "Automatic Musical Genre Classification of Audio Signals.",
        "author": [
            "George Tzanetakis"
        ],
        "year": "2001",
        "doi": "10.5281/zenodo.1415058",
        "url": "https://doi.org/10.5281/zenodo.1415058",
        "ee": "https://zenodo.org/records/1415058/files/Tzanetakis01.pdf",
        "abstract": "Musical genres are categorical descriptions that are used to describe music. They are commonly used to structure the increasing amounts of music available in digital form on the Web and are important for music information retrieval. Genre categorization for audio has traditionally been performed manually. A particular musical genre is characterized by statistical properties related to the instrumentation, rhythmic structure and form of its members. In this work, algorithms for the automatic genre categorization of audio signals are described.  More specifically, we propose a set of features for representing texture and instrumentation. In addition a novel set of features for representing rhythmic structure and strength is proposed. The performance of those feature sets has been evaluated by training statistical pattern recognition classifiers using real world audio collections.  Based on the automatic hierarchical genre classification two graphical user interfaces for browsing and interacting with large audio collections have been developed.",
        "zenodo_id": 1415058,
        "dblp_key": "conf/ismir/Tzanetakis01",
        "keywords": [
            "Musical genres",
            "Categorical descriptions",
            "Music information retrieval",
            "Audio signals",
            "Automatic genre categorization",
            "Texture and instrumentation",
            "Rhythmic structure",
            "Statistical pattern recognition",
            "Audio collections",
            "Graphical user interfaces"
        ],
        "content": "Automatic Musical Genre Classification\nOf Audio Signals\nGeorge Tzanetakis\nComputer Science Department\n35 Olden Street\nPrinceton NJ 08544\n+1 609 258 5030\ngtzan@cs.princeton.eduGeorg Essl\nComputer Science Dep.\n35 Olden Street\nPrinceton NJ 08544\n+1 609 258 5030\ngessl@cs.princeton.eduPerry Cook\nComputer Science and Music Dep.\n35 Olden Street\nPrinceton NJ 08544\n+1 609 258 5030\nprc@cs.princeton.edu\nABSTRACT\nMusical genres are categorical descriptions that are used to\ndescribe music. They are commonly used to structure the\nincreasing amounts of music available in digital form on the\nWeb and are important for music information retrieval.\nGenre categorization for audio has traditionally been\nperformed manually. A particular musical genre is\ncharacterized by statistical properties related to the\ninstrumentation, rhythmic structure and form of its\nmembers. In this work, algorithms for the automatic genre\ncategorization of audio signals are described.  More\nspecifically, we propose a set of features for representing\ntexture and instrumentation. In addition a novel set of\nfeatures for representing rhythmic structure and strength is\nproposed. The performance of those feature sets has been\nevaluated by training statistical pattern recognition\nclassifiers using real world audio collections.  Based on the\nautomatic hierarchical genre classification two graphical\nuser interfaces for browsing and interacting with large\naudio collections have been developed.\n1. INTRODUCTION\nMusical genres are categorical descriptions that are used to\ncharacterize music in music stores, radio stations and now on the\nInternet. Although the division of music into genres is somewhat\nsubjective and arbitrary there are perceptual criteria related to the\ntexture, instrumentation and rhythmic structure of music that can\nbe used to characterize a particular genre. Humans are remarkably\ngood at genre classification as investigated in [1] where it is\nshown that humans can accurately predict a musical genre based\non 250 milliseconds of audio.  This finding suggests that humans\ncan judge genre using only the musical surface without\nconstructing any higher level  theoretic descriptions as has been\nargued in [2]. Up to now genre classification for digitally\navailable music has been performed manually. Therefore\ntechniques for automatic genre classification would be a valuable\naddition to the development of audio information retrieval\nsystems for music.In this work, algorithms for automatic genre classification are\nexplored. A set of features for representing the music surface and\nrhythmic structure of audio signals is proposed. The performance\nof this feature set is evaluated by training statistical pattern\nrecognition classifiers using audio collections collected from\ncompact disks, radio and the web. Audio signals can be\nautomatically classified using a hierarchy of genres that can be\nrepresented as a tree with 15 nodes. Based on this automatic genre\nclassification and the extracted features two graphical user\ninterfaces for browsing and interacting with large digital music\ncollections have been developed. The feature extraction and\ngraphical update of the user interfaces is performed in real time\nand has been used to classify live radio signals.\n2. RELATED WORK\nAn early overview of audio information retrieval (AIR) (including\nspeech and symbolic music information retrieval) is given in [3].\nStatistical pattern recognition based on the extraction of spectral\nfeatures has been used to classify Music vs Speech [4], Isolated\nsounds [5, 6] and Instruments [7]. Features related to timbre\nrecognition have been explored in [8,9]. Extraction of\npsychoacoustic features related to music surface and their use for\nsimilarity judgements and high level semantic descriptions (like\nslow or loud) is explored in [10]. Content-based similarity\nretrieval from large collections of music is described in [11].\nAutomatic beat tracking systems have been proposed in [12, 13]\nand [14] describes a method for the automatic extraction of time\nindexes of occurrence of different percussive timbres from an\naudio signal. Musical genres can be quite subjective making\nautomatic classification difficult. The creation of a more objective\ngenre hierarchy for music information retrieval is discussed in\n[15]. Although the use of such a designed hierarchy would\nimprove classification results it is our belief that there is enough\nstatistical information to adequately characterize musical genre.\nAlthough manually annotated genre information has been used to\nevaluate content-based similarity retrieval algorithms to the best\nof our knowledge, there is no prior published work in automatic\ngenre classification.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies bear this notice and the\nfull citation on the first page.3. FEATURE EXTRACTION\n3.1 Musical Surface Features\nIn this work the term “musical surface” is used to denote the\ncharacteristics of music related to texture, timbre and\ninstrumentation.  The statistics of the spectral distribution over\ntime can be used in order to represent the “musical surface” for\npattern recognition purposes. The following 9-dimensional feature\nvector  is used in our system for this purpose: (mean-Centroid,\nmean-Rolloff, mean-Flux, mean-ZeroCrossings, std-Centroid,\nstd-Rolloff, std-Flux, std-ZeroCrossings,  LowEnegry).\nThe means and standard deviations of these features are calculated\nover a “texture” window of 1 second consisting of 40 “analysis”\nwindows of 20 milliseconds (512 samples at 22050 sampling\nrate). The feature calculation is based on the Short Time Fourier\nTransform (STFT). that can be efficiently calculated using the\nFast Fourier Transform (FFT) algorithm [16].\nThe following features are calculated for each “analysis” window:\n(M[f] is the magnitude of the FFT at frequency bin f and N the\nnumber of frequency bins):\n• Centroid :              \n∑∑=NN\nfMffM\nC\n11\n][][\n                    (1)\nThe Centroid is a measure of spectral brightness.\n• Rolloff :    is the value R such that :\n                                     ∑ ∑=R N\nfM fM\n1 1][85.0][     (2)\n        The rolloff is a measure of spectral shape.\n• Flux:                 ][][ fMfMFp−=               (3)\nwhere pMdenotes the FFT magnitude of the previous\nframe in time. Both magnitude vectors are normalized in\nenergy. Flux is a measure of spectral change.\n• ZeroCrossings: the number of time domain zerocrossings of\nthe signal. ZeroCrossings are useful to detect the amount of\nnoise in a signal.\n• LowEnergy: The percentage of “analysis” windows that\nhave energy less than the average energy of the “analysis”\nwindows over the “texture” window.\n3.2 Rhythm features\nThe calculation of features for representing the rhythmic structure\nof music is based on the Wavelet Transform (WT) which is a\ntechnique for analyzing signals that was developed as an\nalternative to the STFT. More specifically, unlike the STFT that\nprovides uniform time resolution for all frequencies the DWT\nprovides high time resolution for all frequencies, the DWT\nprovides high time resolution and low frequency resolution for\nhigh frequencies and high time and low frequency resolution for\nlow frequencies.The Discrete Wavelet Transform (DWT) is a special case of the\nWT that provides a compact representation of the signal in time\nand frequency that can be computed efficiently. The DWT\nanalysis can be performed using a fast, pyramidal algorithm\nrelated to multirate filterbanks [17]. An introduction to wavelets\ncan be found in [18].\nFor the purposes of this work, the DWT can be viewed as a\ncomputationally efficient way to calculate an octave\ndecomposition of the signal in frequency. More specifically the\nDWT can be viewed as a constant Q (bandwidth / center\nfrequency) with octave spacing between the centers of the filters.\nIn the pyramidal algorithm the signal is analyzed at different\nfrequency bands with different resolutions by decomposing the\nsignal into a coarse approximation and detail information. The\ncoarse approximation is then further decomposed using the same\nwavelet step. The decomposition is achieved by successive\nhighpass and lowpass filtering of the time domain signal and is\ndefined by the following equations:\n∑ − =\nnhigh nkgnxky ]2[][][    (4)\n∑ − =\nnlow nkhnxky ]2[][][      (5)\nwhere ][],[kykylow high are the output of the highpass (g) and\nlowpass (h) filters, respectively after subsampling by two. The\nDAUB4 filters proposed by Daubechies [19] are used.\nThe rhythm feature set is based on detecting the most salient\nperiodicities of the signal. Figure I shows the flow diagam of the\nbeat analysis. The signal is first decomposed into a number of\noctave frequency bands using the DWT. Following this\ndecomposition the time domain amplitude envelope of each band\nis extracted separately. This is achieved by applying full wave\nrectification, low pass filtering and downsampling to each band.\nThe envelopes of each band are then summed together and an\nautocorrelation function is computed. The peaks of the\nautocorrelation function correspond to the various periodicities of\nthe signal’s envelope. These stages are given by the equations:\n1. Full Wave Rectification (FWR):\n])[(][ nxabsny=                       (6)\n2. Low Pass Filtering (LPF): (One Pole filter with an alpha\nvalue of 0.99) i.e:\n][][)1(][ nynx ny αα−−=        (7)\n3. Downsampling (↓↓) by k (k=16 in our implementation):\n][][knxny=                                    (8)\n4. Normalization (NR) (mean removal):\n]][[][][ nxEnxny −=                  (9)FW WT\nFig. I Beat analysis flow diagram\n5. Autocorrelation  (AR) (computed using the FFT for\nefficiency) :\n∑ + =\nnknxnxNny ][][1][           (10)\nThe first five peaks of the autocorrelation function are detected\nand their corresponding periodicities in beats per minute (bpm)\nare calculated and added in a “beat” histogram. This process is\nrepeating by iterating over the signal and accumulating the\nperiodicities in the histogram. A window size of 65536 samples at\n22050 Hz sampling rate with a hop size of 4096 samples is used.\nThe prominent peaks of the final histogram correspond to the\nvarious periodicities of the audio signal and are used as the basis\nfor the rhythm feature calculation.\nThe following features based on the “beat” histogram are used:\n1. Period0: Periodicity in bpm of the first peak Period0\n2. Amplitude0: Relative amplitude (divided by sum of\namplitudes) of the first peak.\n3. RatioPeriod1: Ratio of periodicity of second peak to the\nperiodicity of the first peak\n4. Amplitude1: Relative amplitude of second peak.\n5. RatioPeriod2, Amplitude2, RatioPeriod3, Amplitude3\nThese features represent the strength of  beat (“beatedness”) of the\nsignal and the relations between the prominent periodicities of the\nsignal. This feature vector carries more information than\ntraditional beat tracking systems [11, 12] where a single measure\nof the beat corresponding to the tempo and its strength are used.\nFigure II shows the “beat” histograms of two classical music\npieces and two modern pop music pieces. The fewer and stronger\npeaks of the two pop music histograms indicate the strong\npresence of a regular beat unlike the distributed weaker peaks of\nclassical music.\nFig. II    Beat Histograms for Classical (left) and Pop (right)\nThe 8-dimensional feature vector used to represent rhythmic\nstructure and strength is combined with the 9-dimensional musical\nsurface feature vector to form a 17-dimensional feature vector that\nis used for automatic genre classification.\n4. CLASSIFICATION\nTo evaluate the performance of the proposed feature set, statistical\npattern recognition classifiers were trained and evaluated using\ndata sets collected from radio, compact disks and the Web. Figure\nIII shows the classification hierarchy used for the experiments.\nFor each node in the tree of Figure III, a Gaussian classifier was\ntrained using a dataset of 50 samples (each 30 seconds long).\nUsing the Gaussian classifier each class is represented as a single\nmultidimensional Gaussian distribution with parameters estimated\nfrom the training dataset [20]. The full digital audio data\ncollection consists of 15 genres * 50 files * 30 seconds  = 22500\nseconds (i.e 6.25 hours of audio).\nFor the Musical Genres (Classical, Country…..)  the combined\nfeature set described in this paper was used. For the Classical\nGenres (Orchestra, Piano…) and for the Speech Genres\n(MaleVoice, FemaleVoice…) mel-frequency cepstral coefficients\n[21] (MFCC) were used. MFCC are perceptually motivated\nfeatures commonly used in speech recognition research. In a\nsimilar fashion to the Music Surface features, the means and\nstandard deviations of the first five MFCC over a larger texture\nwindow (1 second long) were calculated. MFCCs can also be used\nin place of the STFT-based music surface features with similar\nclassification results. The use of MFCC as features for classifying\nmusic vs speech has been explored in [22].\nThe speech genres were added to the genre classification\nhierarchy so that the system could be used to classify live radio\nsignals in real time. “Sports announcing” refers to any type of\nspeech over noisy background.WT\nWT\nWTFW\nFW\nFWLPF\nLPF\nLPF\nLPFN\nN\nN\nAR PKP HSTN                                  Music\n      Classical    Country  Disco  HipHop  Jazz  Rock\n    Orchestra Piano Choir StringQuartet\n                                  Speech\n MaleVoice,  FemaleVoice,  SportsAnnouncing\nFig. III   Genre Classification Hierarchy\nTable 1.   Classification accuracy percentage results\nMusicSpeechGenresVoicesClassical\nRandom 50 16    33 25\nGaussian 86 62 74 76\nTable 1. summarizes the classification results as pecentages of\nclassification accuracy. In all cases the results are significantly\nbetter than random classification. These classification results are\ncalculated using a 10-fold evaluation strategy where the\nevaluation data set is randomly partitioned so that 10% is used for\ntesting and 90% for training. The process is iterated with different\nrandom partitions  and the results are averaged (in the evaluation\nof Table.1 one hundred iterations where used).\nTable 2. shows more detailed information about the genre\nclassifier performance in the form of a confusion matrix. The\ncolumns correspond to the actual genre and the rows to the\npredicted genre. For example the cell of row 2, column 1 with\nvalue 0.01 means that 1 percent of the Classical music (column 1)\nwas wrongly classified as Country music (row 2). The percentages\nof correct classifications lie in the diagonal of the confusion\nmatrix. The best predicted genres are classical and hiphop while\nthe worst predicted are jazz and rock. This is due to the fact that\nthe jazz and rock are very broad categories and their boundaries\nare more fuzzy than classical or hiphop.\nTable 3. shows more detailed information about the classical\nmusic classifier performance in the form of a confusion matrix..classiccountryDiscoHiphopjazzRock\nclassic86 2 0 4 18 1\ncountry1 57 5 1 12 13\ndisco0 6 55 4 0 5\nHiphop0 15 28 90 4 18\nJazz7 1 0 0 .3712\nRock6 19 11 0 27 48\nTable 2.   Genre classification confusion matrix\nchoralorchestralPiano string 4tet\nchoral 99 10 16 12\norchestral0 53 2 5\npiano 1 20 75 3\nstring 4tet0 17 7 80\nTable 2.   Classical music classification confusion matrix\nFig. IV Relative feature set importance\nFigure IV shows the relative importance of the “musical surface”\nand “rhythm” feature sets for the automatic genre classification.\nAs expected both feature sets perform better than random and\ntheir combination improves the classification accuracy. The genre\nlabeling was based on the artist or the compact disk that contained\nthe excerpt. In some cases this resulted in outliers that are one of\nthe sources of prediction error. For example the Rock collection\ncontains songs by Sting that are more close to Jazz than Rock\neven for a human listener. Similarly the Jazz collection contains\nsongs with string accompaniment and no rhythm section that\nsound like Classical music. It is likely that replacing these outliers\nwith more characteristic pieces would improve the genre\nclassification results.0102030405060\nRandomRandom\nRhythm\nMusicSurface\nFullFig. IV GenreGram\n5. USER INTERFACES\nTwo new graphical user interfaces for browsing and interacting\nwith collections of audio signals have been developed (Figure\nIV,V) . They are based on the extracted feature vectors and the\nautomatic genre classification results.\n• GenreGram is a dynamic real-time audio display for\nshowing automatic genre classification results. Each genre is\nrepresented as a cylinder that moves up and down in real\ntime based on a classification confidence measure ranging\nfrom 0.0 to 1.0. Each cylinder is texture-mapped with a\nrepresentative image of each category. In addition to being a\nnice demonstration of automatic real time audio\nclassification, the GenreGram gives valuable feedback both\nto the user and the algorithm designer. Different\nclassification decisions and their relative strengths are\ncombined visually, revealing correlations and classification\npatterns. Since the boundaries between musical genres are\nfuzzy, a display like this is more informative than a single\nclassification decision. For example, most of the time a rap\nsong will trigger Male Voice, Sports Announcing and\nHipHop. This exact case is shown in Figure IV.\n• GenreSpace is a tool for visualizing large sound collections\nfor browsing. Each audio file is represented a single point in\na 3D space. Principal Component Analysis (PCA) [23] is\nused to reduce the dimensionality of the feature vector\nrepresenting the file to the 3-dimensional feature vector\ncorresponding to the point coordinates. Coloring of the\npoints is based on the automatic genre classification. The\nuser can zoom, rotate and scale the space to interact with the\ndata. The GenreSpace also represents the relative similarity\nwithin genres by the distance between points. A principal\ncurve [24] can be used to move sequentially through the\npoints in a way that preserves the local clustering\ninformation.\nFig. V GenreSpace\n6. FUTURE WORK\nAn obvious direction for future research is to expand the genre\nhierarchy both in width and depth. The combination of\nsegmentation [25] with automatic genre classification could\nprovide a way to browse audio to locate regions of interest.\nAnother interesting direction is the combination of the graphical\nuser interfaces described with automatic similarity retrieval that\ntakes into account the automatic genre classification. In its current\nform the beat analysis algorithm can not be performed in real time\nas it needs to collect information from the whole signal. A real\ntime version of beat analysis is planned for the future. It is our\nbelief that more rhythmic information can be extracted from audio\nsignals and we plan to investigate the ability of the beat analysis\nto detect rhythmic structure in synthetic stimuli.\n7. SUMMARY\nA feature set for representing music surface and rhythm\ninformation was proposed and used to build automatic genre\nclassification algorithms. The performance of the proposed data\nset was evaluated by training statistical pattern recognition\nclassifiers on real-world data sets. Two new graphical user\ninterfaces based on the extracted feature set and the automatic\ngenre classification were developed.\nThe software used for this paper is available as part of  MARSYAS\n[26] a  software framework for rapid development of computer\naudition application written in C++ and JAVA. It is available as\nfree software under the Gnu Public License (GPL) at:\n http://www.cs.princeton.edu/~gtzan/marsyas.html\n8. ACKNOWLEDGEMENTS\nThis work was funded under NSF grant 9984087 and from gifts\nfrom Intel and Arial Foundation. Douglas Turnbull helped with\nthe implementation of the GenreGram. The authors would also\nlike to thank the anonymous reviewers for their valuable\nfeedback.9. REFERENCES\n[1] Perrot, D., and Gjerdigen, R.O. Scanning the dial: An\nexploration of factors in the identification of musical style. In\nProceedings of the 1999 Society for Music Perception and\nCognition pp.88(abstract)\n[2] Martin, K.,D., Scheirer, E.D., Vercoe, B., L. Musical content\nanalysis through models of audition. In Proceedings of the\n1998 ACM Multimedia Workshop on Content-Based\nProcessing of Music.\n[3] Foote, J. An overview of audio information retrieval.\nMultimedia Systems 1999. 7(1), 42-51.\n[4] Scheirer, E. D. and Slaney, M. Construction and evaluation\nof a robust multifeature speech/music discriminator. In\nProceedings of the 1997 International Conference on\nAcoustics, Speech, and Signal Processing, 1331-1334.\n[5] Wold, E., Blum, T., Keislar, D., and Wheaton, J. Content –\nbased classification, search and retrieval of audio. IEEE\nMultimedia, 1996 3 (2)\n[6] Foote, J., Content-based retrieval of music and audio. In\nMultimedia Storage and Archiving Systems II, 1997 138 -147\n[7] Martin, K. Sound-Source Recognition: A theory and\ncomputational model. PhD thesis, MIT Media Lab.\nhttp://sound.media.mit.edu/~kdm\n[8] Rossignol, S et al. Feature extraction and temporal\nsegmentation of acoustic signals. In Proceedings of\nInternational Computer Music Conference (ICMC), 1998.\n[9] Dubnov, S., Tishby, N., and Cohen, D. Polyspectra as\nmeasures of sound and texture. Journal of New Music\nResearch, vol. 26 1997.\n[10] Scheirer, E. Music Listening Systems. Phd thesis., MIT\nMedia Lab: http://sound.media.mit.edu/~eds\n[11] Welsh, M., Borisov, N., Hill, J., von Behren, R., and Woo,\nA. Querying large collections of music for similarity.\nTechnical Report UCB/CSD00-1096, U.C Berkeley,\nComputer Science Division, 1999.\n[12] Scheirer, E. Tempo and beat analysis of acoustic musical\nsignals. Journal of the Acoustical Society of America\n103(1):588-601.\n[13] Goto, M. and Muraoka, Y. Music understanding at the beat\nlevel: real time beat tracking for audio signals.In D.F Rosenthal and H. Okuno (ed.), Readings in\nComputational Auditory Scene Analysis 156-176.\n[14] Gouyon, F., Pachet, F. and Delerue, O. On the use of zero-\ncrossing rate for an application of classification of percussive\nsounds. Proceedings of the COST G-6 conference on Digital\nAudio Effects (DAFX-00), Verona, Italy, 2000.\n[15] Pachet, F., Cazaly, D. “A classification of  musical genre”,\nContent-Based Multimedia Information Access (RIA)\nConference, Paris, March 2000.\n[16] Oppenheim, A. and Schafer, R. Discrete-Time Signal\nProcessing. Prentice Hall. Edgewood Cliffs, NJ. 1989.\n[17] Mallat, S, G. A theory for multiresolution signal\ndecomposition: The Wavelet representation. IEEE\nTransactions on Pattern Analysis and Machine\nIntelligence,1989, 11, 674-693.\n[18] Mallat, S,G. A wavelet tour of signal processing. Academic\nPress 1999.\n[19] Daubechies, I. Orthonormal bases of compactly supported\nwavelets. Communications on Pure and Applied Math.1988.\nvol.41, 909-996.\n[20] Duda, R. and Hart, P. Pattern classification and scene\nanalysis. John Willey & Sons. 1973.\n[21] Hunt, M., Lennig, M., and Mermelstein, P. Experiments in\nsyllable-based recognition of continuous speech. In\nProceedings of International Conference on Acoustics,\nSpeech and Signal Processing, 1996, 880-883.\n[22] Logan, B. Mel Frequency Cepstral Coefficients for music\nmodeling. Read at the first International Symposium on\nMusic Information Retrieval..\nhttp://ciir.cs.umass.edu/music2000\n[23] Jollife, L. Principal component analysis. Spring Verlag,\n1986.\n[24] Herman, T, Meinicke, P., and Ritter, H. Principal curve\nsonification. In Proceedings of International Conference on\nAuditory Display. 2000.\n[25] Tzanetakis, G. and Cook, P. Mutlifeature audio segmentation\nfor browsing and annotation. In Proceedings of IEEE\nWorkshop on Applications of Signal Processing to Audio\nand Acoustics. 1999.\n[26] Tzanetakis, G. and Cook, P. MARSYAS: a framework for\naudio analysis. Organised Sound 2000 . 4(3)"
    }
]