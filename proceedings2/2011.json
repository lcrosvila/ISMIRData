[
    {
        "title": "Iranian Traditional Music Dastgah Classification.",
        "author": [
            "Sajjad Abdoli"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417425",
        "url": "https://doi.org/10.5281/zenodo.1417425",
        "ee": "https://zenodo.org/records/1417425/files/Abdoli11.pdf",
        "abstract": "In this study, a system for Iranian traditional music Dastgah classification is presented. Persian music is based upon a set of seven major Dastgahs. The Dastgah in Persian music is similar to western musical scales and also Maqams in Turkish and Arabic music. Fuzzy logic type 2 as the basic part of our system has been used for modeling the uncertainty of tuning the scale steps of each Dastgah. The method assumes each performed note as a Fuzzy Set (FS), so each musical piece is a set of FSs. The maximum similarity between this set and theoretical data indicates the desirable Dastgah. In this study, a collection of small-sized dataset for Persian music is also given. The results indicate that the system works accurately on the dataset.",
        "zenodo_id": 1417425,
        "dblp_key": "conf/ismir/Abdoli11",
        "keywords": [
            "Dastgah classification",
            "Iranian traditional music",
            "Fuzzy logic type 2",
            "uncertainty modeling",
            "scale tuning",
            "Maqams",
            "Persian music",
            "seven major Dastgahs",
            "Fuzzy Set (FS)",
            "theoretical data"
        ]
    },
    {
        "title": "Modeling Musical Attributes to Characterize Two-Track Recordings with Bass and Drums.",
        "author": [
            "Jakob Abe\u00dfer",
            "Olivier Lartillot"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417529",
        "url": "https://doi.org/10.5281/zenodo.1417529",
        "ee": "https://zenodo.org/records/1417529/files/AbesserL11.pdf",
        "abstract": "In this publication, we present a method to characterize twotrack audio recordings (bass and drum instruments) based on musical attributes. These attributes are modelled using different regression algorithms. All regression models are trained based on score-based audio features computed from given scores and human annotations of the attributes. We compare five regression model configurations that predict values of different attributes. The regression models are trained based on manual annotations from 11 participants for a data-set of 70 double-track recordings. The average estimation errors within a cross-validation scenario are computed as evaluation measure. Models based on Partial Least Squares Regression (PLSR) with preceding Principal Component Analysis (PCA) and on Support Vector Regression (SVR) performed best.",
        "zenodo_id": 1417529,
        "dblp_key": "conf/ismir/AbesserL11",
        "keywords": [
            "audio recordings",
            "bass and drum instruments",
            "musical attributes",
            "regression algorithms",
            "score-based audio features",
            "human annotations",
            "cross-validation scenario",
            "evaluation measure",
            "Partial Least Squares Regression",
            "Principal Component Analysis"
        ]
    },
    {
        "title": "Compression-based Similarity Measures in Symbolic, Polyphonic Music.",
        "author": [
            "Teppo E. Ahonen",
            "Kjell Lemstr\u00f6m",
            "Simo Linkola"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414816",
        "url": "https://doi.org/10.5281/zenodo.1414816",
        "ee": "https://zenodo.org/records/1414816/files/AhonenLL11.pdf",
        "abstract": "We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks.",
        "zenodo_id": 1414816,
        "dblp_key": "conf/ismir/AhonenLL11",
        "keywords": [
            "binary chromagrams",
            "tonal centroids",
            "quantizing",
            "Normalized Compression Distance (NCD)",
            "comparing music sequences",
            "symbolic music",
            "polyphonic music",
            "comprehensively evaluated",
            "test set",
            "classical music variations"
        ]
    },
    {
        "title": "Music Genre Classification using Similarity Functions.",
        "author": [
            "Yoko Anan",
            "Kohei Hatano",
            "Hideo Bannai",
            "Masayuki Takeda"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418019",
        "url": "https://doi.org/10.5281/zenodo.1418019",
        "ee": "https://zenodo.org/records/1418019/files/AnanHBT11.pdf",
        "abstract": "We consider music classification problems. A typical machine learning approach is to use support vector machines with some kernels. This approach, however, does not seem to be successful enough for classifying music data in our experiments. In this paper, we follow an alternative approach. We employ a (dis)similarity-based learning framework proposed by Wang et al. This (dis)similarity-based approach has a theoretical guarantee that one can obtain accurate classifiers using (dis)similarity measures under a natural assumption. We demonstrate the effectiveness of our approach in computational experiments using Japanese MIDI data.",
        "zenodo_id": 1418019,
        "dblp_key": "conf/ismir/AnanHBT11",
        "keywords": [
            "music classification",
            "support vector machines",
            "kernels",
            "Japanese MIDI data",
            "dis/similarity-based learning",
            "Wang et al.",
            "accuracy",
            "computational experiments",
            "natural assumption",
            "effective"
        ]
    },
    {
        "title": "Multiscale Scattering for Audio Classification.",
        "author": [
            "Joakim And\u00e9n",
            "St\u00e9phane Mallat"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415750",
        "url": "https://doi.org/10.5281/zenodo.1415750",
        "ee": "https://zenodo.org/records/1415750/files/AndenM11.pdf",
        "abstract": "Mel-frequency cepstral coefficients (MFCCs) are efficient audio descriptors providing spectral energy measurements over short time windows of length 23 ms. These measurements, however, lose non-stationary spectral information such as transients or time-varying structures. It is shown that this information can be recovered as spectral co-occurrence coefficients. Scattering operators compute these coefficients with a cascade of wavelet filter banks and modulus rectifiers. The signal can be reconstructed from scattering coefficients by inverting these wavelet modulus operators. An application to genre classification shows that second-order cooccurrence coefficients improve results obtained by MFCC and Delta-MFCC descriptors. 1",
        "zenodo_id": 1415750,
        "dblp_key": "conf/ismir/AndenM11",
        "keywords": [
            "Mel-frequency cepstral coefficients (MFCCs)",
            "spectral energy measurements",
            "non-stationary spectral information",
            "transients",
            "time-varying structures",
            "spectral co-occurrence coefficients",
            "wavelet filter banks",
            "modulus rectifiers",
            "signal reconstruction",
            "genre classification"
        ]
    },
    {
        "title": "Humming Method for Content-Based Music Information Retrieval.",
        "author": [
            "Cristina de la Bandera",
            "Ana M. Barbancho",
            "Lorenzo J. Tard\u00f3n",
            "Simone Sammartino",
            "Isabel Barbancho"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416610",
        "url": "https://doi.org/10.5281/zenodo.1416610",
        "ee": "https://zenodo.org/records/1416610/files/BanderaBTSB11.pdf",
        "abstract": "In this paper a humming method for music information retrieval is presented. The system uses a database with real songs and does not need another type of symbolic representation of them. The system employs an original fingerprint based on chroma vectors to characterize the humming and the references songs. With this fingerprint, it is possible to get the hummed songs without needed of transcription of the notes of the humming or of the songs. The system showed a good performance on Pop/Rock and Spanish folk music.",
        "zenodo_id": 1416610,
        "dblp_key": "conf/ismir/BanderaBTSB11",
        "keywords": [
            "humming method",
            "music information retrieval",
            "database with real songs",
            "chroma vectors",
            "fingerprint based",
            "characterize humming",
            "references songs",
            "transcription of notes",
            "Pop/Rock",
            "Spanish folk music"
        ]
    },
    {
        "title": "Ethnographic Observations of Musicologists at the British Library: Implications for Music Information Retrieval.",
        "author": [
            "Mathieu Barthet",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415670",
        "url": "https://doi.org/10.5281/zenodo.1415670",
        "ee": "https://zenodo.org/records/1415670/files/BarthetD11.pdf",
        "abstract": "Without a rich understanding of user behaviours and needs, music information retrieval (MIR) systems might not be ideally suited to their potential users. In this study, we followed an ethnographic methodology to elicit some of the strategies used by musicologists to explore and document musical performances, in order to investigate if and how technologies could enhance such a process. Observations of musicologists studying historical recordings of classical music were conducted at the British Library. The observations show that the musicologists alternate between a closed listening practice, relying exclusively on aural observations, and a multimodal listening practice, where they interact with various music representations and information sources using different media (e.g. metadata about the recordings and performers, sound visualisations, scores, lyrics and performance videos). The spoken parts of broadcast recordings brought historical/extra-musical clues helping to understand music performance practices. Sound visualisation and computational methods fostered the analysis of specific musical expression patterns. We suggest that software designed for musicologists should facilitate switching between closed and multimodal listening modes, interaction with scores and lyrics, and analysis and annotation of speech and music performance using content-based MIR techniques.",
        "zenodo_id": 1415670,
        "dblp_key": "conf/ismir/BarthetD11",
        "keywords": [
            "music information retrieval",
            "ethnographic methodology",
            "musicologists",
            "historical recordings",
            "multimodal listening practice",
            "closed listening practice",
            "sound visualisation",
            "computational methods",
            "analysis and annotation",
            "content-based MIR techniques"
        ]
    },
    {
        "title": "Urgency Analysis of Audible Alarms in The Operating Room.",
        "author": [
            "Christopher Bennett",
            "Richard McNeer",
            "Colby Leider"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416602",
        "url": "https://doi.org/10.5281/zenodo.1416602",
        "ee": "https://zenodo.org/records/1416602/files/BennettML11.pdf",
        "abstract": "Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms [1\u20134]. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval community, and then we examine how those auditory features correlate to listeners\u2019 perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgency in sound. These results could inform the development of a novel standard regarding the composition of medical audible alarms.",
        "zenodo_id": 1416602,
        "dblp_key": "conf/ismir/BennettML11",
        "keywords": [
            "recent studies",
            "deficiency in performance",
            "medically related audible alarms",
            "alarm fatigue",
            "clinical setting",
            "care providers",
            "auditory features",
            "music information retrieval community",
            "listeners perception of urgency",
            "novel standard regarding the composition of medical audible alarms"
        ]
    },
    {
        "title": "The Million Song Dataset.",
        "author": [
            "Thierry Bertin-Mahieux",
            "Daniel P. W. Ellis",
            "Brian Whitman",
            "Paul Lamere"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.3860557",
        "url": "https://doi.org/10.5281/zenodo.3860557",
        "ee": "http://ismir2011.ismir.net/papers/OS6-1.pdf",
        "abstract": "This upload includes the ESSENTIA analysis output of (a subset of) song snippets from the Million Song Dataset, namely those included in the Taste Profile subset. The audio snippets were collected from 7digital.com and were subsequently analyzed with ESSENTIA 2.1-beta3. Pre-trained SVM models provided by the ESSENTIA authors on their website were applied.\n\nThe file msd_song_jsons.rar contains the ESSENTIA analysis output after applying the SVM models for highlevel feature extraction. Please note that these are 204317 files.\n\nThe file msd_played_songs_essentia.csv.gz contains all one-dimensional real-valued fields of the jsons merged into one csv file with 204317 rows.\n\nThe full procedure and subsequent analysis is described in\n\nFricke, K. R., Greenberg, D. M., Rentfrow, P. J.,  Herzberg, P. Y. (2019). Measuring musical preferences from listening behavior: Data from one million people and 200,000 songs. Psychology of Music, 0305735619868280.",
        "zenodo_id": 3860557,
        "dblp_key": "conf/ismir/Bertin-MahieuxEWL11",
        "keywords": [
            "ESSENTIA analysis output",
            "subset of song snippets",
            "Taste Profile subset",
            "audio snippets collected",
            "ESSENTIA 2.1-beta3",
            "pre-trained SVM models",
            "high-level feature extraction",
            "merged csv file",
            "one-dimensional real-valued fields",
            "204317 files"
        ]
    },
    {
        "title": "Methodology and Resources for The Structural Segmentation of Music Pieces into Autonomous and Comparable Blocks.",
        "author": [
            "Fr\u00e9d\u00e9ric Bimbot",
            "Emmanuel Deruty",
            "Gabriel Sargent",
            "Emmanuel Vincent 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417611",
        "url": "https://doi.org/10.5281/zenodo.1417611",
        "ee": "https://zenodo.org/records/1417611/files/BimbotDSV11.pdf",
        "abstract": "The approach called decomposition into autonomous and comparable blocks specifies a methodology for producing music structure annotation by human listeners based on a set of criteria relying on the listening experience of the human annotator [12]. The present article develops further a number of fundamental notions and practical issues, so as to facilitate the usability and the reproducibility of the approach. We formalize the general methodology as an iterative process which aims at estimating both a structural metric pattern and its realization, by searching empirically for an optimal compromise describing the organization of the content of the music piece in the most economical way, around a typical timescale. Based on experimental observations, we detail some practical considerations and we illustrate the method by an extensive case study. We introduce a set of 500 songs for which we are releasing freely the structural annotations to the research community, for examination, discussion and utilization. 1",
        "zenodo_id": 1417611,
        "dblp_key": "conf/ismir/BimbotDSV11",
        "keywords": [
            "decomposition",
            "autonomous",
            "comparable",
            "blocks",
            "music",
            "structure",
            "annotation",
            "human",
            "listeners",
            "criteria"
        ]
    },
    {
        "title": "How Much Metadata Do We Need in Music Recommendation? A Subjective Evaluation Using Preference Sets.",
        "author": [
            "Dmitry Bogdanov",
            "Perfecto Herrera"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415104",
        "url": "https://doi.org/10.5281/zenodo.1415104",
        "ee": "https://zenodo.org/records/1415104/files/BogdanovH11.pdf",
        "abstract": "In this work we consider distance-based approaches to music recommendation, relying on an explicit set of music tracks provided by the user as evidence of his/her music preferences. Firstly, we propose a purely content-based approach, working on low-level (timbral, temporal, and tonal) and inferred high-level semantic descriptions of music. Secondly, we consider its simple refinement by adding a minimum amount of genre metadata. We compare the proposed approaches with one content-based and three metadata-based baselines. As such, we consider content-based approach working on inferred semantic descriptors, a tag-based recommender exploiting artist tags, a commercial black-box recommender partially employing collaborative filtering information, and a simple genre-based random recommender. We conduct a listening experiment with 19 participants. The obtained results reveal that although the low-level/semantic content-based approach does not achieve the performance of the baseline working exclusively on the inferred semantic descriptors, the proposed refinement provides significant improvement in the listeners\u2019 satisfaction comparable with metadata-based approaches, and surpasses these approaches by the number of novel relevant recommendations. We conclude that the proposed content-based approach refined by simple genre metadata is suited for music discovery not only in the long-tail but also within popular music items.",
        "zenodo_id": 1415104,
        "dblp_key": "conf/ismir/BogdanovH11",
        "keywords": [
            "distance-based approaches",
            "explicit music tracks",
            "content-based approach",
            "low-level timbral",
            "temporal and tonal",
            "inferred high-level semantic",
            "genre metadata",
            "simple refinement",
            "metadata-based approaches",
            "novel relevant recommendations"
        ]
    },
    {
        "title": "Neo-Riemannian Cycle Detection with Weighted Finite-State Transducers.",
        "author": [
            "Jonathan Bragg",
            "Elaine Chew",
            "Stuart M. Shieber"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416152",
        "url": "https://doi.org/10.5281/zenodo.1416152",
        "ee": "https://zenodo.org/records/1416152/files/BraggCS11.pdf",
        "abstract": "This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finitestate transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications.",
        "zenodo_id": 1416152,
        "dblp_key": "conf/ismir/BraggCS11",
        "keywords": [
            "finite-state model",
            "harmonic cycles",
            "neo-Riemannian theorists",
            "triads",
            "harmonic analysis",
            "high accuracy",
            "weighted finitestate transducers",
            "dataset",
            "Franz Schubert",
            "ground truth"
        ]
    },
    {
        "title": "Musical Influence Network Analysis and Rank of Sample-Based Music.",
        "author": [
            "Nicholas J. Bryan",
            "Ge Wang 0002"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415768",
        "url": "https://doi.org/10.5281/zenodo.1415768",
        "ee": "https://zenodo.org/records/1415768/files/BryanW11.pdf",
        "abstract": "Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results.",
        "zenodo_id": 1415768,
        "dblp_key": "conf/ismir/BryanW11",
        "keywords": [
            "Computational analysis",
            "musical influence networks",
            "sample-based music",
            "WhoSampled.com dataset",
            "artist-to-artist relationships",
            "sampled material origins",
            "song-by-song basis",
            "social network metrics",
            "quantify trends and characteristics",
            "collapse-and-sum approach"
        ]
    },
    {
        "title": "Using Sequence Alignment and Voting to Improve Optical Music Recognition from Multiple Recognizers.",
        "author": [
            "Esben Paul Bugge",
            "Kim Lundsteen Juncher",
            "Brian S\u00f8borg Mathiasen",
            "Jakob Grue Simonsen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418175",
        "url": "https://doi.org/10.5281/zenodo.1418175",
        "ee": "https://zenodo.org/records/1418175/files/BuggeJMS11.pdf",
        "abstract": "Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third party OMR tools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is significantly better than only 2 of the 4 commercial recognizers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial improvements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/",
        "zenodo_id": 1418175,
        "dblp_key": "conf/ismir/BuggeJMS11",
        "keywords": [
            "Digitalizing",
            "Optical Music Recognition",
            "Pitch",
            "Duration",
            "Error-correction",
            "Sequence alignment",
            "Multiple sequence alignment",
            "Voting",
            "Open source",
            "Google Code"
        ]
    },
    {
        "title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.",
        "author": [
            "John Ashley Burgoyne",
            "Jonathan Wild",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417547",
        "url": "https://doi.org/10.5281/zenodo.1417547",
        "ee": "https://zenodo.org/records/1417547/files/BurgoyneWF11.pdf",
        "abstract": "Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training data\u2014both in terms of quantity and range of sampling\u2014has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard \u201cHot 100\u201d chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable significant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary figures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research.",
        "zenodo_id": 1417547,
        "dblp_key": "conf/ismir/BurgoyneWF11",
        "keywords": [
            "audio chord recognition",
            "training data",
            "jazz musicians",
            "Billboard Hot 100",
            "time-aligned transcriptions",
            "harmony",
            "complete information",
            "meter",
            "phrase",
            "larger musical structure"
        ]
    },
    {
        "title": "Identifying Emotion Segments in Music by Discovering Motifs in Physiological Data.",
        "author": [
            "Rafael Cabredo",
            "Roberto Sebastian Legaspi",
            "Masayuki Numao"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416872",
        "url": "https://doi.org/10.5281/zenodo.1416872",
        "ee": "https://zenodo.org/records/1416872/files/CabredoLN11.pdf",
        "abstract": "Music can induce different emotions in people. We propose a system that can identify music segments which induce specific emotions from the listener. The work involves building a knowledge base with mappings between affective states (happiness, sadness, etc.) and music features (rhythm, chord progression, etc.). Building this knowledge base requires",
        "zenodo_id": 1416872,
        "dblp_key": "conf/ismir/CabredoLN11",
        "keywords": [
            "Music",
            "emotions",
            "listener",
            "system",
            "identify",
            "mapping",
            "features",
            "knowledge",
            "base",
            "work"
        ]
    },
    {
        "title": "Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-by-Example Applications.",
        "author": [
            "Vijay Chandrasekhar 0001",
            "Matt Sharifi",
            "David A. Ross"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415260",
        "url": "https://doi.org/10.5281/zenodo.1415260",
        "ee": "https://zenodo.org/records/1415260/files/ChandrasekharSR11.pdf",
        "abstract": "We survey and evaluate popular audio fingerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance, size of fingerprints generated compared to size of audio probe, and transmission delay if the fingerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications.",
        "zenodo_id": 1415260,
        "dblp_key": "conf/ismir/ChandrasekharSR11",
        "keywords": [
            "audio fingerprinting schemes",
            "common framework",
            "cell phones",
            "Receiver Operating Characteristic (ROC)",
            "fingerprint size",
            "wireless link",
            "mobile applications",
            "transmission delay",
            "latency",
            "evaluation results"
        ]
    },
    {
        "title": "Music Structural Segmentation by Combining Harmonic and Timbral Information.",
        "author": [
            "Ruofeng Chen",
            "Ming Li"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414774",
        "url": "https://doi.org/10.5281/zenodo.1414774",
        "ee": "https://zenodo.org/records/1414774/files/ChenL11.pdf",
        "abstract": "We propose a novel model for music structural segmentation aiming at combining harmonic and timbral information. We use two-level clustering with splitting initialization and random turbulence to produce segment labels using chroma and MFCC separately as feature. We construct a score matrix to combine segment labels from both aspects. Finally Nonnegative Matrix Factorization and Maximum Likelihood are applied to extract the final segment labels. By comparing sparseness, our method is capable of automatically determining the number of segment types in a given song. The pairwise F-measure of our algorithm can reach 0.63 without rules of music knowledge, running on 180 Beatles songs. We show our model can be easily associated with more sophisticated structural segmentation algorithms and extended to probabilistic models.",
        "zenodo_id": 1414774,
        "dblp_key": "conf/ismir/ChenL11",
        "keywords": [
            "harmonic",
            "timbral",
            "segmentation",
            "chroma",
            "MFCC",
            "feature",
            "Nonnegative Matrix Factorization",
            "Maximum Likelihood",
            "sparseness",
            "pairwise F-measure"
        ]
    },
    {
        "title": "An Acoustic-Phonetic Approach to Vocal Melody Extraction.",
        "author": [
            "Yu-Ren Chien",
            "Hsin-Min Wang",
            "Shyh-Kang Jeng"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416450",
        "url": "https://doi.org/10.5281/zenodo.1416450",
        "ee": "https://zenodo.org/records/1416450/files/ChienWJ11.pdf",
        "abstract": "This paper addresses the problem of extracting vocal melodies from polyphonic audio. In short-term processing, a timbral distance between each pitch contour and the space of human voice is measured, so as to isolate any vocal pitch contour. Computation of the timbral distance is based on an acousticphonetic parametrization of human voiced sound. Longterm processing organizes short-term procedures in such a manner that relatively reliable melody segments are determined first. Tested on vocal excerpts from the ADC 2004 dataset, the proposed system achieves an overall transcription accuracy of 77%.",
        "zenodo_id": 1416450,
        "dblp_key": "conf/ismir/ChienWJ11",
        "keywords": [
            "vocal melodies",
            "polyphonic audio",
            "timbral distance",
            "pitch contour",
            "human voice",
            "acousticphonetic parametrization",
            "voiced sound",
            "longterm processing",
            "melody segments",
            "ADC 2004 dataset"
        ]
    },
    {
        "title": "A Feature Smoothing Method for Chord Recognition Using Recurrence Plots.",
        "author": [
            "Taemin Cho",
            "Juan Pablo Bello"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417557",
        "url": "https://doi.org/10.5281/zenodo.1417557",
        "ee": "https://zenodo.org/records/1417557/files/ChoB11.pdf",
        "abstract": "In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (\u201cchordsize\u201d) repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder.",
        "zenodo_id": 1417557,
        "dblp_key": "conf/ismir/ChoB11",
        "keywords": [
            "chord recognition tasks",
            "repeated patterns within a song",
            "feature smoothing technique",
            "chord boundary information",
            "fine details of the original feature",
            "recurrence plot",
            "pragmatic approach",
            "short repeated segments",
            "synergistic effect",
            "Viterbi decoder"
        ]
    },
    {
        "title": "A Comparison of Statistical and Rule-Based Models for Style-Specific Harmonization.",
        "author": [
            "Ching-Hua Chuan"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417365",
        "url": "https://doi.org/10.5281/zenodo.1417365",
        "ee": "https://zenodo.org/records/1417365/files/Chuan11.pdf",
        "abstract": "The process of generating chords for harmonizing a melody with the goal of mimicking an artist\u2019s style is investigated in this paper. We compared and tested three different approaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identifiable pop/rock bands, and the chords generated by the systems were compared to the ones in the artists\u2019 original work. Evaluations were performed on multiple aspects, including calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the output harmonization, and examining a system\u2019s behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows.",
        "zenodo_id": 1417365,
        "dblp_key": "conf/ismir/Chuan11",
        "keywords": [
            "chords",
            "melody",
            "harmonizing",
            "mimicking",
            "artists style",
            "approaches",
            "rule-based model",
            "statistical model",
            "hybrid system",
            "pop/rock bands"
        ]
    },
    {
        "title": "The Melodic Signature Index for Fast Content-based Retrieval of Symbolic Scores Camelia Constantin.",
        "author": [
            "Cam\u00e9lia Constantin",
            "C\u00e9dric du Mouza",
            "Zo\u00e9 Faget",
            "Philippe Rigaux"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416860",
        "url": "https://doi.org/10.5281/zenodo.1416860",
        "ee": "https://zenodo.org/records/1416860/files/ConstantinMFR11.pdf",
        "abstract": "NEUMA is an on-line library that stores collections of symbolic scores and proposes a public interface to search for melodic pieces based on several kinds of patterns: pitchesbased, with or without rhythms, transposed or not. In addition, searches can be either exact or approximate. We describe an index structure apt at supporting all these searches in a consistent setting. Its distinctive feature is an encoding of the various information that might be involved in the pattern-matching process with algebraic signatures. The properties of these signatures are suitable to represent in a compact and expressive way the sequences of complex features that constitute a melodic description.",
        "zenodo_id": 1416860,
        "dblp_key": "conf/ismir/ConstantinMFR11",
        "keywords": [
            "on-line library",
            "symbolic scores",
            "public interface",
            "search for melodic pieces",
            "patterns",
            "pitches-based",
            "with or without rhythms",
            "transposed or not",
            "exact or approximate",
            "index structure"
        ]
    },
    {
        "title": "Finding Community Structure in Music Genres Networks.",
        "author": [
            "D\u00e9bora C. Corr\u00eaa",
            "Alexandre L. M. Levada",
            "Luciano da F. Costa"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418147",
        "url": "https://doi.org/10.5281/zenodo.1418147",
        "ee": "https://zenodo.org/records/1418147/files/CorreaLC11.pdf",
        "abstract": "Complex networks have shown to be promising mechanisms to represent several aspects of nature, since their topological and structural features help in the understanding of relations, properties and intrinsic characteristics of the data. In this context, we propose to build music networks in order to find community structures of music genres. Our main contributions are twofold: 1) Define a totally unsupervised approach for music genres discrimination; 2) Incorporate topological features in music data analysis. We compared different distance metrics and clustering algorithms. Each song is represented by a vector of conditional probabilities for the note values in its percussion track. Initial results indicate the effectiveness of the proposed methodology.",
        "zenodo_id": 1418147,
        "dblp_key": "conf/ismir/CorreaLC11",
        "keywords": [
            "Complex networks",
            "music genres discrimination",
            "topological features",
            "community structures",
            "music data analysis",
            "distance metrics",
            "clustering algorithms",
            "note values",
            "initial results",
            "proposed methodology"
        ]
    },
    {
        "title": "Combining Content-Based Auto-Taggers with Decision-Fusion.",
        "author": [
            "Emanuele Coviello",
            "Riccardo Miotto",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415666",
        "url": "https://doi.org/10.5281/zenodo.1415666",
        "ee": "https://zenodo.org/records/1415666/files/CovielloML11.pdf",
        "abstract": "To automatically annotate songs with descriptive keywords, a variety of content-based auto-tagging strategies have been proposed in recent years. Different approaches may capture different aspects of a song\u2019s musical content, such as timbre, temporal dynamics, rhythmic qualities, etc. As a result, some auto-taggers may be better suited to model the acoustic characteristics commonly associated with one set of tags, while being less predictive for other tags. This paper proposes decision-fusion, a principled approach to combining the predictions of a diverse collection of content-based autotaggers that focus on various aspects of the musical signal. By modeling the correlations between tag predictions of different auto-taggers, decision-fusion leverages the benefits of each of the original auto-taggers, and achieves superior annotation and retrieval performance.",
        "zenodo_id": 1415666,
        "dblp_key": "conf/ismir/CovielloML11",
        "keywords": [
            "content-based auto-tagging",
            "musical content",
            "timbre",
            "temporal dynamics",
            "rhythmic qualities",
            "acoustic characteristics",
            "decision-fusion",
            "principled approach",
            "correlations",
            "tag predictions"
        ]
    },
    {
        "title": "Feature Extraction and Machine Learning on Symbolic Music using the music21 Toolkit.",
        "author": [
            "Michael Scott Cuthbert",
            "Christopher Ariza",
            "Lisa Friedland"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416288",
        "url": "https://doi.org/10.5281/zenodo.1416288",
        "ee": "https://zenodo.org/records/1416288/files/CuthbertAF11.pdf",
        "abstract": "Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the \u201cfeature\u201d capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system\u2019s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper\u2019s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.",
        "zenodo_id": 1416288,
        "dblp_key": "conf/ismir/CuthbertAF11",
        "keywords": [
            "music21",
            "feature capabilities",
            "symbolic music data",
            "feature extraction",
            "toolkits",
            "data mining",
            "Orange",
            "Weka",
            "Monteverdi",
            "Bach"
        ]
    },
    {
        "title": "A System for Evaluating Singing Enthusiasm for Karaoke.",
        "author": [
            "Ryunosuke Daido",
            "Seongjun Hahm",
            "Masashi Ito",
            "Shozo Makino",
            "Akinori Ito"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417343",
        "url": "https://doi.org/10.5281/zenodo.1417343",
        "ee": "https://zenodo.org/records/1417343/files/DaidoHIMI11.pdf",
        "abstract": "Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: \u201centhusiasm\u201d. First, we investigated whether human listeners can evaluate enthusiasm consistently and whether the listener\u2019s perception matches the singer\u2019s enthusiasm. We then identified three acoustic features relevant to the perception of enthusiasm: A-weighted power, \u201cfall-down\u201d, and vibrato extent. Finally, we developed a system for evaluating singing enthusiasm using these features, and obtained a correlation coefficient of 0.65 between the system output and human evaluation.",
        "zenodo_id": 1417343,
        "dblp_key": "conf/ismir/DaidoHIMI11",
        "keywords": [
            "evaluation",
            "singing skill",
            "karaoke machines",
            "enthusiasm",
            "human listeners",
            "acoustic features",
            "system",
            "correlation coefficient",
            "enthusiasm",
            "evaluation"
        ]
    },
    {
        "title": "Musical Moods: A Mass Participation Experiment for Affective Classification of Music.",
        "author": [
            "Sam Davies",
            "Penelope Allen",
            "Mark Mann",
            "Trevor J. Cox"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415920",
        "url": "https://doi.org/10.5281/zenodo.1415920",
        "ee": "https://zenodo.org/records/1415920/files/DaviesAMC11.pdf",
        "abstract": "In this paper we present our mass participation experiment, Musical Moods. This experiment placed 144 theme tunes online, taken from TV and radio programmes from the last 60 years of the British Broadcasting Corporations (BBC) output.  Members of the public were then invited to audition then rate these according to a set of semantic differentials based on the affective categories of evaluation, potency and activity.  Participants were also asked to rate their familiarity of the theme tune and how much they liked the theme tune.  A final question asked participants to identify the genre of the TV programme with which they associated the tune.  The purpose of this is to aid in the affective classification of large-scale TV archives, such as those possessed by the BBC.  We find correlations between evaluation and potency, potency and activity but none between activity and evaluation but no clear correlation between affect and genre.  This paper presents our key findings from an analysis of the results along with our plans for further analysis. The initial results from this experiment are based on an analyses of over 51,000 answers from over 13,000 participants.",
        "zenodo_id": 1415920,
        "dblp_key": "conf/ismir/DaviesAMC11",
        "keywords": [
            "mass participation experiment",
            "Musical Moods",
            "144 theme tunes",
            "publicly available",
            "ratings and feedback",
            "semantic differentials",
            "evaluation and potency",
            "activity and familiarity",
            "genre identification",
            "affective classification"
        ]
    },
    {
        "title": "Audio-based Music Classification with a Pretrained Convolutional Network.",
        "author": [
            "Sander Dieleman",
            "Philemon Brakel",
            "Benjamin Schrauwen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415188",
        "url": "https://doi.org/10.5281/zenodo.1415188",
        "ee": "https://zenodo.org/records/1415188/files/DielemanBS11.pdf",
        "abstract": "Recently the \u2018Million Song Dataset\u2019, containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well.",
        "zenodo_id": 1415188,
        "dblp_key": "conf/ismir/DielemanBS11",
        "keywords": [
            "convolutional network",
            "artist recognition",
            "genre recognition",
            "key detection",
            "unsupervised pretraining",
            "musically significant timescales",
            "labeled subset",
            "randomly initialized weights",
            "MLP",
            "accuracy"
        ]
    },
    {
        "title": "The Temperament Police: The Truth, the Ground Truth, and Nothing but the Truth.",
        "author": [
            "Simon Dixon",
            "Dan Tidhar",
            "Emmanouil Benetos"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418197",
        "url": "https://doi.org/10.5281/zenodo.1418197",
        "ee": "https://zenodo.org/records/1418197/files/DixonTB11.pdf",
        "abstract": "The tuning system of a keyboard instrument is chosen so that frequently used musical intervals sound as consonant as possible. Temperament refers to the compromise arising from the fact that not all intervals can be maximally consonant simultaneously. Recent work showed that it is possible to estimate temperament from audio recordings with no prior knowledge of the musical score, using a conservative (high precision, low recall) automatic transcription algorithm followed by frequency estimation using quadratic interpolation and bias correction from the log magnitude spectrum. In this paper we develop a harpsichord-specific transcription system to analyse over 500 recordings of solo harpsichord music for which the temperament is specified on the CD sleeve notes. We compare the measured temperaments with the annotations and discuss the differences between temperament as a theoretical construct and as a practical issue for professional performers and tuners. The implications are that ground truth is not always scientific truth, and that content-based analysis has an important role in the study of historical performance practice.",
        "zenodo_id": 1418197,
        "dblp_key": "conf/ismir/DixonTB11",
        "keywords": [
            "tuning system",
            "musical intervals",
            "consonance",
            "temperament compromise",
            "audio recordings",
            "automatic transcription",
            "frequency estimation",
            "log magnitude spectrum",
            "harpsichord-specific transcription",
            "historical performance practice"
        ]
    },
    {
        "title": "An Auditory Streaming Approach for Melody Extraction from Polyphonic Music.",
        "author": [
            "Karin Dressler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416112",
        "url": "https://doi.org/10.5281/zenodo.1416112",
        "ee": "https://zenodo.org/records/1416112/files/Dressler11.pdf",
        "abstract": "This paper proposes an efficient approach for the identification of the predominant voice from polyphonic musical audio. The algorithm implements an auditory streaming model which builds upon tone objects and salient pitches. The formation of voices is based on the regular update of the frequency and the magnitude of so called streaming agents, which aim at salient tones or pitches close to their preferred frequency range. Streaming agents which succeed to assemble a big magnitude start new voice objects, which in turn add adequate tones. The algorithm was evaluated as part of a melody extraction system during the MIREX audio melody extraction evaluation, where it gained very good results in the voicing detection and overall accuracy.",
        "zenodo_id": 1416112,
        "dblp_key": "conf/ismir/Dressler11",
        "keywords": [
            "polyphonic",
            "musical",
            "audio",
            "voice",
            "identification",
            "auditory",
            "streaming",
            "model",
            "tone",
            "objects"
        ]
    },
    {
        "title": "Aligning Semi-Improvised Music Audio with Its Lead Sheet.",
        "author": [
            "Zhiyao Duan",
            "Bryan Pardo"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417325",
        "url": "https://doi.org/10.5281/zenodo.1417325",
        "ee": "https://zenodo.org/records/1417325/files/DuanP11.pdf",
        "abstract": "Existing audio-score alignment methods assume that the audio performance is faithful to a fully-notated MIDI score. For semi-improvised music (e.g. jazz), this assumption is strongly violated. In this paper, we address the problem of aligning semi-improvised music audio with a lead sheet. Our approach does not require prior training on performances of the lead sheet to be aligned. We start by analyzing the problem and propose to represent the lead sheet as a MIDI file together with a structural information file. Then we propose a dynamic-programming-based system to align the chromagram representations of the audio performance and the MIDI score. Techniques are proposed to address the chromagram scaling, key transposition and structural change (e.g. a performer unexpectedly repeats a section) problems. We test our system on 3 jazz lead sheets. For each sheet we align a set of solo piano performances and a set of fullband commercial recordings with different instrumentation and styles. Results show that our system achieves promising results on some highly improvised music.",
        "zenodo_id": 1417325,
        "dblp_key": "conf/ismir/DuanP11",
        "keywords": [
            "audio-score alignment",
            "semi-improvised music",
            "lead sheet",
            "dynamic-programming-based system",
            "chromagram representations",
            "structural information file",
            "key transposition",
            "structural change",
            "promising results",
            "highly improvised music"
        ]
    },
    {
        "title": "Music Structure Segmentation Algorithm Evaluation: Expanding on MIREX 2010 Analyses and Datasets.",
        "author": [
            "Andreas F. Ehmann",
            "Mert Bay",
            "J. Stephen Downie",
            "Ichiro Fujinaga",
            "David De Roure"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418151",
        "url": "https://doi.org/10.5281/zenodo.1418151",
        "ee": "https://zenodo.org/records/1418151/files/EhmannBDFR11.pdf",
        "abstract": "Music audio structure segmentation has been a task in the Music Information Retrieval Evaluation eXchange (MIREX) since 2009. In 2010, five algorithms were evaluated against two datasets (297 and 100 songs) with an almost exclusive focus on western popular music. A new annotated dataset significantly larger in size and with a more diverse range of musical styles became available in 2011. This new dataset comprises over 1,300 songs spanning pop, jazz, classical, and world music styles. The algorithms from the 2010 iteration of MIREX are re-evaluated against this new dataset. This paper presents a detailed analysis of these evaluation results in order to gain a better understanding of the current state-of-the-art in automatic structure segmentation. These expanded analyses focus on the interaction of algorithm performance and rankings with datasets, musical styles, and annotation level. Because the new dataset contains multiple annotations for each song, we also introduce a baseline for expected human performance for this task.",
        "zenodo_id": 1418151,
        "dblp_key": "conf/ismir/EhmannBDFR11"
    },
    {
        "title": "Semantic Annotation and Retrieval of Music using a Bag of Systems Representation.",
        "author": [
            "Katherine Ellis",
            "Emanuele Coviello",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416390",
        "url": "https://doi.org/10.5281/zenodo.1416390",
        "ee": "https://zenodo.org/records/1416390/files/EllisCL11.pdf",
        "abstract": "We present a content-based auto-tagger that leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. This leads to a higher-level, concise \u201cBag of Systems\u201d (BoS) representation of the characteristics of a musical piece. Once songs are represented as a BoS histogram over codewords, traditional algorithms for text document retrieval can be leveraged for music autotagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different classes of generative models at various time resolutions through the selection of the BoS codewords. Experiments show that this enriches the audio representation and leads to superior auto-tagging performance.",
        "zenodo_id": 1416390,
        "dblp_key": "conf/ismir/EllisCL11",
        "keywords": [
            "content-based",
            "auto-tagger",
            "rich dictionary",
            "musical codewords",
            "generative models",
            "timbral characteristics",
            "temporal characteristics",
            "Bag of Systems",
            "musical piece",
            "Bag of Systems representation"
        ]
    },
    {
        "title": "Score-Informed Voice Separation For Piano Recordings.",
        "author": [
            "Sebastian Ewert",
            "Meinard M\u00fcller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417581",
        "url": "https://doi.org/10.5281/zenodo.1417581",
        "ee": "https://zenodo.org/records/1417581/files/EwertM11.pdf",
        "abstract": "The decomposition of a monaural audio recording into musically meaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for realworld non-synthetic piano recordings.",
        "zenodo_id": 1417581,
        "dblp_key": "conf/ismir/EwertM11",
        "keywords": [
            "musical",
            "sound",
            "sources",
            "score",
            "informed",
            "approach",
            "spectrograms",
            "inversion",
            "realworld",
            "synthetic"
        ]
    },
    {
        "title": "The Studio Ontology Framework.",
        "author": [
            "Gy\u00f6rgy Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417735",
        "url": "https://doi.org/10.5281/zenodo.1417735",
        "ee": "https://zenodo.org/records/1417735/files/FazekasS11.pdf",
        "abstract": "This paper introduces the Studio Ontology Framework for describing and sharing detailed information about music production. The primary aim of this ontology is to capture the nuances of record production by providing an explicit, application and situation independent conceptualisation of the studio environment. We may use the ontology to describe real-world recording scenarios involving physical hardware, or (post) production on a personal computer. It builds on Semantic Web technologies and previously published ontologies for knowledge representation and knowledge sharing.",
        "zenodo_id": 1417735,
        "dblp_key": "conf/ismir/FazekasS11",
        "keywords": [
            "Studio",
            "Ontology",
            "Framework",
            "music",
            "production",
            "nuances",
            "record",
            "production",
            "explicit",
            "conceptualisation"
        ]
    },
    {
        "title": "A Scalable Audio Fingerprint Method with Robustness to Pitch-Shifting.",
        "author": [
            "S\u00e9bastien Fenet",
            "Ga\u00ebl Richard",
            "Yves Grenier"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417593",
        "url": "https://doi.org/10.5281/zenodo.1417593",
        "ee": "https://zenodo.org/records/1417593/files/FenetRG11.pdf",
        "abstract": "Audio fingerprint techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing. Although most of nowadays techniques are robust to the majority of them, the quasi-systematic use of a spectral representation makes them possibly sensitive to pitch-shifting. This distortion indeed induces a modification of the spectral content of the signal. In this paper, we propose a novel fingerprint technique, relying on a hashing technique coupled with a CQT-based fingerprint, with a strong robustness to pitch-shifting. Furthermore, we have associated this method with an efficient post-processing for the removal of false alarms. We also present the adaptation of a database pruning technique to our specific context. We have evaluated our approach on a real-life broadcast monitoring scenario. The analyzed data consisted of 120 hours of real radio broadcast (thus containing all the distortions that would be found in an industrial context). The reference database consisted of 30.000 songs. Our method, thanks to its increased robustness to pitch-shifting, shows an excellent detection score.",
        "zenodo_id": 1417593,
        "dblp_key": "conf/ismir/FenetRG11",
        "keywords": [
            "Audio fingerprint techniques",
            "robust to distortions",
            "pitch-shifting",
            "spectrum representation",
            "hashing technique",
            "CQT-based fingerprint",
            "post-processing",
            "database pruning",
            "real-life broadcast monitoring",
            "detection score"
        ]
    },
    {
        "title": "Causal Prediction of Continuous-Valued Music Features.",
        "author": [
            "Peter Foster",
            "Anssi Klapuri",
            "Mark D. Plumbley"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418337",
        "url": "https://doi.org/10.5281/zenodo.1418337",
        "ee": "https://zenodo.org/records/1418337/files/FosterKP11.pdf",
        "abstract": "This paper investigates techniques for predicting sequences of continuous-valued feature vectors extracted from musical audio. In particular, we consider prediction of beatsynchronous Mel-frequency cepstral coefficients and chroma features in a causal setting, where features are predicted as they unfold in time. The methods studied comprise autoregressive models, N-gram models incorporating a smoothing scheme, and a novel technique based on repetition detection using a self-distance matrix. Furthermore, we propose a method for combining predictors, which relies on a running estimate of the error variance of the predictors to inform a linear weighting of the predictor outputs. Results indicate that incorporating information on long-term structure improves the prediction performance for continuous-valued, sequential musical data. For the Beatles data set, combining the proposed self-distance based predictor with both N-gram and autoregressive methods results in an average of 13% improvement compared to a linear predictive baseline.",
        "zenodo_id": 1418337,
        "dblp_key": "conf/ismir/FosterKP11",
        "keywords": [
            "predicting",
            "sequences",
            "continuous-valued",
            "feature",
            "vectors",
            "extracted",
            "musical",
            "audio",
            "causal",
            "setting"
        ]
    },
    {
        "title": "Multi-scale temporal fusion by boosting for music classification.",
        "author": [
            "R\u00e9mi Foucard",
            "Slim Essid",
            "Mathieu Lagrange",
            "Ga\u00ebl Richard"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416904",
        "url": "https://doi.org/10.5281/zenodo.1416904",
        "ee": "https://zenodo.org/records/1416904/files/FoucardELR11.pdf",
        "abstract": "Short-term and long-term descriptors constitute complementary pieces of information in the analysis of audio signals. However, because they are extracted over different time horizons, it is difficult to exploit them concurrently in a fully effective manner. In this paper we propose a novel temporal fusion method that leverages the effectiveness of a given set of features by efficiently combining multi-scale versions of them. This fusion is achieved using a boosting technique exploiting trees as weak classifiers, which has the advantage of performing an embedded feature selection. We apply our algorithm to two standard classification tasks, namely musical instrument recognition and multi-tag classification. Our experiments indicate that the multi-scale approach is able to select different features at different scales and significantly outperforms the mono-scale systems in terms of classification performance.",
        "zenodo_id": 1416904,
        "dblp_key": "conf/ismir/FoucardELR11",
        "keywords": [
            "short-term",
            "long-term",
            "audio signals",
            "complementary pieces",
            "time horizons",
            "exploit concurrently",
            "temporal fusion",
            "weak classifiers",
            "embedded feature selection",
            "classification performance"
        ]
    },
    {
        "title": "Quantifying the Relevance of Locally Extracted Information for Musical Instrument Recognition from Entire Pieces of Music.",
        "author": [
            "Ferdinand Fuhrmann",
            "Perfecto Herrera"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416350",
        "url": "https://doi.org/10.5281/zenodo.1416350",
        "ee": "https://zenodo.org/records/1416350/files/FuhrmannH11.pdf",
        "abstract": "In this work we study the problem of automatic musical instrument recognition from entire pieces of music. In particular, we present and evaluate 4 different methods to select, from an unknown piece of music, relevant excerpts in terms of instrumentation, on top of which instrument recognition techniques are applied to infer the labels. Since the desired information is assumed to be redundant (we may extract just a few labels from a thousands of audio frames) we examine the recognition performance, the amount of data used for processing, and their possible correlation. Experimental results on a collection of Western music pieces reveal state-ofthe-art performance in instrument recognition together with a great reduction of the required input data. However, we also observe a performance ceiling with the currently applied instrument recognition method.",
        "zenodo_id": 1416350,
        "dblp_key": "conf/ismir/FuhrmannH11",
        "keywords": [
            "Automatic",
            "Instrumental",
            "Recognition",
            "Extraction",
            "Redundant",
            "Information",
            "Audio",
            "Frames",
            "Performance",
            "Data"
        ]
    },
    {
        "title": "A Real-Time Signal Processing Framework of Musical Expressive Feature Extraction Using Matlab.",
        "author": [
            "Ren Gang",
            "Gregory Bocko",
            "Justin Lundberg",
            "Stephen Roessner",
            "Dave Headlam",
            "Mark F. Bocko"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418081",
        "url": "https://doi.org/10.5281/zenodo.1418081",
        "ee": "https://zenodo.org/records/1418081/files/GangBLRHB11.pdf",
        "abstract": "In this paper we propose a real-time signal processing framework for musical audio that 1) aligns the audio with an existing music score or creates a musical score by automated music transcription algorithms; and 2) obtains the expressive feature descriptors of music performance by comparing the score with the audio. Real-time audio segmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a realtime setting, these audio segmentation and feature extraction operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing requirements while maintaining feature accuracy, our proposed framework combines the processing stages of prediction, estimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework.",
        "zenodo_id": 1418081,
        "dblp_key": "conf/ismir/GangBLRHB11",
        "keywords": [
            "real-time signal processing",
            "musical audio alignment",
            "automated music transcription",
            "musical score creation",
            "expressive feature descriptors",
            "audio segmentation algorithms",
            "note onset points",
            "score related features",
            "musical expressive features",
            "real-time processing requirements"
        ]
    },
    {
        "title": "A simple-cycles weighted kernel based on harmony structure for similarity retrieval.",
        "author": [
            "Silvia Garc\u00eda-D\u00edez",
            "Marco Saerens",
            "Mathieu Senelle",
            "Fran\u00e7ois Fouss"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415172",
        "url": "https://doi.org/10.5281/zenodo.1415172",
        "ee": "https://zenodo.org/records/1415172/files/Garcia-DiezSSF11.pdf",
        "abstract": "This paper introduces a novel methodology for music similarity retrieval based on chord progressions. From each chord progression, a directed labeled graph containing the interval transitions is extracted. This graph will be used as input for a graph comparison method based on simple cycles \u2013 cycles where the only repeated nodes are the first and the last one. In music, simple cycles represent the repetitive sub-structures of, e.g., modern pop/rock music. By means of a kernel function [10] whose feature space is spanned by these simple cycles, we obtain a kernel matrix (similarity matrix) which can then be used in music similarity retrieval tasks. The resulting algorithm has a time complexity of O(n+m(c+1)), where n is the number of vertices, m is the number of edges, and c is the number of simple cycles. The performance of our method is tested on both an idiom retrieval task, and a cover song retrieval task. Empirical results show the improved accuracy of our method in comparison with other string-matching, and graph-comparison",
        "zenodo_id": 1415172,
        "dblp_key": "conf/ismir/Garcia-DiezSSF11",
        "keywords": [
            "music similarity retrieval",
            "chord progressions",
            "directed labeled graph",
            "interval transitions",
            "graph comparison method",
            "simple cycles",
            "kernel function",
            "feature space",
            "kernel matrix",
            "music retrieval tasks"
        ]
    },
    {
        "title": "Songle: A Web Service for Active Music Listening Improved by User Contributions.",
        "author": [
            "Masataka Goto",
            "Kazuyoshi Yoshii",
            "Hiromasa Fujihara",
            "Matthias Mauch",
            "Tomoyasu Nakano"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416256",
        "url": "https://doi.org/10.5281/zenodo.1416256",
        "ee": "https://zenodo.org/records/1416256/files/GotoYFMN11.pdf",
        "abstract": "This paper describes a public web service for active music listening, Songle, that enriches music listening experiences by using music-understanding technologies based on signal processing. Although various research-level interfaces and technologies have been developed, it has not been easy to get people to use them in everyday life. Songle serves as a showcase to demonstrate how people can benefit from music-understanding technologies by enabling people to experience active music listening interfaces on the web. Songle facilitates deeper understanding of music by visualizing music scene descriptions estimated automatically, such as music structure, hierarchical beat structure, melody line, and chords. When using music-understanding technologies, however, estimation errors are inevitable. Songle therefore features an efficient error correction interface that encourages people to contribute by correcting those errors to improve the web service. We also propose a mechanism of collaborative training for music-understanding technologies, in which corrected errors will be used to improve the music-understanding performance through machine learning techniques. We hope Songle will serve as a research platform where other researchers can exhibit results of their music-understanding technologies to jointly promote the popularization of the field of music information research.",
        "zenodo_id": 1416256,
        "dblp_key": "conf/ismir/GotoYFMN11",
        "keywords": [
            "public web service",
            "active music listening",
            "music-understanding technologies",
            "signal processing",
            "music scene descriptions",
            "error correction interface",
            "collaborative training",
            "music information research",
            "research platform",
            "popularization"
        ]
    },
    {
        "title": "An Interactive System for Electro-Acoustic Music Analysis.",
        "author": [
            "S\u00e9bastien Gulluni",
            "Slim Essid",
            "Olivier Buisson",
            "Ga\u00ebl Richard"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416134",
        "url": "https://doi.org/10.5281/zenodo.1416134",
        "ee": "https://zenodo.org/records/1416134/files/GulluniEBR11.pdf",
        "abstract": "This paper, presents an interactive approach for the analysis of electro-acoustic music. An original classification scheme is devised using relevance feedback and active-learning segment selection in an interactive loop. Validation and correction information given by the user is injected in the learning process at each iteration to achieve more accurate classification. An experimental study is conducted to evaluate and compare the different classification and relevance feedback approaches that are envisaged, using a database of polyphonic pieces (with a varying degree of polyphony). The results show that the different approaches are adapted to different applications and they achieve satisfying performance in a reasonable number of iterations.",
        "zenodo_id": 1416134,
        "dblp_key": "conf/ismir/GulluniEBR11",
        "keywords": [
            "interactive approach",
            "electro-acoustic music",
            "classification scheme",
            "relevance feedback",
            "active-learning segment selection",
            "user feedback",
            "learning process",
            "experimental study",
            "polyphonic pieces",
            "performance"
        ]
    },
    {
        "title": "Using Network Sciences to Rank Musicians and Composers in Brazilian Popular Music.",
        "author": [
            "Charith Gunaratna",
            "Evan Stoner",
            "Ronaldo Menezes"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415818",
        "url": "https://doi.org/10.5281/zenodo.1415818",
        "ee": "https://zenodo.org/records/1415818/files/GunaratnaSM11.pdf",
        "abstract": "Music fascinates and touches most people. This fascination leads to opinions about the music pieces that reflects people\u2019s exposure and personal experience. This inherent bias of people towards music indicates that personal opinion is inappropriate for defining the quality of music and musicians. This paper takes a holistic view of the problem and delves into the understanding of the structure of Brazilian music rooted in Network Sciences. In this paper we work with a large database of albums of Brazilian music and study the structure of collaborations between all the musicians and composers. The collaboration is modelled as a social network of musicians and then analyzed from different perspectives with the goal of describing what we call the structure of that musical genre as well as provide a ranking of musicians and composers.",
        "zenodo_id": 1415818,
        "dblp_key": "conf/ismir/GunaratnaSM11",
        "keywords": [
            "Music",
            "fascination",
            "personal opinion",
            "quality of music",
            "musicians",
            "network sciences",
            "collaborations",
            "social network",
            "musicians",
            "composers"
        ]
    },
    {
        "title": "HarmTrace: Improving Harmonic Similarity Estimation Using Functional Harmony Analysis.",
        "author": [
            "W. Bas de Haas",
            "Jos\u00e9 Pedro Magalh\u00e3es",
            "Remco C. Veltkamp",
            "Frans Wiering"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417063",
        "url": "https://doi.org/10.5281/zenodo.1417063",
        "ee": "https://zenodo.org/records/1417063/files/HaasMVW11.pdf",
        "abstract": "Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences.",
        "zenodo_id": 1417063,
        "dblp_key": "conf/ismir/HaasMVW11",
        "keywords": [
            "Harmony theory",
            "tonal harmony",
            "formalisation",
            "HARMTRACE",
            "chord sequences",
            "harmonic function",
            "harmonic similarity",
            "local alignment",
            "corpus",
            "estimation"
        ]
    },
    {
        "title": "Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio.",
        "author": [
            "Philippe Hamel",
            "Simon Lemieux",
            "Yoshua Bengio",
            "Douglas Eck"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418237",
        "url": "https://doi.org/10.5281/zenodo.1418237",
        "ee": "https://zenodo.org/records/1418237/files/HamelLBE11.pdf",
        "abstract": "This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset.",
        "zenodo_id": 1418237,
        "dblp_key": "conf/ismir/HamelLBE11"
    },
    {
        "title": "The Music Encoding Initiative as a Document-Encoding Framework.",
        "author": [
            "Andrew Hankinson",
            "Perry Roland",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417609",
        "url": "https://doi.org/10.5281/zenodo.1417609",
        "ee": "https://zenodo.org/records/1417609/files/HankinsonRF11.pdf",
        "abstract": "Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards.",
        "zenodo_id": 1417609,
        "dblp_key": "conf/ismir/HankinsonRF11",
        "keywords": [
            "Extensible",
            "Platform",
            "New Notation",
            "Encoding",
            "Schemes",
            "Document-Encoding",
            "Framework",
            "Notation",
            "Encoding",
            "Standards"
        ]
    },
    {
        "title": "Unsupervised Learning of Sparse Features for Scalable Audio Classification.",
        "author": [
            "Mikael Henaff",
            "Kevin Jarrett",
            "Koray Kavukcuoglu",
            "Yann LeCun"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416086",
        "url": "https://doi.org/10.5281/zenodo.1416086",
        "ee": "https://zenodo.org/records/1416086/files/HenaffJKL11.pdf",
        "abstract": "In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets.",
        "zenodo_id": 1416086,
        "dblp_key": "conf/ismir/HenaffJKL11",
        "keywords": [
            "unsupervised",
            "overcomplete dictionary",
            "sparsely decompose",
            "log-scaled spectrograms",
            "efficient encoder",
            "sparse representations",
            "Support Vector Machine (SVM)",
            "GTZAN dataset",
            "linear classifier",
            "large datasets"
        ]
    },
    {
        "title": "NextOne Player: A Music Recommendation System Based on User Behavior.",
        "author": [
            "Yajie Hu",
            "Mitsunori Ogihara"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418301",
        "url": "https://doi.org/10.5281/zenodo.1418301",
        "ee": "https://zenodo.org/records/1418301/files/HuO11.pdf",
        "abstract": "We present a new approach to recommend suitable tracks from a collection of songs to the user. The goal of the system is to recommend songs that are favored by the user, are fresh to the user\u2019s ear, and fit the user\u2019s listening pattern. We use \u201cForgetting Curve\u201d to assess freshness of a song and evaluate \u201cfavoredness\u201d using user log. We analyze user\u2019s listening pattern to estimate the level of interest of the user in the next song. Also, we treat user behavior on the song being played as feedback to adjust the recommendation strategy for the next one. We develop an application to evaluate our approach in the real world. The user logs of trial volunteers show good performance of the proposed method.",
        "zenodo_id": 1418301,
        "dblp_key": "conf/ismir/HuO11",
        "keywords": [
            "freshness",
            "favoredness",
            "listening pattern",
            "Forgetting Curve",
            "user log",
            "user behavior",
            "recommendation strategy",
            "real world",
            "application",
            "trial volunteers"
        ]
    },
    {
        "title": "Exploring The Relationship Between Mood and Creativity in Rock Lyrics.",
        "author": [
            "Xiao Hu 0001",
            "Bei Yu 0002"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415690",
        "url": "https://doi.org/10.5281/zenodo.1415690",
        "ee": "https://zenodo.org/records/1415690/files/HuY11.pdf",
        "abstract": "The relationship between mood and creativity has been widely studied in psychology, however, no conclusion is reached in terms of which mood triggers high creativity, positive or negative. This paper provides new insights to this on-going argument by examining the relationship between lyrics creativity and music mood. We use three computational measures to gauge lyrics creativity: Type-toToken Ratio, word norms fraction, and WordNet similarity. We then test three hypotheses regarding differences in lyrics creativity between music with different moods on 2715 U.S. rock songs. The three measures led to consistent findings that lyrics of negative and sad songs demonstrate higher linguistic creativity than those of positive and happy songs. Our findings support previous studies in psycholinguistics that people write more creatively when the text conveys sad or negative sentiment, and contradict previous research that positive mood triggers more unusual word associations. The result also indicates that different measures capture different aspects of lyrics creativity.",
        "zenodo_id": 1415690,
        "dblp_key": "conf/ismir/HuY11",
        "keywords": [
            "creativity",
            "mood",
            "lyrics",
            "music",
            "computational measures",
            "U.S. rock songs",
            "negative and sad songs",
            "positive and happy songs",
            "previous studies",
            "unusual word associations"
        ]
    },
    {
        "title": "Melody Extraction based on Harmonic Coded Structure.",
        "author": [
            "Sihyun Joo",
            "Sanghun Park",
            "Seokhwan Jo",
            "Chang D. Yoo"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417243",
        "url": "https://doi.org/10.5281/zenodo.1417243",
        "ee": "https://zenodo.org/records/1417243/files/JooPJY11.pdf",
        "abstract": "This paper considers a melody extraction algorithm that estimates the melody in polyphonic audio using the harmonic coded structure (HCS) to model melody in the minimum mean-square-error (MMSE) sense. The HCS is harmonically modulated sinusoids with the amplitudes defined by a set of codewords. The considered algorithm performs melody extraction in two steps: i) pitch-candidate estimation and ii) pitch-sequence identification. In the estimation step, pitch candidates are estimated such that the HCS best represents the polyphonic audio in the MMSE sense. In the identification step, a melody line is selected from many possible pitch sequences based on the properties of melody line. Posterior to the melody line selection, a smoothing process is applied to refine spurious pitches and octave errors. The performance of the algorithm is evaluated and compared using the ADC04 and the MIREX05 dataset. The results show that the performance of the proposed algorithm is better than or comparable to other algorithms submitted to MIREX2009.",
        "zenodo_id": 1417243,
        "dblp_key": "conf/ismir/JooPJY11",
        "keywords": [
            "melody extraction algorithm",
            "polyphonic audio",
            "harmonic coded structure",
            "MMSE sense",
            "pitch-candidate estimation",
            "pitch-sequence identification",
            "HCS",
            "amplitudes defined by a set of codewords",
            "posterior to the melody line selection",
            "smoothing process"
        ]
    },
    {
        "title": "Low Dimensional Visualization of Folk Music Systems Using the Self Organizing Cloud.",
        "author": [
            "Zolt\u00e1n Juh\u00e1sz"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417433",
        "url": "https://doi.org/10.5281/zenodo.1417433",
        "ee": "https://zenodo.org/records/1417433/files/Juhasz11.pdf",
        "abstract": "We describe a computational method derived from self organizing mapping and multidimensional scaling algorithms for automatic classification and visual clustering of large vector databases. Testing the method on a large corpus of folksongs we have found that the performance of the classification and topological clustering was significantly improved compared to current techniques. Applying the method to an analysis of the connections of 31 Eurasian and North-American folk music cultures, a clearly interpretable system of musical connections was revealed. The results show the relevance of the musical language groups in the oral tradition of the humanity.",
        "zenodo_id": 1417433,
        "dblp_key": "conf/ismir/Juhasz11",
        "keywords": [
            "computational method",
            "self organizing mapping",
            "multidimensional scaling",
            "automatic classification",
            "visual clustering",
            "large vector databases",
            "performance improvement",
            "corpus of folksongs",
            "clearly interpretable system",
            "oral tradition"
        ]
    },
    {
        "title": "Probabilistic Modeling of Hierarchical Music Analysis.",
        "author": [
            "Phillip B. Kirlin",
            "David D. Jensen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417119",
        "url": "https://doi.org/10.5281/zenodo.1417119",
        "ee": "https://zenodo.org/records/1417119/files/KirlinJ11.pdf",
        "abstract": "Hierarchical music analysis, as exemplified by Schenkerian analysis, describes the structure of a musical composition by a hierarchy among its notes. Each analysis defines a set of prolongations, where musical objects persist in time even though others are present. We present a formal model for representing hierarchical music analysis, probabilistic interpretations of that model, and an efficient algorithm for computing the most probable analysis under these interpretations. We represent Schenkerian analyses as maximal outerplanar graphs (MOPs). We use this representation to encode the largest known data set of computer-processable Schenkerian analyses, and we use these data to identify statistical regularities in the human-generated analyses. We show that a dynamic programming algorithm can be applied to these regularities to identify the maximum likelihood analysis for a given piece of music.",
        "zenodo_id": 1417119,
        "dblp_key": "conf/ismir/KirlinJ11",
        "keywords": [
            "Hierarchical music analysis",
            "Schenkerian analysis",
            "Musical composition structure",
            "Hierarchy among notes",
            "Prolongations",
            "Probabilistic interpretations",
            "Efficient algorithm",
            "Maximal outerplanar graphs (MOPs)",
            "Data set of computer-processable Schenkerian analyses",
            "Statistical regularities in human-generated analyses"
        ]
    },
    {
        "title": "The potential for automatic assessment of trumpet tone quality.",
        "author": [
            "Trevor Knight",
            "Finn Upham",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418123",
        "url": "https://doi.org/10.5281/zenodo.1418123",
        "ee": "https://zenodo.org/records/1418123/files/KnightUF11.pdf",
        "abstract": "The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (\u2015good\u2016 and \u2015bad\u2016) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (\u2015good,\u2016 \u2015medium,\u2016 and \u2015bad\u2016), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%).",
        "zenodo_id": 1418123,
        "dblp_key": "conf/ismir/KnightUF11",
        "keywords": [
            "machine learning",
            "training",
            "audio features",
            "brass players",
            "support vector machines",
            "cross-validation",
            "groupings of ratings",
            "classifiers",
            "success rate",
            "average success rate"
        ]
    },
    {
        "title": "Computational Approaches for the Understanding of Melody in Carnatic Music.",
        "author": [
            "Gopala K. Koduri",
            "Marius Miron",
            "Joan Serr\u00e0",
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415238",
        "url": "https://doi.org/10.5281/zenodo.1415238",
        "ee": "https://zenodo.org/records/1415238/files/KoduriMSS11.pdf",
        "abstract": "The classical music traditions of the Indian subcontinent, Hindustani and Carnatic, offer an excellent ground on which to test the limitations of current music information research approaches. At the same time, studies based on these music traditions can shed light on how to solve new and complex music modeling problems. Both traditions have very distinct characteristics, specially compared with western ones: they have developed unique instruments, musical forms, performance practices, social uses and context. In this article, we focus on the Carnatic music tradition of south India, especially on its melodic characteristics. We overview the theoretical aspects that are relevant for music information research and discuss the scarce computational approaches developed so far. We put emphasis on the limitations of the current methodologies and we present open issues that have not yet been addressed and that we believe are important to be worked on.",
        "zenodo_id": 1415238,
        "dblp_key": "conf/ismir/KoduriMSS11",
        "keywords": [
            "Indian subcontinent",
            "Hindustani",
            "Carnatic",
            "music information research",
            "instrumentation",
            "musical forms",
            "performance practices",
            "social uses",
            "context",
            "melodic characteristics"
        ]
    },
    {
        "title": "Knowledge Representation Issues in Musical Instrument Ontology Design.",
        "author": [
            "Sefki Kolozali",
            "Mathieu Barthet",
            "Gy\u00f6rgy Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416142",
        "url": "https://doi.org/10.5281/zenodo.1416142",
        "ee": "https://zenodo.org/records/1416142/files/KolozaliBFS11.pdf",
        "abstract": "This paper presents preliminary work on musical instruments ontology design, and investigates heterogeneity and limitations in existing instrument classification schemes. Numerous research to date aims at representing information about musical instruments. The works we examined are based on the well known Hornbostel and Sach\u2019s classification scheme. We developed representations using the Ontology Web Language (OWL), and compared terminological and conceptual heterogeneity using SPARQL queries. We found evidence to support that traditional designs based on taxonomy trees lead to ill-defined knowledge representation, especially in the context of an ontology for the Semantic Web. In order to overcome this issue, it is desirable to have an instrument ontology that exhibits a semantically rich structure.",
        "zenodo_id": 1416142,
        "dblp_key": "conf/ismir/KolozaliBFS11",
        "keywords": [
            "musical instruments ontology design",
            "heterogeneity",
            "existing instrument classification schemes",
            "Hornbostel and Sach\u2019s classification scheme",
            "Ontology Web Language (OWL)",
            "SPARQL queries",
            "traditional designs",
            "Semantic Web",
            "instrument ontology",
            "semantically rich structure"
        ]
    },
    {
        "title": "A Computational Investigation of Melodic Contour Stability in Jewish Torah Trope Performance Traditions.",
        "author": [
            "Peter van Kranenburg",
            "D\u00e1niel P\u00e9ter Bir\u00f3",
            "Steven R. Ness",
            "George Tzanetakis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415612",
        "url": "https://doi.org/10.5281/zenodo.1415612",
        "ee": "https://zenodo.org/records/1415612/files/KranenburgBNT11.pdf",
        "abstract": "The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a \u201cmelodic idea\u201d which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations.",
        "zenodo_id": 1415612,
        "dblp_key": "conf/ismir/KranenburgBNT11",
        "keywords": [
            "cantillation signs",
            "oral music performance",
            "notation",
            "melodic idea",
            "syntax",
            "punctuation",
            "meaning",
            "flexible interpretations",
            "regional traditions",
            "musical influences"
        ]
    },
    {
        "title": "Dynamic Programming in Transposition and Time-Warp Invariant Polyphonic Content-Based Music Retrieval.",
        "author": [
            "Mika Laitinen",
            "Kjell Lemstr\u00f6m"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416094",
        "url": "https://doi.org/10.5281/zenodo.1416094",
        "ee": "https://zenodo.org/records/1416094/files/LaitinenL11.pdf",
        "abstract": "We consider the problem of transposition and time-warp invariant (TTWI) polyphonic content-based music retrieval (CBMR) in symbolically encoded music. For this setting, we introduce two new algorithms based on dynamic programming. Given a query point set, of size m, to be searched for in a database point set, of size n, and applying a search window of width w, our algorithms run in time O(mnw) for finding exact TTWI occurrences, and O(mnw2) for partial occurrences. Our new algorithms are computationally more efficient as their counterparts in the worst case scenario. More importantly, the elegance of our algorithms lies in their simplicity: they are much easier to implement and to understand than the rivalling sweepline-based algorithms. Our solution bears also theoretical interest. Dynamic programming has been used in very basic content-based retrieval problems, but generalizing them to more complex cases has proven to be challenging. In this special, seemingly more complex case, however, dynamic programming seems to be a viable option.",
        "zenodo_id": 1416094,
        "dblp_key": "conf/ismir/LaitinenL11",
        "keywords": [
            "dynamic programming",
            "transposition and time-warp invariant",
            "polyphonic content-based music retrieval",
            "symbolically encoded music",
            "search window",
            "exact TTWI occurrences",
            "partial occurrences",
            "searching for in a database",
            "computational efficiency",
            "simpler implementation"
        ]
    },
    {
        "title": "Social Capital and Music Discovery: An Examination of the Ties through Which Late Adolescents Discover New Music.",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417253",
        "url": "https://doi.org/10.5281/zenodo.1417253",
        "ee": "https://zenodo.org/records/1417253/files/Laplante11.pdf",
        "abstract": "Research on everyday life information seeking has demonstrated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new information. This study employed social network analysis to examine the characteristics of the ties through which late adolescents (15-17 years old) discover new music. In-depth interviews with 19 adolescents were conducted, which generated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent contacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that parents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features.",
        "zenodo_id": 1417253,
        "dblp_key": "conf/ismir/Laplante11",
        "keywords": [
            "weak ties",
            "acquaintances",
            "social network analysis",
            "late adolescents",
            "new music",
            "in-depth interviews",
            "334 ties",
            "strong ties",
            "homophilous",
            "parents"
        ]
    },
    {
        "title": "How Similar Is Too Similar?: Exploring Users&apos; Perceptions of Similarity in Playlist Evaluation.",
        "author": [
            "Jin Ha Lee 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417124",
        "url": "https://doi.org/10.5281/zenodo.1417124",
        "ee": "http://ismir2011.ismir.net/papers/PS1-12.pdf",
        "abstract": "The Audio Music Similarity and Retrieval (AMS) task in the annual Music Information Retrieval eXchange relies on human-evaluation. One limitation of the current design of AMS is that evaluators are provided with scarce contextual information as to why they are evaluating the similarity of the songs and how this information will be used. This study explores the potential use of AMS results for generating playlists based on similarity. We asked participants to listen to a subset of results from the 2010 AMS task and evaluate the set of candidates generated by the algorithms as a playlist generated from a seed song (the query). We found that while similarity does affect how people feel about the candidate set as a playlist, other factors such as variety, metadata, personal preference, familiarity, mix of familiar and new music, etc. also strongly affect users' perceptions of playlist quality as well. We discuss six user behaviors in detail and the implications for the AMS evaluation task.",
        "zenodo_id": 1417125,
        "dblp_key": "conf/ismir/Lee11",
        "keywords": [
            "Audio Music Similarity and Retrieval (AMS)",
            "human-evaluation limitation",
            "playlist generation",
            "results analysis",
            "user behavior study",
            "seed song evaluation",
            "algorithm-generated playlists",
            "playlist quality perception",
            "variety and metadata",
            "personal preference"
        ]
    },
    {
        "title": "Rhythm Extraction from Polyphony Symbolic Music.",
        "author": [
            "Florence Lev\u00e9",
            "Richard Groult",
            "Guillaume Arnaud",
            "Cyril S\u00e9guin",
            "R\u00e9mi Gaymay",
            "Mathieu Giraud"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416368",
        "url": "https://doi.org/10.5281/zenodo.1416368",
        "ee": "https://zenodo.org/records/1416368/files/LeveGASGG11.pdf",
        "abstract": "In this paper, we focus on the rhythmic component of symbolic music similarity, proposing several ways to extract a monophonic rhythmic signature from a symbolic polyphonic score. To go beyond the simple extraction of all time intervals between onsets (noteson extraction), we select notes according to their length (short and long extractions) or their intensities (intensity+/\u2212extractions). Once the rhythm is extracted, we use dynamic programming to compare several sequences. We report results of analysis on the size of rhythm patterns that are specific to a unique piece, as well as experiments on similarity queries (ragtime music and Bach chorale variations). These results show that long and intensity+ extractions are often good choices for rhythm extraction. Our conclusions are that, even from polyphonic symbolic music, rhythm alone can be enough to identify a piece or to perform pertinent music similarity queries, especially when using wise rhythm extractions.",
        "zenodo_id": 1416368,
        "dblp_key": "conf/ismir/LeveGASGG11",
        "keywords": [
            "rhythmic component",
            "symbolic music similarity",
            "monophonic rhythmic signature",
            "noteson extraction",
            "short and long extractions",
            "intensity+/\u2212extractions",
            "dynamic programming",
            "rhythm patterns",
            "unique piece",
            "music similarity queries"
        ]
    },
    {
        "title": "Improving Perceptual Tempo Estimation with Crowd-Sourced Annotations.",
        "author": [
            "Mark Levy"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417583",
        "url": "https://doi.org/10.5281/zenodo.1417583",
        "ee": "https://zenodo.org/records/1417583/files/Levy11.pdf",
        "abstract": "We report the design and results of a web-based experiment intended to support the development and evaluation of tempo estimation algorithms, in which users tap to music and select descriptive labels. Analysis of the tapping data and labels chosen shows that, while different listeners frequently entrain to different metrical levels for some pieces, they rarely disagree about which pieces are fast and which are slow. We show how this result can be used to improve both the evaluation metrics used for automatic tempo estimation and the estimation algorithms themselves. We also report the relative performance of two recent tempo estimation methods according to a further controlled experiment that does not depend on groundtruth values of any kind.",
        "zenodo_id": 1417583,
        "dblp_key": "conf/ismir/Levy11",
        "keywords": [
            "web-based experiment",
            "tempo estimation algorithms",
            "tapping data",
            "descriptive labels",
            "entrainment to metrical levels",
            "listener agreement",
            "evaluation metrics",
            "automatic tempo estimation",
            "tempo estimation methods",
            "controlled experiment"
        ]
    },
    {
        "title": "Expressive Timing from Cross-Performance and Audio-based Alignment Patterns: An Extended Case Study.",
        "author": [
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416992",
        "url": "https://doi.org/10.5281/zenodo.1416992",
        "ee": "https://zenodo.org/records/1416992/files/LiemH11.pdf",
        "abstract": "Audio recordings of classical music pieces reflect the artistic interpretation of the piece as seen by the recorded performing musician. With many recordings being typically available for the same music piece, multiple expressive rendition variations of this piece are obtained, many of which are induced by the underlying musical content. In earlier work, we focused on timing as a means of expressivity, and proposed a light-weight, unsupervised and audio-based method to study timing deviations among different performances through alignment patterns. By using the standard deviation of alignment patterns as a measure for the display of individuality in a recording, structural and interpretational aspects of a music piece turned out to be highlighted in a qualitative case study on five Chopin mazurkas. In this paper, we propose an entropy-based deviation measure as an alternative to the existing standard deviation measure. The obtained results for multiple short-time window resolutions, both from a quantitative and qualitative perspective, strengthen our earlier finding that the found patterns are musically informative and confirm that entropy is a good alternative measure for highlighting expressive timing deviations in recordings.",
        "zenodo_id": 1416992,
        "dblp_key": "conf/ismir/LiemH11",
        "keywords": [
            "Audio recordings",
            "artistic interpretation",
            "performing musician",
            "multiple expressive renditions",
            "alignment patterns",
            "structural and interpretational aspects",
            "quantitative and qualitative results",
            "musical informality",
            "entropy-based deviation measure",
            "expressive timing deviations"
        ]
    },
    {
        "title": "Guitar Tab Mining, Analysis and Ranking.",
        "author": [
            "Robert Macrae",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416476",
        "url": "https://doi.org/10.5281/zenodo.1416476",
        "ee": "https://zenodo.org/records/1416476/files/MacraeD11.pdf",
        "abstract": "With over 4.5 million tablatures and chord sequences (collectively known as tabs), the web holds vast quantities of hand annotated scores in non-standardised text files. These scores are typically error-prone and incomplete, and tab collections contain many duplicates, making retrieval of high quality tabs difficult. Despite this, tabs are by far the most popular means of sharing musical instructions on the internet. We have developed tools that use text analysis and alignment for the automatic retrieval, interpretation and analysis of such tabs in order to filter and estimate the most accurate tabs from the multitude available. We show that the standard means of ranking tabs, such as search engine ranks or user ratings, have little correlation with the accuracy of a tab and that a better ranking method is to use features such as the concurrency between tabs of the same song. We also compare the quality of top-ranked tabs with state-of-the-art chord transcription output and find that the latter provides a more reliable source of chord symbols with an accuracy rate 10% higher than the ranked hand annotations.",
        "zenodo_id": 1416476,
        "dblp_key": "conf/ismir/MacraeD11",
        "keywords": [
            "4.5 million tabs",
            "non-standardised text files",
            "error-prone and incomplete",
            "tab collections duplicates",
            "high quality tabs difficult",
            "web holds vast quantities",
            "standard means of ranking",
            "concurrency between tabs",
            "state-of-the-art chord transcription",
            "more reliable source of chord symbols"
        ]
    },
    {
        "title": "Music Mood Classification of Television Theme Tunes.",
        "author": [
            "Mark Mann",
            "Trevor J. Cox",
            "Francis F. Li"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415884",
        "url": "https://doi.org/10.5281/zenodo.1415884",
        "ee": "https://zenodo.org/records/1415884/files/MannCL11.pdf",
        "abstract": "This paper introduces methods used for Music Mood Classification to assist in the automated tagging of television programme theme tunes for the first time. The methods employed use a knowledge driven approach with tailored parameters extractable from the Matlab MIR Toolbox [1]. Four new features were developed, three based on tonality and one on tempo, to enable a degree of quantified tagging, using support vector machines, employing various kernels, optimised along six mood axes. Using a \u201cnearest neighbour\u201d method of optimisation, a success rate in the range of 80-94% was achieved in being able to classify musical audio on a five point mood scale.",
        "zenodo_id": 1415884,
        "dblp_key": "conf/ismir/MannCL11",
        "keywords": [
            "Music Mood Classification",
            "Automated tagging",
            "Television programme theme tunes",
            "Knowledge driven approach",
            "Matlab MIR Toolbox",
            "Four new features",
            "Tonality",
            "Tempo",
            "Support vector machines",
            "Kernel optimisation"
        ]
    },
    {
        "title": "Three Current Issues In Music Autotagging.",
        "author": [
            "Gon\u00e7alo Marques",
            "Marcos Aur\u00e9lio Domingues",
            "Thibault Langlois",
            "Fabien Gouyon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417757",
        "url": "https://doi.org/10.5281/zenodo.1417757",
        "ee": "https://zenodo.org/records/1417757/files/MarquesDLG11.pdf",
        "abstract": "The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance.",
        "zenodo_id": 1417757,
        "dblp_key": "conf/ismir/MarquesDLG11",
        "keywords": [
            "music autotagging",
            "state-of-the-art",
            "evaluation fragility",
            "tag generality",
            "audio features",
            "tag correlations",
            "performance improvement",
            "natural tag correlations",
            "second stage",
            "exploiting correlations"
        ]
    },
    {
        "title": "New Trends in Musical Genre Classification Using Optimum-Path Forest.",
        "author": [
            "Caio Miguel Marques",
            "Ivan Rizzo Guilherme",
            "Rodrigo Y. M. Nakamura",
            "Jo\u00e3o P. Papa"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417657",
        "url": "https://doi.org/10.5281/zenodo.1417657",
        "ee": "https://zenodo.org/records/1417657/files/MarquesGNP11.pdf",
        "abstract": "Musical genre classification has been paramount in the last years, mainly in large multimedia datasets, in which new songs and genres can be added at every moment by anyone. In this context, we have seen the growing of musical recommendation systems, which can improve the benefits for several applications, such as social networks and collective musical libraries. In this work, we have introduced a recent machine learning technique named Optimum-Path Forest (OPF) for musical genre classification, which has been demonstrated to be similar to the state-of-the-art pattern recognition techniques, but much faster for some applications. Experiments in two public datasets were conducted against Support Vector Machines and a Bayesian classifier to show the validity of our work. In addition, we have executed an experiment using very recent hybrid feature selection techniques based on OPF to speed up feature extraction process.",
        "zenodo_id": 1417657,
        "dblp_key": "conf/ismir/MarquesGNP11",
        "keywords": [
            "Optimum-Path Forest (OPF)",
            "musical genre classification",
            "machine learning technique",
            "Support Vector Machines",
            "Bayesian classifier",
            "public datasets",
            "feature selection techniques",
            "feature extraction process",
            "social networks",
            "collective musical libraries"
        ]
    },
    {
        "title": "Exemplar-based Assignment of Large Missing Audio Parts using String Matching on Tonal Features.",
        "author": [
            "Benjamin Martin 0001",
            "Pierre Hanna",
            "Ta Vinh Thong",
            "Myriam Desainte-Catherine",
            "Pascal Ferraro"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417977",
        "url": "https://doi.org/10.5281/zenodo.1417977",
        "ee": "https://zenodo.org/records/1417977/files/MartinHTDF11.pdf",
        "abstract": "We propose a new approach for assigning audio data in large missing audio parts (from 1 to 16 seconds). Inspired by image inpainting approaches, the proposed method uses the repetitive aspect of music pieces on musical features to recover missing segments via an exemplar-based reconstruction. Tonal features combined with a string matching technique allows locating repeated segments accurately. The evaluation consists in performing on both musician and nonmusician subjects listening tests of randomly reconstructed audio excerpts, and experiments highlight good results in assigning musically relevant parts. The contribution of this paper is twofold: bringing musical features to solve a signal processing problem in the case of large missing audio parts, and successfully applying exemplar-based techniques on musical signals while keeping a musical consistency on audio pieces.",
        "zenodo_id": 1417977,
        "dblp_key": "conf/ismir/MartinHTDF11",
        "keywords": [
            "audio data",
            "missing audio parts",
            "inpainting approaches",
            "musical features",
            "repeated segments",
            "exemplar-based reconstruction",
            "tonal features",
            "string matching technique",
            "musical consistency",
            "signal processing problem"
        ]
    },
    {
        "title": "Cross-Modal Aesthetics from A Feature Extraction Perspective: A Pilot Study.",
        "author": [
            "Alison Mattek",
            "Michael A. Casey"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414906",
        "url": "https://doi.org/10.5281/zenodo.1414906",
        "ee": "https://zenodo.org/records/1414906/files/MattekC11.pdf",
        "abstract": "This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conducted a behavioral experiment asking subjects to assess similarity between 10 musical recordings and 10 works of across subjects as to which images correspond to which audio, even though neither the audio nor the images possessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leaveone-subject-out and leave-one-audio-out crossvalidation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked similar images using only a given audio clip's features. A retrieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual similarity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with p<0.03 for the second model.",
        "zenodo_id": 1414906,
        "dblp_key": "conf/ismir/MattekC11",
        "keywords": [
            "perceptual relationships",
            "auditory and visual domains",
            "behavioral experiment",
            "subjective similarity judgments",
            "feature space",
            "regression models",
            "leaveone-subject-out",
            "leave-one-audio-out",
            "crossvalidation",
            "predicted image features"
        ]
    },
    {
        "title": "Timbre and Melody Features for the Recognition of Vocal Activity and Instrumental Solos in Polyphonic Music.",
        "author": [
            "Matthias Mauch",
            "Hiromasa Fujihara",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415716",
        "url": "https://doi.org/10.5281/zenodo.1415716",
        "ee": "https://zenodo.org/records/1415716/files/MauchFYG11.pdf",
        "abstract": "We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant melody line, and have not been used in the context of vocal/instrumental activity detection. Using a support vector machine hidden Markov model we conduct 14 experiments to validate several combinations of our proposed features. Our results clearly demonstrate the benefit of combining the features: the best performance was always achieved by combining all four features. The top accuracy for vocal activity detection is 87.2%. The more difficult task of detecting instrumental solos equally benefits from the combination of all features and achieves an accuracy of 89.8% and a satisfactory precision of 61.1%. With this paper we also release to the public the 102 annotations we used for training and testing. The annotations offer not only vocal/nonvocal labels, but also distinguish between female and male singers, and different solo instruments. Keywords: vocal activity detection, pitch fluctuation, F0 segregation, instrumental solo detection, ground truth, SVM",
        "zenodo_id": 1415716,
        "dblp_key": "conf/ismir/MauchFYG11",
        "keywords": [
            "instrumental solos",
            "vocal activity detection",
            "support vector machine hidden Markov model",
            "audio features",
            "melody line extraction",
            "ground truth",
            "F0 segregation",
            "polyphonic music recordings",
            "vocal/nonvocal labels",
            "precision"
        ]
    },
    {
        "title": "Structural Change on Multiple Time Scales as a Correlate of Musical Complexity.",
        "author": [
            "Matthias Mauch",
            "Mark Levy"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416240",
        "url": "https://doi.org/10.5281/zenodo.1416240",
        "ee": "https://zenodo.org/records/1416240/files/MauchL11.pdf",
        "abstract": "We propose the novel audio feature structural change for the analysis and visualisation of recorded music, and argue that it is related to a particular notion of musical complexity. Structural change is a meta feature that can be calculated from an arbitrary frame-wise basis feature, with each element in the structural change feature vector representing the change of the basis feature at a different time scale. We describe an efficient implementation of the feature and discuss its properties based on three basis features pertaining to harmony, rhythm and timbre. We present a novel flowerlike visualisation that allows us to illustrate the overall structural change characteristics of a piece of audio in a compact way. Several examples of real-world music and synthesised audio exemplify the characteristics of the structural change feature. We present the results of a web-based listening experiment with 197 participants to show the validity of the proposed feature. Keywords: audio, musical complexity, visualisation",
        "zenodo_id": 1416240,
        "dblp_key": "conf/ismir/MauchL11",
        "keywords": [
            "audio",
            "structural change",
            "music",
            "analysis",
            "visualisation",
            "basis feature",
            "harmony",
            "rhythm",
            "timbre",
            "flowerlike visualisation"
        ]
    },
    {
        "title": "Music Genre Classification by Ensembles of Audio and Lyrics Features.",
        "author": [
            "Rudolf Mayer",
            "Andreas Rauber"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416844",
        "url": "https://doi.org/10.5281/zenodo.1416844",
        "ee": "https://zenodo.org/records/1416844/files/MayerR11.pdf",
        "abstract": "Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set.",
        "zenodo_id": 1416844,
        "dblp_key": "conf/ismir/MayerR11",
        "keywords": [
            "algorithms",
            "music",
            "characteristics",
            "organise",
            "recommend",
            "users",
            "size",
            "private",
            "commercial",
            "collections"
        ]
    },
    {
        "title": "Large-scale music similarity search with spatial trees.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414930",
        "url": "https://doi.org/10.5281/zenodo.1414930",
        "ee": "https://zenodo.org/records/1414930/files/McFeeL11.pdf",
        "abstract": "Many music information retrieval tasks require finding the nearest neighbors of a query item in a high-dimensional space. However, the complexity of computing nearest neighbors grows linearly with size of the database, making exact retrieval impractical for large databases. We investigate modern variants of the classical KD-tree algorithm, which efficiently index high-dimensional data by recursive spatial partitioning. Experiments on the Million Song Dataset demonstrate that content-based similarity search can be significantly accelerated by the use of spatial partitioning structures.",
        "zenodo_id": 1414930,
        "dblp_key": "conf/ismir/McFeeL11",
        "keywords": [
            "music information retrieval",
            "nearest neighbors",
            "high-dimensional space",
            "complexity",
            "exact retrieval",
            "large databases",
            "KD-tree algorithm",
            "spatial partitioning",
            "content-based similarity search",
            "accelerated"
        ]
    },
    {
        "title": "The Natural Language of Playlists.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418119",
        "url": "https://doi.org/10.5281/zenodo.1418119",
        "ee": "https://zenodo.org/records/1418119/files/McFeeL11a.pdf",
        "abstract": "We propose a simple, scalable, and objective evaluation procedure for playlist generation algorithms. Drawing on standard techniques for statistical natural language processing, we characterize playlist algorithms as generative models of strings of songs belonging to some unknown language. To demonstrate the procedure, we compare several playlist algorithms derived from content, semantics, and meta-data. We then develop an efficient algorithm to learn an optimal combination of simple playlist algorithms. Experiments on a large collection of naturally occurring playlists demonstrate the efficacy of the evaluation procedure and learning algorithm.",
        "zenodo_id": 1418119,
        "dblp_key": "conf/ismir/McFeeL11a",
        "keywords": [
            "evaluation procedure",
            "playlist generation algorithms",
            "statistical natural language processing",
            "generative models",
            "unknown language",
            "comparison",
            "efficient algorithm",
            "learning",
            "optimal combination",
            "experiments"
        ]
    },
    {
        "title": "A Musical Web Mining and Audio Feature Extraction Extension to The Greenstone Digital Library Software.",
        "author": [
            "Cory McKay",
            "David Bainbridge 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415006",
        "url": "https://doi.org/10.5281/zenodo.1415006",
        "ee": "https://zenodo.org/records/1415006/files/McKayB11.pdf",
        "abstract": "This paper describes updates to the Greenstone open source digital library software that significantly expand its functionality with respect to music. The first of the two major improvements now allows Greenstone to extract and store classification-oriented features from audio files using a newly updated version of the jAudio software. The second major improvement involves the implementation and integration of the new jSongMiner software, which provides Greenstone with a framework for automatically identifying audio recordings using audio fingerprinting and then extracting extensive metadata about them from a variety of resources available on the Internet. Several illustrative use cases and case studies are discussed.",
        "zenodo_id": 1415006,
        "dblp_key": "conf/ismir/McKayB11",
        "keywords": [
            "Greenstone",
            "digital library",
            "music",
            "classification-oriented features",
            "audio files",
            "jAudio",
            "jSongMiner",
            "audio fingerprinting",
            "metadata",
            "Internet"
        ]
    },
    {
        "title": "Mining the Correlation between Lyrical and Audio Features and the Emergence of Mood.",
        "author": [
            "Matt McVicar",
            "Tim Freeman 0001",
            "Tijl De Bie"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415888",
        "url": "https://doi.org/10.5281/zenodo.1415888",
        "ee": "https://zenodo.org/records/1415888/files/McVicarFB11.pdf",
        "abstract": "Understanding the mood of music holds great potential for recommendation and genre identification problems. Unfortunately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are required. In this paper we present a new unsupervised learning approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially determined by the mood, and that there are no other strong common effects affecting these aspects of music. Based on this assumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Correlation Analysis, and confirm our hypothesis in a subsequent analysis of the results.",
        "zenodo_id": 1415888,
        "dblp_key": "conf/ismir/McVicarFB11",
        "keywords": [
            "unsupervised learning",
            "mood recognition",
            "lyrics",
            "audio",
            "ground truth",
            "multi-modal analysis",
            "Canonical Correlation Analysis",
            "hypothesis",
            "effectiveness",
            "confirmation"
        ]
    },
    {
        "title": "Leveraging Noisy Online Databases for Use in Chord Recognition.",
        "author": [
            "Matt McVicar",
            "Yizhao Ni",
            "Tijl De Bie",
            "Ra\u00fal Santos-Rodriguez"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418311",
        "url": "https://doi.org/10.5281/zenodo.1418311",
        "ee": "https://zenodo.org/records/1418311/files/McVicarNBS11.pdf",
        "abstract": "The most significant problem faced by Machine Learningbased chord recognition systems is arguably the lack of highquality training examples. In this paper, we address this problem by leveraging the availability of chord annotations from guitarist websites. We show that such annotations can be used as partial supervision of a semi-supervised chord recognition method\u2014partial since accurate timing information is lacking. A particular challenge in the exploitation of these data is their low quality, potentially even leading to a performance degradation if used directly. We demonstrate however that a curriculum learning strategy can be used to automatically rank annotations according to their potential for improving the performance. Using this strategy, our experiments show a modest improvement for a simple major/minor chord alphabet, but a highly significant improvement for a much larger chord alphabet.",
        "zenodo_id": 1418311,
        "dblp_key": "conf/ismir/McVicarNBS11",
        "keywords": [
            "high-quality training examples",
            "guitarist websites",
            "partial supervision",
            "curriculum learning",
            "annotation quality",
            "performance degradation",
            "curriculum learning strategy",
            "improvement for a simple major/minor chord alphabet",
            "significant improvement",
            "much larger chord alphabet"
        ]
    },
    {
        "title": "Accelerating The Mixing Phase In Studio Recording Productions By Automatic Audio Alignment.",
        "author": [
            "Nicola Montecchio",
            "Arshia Cont"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415228",
        "url": "https://doi.org/10.5281/zenodo.1415228",
        "ee": "https://zenodo.org/records/1415228/files/MontecchioC11.pdf",
        "abstract": "We propose a system for accelerating the mixing phase in a recording production, by making use of audio alignment techniques to automatically align multiple takes of excerpts of a music piece against a performance of the whole work. We extend the approach of our previous work, based on sequential Montecarlo inference techniques, that was targeted at real-time alignment for score/audio following. The proposed approach is capable of producing partial alignments as well as identifying relevant regions in the partial results with regards to the reference, for better integration within a studio mix workflow. The approach is evaluated using data obtained from two recording sessions of classical music pieces, and we discuss its effectiveness for reducing manual work in a production chain.",
        "zenodo_id": 1415228,
        "dblp_key": "conf/ismir/MontecchioC11",
        "keywords": [
            "accelerating",
            "audio alignment",
            "multiple takes",
            "music piece",
            "performance",
            "Montecarlo inference",
            "real-time alignment",
            "score/audio following",
            "partial alignments",
            "relevant regions"
        ]
    },
    {
        "title": "Chroma Toolbox: Matlab Implementations for Extracting Variants of Chroma-Based Audio Features.",
        "author": [
            "Meinard M\u00fcller",
            "Sebastian Ewert"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416032",
        "url": "https://doi.org/10.5281/zenodo.1416032",
        "ee": "https://zenodo.org/records/1416032/files/MullerE11.pdf",
        "abstract": "Chroma-based audio features, which closely correlate to the aspect of harmony, are a well-established tool in processing and analyzing music data. There are many ways of computing and enhancing chroma features, which results in a large number of chroma variants with different properties. In this paper, we present a chroma toolbox [13], which contains MATLAB implementations for extracting various types of recently proposed pitch-based and chroma-based audio features. Providing the MATLAB implementations on a welldocumented website under a GNU-GPL license, our aim is to foster research in music information retrieval. As another goal, we want to raise awareness that there is no single chroma variant that works best in all applications. To this end, we discuss two example applications showing that the final music analysis result may crucially depend on the initial feature design step.",
        "zenodo_id": 1416032,
        "dblp_key": "conf/ismir/MullerE11",
        "keywords": [
            "Chroma-based audio features",
            "processing and analyzing music data",
            "chroma features",
            "MATLAB implementations",
            "pitch-based and chroma-based",
            "well-established tool",
            "large number of chroma variants",
            "GNU-GPL license",
            "research in music information retrieval",
            "awareness of chroma variant properties"
        ]
    },
    {
        "title": "A Segment-Based Fitness Measure for Capturing Repetitive Structures of Music Recordings.",
        "author": [
            "Meinard M\u00fcller",
            "Peter Grosche",
            "Nanzhu Jiang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416342",
        "url": "https://doi.org/10.5281/zenodo.1416342",
        "ee": "https://zenodo.org/records/1416342/files/MullerGJ11.pdf",
        "abstract": "In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment \u201cexplains\u201d the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music.",
        "zenodo_id": 1416342,
        "dblp_key": "conf/ismir/MullerGJ11",
        "keywords": [
            "fitness measure",
            "audio segment",
            "repetitive structure",
            "enhanced feature representations",
            "tempo variations",
            "instrumentation changes",
            "modulations",
            "practicability",
            "challenging examples",
            "field recordings"
        ]
    },
    {
        "title": "Constrained Spectrum Generation Using A Probabilistic Spectrum Envelope for Mixed Music Analysis.",
        "author": [
            "Toru Nakashika",
            "Tetsuya Takiguchi",
            "Yasuo Ariki"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415774",
        "url": "https://doi.org/10.5281/zenodo.1415774",
        "ee": "https://zenodo.org/records/1415774/files/NakashikaTA11.pdf",
        "abstract": "NMF (Non-negative Matrix Factorization) has been one of the most widely-used techniques for musical signal analysis in recent years. In particular, the supervised type of NMF is garnering much attention in source separation with respect to the analysis accuracy and speed. In this approach, a large number of spectral samples is used for analyzing a signal. If the system has a minimal number of samples, the accuracy deteriorates. Because such methods require all the possible samples for the analysis, it is hard to build a practical analysis system. To analyze signals properly even when short of samples, we propose a novel method that combines a supervised NMF and probabilistic search algorithms. In this approach, it is assumed that each instrumental category has a model-invariant feature called a probabilistic spectrum envelope (PSE). The algorithm starts with learning the PSEs of each category using a technique based on Gaussian Process Regression. Using the PSEs for spectrum generation, an observed spectrum is analyzed under the framework of a supervised NMF. The optimum spectrum can be searched by Genetic Algorithm using sparseness and density constraints.",
        "zenodo_id": 1415774,
        "dblp_key": "conf/ismir/NakashikaTA11",
        "keywords": [
            "Non-negative Matrix Factorization",
            "source separation",
            "supervised type",
            "analysis accuracy",
            "speed",
            "spectrum samples",
            "probabilistic search algorithms",
            "probabilistic spectrum envelope",
            "Gaussian Process Regression",
            "Genetic Algorithm"
        ]
    },
    {
        "title": "A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.",
        "author": [
            "Juhan Nam",
            "Jiquan Ngiam",
            "Honglak Lee",
            "Malcolm Slaney"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418351",
        "url": "https://doi.org/10.5281/zenodo.1418351",
        "ee": "https://zenodo.org/records/1418351/files/NamNLS11.pdf",
        "abstract": "Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results show that the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods.",
        "zenodo_id": 1418351,
        "dblp_key": "conf/ismir/NamNLS11",
        "keywords": [
            "unsupervised feature learning",
            "deep belief networks",
            "musical data",
            "polyphonic piano transcription",
            "classification-based",
            "jointly training classifiers",
            "multiple notes",
            "training speed",
            "classification performance",
            "public piano datasets"
        ]
    },
    {
        "title": "Music Information Robotics: Coping Strategies for Musically Challenged Robots.",
        "author": [
            "Steven R. Ness",
            "Shawn Trail",
            "Peter F. Driessen",
            "W. Andrew Schloss",
            "George Tzanetakis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415868",
        "url": "https://doi.org/10.5281/zenodo.1415868",
        "ee": "https://zenodo.org/records/1415868/files/NessTDST11.pdf",
        "abstract": "In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot(s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition.",
        "zenodo_id": 1415868,
        "dblp_key": "conf/ismir/NessTDST11",
        "keywords": [
            "music robotics",
            "robotic instruments",
            "sound generation",
            "actuators",
            "performances",
            "compositions",
            "control messages",
            "musical understanding",
            "batch processing",
            "real time performance"
        ]
    },
    {
        "title": "Associations between Musicology and Music Information Retrieval.",
        "author": [
            "Kerstin Neubarth",
            "Mathieu Bergeron",
            "Darrell Conklin"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416564",
        "url": "https://doi.org/10.5281/zenodo.1416564",
        "ee": "https://zenodo.org/records/1416564/files/NeubarthBC11.pdf",
        "abstract": "A higher level of interdisciplinary collaboration between music information retrieval (MIR) and musicology has been proposed both in terms of MIR tools for musicology, and musicological motivation and interpretation of MIR research. Applying association mining and content citation analysis",
        "zenodo_id": 1416564,
        "dblp_key": "conf/ismir/NeubarthBC11"
    },
    {
        "title": "On the Importance of &quot;Real&quot; Audio Data for MIR Algorithm Evaluation at the Note-Level A Comparative Study.",
        "author": [
            "Bernhard Niedermayer",
            "Sebastian B\u00f6ck",
            "Gerhard Widmer"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417923",
        "url": "https://doi.org/10.5281/zenodo.1417923",
        "ee": "https://zenodo.org/records/1417923/files/NiedermayerBW11.pdf",
        "abstract": "A considerable number of MIR tasks requires annotations at the note-level for the purpose of in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms \u2013 Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithms\u2019 performance on the different datasets varies considerably, but synthesized audio, does not necessarily yield better results.",
        "zenodo_id": 1417923,
        "dblp_key": "conf/ismir/NiedermayerBW11",
        "keywords": [
            "audio quality",
            "source",
            "Onset Detection",
            "Audio Alignment",
            "piano pieces",
            "commercial high-quality sample library",
            "real (computer-controlled) grand piano",
            "artistic changes in tempo and dynamics",
            "natural asynchronies",
            "performance varies considerably"
        ]
    },
    {
        "title": "Classifying Bach&apos;s Handwritten C-Clefs.",
        "author": [
            "Masahiro Niitsuma",
            "Yo Tomita"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415822",
        "url": "https://doi.org/10.5281/zenodo.1415822",
        "ee": "https://zenodo.org/records/1415822/files/NiitsumaT11.pdf",
        "abstract": "The aim of this study is to explore how we could use computational technology to help determination of the chronology of music manuscripts. Applying a battery of techniques to Bach\u2019s manuscripts reveals the limitation in current image processing techniques, thereby clarifying future tasks. Analysis of C-clefs, the chosen musical symbol for this study, extracted from Bach\u2019s manuscripts dating from 1708\u20131748, is also carried out. Random forest using 15 features produces significant accuracy for chronological classification.",
        "zenodo_id": 1415822,
        "dblp_key": "conf/ismir/NiitsumaT11",
        "keywords": [
            "computational technology",
            "chronology of music manuscripts",
            "image processing techniques",
            "Bach\u2019s manuscripts",
            "C-clefs",
            "music symbol",
            "future tasks",
            "random forest",
            "15 features",
            "chronological classification"
        ]
    },
    {
        "title": "Stochastic Modeling of a Musical Performance with Expressive Representations from the Musical Score.",
        "author": [
            "Kenta Okumura",
            "Shinji Sako",
            "Tadashi Kitamura"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417093",
        "url": "https://doi.org/10.5281/zenodo.1417093",
        "ee": "https://zenodo.org/records/1417093/files/OkumuraSK11.pdf",
        "abstract": "This paper presents a method for describing the characteristics of human musical performance. We consider the problem of building models that express the ways in which deviations from a strict interpretations of the score occurs in the performance, and that cluster these deviations automatically. The clustering process is performed using expressive representations unambiguously notated on the musical score, without any arbitrariness by the human observer. The result of clustering is obtained as hierarchical tree structures for each deviational factor that occurred during the operation of the instrument. This structure represents an approximation of the performer\u2019s interpretation with information notated on the score they used during the performance. This model represents the conditions that generate the difference in the fluctuation of performance expression and the amounts of deviational factors directly from the data of real performance. Through validations of applying the method to the data measured from real performances, we show that the use of information regarding expressive representation on the musical score enables the efficient estimation of generative-model for the musical performance.",
        "zenodo_id": 1417093,
        "dblp_key": "conf/ismir/OkumuraSK11",
        "keywords": [
            "musical performance",
            "deviations from score",
            "automatic clustering",
            "expressive representations",
            "hierarchical tree structures",
            "interpretation approximation",
            "fluctuation of performance",
            "generative-model estimation",
            "real performances",
            "information regarding expressive representation"
        ]
    },
    {
        "title": "MusiCLEF: a Benchmark Activity in Multimodal Music Information Retrieval.",
        "author": [
            "Nicola Orio",
            "David Rizo",
            "Riccardo Miotto",
            "Markus Schedl",
            "Nicola Montecchio",
            "Olivier Lartillot"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416506",
        "url": "https://doi.org/10.5281/zenodo.1416506",
        "ee": "https://zenodo.org/records/1416506/files/OrioRMSML11.pdf",
        "abstract": "This work presents the rationale, tasks and procedures of MusiCLEF, a novel benchmarking activity that has been developed along with the Cross-Language Evaluation Forum (CLEF). The main goal of MusiCLEF is to promote the development of new methodologies for music access and retrieval on real public music collections, which can combine content-based information, automatically extracted from music files, with contextual information, provided by users via tags, comments, or reviews. Moreover, MusiCLEF aims at maintaining a tight connection with real application scenarios, focusing on issues on music access and retrieval that are faced by professional users. To this end, this year\u2019s evaluation campaign focused on two main tasks: automatic categorization of music to be used as soundtrack of TV shows and automatic identification of the digitized material of a music digital library.",
        "zenodo_id": 1416506,
        "dblp_key": "conf/ismir/OrioRMSML11",
        "keywords": [
            "MusiCLEF",
            "Cross-Language Evaluation Forum",
            "music access and retrieval",
            "real public music collections",
            "content-based information",
            "contextual information",
            "professional users",
            "TV shows",
            "music digital library",
            "evaluation campaign"
        ]
    },
    {
        "title": "Incremental Bayesian Audio-to-Score Alignment with Flexible Harmonic Structure Models.",
        "author": [
            "Takuma Otsuka",
            "Kazuhiro Nakadai",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418159",
        "url": "https://doi.org/10.5281/zenodo.1418159",
        "ee": "https://zenodo.org/records/1418159/files/OtsukaNOO11.pdf",
        "abstract": "Music information retrieval, especially the audio-to-score alignment problem, often involves a matching problem between the audio and symbolic representations. We must cope with uncertainty in the audio signal generated from the score in a symbolic representation such as the variation in the timbre or temporal fluctuations. Existing audio-to-score alignment methods are sometimes vulnerable to the uncertainty in which multiple notes are simultaneously played with a variety of timbres because these methods rely on static observation models. For example, a chroma vector or a fixed harmonic structure template is used under the assumption that musical notes in a chord are all in the same volume and timbre. This paper presents a particle filterbased audio-to-score alignment method with a flexible observation model based on latent harmonic allocation. Our method adapts to the harmonic structure for the audio-toscore matching based on the observation of the audio signal through Bayesian inference. Experimental results with 20 polyphonic songs reveal that our method is effective when more number of instruments are involved in the ensemble.",
        "zenodo_id": 1418159,
        "dblp_key": "conf/ismir/OtsukaNOO11",
        "keywords": [
            "audio-to-score alignment",
            "matching problem",
            "audio signal",
            "symbolic representations",
            "timbre",
            "temporal fluctuations",
            "chroma vector",
            "fixed harmonic structure template",
            "Bayesian inference",
            "latent harmonic allocation"
        ]
    },
    {
        "title": "l1-Graph Based Music Structure Analysis.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos",
            "Gonzalo R. Arce"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417335",
        "url": "https://doi.org/10.5281/zenodo.1417335",
        "ee": "https://zenodo.org/records/1417335/files/PanagakisKA11.pdf",
        "abstract": "An unsupervised approach for automatic music structure analysis is proposed resorting to the following assumption: If the feature vectors extracted from a specific music segment are drawn from a single subspace, then the sequence of feature vectors extracted from a music recording will lie in a union of as many subspaces as the music segments in this recording are. It is well known that each feature vector stemming from a union of independent linear subspaces admits a sparse representation with respect to a dictionary formed by all other feature vectors with nonzero coefficients associated only to feature vectors that stem from its own subspace. Such sparse representation reveals the relationships among the feature vectors and it is used to construct a similarity graph, the so-called \u21131-graph. Accordingly, the segmentation of audio features is obtained by applying spectral clustering to the \u21131-graph. The performance of the just described approach is assessed by conducting experiments on the PopMusic and the UPF Beatles benchmark datasets. Promising results are reported.",
        "zenodo_id": 1417335,
        "dblp_key": "conf/ismir/PanagakisKA11",
        "keywords": [
            "unsupervised",
            "automatic",
            "music",
            "structure",
            "analysis",
            "subspace",
            "sparse",
            "representation",
            "spectral",
            "clustering"
        ]
    },
    {
        "title": "Sparse Signal Decomposition on Hybrid Dictionaries Using Musical Priors.",
        "author": [
            "H\u00e9l\u00e8ne Papadopoulos",
            "Matthieu Kowalski"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416264",
        "url": "https://doi.org/10.5281/zenodo.1416264",
        "ee": "https://zenodo.org/records/1416264/files/PapadopoulosK11.pdf",
        "abstract": "This paper investigates the use of musical priors for sparse expansion of audio signals of music on overcomplete dictionaries taken from the union of two orthonormal bases. More specifically, chord information is used to build a structured model that takes into account dependencies between coefficients of the decomposition. Evaluation on various music signals shows that our approach provides results whose quality measured by the signal-to-noise ratio corresponds to state-of-the-art approaches, and shows that our model is relevant to represent audio signals of Western tonal music and opens new perspectives.",
        "zenodo_id": 1416264,
        "dblp_key": "conf/ismir/PapadopoulosK11",
        "keywords": [
            "musical priors",
            "sparse expansion",
            "audio signals",
            "overcomplete dictionaries",
            "tonal music",
            "structured model",
            "dependencies",
            "signal-to-noise ratio",
            "evaluation",
            "approaches"
        ]
    },
    {
        "title": "New Approaches to Optical Music Recognition.",
        "author": [
            "Christopher Raphael",
            "Jingya Wang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414856",
        "url": "https://doi.org/10.5281/zenodo.1414856",
        "ee": "https://zenodo.org/records/1414856/files/RaphaelW11.pdf",
        "abstract": "We present the beginnings of a new system for optical music recognition (OMR), aimed toward the score images of the International Music Score Library Project (IMSLP). Our system focuses on measures as the basic unit of recognition. We identify candidate composite symbols (chords and beamed groups) using grammatically-formulated top-down model-based methods, while employing template matching to find isolated rigid symbols. We reconcile these overlapping symbols by seeking non-overlapping variants of the composite symbols that best account for the pixel data. We present results on a representative score from the IMSLP.",
        "zenodo_id": 1414856,
        "dblp_key": "conf/ismir/RaphaelW11",
        "keywords": [
            "Optical Music Recognition",
            "International Music Score Library Project",
            "Measures as the basic unit",
            "Grammatically-formulated top-down model-based methods",
            "Template matching",
            "Isolated rigid symbols",
            "Reconciling overlapping symbols",
            "Non-overlapping variants",
            "Pixel data",
            "Representative score"
        ]
    },
    {
        "title": "An Empirical Study of Multi-Label Classifiers for Music Tag Annotation.",
        "author": [
            "Chris Sanden",
            "John Z. Zhang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416432",
        "url": "https://doi.org/10.5281/zenodo.1416432",
        "ee": "https://zenodo.org/records/1416432/files/SandenZ11.pdf",
        "abstract": "In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a series of empirical experiments to evaluate a set of multi-label classifiers and demonstrate which ones are more suitable for music tag annotation. Furthermore, we discuss various factors in the classification process, such as feature sets, frame sizes, etc. Experiments on two publicly available datasets show that the Calibrated Label Ranking (CLR) algorithm outperforms the other classifiers for a selection of evaluation measures.",
        "zenodo_id": 1416432,
        "dblp_key": "conf/ismir/SandenZ11",
        "keywords": [
            "automatic music tag annotation",
            "computational classification process",
            "acoustic features",
            "music tags",
            "empirical experiments",
            "multi-label classifiers",
            "Calibrated Label Ranking (CLR)",
            "evaluation measures",
            "publicly available datasets",
            "classification process factors"
        ]
    },
    {
        "title": "A Regularity-Constrained Viterbi Algorithm and Its Application to The Structural Segmentation of Songs.",
        "author": [
            "Gabriel Sargent",
            "Fr\u00e9d\u00e9ric Bimbot",
            "Emmanuel Vincent 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415950",
        "url": "https://doi.org/10.5281/zenodo.1415950",
        "ee": "https://zenodo.org/records/1415950/files/SargentBV11.pdf",
        "abstract": "This paper presents a general approach for the structural segmentation of songs. It is formalized as a cost optimization problem that combines properties of the musical content and prior regularity assumption on the segment length. A versatile implementation of this approach is proposed by means of a Viterbi algorithm, and the design of the costs are discussed. We then present two systems derived from this approach, based on acoustic and symbolic features respectively. The advantages of the regularity constraint are evaluated on a database of 100 popular songs by showing a significant improvement of the segmentation performance in terms of F-measure.",
        "zenodo_id": 1415950,
        "dblp_key": "conf/ismir/SargentBV11",
        "keywords": [
            "structural segmentation",
            "cost optimization problem",
            "musical content",
            "prior regularity assumption",
            "Viterbi algorithm",
            "acoustic features",
            "symbolic features",
            "database of 100 popular songs",
            "F-measure",
            "significant improvement"
        ]
    },
    {
        "title": "Investigating the Similarity Space of Music Artists on the Micro-Blogosphere.",
        "author": [
            "Markus Schedl",
            "Peter Knees",
            "Sebastian B\u00f6ck"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414808",
        "url": "https://doi.org/10.5281/zenodo.1414808",
        "ee": "https://zenodo.org/records/1414808/files/SchedlKB11.pdf",
        "abstract": "Microblogging services such as Twitter have become an important means to share information. In this paper, we thoroughly analyze their potential for a key challenge in the field of MIR, namely the elaboration of perceptually meaningful similarity measures. To this end, comprehensiveevaluation experiments were conducted using Twitter posts gathered during a period of several months. We investigated 23,100 combinations of different term weighting strategies, normalization methods, index term sets, Twitter query schemes, and similarity measurement techniques, aiming at determining in which way they influence the similarity estimates\u2019 quality. Evaluation was performed on the task of similar artist retrieval. Two data sets were used: one of 224 well-known artists with a uniform genre distribution, the other constituting a collection of 3,000 artists extracted from last.fm and allmusic.com.",
        "zenodo_id": 1414808,
        "dblp_key": "conf/ismir/SchedlKB11",
        "keywords": [
            "Twitter",
            "microblogging",
            "information sharing",
            "MIR",
            "perceptually meaningful similarity measures",
            "comprehensive evaluation experiments",
            "term weighting strategies",
            "normalization methods",
            "index term sets",
            "Twitter query schemes"
        ]
    },
    {
        "title": "Modeling Musical Emotion Dynamics with Conditional Random Fields.",
        "author": [
            "Erik M. Schmidt",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416606",
        "url": "https://doi.org/10.5281/zenodo.1416606",
        "ee": "https://zenodo.org/records/1416606/files/SchmidtK11.pdf",
        "abstract": "Human emotion responses to music are dynamic processes that evolve naturally over time in synchrony with the music. It is because of this dynamic nature that systems which seek to predict emotion in music must necessarily analyze such processes on short-time intervals, modeling not just the relationships between acoustic data and emotion parameters, but how those relationships evolve over time. In this work we seek to model such relationships using a conditional random field (CRF), a powerful graphical model which is trained to predict the conditional probability p(y|x) for a sequence of labels y given a sequence of features x. Treating our features as deterministic, we retain the rich local subtleties present in the data, which is especially applicable to contentbased audio analysis, given the abundance of data in these problems. We train our graphical model on the emotional responses of individual annotators in an 11\u00d711 quantized representation of the arousal-valence (A-V) space. Our model is fully connected, and can produce estimates of the conditional probability for each A-V bin, allowing us to easily model complex emotion-space distributions (e.g. multimodal) as an A-V heatmap.",
        "zenodo_id": 1416606,
        "dblp_key": "conf/ismir/SchmidtK11",
        "keywords": [
            "Dynamic processes",
            "Music emotion responses",
            "Synchrony with music",
            "Short-time intervals",
            "Conditional random field (CRF)",
            "Conditional probability",
            "Arousal-valence (A-V) space",
            "Annotators",
            "Emotion-space distributions",
            "Multimodal"
        ]
    },
    {
        "title": "Using Mutual Proximity to Improve Content-Based Audio Similarity.",
        "author": [
            "Dominik Schnitzer",
            "Arthur Flexer",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417979",
        "url": "https://doi.org/10.5281/zenodo.1417979",
        "ee": "https://zenodo.org/records/1417979/files/SchnitzerFSW11.pdf",
        "abstract": "This work introduces Mutual Proximity, an unsupervised method which transforms arbitrary distances to similarities computed from the shared neighborhood of two data points. This reinterpretation aims to correct inconsistencies in the original distance space, like the hub phenomenon. Hubs are objects which appear unwontedly often as nearest neighbors in predominantly high-dimensional spaces. We apply Mutual Proximity to a widely used and standard content-based audio similarity algorithm. The algorithm is known to be negatively affected by the high number of hubs it produces. We show that without a modification of the audio similarity features or inclusion of additional knowledge about the datasets, applying Mutual Proximity leads to a significant increase of retrieval quality: (1) hubs decrease and (2) the k-nearest-neighbor classification rates increase significantly. The results of this paper show that taking the mutual neighborhood of objects into account is an important aspect which should be considered for this class of content-based audio similarity algorithms.",
        "zenodo_id": 1417979,
        "dblp_key": "conf/ismir/SchnitzerFSW11",
        "keywords": [
            "Mutual Proximity",
            "unsupervised method",
            "arbitrary distances",
            "similarity computed",
            "shared neighborhood",
            "hub phenomenon",
            "high-dimensional spaces",
            "audio similarity algorithm",
            "negatively affected",
            "significant increase"
        ]
    },
    {
        "title": "A Re-ordering Strategy for Accelerating Index-based Audio Fingerprinting.",
        "author": [
            "Hendrik Schreiber 0001",
            "Peter Grosche",
            "Meinard M\u00fcller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417607",
        "url": "https://doi.org/10.5281/zenodo.1417607",
        "ee": "https://zenodo.org/records/1417607/files/SchreiberGM11.pdf",
        "abstract": "The Haitsma/Kalker audio fingerprinting system [4] has been in use for years, but its search algorithm\u2019s scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprint\u2019s subprints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of",
        "zenodo_id": 1417607,
        "dblp_key": "conf/ismir/SchreiberGM11",
        "keywords": [
            "Haitsma/Kalker audio fingerprinting system",
            "search algorithm scalability",
            "re-ordering subprints",
            "index-based retrieval step",
            "overall search performance",
            "increased significantly",
            "longer fingerprints",
            "re-ordering",
            "higher performance gains",
            "up to a factor"
        ]
    },
    {
        "title": "Multi-Modal Non-Prototypical Music Mood Analysis in Continuous Space: Reliability and Performances.",
        "author": [
            "Bj\u00f6rn W. Schuller",
            "Felix Weninger",
            "Johannes Dorfner"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417097",
        "url": "https://doi.org/10.5281/zenodo.1417097",
        "ee": "https://zenodo.org/records/1417097/files/SchullerWD11.pdf",
        "abstract": "Music Mood Classification is frequently turned into \u2018Music Mood Regression\u2019 by using a continuous dimensional model rather than discrete mood classes. In this paper we report on automatic analysis of performances in a mood space spanned by arousal and valence on the 2.6 k songs NTWICM corpus of popular UK chart music in full realism, i. e., by automatic web-based retrieval of lyrics and diverse acoustic features without pre-selection of prototypical cases. We discuss optimal modeling of the gold standard by introducing the evaluator weighted estimator principle, group-wise feature relevance, \u2018tuning\u2019 of the regressor, and compare early and late fusion strategies. In the result, correlation coefficients of .736 (valence) and .601 (arousal) are reached on previously unseen test data.",
        "zenodo_id": 1417097,
        "dblp_key": "conf/ismir/SchullerWD11",
        "keywords": [
            "Music Mood Classification",
            "Continuous dimensional model",
            "Discrete mood classes",
            "Automatic analysis",
            "Mood space",
            "Arousal and valence",
            "NTWICM corpus",
            "Popular UK chart music",
            "Web-based retrieval",
            "Acoustic features"
        ]
    },
    {
        "title": "Analysis of Acoustic Features for Automated Multi-Track Mixing.",
        "author": [
            "Jeffrey J. Scott",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416038",
        "url": "https://doi.org/10.5281/zenodo.1416038",
        "ee": "https://zenodo.org/records/1416038/files/ScottK11.pdf",
        "abstract": "The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a finished product, namely determining the relative gain levels of each track to produce a final, mixed song. By modeling the time-varying mixing coefficients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks.",
        "zenodo_id": 1416038,
        "dblp_key": "conf/ismir/ScottK11",
        "keywords": [
            "digital music content",
            "multi-track recording",
            "professional quality work",
            "fundamental processes",
            "relative gain levels",
            "final mixed song",
            "time-varying mixing coefficients",
            "linear dynamical system",
            "weight vector",
            "instrument features"
        ]
    },
    {
        "title": "Modeling Melodic Improvisation in Turkish Folk Music Using Variable-Length Markov Models.",
        "author": [
            "Sertan Sent\u00fcrk",
            "Parag Chordia"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415570",
        "url": "https://doi.org/10.5281/zenodo.1415570",
        "ee": "https://zenodo.org/records/1415570/files/SenturkC11.pdf",
        "abstract": "The paper describes a new database, which currently consists of 64 songs encompassing approximately 6600 notes, and a system, which uses Variable-Length Markov Models (VLMM) to predict the melodies in the uzun hava (long tune) form, a melodic structure in Turkish folk music. The work shows VLMMs are highly predictive. This suggests that variable-length Markov models (VLMMs) may be applied to makam-based and non-metered musical forms, in addition to Western musical traditions. To the best of our knowledge, the work presents the first symbolic, machine readable database of uzun havas and the first application of predictive modeling in Turkish folk music.",
        "zenodo_id": 1415570,
        "dblp_key": "conf/ismir/SenturkC11",
        "keywords": [
            "database",
            "Variable-Length Markov Models (VLMM)",
            "melodies",
            "Turkish folk music",
            "makam-based",
            "non-metered musical forms",
            "Western musical traditions",
            "symbolic",
            "machine readable database",
            "first application"
        ]
    },
    {
        "title": "A Multicultural Approach in Music Information Research.",
        "author": [
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416592",
        "url": "https://doi.org/10.5281/zenodo.1416592",
        "ee": "https://zenodo.org/records/1416592/files/Serra11.pdf",
        "abstract": "Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facilitating the access of a small part of the world\u2019s information to a small part of the world's population. The current IT research efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a result, most of our data models, cognition models, user models, interaction models, ontologies, etc., are culturally biased. This fact is quite evident in music information research, since, despite the world's richness in terms of musical culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Research Council entitled \"CompMusic: Computational Models for the discovery of the world's music.\" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges.",
        "zenodo_id": 1416592,
        "dblp_key": "conf/ismir/Serra11",
        "keywords": [
            "multicultural reality",
            "paradigms of western culture",
            "IT research efforts",
            "information bias",
            "western centered approach",
            "data models",
            "cognition models",
            "user models",
            "interaction models",
            "ontologies"
        ]
    },
    {
        "title": "Assessing the Tuning of Sung Indian Classical Music.",
        "author": [
            "Joan Serr\u00e0",
            "Gopala K. Koduri",
            "Marius Miron",
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415102",
        "url": "https://doi.org/10.5281/zenodo.1415102",
        "ee": "https://zenodo.org/records/1415102/files/SerraKMS11.pdf",
        "abstract": "The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them.",
        "zenodo_id": 1415102,
        "dblp_key": "conf/ismir/SerraKMS11",
        "keywords": [
            "The issue of tuning in Indian classical music",
            "historical debate",
            "contemporary practice",
            "sung performances",
            "Carnatic and Hindustani music",
            "empiric and quantitative approach",
            "stable fundamental frequencies",
            "interval histograms",
            "music sources",
            "theoretical values"
        ]
    },
    {
        "title": "Complexity Driven Recombination of MIDI Loops.",
        "author": [
            "George Sioros",
            "Carlos Guedes"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415804",
        "url": "https://doi.org/10.5281/zenodo.1415804",
        "ee": "https://zenodo.org/records/1415804/files/SiorosG11.pdf",
        "abstract": "An algorithm and a software application for recombining in real time MIDI drum loops that makes use of a novel analysis of rhythmic patterns that sorts them in order of their complexity is presented. We measure rhythmic complexity by comparing each rhythmic pattern found in the loops to a metrical template characteristic of its time signature. The complexity measure is used to sort the MIDI loops prior to utilizing them in the recombination algorithm. This way, the user can effectively control the complexity and variation in the generated rhythm during performance.",
        "zenodo_id": 1415804,
        "dblp_key": "conf/ismir/SiorosG11",
        "keywords": [
            "algorithm",
            "recombining",
            "real-time",
            "MIDI",
            "drum",
            "loops",
            "novel",
            "analysis",
            "rhythmic",
            "patterns"
        ]
    },
    {
        "title": "Tarsos a Platform to Explore Pitch Scales in Non-Western and Western Music.",
        "author": [
            "Joren Six",
            "Olmo Cornelis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418355",
        "url": "https://doi.org/10.5281/zenodo.1418355",
        "ee": "https://zenodo.org/records/1418355/files/SixC11.pdf",
        "abstract": "This paper presents Tarsos 1 , a modular software platform to extract and analyze pitch and scale organization in music, especially geared towards the analysis of non-Western music. Tarsos aims to be a user-friendly, graphical tool to explore tone scales and pitch organization in music of the world. With Tarsos pitch annotations are extracted from an audio signal that are then processed to form musicologically meaningful representations. These representations cover more than the typical Western 12 pitch classes, since a fine-grained resolution of 1200 cents is used. Both scales with and without octave equivalence can be displayed graphically. The Tarsos API 2 creates opportunities to analyse large sets of ethnic music automatically. The graphical user interface can be used for detailed, manually adjusted analysis of specific songs. Several output modalities make Tarsos an interesting tool for musicological analysis, educational purposes and even for artistic productions.",
        "zenodo_id": 1418355,
        "dblp_key": "conf/ismir/SixC11",
        "keywords": [
            "Tarsos 1",
            "modular software platform",
            "pitch and scale organization",
            "non-Western music",
            "user-friendly",
            "graphical tool",
            "tone scales",
            "pitch organization",
            "world music",
            "Tarsos API"
        ]
    },
    {
        "title": "Design and creation of a large-scale database of structural annotations.",
        "author": [
            "Jordan Bennett Louis Smith",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga",
            "David De Roure",
            "J. Stephen Downie"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416884",
        "url": "https://doi.org/10.5281/zenodo.1416884",
        "ee": "https://zenodo.org/records/1416884/files/SmithBFRD11.pdf",
        "abstract": "This paper describes the design and creation of an unprecedentedly large database of over 2400 structural annotations of nearly 1400 musical recordings. The database is intended to be a test set for algorithms that will be used to analyze a much larger corpus of hundreds of thousands of recordings, as part of the Structural Analysis of Large Amounts of Musical Information (SALAMI) project. This paper describes the design goals of the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the development of an annotation format and procedure that adapts work by Peeters and Deruty [10], and the management and execution of the project. We also summarize some of the properties of the resulting corpus of annotations, including average inter-annotator agreement.",
        "zenodo_id": 1416884,
        "dblp_key": "conf/ismir/SmithBFRD11",
        "keywords": [
            "database",
            "structural annotations",
            "musical recordings",
            "SALAMI project",
            "algorithm analysis",
            "corpus of recordings",
            "average inter-annotator agreement",
            "unprecedentedly large",
            "practical issues",
            "development of annotation format"
        ]
    },
    {
        "title": "A Comparative Study of Collaborative vs. Traditional Musical Mood Annotation.",
        "author": [
            "Jacquelin A. Speck",
            "Erik M. Schmidt",
            "Brandon G. Morton",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417075",
        "url": "https://doi.org/10.5281/zenodo.1417075",
        "ee": "https://zenodo.org/records/1417075/files/SpeckSMK11.pdf",
        "abstract": "Organizing music by emotional association is a natural process for humans, but the ambiguous nature of emotion makes it a difficult task for machines. Automatic systems for music emotion recognition rely on ground truth data collected from humans, and more effective methods for collecting such data are being continuously developed. In previous work, we developed MoodSwings, an online collaborative game for crowdsourcing dynamic (persecond) mood ratings from multiple players within the twodimensional arousal-valence (A-V) representation of emotion. MoodSwings has proven effective for data collection, but potential data effects caused by collaborative labeling have not yet been analyzed. In this work, we compare the effectiveness of MoodSwings to that of a more traditional data collection method, where annotation is performed by single, paid annotators. We implement a simplified labeling task to run on Amazon\u2019s crowdsourcing engine, Mechanical Turk (MTurk), and analyze the labels collected with each method. A statistical comparison shows consistencies between MoodSwings and MTurk data, and we produce similar results using each as training data for automatic emotion production via supervised machine learning. Furthermore the new dataset collected via MTurk has been made available to the Music Information Retrieval community.",
        "zenodo_id": 1417075,
        "dblp_key": "conf/ismir/SpeckSMK11",
        "keywords": [
            "Automatic systems for music emotion recognition",
            "Ground truth data collected from humans",
            "Crowdsourcing dynamic mood ratings",
            "Two-dimensional arousal-valence representation",
            "MoodSwings online collaborative game",
            "Mechanical Turk crowdsourcing engine",
            "Comparison of data collection methods",
            "Statistical analysis of labels",
            "Similar results using each as training data",
            "New dataset collected via MTurk"
        ]
    },
    {
        "title": "MIR in School? Lessons from Ethnographic Observation of Secondary School Music Classes.",
        "author": [
            "Dan Stowell",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416980",
        "url": "https://doi.org/10.5281/zenodo.1416980",
        "ee": "https://zenodo.org/records/1416980/files/StowellD11.pdf",
        "abstract": "To help maximise the usefulness of MIR technologies in the wider community, we conducted an ethnographic study of music lessons in secondary schools in London, UK. The purpose is to understand better how musical concepts are negotiated with and without technology, so we can understand when and how MIR tools might be useful. We report on some of the themes uncovered, both about the range of technologies deployed in schools and about the ways different musical concepts are discussed. Importantly, this rich observation elicits some of the nuances between various highand low-technologies. In particular, we discuss issues of multimodality and the role of technologies such as Youtube, as well as specific issues around musical concepts such as genre and rhythm.",
        "zenodo_id": 1416980,
        "dblp_key": "conf/ismir/StowellD11",
        "keywords": [
            "ethnographic study",
            "music lessons",
            "secondary schools",
            "MIR technologies",
            "musical concepts",
            "technologies deployed",
            "musical concepts discussed",
            "nuances between high- and low-technologies",
            "multimodality",
            "YouTube"
        ]
    },
    {
        "title": "OCR based post processing of OMR for the recovery of transposing instruments in complex orchestral scores.",
        "author": [
            "Verena Thomas",
            "Christian Wagner 0003",
            "Michael Clausen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415790",
        "url": "https://doi.org/10.5281/zenodo.1415790",
        "ee": "https://zenodo.org/records/1415790/files/ThomasWC11.pdf",
        "abstract": "Given a scanned score page, Optical Music Recognition (OMR) attempts to reconstruct all contained music information. However, the available OMR systems lack the ability to recognize transposition information contained in complex orchestral scores. 1 An additional unsolved OMR problem is the handling of orchestral scores using compressed notation. 2 Here, the information of which instrument has to play which staff is crucial for a correct interpretation of the score. But this mapping is lost along the pages of the score during the OMR process. In this paper, we present a method for retrieving the instrumentation and transposition information of orchestral scores. In our approach, we combine the results of Optical Character Recognition (OCR) and OMR to regain the information available through text annotations of the score. In addition, a method to reconstruct the instrument and transposition information for staves where text annotations were omitted or not recognized is presented. In an evaluation we analyze the impact of transposition information on the quality of score-audio synchronizations of orchestral music. The results show that the knowledge of transposing instruments improves the synchronization accuracy and that our method helps in regaining this knowledge.",
        "zenodo_id": 1415790,
        "dblp_key": "conf/ismir/ThomasWC11",
        "keywords": [
            "transposition information",
            "complex orchestral scores",
            "compressed notation",
            "instrumentation",
            "transposition information",
            "score-audio synchronizations",
            "instrument mapping",
            "staves",
            "text annotations",
            "retrieving information"
        ]
    },
    {
        "title": "A Postprocessing Technique for Improved Harmonic/Percussion Separation for Polyphonic Music.",
        "author": [
            "Balaji Thoshkahna",
            "Ramakrishnan R. Kalpathi"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417056",
        "url": "https://doi.org/10.5281/zenodo.1417056",
        "ee": "https://zenodo.org/records/1417056/files/ThoshkahnaK11.pdf",
        "abstract": "In this paper we propose a postprocessing technique for a spectrogram diffusion based harmonic/percussion decomposition algorithm. The proposed technique removes harmonic instrument leakages in the percussion enhanced outputs of the baseline algorithm. The technique uses median filtering and an adaptive detection of percussive segments in subbands followed by piecewise signal reconstruction using envelope properties to ensure that percussion is enhanced while harmonic leakages are suppressed. A new binary mask is created for the percussion signal which upon applying on the original signal improves harmonic versus percussion separation. We compare our algorithm with two recent techniques and show that on a database of polyphonic Indian music, the postprocessing algorithm improves the harmonic versus percussion decomposition significantly.",
        "zenodo_id": 1417056,
        "dblp_key": "conf/ismir/ThoshkahnaK11",
        "keywords": [
            "postprocessing",
            "spectrogram diffusion",
            "harmonic/percussion decomposition",
            "harmonic instrument leakages",
            "percussive segments",
            "piecewise signal reconstruction",
            "envelope properties",
            "binary mask",
            "harmonic versus percussion separation",
            "polyphonic Indian music"
        ]
    },
    {
        "title": "A Transient Detection Algorithm for Audio Using Iterative Analysis of STFT.",
        "author": [
            "Balaji Thoshkahna",
            "Fran\u00e7ois Xavier Nsabimana",
            "Ramakrishnan R. Kalpathi"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415630",
        "url": "https://doi.org/10.5281/zenodo.1415630",
        "ee": "https://zenodo.org/records/1415630/files/ThoshkahnaNK11.pdf",
        "abstract": "We propose an iterative algorithm to detect transient segments in audio signals. Short time Fourier transform (STFT) is used to detect rapid local changes in the audio signal. The algorithm has two steps that iteratively (a) calculate a function of the STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of monophonic sounds, the algorithm achieved an F-measure of 90% while on our database of polyphonic audio an F-measure of 91% was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme.",
        "zenodo_id": 1415630,
        "dblp_key": "conf/ismir/ThoshkahnaNK11",
        "keywords": [
            "iterative algorithm",
            "detect transient segments",
            "audio signals",
            "short time Fourier transform",
            "rapid local changes",
            "localised spectral noise",
            "ground truth dataset",
            "F-measure",
            "preprocessing step",
            "tempo analysis algorithm"
        ]
    },
    {
        "title": "Factorization of Overlapping Harmonic Sounds Using Approximate Matching Pursuit.",
        "author": [
            "Steven K. Tjoa",
            "K. J. Ray Liu"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414962",
        "url": "https://doi.org/10.5281/zenodo.1414962",
        "ee": "https://zenodo.org/records/1414962/files/TjoaL11.pdf",
        "abstract": "Factorization of polyphonic musical signals remains a difficult problem due to the presence of overlapping harmonics. Existing dictionary learning methods cannot guarantee that the learned dictionary atoms are semantically meaningful. In this paper, we explore the factorization of harmonic musical signals when a fixed dictionary of harmonic sounds is already present. We propose a method called approximate matching pursuit (AMP) that can efficiently decompose harmonic sounds by using a known predetermined dictionary. We illustrate the effectiveness of AMP by decomposing polyphonic musical spectra with respect to a large dictionary of instrumental sounds. AMP executes faster than orthogonal matching pursuit yet performs comparably based upon recall and precision.",
        "zenodo_id": 1414962,
        "dblp_key": "conf/ismir/TjoaL11",
        "keywords": [
            "Factorization",
            "polyphonic",
            "musical",
            "signals",
            "overlapping",
            "harmonics",
            "existing",
            "dictionary",
            "learning",
            "methods"
        ]
    },
    {
        "title": "Elementary Sources: Latent Component Analysis for Music Composition.",
        "author": [
            "Spencer S. Topel",
            "Michael A. Casey"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417351",
        "url": "https://doi.org/10.5281/zenodo.1417351",
        "ee": "https://zenodo.org/records/1417351/files/TopelC11.pdf",
        "abstract": "Complexity of music audio signals creates an access problem to specific musical objects or structures within the source samples. Instead of employing more commonly used audio analysis or production techniques to access features, we describe extraction of sub-mixtures from real-world audio using a Probabilistic Latent Component Analysis-based decomposition tool for music composition. This is highlighted with the presentation of a prior relevant compositional approach named Spectral Music along with a discussion of five compositions extending these principles using methods more commonly associated with source separation research.",
        "zenodo_id": 1417351,
        "dblp_key": "conf/ismir/TopelC11",
        "keywords": [
            "audio analysis",
            "music composition",
            "Probabilistic Latent Component Analysis",
            "sub-mixtures",
            "source separation research",
            "Spectral Music",
            "composition principles",
            "source samples",
            "access problem",
            "specific musical objects"
        ]
    },
    {
        "title": "Tempo Estimation Based on Linear Prediction and Perceptual Modelling.",
        "author": [
            "Georgina Tryfou",
            "Aki H\u00e4rm\u00e4",
            "Athanasios Mouchtaris"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416386",
        "url": "https://doi.org/10.5281/zenodo.1416386",
        "ee": "https://zenodo.org/records/1416386/files/TryfouHM11.pdf",
        "abstract": "Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece.",
        "zenodo_id": 1416386,
        "dblp_key": "conf/ismir/TryfouHM11",
        "keywords": [
            "tempo estimation",
            "automatic induction",
            "musical excerpt",
            "feature list",
            "periodicities",
            "Gammatone subspace analysis",
            "Linear Prediction Error Filters (LPEFs)",
            "tempo perception",
            "human listeners",
            "evaluation"
        ]
    },
    {
        "title": "A Preplexity Based Cover Song Matching System for Short Length Queries.",
        "author": [
            "Erdem Unal",
            "Elaine Chew",
            "Panayiotis G. Georgiou",
            "Shrikanth S. Narayanan"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415134",
        "url": "https://doi.org/10.5281/zenodo.1415134",
        "ee": "https://zenodo.org/records/1415134/files/UnalCGN11.pdf",
        "abstract": "A music retrieval system that matches a short length music query with its variations in a database is proposed. In order to avoid the negative effects of different orchestration and performance style and tempo on transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using ngrams of sizes 2 through 6.  We observe improvements in retrieval performance with increasing query length and ngram order. The improvement converges to a little over one for all query lengths tested when n reaches 6.",
        "zenodo_id": 1415134,
        "dblp_key": "conf/ismir/UnalCGN11",
        "keywords": [
            "music retrieval system",
            "matches music query",
            "database variations",
            "orchestration and performance style",
            "tempo effects",
            "transcription and matching",
            "mid-level representation schema",
            "tonal modeling approach",
            "sequence of music tags",
            "major and minor triad labels"
        ]
    },
    {
        "title": "Information Retrieval Meta-Evaluation: Challenges and Opportunities in the Music Domain.",
        "author": [
            "Juli\u00e1n Urbano"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417897",
        "url": "https://doi.org/10.5281/zenodo.1417897",
        "ee": "https://zenodo.org/records/1417897/files/Urbano11.pdf",
        "abstract": "The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now. Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged. The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues. From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line.",
        "zenodo_id": 1417897,
        "dblp_key": "conf/ismir/Urbano11",
        "keywords": [
            "Music Information Retrieval",
            "rigorous scientific evaluations",
            "Music Information Retrieval Evaluation eXchange",
            "experimental validity",
            "Text Information Retrieval",
            "evaluation forum",
            "reliability and correctness",
            "music community",
            "evaluation of music systems",
            "IR cycle"
        ]
    },
    {
        "title": "Audio Music Similarity and Retrieval: Evaluation Power and Stability.",
        "author": [
            "Juli\u00e1n Urbano",
            "Diego Mart\u00edn 0001",
            "M\u00f3nica Marrero",
            "Jorge Morato"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417269",
        "url": "https://doi.org/10.5281/zenodo.1417269",
        "ee": "https://zenodo.org/records/1417269/files/UrbanoMMM11.pdf",
        "abstract": "In this paper we analyze the reliability of the results in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluation, that is, how often a significant difference is found between systems and how often these significant differences are incorrect. We study the effect of using different effectiveness measures with different sets of relevance judgments, for varying number of queries and alternative statistical procedures. Different measures are shown to behave similarly overall, though some are much more sensitive and stable than others. The use of different statistical procedures does improve the reliability of the results, and it allows using as little as half the number of queries currently used in MIREX evaluations while still offering very similar reliability levels. We also conclude that experimenters can be very confident that if a significant difference is found between two systems, the difference is indeed real.",
        "zenodo_id": 1417269,
        "dblp_key": "conf/ismir/UrbanoMMM11",
        "keywords": [
            "reliability",
            "evaluation",
            "Audio Music Similarity",
            "Retrieval systems",
            "power",
            "stability",
            "effectiveness measures",
            "relevance judgments",
            "statistical procedures",
            "experimenters"
        ]
    },
    {
        "title": "Modeling Dynamic Patterns for Emotional Content in Music.",
        "author": [
            "Yonatan Vaizman",
            "Roni Y. Granot",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416580",
        "url": "https://doi.org/10.5281/zenodo.1416580",
        "ee": "https://zenodo.org/records/1416580/files/VaizmanGL11.pdf",
        "abstract": "Emotional content is a major component in music. It has long been a research topic of interest to discover the acoustic patterns in the music that carry that emotional information, and enable performers to communicate emotional messages to listeners. Previous works looked in the audio signal for local cues, most of which assume monophonic music, and their statistics over time. Here, we used generic audio features, that can be calculated for any audio signal, and focused on the progression of these features through time, investigating how informative the dynamics of the audio is for emotional content. Our data is comprised of piano and vocal improvisations of musically trained performers, instructed to convey 4 categorical emotions. We applied Dynamic Texture Mixture (DTM), that models both the instantaneous sound qualities and their dynamics, and demonstrated the strength of the model. We further showed that once taking the dynamics into account even highly reduced versions of the generic audio features carry a substantial amount of information about the emotional content. Finally, we demonstrate how interpreting the parameters of the trained models can yield interesting cognitive suggestions.",
        "zenodo_id": 1416580,
        "dblp_key": "conf/ismir/VaizmanGL11",
        "keywords": [
            "emotional content",
            "acoustic patterns",
            "music",
            "performers",
            "listeners",
            "generic audio features",
            "progression of features",
            "dynamics",
            "musically trained performers",
            "categorical emotions"
        ]
    },
    {
        "title": "Pulse Detection in Syncopated Rhythms Using Neural Oscillators.",
        "author": [
            "Marc J. Velasco",
            "Edward W. Large"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417519",
        "url": "https://doi.org/10.5281/zenodo.1417519",
        "ee": "https://zenodo.org/records/1417519/files/VelascoL11.pdf",
        "abstract": "Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms.",
        "zenodo_id": 1417519,
        "dblp_key": "conf/ismir/VelascoL11",
        "keywords": [
            "syncopated rhythms",
            "perceived periodicities",
            "rhythmic stimuli",
            "energy at frequencies",
            "perceived pulse or meter",
            "metrical frequencies",
            "nonlinear resonance",
            "duple and triple meters",
            "resonance at the pulse frequency",
            "pulse detection"
        ]
    },
    {
        "title": "Automatic Pitch Detection in Printed Square Notation.",
        "author": [
            "Gabriel Vigliensoni",
            "John Ashley Burgoyne",
            "Andrew Hankinson",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415580",
        "url": "https://doi.org/10.5281/zenodo.1415580",
        "ee": "https://zenodo.org/records/1415580/files/VigliensoniBHF11.pdf",
        "abstract": "In this paper we present our research in the development of a pitch-finding system to extract the pitches of neumes\u2014some of the oldest representations of pitch in Western music\u2014 from the Liber Usualis, a well-known compendium of plainchant as used in the Roman Catholic church. Considerations regarding the staff position, staff removal, spaceand linezones, as well as how we treat specific neume classes and modifiers are covered. This type of notation presents a challenge for traditional optical music recognition (OMR) systems because individual note pitches are indivisible from the larger ligature group that forms the neume. We have created a dataset of correctly-notated transcribed chant for comparing the performance of different variants of our pitch-finding system. The best result showed a recognition rate of 97% tested with more than 2000 neumes.",
        "zenodo_id": 1415580,
        "dblp_key": "conf/ismir/VigliensoniBHF11",
        "keywords": [
            "pitch-finding system",
            "neumes",
            "Liber Usualis",
            "plainchant",
            "Roman Catholic church",
            "staff position",
            "staff removal",
            "spaceand linezones",
            "neume classes",
            "modifiers"
        ]
    },
    {
        "title": "Peachnote: Music Score Search and Analysis Platform.",
        "author": [
            "Vladimir Viro"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417261",
        "url": "https://doi.org/10.5281/zenodo.1417261",
        "ee": "https://zenodo.org/records/1417261/files/Viro11.pdf",
        "abstract": "Hundreds of thousands of music scores are being digitized by libraries all over the world. In contrast to books, they generally remain inaccessible for content-based retrieval and algorithmic analysis. There is no analogue to Google Books for music scores, and there exist no large corpora of symbolic music data that would empower musicology in the way large text corpora are empowering computational linguistics, sociology, history, and other humanities that have printed word as their major source of evidence about their research subjects. We want to help change that. In this paper we present the first result of our work in this direction the Music Ngram Viewer and search engine, an analog of Google Books Ngram Viewer and Google Books search for music scores.",
        "zenodo_id": 1417261,
        "dblp_key": "conf/ismir/Viro11",
        "keywords": [
            "Digitization",
            "Music scores",
            "Content-based retrieval",
            "Algorithmic analysis",
            "Google Books",
            "Corpora",
            "Musicology",
            "Computational linguistics",
            "Sociology",
            "History"
        ]
    },
    {
        "title": "Music Emotion Classification of Chinese Songs based on Lyrics Using TF*IDF and Rhyme.",
        "author": [
            "Xing Wang",
            "Xiaoou Chen",
            "Deshun Yang",
            "Yuqian Wu"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416458",
        "url": "https://doi.org/10.5281/zenodo.1416458",
        "ee": "https://zenodo.org/records/1416458/files/WangCYW11.pdf",
        "abstract": "This paper presents the outcomes of research into an automatic classification system based on the lingual part of music. Two novel kinds of short features are extracted from lyrics using tf*idf and rhyme. Meta-learning algorithm is adapted to combine these two sets of features. Results show that our features promote the accuracy of classification and meta-learning algorithm is effective in fusing the two features.",
        "zenodo_id": 1416458,
        "dblp_key": "conf/ismir/WangCYW11",
        "keywords": [
            "automatic classification system",
            "lingual part of music",
            "novel kinds of short features",
            "tf*idf",
            "rhyme",
            "meta-learning algorithm",
            "fusion of features",
            "accuracy of classification",
            "effective in fusing",
            "two sets of features"
        ]
    },
    {
        "title": "Learning the Similarity of Audio Music in Bag-of-frames Representation from Tagged Music Data.",
        "author": [
            "Ju-Chiang Wang",
            "Hung-Shin Lee",
            "Hsin-Min Wang",
            "Shyh-Kang Jeng"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417219",
        "url": "https://doi.org/10.5281/zenodo.1417219",
        "ee": "https://zenodo.org/records/1417219/files/WangLWJ11.pdf",
        "abstract": "Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems. In this paper, we apply the bag-offrames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF modeling.",
        "zenodo_id": 1417219,
        "dblp_key": "conf/ismir/WangLWJ11",
        "keywords": [
            "cold-start problem",
            "low-level acoustic features",
            "Music Information Retrieval (MIR)",
            "bag-of-frames (BOF)",
            "Music tags",
            "Gaussian mixture model (GMM)",
            "audio-based query-by-example MIR",
            "audio-based music similarity computation",
            "performance improvement",
            "prior weights"
        ]
    },
    {
        "title": "Potential Relationship Discovery in Tag-Aware Music Style Clustering and Artist Social Networks.",
        "author": [
            "Dingding Wang 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418221",
        "url": "https://doi.org/10.5281/zenodo.1418221",
        "ee": "https://zenodo.org/records/1418221/files/WangO11.pdf",
        "abstract": "With the rapid growth of music information and data in today\u2019s ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods usually assume music items are independent of each other. However, in real world applications, do there exist some relationships among them. In this paper, we construct the social relation graph among different music artists by extracting the friendship information from social media such as Twitter, and incorporate the generated social networking graph into tag-based music style clustering. Experiments on real data show the effectiveness of this novel integration of different information sources.",
        "zenodo_id": 1418221,
        "dblp_key": "conf/ismir/WangO11",
        "keywords": [
            "music",
            "information",
            "analysis",
            "social",
            "media",
            "friendship",
            "graph",
            "tag-based",
            "music",
            "style"
        ]
    },
    {
        "title": "User studies in the Music Information Retrieval Literature.",
        "author": [
            "David M. Weigl",
            "Catherine Guastavino"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417811",
        "url": "https://doi.org/10.5281/zenodo.1417811",
        "ee": "https://zenodo.org/records/1417811/files/WeiglG11.pdf",
        "abstract": "This paper presents an overview of user studies in the Music Information Retrieval (MIR) literature. A focus on the user has repeatedly been identified as a key requirement for future MIR research; yet empirical user studies have been relatively sparse in the literature, the overwhelming research attention in MIR remaining systems-focused. We present research topics, methodologies, and design implications covered in the user studies conducted thus far.",
        "zenodo_id": 1417811,
        "dblp_key": "conf/ismir/WeiglG11",
        "keywords": [
            "user studies",
            "Music Information Retrieval",
            "key requirement",
            "future MIR research",
            "empirical user studies",
            "sparse in the literature",
            "systems-focused",
            "research topics",
            "methodologies",
            "design implications"
        ]
    },
    {
        "title": "Automatic Assessment of Singer Traits in Popular Music: Gender, Age, Height and Race.",
        "author": [
            "Felix Weninger",
            "Martin W\u00f6llmer",
            "Bj\u00f6rn W. Schuller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417821",
        "url": "https://doi.org/10.5281/zenodo.1417821",
        "ee": "https://zenodo.org/records/1417821/files/WeningerWS11.pdf",
        "abstract": "We investigate fully automatic recognition of singer traits, i. e., gender, age, height and \u2018race\u2019 of the main performing artist(s) in recorded popular music. Monaural source separation techniques are combined to simultaneously enhance harmonic parts and extract the leading voice. For evaluation the UltraStar database of 581 pop music songs with 516 distinct singers is chosen. Extensive test runs with Long Short-Term Memory sequence classification reveal that binary classification of gender, height, race and age reaches up to 89.6, 72.1, 63.3 and 57.6 % unweighted accuracy on beat level in unseen test data.",
        "zenodo_id": 1417821,
        "dblp_key": "conf/ismir/WeningerWS11",
        "keywords": [
            "fully automatic",
            "singer traits",
            "gender",
            "age",
            "height",
            "race",
            "pop music",
            "UltraStar database",
            "Long Short-Term Memory",
            "sequence classification"
        ]
    },
    {
        "title": "Adapting Metrics for Music Similarity Using Comparative Ratings.",
        "author": [
            "Daniel Wolff",
            "Tillman Weyde"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416440",
        "url": "https://doi.org/10.5281/zenodo.1416440",
        "ee": "https://zenodo.org/records/1416440/files/WolffW11.pdf",
        "abstract": "Understanding how we relate and compare pieces of music has been a topic of great interest in musicology as well as for business applications, such as music recommender systems. The way music is compared seems to vary among both individuals and cultures. Adapting a generic model to user ratings is useful for personalisation and can help to better understand such differences. This paper presents an approach to use machine learning techniques for analysing user data that specifies song similarity. We explore the potential for learning generalisable similarity measures with two stateof-the-art algorithms for learning metrics. We use the audio clips and user ratings in the MagnaTagATune dataset, enriched with genre annotations from the Magnatune label.",
        "zenodo_id": 1416440,
        "dblp_key": "conf/ismir/WolffW11",
        "keywords": [
            "musicology",
            "business applications",
            "music recommender systems",
            "individuals",
            "cultures",
            "generic model",
            "user ratings",
            "personalisation",
            "understanding differences",
            "machine learning techniques"
        ]
    },
    {
        "title": "A Two-Fold Dynamic Programming Approach to Beat Tracking for Audio Music with Time-Varying Tempo.",
        "author": [
            "Fu-Hai Frank Wu",
            "Tsung-Chi Lee",
            "Jyh-Shing Roger Jang",
            "Kaichun K. Chang",
            "Chun-Hung Lu",
            "Wen-Nan Wang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416782",
        "url": "https://doi.org/10.5281/zenodo.1416782",
        "ee": "https://zenodo.org/records/1416782/files/WuLJCLW11.pdf",
        "abstract": "Automatic beat tracking and tempo estimation challenging tasks, especially for audio music with timevarying tempo. This paper proposes a two-fold dynamic programming (DP) approach to deal with beat tracking with time-varying tempo. In particular, the first DP computes the tempo curve from the tempogram. The second DP identifies the optimum beat positions from the novelty and tempo curves. Experimental results demonstrate satisfactory performance for music with significant tempo variations. The proposed approach was submitted to the task of audio beat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable tempo.",
        "zenodo_id": 1416782,
        "dblp_key": "conf/ismir/WuLJCLW11",
        "keywords": [
            "Automatic",
            "beat",
            "tracking",
            "tempo",
            "estimation",
            "challenging",
            "tasks",
            "especially",
            "audio",
            "music"
        ]
    },
    {
        "title": "Segmentation, Clustering, and Display in a Personal Audio Database for Musicians.",
        "author": [
            "Guangyu Xia",
            "Dawen Liang",
            "Roger B. Dannenberg",
            "Mark Harvilla"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418185",
        "url": "https://doi.org/10.5281/zenodo.1418185",
        "ee": "https://zenodo.org/records/1418185/files/XiaLDH11.pdf",
        "abstract": "Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult passages). We introduce a novel music classification system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation.",
        "zenodo_id": 1418185,
        "dblp_key": "conf/ismir/XiaLDH11",
        "keywords": [
            "music classification system",
            "Eigenmusic",
            "Adaboost",
            "separation",
            "organizing",
            "alignment",
            "digital music display interface",
            "graphical input",
            "output",
            "conventional music notation"
        ]
    },
    {
        "title": "Fast Hamming Space Search for Audio Fingerprinting Systems.",
        "author": [
            "Qingmei Xiao",
            "Motoyuki Suzuki",
            "Kenji Kita"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418143",
        "url": "https://doi.org/10.5281/zenodo.1418143",
        "ee": "https://zenodo.org/records/1418143/files/XiaoSK11.pdf",
        "abstract": "In music information retrieval, a huge search space has to be explored because a query audio clip can start at any position of any music in the database, and also a query is often corrupted by significant noise and distortion. Audio fingerprints have recently attracted much attention in music information retrieval, for they provide a compact representation of the perceptually relevant parts of audio signals. In this paper, we propose an extremely fast method of exploring a huge Hamming space for audio fingerprinting systems. The effectiveness of the proposed method has been evaluated by experiments using a database of 8,740 songs.",
        "zenodo_id": 1418143,
        "dblp_key": "conf/ismir/XiaoSK11",
        "keywords": [
            "music information retrieval",
            "huge search space",
            "query audio clip",
            "database",
            "query corrupted",
            "audio fingerprints",
            "compact representation",
            "Hamming space",
            "audio fingerprinting systems",
            "efficacy evaluation"
        ]
    },
    {
        "title": "Music Tagging with Regularized Logistic Regression.",
        "author": [
            "Bo Xie 0002",
            "Wei Bian",
            "Dacheng Tao",
            "Parag Chordia"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415258",
        "url": "https://doi.org/10.5281/zenodo.1415258",
        "ee": "https://zenodo.org/records/1415258/files/XieBTC11.pdf",
        "abstract": "In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of music. We first vector-quantize the delta MFCC features using k-means and construct \u201cbag-of-words\u201d representation for each song. We then learn the parameters of these logistic regression algorithms from the \u201cbag-of-words\u201d vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve stateof-the-art performance in annotation and retrieval tasks evaluated on CAL-500.",
        "zenodo_id": 1415258,
        "dblp_key": "conf/ismir/XieBTC11",
        "keywords": [
            "delta MFCC features",
            "vector-quantization",
            "k-means",
            "bag-of-words representation",
            "logistic regression",
            "training set",
            "ground truth labels",
            "linear classifiers",
            "prediction confidence",
            "music annotation and retrieval tasks"
        ]
    },
    {
        "title": "A Vocabulary-Free Infinity-Gram Model for Nonparametric Bayesian Chord Progression Analysis.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417389",
        "url": "https://doi.org/10.5281/zenodo.1417389",
        "ee": "https://zenodo.org/records/1417389/files/YoshiiG11.pdf",
        "abstract": "This paper presents probabilistic n-gram models for symbolic chord sequences. To overcome the fundamental limitations in conventional models\u2014that the model optimality is not guaranteed, that the value of n is fixed uniquely, and that a vocabulary of chord types (e.g., major, minor, \u00b7 \u00b7 \u00b7 ) is defined in an arbitrary way\u2014we propose a vocabulary-free infinity-gram model based on Bayesian nonparametrics. It accepts any combinations of notes as chord types and allows each chord appearing in a sequence to have an unbounded and variable-length context. All possibilities of n are taken into account when calculating the predictive probability of a next chord given a particular context, and when an unseen chord type emerges we can avoid out-of-vocabulary error by adaptively evaluating the 0-gram probability, i.e., the combinatorial probability of note components. Our experiments using Beatles songs showed that the predictive performance of the proposed model is better than that of the state-of-theart models and that we could find stochastically-coherent chord patterns by sorting variable-length n-grams in a line according to their generative probabilities.",
        "zenodo_id": 1417389,
        "dblp_key": "conf/ismir/YoshiiG11",
        "keywords": [
            "probabilistic n-gram models",
            "symbolic chord sequences",
            "Bayesian nonparametrics",
            "vocabulary-free",
            "infinity-gram model",
            "conventional models limitations",
            "combinatorial probability",
            "stochastically-coherent chord patterns",
            "predictive probability",
            "unseen chord type handling"
        ]
    },
    {
        "title": "An Investigation of Musical Timbre: Uncovering Salient Semantic Descriptors and Perceptual Dimensions.",
        "author": [
            "Asteris I. Zacharakis",
            "Konstantinos Pastiadis",
            "Georgios Papadelis",
            "Joshua D. Reiss"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414964",
        "url": "https://doi.org/10.5281/zenodo.1414964",
        "ee": "https://zenodo.org/records/1414964/files/ZacharakisPPR11.pdf",
        "abstract": "A study on the verbal attributes of musical timbre was conducted in an effort to identify the most significant semantic descriptors and to quantify the association between prominent timbral aspects and several categorical properties of environmental entities. A verbal attribute magnitude estimation (VAME) type of listening test in which participants were asked to describe 23 musical sounds using 30 Greek adjectives together with verbal terms of their own choice was designed and conducted for this purpose. Factor and Cluster Analysis were performed on the subjective evaluation data in order to shed some light on the relationships between the adjectives that were proposed and to conclude to the number and quality of the salient perceptual dimensions required for the description of this set of sounds.",
        "zenodo_id": 1414964,
        "dblp_key": "conf/ismir/ZacharakisPPR11",
        "keywords": [
            "verbal attributes",
            "musical timbre",
            "semantic descriptors",
            "quantify association",
            "environmental entities",
            "verbal attribute magnitude estimation",
            "adjectives",
            "categorical properties",
            "listening test",
            "factor and cluster analysis"
        ]
    },
    {
        "title": "Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011, Miami, Florida, USA, October 24-28, 2011",
        "author": [
            "Anssi Klapuri",
            "Colby Leider"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://ismir2011.ismir.net/",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2011"
    }
]